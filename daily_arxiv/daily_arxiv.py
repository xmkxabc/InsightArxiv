#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ArXiv New Fetcher â€” å•æ–‡ä»¶å¯æ‰§è¡Œè„šæœ¬ï¼ˆæœ¬åœ°ä¸ GitHub Runner çš†å¯ç”¨ï¼‰

çƒ­ä¿®å¤ï¼ˆ2025-08-06 01:10 SGTï¼‰
- **æ ¹å›  A**ï¼šä¸åŒåˆ†ç±»é¡µé¢å­˜åœ¨ä¸¤ç§ç»“æ„ï¼š
  1) æ¯ä¸ªå°èŠ‚ï¼ˆNew / Cross / Replacementï¼‰å„åœ¨ **ç‹¬ç«‹çš„ `<dl>`** ä¸­ï¼›
  2) ä¸‰ä¸ªå°èŠ‚å…±ç”¨ **åŒä¸€ä¸ª `<dl>`**ï¼Œç”±å¤šä¸ª `<h3>` åˆ†æ®µã€‚æ—§å†™æ³•ç›´æ¥ `./dt` ä¼šæŠŠæ•´æ®µæ‰€æœ‰ `dt` ä¸€æŠŠæŠ“æˆ–æŠ“ä¸åˆ°ã€‚
- **æ ¹å›  B**ï¼šå°èŠ‚æ ‡é¢˜æ–‡æ¡ˆå­˜åœ¨å˜ä½“ï¼š`Cross lists / Cross-lists / Cross submissions`ã€`Replacement / Replacements / Replacement submissions`ã€‚
- **ä¿®å¤**ï¼š
  1) ç»Ÿä¸€ä»¥ **`<dl>` ä¸ºå¤–å±‚å®¹å™¨éå†**ï¼›è‹¥ä¸€ä¸ª `<dl>` å«æœ‰å¤šä¸ª `<h3>`ï¼Œåˆ™å¯¹ **æ¯ä¸ª `<h3>`** å‘åé€ä¸ªåŒçº§å…„å¼Ÿæ‰«æ `dt`ï¼Œ**åœ¨é‡åˆ°ä¸‹ä¸€ä¸ª `<h3>` æ—¶åœæ­¢**ï¼Œä»è€Œä¿è¯åªå–å½“å‰å°èŠ‚çš„æ¡ç›®ã€‚
  2) å°èŠ‚è¯†åˆ«çš„æ­£åˆ™æ›´å®½æ¾ï¼Œå…¼å®¹ä¸Šè¿°å˜ä½“ï¼›
  3) æ—¥å¿—å¢å¼ºï¼Œä¾¿äºæ ¸å¯¹â€œæ ‡é¢˜æ˜¾ç¤ºçš„æ¡æ•° vs å®é™…è§£æåˆ°çš„ `dt` æ•°â€ã€‚

å­—æ®µ & è¾“å‡ºï¼š
- ä¸ä½ çš„çº¦å®šä¿æŒä¸€è‡´ï¼š`primary_category`/`cate` è¾“å‡º **ä»£ç **ï¼ˆå¦‚ `cs.CR`ï¼‰ï¼›`url/pdf_url` ä½¿ç”¨ **http**ï¼Œ`new` æ®µé»˜è®¤è¡¥ `v1`ï¼›ä¸è¿½åŠ `.pdf`ï¼›
- `include_cross` / `include_repl` æ”¯æŒ **ç¨‹åºå¸¸é‡ / ç¯å¢ƒå˜é‡ / CLI** ä¸‰å±‚æ§åˆ¶ï¼ˆä¼˜å…ˆçº§ï¼šCLI > ENV > å¸¸é‡ï¼‰ã€‚

ä¾èµ–ï¼š
  pip install scrapy arxiv tqdm pydispatcher

ç”¨æ³•ï¼š
  æœ¬åœ°ï¼š  python scripts/arxiv_new_fetch.py               # è¾“å‡ºåˆ° ./arxiv_new_YYYYMMDD.jsonl
  æœ¬åœ°ï¼š  CATEGORIES="cs.AI,cs.CL" python scripts/arxiv_new_fetch.py --out today.jsonl
  Runnerï¼špython scripts/arxiv_new_fetch.py            # è‹¥è®¾ç½®äº† RAW_JSONL_FILE/TARGET_DATEï¼Œä¼šæŒ‰å…¶è¾“å‡º
"""

from __future__ import annotations
import os
import re
import sys
import json
import time
import argparse
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Tuple

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy import signals
from pydispatch import dispatcher

try:
    import arxiv  # ç”¨äºæ‰¹é‡å¯ŒåŒ–
    HAVE_ARXIV = True
except Exception:
    HAVE_ARXIV = False

try:
    from tqdm import tqdm
except Exception:
    # å…¼å®¹æ—  tqdm ç¯å¢ƒ
    def tqdm(x, **_):
        return x

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é»˜è®¤é…ç½®ï¼ˆå¯è¢« ç¨‹åºå¸¸é‡ / ç¯å¢ƒå˜é‡ / CLI è¦†ç›–ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_CATEGORIES: List[str] = [
    "cs.AI","cs.AR","cs.CC","cs.CE","cs.CG","cs.CL","cs.CR","cs.CV","cs.CY","cs.DB",
    "cs.DC","cs.DL","cs.DM","cs.DS","cs.ET","cs.FL","cs.GL","cs.GR","cs.GT","cs.HC",
    "cs.IR","cs.IT","cs.LG","cs.LO","cs.MA","cs.MM","cs.MS","cs.NA","cs.NE","cs.NI",
    "cs.OH","cs.OS","cs.PF","cs.PL","cs.RO","cs.SC","cs.SD","cs.SE","cs.SI","cs.SY",
    "eess.AS","eess.IV","eess.SP","eess.SY","math.NA","stat.AP","q-fin.MF",
]

# ğŸ‘‰ ä¾¿äºéªŒè¯ cross/replï¼Œé»˜è®¤å…ˆ **æ‰“å¼€**ï¼ˆä½ ä¹Ÿå¯é€šè¿‡ ENV/CLI æ”¹æˆå…³é—­ï¼‰
DEFAULT_INCLUDE_CROSS: bool = True
DEFAULT_INCLUDE_REPL:  bool = True

SGT = timezone(timedelta(hours=8))  # æ–°åŠ å¡æ—¶åŒºï¼ˆç”¨äºé»˜è®¤è¾“å‡ºæ–‡ä»¶åï¼‰

# ç¯å¢ƒå˜é‡è¾…åŠ©ï¼ˆæ”¯æŒå¤šåï¼‰
def env_bool_multi(names, default: bool) -> bool:
    if isinstance(names, str):
        names = [names]
    hit = None
    for name in names:
        v = os.getenv(name)
        if v is None:
            continue
        val = str(v).strip().lower() in ("1","true","t","yes","y")
        hit = val
        break
    return default if hit is None else hit

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Spider å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ArxivNewSpider(scrapy.Spider):
    name = "arxiv_new"
    allowed_domains = ["arxiv.org"]

    RE_CATEGORIES = re.compile(r"\(([a-z\-]+(?:\.[A-Za-z0-9\-]+)?)\)")
    RE_ID_FROM_ABS = re.compile(r"/abs/(.+)$")
    RE_STRIP_VER = re.compile(r"v\d+$")

    # æ®µè½è¯†åˆ«æ­£åˆ™ï¼ˆå¿½ç•¥å¤§å°å†™ï¼Œå…¼å®¹åˆ—è¡¨/æäº¤å¤šæ–‡æ¡ˆï¼‰
    PAT_NEW   = re.compile(r"\bnew\s+submissions\b", re.I)
    PAT_CROSS = re.compile(r"\bcross\b.*\b(lists?|submissions?)\b", re.I)
    PAT_REPL  = re.compile(r"\breplacement(s)?\b(?:.*\bsubmissions?\b)?", re.I)

    BASE_SETTINGS = dict(
        ROBOTSTXT_OBEY=True,
        USER_AGENT="InsightArxivBot/1.0 (+contact)",
        CONCURRENT_REQUESTS=12,
        CONCURRENT_REQUESTS_PER_DOMAIN=12,
        DOWNLOAD_TIMEOUT=20,
        RETRY_TIMES=2,
        AUTOTHROTTLE_ENABLED=True,
        AUTOTHROTTLE_START_DELAY=0.25,
        AUTOTHROTTLE_MAX_DELAY=2.0,
        RANDOMIZE_DOWNLOAD_DELAY=True,
        HTTPCACHE_ENABLED=False, # é»˜è®¤å…³é—­ç¼“å­˜ï¼ˆå¯å¼€å¯ï¼‰
        HTTPCACHE_EXPIRATION_SECS=1800,
        HTTPCACHE_DIR="httpcache",
        ITEM_PIPELINES={},
        # "LOG_LEVEL": "INFO",
    )

    def __init__(self, *, categories: List[str], window: str, show: int,
                 include_cross: bool, include_repl: bool):
        super().__init__()
        self.categories = categories
        self.window = window
        self.show = show
        self.include_cross = include_cross
        self.include_repl = include_repl
        self.start_urls = [f"https://arxiv.org/list/{c}/{self.window}?show={self.show}" for c in self.categories]
        self.seen_ids = set()  # è·¨åˆ†ç±»å»é‡ï¼ˆæŒ‰æ— ç‰ˆæœ¬ idï¼‰
        self.logger.info(
            f"[init] cats={len(self.categories)} window={self.window} show={self.show} "
            f"include_cross={self.include_cross} include_repl={self.include_repl}"
        )

    # å·¥å…·ï¼šå®‰å…¨å–æ–‡æœ¬ï¼ˆä¿ç•™åŸºæœ¬ç©ºæ ¼ï¼Œä½†å‹ç¼©å¤šç©ºæ ¼ï¼‰
    def _text(self, sel, css: str) -> str:
        if not sel:
            return ""
        t = " ".join(sel.css(css + " ::text").getall()).replace("\n", " ")
        while "  " in t:
            t = t.replace("  ", " ")
        return t.strip()

    def _sec_flag_from_title(self, title: str) -> str | None:
        low = " ".join(title.split()).lower()
        if self.PAT_NEW.search(low):
            return "new"
        if self.PAT_CROSS.search(low):
            return "cross"
        if self.PAT_REPL.search(low):
            return "repl"
        return None

    def _iter_dt_dd_between_h3(self, h3) -> List[Tuple[scrapy.Selector, scrapy.Selector | None]]:
        """ä»å½“å‰ `<h3>` èµ·å‘åæ‰«æåŒçº§å…„å¼Ÿï¼Œæ”¶é›†ç›´åˆ°ä¸‹ä¸€ä¸ª `<h3>`ï¼Œè¿”å› (dt, dd) åˆ—è¡¨ã€‚"""
        out: List[Tuple[scrapy.Selector, scrapy.Selector | None]] = []
        for sib in h3.xpath("following-sibling::*"):
            name = sib.xpath("name()").get("")
            if not name:
                continue
            name = name.lower()
            if name == "h3":
                break
            if name == "dt":
                dd = sib.xpath("following-sibling::*[1][self::dd]")
                out.append((sib, dd[0] if dd else None))
        return out

    def parse(self, response):
        cat = response.url.split("/")[-2]
        self.logger.info(f"[parse] {cat}: {response.url}")

        total_yield = 0

        # éå†æ‰€æœ‰ <dl> å®¹å™¨ï¼›å…¼å®¹ï¼šä¸€ä¸ª dl åªæœ‰ä¸€ä¸ª h3ï¼Œæˆ–ä¸€ä¸ª dl æœ‰å¤šä¸ª h3
        for dl in response.xpath("//div[@id='dlpage']//dl"):
            h3_nodes = dl.xpath("./h3")
            if not h3_nodes:
                continue

            # æƒ…å†µä¸€ï¼šä¸€ä¸ª dl åªæœ‰ä¸€ä¸ª h3ï¼Œç›´æ¥ ./dt å–è¯¥å°èŠ‚æ¡ç›®
            if len(h3_nodes) == 1:
                h3 = h3_nodes[0]
                title = " ".join(h3.xpath(".//text()").getall()).strip()
                sec_flag = self._sec_flag_from_title(title)
                if not sec_flag:
                    continue
                if sec_flag == "cross" and not self.include_cross:
                    continue
                if sec_flag == "repl" and not self.include_repl:
                    continue
                dt_list = dl.xpath("./dt")
                self.logger.info(f"  section={sec_flag} dt_nodes={len(dt_list)} title='{title}'")
                for dt in dt_list:
                    dd = dt.xpath("following-sibling::dd[1]")
                    dd = dd[0] if dd else None
                    item = self._build_item_from_dt_dd(dt, dd, sec_flag)
                    if item is not None:
                        total_yield += 1
                        yield item
            else:
                # æƒ…å†µäºŒï¼šä¸€ä¸ª dl å«å¤šä¸ª h3ï¼Œå°èŠ‚é—´åŒçº§åˆ†éš”ï¼›å¯¹æ¯ä¸ª h3 å‘åæ‰«åˆ°ä¸‹ä¸€ä¸ª h3
                for h3 in h3_nodes:
                    title = " ".join(h3.xpath(".//text()").getall()).strip()
                    sec_flag = self._sec_flag_from_title(title)
                    if not sec_flag:
                        continue
                    if sec_flag == "cross" and not self.include_cross:
                        continue
                    if sec_flag == "repl" and not self.include_repl:
                        continue
                    pairs = self._iter_dt_dd_between_h3(h3)
                    self.logger.info(f"  section={sec_flag} dt_nodes={len(pairs)} title='{title}'")
                    for dt, dd in pairs:
                        item = self._build_item_from_dt_dd(dt, dd, sec_flag)
                        if item is not None:
                            total_yield += 1
                            yield item

        self.logger.info(f"  yielded={total_yield}")

    # å°†ä¸€ä¸ª (dt, dd) èŠ‚ç‚¹è§£æä¸º itemï¼›å¤±è´¥è¿”å› None
    def _build_item_from_dt_dd(self, dt: scrapy.Selector, dd: scrapy.Selector | None, sec_flag: str):
        # 1) abs é“¾æ¥
        abs_href = dt.xpath(".//a[contains(@href, '/abs/')]/@href").get()
        if not abs_href:
            return None
        m = self.RE_ID_FROM_ABS.search(abs_href)
        if not m:
            return None
        full_id = m.group(1)                          # 2508.00906 æˆ– 2508.00906v1
        base_id = self.RE_STRIP_VER.sub("", full_id)  # å»ç‰ˆæœ¬ï¼š2508.00906
        if base_id in self.seen_ids:
            return None
        self.seen_ids.add(base_id)

        # 2) æ ‡é¢˜ / ä½œè€…
        title_txt = (self._text(dd, ".list-title").replace("Title:", "").strip() if dd is not None else "")
        authors = (dd.css(".list-authors a::text").getall() if dd is not None else [])

        # 3) åˆ†ç±»ï¼šæ‰€æœ‰ä»£ç  + ä¸»ç±»ä»£ç 
        subjects_text_for_codes = self._text(dd, ".list-subjects") if dd is not None else ""
        all_cats = sorted(set(self.RE_CATEGORIES.findall(subjects_text_for_codes)))
        primary_code = ""
        if dd is not None:
            primary_block = self._text(dd, ".list-subjects .primary-subject")
            m2 = self.RE_CATEGORIES.search(primary_block)
            if m2:
                primary_code = m2.group(1)
        if not primary_code and all_cats:
            primary_code = all_cats[0]

        # 4) ç‰ˆæœ¬åŒ–é“¾æ¥
        if "v" in full_id:
            versioned = full_id
        elif sec_flag == "new":
            versioned = base_id + "v1"
        else:
            versioned = base_id

        abs_url = f"http://arxiv.org/abs/{versioned}"
        pdf_url = f"http://arxiv.org/pdf/{versioned}"

        return {
            "id": base_id,
            "title": title_txt,
            "authors": authors,
            "categories": all_cats,
            "primary_category": primary_code,   # ä»£ç ï¼Œå¦‚ cs.CR
            "pdf_link": None,
            "comments": "",
            "url": abs_url,
            "summary": "",
            "comment": None,
            "pdf_url": pdf_url,
            "cate": primary_code,
            "date": None,
            "updated": None,
            "section": sec_flag,
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ‰¹é‡å…ƒæ•°æ®è¡¥é½ï¼ˆæŒ‰æ‰¹ 200 æ¡ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enrich_with_arxiv_api(rows: List[Dict], batch_size: int, delay_sec: float, retries: int) -> List[Dict]:
    """ä½¿ç”¨ arXiv å®˜æ–¹ API æ‰¹é‡è¡¥é½æ‘˜è¦ã€è¯„è®ºã€æ—¶é—´ç­‰ï¼›å¤±è´¥ä¸ä¸­æ–­ã€‚"""
    if not rows or not HAVE_ARXIV:
        return rows

    client = arxiv.Client(page_size=batch_size, delay_seconds=delay_sec, num_retries=retries)

    ids = [r["id"] for r in rows if r.get("id")]

    def chunked(seq, n):
        for i in range(0, len(seq), n):
            yield seq[i:i+n]

    meta: Dict[str, Dict] = {}
    for chunk in tqdm(list(chunked(ids, batch_size)), desc="arXiv API", unit="batch"):
        results = []
        for attempt in range(retries):
            try:
                search = arxiv.Search(id_list=chunk)
                results = list(client.results(search))
                break
            except Exception:
                if attempt == retries - 1:
                    results = []
                else:
                    time.sleep(2)
        for r in results:
            rid = getattr(r, "get_short_id", lambda: None)() or r.entry_id.split("/")[-1]
            rid = re.sub(r"v\d+$", "", rid)
            authors = [getattr(a, "name", str(a)) for a in (getattr(r, "authors", None) or [])]
            cats = []
            if getattr(r, "categories", None):
                cats = sorted({str(c) for c in r.categories})
            primary = ""
            if getattr(r, "primary_category", None):
                primary = str(r.primary_category)
            meta[rid] = {
                "title": (getattr(r, "title", "") or "").strip(),
                "authors": authors,
                "abstract": getattr(r, "summary", None),
                "comment": getattr(r, "comment", None) or None,
                "categories": cats,
                "primary_category": primary,
                "abs_url": f"http://arxiv.org/abs/{r.entry_id.split('/')[-1]}",
                "pdf_url": f"http://arxiv.org/pdf/{r.entry_id.split('/')[-1]}",
                "published_at": getattr(r, "published", None).date().isoformat() if getattr(r, "published", None) else None,
                "updated_at": getattr(r, "updated", None).date().isoformat() if getattr(r, "updated", None) else None,
            }
        time.sleep(delay_sec)

    out: List[Dict] = []
    for row in rows:
        iid = row.get("id")
        add = meta.get(iid, {})
        merged = dict(row)
        # ä»…åœ¨ç¼ºå¤±æ—¶è¡¥åŸºç¡€å­—æ®µ
        for k, v in (
            ("title", add.get("title")),
            ("authors", add.get("authors")),
            ("categories", add.get("categories")),
            ("pdf_url", add.get("pdf_url")),
        ):
            if not merged.get(k) and v:
                merged[k] = v
        # æ‘˜è¦/æ—¥æœŸï¼šæŒ‰ä½ çš„ç¤ºä¾‹å†™å…¥
        if add.get("abstract") is not None:
            merged["summary"] = add["abstract"] or ""
        if add.get("published_at"):
            merged["date"] = add["published_at"]  # YYYY-MM-DD
        if add.get("updated_at"):
            merged["updated"] = add["updated_at"]
        out.append(merged)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·ï¼šå†™ JSONLï¼ˆåŸå­æ›¿æ¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_jsonl(rows: List[Dict], path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    os.replace(tmp, path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI ä¸ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_args():
    # è¾“å‡ºä¼˜å…ˆï¼šOUT_PATH > RAW_JSONL_FILE > TARGET_DATE æ¨å¯¼ > ./arxiv_new_YYYYMMDD.jsonl
    env_out = os.getenv("OUT_PATH") or os.getenv("RAW_JSONL_FILE")
    if not env_out and os.getenv("TARGET_DATE"):
        env_out = f"data/{os.environ['TARGET_DATE']}.jsonl"

    env_categories = os.getenv("CATEGORIES")
    env_window = os.getenv("WINDOW")
    env_show = os.getenv("SHOW")

    # include_cross / include_repl é»˜è®¤ä½¿ç”¨ç¨‹åºå¸¸é‡ï¼Œå¯è¢« ENV/CLI è¦†ç›–
    include_cross_default = env_bool_multi(["INCLUDE_CROSS", "include_cross"], DEFAULT_INCLUDE_CROSS)
    include_repl_default  = env_bool_multi(["INCLUDE_REPL", "include_repl"], DEFAULT_INCLUDE_REPL)

    p = argparse.ArgumentParser(description="Fetch arXiv /new for multiple categories and output JSONL.")
    p.add_argument("--out", default=env_out or os.path.join(".", f"arxiv_new_{datetime.now(SGT).strftime('%Y%m%d')}.jsonl"),
                   help="è¾“å‡º JSONL è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰ï¼›è‹¥è®¾ç½® RAW_JSONL_FILE/TARGET_DATE ä¼šè‡ªåŠ¨åŒ¹é… Actions å‘½å")
    p.add_argument("--categories", default=env_categories or ",".join(DEFAULT_CATEGORIES),
                   help="é€—å·åˆ†éš”çš„åˆ†ç±»åˆ—è¡¨ï¼ˆENV:CATEGORIES å¯è¦†ç›–ï¼‰")
    p.add_argument("--window", choices=["new","recent","pastweek"], default=(env_window or "new"),
                   help="åˆ—è¡¨çª—å£ï¼ˆé»˜è®¤ newï¼›ENV:WINDOW å¯è¦†ç›–ï¼‰")
    p.add_argument("--show", type=int, default=int(env_show or 2000),
                   help="æ¯é¡µå±•ç¤ºæ¡æ•°ï¼ˆé»˜è®¤ 2000ï¼›ENV:SHOW å¯è¦†ç›–ï¼‰")

    # æ®µè½å¼€å…³ï¼ˆé»˜è®¤æ‰“å¼€ä»¥ä¾¿éªŒè¯ï¼›å¯ç”¨ ç¨‹åºå¸¸é‡ / ENV / CLI è°ƒæ•´ï¼‰
    p.add_argument("--include-cross", action="store_true", default=include_cross_default, help="åŒ…å« Cross-lists / Cross submissions")
    p.add_argument("--include-repl", action="store_true", default=include_repl_default, help="åŒ…å« Replacements / Replacement submissions")

    # Scrapy è°ƒä¼˜
    p.add_argument("--concurrent-req", type=int, default=int(os.getenv("CONCURRENT_REQUESTS", 12)))
    p.add_argument("--concurrent-per-domain", type=int, default=int(os.getenv("CR_PER_DOMAIN", 12)))
    p.add_argument("--download-timeout", type=int, default=int(os.getenv("DOWNLOAD_TIMEOUT", 20)))
    p.add_argument("--retry-times", type=int, default=int(os.getenv("RETRY_TIMES", 2)))
    p.add_argument("--at-start", type=float, default=float(os.getenv("AT_START", 0.25)))
    p.add_argument("--at-max", type=float, default=float(os.getenv("AT_MAX", 2.0)))
    p.add_argument("--httpcache-ttl", type=int, default=int(os.getenv("CACHE_TTL", 1800)))
    p.add_argument("--log-level", default=os.getenv("LOG_LEVEL", "INFO"))

    # å…ƒæ•°æ®å¯ŒåŒ–ï¼ˆé»˜è®¤ Trueï¼‰
    p.add_argument("--enrich", dest="enrich", action="store_true", help="æŠ“å–åä½¿ç”¨ arXiv API æ‰¹é‡è¡¥é½å…ƒæ•°æ®")
    p.add_argument("--no-enrich", dest="enrich", action="store_false", help="ä¸åšå…ƒæ•°æ®è¡¥é½ï¼ˆæ›´å¿«ï¼‰")
    p.set_defaults(enrich=env_bool_multi(["ENRICH", "enrich"], True))

    p.add_argument("--batch-size", type=int, default=int(os.getenv("ARXIV_BATCH_SIZE", 200)), help="arXiv API æ¯æ‰¹æ¡æ•°")
    p.add_argument("--delay-sec", type=float, default=float(os.getenv("ARXIV_DELAY_SEC", 3.0)), help="æ‰¹é—´éš”ç§’")
    p.add_argument("--retries", type=int, default=int(os.getenv("ARXIV_RETRIES", 5)), help="API é‡è¯•æ¬¡æ•°")

    return p.parse_args()


def main():
    args = parse_args()

    categories = [c.strip() for c in args.categories.split(",") if c.strip()]

    # å°† CLI è¦†ç›–åˆ° Scrapy è®¾ç½®ï¼ˆç¡®ä¿ CLI ç”Ÿæ•ˆï¼‰
    settings = dict(ArxivNewSpider.BASE_SETTINGS)
    settings.update(dict(
        CONCURRENT_REQUESTS=args.concurrent_req,
        CONCURRENT_REQUESTS_PER_DOMAIN=args.concurrent_per_domain,
        DOWNLOAD_TIMEOUT=args.download_timeout,
        RETRY_TIMES=args.retry_times,
        AUTOTHROTTLE_START_DELAY=args.at_start,
        AUTOTHROTTLE_MAX_DELAY=args.at_max,
        HTTPCACHE_EXPIRATION_SECS=args.httpcache_ttl,
        LOG_LEVEL=args.log_level,
    ))

    collected: List[Dict] = []

    def _collect_item(item, response, spider):
        try:
            collected.append(dict(item))
        except Exception:
            pass

    dispatcher.connect(_collect_item, signal=signals.item_scraped)

    process = CrawlerProcess(settings=settings)
    process.crawl(ArxivNewSpider,
                  categories=categories,
                  window=args.window,
                  show=args.show,
                  include_cross=args.include_cross,
                  include_repl=args.include_repl)

    try:
        process.start()  # é˜»å¡ç›´åˆ°çˆ¬å®Œ
    except Exception as e:
        print(f"âŒ Scrapy è¿è¡Œå¤±è´¥ï¼š{e}", file=sys.stderr)
        sys.exit(2)

    rows = collected
    if args.enrich:
        if not HAVE_ARXIV:
            print("âš ï¸ æœªå®‰è£… arxiv åº“ï¼Œè·³è¿‡å¯ŒåŒ–ï¼ˆpip install arxivï¼‰")
        else:
            rows = enrich_with_arxiv_api(collected, args.batch_size, args.delay_sec, args.retries)

    write_jsonl(rows, args.out)
    print(f"\nâœ… å®Œæˆï¼š{len(rows)} æ¡ï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{args.out}")


if __name__ == "__main__":
    main()
