import os
import glob
import json
import re
from collections import defaultdict, Counter
from datetime import datetime
import multiprocessing
from functools import partial
import shutil

# --- æ–°å¢ï¼šå¼•å…¥NLTKè¿›è¡Œè¯å½¢è¿˜åŸï¼Œæå‡æœç´¢è´¨é‡ ---
# é¦–æ¬¡è¿è¡Œæ—¶ï¼Œéœ€è¦å®‰è£…NLTK: pip install nltk
try:
    import nltk
    from nltk.stem import WordNetLemmatizer
    # æ£€æŸ¥å¹¶ä¸‹è½½NLTKæ‰€éœ€æ•°æ®
    try:
        nltk.data.find('corpora/wordnet')
        nltk.data.find('corpora/omw-1.4')
    except LookupError:
        print("é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨ä¸‹è½½NLTKæ•°æ®åŒ… (wordnet, omw-1.4)...")
        nltk.download('wordnet')
        nltk.download('omw-1.4')
    lemmatizer = WordNetLemmatizer()
    print("âœ… NLTK WordNetLemmatizer loaded.")
except ImportError:
    nltk = None
    lemmatizer = None
    print("âš ï¸ Warning: 'nltk' not installed. English search will not use lemmatization.")

# å°è¯•å¯¼å…¥jiebaç”¨äºä¸­æ–‡åˆ†è¯
try:
    import jieba
    print("âœ… Jieba loaded for Chinese tokenization.")
except ImportError:
    jieba = None
    print("âš ï¸ Warning: 'jieba' not installed. Chinese search will be limited to exact keyword matches.")

# æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨ï¼Œç”¨äºæ„å»ºæœç´¢ç´¢å¼•æ—¶å¿½ç•¥è¿™äº›å¸¸è§è¯
STOP_WORDS = {
    # English
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'this', 'that', 'these', 'those', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',
    'might', 'must', 'can', 'shall', 'we', 'they', 'you', 'it', 'he', 'she', 'his', 'her',
    'its', 'their', 'our', 'your', 'my', 'me', 'him', 'them', 'us', 'from', 'up', 'out',
    'down', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there',
    'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',
    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',
    'can', 'just', 'now', 'also', 'however', 'although', 'though',
    # Domain-specific English
    'paper', 'method', 'approach', 'model', 'result', 'results', 'show', 'shows', 'using',
    'used', 'use', 'based', 'propose', 'proposed', 'algorithm', 'algorithms', 'study',
    'studies', 'work', 'new', 'novel', 'task', 'tasks', 'data', 'dataset',
    # Chinese
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'ä½ ', 'ä»–', 'å¥¹', 'ä¸º', 'ä¹‹', 'ä»¥', 'è€Œ', 'äº', 'ç­‰', 'ä¹Ÿ',
    'è¿˜', 'å°±', 'éƒ½', 'äºº', 'å› ', 'æ­¤', 'è¢«', 'ä»', 'åˆ°', 'ç€', 'ä¸ª', 'ä»¬', 'å…¶ä¸­', 'å¦‚æœ', 'é‚£ä¹ˆ',
    'æˆ‘ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒ', 'å®ƒä»¬', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å¹¶ä¸”', 'è€Œä¸”', 'ç ”ç©¶', 'è®ºæ–‡',
    'æ–¹æ³•', 'æ¨¡å‹', 'ç»“æœ', 'åŸºäº', 'é€šè¿‡', 'ä½¿ç”¨', 'æ˜¾ç¤º', 'æå‡º', 'ä¸€ä¸ª', 'ä¸€ç§', 'æœ¬æ–‡'
}

is_english_word = re.compile(r'^[a-z]+$')

def tokenize_text(text: str) -> set:
    """
    ä¸€ä¸ªç»Ÿä¸€çš„ã€æ›´æ™ºèƒ½çš„åˆ†è¯å™¨ï¼Œç”¨äºå¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ (V4 - å¼•å…¥N-grams)ã€‚
    - è½¬æ¢ä¸ºå°å†™ã€‚
    - ä½¿ç”¨ jieba (å¦‚æœå¯ç”¨) è¿›è¡Œä¸­è‹±æ–‡æ··åˆåˆ†è¯ã€‚
    - ä½¿ç”¨ NLTK (å¦‚æœå¯ç”¨) å¯¹è‹±æ–‡å•è¯è¿›è¡Œè¯å½¢è¿˜åŸ (Lemmatization)ï¼Œä¾‹å¦‚ 'models' -> 'model'ã€‚
    - ç”Ÿæˆ unigrams, bigrams, å’Œ trigrams (ä¸€å…ƒã€äºŒå…ƒã€ä¸‰å…ƒè¯ç»„) ä»¥æ”¯æŒçŸ­è¯­æœç´¢ã€‚
    - ç§»é™¤åœç”¨è¯ã€è¿‡çŸ­çš„è¯å’Œçº¯æ•°å­—ã€‚
    """
    if not text or not isinstance(text, str):
        return set()

    # 1. é¢„å¤„ç†
    text = text.lower()
    text = re.sub(r'[-_]', ' ', text)

    # 2. åˆ†è¯
    if jieba:
        # ä½¿ç”¨æœç´¢æ¨¡å¼åˆ†è¯ï¼Œèƒ½æ›´å¥½åœ°å¤„ç†å¾…æœç´¢çš„æ–‡æœ¬
        initial_tokens = jieba.cut_for_search(text)
    else:
        # ç®€å•çš„è‹±æ–‡/æ•°å­—åˆ†è¯å™¨
        initial_tokens = re.findall(r'[a-z]+', text) # åªåŒ¹é…å­—æ¯ï¼Œå¿½ç•¥æ•°å­—

    # 3. æ¸…ç†ã€è¯å½¢è¿˜åŸå’Œè¿‡æ»¤ï¼Œç”Ÿæˆä¸€ä¸ªæœ‰åºçš„è¯å…ƒåˆ—è¡¨
    cleaned_tokens = []
    for token in initial_tokens:
        clean_token = token.strip()
        
        # å¯¹çº¯è‹±æ–‡å­—æ¯çš„è¯è¿›è¡Œè¯å½¢è¿˜åŸ (å¦‚æœNLTKå¯ç”¨)
        if lemmatizer and is_english_word.match(clean_token):
            clean_token = lemmatizer.lemmatize(clean_token)

        # è¿‡æ»¤æ‰åœç”¨è¯å’Œè¿‡çŸ­çš„è¯
        if clean_token and clean_token not in STOP_WORDS and not clean_token.isdigit() and len(clean_token) >= 2:
            cleaned_tokens.append(clean_token)

    # 4. ç”Ÿæˆ N-grams (ä¸€å…ƒã€äºŒå…ƒã€ä¸‰å…ƒè¯ç»„)
    all_grams = set(cleaned_tokens)  # é¦–å…ˆæ·»åŠ æ‰€æœ‰å•ä¸ªè¯ (unigrams)
    # ç”ŸæˆäºŒå…ƒè¯ç»„ (bigrams)
    all_grams.update(" ".join(cleaned_tokens[i:i+2]) for i in range(len(cleaned_tokens) - 1))
    # ç”Ÿæˆä¸‰å…ƒè¯ç»„ (trigrams)
    all_grams.update(" ".join(cleaned_tokens[i:i+3]) for i in range(len(cleaned_tokens) - 2))
            
    return all_grams

def process_paper_for_indexing(paper_data: dict) -> tuple:
    """
    å¤„ç†å•ä¸ªè®ºæ–‡ï¼Œæå–ç”¨äºæœç´¢å’Œåˆ†ç±»ç´¢å¼•çš„ä¿¡æ¯ã€‚
    è¿™æ˜¯ä¸ºå¤šè¿›ç¨‹è®¾è®¡çš„ç‹¬ç«‹å·¥ä½œå•å…ƒã€‚
    """
    paper_id = paper_data.get("id")
    if not paper_id:
        return None, None, None, None

    # 1. æå–å¹¶å¤„ç†ä¸»è¦æ–‡æœ¬çš„æœç´¢è¯å…ƒ (title, abstract, etc.)
    main_text_to_index = " ".join(filter(None, [
        paper_data.get("title", ""),
        paper_data.get("abstract", ""),
        paper_data.get("zh_title", ""),
        paper_data.get("translation", ""),
        paper_data.get("tldr", ""),
        paper_data.get("ai_comments", ""),
        paper_data.get("comment", ""),
    ]))
    search_tokens = tokenize_text(main_text_to_index)

    # 2. å•ç‹¬å¤„ç†å¹¶æ·»åŠ é«˜è´¨é‡çš„å…³é”®è¯çŸ­è¯­
    keyword_phrases = set()
    for keyword in paper_data.get("keywords", []):
        if keyword and isinstance(keyword, str):
            # æ¸…ç†å…³é”®è¯ï¼šè½¬å°å†™ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
            clean_keyword = " ".join(keyword.lower().strip().split())
            if clean_keyword and len(clean_keyword) > 2 and clean_keyword not in STOP_WORDS:
                keyword_phrases.add(clean_keyword)
    
    # 3. åˆå¹¶ä¸¤ç§è¯å…ƒ
    all_search_tokens = search_tokens.union(keyword_phrases)

    # æå–åˆ†ç±»
    categories = paper_data.get("categories", [])

    # æŒ‰æœˆä»½èšåˆ
    year_month = paper_data.get('date', '')[:7]

    return paper_id, all_search_tokens, categories, year_month

def build_database_from_jsonl_fixed():
    """
    æ„å»ºæ•°æ®åº“çš„ä¸»å‡½æ•°ã€‚
    å®ƒä» 'data' ç›®å½•ä¸‹çš„æ‰€æœ‰ *_AI_enhanced_Chinese.jsonl æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œ
    å¹¶æ™ºèƒ½åœ°å¤„ç†è®ºæ–‡ç‰ˆæœ¬æ›´æ–°ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆæœ¬ã€‚ç„¶åç”Ÿæˆï¼š
    1. æŒ‰æœˆä»½åˆ†ç‰‡çš„æ•°æ®æ–‡ä»¶ (database-YYYY-MM.json)
    2. ä¸€ä¸ªæ¸…å•æ–‡ä»¶ (index.json)
    3. ä¸€ä¸ªä¸“ç”¨çš„æœç´¢ç´¢å¼•æ–‡ä»¶ (search_index.json)
    4. ä¸€ä¸ªå…¨æ–°çš„åˆ†ç±»ç´¢å¼•æ–‡ä»¶ (category_index.json)
    """
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æœ€æ–°ç‰ˆæœ¬çš„è®ºæ–‡ï¼Œé”®ä¸ºåŸºç¡€ID (e.g., "2401.12345")
    all_papers_map = {} 
    skipped_paper_count = 0

    output_dir = "docs/data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºç›®å½•: {output_dir}")

    jsonl_files = glob.glob("data/*_AI_enhanced_Chinese.jsonl")
    if not jsonl_files:
        print("é”™è¯¯: åœ¨ 'data' ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• '_AI_enhanced_Chinese.jsonl' æ–‡ä»¶ã€‚")
        return

    print(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ª .jsonl æ•°æ®æºæ–‡ä»¶ã€‚å¼€å§‹å¤„ç†...")

    # --- ç¬¬ä¸€é˜¶æ®µï¼šè¯»å–æ‰€æœ‰æ•°æ®ï¼Œå»é‡å¹¶åªä¿ç•™æœ€æ–°ç‰ˆæœ¬ ---
    for jsonl_file in sorted(jsonl_files): # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        base_name = os.path.basename(jsonl_file)
        date_from_filename_match = re.match(r'(\d{4}-\d{2}-\d{2})', base_name)
        if not date_from_filename_match:
            print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å {base_name} ä¸­æå–æ—¥æœŸï¼Œå·²è·³è¿‡ã€‚")
            continue
        file_date = date_from_filename_match.group(1)

        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    raw_data = json.loads(line)
                    paper_id_full = raw_data.get("id")

                    if not paper_id_full or not isinstance(paper_id_full, str):
                        skipped_paper_count += 1
                        continue

                    # --- [æ ¸å¿ƒç‰ˆæœ¬æ›´æ–°é€»è¾‘ - ä¼˜åŒ–ç‰ˆ] ---
                    # æ­¥éª¤ 1: æå–åŸºç¡€IDå’Œç‰ˆæœ¬å·
                    match = re.match(r'(\d+\.\d+)(v\d+)?', paper_id_full)
                    if not match:
                        skipped_paper_count += 1
                        continue
                    
                    base_id = match.group(1)
                    current_version = int(match.group(2)[1:]) if match.group(2) else 1

                    # æ­¥éª¤ 2: æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    existing_paper = all_papers_map.get(base_id)
                    if existing_paper and existing_paper.get('_version', 0) > current_version:
                        # å¦‚æœå·²å­˜åœ¨çš„ç‰ˆæœ¬æ˜ç¡®é«˜äºå½“å‰ç‰ˆæœ¬ï¼Œåˆ™è·³è¿‡ã€‚
                        # æ³¨æ„ï¼šè¿™é‡Œåªç”¨ > è€Œä¸æ˜¯ >=ï¼Œå…è®¸ç‰ˆæœ¬ç›¸åŒæ—¶è¿›è¡Œå…ƒæ•°æ®æ›´æ–°ã€‚
                        continue

                    # æ­¥éª¤ 3: æ— è®ºå¦‚ä½•éƒ½å…ˆè¿›è¡Œæ•°æ®æ•´å½¢ï¼Œå‡†å¤‡ä¸€ä¸ªå®Œæ•´çš„ paper_data å¯¹è±¡
                    ai_enhanced_info = raw_data.get("AI", {})
                    keywords_str = ai_enhanced_info.get("keywords", "")
                    keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()] if isinstance(keywords_str, str) else []

                    current_paper_data = {
                        "id": base_id, # å­˜å‚¨åŸºç¡€ID
                        "full_id": paper_id_full, # ä¿ç•™å®Œæ•´IDä¾›å‚è€ƒ
                        "_version": current_version, # å†…éƒ¨ä½¿ç”¨ï¼Œè®°å½•ç‰ˆæœ¬å·
                        "title": raw_data.get("title", "æ— æ ‡é¢˜"),
                        "date": file_date,
                        "url": raw_data.get("url", f"http://arxiv.org/abs/{paper_id_full}"),
                        "pdf_url": raw_data.get("pdf_link", f"http://arxiv.org/pdf/{paper_id_full}"),
                        "authors": ", ".join(raw_data.get("authors", [])),
                        "abstract": raw_data.get("summary", raw_data.get("abstract", "")),
                        "comment": raw_data.get("comment", raw_data.get("comments", "")),
                        "categories": raw_data.get("categories", []),
                        "updated": raw_data.get("updated", file_date),
                        "first_published": raw_data.get("published", raw_data.get("date", file_date)),
                        "zh_title": ai_enhanced_info.get("title_translation"),
                        "translation": ai_enhanced_info.get("translation"),
                        "keywords": keywords_list,
                        "tldr": ai_enhanced_info.get("tldr"),
                        "ai_comments": ai_enhanced_info.get("comments"),
                        "motivation": ai_enhanced_info.get("motivation"),
                        "method": ai_enhanced_info.get("method"),
                        "results": ai_enhanced_info.get("result"),
                        "conclusion": ai_enhanced_info.get("conclusion")
                    }

                    # æ­¥éª¤ 4: æ‰§è¡Œæœ€ç»ˆçš„æ›´æ–°é€»è¾‘
                    if not existing_paper or current_version > existing_paper.get('_version', 0):
                        # å¦‚æœæ˜¯æ–°è®ºæ–‡ï¼Œæˆ–è€…å½“å‰ç‰ˆæœ¬æ›´é«˜ï¼Œåˆ™å®Œå…¨æ›¿æ¢
                        all_papers_map[base_id] = current_paper_data
                    elif current_version == existing_paper.get('_version', 0):
                        # å¦‚æœç‰ˆæœ¬ç›¸åŒï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ€§åœ°æ›´æ–°å…ƒæ•°æ®ï¼Œæ¯”å¦‚ä¿ç•™æ›´æ—©çš„å‘å¸ƒæ—¥æœŸ
                        existing_paper['first_published'] = min(existing_paper['first_published'], current_paper_data['first_published'])
                        existing_paper['updated'] = max(existing_paper['updated'], current_paper_data['updated'])
                        if current_paper_data.get("comment"):
                            existing_paper['comment'] = current_paper_data['comment']

                except (json.JSONDecodeError, AttributeError, TypeError) as e:
                    # print(f"Skipping line due to error: {e}") # for debugging
                    skipped_paper_count += 1
                    continue

    total_paper_count = len(all_papers_map)
    if total_paper_count == 0:
        print("è­¦å‘Š: æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•è®ºæ–‡ã€‚")
        return

    print(f"\nå¤„ç†å®Œæˆã€‚æ•°æ®åº“å°†åŒ…å« {total_paper_count} ç¯‡å”¯ä¸€çš„æœ€æ–°ç‰ˆæœ¬è®ºæ–‡ã€‚")
    if skipped_paper_count > 0:
        print(f"å› æ ¼å¼é”™è¯¯æˆ–ç‰ˆæœ¬é™ˆæ—§ï¼Œæ€»å…±è·³è¿‡äº† {skipped_paper_count} æ¡è®°å½•ã€‚")

    # --- å‡†å¤‡ä¸´æ—¶ç›®å½• ---
    temp_index_dir = os.path.join(output_dir, "temp_index")
    if os.path.exists(temp_index_dir):
        shutil.rmtree(temp_index_dir)
    os.makedirs(temp_index_dir)
    print(f"å·²åˆ›å»ºä¸´æ—¶ç´¢å¼•ç›®å½•: {temp_index_dir}")

    # æ¸…ç†æ—§çš„ç´¢å¼•æ–‡ä»¶ï¼Œé¿å…æ–°æ—§æ–‡ä»¶æ··åˆ
    print("æ­£åœ¨æ¸…ç†æ—§çš„ç´¢å¼•æ–‡ä»¶...")
    for f in glob.glob(os.path.join(output_dir, "search_index_*.*")):
        os.remove(f)
    if os.path.exists(os.path.join(output_dir, "search_index_manifest.json")):
        os.remove(os.path.join(output_dir, "search_index_manifest.json"))

    # --- ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ„å»ºç´¢å¼• ---
    print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ„å»ºç´¢å¼• (ä½¿ç”¨ {multiprocessing.cpu_count()} ä¸ªæ ¸å¿ƒ)...")
    
    papers_list = list(all_papers_map.values())
    
    # æ¸…ç†å†…éƒ¨å­—æ®µ
    for paper in papers_list:
        if '_version' in paper:
            del paper['_version']

    temp_file_handles = {}
    category_index = defaultdict(set)
    monthly_data = defaultdict(list)
    with multiprocessing.Pool() as pool:
        # ä½¿ç”¨ imap_unordered æ¥è·å–ç»“æœï¼Œè¿™æ ·å¯ä»¥æ›´å¿«åœ°å¤„ç†å¹¶æ˜¾ç¤ºè¿›åº¦
        # å®ƒè¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰ç»“æœï¼Œæ›´èŠ‚çœå†…å­˜
        results_iterator = pool.imap_unordered(process_paper_for_indexing, papers_list)
        
        processed_count = 0
        for i, result in enumerate(results_iterator):
            if result is None or result[0] is None:
                continue
            
            paper_id, search_tokens, categories, year_month = result
            
            # --- æ ¸å¿ƒä¼˜åŒ–ï¼šå°†æœç´¢è¯å…ƒå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º ---
            # æ ¸å¿ƒæ”¹åŠ¨ï¼šä¸å°†æœç´¢è¯å…ƒèšåˆåˆ°å†…å­˜ï¼Œè€Œæ˜¯ç›´æ¥å†™å…¥åˆ°æŒ‰åˆ†å—é”®ç»„ç»‡çš„ä¸´æ—¶æ–‡ä»¶
            for token in search_tokens:
                first_char = token[0].lower()
                # ç¡®å®šåˆ†å—é”® (a-z, 0 for digits, zh for others)
                chunk_key = '0' if first_char.isdigit() else first_char if first_char.isalpha() and ord(first_char) < 128 else 'zh'
                
                if chunk_key not in temp_file_handles:
                    filepath = os.path.join(temp_index_dir, f"{chunk_key}.part")
                    temp_file_handles[chunk_key] = open(filepath, 'a', encoding='utf-8')
                
                # ä»¥åˆ¶è¡¨ç¬¦åˆ†éš”çš„æ ¼å¼å†™å…¥ï¼Œä¾¿äºåç»­å¤„ç†
                temp_file_handles[chunk_key].write(f"{token}\t{paper_id}\n")
            
            # åˆ†ç±»ç´¢å¼•å’Œæœˆåº¦æ•°æ®è¾ƒå°ï¼Œå¯ä»¥ç»§ç»­åœ¨å†…å­˜ä¸­èšåˆ
            for category in categories:
                category_index[category].add(paper_id)

            # æŒ‰æœˆä»½èšåˆæ•°æ® (éœ€è¦åŸå§‹paper_data)
            if year_month:
                 monthly_data[year_month].append(all_papers_map[paper_id])

            processed_count += 1
            if (i + 1) % 1000 == 0:
                print(f"  ...å·²å¤„ç† {i+1}/{total_paper_count} ç¯‡è®ºæ–‡")

    # å…³é—­æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å¥æŸ„
    for handle in temp_file_handles.values():
        handle.close()

    print(f"âœ… å¤šè¿›ç¨‹å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_count} ç¯‡è®ºæ–‡ã€‚")
    print("ä¸­é—´ç´¢å¼•æ–‡ä»¶å·²å†™å…¥ä¸´æ—¶ç›®å½•ã€‚")

    # é‡Šæ”¾å†…å­˜
    del all_papers_map
    del papers_list

    # --- ç¬¬ä¸‰é˜¶æ®µï¼šä»ä¸´æ—¶æ–‡ä»¶æ„å»ºæœ€ç»ˆçš„åˆ†å—æœç´¢ç´¢å¼• ---
    print("\nâš™ï¸ å¼€å§‹ä»ä¸´æ—¶æ–‡ä»¶æ„å»ºæœ€ç»ˆçš„åˆ†å—æœç´¢ç´¢å¼•...")
    # ä¸ºä¸åŒé•¿åº¦çš„è¯æ¡è®¾ç½®ä¸åŒçš„é¢‘ç‡é˜ˆå€¼
    # è‹±æ–‡è¯æ¡é˜ˆå€¼
    MIN_FREQ_UNIGRAM = 2
    MIN_FREQ_BIGRAM = 3
    MIN_FREQ_TRIGRAM = 3
    # ä¸ºä¸­æ–‡è¯æ¡è®¾ç½®æ›´é«˜çš„é˜ˆå€¼ï¼Œä»¥å¤§å¹…å‡å°ç´¢å¼•ä½“ç§¯
    MIN_FREQ_UNIGRAM_ZH = 7  # å†æ¬¡æé«˜
    MIN_FREQ_BIGRAM_ZH = 10  # å†æ¬¡æé«˜
    MIN_FREQ_TRIGRAM_ZH = 10 # å†æ¬¡æé«˜

    chunk_manifest = []
    stats = {"total_unique_words": 0, "chunks_created": 0, "total_size_mb": 0}

    part_files = glob.glob(os.path.join(temp_index_dir, "*.part"))

    for part_file in part_files:
        chunk_key = os.path.basename(part_file).replace('.part', '')
        print(f"  -> æ­£åœ¨å¤„ç†åˆ†å—: {chunk_key}")
        
        # æ ¹æ®åˆ†å—ç±»å‹ï¼Œé€‰æ‹©ä¸åŒçš„è¿‡æ»¤é˜ˆå€¼
        is_chinese_chunk = (chunk_key == 'zh')
        if is_chinese_chunk:
            print("     - åº”ç”¨æ›´é«˜çš„ä¸­æ–‡è¯æ¡è¿‡æ»¤é˜ˆå€¼ã€‚")
            unigram_thresh, bigram_thresh, trigram_thresh = MIN_FREQ_UNIGRAM_ZH, MIN_FREQ_BIGRAM_ZH, MIN_FREQ_TRIGRAM_ZH
        else:
            unigram_thresh, bigram_thresh, trigram_thresh = MIN_FREQ_UNIGRAM, MIN_FREQ_BIGRAM, MIN_FREQ_TRIGRAM
        
        # 1. åœ¨å†…å­˜ä¸­ä¸ºå½“å‰åˆ†å—æ„å»ºç´¢å¼•
        chunk_index = defaultdict(list)
        with open(part_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    token, paper_id = line.strip().split('\t')
                    chunk_index[token].append(paper_id)
                except ValueError:
                    continue # Skip malformed lines

        # 2. è¿‡æ»¤ä½é¢‘è¯ (æ›´æ™ºèƒ½çš„ç­–ç•¥)
        frequent_chunk_index = {}
        for token, ids in chunk_index.items():
            num_words = token.count(' ') + 1
            freq = len(ids)
            
            keep = False
            if num_words == 1 and freq >= unigram_thresh:
                keep = True
            elif num_words == 2 and freq >= bigram_thresh:
                keep = True
            elif num_words >= 3 and freq >= trigram_thresh:
                keep = True
            
            if keep:
                # å»é‡å¹¶æ’åº
                frequent_chunk_index[token] = sorted(list(set(ids)))
        
        if not frequent_chunk_index:
            print(f"     - åˆ†å— {chunk_key} æ— é«˜é¢‘è¯ï¼Œå·²è·³è¿‡ã€‚")
            continue

        # 3. å†™å…¥æœ€ç»ˆçš„JSONåˆ†å—æ–‡ä»¶
        chunk_filename = f"search_index_{chunk_key}.json"
        chunk_path = os.path.join(output_dir, chunk_filename)
        with open(chunk_path, 'w', encoding='utf-8') as f:
            json.dump(frequent_chunk_index, f, ensure_ascii=False, separators=(',', ':'))

        # 4. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        chunk_size = os.path.getsize(chunk_path) / (1024 * 1024)
        word_count = len(frequent_chunk_index)
        stats["total_unique_words"] += word_count
        stats["total_size_mb"] += chunk_size
        stats["chunks_created"] += 1
        
        chunk_manifest.append({
            "key": chunk_key,
            "filename": chunk_filename,
            "wordCount": word_count,
            "sizeMB": round(chunk_size, 2)
        })
        print(f"  âœ… åˆ†å— '{chunk_key}' åˆ›å»ºå®Œæˆ: {word_count} è¯æ±‡, {chunk_size:.2f} MB")

    # 5. æ¸…ç†ä¸´æ—¶ç›®å½•
    shutil.rmtree(temp_index_dir)
    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶ç´¢å¼•ç›®å½•: {temp_index_dir}")

    # 6. åˆ›å»ºæœç´¢ç´¢å¼•æ¸…å•æ–‡ä»¶
    search_manifest_path = os.path.join(output_dir, "search_index_manifest.json")
    search_manifest_data = {
        "version": "2.0", "chunked": True, "description": "åˆ†å—æœç´¢ç´¢å¼•ï¼Œä¼˜åŒ–åŠ è½½æ€§èƒ½",
        "chunks": sorted(chunk_manifest, key=lambda x: x["key"]), "totalWords": stats["total_unique_words"],
        "totalChunks": stats["chunks_created"], "totalSizeMB": round(stats["total_size_mb"], 2),
        "generatedAt": datetime.now().isoformat()
    }
    with open(search_manifest_path, 'w', encoding='utf-8') as f:
        json.dump(search_manifest_data, f, indent=2, ensure_ascii=False)
    print("âœ… æˆåŠŸå†™å…¥æœç´¢ç´¢å¼•æ¸…å•æ–‡ä»¶ search_index_manifest.jsonã€‚")

    # --- ç¬¬å››é˜¶æ®µï¼šå†™å…¥æœˆåº¦æ•°æ®ã€åˆ†ç±»ç´¢å¼•å’Œä¸»æ¸…å•æ–‡ä»¶ ---
    print("\nå¼€å§‹å†™å…¥å…¶ä»–æ•°æ®åº“æ–‡ä»¶...")
    for month, papers in monthly_data.items():
        sorted_papers = sorted(papers, key=lambda p: p['date'], reverse=True)
        month_file_path = os.path.join(output_dir, f"database-{month}.json")
        with open(month_file_path, 'w', encoding='utf-8') as f:
            json.dump(sorted_papers, f, indent=2, ensure_ascii=False)
    print(f"æˆåŠŸå†™å…¥ {len(monthly_data)} ä¸ªæœˆåº¦æ•°æ®åˆ†ç‰‡æ–‡ä»¶ã€‚")

    available_months = sorted(monthly_data.keys(), reverse=True)
    current_date = datetime.now().strftime("%Y-%m-%d")
    manifest = {
        "availableMonths": available_months, 
        "totalPaperCount": total_paper_count,
        "lastUpdated": current_date
    }
    manifest_file_path = os.path.join(output_dir, "index.json")
    with open(manifest_file_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    print("æˆåŠŸå†™å…¥æ¸…å•æ–‡ä»¶ index.jsonã€‚")

    final_category_index = {category: list(ids) for category, ids in category_index.items()}
    category_index_file_path = os.path.join(output_dir, "category_index.json")
    with open(category_index_file_path, 'w', encoding='utf-8') as f:
        json.dump(final_category_index, f, ensure_ascii=False)
    print("æˆåŠŸå†™å…¥åˆ†ç±»ç´¢å¼•æ–‡ä»¶ category_index.jsonã€‚")
    old_db_path = "docs/database.json"
    if os.path.exists(old_db_path):
        os.remove(old_db_path)
        print(f"å·²åˆ é™¤æ—§çš„æ•°æ®æ–‡ä»¶: {old_db_path}")

# ä¸ºäº†å®‰å…¨åœ°åº”ç”¨ä¿®å¤ï¼Œæˆ‘å°†åŸå‡½æ•°é‡å‘½åï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„è°ƒç”¨å®ƒçš„å‡½æ•°
build_database_from_jsonl = build_database_from_jsonl_fixed

if __name__ == "__main__":
    build_database_from_jsonl()