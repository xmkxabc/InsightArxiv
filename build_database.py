import os
import glob
import json
import re
from collections import defaultdict, Counter
from datetime import datetime

# å°è¯•å¯¼å…¥jiebaç”¨äºä¸­æ–‡åˆ†è¯
try:
    import jieba
    print("âœ… Jieba loaded for Chinese tokenization.")
except ImportError:
    jieba = None
    print("âš ï¸ Warning: 'jieba' not installed. Chinese search will be limited to exact keyword matches.")

# æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨ï¼Œç”¨äºæ„å»ºæœç´¢ç´¢å¼•æ—¶å¿½ç•¥è¿™äº›å¸¸è§è¯
STOP_WORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'this', 'that', 'these', 'those', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',
    'might', 'must', 'can', 'shall', 'we', 'they', 'you', 'it', 'he', 'she', 'his', 'her',
    'its', 'their', 'our', 'your', 'my', 'me', 'him', 'them', 'us', 'from', 'up', 'out',
    'down', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there',
    'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',
    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'can', 'just', 'now', 'also', 'however', 'although', 'though',
    'paper', 'method', 'approach', 'model', 'result', 'results', 'show', 'shows',
    'using', 'used', 'use', 'based', 'propose', 'proposed', 'algorithm', 'algorithms'
}

def create_chunked_index(search_index: dict, output_dir: str):
    """åˆ›å»ºåˆ†å—çš„æœç´¢ç´¢å¼•ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    
    # æŒ‰å­—æ¯åˆ†å—ï¼ˆä»…è‹±æ–‡è¯æ±‡ï¼‰
    chunks = defaultdict(dict)
    chinese_words = {}  # å­˜å‚¨ä¸­æ–‡è¯æ±‡
    stats = {"total_words": 0, "chunks_created": 0, "total_size_mb": 0}
    
    print("å¼€å§‹åˆ†è¯å’Œåˆ†å—...")
    for word, paper_ids in search_index.items():
        stats["total_words"] += 1
        first_char = word[0].lower()
        
        # åªå¯¹è‹±æ–‡è¯æ±‡è¿›è¡Œåˆ†å—
        if first_char.isalpha() and ord(first_char) < 128:  # ASCIIå­—æ¯
            chunks[first_char][word] = paper_ids
        elif first_char.isdigit():
            chunks['0'][word] = paper_ids  # æ•°å­—
        else:
            # ä¸­æ–‡è¯æ±‡ç»Ÿä¸€æ”¾åˆ°ä¸€ä¸ªç‰¹æ®Šåˆ†å—ä¸­
            chinese_words[word] = paper_ids
    
    # å¦‚æœæœ‰ä¸­æ–‡è¯æ±‡ï¼Œåˆ›å»ºä¸“é—¨çš„ä¸­æ–‡åˆ†å—
    if chinese_words:
        chunks['zh'] = chinese_words
        print(f"ä¸­æ–‡è¯æ±‡åˆ†å—: {len(chinese_words)} ä¸ªè¯æ±‡")
    
    # ä¿å­˜åˆ†å—æ–‡ä»¶ï¼ˆä¼˜åŒ–å†™å…¥ï¼‰
    chunk_manifest = []
    
    print("å†™å…¥åˆ†å—æ–‡ä»¶...")
    for chunk_key, chunk_data in chunks.items():
        if not chunk_data:  # è·³è¿‡ç©ºåˆ†å—
            continue
            
        chunk_filename = f"search_index_{chunk_key}.json"
        chunk_path = os.path.join(output_dir, chunk_filename)
        
        # ä½¿ç”¨æ›´ç´§å‡‘çš„JSONæ ¼å¼
        with open(chunk_path, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, ensure_ascii=False, separators=(',', ':'))
        
        chunk_size = os.path.getsize(chunk_path) / (1024 * 1024)
        stats["total_size_mb"] += chunk_size
        stats["chunks_created"] += 1
        
        chunk_manifest.append({
            "key": chunk_key,
            "filename": chunk_filename,
            "wordCount": len(chunk_data),
            "sizeMB": round(chunk_size, 2)
        })
        
        print(f"âœ… åˆ†å— '{chunk_key}': {len(chunk_data)} è¯æ±‡, {chunk_size:.2f} MB")
    
    # ä¿å­˜åˆ†å—æ¸…å•
    manifest_path = os.path.join(output_dir, "search_index_manifest.json")
    manifest_data = {
        "version": "2.0",
        "chunked": True,
        "description": "åˆ†å—æœç´¢ç´¢å¼•ï¼Œä¼˜åŒ–åŠ è½½æ€§èƒ½",
        "chunks": sorted(chunk_manifest, key=lambda x: x["key"]),
        "totalWords": stats["total_words"],
        "totalChunks": stats["chunks_created"],
        "totalSizeMB": round(stats["total_size_mb"], 2),
        "generatedAt": datetime.now().isoformat()
    }
    
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(manifest_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ åˆ†å—ç´¢å¼•åˆ›å»ºå®Œæˆ:")
    print(f"   - æ€»è¯æ±‡æ•°: {stats['total_words']:,}")
    print(f"   - åˆ†å—æ•°é‡: {stats['chunks_created']}")
    print(f"   - æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"   - å¹³å‡åˆ†å—å¤§å°: {stats['total_size_mb']/stats['chunks_created']:.2f} MB")
    
    return manifest_data

def build_optimized_search_index(search_index_dict: dict, output_dir: str):
    """æ„å»ºä¼˜åŒ–çš„æœç´¢ç´¢å¼•ï¼ˆä»…åˆ†å—ç‰ˆæœ¬ï¼Œä¸ç”Ÿæˆå¤§æ–‡ä»¶ï¼‰"""
    
    print("\nå¼€å§‹æ„å»ºä¼˜åŒ–çš„æœç´¢ç´¢å¼•...")
    
    # è¿‡æ»¤ä½é¢‘è¯æ±‡ï¼ˆå‡ºç°æ¬¡æ•°å°‘äº2æ¬¡çš„è¯æ±‡ï¼‰
    MIN_FREQUENCY = 2
    word_counter = Counter()
    
    # ç»Ÿè®¡è¯é¢‘
    for word, paper_ids in search_index_dict.items():
        word_counter[word] = len(paper_ids)
    
    # è¿‡æ»¤ä½é¢‘è¯æ±‡
    filtered_search_index = {}
    for word, paper_ids in search_index_dict.items():
        if word_counter[word] >= MIN_FREQUENCY:
            filtered_search_index[word] = paper_ids
    
    print(f"è¿‡æ»¤åä¿ç•™äº† {len(filtered_search_index)} ä¸ªè¯æ±‡ï¼ˆåŸå§‹: {len(search_index_dict)}ï¼‰")
    
    # ğŸš€ ç›´æ¥åˆ›å»ºåˆ†å—ç‰ˆæœ¬ï¼Œä¸ç”Ÿæˆå¤§æ–‡ä»¶
    print("åˆ›å»ºåˆ†å—æœç´¢ç´¢å¼•...")
    create_chunked_index(filtered_search_index, output_dir)
    
    # ğŸ—‘ï¸ æ¸…ç†å¯èƒ½å­˜åœ¨çš„å¤§æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    large_files_to_remove = [
        os.path.join(output_dir, "search_index.json"),
        os.path.join(output_dir, "search_index.json.gz")
    ]
    
    for file_path in large_files_to_remove:
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                print(f"å·²åˆ é™¤å¤§æ–‡ä»¶: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
    
    return filtered_search_index

def build_database_from_jsonl():
    """
    æ„å»ºæ•°æ®åº“çš„ä¸»å‡½æ•°ã€‚
    å®ƒä» 'data' ç›®å½•ä¸‹çš„æ‰€æœ‰ *_AI_enhanced_Chinese.jsonl æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œ
    å¹¶æ™ºèƒ½åœ°å¤„ç†è®ºæ–‡ç‰ˆæœ¬æ›´æ–°ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆæœ¬ã€‚ç„¶åç”Ÿæˆï¼š
    1. æŒ‰æœˆä»½åˆ†ç‰‡çš„æ•°æ®æ–‡ä»¶ (database-YYYY-MM.json)
    2. ä¸€ä¸ªæ¸…å•æ–‡ä»¶ (index.json)
    3. ä¸€ä¸ªä¸“ç”¨çš„æœç´¢ç´¢å¼•æ–‡ä»¶ (search_index.json)
    4. ä¸€ä¸ªå…¨æ–°çš„åˆ†ç±»ç´¢å¼•æ–‡ä»¶ (category_index.json)
    """
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æœ€æ–°ç‰ˆæœ¬çš„è®ºæ–‡ï¼Œé”®ä¸ºåŸºç¡€ID (e.g., "2401.12345")
    all_papers_map = {} 
    skipped_paper_count = 0

    output_dir = "docs/data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºç›®å½•: {output_dir}")

    jsonl_files = glob.glob("data/*_AI_enhanced_Chinese.jsonl")
    if not jsonl_files:
        print("é”™è¯¯: åœ¨ 'data' ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• '_AI_enhanced_Chinese.jsonl' æ–‡ä»¶ã€‚")
        return

    print(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ª .jsonl æ•°æ®æºæ–‡ä»¶ã€‚å¼€å§‹å¤„ç†...")

    # --- ç¬¬ä¸€é˜¶æ®µï¼šè¯»å–æ‰€æœ‰æ•°æ®ï¼Œå»é‡å¹¶åªä¿ç•™æœ€æ–°ç‰ˆæœ¬ ---
    for jsonl_file in sorted(jsonl_files): # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        base_name = os.path.basename(jsonl_file)
        date_from_filename_match = re.match(r'(\d{4}-\d{2}-\d{2})', base_name)
        if not date_from_filename_match:
            print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å {base_name} ä¸­æå–æ—¥æœŸï¼Œå·²è·³è¿‡ã€‚")
            continue
        file_date = date_from_filename_match.group(1)

        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    raw_data = json.loads(line)
                    paper_id_full = raw_data.get("id")

                    if not paper_id_full or not isinstance(paper_id_full, str):
                        skipped_paper_count += 1
                        continue

                    # --- [æ ¸å¿ƒç‰ˆæœ¬æ›´æ–°é€»è¾‘] ---
                    # 1. æå–åŸºç¡€IDå’Œç‰ˆæœ¬å·
                    match = re.match(r'(\d+\.\d+)(v\d+)?', paper_id_full)
                    if not match:
                        skipped_paper_count += 1
                        continue
                    
                    base_id = match.group(1)
                    version = int(match.group(2)[1:]) if match.group(2) else 1

                    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    existing_paper = all_papers_map.get(base_id)
                    if existing_paper and existing_paper.get('_version', 1) >= version:
                        # å¦‚æœmapä¸­å·²å­˜åœ¨çš„è®ºæ–‡ç‰ˆæœ¬(existing_paper)æ¯”å½“å‰å¤„ç†çš„è®ºæ–‡ç‰ˆæœ¬(version)æ›´é«˜æˆ–ç›¸åŒï¼Œ
                        # åˆ™è·³è¿‡å½“å‰è¿™æ¡è®°å½•ï¼Œä»è€Œä¿ç•™mapä¸­å·²æœ‰çš„é«˜ç‰ˆæœ¬ã€‚
                        continue

                    # --- æ•°æ®æ•´å½¢ ---
                    ai_enhanced_info = raw_data.get("AI", {})
                    keywords_str = ai_enhanced_info.get("keywords", "")
                    keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()] if isinstance(keywords_str, str) else []

                    paper_data = {
                        "id": base_id, # å­˜å‚¨åŸºç¡€ID
                        "full_id": paper_id_full, # ä¿ç•™å®Œæ•´IDä¾›å‚è€ƒ
                        "_version": version, # å†…éƒ¨ä½¿ç”¨ï¼Œè®°å½•ç‰ˆæœ¬å·
                        "title": raw_data.get("title", "æ— æ ‡é¢˜"),
                        "date": file_date,
                        "url": raw_data.get("url", f"http://arxiv.org/abs/{paper_id_full}"),
                        "pdf_url": raw_data.get("pdf_link", f"http://arxiv.org/pdf/{paper_id_full}"),
                        "authors": ", ".join(raw_data.get("authors", [])),
                        "abstract": raw_data.get("summary", raw_data.get("abstract", "")),
                        "comment": raw_data.get("comments", ""), # ä¿®æ­£ï¼šçˆ¬è™«ä¸­çš„å­—æ®µæ˜¯ 'comments'
                        "categories": raw_data.get("categories", []),
                        "updated": raw_data.get("updated", file_date),
                        "first_published": raw_data.get("date", file_date), # ç¬¬ä¸€æ¬¡å‘å¸ƒæ—¥æœŸ

                        "zh_title": ai_enhanced_info.get("title_translation"),
                        "translation": ai_enhanced_info.get("translation"),
                        "keywords": keywords_list,
                        "tldr": ai_enhanced_info.get("tldr"),
                        "ai_comments": ai_enhanced_info.get("comments"),
                        "motivation": ai_enhanced_info.get("motivation"),
                        "method": ai_enhanced_info.get("method"),
                        "results": ai_enhanced_info.get("result"),
                        "conclusion": ai_enhanced_info.get("conclusion")
                    }
                    
                    all_papers_map[base_id] = paper_data

                except (json.JSONDecodeError, AttributeError, TypeError) as e:
                    # print(f"Skipping line due to error: {e}") # for debugging
                    skipped_paper_count += 1
                    continue

    total_paper_count = len(all_papers_map)
    if total_paper_count == 0:
        print("è­¦å‘Š: æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•è®ºæ–‡ã€‚")
        return

    print(f"\nå¤„ç†å®Œæˆã€‚æ•°æ®åº“å°†åŒ…å« {total_paper_count} ç¯‡å”¯ä¸€çš„æœ€æ–°ç‰ˆæœ¬è®ºæ–‡ã€‚")
    if skipped_paper_count > 0:
        print(f"å› æ ¼å¼é”™è¯¯æˆ–ç‰ˆæœ¬é™ˆæ—§ï¼Œæ€»å…±è·³è¿‡äº† {skipped_paper_count} æ¡è®°å½•ã€‚")

    # --- ç¬¬äºŒé˜¶æ®µï¼šä» all_papers_map æ„å»ºæœ€ç»ˆçš„æ•°æ®ç»“æ„å’Œç´¢å¼• ---
    monthly_data = defaultdict(list)
    search_index = defaultdict(set)
    category_index = defaultdict(set)

    for paper_id, paper_data in all_papers_map.items():
        # æ¸…ç†æ‰å†…éƒ¨ä½¿ç”¨çš„å­—æ®µ
        del paper_data['_version']
        
        # æŒ‰æœˆä»½èšåˆ
        year_month = paper_data['date'][:7]
        monthly_data[year_month].append(paper_data)

        # æ„å»ºæœç´¢ç´¢å¼• - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å¤šæ–‡æœ¬æº
        # 1. ç´¢å¼•è‹±æ–‡æ–‡æœ¬
        english_text_sources = [
            paper_data.get("title", ""),
            paper_data.get("abstract", ""),
            paper_data.get("tldr", ""),
        ]
        english_text_to_index = " ".join(filter(None, english_text_sources)).lower()
        tokens = re.findall(r'\b[a-z]{3,}\b', english_text_to_index)
        for token in tokens:
            if token not in STOP_WORDS and len(token) >= 3:
                search_index[token].add(paper_id)

        # 2. å¦‚æœjiebaå¯ç”¨ï¼Œç´¢å¼•ä¸­æ–‡æ–‡æœ¬
        if jieba:
            chinese_text_sources = [
                paper_data.get("zh_title", ""),
                paper_data.get("translation", ""),
                paper_data.get("ai_comments", ""),
            ]
            chinese_text_to_index = "".join(filter(None, chinese_text_sources))
            chinese_tokens = jieba.cut_for_search(chinese_text_to_index) # ä½¿ç”¨æœç´¢å¼•æ“æ¨¡å¼
            for token in chinese_tokens:
                token = token.strip().lower()
                if token and token not in STOP_WORDS and len(token) > 1:
                    search_index[token].add(paper_id)

        # 3. æ·»åŠ å®Œæ•´çš„å…³é”®è¯åˆ°æœç´¢ç´¢å¼• (å¤„ç†ä¸­è‹±æ–‡æ··åˆçš„å…³é”®è¯)
        for keyword in paper_data.get("keywords", []):
            if keyword and keyword.strip():
                clean_keyword = keyword.strip().lower()
                if clean_keyword not in STOP_WORDS:
                    search_index[clean_keyword].add(paper_id)

        # æ„å»ºåˆ†ç±»ç´¢å¼•
        for category in paper_data.get("categories", []):
            category_index[category].add(paper_id)

    # --- å¼€å§‹å†™å…¥æ–‡ä»¶ ---
    print("\nå¼€å§‹å†™å…¥æ•°æ®åº“æ–‡ä»¶...")
    for month, papers in monthly_data.items():
        sorted_papers = sorted(papers, key=lambda p: p['date'], reverse=True)
        month_file_path = os.path.join(output_dir, f"database-{month}.json")
        with open(month_file_path, 'w', encoding='utf-8') as f:
            json.dump(sorted_papers, f, indent=2, ensure_ascii=False)
    print(f"æˆåŠŸå†™å…¥ {len(monthly_data)} ä¸ªæœˆåº¦æ•°æ®åˆ†ç‰‡æ–‡ä»¶ã€‚")

    available_months = sorted(monthly_data.keys(), reverse=True)
    current_date = datetime.now().strftime("%Y-%m-%d")
    manifest = {
        "availableMonths": available_months, 
        "totalPaperCount": total_paper_count,
        "lastUpdated": current_date
    }
    manifest_file_path = os.path.join(output_dir, "index.json")
    with open(manifest_file_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    print("æˆåŠŸå†™å…¥æ¸…å•æ–‡ä»¶ index.jsonã€‚")

    # æ„å»ºå¹¶ä¿å­˜ä¼˜åŒ–çš„æœç´¢ç´¢å¼•
    final_search_index = {token: list(ids) for token, ids in search_index.items()}
    build_optimized_search_index(final_search_index, output_dir)
    
    # æ„å»ºå¹¶ä¿å­˜åˆ†ç±»ç´¢å¼•
    final_category_index = {category: list(ids) for category, ids in category_index.items()}
    category_index_file_path = os.path.join(output_dir, "category_index.json")
    with open(category_index_file_path, 'w', encoding='utf-8') as f:
        json.dump(final_category_index, f, ensure_ascii=False)
    print("æˆåŠŸå†™å…¥åˆ†ç±»ç´¢å¼•æ–‡ä»¶ category_index.jsonã€‚")
    old_db_path = "docs/database.json"
    if os.path.exists(old_db_path):
        os.remove(old_db_path)
        print(f"å·²åˆ é™¤æ—§çš„æ•°æ®æ–‡ä»¶: {old_db_path}")

if __name__ == "__main__":
    build_database_from_jsonl()