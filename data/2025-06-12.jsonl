{"id": "2506.09148", "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09148v1"}
{"id": "2506.09312", "title": "What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?", "authors": ["Erik Buchholz", "Natasha Fernandes", "David D. Nguyen", "Alsharif Abuadbba", "Surya Nepal", "Salil S. Kanhere"], "summary": "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09312v1"}
{"id": "2506.09353", "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt", "authors": ["Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across\nvarious applications but remain vulnerable to malicious queries that exploit\nthe visual modality. Existing alignment approaches typically fail to resist\nmalicious queries while preserving utility on benign ones effectively. To\naddress these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),\nwhich is built upon two key innovations. First, we introduce the Visual Safety\nPrompt, which appends a trainable padding region around the input image. It\npreserves visual features and expands the optimization space. Second, we\npropose Deep Alignment, a novel approach to train the visual safety prompt\nthrough supervision in the model's activation space. It enhances the inherent\nability of LVLMs to perceive malicious queries, achieving deeper alignment than\nprior works. Extensive experiments across five benchmarks on two representative\nLVLMs demonstrate that DAVSP effectively resists malicious queries while\npreserving benign input utility. Furthermore, DAVSP exhibits great cross-model\ngeneration ability. Ablation studies further reveal that both the Visual Safety\nPrompt and Deep Alignment are essential components, jointly contributing to its\noverall effectiveness. The code is publicly available at\nhttps://github.com/zhangyitonggg/DAVSP.", "comment": "16 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09353v1"}
{"id": "2506.09365", "title": "ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)", "authors": ["Ronal Singh", "Mohan Baruwal Chhetri", "Surya Nepal", "Cecile Paris"], "summary": "Modern Security Operations Centres (SOCs) integrate diverse tools, such as\nSIEM, IDS, and XDR systems, offering rich contextual data, including alert\nenrichments, flow features, and similar case histories. Yet, analysts must\nstill manually determine which of these contextual cues are most relevant when\nvalidating specific alerts. We introduce ContextBuddy, an AI assistant that\nlearns from analysts' prior investigations to help them identify the most\nrelevant context for new alerts. Rather than providing enrichments,\nContextBuddy models how analysts have previously selected context and suggests\ntailored cues based on the characteristics of each alert. We formulate context\nselection as a sequential decision-making problem and apply imitation learning\n(IL) to capture analysts' strategies, evaluating multiple IL approaches.\nThrough staged evaluation, we validate ContextBuddy using two intrusion\ndetection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,\nContextBuddy helped simulated reinforcement learning analysts improve\nclassification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%\nfor UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and\nkeeping false positives below 1%. Decision confidence among agents also\nimproved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =\n0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%\n(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results\ndemonstrate that by learning context-selection patterns from analysts,\nContextBuddy can yield notable improvements in investigation effectiveness and\nefficiency.", "comment": "27 pages, 33 figures, 7 tables, under review", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09365v1"}
{"id": "2506.09480", "title": "Reliability of Capacitive Read in Arrays of Ferroelectric Capacitors", "authors": ["Luca Fehlings", "Muhtasim Alam Chowdhury", "Banafsheh Saber Latibari", "Soheil Salehi", "Erika Covi"], "summary": "The non-destructive capacitance read-out of ferroelectric capacitors (FeCaps)\nbased on doped HfO$_2$ metal-ferroelectric-metal (MFM) structures offers the\npotential for low-power and highly scalable crossbar arrays. This is due to a\nnumber of factors, including the selector-less design, the absence of sneak\npaths, the power-efficient charge-based read operation, and the reduced IR\ndrop. Nevertheless, a reliable capacitive readout presents certain challenges,\nparticularly in regard to device variability and the trade-off between read\nyield and read disturbances, which can ultimately result in bit-flips. This\npaper presents a digital read macro for HfO$_2$ FeCaps and provides design\nguidelines for capacitive readout of HfO$_2$ FeCaps, taking device-centric\nreliability and yield challenges into account. An experimentally calibrated\nphysics-based compact model of HfO$_2$ FeCaps is employed to investigate the\nreliability of the read-out operation of the FeCap macro through Monte Carlo\nsimulations. Based on this analysis, we identify limitations posed by the\ndevice variability and propose potential mitigation strategies through\ndesign-technology co-optimization (DTCO) of the FeCap device characteristics\nand the CMOS circuit design. Finally, we examine the potential applications of\nthe FeCap macro in the context of secure hardware. We identify potential\nsecurity threats and propose strategies to enhance the robustness of the\nsystem.", "comment": "4 pages, 6 figures, submitted and presented at ISCAS 2025, London", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.09480v1"}
{"id": "2506.09098", "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "comment": "https://youtu.be/AQAgVdrx1DE", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09098v1"}
{"id": "2506.09066", "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09066v1"}
{"id": "2506.09176", "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "comment": "ICML 2025 Poster", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09176v1"}
{"id": "2506.09052", "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "comment": "7 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09052v1"}
{"id": "2506.09239", "title": "Rejection-Sampled Linear Codes for Lossy Compression and Channel Simulation", "authors": ["Cheuk Ting Li", "Jianguo Zhao"], "summary": "We show that a linear code combined with rejection sampling can give a\ncapacity-achieving scheme for simulating channels with additive noises with\nexchangeable distributions. Hence, it can be used in lossy source coding to\nachieve the rate-distortion function. Interestingly, unlike conventional linear\ncovering codes for lossy compression which concerns the trade-off between the\nrate and the covering radius, our construction only requires the linear code to\nhave a large distance (not a large covering radius), and is not sensitive to\nthe rate of the linear code. Experiments reveal that our construction can\noutperform conventional covering codes for lossy source coding with Hamming\ndistortion for a certain range of distortion levels, and performs well even\nwhen the blocklength is small (e.g., 24).", "comment": "12 pages, 5 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09239v1"}
{"id": "2506.09230", "title": "Formal Methods Meets Readability: Auto-Documenting JML Java Code", "authors": ["Juan Carlos Recio Abad", "Ruben Saborido", "Francisco Chicano"], "summary": "This paper investigates whether formal specifications using Java Modeling\nLanguage (JML) can enhance the quality of Large Language Model (LLM)-generated\nJavadocs. While LLMs excel at producing documentation from code alone, we\nhypothesize that incorporating formally verified invariants yields more\ncomplete and accurate results. We present a systematic comparison of\ndocumentation generated from JML-annotated and non-annotated Java classes,\nevaluating quality through both automated metrics and expert analysis. Our\nfindings demonstrate that JML significantly improves class-level documentation\ncompleteness, with more moderate gains at the method level. Formal\nspecifications prove particularly effective in capturing complex class\ninvariants and design contracts that are frequently overlooked in code-only\ndocumentation. A threshold effect emerges, where the benefits of JML become\nmore pronounced for classes with richer sets of invariants. While JML enhances\nspecification coverage, its impact on core descriptive quality is limited,\nsuggesting that formal specifications primarily ensure comprehensive coverage\nrather than fundamentally altering implementation descriptions. These results\noffer actionable insights for software teams adopting formal methods in\ndocumentation workflows, highlighting scenarios where JML provides clear\nadvantages. The study contributes to AI-assisted software documentation\nresearch by demonstrating how formal methods and LLMs can synergistically\nimprove documentation quality.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09230v1"}
{"id": "2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "comment": "in French language", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09089v1"}
{"id": "2506.09234", "title": "Transaction Categorization with Relational Deep Learning in QuickBooks", "authors": ["Kaiwen Dong", "Padmaja Jonnalagedda", "Xiang Gao", "Ayan Acharya", "Maria Kissa", "Mauricio Flores", "Nitesh V. Chawla", "Kamalika Das"], "summary": "Automatic transaction categorization is crucial for enhancing the customer\nexperience in QuickBooks by providing accurate accounting and bookkeeping. The\ndistinct challenges in this domain stem from the unique formatting of\ntransaction descriptions, the wide variety of transaction categories, and the\nvast scale of the data involved. Furthermore, organizing transaction data in a\nrelational database creates difficulties in developing a unified model that\ncovers the entire database. In this work, we develop a novel graph-based model,\nnamed Rel-Cat, which is built directly over the relational database. We\nintroduce a new formulation of transaction categorization as a link prediction\ntask within this graph structure. By integrating techniques from natural\nlanguage processing and graph machine learning, our model not only outperforms\nthe existing production model in QuickBooks but also scales effectively to a\ngrowing customer base with a simpler, more effective architecture without\ncompromising on accuracy. This design also helps tackle a key challenge of the\ncold start problem by adapting to minimal data.", "comment": "Accepted to ECML-PKDD 2025", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09234v1"}
{"id": "2506.09764", "title": "Alice and the Caterpillar: A more descriptive null model for assessing data mining results", "authors": ["Giulia Preti", "Gianmarco De Francisci Morales", "Matteo Riondato"], "summary": "We introduce novel null models for assessing the results obtained from\nobserved binary transactional and sequence datasets, using statistical\nhypothesis testing. Our null models maintain more properties of the observed\ndataset than existing ones. Specifically, they preserve the Bipartite Joint\nDegree Matrix of the bipartite (multi-)graph corresponding to the dataset,\nwhich ensures that the number of caterpillars, i.e., paths of length three, is\npreserved, in addition to other properties considered by other models. We\ndescribe Alice, a suite of Markov chain Monte Carlo algorithms for sampling\ndatasets from our null models, based on a carefully defined set of states and\nefficient operations to move between them. The results of our experimental\nevaluation show that Alice mixes fast and scales well, and that our null model\nfinds different significant results than ones previously considered in the\nliterature.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.09764v1"}
{"id": "2506.09335", "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "authors": ["Moshi Wei", "Sparks Li"], "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "comment": "11 pages, 1 figures,", "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09335v1"}
{"id": "2506.09180", "title": "Optimal Task Offloading with Firm Deadlines for Mobile Edge Computing Systems", "authors": ["Khai Doan", "Wesley Araujo", "Evangelos Kranakis", "Ioannis Lambadaris", "Yannis Viniotis", "Wonjae Shin"], "summary": "Under a dramatic increase in mobile data traffic, a promising solution for\nedge computing systems to maintain their local service is the task migration\nthat may be implemented by means of Autonomous mobile agents (AMA). In\ndesigning an optimal scheme for task offloading to AMA, we define a system cost\nas a minimization objective function that comprises two parts. First, an\noffloading cost which can be interpreted as the cost of using computational\nresources from the AMA. Second, a penalty cost due to potential task\nexpiration. To minimize the expected (timeaverage) cost over a given time\nhorizon, we formulate a Dynamic programming (DP). However, the DP Equation\nsuffers from the well-known curse of dimensionality, which makes computations\nintractable, especially for infinite system state space. To reduce the\ncomputational burden, we identify three important properties of the optimal\npolicy and show that it suffices to evaluate the DP Equation on a finite subset\nof the state space only. We then prove that the optimal task offloading\ndecision at a state can be inferred from that at its adjacent states, further\nreducing the computational load. We present simulations to verify the\ntheoretical results and to provide insights into the considered system.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09180v1"}
{"id": "2506.09159", "title": "MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge", "authors": ["Antonio Calagna", "Yenchia Yu", "Paolo Giaccone", "Carla Fabiana Chiasserini"], "summary": "Stateful migration has emerged as the dominant technology to support\nmicroservice mobility at the network edge while ensuring a satisfying\nexperience to mobile end users. This work addresses two pivotal challenges,\nnamely, the implementation and the orchestration of the migration process. We\nfirst introduce a novel framework that efficiently implements stateful\nmigration and effectively orchestrates the migration process by fulfilling both\nnetwork and application KPI targets. Through experimental validation using\nrealistic microservices, we then show that our solution (i) greatly improves\nmigration performance, yielding up to 77% decrease of the migration downtime\nwith respect to the state of the art, and (ii) successfully addresses the\nstrict user QoE requirements of critical scenarios featuring latency-sensitive\nmicroservices. Further, we consider two practical use cases, featuring,\nrespectively, a UAV autopilot microservice and a multi-object tracking task,\nand demonstrate how our framework outperforms current state-of-the-art\napproaches in configuring the migration process and in meeting KPI targets.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09159v1"}
{"id": "2506.09387", "title": "Epass: Efficient and Privacy-Preserving Asynchronous Payment on Blockchain", "authors": ["Weijie Wang", "Jinwen Liang", "Chuan Zhang", "Ximeng Liu", "Liehuang Zhu", "Song Guo"], "summary": "Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,\noffering consumers to get the product immediately and defer payments.\nMeanwhile, emerging blockchain technologies endow BNPL platforms with digital\ncurrency transactions, allowing BNPL platforms to integrate with digital\nwallets. However, the transparency of transactions causes critical privacy\nconcerns because malicious participants may derive consumers' financial\nstatuses from on-chain asynchronous payments. Furthermore, the newly created\ntransactions for deferred payments introduce additional time overheads, which\nweaken the scalability of BNPL services. To address these issues, we propose an\nefficient and privacy-preserving blockchain-based asynchronous payment scheme\n(Epass), which has promising scalability while protecting the privacy of\non-chain consumer transactions. Specifically, Epass leverages locally\nverifiable signatures to guarantee the privacy of consumer transactions against\nmalicious acts. Then, a privacy-preserving asynchronous payment scheme can be\nfurther constructed by leveraging time-release encryption to control trapdoors\nof redactable blockchain, reducing time overheads by modifying transactions for\ndeferred payment. We give formal definitions and security models, generic\nstructures, and formal proofs for Epass. Extensive comparisons and experimental\nanalysis show that \\textsf{Epass} achieves KB-level communication costs, and\nreduces time overhead by more than four times in comparisons with locally\nverifiable signatures and Go-Ethereum private test networks.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09387v1"}
{"id": "2506.09963", "title": "Dynamic Hypergraph Partitioning of Quantum Circuits with Hybrid Execution", "authors": ["Shane Sweeney", "Krishnendu Guha"], "summary": "Quantum algorithms offer an exponential speedup over classical algorithms for\na range of computational problems. The fundamental mechanisms underlying\nquantum computation required the development and construction of quantum\ncomputers. These devices are referred to as NISQ (Noisy Intermediate-Scale\nQuantum) devices. Not only are NISQ devices extremely limited in their qubit\ncount but they also suffer from noise during computation and this problem only\ngets worse as the size of the circuit increases which limits the practical use\nof quantum computers for modern day applications. This paper will focus on\nutilizing quantum circuit partitioning to overcome the inherent issues of NISQ\ndevices. Partitioning a quantum circuit into smaller subcircuits has allowed\nfor the execution of quantum circuits that are too large to fit on one quantum\ndevice. There have been many previous approaches to quantum circuit\npartitioning and each of these approaches differ in how they work with some\nfocusing on hardware-aware partitioning, optimal graph-based partitioning,\nmulti-processor architectures and many more. These approaches achieve success\nin their objective but they often fail to scale well which impacts cost and\nnoise. The ultimate goal of this paper is to mitigate these issues by\nminimizing 3 important metrics; noise, time and cost. To achieve this we use\ndynamic partitioning for practical circuit cutting and we take advantage of the\nbenefits of hybrid execution where classical computation will be used alongside\nquantum hardware. This approach has proved to be beneficial with respect to\nnoise with classical execution enabling a 42.30% reduction in noise and a 40%\nreduction in the number of qubits required in cases where a mixture of\nclassical and quantum computation were required.", "comment": "11 pages", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.09963v1"}
{"id": "2506.09169", "title": "Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport", "authors": ["Yuemin Mao", "Bardienus P. Duisterhof", "Moonyoung Lee", "Jeffrey Ichnowski"], "summary": "Object transport tasks are fundamental in robotic automation, emphasizing the\nimportance of efficient and secure methods for moving objects. Non-prehensile\ntransport can significantly improve transport efficiency, as it enables\nhandling multiple objects simultaneously and accommodating objects unsuitable\nfor parallel-jaw or suction grasps. Existing approaches incorporate constraints\nbased on the Coulomb friction model, which is imprecise during fast motions\nwhere inherent mechanical vibrations occur. Imprecise constraints can cause\ntransported objects to slide or even fall off the tray. To address this\nlimitation, we propose a novel method to learn a friction model using acoustic\nsensing that maps a tray's motion profile to a dynamically conditioned friction\ncoefficient. This learned model enables an optimization-based motion planner to\nadjust the friction constraint at each control step according to the planned\nmotion at that step. In experiments, we generate time-optimized trajectories\nfor a UR5e robot to transport various objects with constraints using both the\nstandard Coulomb friction model and the learned friction model. Results suggest\nthat the learned friction model reduces object displacement by up to 86.0%\ncompared to the baseline, highlighting the effectiveness of acoustic sensing in\nlearning real-world friction constraints.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09169v1"}
{"id": "2506.09067", "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09067v1"}
{"id": "2506.09250", "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "authors": ["C. Opus", "A. Lawsen"], "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "comment": "Comment on: arXiv:2506.06941", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09250v1"}
{"id": "2506.09080", "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09080v1"}
{"id": "2506.09570", "title": "Spectral Efficiency Maximization for DMA-enabled Multiuser MISO with Statistical CSI", "authors": ["Hao Xu", "Boyu Ning", "Chongjun Ouyang", "Hongwen Yang"], "summary": "Dynamic metasurface antennas (DMAs) offer the potential to achieve\nlarge-scale antenna arrays with low power consumption and reduced hardware\ncosts, making them a promising technology for future communication systems.\nThis paper investigates the spectral efficiency (SE) of DMA-enabled multiuser\nmultiple-input single-output (MISO) systems in both uplink and downlink\ntransmissions, using only statistical channel state information (CSI) to\nmaximize the ergodic sum rate of multiple users. For the uplink system, we\nconsider two decoding rules: minimum mean square error (MMSE) with and without\nsuccessive interference cancellation (SIC). For both decoders, we derive\nclosed-form surrogates to substitute the original expressions of ergodic sum\nrate and formulate tractable optimization problems for designing DMA weights.\nThen, a weighted MMSE (WMMSE)-based algorithm is proposed to maximize the\nergodic sum rate. For the downlink system, we derive an approximate expression\nfor the ergodic sum rate and formulate a hybrid analog/digital beamforming\noptimization problem that jointly optimizes the digital precoder and DMA\nweights. A penalty dual decomposition (PDD)-based algorithm is proposed by\nleveraging the fractional programming framework. Numerical results validate the\naccuracy of the derived surrogates and highlight the superiority of the\nproposed algorithms over baseline schemes. It is shown that these algorithms\nare effective across various DMA settings and are particularly well-suited for\nsystem design in fast time-varying channels.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09570v1"}
{"id": "2506.09289", "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09289v1"}
{"id": "2506.09153", "title": "Real-Time Confidence Detection through Facial Expressions and Hand Gestures", "authors": ["Tanjil Hasan Sakib", "Samia Jahan Mojumder", "Rajan Das Gupta", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Real-time face orientation recognition is a cutting-edge technology meant to\ntrack and analyze facial movements in virtual environments such as online\ninterviews, remote meetings, and virtual classrooms. As the demand for virtual\ninteractions grows, it becomes increasingly important to measure participant\nengagement, attention, and overall interaction. This research presents a novel\nsolution that leverages the Media Pipe Face Mesh framework to identify facial\nlandmarks and extract geometric data for calculating Euler angles, which\ndetermine head orientation in real time. The system tracks 3D facial landmarks\nand uses this data to compute head movements with a focus on accuracy and\nresponsiveness. By studying Euler angles, the system can identify a user's head\norientation with an accuracy of 90\\%, even at a distance of up to four feet.\nThis capability offers significant enhancements for monitoring user\ninteraction, allowing for more immersive and interactive virtual ex-periences.\nThe proposed method shows its reliability in evaluating participant\nattentiveness during online assessments and meetings. Its application goes\nbeyond engagement analysis, potentially providing a means for improving the\nquality of virtual communication, fostering better understanding between\nparticipants, and ensuring a higher level of interaction in digital spaces.\nThis study offers a basis for future developments in enhancing virtual user\nexperiences by integrating real-time facial tracking technologies, paving the\nway for more adaptive and interactive web-based platform.", "comment": "Accepted in MECON 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09153v1"}
{"id": "2506.09749", "title": "Large Language Models for Design Structure Matrix Optimization", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09749v1"}
{"id": "2506.09866", "title": "ELRUHNA: Elimination Rule-basedHypergraph Alignment", "authors": ["Cameron Ibrahim", "S M Ferdous", "Ilya Safro", "Marco Minutoli", "Mahantesh Halappanavar"], "summary": "Hypergraph alignment is a well-known NP-hard problem with numerous practical\napplications across domains such as bioinformatics, social network analysis,\nand computer vision. Despite its computational complexity, practical and\nscalable solutions are urgently needed to enable pattern discovery and entity\ncorrespondence in high-order relational data. The problem remains understudied\nin contrast to its graph based counterpart. In this paper, we propose ELRUHNA,\nan elimination rule-based framework for unsupervised hypergraph alignment that\noperates on the bipartite representation of hypergraphs. We introduce the\nincidence alignment formulation, a binary quadratic optimization approach that\njointly aligns vertices and hyperedges. ELRUHNA employs a novel similarity\npropagation scheme using local matching and cooling rules, supported by an\ninitialization strategy based on generalized eigenvector centrality for\nincidence matrices. Through extensive experiments on real-world datasets, we\ndemonstrate that ELRUHNA achieves higher alignment accuracy compared to\nstate-of-the-art algorithms, while scaling effectively to large hypergraphs.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.09866v1"}
{"id": "2506.09434", "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09434v1"}
{"id": "2506.09187", "title": "A Data-driven Predictive Control Architecture for Train Thermal Energy Management", "authors": ["Ahmed Aboudonia", "Johannes Estermann", "Keith Moffat", "Manfred Morari", "John Lygeros"], "summary": "We aim to improve the energy efficiency of train climate control\narchitectures, with a focus on a specific class of regional trains operating\nthroughout Switzerland, especially in Zurich and Geneva. Heating, Ventilation,\nand Air Conditioning (HVAC) systems represent the second largest energy\nconsumer in these trains after traction. The current architecture comprises a\nhigh-level rule-based controller and a low-level tracking controller. To\nimprove train energy efficiency, we propose adding a middle data-driven\npredictive control layer aimed at minimizing HVAC energy consumption while\nmaintaining passenger comfort. The scheme incorporates a multistep prediction\nmodel developed using real-world data collected from a limited number of train\ncoaches. To validate the effectiveness of the proposed architecture, we conduct\nmultiple experiments on a separate set of train coaches; our results suggest\nenergy savings between 10% and 35% with respect to the current architecture.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09187v1"}
{"id": "2506.09197", "title": "Adaptive Bandwidth Sharing for Optimizing QoE of Real-Time Video", "authors": ["Sushi Anna George", "Vinay Joseph"], "summary": "The concept of spectrum or bandwidth sharing has gained significant global\nattention as a means to enhance the efficiency of real-time traffic management\nin wireless networks. Effective bandwidth sharing enables optimal utilization\nof available resources, reducing congestion and improving QoE for\ndelay-sensitive applications such as real-time video transmission. In this\npaper, we propose a novel iterative semi-static bandwidth sharing policy that\nbalances the advantages of both static and dynamic sharing approaches. Our\napproach minimizes the frequency of coordination between network operators\nwhile ensuring efficient resource allocation and meeting the stringent QoE\ndemands of real-time traffic. The proposed policy iteratively optimizes both\nthe spectrum sharing between operators and the resource allocation for\nindividual clients. We establish strong theoretical guarantees for the\noptimality of the proposed policy and prove that it converges to the optimal\nstatic sharing policy irrespective of initial conditions or fluctuations in\ntraffic arrival rates. Additionally, we conduct extensive simulations to\nevaluate the impact of key system parameters - including step size, hyperperiod\nlength, and arrival process dynamics - on the performance of our policy. Our\nresults demonstrate the effectiveness of the proposed approach in achieving\nnear-optimal bandwidth allocation with reduced overhead, making it a practical\nsolution for real-time wireless applications.", "comment": "arXiv admin note: text overlap with arXiv:2401.10681", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09197v1"}
{"id": "2506.09426", "title": "Exploiting Control-flow Enforcement Technology for Sound and Precise Static Binary Disassembly", "authors": ["Brian Zhao", "Yiwei Yang", "Yusheng Zheng", "Andi Quinn"], "summary": "Rewriting x86_64 binaries-whether for security hardening, dynamic\ninstrumentation, or performance profiling is notoriously difficult due to\nvariable-length instructions, interleaved code and data, and indirect jumps to\narbitrary byte offsets. Existing solutions (e.g., \"superset disassembly\")\nensure soundness but incur significant overhead and produce large rewritten\nbinaries, especially for on-the-fly instrumentation. This paper addresses these\nchallenges by introducing the Time Variance Authority (TVA), which leverages\nIntel's Control-Flow Enforcement Technology (CET). By recognizing endbr64 as\nthe only valid indirect jump target, TVA prunes spurious disassembly paths\nwhile preserving soundness and emulates CET constraints on processors lacking\nnative CET support, effectively mitigating ROP/JOP exploits without new\nhardware. We implement TVA by modernizing the Multiverse rewriter for 64-bit\nLinux. Our evaluation on SPEC CPU2017 and real-world applications shows that\nTVA-guided rewriting achieves up to 1.3x faster instrumentation time. These\nresults underscore TVA's feasibility as a high-performance, uprobes-free\nalternative for robust x86_64 binary analysis and rewriting.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.09426v1"}
{"id": "2506.09418", "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G", "authors": ["Ryan Barker", "Fatemeh Afghah"], "summary": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G.", "comment": "4 pages, 1 figure", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09418v1"}
{"id": "2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09160v1"}
{"id": "2506.09182", "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method", "authors": ["Hang Zhou", "Chengyuan Ma", "Shiyu Shen", "Xiaopeng Li"], "summary": "With the rapid development of automated vehicles (AVs) in recent years,\ncommercially available AVs are increasingly demonstrating high-level automation\ncapabilities. However, most existing AV safety evaluation methods are primarily\ndesigned for simple maneuvers such as car-following and lane-changing. While\nsuitable for basic tests, these methods are insufficient for assessing\nhigh-level automation functions deployed in more complex environments. First,\nthese methods typically use crash rate as the evaluation metric, whose accuracy\nheavily depends on the quality and completeness of naturalistic driving\nenvironment data used to estimate scenario probabilities. Such data is often\ndifficult and expensive to collect. Second, when applied to diverse scenarios,\nthese methods suffer from the curse of dimensionality, making large-scale\nevaluation computationally intractable. To address these challenges, this paper\nproposes a novel framework for full-scenario AV safety evaluation. A unified\nmodel is first introduced to standardize the representation of diverse driving\nscenarios. This modeling approach constrains the dimension of most scenarios to\na regular highway setting with three lanes and six surrounding background\nvehicles, significantly reducing dimensionality. To further avoid the\nlimitations of probability-based method, we propose a volume-based evaluation\nmethod that quantifies the proportion of risky scenarios within the entire\nscenario space. For car-following scenarios, we prove that the set of safe\nscenarios is convex under specific settings, enabling exact volume computation.\nExperimental results validate the effectiveness of the proposed volume-based\nmethod using both AV behavior models from existing literature and six\nproduction AV models calibrated from field-test trajectory data in the Ultra-AV\ndataset. Code and data will be made publicly available upon acceptance of this\npaper.", "comment": "NA", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09182v1"}
{"id": "2506.09068", "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09068v1"}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1"}
{"id": "2506.09084", "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09084v1"}
{"id": "2506.09651", "title": "On the Ding and Helleseth's 8th open problem about optimal ternary cyclic codes", "authors": ["Dong He", "Peipei Zheng", "Qunying Liao"], "summary": "The cyclic code is a subclass of linear codes and has applications in\nconsumer electronics, data storage systems and communication systems due to the\nefficient encoding and decoding algorithms. In 2013, Ding, et al. presented\nnine open problems about optimal ternary cyclic codes. Till now, the 1st, 2nd,\n6th and 7th problems were completely solved, the 3rd, 8th and 9th problems were\nincompletely solved. In this manuscript, we focus on the 8th problem. By\ndetermining the root set of some special polynomials over finite fields, we\npresent a counterexample and a sufficient condition for the ternary cyclic code\n$\\mathcal{C}_{(1, e)}$ optimal. Furthermore, basing on the properties of finite\nfields, we construct a class of optimal ternary cyclic codes with respect to\nthe Sphere Packing Bound, and show that these codes are not equivalent to any\nknown codes.", "comment": "17 pages", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09651v1"}
{"id": "2506.09370", "title": "Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study", "authors": ["Rohit Mehra", "Priyavanshi Pathania", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "Advances in technologies like artificial intelligence and metaverse have led\nto a proliferation of software systems in business and everyday life. With this\nwidespread penetration, the carbon emissions of software are rapidly growing as\nwell, thereby negatively impacting the long-term sustainability of our\nenvironment. Hence, optimizing software from a sustainability standpoint\nbecomes more crucial than ever. We believe that the adoption of automated tools\nthat can identify energy-inefficient patterns in the code and guide appropriate\nrefactoring can significantly assist in this optimization. In this extended\nabstract, we present an industry case study that evaluates the sustainability\nimpact of refactoring energy-inefficient code patterns identified by automated\nsoftware sustainability assessment tools for a large application. Preliminary\nresults highlight a positive impact on the application's sustainability\npost-refactoring, leading to a 29% decrease in per-user per-month energy\nconsumption.", "comment": "3 pages. To be published in the proceedings of 38th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2023),\n  Kirchberg, Luxembourg", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09370v1"}
{"id": "2506.09212", "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09212v1"}
{"id": "2506.09755", "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09755v1"}
{"id": "2506.09221", "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation", "authors": ["Michael Soprano"], "summary": "The spread of online misinformation poses serious threats to democratic\nsocieties. Traditionally, expert fact-checkers verify the truthfulness of\ninformation through investigative processes. However, the volume and immediacy\nof online content present major scalability challenges. Crowdsourcing offers a\npromising alternative by leveraging non-expert judgments, but it introduces\nconcerns about bias, accuracy, and interpretability. This thesis investigates\nhow human intelligence can be harnessed to assess the truthfulness of online\ninformation, focusing on three areas: misinformation assessment, cognitive\nbiases, and automated fact-checking systems. Through large-scale crowdsourcing\nexperiments and statistical modeling, it identifies key factors influencing\nhuman judgments and introduces a model for the joint prediction and explanation\nof truthfulness. The findings show that non-expert judgments often align with\nexpert assessments, particularly when factors such as timing and experience are\nconsidered. By deepening our understanding of human judgment and bias in\ntruthfulness assessment, this thesis contributes to the development of more\ntransparent, trustworthy, and interpretable systems for combating\nmisinformation.", "comment": "PhD thesis, University of Udine, defended May 2023, 458 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09221v1"}
{"id": "2506.09600", "title": "Effective Red-Teaming of Policy-Adherent Agents", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09600v1"}
{"id": "2506.09273", "title": "Data-Driven Nonlinear Regulation: Gaussian Process Learning", "authors": ["Telema Harry", "Martin Guay", "Shimin Wang", "Richard D. Braatz"], "summary": "This article addresses the output regulation problem for a class of nonlinear\nsystems using a data-driven approach. An output feedback controller is proposed\nthat integrates a traditional control component with a data-driven learning\nalgorithm based on Gaussian Process (GP) regression to learn the nonlinear\ninternal model. Specifically, a data-driven technique is employed to directly\napproximate the unknown internal model steady-state map from observed\ninput-output data online. Our method does not rely on model-based observers\nutilized in previous studies, making it robust and suitable for systems with\nmodelling errors and model uncertainties. Finally, we demonstrate through\nnumerical examples and detailed stability analysis that, under suitable\nconditions, the closed-loop system remains bounded and converges to a compact\nset, with the size of this set decreasing as the accuracy of the data-driven\nmodel improves over time.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09273v1"}
{"id": "2506.09245", "title": "Age of Information in Unreliable Tandem Queues", "authors": ["Muthukrishnan Senthilkumar", "Aresh Dadlani", "Hina Tabassum"], "summary": "Stringent demands for timely information delivery, driven by the widespread\nadoption of real-time applications and the Internet of Things, have established\nthe age of information (AoI) as a critical metric for quantifying data\nfreshness. Existing AoI models often assume multi-hop communication networks\nwith fully reliable nodes, which may not accurately capture scenarios involving\nnode transmission failures. This paper presents an analytical framework for two\nconfigurations of tandem queue systems, where status updates generated by a\nsingle sensor are relayed to a destination monitor through unreliable\nintermediate nodes. Using the probability generating function, we first derive\nthe sojourn time distribution for an infinite-buffer M/M/1 tandem system with\ntwo unreliable nodes. We then extend our analysis to an M/G/1 tandem system\nwith an arbitrary number of unreliable nodes, employing the supplementary\nvariable technique while assuming that only the first node has an infinite\nbuffer. Numerical results demonstrate the impact of key system parameters on\nthe average AoI in unreliable tandem queues with Markovian and non-Markovian\nservice times.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09245v1"}
{"id": "2506.09596", "title": "FPGA-Based Multiplier with a New Approximate Full Adder for Error-Resilient Applications", "authors": ["Ali Ranjbar", "Elham Esmaeili", "Roghayeh Rafieisangari", "Nabiollah Shiri"], "summary": "Electronic devices primarily aim to offer low power consumption, high speed,\nand a compact area. The performance of very large-scale integration (VLSI)\ndevices is influenced by arithmetic operations, where multiplication is a\ncrucial operation. Therefore, a high-speed multiplier is essential for\ndeveloping any signal-processing module. Numerous multipliers have been\nreviewed in existing literature, and their speed is largely determined by how\npartial products (PPs) are accumulated. To enhance the speed of multiplication\nbeyond current methods, an approximate adder-based multiplier is introduced.\nThis approach allows for the simultaneous addition of PPs from two consecutive\nbits using a novel approximate adder. The proposed multiplier is utilized in a\nmean filter structure and implemented in ISE Design Suite 14.7 using VHDL and\nsynthesized on the Xilinx Spartan3-XC3S400 FPGA board. Compared to the\nliterature, the proposed multiplier achieves power and power-delay product\n(PDP) improvements of 56.09% and 73.02%, respectively. The validity of the\nexpressed multiplier is demonstrated through the mean filter system. Results\nshow that it achieves power savings of 33.33%. Additionally, the proposed\nmultiplier provides more accurate results than other approximate multipliers by\nexpressing higher values of peak signal-to-noise ratio (PSNR), (30.58%), and\nstructural similarity index metric (SSIM), (22.22%), while power consumption is\nin a low range.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.09596v1"}
{"id": "2506.09231", "title": "Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality", "authors": ["Saba Tabatabaee", "Suzanne Boyce", "Liran Oren", "Mark Tiede", "Carol Espy-Wilson"], "summary": "Speech is produced through the coordination of vocal tract constricting\norgans: lips, tongue, velum, and glottis. Previous works developed Speech\nInversion (SI) systems to recover acoustic-to-articulatory mappings for lip and\ntongue constrictions, called oral tract variables (TVs), which were later\nenhanced by including source information (periodic and aperiodic energies, and\nF0 frequency) as proxies for glottal control. Comparison of the nasometric\nmeasures with high-speed nasopharyngoscopy showed that nasalance can serve as\nground truth, and that an SI system trained with it reliably recovers velum\nmovement patterns for American English speakers. Here, two SI training\napproaches are compared: baseline models that estimate oral TVs and nasalance\nindependently, and a synergistic model that combines oral TVs and source\nfeatures with nasalance. The synergistic model shows relative improvements of\n5% in oral TVs estimation and 9% in nasalance estimation compared to the\nbaseline models.", "comment": "Accepted to be presented at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09231v1"}
{"id": "2506.09061", "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "comment": "4 figures, 7 pages, IEEE conference template", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09061v1"}
{"id": "2506.09167", "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "comment": "13 pages", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09167v1"}
{"id": "2506.09065", "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "comment": "6 pages, 8 figures, and 1 table", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09065v1"}
{"id": "2506.09073", "title": "Understanding and Improving Data Repurposing", "authors": ["J. Parsons", "R. Lukyanenko", "B. Greenwood", "C. Cooper"], "summary": "We live in an age of unprecedented opportunities to use existing data for\ntasks not anticipated when those data were collected, resulting in widespread\ndata repurposing. This commentary defines and maps the scope of data\nrepurposing to highlight its importance for organizations and society and the\nneed to study data repurposing as a frontier of data management. We explain how\nrepurposing differs from original data use and data reuse and then develop a\nframework for data repurposing consisting of concepts and activities for\nadapting existing data to new tasks. The framework and its implications are\nillustrated using two examples of repurposing, one in healthcare and one in\ncitizen science. We conclude by suggesting opportunities for research to better\nunderstand data repurposing and enable more effective data repurposing\npractices.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09073v1"}
{"id": "2506.09443", "title": "LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge", "authors": ["Songze Li", "Chuokun Xu", "Jiaying Wang", "Xueluan Gong", "Chen Chen", "Jirui Zhang", "Jun Wang", "Kwok-Yan Lam", "Shouling Ji"], "summary": "Large Language Models (LLMs) have demonstrated remarkable intelligence across\nvarious tasks, which has inspired the development and widespread adoption of\nLLM-as-a-Judge systems for automated model testing, such as red teaming and\nbenchmarking. However, these systems are susceptible to adversarial attacks\nthat can manipulate evaluation outcomes, raising concerns about their\nrobustness and, consequently, their trustworthiness. Existing evaluation\nmethods adopted by LLM-based judges are often piecemeal and lack a unified\nframework for comprehensive assessment. Furthermore, prompt template and model\nselections for improving judge robustness have been rarely explored, and their\nperformance in real-world settings remains largely unverified. To address these\ngaps, we introduce RobustJudge, a fully automated and scalable framework\ndesigned to systematically evaluate the robustness of LLM-as-a-Judge systems.\nRobustJudge investigates the impact of attack methods and defense strategies\n(RQ1), explores the influence of prompt template and model selection (RQ2), and\nassesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our\nmain findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range\nof adversarial attacks, including Combined Attack and PAIR, while defense\nmechanisms such as Re-tokenization and LLM-based Detectors offer improved\nprotection; (2) Robustness is highly sensitive to the choice of prompt template\nand judge models. Our proposed prompt template optimization method can improve\nrobustness, and JudgeLM-13B demonstrates strong performance as a robust\nopen-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals\npreviously unreported vulnerabilities. The source code of RobustJudge is\nprovided at https://github.com/S3IC-Lab/RobustJudge.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09443v1"}
{"id": "2506.09182", "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method", "authors": ["Hang Zhou", "Chengyuan Ma", "Shiyu Shen", "Xiaopeng Li"], "summary": "With the rapid development of automated vehicles (AVs) in recent years,\ncommercially available AVs are increasingly demonstrating high-level automation\ncapabilities. However, most existing AV safety evaluation methods are primarily\ndesigned for simple maneuvers such as car-following and lane-changing. While\nsuitable for basic tests, these methods are insufficient for assessing\nhigh-level automation functions deployed in more complex environments. First,\nthese methods typically use crash rate as the evaluation metric, whose accuracy\nheavily depends on the quality and completeness of naturalistic driving\nenvironment data used to estimate scenario probabilities. Such data is often\ndifficult and expensive to collect. Second, when applied to diverse scenarios,\nthese methods suffer from the curse of dimensionality, making large-scale\nevaluation computationally intractable. To address these challenges, this paper\nproposes a novel framework for full-scenario AV safety evaluation. A unified\nmodel is first introduced to standardize the representation of diverse driving\nscenarios. This modeling approach constrains the dimension of most scenarios to\na regular highway setting with three lanes and six surrounding background\nvehicles, significantly reducing dimensionality. To further avoid the\nlimitations of probability-based method, we propose a volume-based evaluation\nmethod that quantifies the proportion of risky scenarios within the entire\nscenario space. For car-following scenarios, we prove that the set of safe\nscenarios is convex under specific settings, enabling exact volume computation.\nExperimental results validate the effectiveness of the proposed volume-based\nmethod using both AV behavior models from existing literature and six\nproduction AV models calibrated from field-test trajectory data in the Ultra-AV\ndataset. Code and data will be made publicly available upon acceptance of this\npaper.", "comment": "NA", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09182v1"}
{"id": "2506.09217", "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09217v1"}
{"id": "2506.09071", "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09071v1"}
{"id": "2506.09390", "title": "Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making", "authors": ["Kehan Zheng", "Jinfeng Zhou", "Hongning Wang"], "summary": "Large language models are increasingly used in strategic decision-making\nsettings, yet evidence shows that, like humans, they often deviate from full\nrationality. In this study, we compare LLMs and humans using experimental\nparadigms directly adapted from behavioral game-theory research. We focus on\ntwo well-studied strategic games, Rock-Paper-Scissors and the Prisoner's\nDilemma, which are well known for revealing systematic departures from rational\nplay in human subjects. By placing LLMs in identical experimental conditions,\nwe evaluate whether their behaviors exhibit the bounded rationality\ncharacteristic of humans. Our findings show that LLMs reproduce familiar human\nheuristics, such as outcome-based strategy switching and increased cooperation\nwhen future interaction is possible, but they apply these rules more rigidly\nand demonstrate weaker sensitivity to the dynamic changes in the game\nenvironment. Model-level analyses reveal distinctive architectural signatures\nin strategic behavior, and even reasoning models sometimes struggle to find\neffective strategies in adaptive situations. These results indicate that\ncurrent LLMs capture only a partial form of human-like bounded rationality and\nhighlight the need for training methods that encourage flexible opponent\nmodeling and stronger context awareness.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09390v1"}
{"id": "2506.09085", "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09085v1"}
{"id": "2506.09689", "title": "BF-Max: an Efficient Bit Flipping Decoder with Predictable Decoding Failure Rate", "authors": ["Alessio Baldelli", "Marco Baldi", "Franco Chiaraluce", "Paolo Santini"], "summary": "The Bit-Flipping (BF) decoder, thanks to its very low computational\ncomplexity, is widely employed in post-quantum cryptographic schemes based on\nModerate Density Parity Check codes in which, ultimately, decryption boils down\nto syndrome decoding. In such a setting, for security concerns, one must\nguarantee that the Decoding Failure Rate (DFR) is negligible. Such a condition,\nhowever, is very difficult to guarantee, because simulations are of little help\nand the decoder performance is difficult to model theoretically. In this paper,\nwe introduce a new version of the BF decoder, that we call BF-Max,\ncharacterized by the fact that in each iteration only one bit (the least\nreliable) is flipped. When the number of iterations is equal to the number of\nerrors to be corrected, we are able to develop a theoretical characterization\nof the DFR that tightly matches with numerical simulations. We also show how\nBF-Max can be implemented efficiently, achieving low complexity and making it\ninherently constant time. With our modeling, we are able to accurately predict\nvalues of DFR that are remarkably lower than those estimated by applying other\napproaches.", "comment": "5 pages plus 1 page that contains only bibliography, 2 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09689v1"}
{"id": "2506.09396", "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "authors": ["Zongjie Li", "Shuai Wang"], "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09396v1"}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software.", "comment": "Accepted at CSCW 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09216v1"}
{"id": "2506.09822", "title": "Superstudent intelligence in thermodynamics", "authors": ["Rebecca Loubet", "Pascal Zittlau", "Marco Hoffmann", "Luisa Vollmer", "Sophie Fellenz", "Heike Leitte", "Fabian Jirasek", "Johannes Lenhard", "Hans Hasse"], "summary": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.", "comment": "This document is the unedited Author's version of a yet to be\n  Submitted Work to Physical Review Physics Education Research. 15 pages, 2\n  figures, Graphical Abstract, Highlights and SI available (12 pages)", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09822v1"}
{"id": "2506.09726", "title": "Don't be Afraid of Cell Complexes! An Introduction from an Applied Perspective", "authors": ["Josef Hoppe", "Vincent P. Grande", "Michael T. Schaub"], "summary": "Cell complexes (CCs) are a higher-order network model deeply rooted in\nalgebraic topology that has gained interest in signal processing and network\nscience recently. However, while the processing of signals supported on CCs can\nbe described in terms of easily-accessible algebraic or combinatorial notions,\nthe commonly presented definition of CCs is grounded in abstract concepts from\ntopology and remains disconnected from the signal processing methods developed\nfor CCs. In this paper, we aim to bridge this gap by providing a simplified\ndefinition of CCs that is accessible to a wider audience and can be used in\npractical applications. Specifically, we first introduce a simplified notion of\nabstract regular cell complexes (ARCCs). These ARCCs only rely on notions from\nalgebra and can be shown to be equivalent to regular cell complexes for most\npractical applications. Second, using this new definition we provide an\naccessible introduction to (abstract) cell complexes from a perspective of\nnetwork science and signal processing. Furthermore, as many practical\napplications work with CCs of dimension 2 and below, we provide an even simpler\ndefinition for this case that significantly simplifies understanding and\nworking with CCs in practice.", "comment": "Preprint version, comments welcome!", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09726v1"}
{"id": "2506.09195", "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09195v1"}
{"id": "2506.09388", "title": "Integer-Clustering Optimization of Hydrogen and Battery EV Fleets Considering DERs", "authors": ["Sijia Geng", "Thomas Lee", "Dharik Mallapragada", "Audun Botterud"], "summary": "Electrified transportation leads to a tighter integration between\ntransportation and energy distribution systems. In this work, we develop\nscalable optimization models to co-design hydrogen and battery electric vehicle\n(EV) fleets, distributed energy resources, and fast-charging and\nhydrogen-fueling infrastructure to efficiently meet transportation demands. A\nnovel integer-clustering formulation is used for optimizing fleet-level EV\noperation while maintaining accurate individual vehicle dispatch, which\nsignificantly improves the computation efficiency with guaranteed performance.\nWe apply the optimization model to Boston's public transit bus network using\nreal geospatial data and cost parameters. Realistic insights are provided into\nthe future evolution of coupled electricity-transportation-hydrogen systems,\nincluding the effects of electricity price structure, hydrogen fuel cost,\ncarbon emission constraint, temperature effects on EV range, and distribution\nsystem upgrade cost.", "comment": "10 pages, 9 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09388v1"}
{"id": "2506.09268", "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09268v1"}
{"id": "2506.09198", "title": "Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation", "authors": ["Ali Rezaei", "Luc Jaulmes", "Maria Bahna", "Oliver Thomson Brown", "Antonio Barbalace"], "summary": "Scalable classical simulation of quantum circuits is crucial for advancing\nboth quantum algorithm development and hardware validation. In this work, we\nfocus on performance enhancements through meticulous low-level tuning on a\nsingle-node system, thereby not only advancing the performance of classical\nquantum simulations but also laying the groundwork for scalable, heterogeneous\nimplementations that may eventually bridge the gap toward noiseless quantum\ncomputing. Although similar efforts in low-level tuning have been reported in\nthe literature, such implementations have not been released as open-source\nsoftware, thereby impeding independent evaluation and further development. We\nintroduce an open-source, high-performance extension to the QuEST simulator\nthat brings state-of-the-art low-level and NUMA optimizations to modern\ncomputers. Our approach emphasizes locality-aware computation and incorporates\nhardware-specific optimizations such as NUMA-aware memory allocation, thread\npinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory\nprefetching. Experiments demonstrate significant speedups - 5.5-6.5x for\nsingle-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum\nCircuits (RQC), and 1.8x for Quantum Fourier Transform (QFT), demonstrating\nthat rigorous performance tuning can substantially extend the practical\nsimulation capacity of classical quantum simulators on current hardware.", "comment": "12 pages, 9 figures, 2 tables, 2 pseudocodes", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09198v1"}
{"id": "2506.09521", "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "authors": ["Ünal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanuël A. P. Habets", "Nils Peters"], "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09521v1"}
{"id": "2506.09242", "title": "Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism", "authors": ["Jonas Latt", "Christophe Coreixas"], "summary": "This article presents the principles, software architecture, and performance\nanalysis of the GPU port of the lattice Boltzmann software library Palabos (J.\nLatt et al., \"Palabos: Parallel lattice Boltzmann solver\", Comput. Math. Appl.\n81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which\nnumerical components are selectively assigned to either the CPU or the GPU,\ndepending on considerations of performance or convenience. This design enables\na progressive porting strategy, allowing most features of the original\nCPU-based codebase to be gradually and seamlessly adapted to GPU execution. The\nnew architecture builds upon two complementary paradigms: a classical\nobject-oriented structure for CPU execution, and a data-oriented counterpart\nfor GPUs, which reproduces the modularity of the original code while\neliminating object-oriented overhead detrimental to GPU performance. Central to\nthis approach is the use of modern C++, including standard parallel algorithms\nand template metaprogramming techniques, which permit the generation of\nhardware-agnostic computational kernels. This facilitates the development of\nuser-defined, GPU-accelerated components such as collision operators or\nboundary conditions, while preserving compatibility with the existing codebase\nand avoiding the need for external libraries or non-standard language\nextensions. The correctness and performance of the GPU-enabled Palabos are\ndemonstrated through a series of three-dimensional multiphysics benchmarks,\nincluding the laminar-turbulent transition in a Taylor-Green vortex, lid-driven\ncavity flow, and pore-scale flow in Berea sandstone. Despite the high-level\nabstraction of the implementation, the single-GPU performance is similar to\nCUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling\nacross all test cases.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09242v1"}
{"id": "2506.09181", "title": "Energy efficiency of DMAs vs. conventional MIMO: a sensitivity analysis", "authors": ["Pablo Ramírez-Espinosa", "David Morales-Jiménez", "Beatriz Soret"], "summary": "Motivated by the stringent and challenging need for `greener communications'\nin increasingly power-hungry 5G networks, this paper presents a detailed energy\nefficiency analysis for three different multi-antenna architectures, namely\nfully-digital arrays, hybrid arrays, and dynamic metasurface antennas (DMAs).\nBy leveraging a circuital model, which captures mutual coupling, insertion\nlosses, propagation through the waveguides in DMAs and other electromagnetic\nphenomena, we design a transmit Wiener filter solution for the three systems.\nWe then use these results to analyze the energy efficiency, considering\ndifferent consumption models and supplied power, and with particular focus on\nthe impact of the physical phenomena. DMAs emerge as an efficient alternative\nto classical arrays across diverse tested scenarios, most notably under low\ntransmission power, strong coupling, and scalability requirements.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09181v1"}
{"id": "2506.09095", "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Clárisa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09095v1"}
{"id": "2506.09102", "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation", "authors": ["Mihaela van der Schaar", "Richard Peck", "Eoin McKinney", "Jim Weatherall", "Stuart Bailey", "Justine Rochon", "Chris Anagnostopoulos", "Pierre Marquet", "Anthony Wood", "Nicky Best", "Harry Amad", "Julianna Piskorz", "Krzysztof Kacprzyk", "Rafik Salama", "Christina Gunther", "Francesca Frau", "Antoine Pugeat", "Ramon Hernandez"], "summary": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09102v1"}
{"id": "2506.09966", "title": "Tight Paths and Tight Pairs in Weighted Directed Graphs", "authors": ["José Luis Balcázar"], "summary": "We state the graph-theoretic computational problem of finding tight paths in\na directed, edge-weighted graph, as well as its simplification of finding tight\npairs. These problems are motivated by the need of algorithms that find\nso-called basic antecedents in closure spaces, in one specific approach to data\nanalysis. We discuss and compare several algorithms to approach these problems.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.09966v1"}
{"id": "2506.09070", "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09070v1"}
{"id": "2506.09464", "title": "Efficient Modular Multiplier over GF (2^m) for ECPM", "authors": ["Ruby Kumari", "Gaurav Purohit", "Abhijit Karmakar"], "summary": "Elliptic curve cryptography (ECC) has emerged as the dominant public-key\nprotocol, with NIST standardizing parameters for binary field GF(2^m) ECC\nsystems. This work presents a hardware implementation of a Hybrid\nMultiplication technique for modular multiplication over binary field GF(2m),\ntargeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the\ncombination of conventional multiplication (CM) and Karatsuba multiplication\n(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation\nuses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,\nreducing computational complexity and enhancing efficiency. The design is\nevaluated in three areas: Resource Utilization For m=163, the hybrid design\nuses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For\nm=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and\nbit-parallel implementations. Delay Performance For m=163, achieves 13.31ns\ndelay, improving by 37.60% over bit-parallel implementations. For m=233,\nmaintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,\noutperforming bit-parallel (75,337) and digit-serial (43,179) implementations.\nFor m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over\nbit-parallel designs. Results show the hybrid technique significantly improves\nspeed, hardware efficiency, and resource utilization for ECC cryptographic\nsystems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09464v1"}
{"id": "2506.09505", "title": "On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems", "authors": ["Dumitrel Loghin", "Shuang Liang", "Shengwei Liu", "Xiong Liu", "Pingcheng Ruan", "Zhigang Ye"], "summary": "Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling\nblockchains and bringing Web3 to life. At the same time, ZKP for transactions\nrunning on the Ethereum Virtual Machine require powerful servers with hundreds\nof CPU cores. The current zkProver implementation from Polygon is optimized for\nx86-64 CPUs by vectorizing key operations, such as Merkle tree building with\nPoseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX\nand AVX512). With these optimizations, a ZKP for a batch of transactions is\ngenerated in less than two minutes. With the advent of cloud servers with ARM\nwhich are at least 10% cheaper than x86-64 servers and the implementation of\nARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over\ntheir x86-64 counterparts. Unfortunately, our analysis shows that current ARM\nCPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web\nServices (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X\nslower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX\nand AVX512, respectively, when building a Merkle tree with over four million\nleaves. This low performance is due to (1) smaller vector size in these ARM\nCPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the\nother hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as\npowerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing\nthe vector size to 512 bits will enable higher performance in ARM CPUs compared\nto their x86-64 counterparts while maintaining their price advantage.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09505v1"}
{"id": "2506.09284", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09284v1"}
{"id": "2506.09079", "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09079v1"}
{"id": "2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09420v1"}
{"id": "2506.09087", "title": "Spiking Neural Models for Decision-Making Tasks with Learning", "authors": ["Sophie Jaffard", "Giulia Mezzadri", "Patricia Reynaud-Bouret", "Etienne Tanré"], "summary": "In cognition, response times and choices in decision-making tasks are\ncommonly modeled using Drift Diffusion Models (DDMs), which describe the\naccumulation of evidence for a decision as a stochastic process, specifically a\nBrownian motion, with the drift rate reflecting the strength of the evidence.\nIn the same vein, the Poisson counter model describes the accumulation of\nevidence as discrete events whose counts over time are modeled as Poisson\nprocesses, and has a spiking neurons interpretation as these processes are used\nto model neuronal activities. However, these models lack a learning mechanism\nand are limited to tasks where participants have prior knowledge of the\ncategories. To bridge the gap between cognitive and biological models, we\npropose a biologically plausible Spiking Neural Network (SNN) model for\ndecision-making that incorporates a learning mechanism and whose neurons\nactivities are modeled by a multivariate Hawkes process. First, we show a\ncoupling result between the DDM and the Poisson counter model, establishing\nthat these two models provide similar categorizations and reaction times and\nthat the DDM can be approximated by spiking Poisson neurons. To go further, we\nshow that a particular DDM with correlated noise can be derived from a Hawkes\nnetwork of spiking neurons governed by a local learning rule. In addition, we\ndesigned an online categorization task to evaluate the model predictions. This\nwork provides a significant step toward integrating biologically relevant\nneural mechanisms into cognitive models, fostering a deeper understanding of\nthe relationship between neural activity and behavior.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09087v1"}
{"id": "2506.09931", "title": "Faster-than-Nyquist Signaling is Good for Single-Carrier ISAC: An Analytical Study", "authors": ["Shuangyang Li", "Fan Liu", "Yifeng Xiong", "Weijie Yuan", "Baoming Bai", "Christos Masouros", "Giuseppe Caire"], "summary": "In this paper, we provide an analytical study of single-carrier\nfaster-than-Nyquist (FTN) signaling for integrated sensing and communications\n(ISAC). Our derivations show that FTN is advantageous for ISAC, and reveal new\ninsights that these advantages come from the fact that FTN signaling can\neffectively avoid the spectral aliasing due to the mismatch between the symbol\nrate and the bandwidth of the shaping pulse. Specifically, the communication\nspectral efficiency advantages of FTN signaling over time-invariant multipath\nchannels are analytically shown, where both upper- and lower-bounds on the\nspectral efficiency are derived. We show that the gap between these two bounds\ncorresponds to the potential signal-to-noise ratio (SNR) variation due to the\npresence of multipath delay and spectral aliasing, which diminishes as the\nsymbol rate grows higher. Particularly, in the limiting case, this SNR\nvariation disappears while the degree of freedom (DoF) of the system attain the\nmaximum. Furthermore, the sensing advantages for FTN signals are verified in\nterms of the expected normalized squared ambiguity function. We show that FTN\nsignals generally enjoy a more robust ranging performance. More importantly, we\nprove that FTN signaling can effectively avoid the undesired peaks in the\nconsidered ambiguity function along the Doppler dimension, thereby reducing the\nambiguities in velocity estimation. All these conclusions are explicitly\nverified by numerical results.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09931v1"}
{"id": "2506.09550", "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries", "authors": ["Fanpeng Yang", "Xu Ma", "Shuling Wang", "Xiong Xu", "Qinxiang Cao", "Naijun Zhan", "Xiaofeng Li", "Bin Gu"], "summary": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09550v1"}
{"id": "2506.09220", "title": "Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use", "authors": ["Karen Joy", "Tawfiq Ammari", "Alyssa Sheehan"], "summary": "This paper examines the gap between the promises and real-world performance\nof emerging AI personal assistants. Drawing on interviews with early adopters\nof devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and\nDocus, we map user experiences through the lens of Uses and Gratifications and\nUncertainty Reduction Theory. We identify three core types of user uncertainty,\nfunctional, interactional, and social, and explore how each disrupts different\nuser gratifications. We show that while marketing hype fuels initial adoption,\nunmet expectations often result in frustration or abandonment. Our findings\nhighlight the importance of transparency, task-specific design, and user\ncontrol over contextual memory and personalization. We provide design and\npolicy recommendations, including user-facing explainability tools and calls\nfor regulatory benchmarks such as CI Bench, to guide ethical and interpretable\nAI integration. Our study offers actionable insights for creating more usable,\ntrustworthy, and socially aligned AI assistants.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09220v1"}
{"id": "2506.09332", "title": "Natural Language Guided Ligand-Binding Protein Design", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09332v1"}
{"id": "2506.09947", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09947v1"}
{"id": "2506.09331", "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "authors": ["Arjun Vaithilingam Sudhakar"], "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09331v1"}
{"id": "2506.09392", "title": "Voltage-Controlled Oscillator and Memristor-Based Analog Computing for Solving Systems of Linear Equations", "authors": ["Hao Li", "Rizwan S. Peerla", "Frank Barrows", "Francesco Caravelli", "Bibhu Datta Sahoo"], "summary": "Matrix computations have become increasingly significant in many data-driven\napplications. However, Moores law for digital computers has been gradually\napproaching its limit in recent years. Moreover, digital computers encounter\nsubstantial complexity when performing matrix computations and need a long time\nto finish the computations, and existing analog matrix computation schemes\nrequire a large chip area and power consumption. This paper proposes a linear\nalgebra system of equations based on integrators, which features low power\nconsumption, compact area, and fast computation time. Due to the simple\nstructure of the ring oscillator, the ring oscillator-based integrator exhibits\na compact area and low power consumption. Therefore, ring oscillator-based\nintegrators are introduced into the linear algebra system of equations, and\nthis system can be used to compute the linear algebra equations of the matrix\nwith either positive or negative values. This paper provides a detailed\nanalysis and verification of the proposed circuit structure. Compared to\nsimilar circuits, this work has significant advantages in terms of area, power\nconsumption, and computation speed.", "comment": "11 pages, 22 figures, Journal", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09392v1"}
{"id": "2506.09647", "title": "Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach", "authors": ["Lei Deng", "Wenhan Xu", "Jingwei Li", "Danny H. K. Tsang"], "summary": "Real-time network traffic forecasting is crucial for network management and\nearly resource allocation. Existing network traffic forecasting approaches\noperate under the assumption that the network traffic data is fully observed.\nHowever, in practical scenarios, the collected data are often incomplete due to\nvarious human and natural factors. In this paper, we propose a generative model\napproach for real-time network traffic forecasting with missing data. Firstly,\nwe model the network traffic forecasting task as a tensor completion problem.\nSecondly, we incorporate a pre-trained generative model to achieve the low-rank\nstructure commonly associated with tensor completion. The generative model\neffectively captures the intrinsic low-rank structure of network traffic data\nduring pre-training and enables the mapping from a compact latent\nrepresentation to the tensor space. Thirdly, rather than directly optimizing\nthe high-dimensional tensor, we optimize its latent representation, which\nsimplifies the optimization process and enables real-time forecasting. We also\nestablish a theoretical recovery guarantee that quantifies the error bound of\nthe proposed approach. Experiments on real-world datasets demonstrate that our\napproach achieves accurate network traffic forecasting within 100 ms, with a\nmean absolute error (MAE) below 0.002, as validated on the Abilene dataset.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09647v1"}
{"id": "2506.09464", "title": "Efficient Modular Multiplier over GF (2^m) for ECPM", "authors": ["Ruby Kumari", "Gaurav Purohit", "Abhijit Karmakar"], "summary": "Elliptic curve cryptography (ECC) has emerged as the dominant public-key\nprotocol, with NIST standardizing parameters for binary field GF(2^m) ECC\nsystems. This work presents a hardware implementation of a Hybrid\nMultiplication technique for modular multiplication over binary field GF(2m),\ntargeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the\ncombination of conventional multiplication (CM) and Karatsuba multiplication\n(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation\nuses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,\nreducing computational complexity and enhancing efficiency. The design is\nevaluated in three areas: Resource Utilization For m=163, the hybrid design\nuses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For\nm=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and\nbit-parallel implementations. Delay Performance For m=163, achieves 13.31ns\ndelay, improving by 37.60% over bit-parallel implementations. For m=233,\nmaintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,\noutperforming bit-parallel (75,337) and digit-serial (43,179) implementations.\nFor m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over\nbit-parallel designs. Results show the hybrid technique significantly improves\nspeed, hardware efficiency, and resource utilization for ECC cryptographic\nsystems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09464v1"}
{"id": "2506.09549", "title": "A Study on Speech Assessment with Visual Cues", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Nasir Saleem", "Amir Hussain", "Hsin-Min Wang", "Yu Tsao"], "summary": "Non-intrusive assessment of speech quality and intelligibility is essential\nwhen clean reference signals are unavailable. In this work, we propose a\nmultimodal framework that integrates audio features and visual cues to predict\nPESQ and STOI scores. It employs a dual-branch architecture, where spectral\nfeatures are extracted using STFT, and visual embeddings are obtained via a\nvisual encoder. These features are then fused and processed by a CNN-BLSTM with\nattention, followed by multi-task learning to simultaneously predict PESQ and\nSTOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND\ncorpus, show that our model outperforms the audio-only baseline. Under seen\nnoise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%\n(0.7403->0.8253) for STOI. These results highlight the effectiveness of\nincorporating visual cues in enhancing the accuracy of non-intrusive speech\nassessment.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09549v1"}
{"id": "2506.09275", "title": "A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO", "authors": ["Jonas Svedas", "Hannah Watson", "Nathan Laubeuf", "Diksha Moolchandani", "Abubakr Nada", "Arjun Singh", "Dwaipayan Biswas", "James Myers", "Debjyoti Bhattacharjee"], "summary": "Distributed deep neural networks (DNNs) have become a cornerstone for scaling\nmachine learning to meet the demands of increasingly complex applications.\nHowever, the rapid growth in model complexity far outpaces CMOS technology\nscaling, making sustainable and efficient system design a critical challenge.\nAddressing this requires coordinated co-design across software, hardware, and\ntechnology layers. Due to the prohibitive cost and complexity of deploying\nfull-scale training systems, simulators play a pivotal role in enabling this\ndesign exploration. This survey reviews the landscape of distributed DNN\ntraining simulators, focusing on three major dimensions: workload\nrepresentation, simulation infrastructure, and models for total cost of\nownership (TCO) including carbon emissions. It covers how workloads are\nabstracted and used in simulation, outlines common workload representation\nmethods, and includes comprehensive comparison tables covering both simulation\nframeworks and TCO/emissions models, detailing their capabilities, assumptions,\nand areas of focus. In addition to synthesizing existing tools, the survey\nhighlights emerging trends, common limitations, and open research challenges\nacross the stack. By providing a structured overview, this work supports\ninformed decision-making in the design and evaluation of distributed training\nsystems.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09275v1"}
{"id": "2506.09186", "title": "Not all those who drift are lost: Drift correction and calibration scheduling for the IoT", "authors": ["Aaron Hurst", "Andrey V. Kalinichev", "Klaus Koren", "Daniel E. Lucani"], "summary": "Sensors provide a vital source of data that link digital systems with the\nphysical world. However, as sensors age, the relationship between what they\nmeasure and what they output changes. This is known as sensor drift and poses a\nsignificant challenge that, combined with limited opportunity for\nre-calibration, can severely limit data quality over time. Previous approaches\nto drift correction typically require large volumes of ground truth data and do\nnot consider measurement or prediction uncertainty. In this paper, we propose a\nprobabilistic sensor drift correction method that takes a fundamental approach\nto modelling the sensor response using Gaussian Process Regression. Tested\nusing dissolved oxygen sensors, our method delivers mean squared error (MSE)\nreductions of up to 90% and more than 20% on average. We also propose a novel\nuncertainty-driven calibration schedule optimisation approach that builds on\ntop of drift correction and further reduces MSE by up to 15.7%.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09186v1"}
{"id": "2506.09100", "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09100v1"}
{"id": "2506.09107", "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines", "authors": ["Athena Vakali", "Ilias Dimitriadis"], "summary": "AI models have become active decision makers, often acting without human\nsupervision. The rapid advancement of AI technology has already caused harmful\nincidents that have hurt individuals and societies and AI unfairness in heavily\ncriticized. It is urgent to disrupt AI pipelines which largely neglect human\nprinciples and focus on computational biases exploration at the data (pre),\nmodel(in), and deployment (post) processing stages. We claim that by exploiting\nthe advances of agents technology, we will introduce cautious, prompt, and\nongoing fairness watch schemes, under realistic, systematic, and human-centric\nfairness expectations. We envision agents as fairness guardians, since agents\nlearn from their environment, adapt to new information, and solve complex\nproblems by interacting with external tools and other systems. To set the\nproper fairness guardrails in the overall AI pipeline, we introduce a\nfairness-by-design approach which embeds multi-role agents in an end-to-end\n(human to AI) synergetic scheme. Our position is that we may design adaptive\nand realistic AI fairness frameworks, and we introduce a generalized algorithm\nwhich can be customized to the requirements and goals of each AI decision\nmaking scenario. Our proposed, so called FAIRTOPIA framework, is structured\nover a three-layered architecture, which encapsulates the AI pipeline inside an\nagentic guardian and a knowledge-based, self-refining layered scheme. Based on\nour proposition, we enact fairness watch in all of the AI pipeline stages,\nunder robust multi-agent workflows, which will inspire new fairness research\nhypothesis, heuristics, and methods grounded in human-centric, systematic,\ninterdisciplinary, socio-technical principles.", "comment": "11 pages, 4 figures", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09107v1"}
{"id": "2506.09959", "title": "Almost-Optimal Local-Search Methods for Sparse Tensor PCA", "authors": ["Max Lovig", "Conor Sheehan", "Konstantinos Tsirkas", "Ilias Zadik"], "summary": "Local-search methods are widely employed in statistical applications, yet\ninterestingly, their theoretical foundations remain rather underexplored,\ncompared to other classes of estimators such as low-degree polynomials and\nspectral methods. Of note, among the few existing results recent studies have\nrevealed a significant \"local-computational\" gap in the context of a\nwell-studied sparse tensor principal component analysis (PCA), where a broad\nclass of local Markov chain methods exhibits a notable underperformance\nrelative to other polynomial-time algorithms. In this work, we propose a series\nof local-search methods that provably \"close\" this gap to the best known\npolynomial-time procedures in multiple regimes of the model, including and\ngoing beyond the previously studied regimes in which the broad family of local\nMarkov chain methods underperforms. Our framework includes: (1) standard greedy\nand randomized greedy algorithms applied to the (regularized) posterior of the\nmodel; and (2) novel random-threshold variants, in which the randomized greedy\nalgorithm accepts a proposed transition if and only if the corresponding change\nin the Hamiltonian exceeds a random Gaussian threshold-rather that if and only\nif it is positive, as is customary. The introduction of the random thresholds\nenables a tight mathematical analysis of the randomized greedy algorithm's\ntrajectory by crucially breaking the dependencies between the iterations, and\ncould be of independent interest to the community.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.09959v1"}
{"id": "2506.09075", "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09075v1"}
{"id": "2506.09204", "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.09204v1"}
{"id": "2506.09529", "title": "Gradient-Weighted, Data-Driven Normalization for Approximate Border Bases -- Concept and Computation", "authors": ["Hiroshi Kera", "Achim Kehrein"], "summary": "This paper studies the concept and the computation of approximately vanishing\nideals of a finite set of data points. By data points, we mean that the points\ncontain some uncertainty, which is a key motivation for the approximate\ntreatment. A careful review of the existing border basis concept for an exact\ntreatment motivates a new adaptation of the border basis concept for an\napproximate treatment. In the study of approximately vanishing polynomials, the\nnormalization of polynomials plays a vital role. So far, the most common\nnormalization in computational commutative algebra uses the coefficient norm of\na polynomial. Inspired by recent developments in machine learning, the present\npaper proposes and studies the use of gradient-weighted normalization. The\ngradient-weighted semi-norm evaluates the gradient of a polynomial at the data\npoints. This data-driven nature of gradient-weighted normalization produces, on\nthe one hand, better stability against perturbation and, on the other hand,\nvery significantly, invariance of border bases with respect to scaling the data\npoints. Neither property is achieved with coefficient normalization. In\nparticular, we present an example of the lack of scaling invariance with\nrespect to coefficient normalization, which can cause an approximate border\nbasis computation to fail. This is extremely relevant because scaling of the\npoint set is often recommended for preprocessing the data. Further, we use an\nexisting algorithm with coefficient normalization to show that it is easily\nadapted to gradient-weighted normalization. The analysis of the adapted\nalgorithm only requires tiny changes, and the time complexity remains the same.\nFinally, we present numerical experiments on three affine varieties to\ndemonstrate the superior stability of our data-driven normalization over\ncoefficient normalization. We obtain robustness to perturbations and invariance\nto scaling.", "comment": "39 pages, 3 figures, 2 tables. Extended version of \"Border basis\n  computation with gradient-weighted normalization\" from ISSAC'22", "cate": "cs.SC", "url": "http://arxiv.org/abs/2506.09529v1"}
{"id": "2506.09502", "title": "The Secure Overview and Analysis OF 3GPP MAC CE", "authors": ["Jin Cao", "Yuanyuan Yang", "Ruhui Ma", "Sheng Li", "Hui Li"], "summary": "To more effectively control and allocate network resources, MAC CE has been\nintroduced into the network protocol, which is a type of control signaling\nlocated in the MAC layer. Since MAC CE lacks encryption and integrity\nprotection mechanisms provided by PDCP, the control signaling carried by MAC CE\nis vulnerable to interception or tampering by attackers during resource\nscheduling and allocation. Currently, the 3GPP has analyzed the security risks\nof Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to\nthe UE via MAC CE by the network can lead to privacy leaks and network attacks.\nHowever, in addition to LTM, there may be other potential security\nvulnerabilities in other protocol procedures. Therefore, this paper explores\nthe security threats to MAC CE and the corresponding protection mechanisms. The\nresearch is expected to support the 3GPP's study of MAC CE and be integrated\nwith the security research of lower-layer protocols, thereby enhancing the\nsecurity and reliability of the entire communication system.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09502v1"}
{"id": "2506.09758", "title": "Mainframe-style channel controllers for modern disaggregated memory systems", "authors": ["Zikai Liu", "Jasmin Schult", "Pengcheng Xu", "Timothy Roscoe"], "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09758v1"}
{"id": "2506.09366", "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09366v1"}
{"id": "2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09081v1"}
{"id": "2506.09498", "title": "Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Yoshua Bengio", "Sungjin Ahn"], "summary": "Diffusion models have recently emerged as a powerful approach for trajectory\nplanning. However, their inherently non-sequential nature limits their\neffectiveness in long-horizon reasoning tasks at test time. The recently\nproposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by\ncombining diffusion with tree-based search, achieving state-of-the-art\nperformance on complex planning problems. Despite its strengths, our analysis\nshows that MCTD incurs substantial computational overhead due to the sequential\nnature of tree search and the cost of iterative denoising. To address this, we\npropose Fast-MCTD, a more efficient variant that preserves the strengths of\nMCTD while significantly improving its speed and scalability. Fast-MCTD\nintegrates two techniques: Parallel MCTD, which enables parallel rollouts via\ndelayed tree updates and redundancy-aware selection; and Sparse MCTD, which\nreduces rollout length through trajectory coarsening. Experiments show that\nFast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or\nimproving planning performance. Remarkably, it even outperforms Diffuser in\ninference speed on some tasks, despite Diffuser requiring no search and\nyielding weaker solutions. These results position Fast-MCTD as a practical and\nscalable solution for diffusion-based inference-time reasoning.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09498v1"}
{"id": "2506.09090", "title": "Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications", "authors": ["Arthur Oghlukyan", "Nuria Gomez Blas"], "summary": "This paper presents a comprehensive analysis of an enhanced asynchronous\nAdaBoost framework for federated learning (FL), focusing on its application\nacross five distinct domains: computer vision on edge devices, blockchain-based\nmodel transparency, on-device mobile personalization, IoT anomaly detection,\nand federated healthcare diagnostics. The proposed algorithm incorporates\nadaptive communication scheduling and delayed weight compensation to reduce\nsynchronization frequency and communication overhead while preserving or\nimproving model accuracy. We examine how these innovations improve\ncommunication efficiency, scalability, convergence, and robustness in each\ndomain. Comparative metrics including training time, communication overhead,\nconvergence iterations, and classification accuracy are evaluated using data\nand estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical\nresults show, for example, training time reductions on the order of 20-35% and\ncommunication overhead reductions of 30-40% compared to baseline AdaBoost, with\nconvergence achieved in significantly fewer boosting rounds. Tables and charts\nsummarize these improvements by domain. Mathematical formulations of the\nadaptive scheduling rule and error-driven synchronization thresholds are\nprovided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency\nand robustness across diverse FL scenarios, suggesting broad applicability of\nthe approach.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09090v1"}
{"id": "2506.09091", "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "comment": "11 pages, 2 figures, AGI-25", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09091v1"}
{"id": "2506.09601", "title": "ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs", "authors": ["Sota Nakashima", "Yuta Ishimoto", "Masanari Kondo", "Tao Xiao", "Yasutaka Kamei"], "summary": "Technical debt refers to suboptimal code that degrades software quality. When\ndevelopers intentionally introduce such debt, it is called self-admitted\ntechnical debt (SATD). Since SATD hinders maintenance, identifying its\ncategories is key to uncovering quality issues. Traditionally, constructing\nsuch taxonomies requires manually inspecting SATD comments and surrounding\ncode, which is time-consuming, labor-intensive, and often inconsistent due to\nannotator subjectivity. This study presents ASTAGEN, an initial step toward\nautomating SATD taxonomy generation using large language models (LLMs). Given a\ncomment and its surrounding code, ASTAGEN first generates a concise explanation\nfor each SATD comment, then incrementally generates and updates categories to\nconstruct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:\nquantum software, smart contracts, and machine learning. It successfully\nrecovers domain-specific categories reported in prior work, such as Layer\nConfiguration in machine learning. Compared to a naive use of an LLM, ASTAGEN\nproduces more consistent category assignments due to its explanation-driven,\niterative design. It also completes taxonomy generation in under two hours and\nfor less than one USD, even on the largest dataset. These results suggest that\nwhile full automation remains challenging, ASTAGEN is able to support\nsemi-automated taxonomy construction. Furthermore, our work opens up avenues\nfor future work, such as automatic taxonomy generation in other areas.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09601v1"}
{"id": "2506.09236", "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review", "authors": ["Erin Argo", "Tanim Ahmed", "Sarah Gable", "Callie Hampton", "Jeronimo Grandi", "Regis Kopper"], "summary": "During the past decade, there has been a significant increase in research\nfocused on integrating AR User Interfaces into public safety applications,\nparticularly for first responders in the domains of Emergency Medical Services,\nFirefighting, and Law Enforcement. This paper presents the results of a scoping\nreview involving the application of AR user interfaces in the public safety\ndomain and applies an established systematic review methodology to provide a\ncomprehensive analysis of the current research landscape, identifying key\ntrends, challenges, and gaps in the literature. This review includes\npeer-reviewed publications indexed by the major scientific databases up to\nApril 2025. A basic keyword search retrieved 1,751 papers, of which 90 were\ndeemed relevant for this review. An in-depth analysis of the literature allowed\nthe development of a faceted taxonomy that categorizes AR user interfaces for\npublic safety. This classification lays a solid foundation for future research,\nwhile also highlighting key design considerations, challenges, and gaps in the\nliterature. This review serves as a valuable resource for researchers and\ndevelopers, offering insights that can drive further advances in the field.", "comment": "19 pages, 4 figures, 8 tables", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09236v1"}
{"id": "2506.09891", "title": "Causal Climate Emulation with Bayesian Filtering", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "comment": "32 pages, 21 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09891v1"}
{"id": "2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09420v1"}
{"id": "2506.09410", "title": "Large-scale LH2 pipeline infrastructure concept for airports", "authors": ["H. A. Krog", "Y. Jooss", "H. Fyhn", "P. Nekså", "I. Hjorth"], "summary": "Infrastructure and processes for handling of liquid hydrogen (LH2) is needed\nto enable large-scale decarbonization of aviation with hydrogen aircraft. At\nlarge airports, pipeline and hydrant systems will be important for a mature\nhydrogen-powered air travel market. As the vaporization of LH2 is a challenge\nin fuel handling, the pipeline infrastructure must be designed and operated\nsuch that the fuel is subcooled. Through modelling and simulation of aircraft\ntanks refuelling by a pipeline infrastructure concept, it is found that\ncontinuous recycling of LH2 within the system is needed to maintain subcooling,\nand the pump operation is important for preventing flashing. With the proposed\nconcept, some hydrogen vapor is formed in the aircraft tank, but the vapor can\nbe utilised by hydrogen-powered ground support equipment.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09410v1"}
{"id": "2506.09703", "title": "Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks", "authors": ["Huan Lin", "Chenguang Zhu", "Lianghui Ding", "Feng Yang"], "summary": "Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to\naddress communication network split issues and restore connectivity. However,\nexisting graph learning-based resilient algorithms face over-aggregation and\nnon-convergence problems caused by uneven and sparse topology under massive\ndamage scenarios. To alleviate these problems, we propose a novel Multi-Level\nDamage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery\ntrajectories by mining information from destroyed UAVs. We first introduce a\nMulti-Branch Damage Attention (MBDA) module, which forms a sequence of\nmulti-hop Damage Attentive Graphs (mDAG) with different ranges of receptive\nfields. Each mDAG links only remaining and damaged nodes to ensure a more even\ndegree distribution for mitigating over-aggregation, and utilizes multi-hop\ndilation to establish more links for sparse topology enhancement. To resort to\nthe mDAG, we propose a Dilated Graph Convolution Network (DGCN), which\ngenerates the optimal recovery trajectories with theoretically proven\nconvergence under massive damage cases. Simulation results show that the\nproposed algorithm can guarantee the connectivity restoration under large swarm\nand damage scales, while significantly expediting the recovery time by 75.94%\nand improving the topology uniformity after recovery.", "comment": "15 pages. arXiv admin note: text overlap with arXiv:2411.11342", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09703v1"}
{"id": "2506.09758", "title": "Mainframe-style channel controllers for modern disaggregated memory systems", "authors": ["Zikai Liu", "Jasmin Schult", "Pengcheng Xu", "Timothy Roscoe"], "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09758v1"}
{"id": "2506.09606", "title": "Unmasking real-world audio deepfakes: A data-centric approach", "authors": ["David Combei", "Adriana Stan", "Dan Oneata", "Nicolas Müller", "Horia Cucu"], "summary": "The growing prevalence of real-world deepfakes presents a critical challenge\nfor existing detection systems, which are often evaluated on datasets collected\njust for scientific purposes. To address this gap, we introduce a novel dataset\nof real-world audio deepfakes. Our analysis reveals that these real-world\nexamples pose significant challenges, even for the most performant detection\nmodels. Rather than increasing model complexity or exhaustively search for a\nbetter alternative, in this work we focus on a data-centric paradigm, employing\nstrategies like dataset curation, pruning, and augmentation to improve model\nrobustness and generalization.\n  Through these methods, we achieve a 55% relative reduction in EER on the\nIn-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on\nour newly proposed real-world deepfakes dataset, AI4T. These results highlight\nthe transformative potential of data-centric approaches in enhancing deepfake\ndetection for real-world applications. Code and data available at:\nhttps://github.com/davidcombei/AI4T.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09606v1"}
{"id": "2506.09280", "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09280v1"}
{"id": "2506.09194", "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks", "authors": ["Emirhan Bilgiç", "Neslihan Serap Şengör", "Namık Berk Yalabık", "Yavuz Selim İşler", "Aykut Görkem Gelen", "Rahmi Elibol"], "summary": "This study examines the integration of Contrastive Predictive Coding (CPC)\nwith Spiking Neural Networks (SNN). While CPC learns the predictive structure\nof data to generate meaningful representations, SNN mimics the computational\nprocesses of biological neural systems over time. In this study, the goal is to\ndevelop a predictive coding model with greater biological plausibility by\nprocessing inputs and outputs in a spike-based system. The proposed model was\ntested on the MNIST dataset and achieved a high classification rate in\ndistinguishing positive sequential samples from non-sequential negative\nsamples. The study demonstrates that CPC can be effectively combined with SNN,\nshowing that an SNN trained for classification tasks can also function as an\nencoding mechanism. Project codes and detailed results can be accessed on our\nGitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal\n  Processing and Communications Applications Conference (SIU)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09194v1"}
{"id": "2506.09161", "title": "An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation", "authors": ["Rajan Das Gupta", "Md Imrul Hasan Showmick", "Mushfiqur Rahman Abir", "Shanjida Akter", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Early and accurate detection of brain abnormalities, such as tumors and\nstrokes, is essential for timely intervention and improved patient outcomes. In\nthis study, we present a deep learning-based system capable of identifying both\nbrain tumors and strokes from MRI images, along with their respective stages.\nWe have executed two groundbreaking strategies involving convolutional neural\nnetworks, MobileNet V2 and ResNet-50-optimized through transfer learning to\nclassify MRI scans into five diagnostic categories. Our dataset, aggregated and\naugmented from various publicly available MRI sources, was carefully curated to\nensure class balance and image diversity. To enhance model generalization and\nprevent overfitting, we applied dropout layers and extensive data augmentation.\nThe models achieved strong performance, with training accuracy reaching 93\\%\nand validation accuracy up to 88\\%. While ResNet-50 demonstrated slightly\nbetter results, Mobile Net V2 remains a promising option for real-time\ndiagnosis in low resource settings due to its lightweight architecture. This\nresearch offers a practical AI-driven solution for early brain abnormality\ndetection, with potential for clinical deployment and future enhancement\nthrough larger datasets and multi modal inputs.", "comment": "Accepted in MECON 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09161v1"}
{"id": "2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09160v1"}
{"id": "2506.09665", "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09665v1"}
{"id": "2506.09599", "title": "Energy Aware Development of Neuromorphic Implantables: From Metrics to Action", "authors": ["Enrique Barba Roque", "Luis Cruz"], "summary": "Spiking Neural Networks (SNNs) and neuromorphic computing present a promising\nalternative to traditional Artificial Neural Networks (ANNs) by significantly\nimproving energy efficiency, particularly in edge and implantable devices.\nHowever, assessing the energy performance of SNN models remains a challenge due\nto the lack of standardized and actionable metrics and the difficulty of\nmeasuring energy consumption in experimental neuromorphic hardware. In this\npaper, we conduct a preliminary exploratory study of energy efficiency metrics\nproposed in the SNN benchmarking literature. We classify 13 commonly used\nmetrics based on four key properties: Accessibility, Fidelity, Actionability,\nand Trend-Based analysis. Our findings indicate that while many existing\nmetrics provide useful comparisons between architectures, they often lack\npractical insights for SNN developers. Notably, we identify a gap between\naccessible and high-fidelity metrics, limiting early-stage energy assessment.\nAdditionally, we emphasize the lack of metrics that provide practitioners with\nactionable insights, making it difficult to guide energy-efficient SNN\ndevelopment. To address these challenges, we outline research directions for\nbridging accessibility and fidelity and finding new Actionable metrics for\nimplantable neuromorphic devices, introducing more Trend-Based metrics, metrics\nthat reflect changes in power requirements, battery-aware metrics, and\nimproving energy-performance tradeoff assessments. The results from this paper\npave the way for future research on enhancing energy metrics and their\nActionability for SNNs.", "comment": "ICT45 2025 submission", "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.09599v1"}
{"id": "2506.09950", "title": "Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher", "authors": ["La Scala Roberto", "Sharwan Kumar Tiwari"], "summary": "The multistep solving strategy consists in a divide-and-conquer approach:\nwhen a multivariate polynomial system is computationally infeasible to solve\ndirectly, one variable is assigned over the elements of the base finite field,\nand the procedure is recursively applied to the resulting simplified systems.\nIn a previous work by the same authors (among others), this approach proved\neffective in the algebraic cryptanalysis of the Trivium cipher. In this paper,\nwe present a new implementation of the corresponding algorithm based on a\nDepth-First Search strategy, along with a novel complexity analysis leveraging\ntree structures. We further introduce the notion of an \"oracle function\" as a\ngeneral predictive tool for deciding whether the evaluation of a new variable\nis necessary to simplify the current polynomial system. This notion allows us\nto unify all previously proposed variants of the multistep strategy, including\nthe classical hybrid approach, by appropriately selecting the oracle function.\nFinally, we apply the multistep solving strategy to the cryptanalysis of the\nlow-latency block cipher Aradi, recently introduced by the NSA. We present the\nfirst full round algebraic attack, raising concerns about the cipher's actual\nsecurity with respect to its key length.", "comment": "19 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09950v1"}
{"id": "2506.09209", "title": "Revisiting Graph Projections for Effective Complementary Product Recommendation", "authors": ["Leandro Anghinoni", "Pablo Zivic", "Jorge Adrian Sanchez"], "summary": "Complementary product recommendation is a powerful strategy to improve\ncustomer experience and retail sales. However, recommending the right product\nis not a simple task because of the noisy and sparse nature of user-item\ninteractions. In this work, we propose a simple yet effective method to predict\na list of complementary products given a query item, based on the structure of\na directed weighted graph projected from the user-item bipartite graph. We\nrevisit bipartite graph projections for recommender systems and propose a novel\napproach for inferring complementarity relationships from historical user-item\ninteractions. We compare our model with recent methods from the literature and\nshow, despite the simplicity of our approach, an average improvement of +43%\nand +38% over sequential and graph-based recommenders, respectively, over\ndifferent benchmarks.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09209v1"}
{"id": "2506.09211", "title": "An Introduction to Solving the Least-Squares Problem in Variational Data Assimilation", "authors": ["I. Daužickaitė", "M. A. Freitag", "S. Gürol", "A. S. Lawless", "A. Ramage", "J. A. Scott", "J. M. Tabeart"], "summary": "Variational data assimilation is a technique for combining measured data with\ndynamical models. It is a key component of Earth system state estimation and is\ncommonly used in weather and ocean forecasting. The approach involves a\nlarge-scale generalized nonlinear least-squares problem. Solving the resulting\nsequence of sparse linear subproblems requires the use of sophisticated\nnumerical linear algebra methods. In practical applications, the computational\ndemands severely limit the number of iterations of a Krylov subspace solver\nthat can be performed and so high-quality preconditioners are vital. In this\npaper, we introduce variational data assimilation from a numerical linear\nalgebra perspective and review current solution techniques, with a focus on the\nchallenges that arise in large-scale geophysical systems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09211v1"}
{"id": "2506.09189", "title": "Fractional Fourier Sound Synthesis", "authors": ["Esteban Gutiérrez", "Rodrigo Cádiz", "Carlos Sing Long", "Frederic Font", "Xavier Serra"], "summary": "This paper explores the innovative application of the Fractional Fourier\nTransform (FrFT) in sound synthesis, highlighting its potential to redefine\ntime-frequency analysis in audio processing. As an extension of the classical\nFourier Transform, the FrFT introduces fractional order parameters, enabling a\ncontinuous interpolation between time and frequency domains and unlocking\nunprecedented flexibility in signal manipulation. Crucially, the FrFT also\nopens the possibility of directly synthesizing sounds in the alpha-domain,\nproviding a unique framework for creating timbral and dynamic characteristics\nunattainable through conventional methods. This work delves into the\nmathematical principles of the FrFT, its historical evolution, and its\ncapabilities for synthesizing complex audio textures. Through experimental\nanalyses, we showcase novel sound design techniques, such as alpha-synthesis\nand alpha-filtering, which leverage the FrFT's time-frequency rotation\nproperties to produce innovative sonic results. The findings affirm the FrFT's\nvalue as a transformative tool for composers, sound designers, and researchers\nseeking to push the boundaries of auditory creativity.", "comment": "Accepted to the International Computer Music Conference (ICMC) 2025\n  held in Boston, USA. 6 pages and 2 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09189v1"}
{"id": "2506.09525", "title": "Beyond Personalization: Federated Recommendation with Calibration via Low-rank Decomposition", "authors": ["Jundong Chen", "Honglei Zhang", "Haoxuan Li", "Chunxu Zhang", "Zhiwei Li", "Yidong Li"], "summary": "Federated recommendation (FR) is a promising paradigm to protect user privacy\nin recommender systems. Distinct from general federated scenarios, FR\ninherently needs to preserve client-specific parameters, i.e., user embeddings,\nfor privacy and personalization. However, we empirically find that globally\naggregated item embeddings can induce skew in user embeddings, resulting in\nsuboptimal performance. To this end, we theoretically analyze the user\nembedding skew issue and propose Personalized Federated recommendation with\nCalibration via Low-Rank decomposition (PFedCLR). Specifically, PFedCLR\nintroduces an integrated dual-function mechanism, implemented with a buffer\nmatrix, to jointly calibrate local user embedding and personalize global item\nembeddings. To ensure efficiency, we employ a low-rank decomposition of the\nbuffer matrix to reduce the model overhead. Furthermore, for privacy, we train\nand upload the local model before personalization, preventing the server from\naccessing sensitive information. Extensive experiments demonstrate that PFedCLR\neffectively mitigates user embedding skew and achieves a desirable trade-off\namong performance, efficiency, and privacy, outperforming state-of-the-art\n(SOTA) methods.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09525v1"}
{"id": "2506.09383", "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09383v1"}
{"id": "2506.09082", "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "comment": "First two authors contribute equally", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09082v1"}
{"id": "2506.09655", "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09655v1"}
{"id": "2506.09091", "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "comment": "11 pages, 2 figures, AGI-25", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09091v1"}
{"id": "2506.09452", "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "comment": "Submitted to IEEE S&P 2026", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09452v1"}
{"id": "2506.09636", "title": "Translating a VDM Model of a Medical Device into Kapture", "authors": ["Joe Hare", "Leo Freitas", "Ken Pierce"], "summary": "As the complexity of safety-critical medical devices increases, so does the\nneed for clear, verifiable, software requirements. This paper explores the use\nof Kapture, a formal modelling tool developed by D-RisQ, to translate an\nexisting formal VDM model of a medical implant for treating focal epilepsy\ncalled CANDO. The work was undertaken without prior experience in formal\nmethods. The paper assess Kapture's usability, the challenges of formal\nmodelling, and the effectiveness of the translated model. The result is a model\nin Kapture which covers over 90% of the original VDM model, and produces\nmatching traces of results. While several issues were encountered during design\nand implementation, mainly due to the initial learning curve, this paper\ndemonstrates that complex systems can be effectively modelled in Kapture by\ninexperienced users and highlights some difficulties in translating VDM\nspecifications to Kapture.", "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09636v1"}
{"id": "2506.09292", "title": "AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education", "authors": ["Brooklyn J. Corbett", "Jason M. Tangen"], "summary": "Misconceptions in psychology and education persist despite clear\ncontradictory evidence, resisting traditional correction methods. This study\ninvestigated whether personalised AI dialogue could effectively correct these\nstubborn beliefs. In a preregistered experiment (N = 375), participants holding\nstrong psychology misconceptions engaged in one of three interventions: (1)\npersonalised AI dialogue targeting their specific misconception, (2) generic\ntextbook-style refutation, or (3) neutral AI dialogue (control). Results showed\nthat personalised AI dialogue produced significantly larger immediate belief\nreductions compared to both textbook reading and neutral dialogue. This\nadvantage persisted at 10-day follow-up but diminished by 2 months, where AI\ndialogue and textbook conditions converged while both remained superior to\ncontrol. Both AI conditions generated significantly higher engagement and\nconfidence than textbook reading, demonstrating the motivational benefits of\nconversational interaction. These findings demonstrate that AI dialogue can\naccelerate initial belief correction through personalised, interactive\nengagement that disrupts the cognitive processes maintaining misconceptions.\nHowever, the convergence of effects over time suggests brief interventions\nrequire reinforcement for lasting change. Future applications should integrate\nAI tutoring into structured educational programs with spaced reinforcement to\nsustain the initial advantages of personalised dialogue.", "comment": "Originally posted as https://doi.org/10.31234/osf.io/x4wqh_v1", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09292v1"}
{"id": "2506.09913", "title": "A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals", "authors": ["Brian N. Granzow", "Stephen D. Bond", "D. Thomas Seidl", "Bernhard Endtmayer"], "summary": "We consider estimating the discretization error in a nonlinear functional\n$J(u)$ in the setting of an abstract variational problem: find $u \\in\n\\mathcal{V}$ such that $B(u,\\varphi) = L(\\varphi) \\; \\forall \\varphi \\in\n\\mathcal{V}$, as approximated by a Galerkin finite element method. Here,\n$\\mathcal{V}$ is a Hilbert space, $B(\\cdot,\\cdot)$ is a bilinear form, and\n$L(\\cdot)$ is a linear functional. We consider well-known error estimates\n$\\eta$ of the form $J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$, where $u_h$\ndenotes a finite element approximation to $u$, and $z$ denotes the solution to\nan auxiliary adjoint variational problem. We show that there exist nonlinear\nfunctionals for which error estimates of this form are not reliable, even in\nthe presence of an exact adjoint solution solution $z$. An estimate $\\eta$ is\nsaid to be reliable if there exists a constant $C \\in \\mathbb{R}_{>0}$\nindependent of $u_h$ such that $|J(u) - J(u_h)| \\leq C|\\eta|$. We present\nseveral example pairs of bilinear forms and nonlinear functionals where\nreliability of $\\eta$ is not achieved.", "comment": "6 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09913v1"}
{"id": "2506.09513", "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "comment": "24 pages, 6 figures, 7 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09513v1"}
{"id": "2506.09447", "title": "Optimization and Control Technologies for Renewable-Dominated Hydrogen-Blended Integrated Gas-Electricity System: A Review", "authors": ["Wenxin Liu", "Jiakun Fang", "Shichang Cui", "Iskandar Abdullaev", "Suyang Zhou", "Xiaomeng Ai", "Jinyu Wen"], "summary": "The growing coupling among electricity, gas, and hydrogen systems is driven\nby green hydrogen blending into existing natural gas pipelines, paving the way\ntoward a renewable-dominated energy future. However, the integration poses\nsignificant challenges, particularly ensuring efficient and safe operation\nunder varying hydrogen penetration and infrastructure adaptability. This paper\nreviews progress in optimization and control technologies for hydrogen-blended\nintegrated gas-electricity system. First, key technologies and international\ndemonstration projects are introduced to provide an overview of current\ndevelopments. Besides, advances in gas-electricity system integration,\nincluding modeling, scheduling, planning and market design, are reviewed\nrespectively. Then, the potential for cross-system fault propagation is\nhighlighted, and practical methods for safety analysis and control are\nproposed. Finally, several possible research directions are introduced, aiming\nto ensure efficient renewable integration and reliable operation.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09447v1"}
{"id": "2506.09878", "title": "Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks", "authors": ["Ryan Barker"], "summary": "Virtualising the Radio Access Network (RAN) is widely touted as the\ncorner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current\ndiscourse often isolates spectrum policy, cloud engineering and organisational\nreadiness into silos. This paper delivers an integrated analysis that spans\nscience, technology, business strategy and culture. I first review\nspectrum-auction economics and show-via a comparative study of T-Mobile US and\nVerizon-that mid-band contiguity leveraged through software-defined carrier\naggregation outperforms mmWave-centric deployments in both coverage and churn\nmetrics. I then formalise the technical foundations of virtualised and open\nRAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths\nand quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute\nplatforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent\nControllers are examined alongside AI ML pipelines that enable\ndigital-twin-driven optimisation. A security cost model extends recent O-RAN\nmeasurements to show how 256-bit cipher enforcement adds 35-60 us latency\nunless mitigated by inline crypto off-load. Finally, a national automation case\nstudy of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction\nonce cultural change errors are corrected. I conclude with open research\nchallenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust\norchestration, offering actionable recommendations for operators, vendors and\nresearchers.", "comment": "12 pages, 4 figures, 8 tables", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09878v1"}
{"id": "2506.09825", "title": "On the Impossibility of a Perfect Hypervisor", "authors": ["Mordechai Guri"], "summary": "We establish a fundamental impossibility result for a `perfect hypervisor',\none that (1) preserves every observable behavior of any program exactly as on\nbare metal and (2) adds zero timing or resource overhead.\n  Within this model we prove two theorems. (1) Indetectability Theorem. If such\na hypervisor existed, no guest-level program, measurement, or timing test could\ndistinguish it from native execution; all traces, outputs, and timings would be\nidentical.\n  (2) Impossibility Theorem. Despite that theoretical indetectability, a\nperfect hypervisor cannot exist on any machine with finite computational\nresources.\n  These results are architecture-agnostic and extend beyond hypervisors to any\nvirtualization layer emulators, sandboxes, containers, or\nruntime-instrumentation frameworks. Together they provide a formal foundation\nfor future work on the principles and limits of virtualization.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09825v1"}
{"id": "2506.09653", "title": "Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women", "authors": ["Sakshi Joshi", "Eldho Ittan George", "Tahir Javed", "Kaushal Bhogale", "Nikhil Narasimhan", "Mitesh M. Khapra"], "summary": "Digital inclusion remains a challenge for marginalized communities,\nespecially rural women in low-resource language regions like Bhojpuri.\nVoice-based access to agricultural services, financial transactions, government\nschemes, and healthcare is vital for their empowerment, yet existing ASR\nsystems for this group remain largely untested. To address this gap, we create\nSRUTI ,a benchmark consisting of rural Bhojpuri women speakers. Evaluation of\ncurrent ASR models on SRUTI shows poor performance due to data scarcity, which\nis difficult to overcome due to social and cultural barriers that hinder\nlarge-scale data collection. To overcome this, we propose generating synthetic\nspeech using just 25-30 seconds of audio per speaker from approximately 100\nrural women. Augmenting existing datasets with this synthetic data achieves an\nimprovement of 4.7 WER, providing a scalable, minimally intrusive solution to\nenhance ASR and promote digital inclusion in low-resource language.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09653v1"}
{"id": "2506.09282", "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs", "authors": ["Dhruv Parikh", "Viktor Prasanna"], "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.", "comment": "IC3", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09282v1"}
{"id": "2506.09195", "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09195v1"}
{"id": "2506.09162", "title": "The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset", "authors": ["Tyler J. Richards", "Adam E. Flanders", "Errol Colak", "Luciano M. Prevedello", "Robyn L. Ball", "Felipe Kitamura", "John Mongan", "Maryam Vazirabad", "Hui-Ming Lin", "Anne Kendell", "Thanat Kanthawang", "Salita Angkurawaranon", "Emre Altinmakas", "Hakan Dogan", "Paulo Eduardo de Aguiar Kuriki", "Arjuna Somasundaram", "Christopher Ruston", "Deniz Bulja", "Naida Spahovic", "Jennifer Sommer", "Sirui Jiang", "Eduardo Moreno Judice de Mattos Farina", "Eduardo Caminha Nunes", "Michael Brassil", "Megan McNamara", "Johanna Ortiz", "Jacob Peoples", "Vinson L. Uytana", "Anthony Kam", "Venkata N. S. Dola", "Daniel Murphy", "David Vu", "Dataset Contributor Group", "Dataset Annotator Group", "Competition Data Notebook Group", "Jason F. Talbott"], "summary": "The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging\nSpine Classification (LumbarDISC) dataset is the largest publicly available\ndataset of adult MRI lumbar spine examinations annotated for degenerative\nchanges. The dataset includes 2,697 patients with a total of 8,593 image series\nfrom 8 institutions across 6 countries and 5 continents. The dataset is\navailable for free for non-commercial use via Kaggle and RSNA Medical Imaging\nResource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine\nDegenerative Classification competition where competitors developed deep\nlearning models to grade degenerative changes in the lumbar spine. The degree\nof spinal canal, subarticular recess, and neural foraminal stenosis was graded\nat each intervertebral disc level in the lumbar spine. The images were\nannotated by expert volunteer neuroradiologists and musculoskeletal\nradiologists from the RSNA, American Society of Neuroradiology, and the\nAmerican Society of Spine Radiology. This dataset aims to facilitate research\nand development in machine learning and lumbar spine imaging to lead to\nimproved patient care and clinical efficiency.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09162v1"}
{"id": "2506.09178", "title": "Understanding Self-Regulated Learning Behavior Among High and Low Dropout Risk Students During CS1: Combining Trace Logs, Dropout Prediction and Self-Reports", "authors": ["Denis Zhidkikh", "Ville Isomöttönen", "Toni Taipalus"], "summary": "The introductory programming course (CS1) at the university level is often\nperceived as particularly challenging, contributing to high dropout rates among\nComputer Science students. Identifying when and how students encounter\ndifficulties in this course is critical for providing targeted support. This\nstudy explores the behavioral patterns of CS1 students at varying dropout risks\nusing self-regulated learning (SRL) as the theoretical framework. Using\nlearning analytics, we analyzed trace logs and task performance data from a\nvirtual learning environment to map resource usage patterns and used student\ndropout prediction to distinguish between low and high dropout risk behaviors.\nData from 47 consenting students were used to carry out the analysis.\nAdditionally, self-report questionnaires from 29 participants enriched the\ninterpretation of observed patterns. The findings reveal distinct weekly\nlearning strategy types and categorize course behavior. Among low dropout risk\nstudents, three learning strategies were identified that different in how\nstudents prioritized completing tasks and reading course materials. High\ndropout risk students exhibited nine different strategies, some representing\ntemporary unsuccessful strategies that can be recovered from, while others\nindicating behaviors of students on the verge of dropping out. This study\nhighlights the value of combining student behavior profiling with predictive\nlearning analytics to explain dropout predictions and devise targeted\ninterventions. Practical findings of the study can in turn be used to help\nteachers, teaching assistants and other practitioners to better recognize and\naddress students at the verge of dropping out.", "comment": "29 pages, 8 figures, 3 tables, submitted to ACM Transactions on\n  Computing Education", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09178v1"}
{"id": "2506.09909", "title": "TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model", "authors": ["Yijie Deng", "Lei Han", "Lu Fang"], "summary": "Neural rendering algorithms have revolutionized computer graphics, yet their\nimpact on real-time rendering under arbitrary lighting conditions remains\nlimited due to strict latency constraints in practical applications. The key\nchallenge lies in formulating a compact yet expressive material representation.\nTo address this, we propose TransGI, a novel neural rendering method for\nreal-time, high-fidelity global illumination. It comprises an object-centric\nneural transfer model for material representation and a radiance-sharing\nlighting system for efficient illumination. Traditional BSDF representations\nand spatial neural material representations lack expressiveness, requiring\nthousands of ray evaluations to converge to noise-free colors. Conversely,\nreal-time methods trade quality for efficiency by supporting only diffuse\nmaterials. In contrast, our object-centric neural transfer model achieves\ncompactness and expressiveness through an MLP-based decoder and vertex-attached\nlatent features, supporting glossy effects with low memory overhead. For\ndynamic, varying lighting conditions, we introduce local light probes capturing\nscene radiance, coupled with an across-probe radiance-sharing strategy for\nefficient probe generation. We implemented our method in a real-time rendering\nengine, combining compute shaders and CUDA-based neural networks. Experimental\nresults demonstrate that our method achieves real-time performance of less than\n10 ms to render a frame and significantly improved rendering quality compared\nto baseline methods.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09909v1"}
{"id": "2506.09205", "title": "Genetic Transformer-Assisted Quantum Neural Networks for Optimal Circuit Design", "authors": ["Haiyan Wang"], "summary": "We introduce Genetic Transformer Assisted Quantum Neural Networks (GTQNNs), a\nhybrid learning framework that combines a transformer encoder with a shallow\nvariational quantum circuit and automatically fine tunes the circuit via the\nNSGA-II multi objective genetic algorithm. The transformer reduces\nhigh-dimensional classical data to a compact, qubit sized representation, while\nNSGA-II searches for Pareto optimal circuits that (i) maximize classification\naccuracy and (ii) minimize primitive gate count an essential constraint for\nnoisy intermediate-scale quantum (NISQ) hardware. Experiments on four\nbenchmarks (Iris, Breast Cancer, MNIST, and Heart Disease) show that GTQNNs\nmatch or exceed state of the art quantum models while requiring much fewer\ngates for most cases. A hybrid Fisher information analysis further reveals that\nthe trained networks operate far from barren plateaus; the leading curvature\ndirections increasingly align with the quantum subspace as the qubit budget\ngrows, confirming that the transformer front end has effectively condensed the\ndata. Together, these results demonstrate that GTQNNs deliver competitive\nperformance with a quantum resource budget well suited to present-day NISQ\ndevices.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09205v1"}
{"id": "2506.09221", "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation", "authors": ["Michael Soprano"], "summary": "The spread of online misinformation poses serious threats to democratic\nsocieties. Traditionally, expert fact-checkers verify the truthfulness of\ninformation through investigative processes. However, the volume and immediacy\nof online content present major scalability challenges. Crowdsourcing offers a\npromising alternative by leveraging non-expert judgments, but it introduces\nconcerns about bias, accuracy, and interpretability. This thesis investigates\nhow human intelligence can be harnessed to assess the truthfulness of online\ninformation, focusing on three areas: misinformation assessment, cognitive\nbiases, and automated fact-checking systems. Through large-scale crowdsourcing\nexperiments and statistical modeling, it identifies key factors influencing\nhuman judgments and introduces a model for the joint prediction and explanation\nof truthfulness. The findings show that non-expert judgments often align with\nexpert assessments, particularly when factors such as timing and experience are\nconsidered. By deepening our understanding of human judgment and bias in\ntruthfulness assessment, this thesis contributes to the development of more\ntransparent, trustworthy, and interpretable systems for combating\nmisinformation.", "comment": "PhD thesis, University of Udine, defended May 2023, 458 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09221v1"}
{"id": "2506.09266", "title": "Improved error bounds for Koopman operator and reconstructed trajectories approximations with kernel-based methods", "authors": ["Diego Olguín", "Axel Osses", "Héctor Ramírez"], "summary": "In this article, we propose a new error bound for Koopman operator\napproximation using Kernel Extended Dynamic Mode Decomposition. The new\nestimate is $O(N^{-1/2})$, with a constant related to the probability of\nsuccess of the bound, given by Hoeffding's inequality, similar to other\nmethodologies, such as Philipp et al. Furthermore, we propose a \\textit{lifting\nback} operator to obtain trajectories generated by embedding the initial state\nand iterating a linear system in a higher dimension. This naturally yields an\n$O(N^{-1/2})$ error bound for mean trajectories. Finally, we show numerical\nresults including an example of nonlinear system, exhibiting successful\napproximation with exponential decay faster than $-1/2$, as suggested by the\ntheoretical results.", "comment": "24 pages, 6 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09266v1"}
{"id": "2506.09206", "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09206v1"}
{"id": "2506.09559", "title": "Identity and Access Management for the Computing Continuum", "authors": ["Chalima Dimitra Nassar Kyriakidou", "Athanasia Maria Papathanasiou", "Vasilios A. Siris", "Nikos Fotiou", "George C. Polyzos", "Eduardo Cánovas Martínez", "Antonio Skarmeta"], "summary": "The computing continuum introduces new challenges for access control due to\nits dynamic, distributed, and heterogeneous nature. In this paper, we propose a\nZero-Trust (ZT) access control solution that leverages decentralized\nidentification and authentication mechanisms based on Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs). Additionally, we employ\nRelationship-Based Access Control (ReBAC) to define policies that capture the\nevolving trust relationships inherent in the continuum. Through a\nproof-of-concept implementation, we demonstrate the feasibility and efficiency\nof our solution, highlighting its potential to enhance security and trust in\ndecentralized environments.", "comment": "Proceedings of the 2nd International Workshop on MetaOS for the\n  Cloud-Edge-IoT Continuum, pp 33-39. 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09559v1"}
{"id": "2506.09384", "title": "Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation", "authors": ["Chendong Xin", "Mingrui Yu", "Yongpeng Jiang", "Zhefeng Zhang", "Xiang Li"], "summary": "Kinematic retargeting from human hands to robot hands is essential for\ntransferring dexterity from humans to robots in manipulation teleoperation and\nimitation learning. However, due to mechanical differences between human and\nrobot hands, completely reproducing human motions on robot hands is impossible.\nExisting works on retargeting incorporate various optimization objectives,\nfocusing on different aspects of hand configuration. However, the lack of\nexperimental comparative studies leaves the significance and effectiveness of\nthese objectives unclear. This work aims to analyze these retargeting\nobjectives for dexterous manipulation through extensive real-world comparative\nexperiments. Specifically, we propose a comprehensive retargeting objective\nformulation that integrates intuitively crucial factors appearing in recent\napproaches. The significance of each factor is evaluated through experimental\nablation studies on the full objective in kinematic posture retargeting and\nreal-world teleoperated manipulation tasks. Experimental results and\nconclusions provide valuable insights for designing more accurate and effective\nretargeting algorithms for real-world dexterous manipulation.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09384v1"}
{"id": "2506.09083", "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "authors": ["Jerry Lin", "Partick P. W. Chen"], "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "comment": "4 pages, 3 figures, 1 Table", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09083v1"}
{"id": "2506.09656", "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives", "authors": ["Wei Zeng", "Hengshu Zhu", "Chuan Qin", "Han Wu", "Yihang Cheng", "Sirui Zhang", "Xiaowei Jin", "Yinuo Shen", "Zhenxing Wang", "Feimin Zhong", "Hui Xiong"], "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09656v1"}
{"id": "2506.09092", "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09092v1"}
{"id": "2506.09641", "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "authors": ["Anna Stein", "Kevin Tang"], "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "comment": "Submitted to Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09641v1"}
{"id": "2506.09683", "title": "Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead", "authors": ["Priyavanshi Pathania", "Nikhil Bamby", "Rohit Mehra", "Samarth Sikand", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "The proliferation of software and AI comes with a hidden risk: its growing\nenergy and carbon footprint. As concerns regarding environmental sustainability\ncome to the forefront, understanding and optimizing how software impacts the\nenvironment becomes paramount. In this paper, we present a state-of-the-art\nreview of methods and tools that enable the measurement of software and\nAI-related energy and/or carbon emissions. We introduce a taxonomy to\ncategorize the existing work as Monitoring, Estimation, or Black-Box\napproaches. We delve deeper into the tools and compare them across different\ndimensions and granularity - for example, whether their measurement encompasses\nenergy and carbon emissions and the components considered (like CPU, GPU, RAM,\netc.). We present our observations on the practical use (component wise\nconsolidation of approaches) as well as the challenges that we have identified\nacross the current state-of-the-art. As we start an initiative to address these\nchallenges, we emphasize active collaboration across the community in this\nimportant field.", "comment": "8 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09683v1"}
{"id": "2506.09354", "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09354v1"}
{"id": "2506.09789", "title": "Delegations as Adaptive Representation Patterns: Rethinking Influence in Liquid Democracy", "authors": ["Davide Grossi", "Andreas Nitsche"], "summary": "Liquid democracy is a mechanism for the division of labor in decision-making\nthrough the transitive delegation of influence. In essence, all individuals\npossess the autonomy to determine the issues with which they will engage\ndirectly, while for other matters, they may appoint a representative of their\nchoosing. So far, the literature has studied the delegation structures emerging\nin liquid democracy as static. As a result, transitivity defined as the\ncapacity to transfer acquired authority to another entity, has been identified\nas a concern as it would be conducive to unrestrained accumulation of power.\n  Focusing on the implementation of liquid democracy supported by the\nLiquidFeedback software, we propose a novel approach to assessing the influence\nof voting nodes in a transitive delegation graph, taking into account the\nprocess nature of real-world liquid democracy in which delegation and voting\nare distinct and increasingly independent activities. By introducing a novel\nmodel of delegations in liquid democracy, we show how transitivity may in fact\ncontribute to an effective regulation of deliberation influence and\ndecision-making power. While maintaining the one-person, one-vote paradigm for\nall votes cast, the anticipated influence of an agent, to the extent it is\nstemming from transitivity, experiences a precipitous decline following an\nexponential trajectory.\n  In general, it is our objective to move the first steps towards a rigorous\nanalysis of liquid democracy as an adaptive democratic representation process.\nThe adaptivity aspect of liquid democracy has not yet been explored within the\nexisting academic literature despite it being, we believe, one of its most\nimportant features. We therefore also outline a research agenda focusing on\nthis aspect of liquid democracy.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09789v1"}
{"id": "2506.09512", "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "comment": "7 pages, 1 figure", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09512v1"}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "comment": "6 pages, 9 figures, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09397v1"}
{"id": "2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "comment": "5 pages, 2 figures", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09707v1"}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "comment": "6 pages, 9 figures, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09397v1"}
{"id": "2506.09225", "title": "Near-Field Sensing Enabled Predictive Beamforming: Fundamentals, Framework, and Opportunities", "authors": ["Hao Jiang", "Zhaolin Wang", "Yue Liu", "Hyundong Shin", "Arumugam Nallanathan", "Yuanwei Liu"], "summary": "The article proposes a novel near-field predictive beamforming framework for\nhigh-mobility wireless networks. Specifically, due to the spherical waves and\nnon-uniform Doppler frequencies brought by the near-field region, the new\nability of full-dimensional location and velocity sensing is characterized.\nBuilding on this foundation, the near-field predictive beamforming framework is\nproposed to proactively design beamformers for mobility users following\narbitrary trajectories. Compared to the conventional far-field counterpart, the\nnear-field predictive beamforming stands out due to: i) Prior-Knowledge-Free\nPrediction, and ii) Low-Complexity and Generalizable System Design. To realize\nthese advantages, the implementation methods are discussed, followed by a case\nstudy confirming the benefits of the proposed framework. Finally, the article\nhighlights promising research opportunities inspired by the proposed framework.", "comment": "Submitted to IEEE", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09225v1"}
{"id": "2506.09377", "title": "An Interpretable Two-Stage Feature Decomposition Method for Deep Learning-based SAR ATR", "authors": ["Chenwei Wang", "Renjie Xu", "Congwen Wu", "Cunyi Yin", "Ziyun Liao", "Deqing Mao", "Sitong Zhang", "Hong Yan"], "summary": "Synthetic aperture radar automatic target recognition (SAR ATR) has seen\nsignificant performance improvements with deep learning. However, the black-box\nnature of deep SAR ATR introduces low confidence and high risks in\ndecision-critical SAR applications, hindering practical deployment. To address\nthis issue, deep SAR ATR should provide an interpretable reasoning basis $r_b$\nand logic $\\lambda_w$, forming the reasoning logic $\\sum_{i} {{r_b^i} \\times\n{\\lambda_w^i}} =pred$ behind the decisions. Therefore, this paper proposes a\nphysics-based two-stage feature decomposition method for interpretable deep SAR\nATR, which transforms uninterpretable deep features into attribute scattering\ncenter components (ASCC) with clear physical meanings. First, ASCCs are\nobtained through a clustering algorithm. To extract independent physical\ncomponents from deep features, we propose a two-stage decomposition method. In\nthe first stage, a feature decoupling and discrimination module separates deep\nfeatures into approximate ASCCs with global discriminability. In the second\nstage, a multilayer orthogonal non-negative matrix tri-factorization (MLO-NMTF)\nfurther decomposes the ASCCs into independent components with distinct physical\nmeanings. The MLO-NMTF elegantly aligns with the clustering algorithms to\nobtain ASCCs. Finally, this method ensures both an interpretable reasoning\nprocess and accurate recognition results. Extensive experiments on four\nbenchmark datasets confirm its effectiveness, showcasing the method's\ninterpretability, robust recognition performance, and strong generalization\ncapability.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09377v1"}
{"id": "2506.09185", "title": "Whole-Person Education for AI Engineers", "authors": ["Rubaina Khan", "Tammy Mackenzie", "Sreyoshi Bhaduri", "Animesh Paul", "Branislav Radeljić", "Joshua Owusu Ansah", "Beyza Nur Guler", "Indrani Bhaduri", "Rodney Kimbangu", "Nils Ever Murrugarra Llerena", "Hayoung Shin", "Lilianny Virguez", "Rosa Paccotacya Yanque", "Thomas Mekhaël", "Allen Munoriyarwa", "Leslie Salgado", "Debarati Basu", "Curwyn Mapaling", "Natalie Perez", "Yves Gaudet", "Paula Larrondo"], "summary": "This autoethnographic study explores the need for interdisciplinary education\nspanning both technical and philosophical skills - as such, this study\nleverages whole-person education as a theoretical approach needed in AI\nengineering education to address the limitations of current paradigms that\nprioritize technical expertise over ethical and societal considerations.\nDrawing on a collaborative autoethnography approach of fourteen diverse\nstakeholders, the study identifies key motivations driving the call for change,\nincluding the need for global perspectives, bridging the gap between academia\nand industry, integrating ethics and societal impact, and fostering\ninterdisciplinary collaboration. The findings challenge the myths of\ntechnological neutrality and technosaviourism, advocating for a future where AI\nengineers are equipped not only with technical skills but also with the ethical\nawareness, social responsibility, and interdisciplinary understanding necessary\nto navigate the complex challenges of AI development. The study provides\nvaluable insights and recommendations for transforming AI engineering education\nto ensure the responsible development of AI technologies.", "comment": "conference pre-print, position paper. 21 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09185v1"}
{"id": "2506.09997", "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09997v1"}
{"id": "2506.09404", "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09404v1"}
{"id": "2506.09260", "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09260v1"}
{"id": "2506.09309", "title": "A discontinuous Galerkin plane wave neural network method for Helmholtz equation and Maxwell's equations", "authors": ["Long Yuan", "Menghui Wu", "Qiya Hu"], "summary": "In this paper we propose a discontinuous Galerkin plane wave neural network\n(DGPWNN) method for approximately solving Helmholtz equation and Maxwell's\nequations. In this method, we define an elliptic-type variational problem as in\nthe plane wave least square method with $h-$refinement and introduce the\nadaptive construction of recursively augmented discontinuous Galerkin subspaces\nwhose basis functions are realizations of element-wise neural network functions\nwith $hp-$refinement, where the activation function is chosen as a\ncomplex-valued exponential function like the plane wave function.\n  A sequence of basis functions approaching the unit residuals are recursively\ngenerated by iteratively solving quasi-maximization problems associated with\nthe underlying residual functionals and the intersection of the closed unit\nball and discontinuous plane wave neural network spaces. The convergence\nresults of the DGPWNN method are established without the assumption on the\nboundedness of the neural network parameters. Numerical experiments confirm the\neffectiveness of the proposed method.", "comment": "31 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09309v1"}
{"id": "2506.09448", "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "comment": "Accepted to Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09448v1"}
{"id": "2506.09562", "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning", "authors": ["Songze Li", "Mingxuan Zhang", "Oubo Ma", "Kang Wei", "Shouling Ji"], "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09562v1"}
{"id": "2506.09406", "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09406v1"}
{"id": "2506.09106", "title": "Bias Analysis in Unconditional Image Generative Models", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09106v1"}
{"id": "2506.09659", "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09659v1"}
{"id": "2506.09093", "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09093v1"}
{"id": "2506.09732", "title": "End-to-End Dynamic Metasurface Antenna Wireless System: Prototype, Opportunities, and Challenges", "authors": ["François Yven", "Jean Tapie", "Jérôme Sol", "Philipp del Hougne"], "summary": "Dynamic metasurface antennas (DMAs) are a promising hybrid analog/digital\nbeamforming technology to realize next-generation wireless systems with low\ncost, footprint, and power consumption. The research on DMA-empowered wireless\nsystems is still at an early stage, mostly limited to theoretical studies under\nsimplifying assumptions on the one hand and a few antenna-level experiments on\nthe other hand. Substantial knowledge gaps arise from the lack of complete\nend-to-end DMA-empowered wireless system prototypes. In addition, recently\nunveiled benefits of strong inter-element mutual coupling (MC) in DMAs remain\nuntapped. Here, we demonstrate a K-band prototype of an end-to-end wireless\nsystem based on a DMA with strong inter-element MC. To showcase the flexible\ncontrol over the DMA's radiation pattern, we present an experimental case study\nof simultaneously steering a beam to a desired transmitter and a null to an\nundesired jammer, achieving up to 43~dB discrimination. Using software-defined\nradios, we transmit and receive QPSK OFDM waveforms to evaluate the bit error\nrate. We also discuss algorithmic and technological challenges associated with\nenvisioned future evolutions of our end-to-end testbed and real-life DMA-based\nwireless systems.", "comment": "7 pages, 4 figures, submitted to an IEEE Journal", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09732v1"}
{"id": "2506.09702", "title": "Mapping NVD Records to Their VFCs: How Hard is it?", "authors": ["Huu Hung Nguyen", "Duc Manh Tran", "Yiran Cheng", "Thanh Le-Cong", "Hong Jin Kang", "Ratnadira Widyasari", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "summary": "Mapping National Vulnerability Database (NVD) records to vulnerability-fixing\ncommits (VFCs) is crucial for vulnerability analysis but challenging due to\nsparse explicit links in NVD references.This study explores this mapping's\nfeasibility through an empirical approach. Manual analysis of NVD references\nshowed Git references enable over 86% success, while non-Git references achieve\nunder 14%. Using these findings, we built an automated pipeline extracting\n31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,\nmainly from Git references. To fill gaps, we mined six external security\ndatabases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,\nand GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%\nprecision. Combining these, we mapped 26,710 unique records (11.3% coverage)\nfrom 7,634 projects, with overlap between NVD and external databases, plus\nunique GitHub contributions. Despite success with Git references, 88.7% of\nrecords remain unmapped, highlighting the difficulty without Git links. This\nstudy offers insights for enhancing vulnerability datasets and guiding future\nautomated security research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09702v1"}
{"id": "2506.09362", "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09362v1"}
{"id": "2506.09523", "title": "Adaptive event-triggered robust tracking control of soft robots", "authors": ["Renjie Ma", "Ziyao Qu", "Zhijian Hu", "Dong Zhao", "Marios M. Polycarpou"], "summary": "Soft robots manufactured with flexible materials can be highly compliant and\nadaptive to their surroundings, which facilitates their application in areas\nsuch as dexterous manipulation and environmental exploration. This paper aims\nat investigating the tracking control problem for soft robots under uncertainty\nsuch as unmodeled dynamics and external disturbance. First, we establish a\nnovel switching function and design the compensated tracking error dynamics by\nvirtue of the command filter. Then, based on the backstepping methodology, the\nvirtual controllers and the adaptive logic estimating the supremum of\nuncertainty impacts are developed for synthesizing an event-triggered control\nstrategy. In addition, the uniformed finite-time stability certification is\nderived for different scenarios of the switching function. Finally, we perform\na case study of a soft robot to illustrate the effectiveness of the proposed\ncontrol algorithm.", "comment": "8 pages, 7 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09523v1"}
{"id": "2506.09418", "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G", "authors": ["Ryan Barker", "Fatemeh Afghah"], "summary": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G.", "comment": "4 pages, 1 figure", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09418v1"}
{"id": "2506.09804", "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09804v1"}
{"id": "2506.09463", "title": "Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP", "authors": ["Soumyajit Chatterjee", "Rahul Utkoor", "Uppu Eshwar", "Sathya Peri", "V. Krishna Nandivada"], "summary": "Efficient task scheduling is paramount in parallel programming on multi-core\narchitectures, where tasks are fundamental computational units. QR\nfactorization is a critical sub-routine in Sequential Least Squares Quadratic\nProgramming (SLSQP) for solving non-linear programming (NLP) problems. QR\nfactorization decomposes a matrix into an orthogonal matrix Q and an upper\ntriangular matrix R, which are essential for solving systems of linear\nequations arising from optimization problems. SLSQP uses an in-place version of\nQR factorization, which requires storing intermediate results for the next\nsteps of the algorithm. Although DAG-based approaches for QR factorization are\nprevalent in the literature, they often lack control over the intermediate\nkernel results, providing only the final output matrices Q and R. This\nlimitation is particularly challenging in SLSQP, where intermediate results of\nQR factorization are crucial for back-substitution logic at each iteration. Our\nwork introduces novel scheduling techniques using a two-queue approach to\nexecute the QR factorization kernel effectively. This approach, implemented in\nhigh-level C++ programming language, facilitates compiler optimizations and\nallows storing intermediate results required by back-substitution logic.\nEmpirical evaluations demonstrate substantial performance gains, including a\n10x improvement over the sequential QR version of the SLSQP algorithm.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09463v1"}
{"id": "2506.09253", "title": "Development of a Photon-Counting Deadtime Noise Model that Extends Dynamic Range and Resolution in Atmospheric Lidar", "authors": ["Grant J. Kirchhoff", "Matthew Hayman", "Willem J. Marais", "Jeffrey P. Thayer", "Rory A. Barton-Grimley"], "summary": "This work derives and validates a noise model that encapsulates deadtime of\nnon-paralyzable detectors with random photon arrivals to enable advanced\nprocessing, like maximum-likelihood estimation, of high resolution atmospheric\nlidar profiles while accounting for deadtime bias. This estimator was validated\nacross a wide dynamic range at high resolution (4 millimeters in range, 17\nmilliseconds in time). Experiments demonstrate that the noise model outperforms\nthe current state-of-the-art for very short time-of-flight (2 nanoseconds) and\nextended targets (1 microsecond). The proposed noise model also produces\naccurate deadtime correction for very short integration times. This work sets\nthe foundation for further study into accurate retrievals of high flux and\ndynamic atmospheric features, e.g., clouds and aerosol layers.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09253v1"}
{"id": "2506.09661", "title": "A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma", "authors": ["Garima Jain", "Sanghamitra Pati", "Mona Duggal", "Amit Sethi", "Abhijeet Patil", "Gururaj Malekar", "Nilesh Kowe", "Jitender Kumar", "Jatin Kashyap", "Divyajeet Rout", "Deepali", "Hitesh", "Nishi Halduniya", "Sharat Kumar", "Heena Tabassum", "Rupinder Singh Dhaliwal", "Sucheta Devi Khuraijam", "Sushma Khuraijam", "Sharmila Laishram", "Simmi Kharb", "Sunita Singh", "K. Swaminadtan", "Ranjana Solanki", "Deepika Hemranjani", "Shashank Nath Singh", "Uma Handa", "Manveen Kaur", "Surinder Singhal", "Shivani Kalhan", "Rakesh Kumar Gupta", "Ravi. S", "D. Pavithra", "Sunil Kumar Mahto", "Arvind Kumar", "Deepali Tirkey", "Saurav Banerjee", "L. Sreelakshmi"], "summary": "Oral squamous cell carcinoma OSCC is a major global health burden,\nparticularly in several regions across Asia, Africa, and South America, where\nit accounts for a significant proportion of cancer cases. Early detection\ndramatically improves outcomes, with stage I cancers achieving up to 90 percent\nsurvival. However, traditional diagnosis based on histopathology has limited\naccessibility in low-resource settings because it is invasive,\nresource-intensive, and reliant on expert pathologists. On the other hand, oral\ncytology of brush biopsy offers a minimally invasive and lower cost\nalternative, provided that the remaining challenges, inter observer variability\nand unavailability of expert pathologists can be addressed using artificial\nintelligence. Development and validation of robust AI solutions requires access\nto large, labeled, and multi-source datasets to train high capacity models that\ngeneralize across domain shifts. We introduce the first large and multicenter\noral cytology dataset, comprising annotated slides stained with\nPapanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten\ntertiary medical centers in India. The dataset is labeled and annotated by\nexpert pathologists for cellular anomaly classification and detection, is\ndesigned to advance AI driven diagnostic methods. By filling the gap in\npublicly available oral cytology datasets, this resource aims to enhance\nautomated detection, reduce diagnostic errors, and improve early OSCC diagnosis\nin resource-constrained settings, ultimately contributing to reduced mortality\nand better patient outcomes worldwide.", "comment": "7 pages, 2 figurs", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09661v1"}
{"id": "2506.09472", "title": "Situated Bayes -- Feminist and Pluriversal Perspectives on Bayesian Knowledge", "authors": ["Juni Schindler", "Goda Klumbytė", "Matthew Fuller"], "summary": "This is the introduction and lead article to the Situated Bayes special issue\nof Computational Culture. The article introduces Bayes' Theorem and aspects of\nits contemporary uses, for instance in machine learning. A mathematical\ndiscussion is developed alongside a consideration of Bayes Theorem in relation\nto critical theories of knowledge, specifically the discussion of situated\nknowledge in feminist theories of science, pluriversal knowledge in decolonial\ntheory, and critical approaches to mathematics. We discuss whether there are\npossible resonances between Bayesian mapping of multiple functions and the idea\nof the subjective on the one hand and these theoretical propositions on the\nother and propose further lines of enquiry for future research. In closing the\nintroduction, the contributions to the special issue are briefly described.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09472v1"}
{"id": "2506.09485", "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09485v1"}
{"id": "2506.09409", "title": "MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed", "authors": ["Jiaqi Samantha Zhan", "Crystina Zhang", "Shengyao Zhuang", "Xueguang Ma", "Jimmy Lin"], "summary": "Effective video retrieval remains challenging due to the complexity of\nintegrating visual, auditory, and textual modalities. In this paper, we explore\nunified retrieval methods using OmniEmbed, a powerful multimodal embedding\nmodel from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task.\nEvaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates\nunified embeddings for text, images, audio, and video, enabling robust\nmultimodal retrieval. By finetuning OmniEmbed with the combined multimodal\ndata--visual frames, audio tracks, and textual descriptions provided in\nMultiVENT 2.0, we achieve substantial improvements in complex, multilingual\nvideo retrieval tasks. Our submission achieved the highest score on the MAGMaR\nshared task leaderboard among public submissions as of May 20th, 2025,\nhighlighting the practical effectiveness of our unified multimodal retrieval\napproach. Model checkpoint in this work is opensourced.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09409v1"}
{"id": "2506.09361", "title": "Overcoming logarithmic singularities in the Cahn-Hilliard equation with Flory-Huggins potential: An unconditionally convergent ADMM approach", "authors": ["Ruo Li", "Shengtong Liang", "Zhonghua Qiao"], "summary": "The Cahn-Hilliard equation with Flory-Huggins potential serves as a\nfundamental phase field model for describing phase separation phenomena. Due to\nthe presence of logarithmic singularities at $u=\\pm 1$, the solution $u$ is\nconstrained within the interval $(-1,1)$. While convex splitting schemes are\ncommonly employed to preserve this bound and guarantee unconditional unique\nsolvability, their practical implementation requires solving nonlinear systems\ncontaining singular logarithmic terms at each time step. This introduces\nsignificant challenges in both ensuring convergence of iterative solvers and\nmaintaining the solution bounds throughout the iterations. Existing solvers\noften rely on restrictive conditions -- such as the strict separation property\nor small time step sizes -- to ensure convergence, which can limit their\napplicability. In this work, we introduce a novel iterative solver that is\nspecifically designed for singular nonlinear systems, with the use of a variant\nof the alternating direction method of multipliers (ADMM). By developing a\ntailored variable splitting strategy within the ADMM framework, our method\nefficiently decouples the challenging logarithmic nonlinearity, enabling\neffective handling of singularities. Crucially, we rigorously prove the\nunconditional convergence of our ADMM-based solver, which removes the need for\ntime step constraints or strict separation conditions. This allows us to fully\nleverage the unconditional solvability offered by convex splitting schemes.\nComprehensive numerical experiments demonstrate the superior efficiency and\nrobustness of our ADMM variant, strongly validating both our algorithmic design\nand theoretical results.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09361v1"}
{"id": "2506.09487", "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09487v1"}
{"id": "2506.09569", "title": "The Rabin cryptosystem over number fields", "authors": ["Alessandro Cobbe", "Andreas Nickel", "Akay Schuster"], "summary": "We extend Rabin's cryptosystem to general number fields. We show that\ndecryption of a random plaintext is as hard as the integer factorisation\nproblem, provided the modulus in our scheme has been chosen carefully. We\ninvestigate the performance of our new cryptosystem in comparison with the\nclassical Rabin scheme and a more recent version over the Gaussian integers.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09569v1"}
{"id": "2506.09422", "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09422v1"}
{"id": "2506.09109", "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "comment": "Preprint, under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09109v1"}
{"id": "2506.09977", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09977v1"}
{"id": "2506.09096", "title": "Intra-Trajectory Consistency for Reward Modeling", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "comment": "Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09096v1"}
{"id": "2506.09810", "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "authors": ["Minoh Jeong", "Alfred Hero"], "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09810v1"}
{"id": "2506.09713", "title": "A First Look at Bugs in LLM Inference Engines", "authors": ["Mugeng Liu", "Siqi Zhong", "Weichen Bi", "Yixuan Zhang", "Zhiyang Chen", "Zhenpeng Chen", "Xuanzhe Liu", "Yun Ma"], "summary": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers.", "comment": "Under review", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09713v1"}
{"id": "2506.09696", "title": "Patterns of Patterns III", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance.", "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09696v1"}
{"id": "2506.09573", "title": "Probability-One Optimization of Generalized Rayleigh Quotient Sum For Multi-Source Generalized Total Least-Squares", "authors": ["Dominik Friml", "Pavel Václavek"], "summary": "This paper addresses the global optimization of the sum of the Rayleigh\nquotient and the generalized Rayleigh quotient on the unit sphere. While\nvarious methods have been proposed for this problem, they do not guarantee\nconvergence to the global maximizer. To overcome this limitation, we introduce\na probability-one homotopy optimization method that, under certain conditions,\nguarantees convergence to the global maximizer. The proposed method is analyzed\nalongside state-of-the-art approaches through numerical experiments, evaluating\ntheir performance in terms of convergence speed and ability to reach the global\nmaximizer. Furthermore, we demonstrate how this ties in with the multi-source\nBayesian Generalized Total Least-Squares (B-GTLS) problem, illustrating its\napplicability.", "comment": "This is the preprint version prior to peer review", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09573v1"}
{"id": "2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09175v1"}
{"id": "2506.09505", "title": "On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems", "authors": ["Dumitrel Loghin", "Shuang Liang", "Shengwei Liu", "Xiong Liu", "Pingcheng Ruan", "Zhigang Ye"], "summary": "Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling\nblockchains and bringing Web3 to life. At the same time, ZKP for transactions\nrunning on the Ethereum Virtual Machine require powerful servers with hundreds\nof CPU cores. The current zkProver implementation from Polygon is optimized for\nx86-64 CPUs by vectorizing key operations, such as Merkle tree building with\nPoseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX\nand AVX512). With these optimizations, a ZKP for a batch of transactions is\ngenerated in less than two minutes. With the advent of cloud servers with ARM\nwhich are at least 10% cheaper than x86-64 servers and the implementation of\nARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over\ntheir x86-64 counterparts. Unfortunately, our analysis shows that current ARM\nCPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web\nServices (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X\nslower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX\nand AVX512, respectively, when building a Merkle tree with over four million\nleaves. This low performance is due to (1) smaller vector size in these ARM\nCPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the\nother hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as\npowerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing\nthe vector size to 512 bits will enable higher performance in ARM CPUs compared\nto their x86-64 counterparts while maintaining their price advantage.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09505v1"}
{"id": "2506.09255", "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization", "authors": ["Saeed Hashemi", "Genchang Peng", "Mehrdad Nourani", "Omar Nofal", "Jay Harvey"], "summary": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.", "comment": "Accepted to be presented at the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025). This\n  version is submitted to arXiv prior to final IEEE formatting and publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09255v1"}
{"id": "2506.09949", "title": "Sampling Theory for Super-Resolution with Implicit Neural Representations", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.", "comment": "arXiv admin note: text overlap with arXiv:2405.18410", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09949v1"}
{"id": "2506.09632", "title": "Ties of Trust: a bowtie model to uncover trustor-trustee relationships in LLMs", "authors": ["Eva Paraschou", "Maria Michali", "Sofia Yfantidou", "Stelios Karamanidis", "Stefanos Rafail Kalogeros", "Athena Vakali"], "summary": "The rapid and unprecedented dominance of Artificial Intelligence (AI),\nparticularly through Large Language Models (LLMs), has raised critical trust\nchallenges in high-stakes domains like politics. Biased LLMs' decisions and\nmisinformation undermine democratic processes, and existing trust models fail\nto address the intricacies of trust in LLMs. Currently, oversimplified,\none-directional approaches have largely overlooked the many relationships\nbetween trustor (user) contextual factors (e.g. ideology, perceptions) and\ntrustee (LLMs) systemic elements (e.g. scientists, tool's features). In this\nwork, we introduce a bowtie model for holistically conceptualizing and\nformulating trust in LLMs, with a core component comprehensively exploring\ntrust by tying its two sides, namely the trustor and the trustee, as well as\ntheir intricate relationships. We uncover these relationships within the\nproposed bowtie model and beyond to its sociotechnical ecosystem, through a\nmixed-methods explanatory study, that exploits a political discourse analysis\ntool (integrating ChatGPT), by exploring and responding to the next critical\nquestions: 1) How do trustor's contextual factors influence trust-related\nactions? 2) How do these factors influence and interact with trustee systemic\nelements? 3) How does trust itself vary across trustee systemic elements? Our\nbowtie-based explanatory analysis reveals that past experiences and familiarity\nsignificantly shape trustor's trust-related actions; not all trustor contextual\nfactors equally influence trustee systemic elements; and trustee's\nhuman-in-the-loop features enhance trust, while lack of transparency decreases\nit. Finally, this solid evidence is exploited to deliver recommendations,\ninsights and pathways towards building robust trusting ecosystems in LLM-based\nsolutions.", "comment": "Accepted for publication at The 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25). This version corresponds to the\n  camera-ready manuscript submitted to the conference proceedings", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09632v1"}
{"id": "2506.09898", "title": "Discrete Scale-invariant Metric Learning for Efficient Collaborative Filtering", "authors": ["Yan Zhang", "Li Deng", "Lixin Duan", "Sami Azam"], "summary": "Metric learning has attracted extensive interest for its ability to provide\npersonalized recommendations based on the importance of observed user-item\ninteractions. Current metric learning methods aim to push negative items away\nfrom the corresponding users and positive items by an absolute geometrical\ndistance margin. However, items may come from imbalanced categories with\ndifferent intra-class variations. Thus, the absolute distance margin may not be\nideal for estimating the difference between user preferences over imbalanced\nitems. To this end, we propose a new method, named discrete scale-invariant\nmetric learning (DSIML), by adding binary constraints to users and items, which\nmaps users and items into binary codes of a shared Hamming subspace to speed up\nthe online recommendation. Specifically, we firstly propose a scale-invariant\nmargin based on angles at the negative item points in the shared Hamming\nsubspace. Then, we derive a scale-invariant triple hinge loss based on the\nmargin. To capture more preference difference information, we integrate a\npairwise ranking loss into the scale-invariant loss in the proposed model. Due\nto the difficulty of directly optimizing the mixed integer optimization problem\nformulated with \\textit{log-sum-exp} functions, we seek to optimize its\nvariational quadratic upper bound and learn hash codes with an alternating\noptimization strategy. Experiments on benchmark datasets clearly show that our\nproposed method is superior to competitive metric learning and hashing-based\nbaselines for recommender systems. The implementation code is available at\nhttps://github.com/AnonyFeb/dsml.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09898v1"}
{"id": "2506.09394", "title": "Subspace-constrained randomized coordinate descent for linear systems with good low-rank matrix approximations", "authors": ["Jackie Lok", "Elizaveta Rebrova"], "summary": "The randomized coordinate descent (RCD) method is a classical algorithm with\nsimple, lightweight iterations that is widely used for various optimization\nproblems, including the solution of positive semidefinite linear systems. As a\nlinear solver, RCD is particularly effective when the matrix is\nwell-conditioned; however, its convergence rate deteriorates rapidly in the\npresence of large spectral outliers. In this paper, we introduce the\nsubspace-constrained randomized coordinate descent (SC-RCD) method, in which\nthe dynamics of RCD are restricted to an affine subspace corresponding to a\ncolumn Nystr\\\"{o}m approximation, efficiently computed using the recently\nanalyzed RPCholesky algorithm. We prove that SC-RCD converges at a rate that is\nunaffected by large spectral outliers, making it an effective and\nmemory-efficient solver for large-scale, dense linear systems with rapidly\ndecaying spectra, such as those encountered in kernel ridge regression.\nExperimental validation and comparisons with related solvers based on\ncoordinate descent and the conjugate gradient method demonstrate the efficiency\nof SC-RCD. Our theoretical results are derived by developing a more general\nsubspace-constrained framework for the sketch-and-project method. This\nframework generalizes popular algorithms such as randomized Kaczmarz and\ncoordinate descent, and provides a flexible, implicit preconditioning strategy\nfor a variety of iterative solvers, which may be of independent interest.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09394v1"}
{"id": "2506.09709", "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "comment": "Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09709v1"}
{"id": "2506.09580", "title": "The Everyday Security of Living with Conflict", "authors": ["Jessica McClearn", "Reem Talhouk", "Rikke Bjerg Jensen"], "summary": "When `cyber' is used as a prefix, attention is typically drawn to the\ntechnological and spectacular aspects of war and conflict -- and, by extension,\nsecurity. We offer a different approach to engaging with and understanding\nsecurity in such contexts, by foregrounding the everyday -- mundane --\nexperiences of security within communities living with and fleeing from war. We\ndo so through three vignettes from our field research in Colombia, Lebanon and\nSweden, respectively, and by highlighting the significance of ethnography for\nsecurity research with communities living in regions afflicted by war. We\nconclude by setting out a call to action for security researchers and\npractitioners to consider such lived experiences in the design of security\ntechnology that aims to cater to the needs of communities in `global conflict\nand disaster regions'.", "comment": "Published in IEEE Security and Privacy Magazine", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09580v1"}
{"id": "2506.09444", "title": "Design of an innovative robotic surgical instrument for circular stapling", "authors": ["Paul Tucan", "Nadim Al Hajjar", "Calin Vaida", "Alexandru Pusca", "Tiberiu Antal", "Corina Radu", "Daniel Jucan", "Adrian Pisla", "Damien Chablat", "Doina Pisla"], "summary": "Esophageal cancer remains a highly aggressive malignancy with low survival\nrates, requiring advanced surgical interventions like esophagectomy.\nTraditional manual techniques, including circular staplers, face challenges\nsuch as limited precision, prolonged recovery times, and complications like\nleaks and tissue misalignment. This paper presents a novel robotic circular\nstapler designed to enhance the dexterity in confined spaces, improve tissue\nalignment, and reduce post-operative risks. Integrated with a cognitive robot\nthat serves as a surgeon's assistant, the surgical stapler uses three actuators\nto perform anvil motion, cutter/stapler motion and allows a 75-degree bending\nof the cartridge (distal tip). Kinematic analysis is used to compute the\nstapler tip's position, ensuring synchronization with a robotic system.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09444v1"}
{"id": "2506.09113", "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "comment": "Seedance 1.0 Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09113v1"}
{"id": "2506.09985", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09985v1"}
{"id": "2506.09099", "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "authors": ["Joshua Barron", "Devin White"], "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09099v1"}
{"id": "2506.09870", "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "authors": ["Maximilian Egger", "Rawad Bitar"], "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09870v1"}
{"id": "2506.09759", "title": "Towards Bridging Formal Methods and Human Interpretability", "authors": ["Abhijit Paul", "Proma Chowdhury", "Kazi Sakib"], "summary": "Labeled Transition Systems (LTS) are integral to model checking and design\nrepair tools. System engineers frequently examine LTS designs during model\nchecking or design repair to debug, identify inconsistencies, and validate\nsystem behavior. Despite LTS's significance, no prior research has examined\nhuman comprehension of these designs. To address this, we draw on traditional\nsoftware engineering and graph theory, identifying 7 key metrics: cyclomatic\ncomplexity, state space size, average branching factor, maximum depth, Albin\ncomplexity, modularity, and redundancy. We created a dataset of 148 LTS\ndesigns, sampling 48 for 324 paired comparisons, and ranked them using the\nBradley-Terry model. Through Kendall's Tau correlation analysis, we found that\nAlbin complexity ($\\tau = 0.444$), state space size ($\\tau = 0.420$),\ncyclomatic complexity ($\\tau = 0.366$), and redundancy ($\\tau = 0.315$) most\naccurately reflect human comprehension of LTS designs. To showcase the metrics'\nutility, we applied the Albin complexity metric within the Fortis design repair\ntool, ranking system redesigns. This ranking reduced annotators' comprehension\ntime by 39\\%, suggesting that metrics emphasizing human factors can enhance\nformal design interpretability.", "comment": "Need to improve data annotation process in methodology section", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09759v1"}
{"id": "2506.09801", "title": "Investigating the Perception of Translational Shape-Changing Haptic Interfaces", "authors": ["Qihan Yang", "Xin Zhou", "Adam J. Spiers"], "summary": "Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.\nHowever, compared to more established stimulus modalities, such as vibration,\nthere is sparse literature on the perception of dynamic shapes. Furthermore,\nthe influence of properties such as grasp types and displacement\nmagnitude/direction has not been formally evaluated. This work attempts to\ninitiate a formal perceptual evaluation of SCHIs via a psychophysical user\nstudy involving a 1-DOF translational shape-changing interface that can move\nits body with 1.25-micrometer resolution. Participants completed a Method of\nConstant Stimulus study while holding the device with three different grasps.\nStimuli direction occurred both toward and away from the thumb, while the\nstandard stimuli varied between small (0.48 mm) and large (6 mm). Our results\nindicate that translational SCHIs should maximize the translation magnitude\nrather than the number of fingers in contact. We also demonstrated how to apply\nour findings to real-world applications via a simple 'paddle game', where we\ncompared conventional linear mapping with non-linear mapping derived from our\nperceptual experiment outcomes between the device position and its represented\nvalue. Results indicate that the non-linear mapping was more effective, with\nimproved error distribution. We hope this work inspires further formal\nperceptual investigation into other SCHI morphologies.", "comment": "7 pages, 8 figures. Accepted version to appear in: Proceedings of the\n  IEEE World Haptics Conference (WHC), 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09801v1"}
{"id": "2506.09685", "title": "Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error", "authors": ["Armin Gießler", "Albertus Johannes Malan", "Sören Hohmann"], "summary": "In this paper, we present a novel method for computing the optimal feedback\ngain of the infinite-horizon Linear Quadratic Regulator (LQR) problem via an\nordinary differential equation. We introduce a novel continuous-time Bellman\nerror, derived from the Hamilton-Jacobi-Bellman (HJB) equation, which\nquantifies the suboptimality of stabilizing policies and is parametrized in\nterms of the feedback gain. We analyze its properties, including its effective\ndomain, smoothness, coerciveness and show the existence of a unique stationary\npoint within the stability region. Furthermore, we derive a closed-form\ngradient expression of the Bellman error that induces a gradient flow. This\nconverges to the optimal feedback and generates a unique trajectory which\nexclusively comprises stabilizing feedback policies. Additionally, this work\nadvances interesting connections between LQR theory and Reinforcement Learning\n(RL) by redefining suboptimality of the Algebraic Riccati Equation (ARE) as a\nBellman error, adapting a state-independent formulation, and leveraging\nLyapunov equations to overcome the infinite-horizon challenge. We validate our\nmethod in a simulation and compare it to the state of the art.", "comment": "submitted to Conference on Decision and Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09685v1"}
{"id": "2506.09189", "title": "Fractional Fourier Sound Synthesis", "authors": ["Esteban Gutiérrez", "Rodrigo Cádiz", "Carlos Sing Long", "Frederic Font", "Xavier Serra"], "summary": "This paper explores the innovative application of the Fractional Fourier\nTransform (FrFT) in sound synthesis, highlighting its potential to redefine\ntime-frequency analysis in audio processing. As an extension of the classical\nFourier Transform, the FrFT introduces fractional order parameters, enabling a\ncontinuous interpolation between time and frequency domains and unlocking\nunprecedented flexibility in signal manipulation. Crucially, the FrFT also\nopens the possibility of directly synthesizing sounds in the alpha-domain,\nproviding a unique framework for creating timbral and dynamic characteristics\nunattainable through conventional methods. This work delves into the\nmathematical principles of the FrFT, its historical evolution, and its\ncapabilities for synthesizing complex audio textures. Through experimental\nanalyses, we showcase novel sound design techniques, such as alpha-synthesis\nand alpha-filtering, which leverage the FrFT's time-frequency rotation\nproperties to produce innovative sonic results. The findings affirm the FrFT's\nvalue as a transformative tool for composers, sound designers, and researchers\nseeking to push the boundaries of auditory creativity.", "comment": "Accepted to the International Computer Music Conference (ICMC) 2025\n  held in Boston, USA. 6 pages and 2 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09189v1"}
{"id": "2506.09554", "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators", "authors": ["Mayank Arya", "Yogesh Simmhan"], "summary": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide\nrange of domains, for tasks as diverse as code generation and robot navigation.\nWhile LLMs are usually served from cloud data centers, mission-critical and\nprivacy-sensitive applications may require local hosting of open LLM models.\nGiven the large GPU memory footprint needed for LLMs, edge accelerators such as\nNvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.\nHowever, the feasibility and performance of LLM inference on edge accelerators\nis under-explored. This study presents a detailed evaluation of LLM inference\non the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B\nparameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We\ninvestigate the impact of varying batch sizes, sequence lengths, and\nquantization levels on latency, throughput, and perplexity, and also explore\nvarious custom power modes on the Orin AGX to perform power and energy\nconsumption analysis. Our findings offer interesting insights on the trade-offs\nbetween efficiency, inference speed and resource use, e.g., increasing the\nsequence length causes a decrease in token throughput and quantization causes\nsmaller LLMs to be slower. These results can help optimize LLM serving on edge\naccelerators for practical applications.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09554v1"}
{"id": "2506.09439", "title": "Eigenvalue-Based Detection in MIMO Systems for Integrated Sensing and Communication", "authors": ["Alex Obando", "Saman Atapattu", "Prathapasinghe Dharmawansa", "Akram Hourani", "Kandeepan Sithamparanathan"], "summary": "This paper considers a MIMO Integrated Sensing and Communication (ISAC)\nsystem, where a base station simultaneously serves a MIMO communication user\nand a remote MIMO sensing receiver, without channel state information (CSI) at\nthe transmitter. Existing MIMO ISAC literature often prioritizes communication\nrate or detection probability, typically under constant false-alarm rate (CFAR)\nassumptions, without jointly analyzing detection reliability and communication\nconstraints. To address this gap, we adopt an eigenvalue-based detector for\nrobust sensing and use a performance metric, the total detection error, that\njointly captures false-alarm and missed-detection probabilities. We derive\nnovel closed-form expressions for both probabilities under the eigenvalue\ndetector, enabling rigorous sensing analysis. Using these expressions, we\nformulate and solve a joint power allocation and threshold optimization problem\nthat minimizes total detection error while meeting a minimum communication rate\nrequirement. Simulation results demonstrate that the proposed joint design\nsubstantially outperforms conventional CFAR-based schemes, highlighting the\nbenefits of power- and threshold-aware optimization in MIMO ISAC systems.", "comment": "10 pages, IEEE 102nd Vehicular Technology Conference (VTC2025-Fall),\n  Chengdu, China", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09439v1"}
{"id": "2506.09063", "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09063v1"}
{"id": "2506.09731", "title": "The Path is the Goal: a Study on the Nature and Effects of Shortest-Path Stability Under Perturbation of Destination", "authors": ["Giuliano Cornacchia", "Mirco Nanni"], "summary": "This work examines the phenomenon of path variability in urban navigation,\nwhere small changes in destination might lead to significantly different\nsuggested routes. Starting from an observation of this variability over the\ncity of Barcelona, we explore whether this is a localized or widespread\noccurrence and identify factors influencing path variability. We introduce the\nconcept of \"path stability\", a measure of how robust a suggested route is to\nminor destination adjustments, define a detailed experimentation process and\napply it across multiple cities worldwide. Our analysis shows that path\nstability is shaped by city-specific factors and trip characteristics, also\nidentifying some common patterns. Results reveal significant heterogeneity in\npath stability across cities, allowing for categorization into \"stable\" and\n\"unstable\" cities. These findings offer new insights for urban planning and\ntraffic management, highlighting opportunities for optimizing navigation\nsystems to enhance route consistency and urban mobility.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09731v1"}
{"id": "2506.09279", "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09279v1"}
{"id": "2506.09435", "title": "FNPF-SEM: A parallel spectral element model in Firedrake for fully nonlinear water wave simulations", "authors": ["Jens Visbech", "Anders Melander", "Allan Peter Engsig-Karup"], "summary": "We present a new parallel spectral element solver, FNPF-SEM, for simulating\nlinear and fully nonlinear potential flow-based water waves and their\ninteraction with offshore structures. The tool is designed as a general-purpose\nwave model for offshore engineering applications. Built within the open-source\nframework Firedrake, the new FNPF-SEM model is designed as a computational tool\ncapable of capturing both linear and nonlinear wave phenomena with high\naccuracy and efficiency, with support for high-order (spectral) finite\nelements. Additionally, Firedrake provides native support for MPI-based\nparallelism, allowing for efficient multi-CPU distributed computations needed\nfor large-scale simulations. We demonstrate the capabilities of the high-order\nspectral element model through h- and p-convergence studies, and weak and\nstrong scaling tests. Validation is performed against analytical solutions and\nexperimental data for several benchmark cases, including nonlinear high-order\nharmonic generation and linear and nonlinear wave interactions with a cylinder\nand a breakwater. The new FNPF-SEM model offers a numerical framework for\nsimulating wave propagation and wave-structure interactions, with the following\nkey features: i) the ability to represent complex geometries through flexible,\nunstructured finite element meshes; ii) reduced numerical diffusion and\ndispersion by using high-order polynomial expansions; and iii) scalability to\nfull- and large-scale simulations over long time periods through a parallel\nimplementation.", "comment": "23 pages, 19 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09435v1"}
{"id": "2506.09792", "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "comment": "Accepted by Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09792v1"}
{"id": "2506.09662", "title": "Empirical Quantification of Spurious Correlations in Malware Detection", "authors": ["Bianca Perasso", "Ludovico Lozza", "Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Fabio Roli"], "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09662v1"}
{"id": "2506.09485", "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09485v1"}
{"id": "2506.09229", "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "comment": "24 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09229v1"}
{"id": "2410.16222", "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2410.16222v2"}
{"id": "2506.09101", "title": "Feature Shift Localization Network", "authors": ["Míriam Barrabés", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "comment": "9 pages, 2 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09101v1"}
{"id": "2506.09900", "title": "Corrections to Friis noise factor formulas for cascade networks", "authors": ["Ankitha E Bangera"], "summary": "The signal-to-noise ratio of a multistage cascade network is often estimated\nusing the well-known Friis' formulas for noise factors (or the noise figures in\ndecibels). However, this article addresses the major errors in Friis' noise\nfactor formulas for higher stages. Additionally, we re-derive the correct\nformulas to calculate the stage-wise noise factors for cascade networks from\nthe basic definition of noise factors. We then present a comparison of our\nderived formulas with Friis' noise factor formulas. Contrary to Friis' formula,\nwe define the total noise factor of an n-stage cascade network as the product\nof its stage-wise noise factors. We further validate our derived formulas for a\ncascade network by correlating them with the expressions for a staircase\navalanche photodiode.", "comment": "Friis' noise factor formulas for cascade networks are often used to\n  derive other related formulas for devices involving cascade mechanism (eg.\n  staircase APDs). However, Friis' equations for higher stages are themselves\n  incorrect. This article points out the existing mistakes and re-derives the\n  correct formulas for a cascade network's noise factor (10 pages, 3 figures,\n  preprint under submission)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09900v1"}
{"id": "2506.09845", "title": "variability.dev: Towards an Online Toolbox for Feature Modeling", "authors": ["Tobias Heß", "Lukas Ostheimer", "Tobias Betz", "Simon Karrer", "Tim Jannik Schmidt", "Pierre Coquet", "Sean Semmler", "Thomas Thüm"], "summary": "The emergence of feature models as the default to model the variability in\nconfigurable systems fosters a rich diversity in applications, application\ndomains, and perspectives. Independent of their domain, modelers require to\nopen, view, edit, transform, save, and configure models as well as to\ncollaborate with others. However, at the time of writing, the top five results\nwhen googling ``Online Editor Feature Model'' point to editors that either have\nminimal functionality, are unmaintained or defunct, or require an offline\ninstallation, such as FeatureIDE. In this work we present a preview of our\nin-development online toolbox for feature modeling, variability.dev. In\nparticular, we showcase our collaborative feature-model editor and our online\nconfigurator both of which are built on top of the FeatureIDE library.", "comment": "Presented at 6th International Workshop on Languages for Modelling\n  Variability (MODEVAR'24) (arXiv:cs/2402.15511). 5 pages, 3 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09845v1"}
{"id": "2506.09968", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "comment": "14 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09968v1"}
{"id": "2506.09817", "title": "Enhanced V2X Communication Using Game-Theory Based Adaptive MAC Protocols", "authors": ["Dhrumil Bhatt", "Nirbhay Singhal"], "summary": "This paper presents an enhanced Vehicle-to-Everything (V2X) communication\nsystem featuring adaptive Medium Access Control (MAC) using game theory. Our\napproach integrates dynamic transmission power control, dynamic beacon rates,\ncontention window adaptation, and implicit acknowledgment mechanisms within a\nManhattan-like grid-based mobility scenario. Simulations are conducted in a\ncircular coverage area, incorporating refined signal propagation models and\nprobabilistic vehicle mobility with boundary reflection. The results\ndemonstrate effective beacon delivery with average delays under 0.35 s and\npacket loss rates less than 1% in high-density conditions specifically, with up\nto 80 vehicles operating within a 250 m radius. Key innovations include game\ntheory-based environment-aware transmission parameter adaptation and a scalable\ndesign suited for interference-prone V2X deployments.", "comment": "Accepted at the 16th ICCCNT", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09817v1"}
{"id": "2506.09206", "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09206v1"}
{"id": "2506.09823", "title": "Frosty for partial synchrony", "authors": ["Stephen Buttolph", "Andrew Lewis-Pye", "Kevin Sekniqi"], "summary": "Snowman is the consensus protocol used by blockchains on Avalanche. Recent\nwork has shown both how to augment Snowman with a `liveness' module called\n`Frosty' that protects against liveness attacks, and also how to modify Snowman\nso as to be consistent in partial synchrony. Since Frosty assumes (a strong\nform of) synchrony, the aim of this note is to show how to modify Frosty to\ndeal with the partially synchronous version of Snowman.", "comment": "arXiv admin note: substantial text overlap with arXiv:2404.14250,\n  arXiv:2501.15904", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09823v1"}
{"id": "2506.09649", "title": "Mathematical proof of errors in Capasso's excess noise factor formula for an n-step staircase multiplier", "authors": ["Ankitha E Bangera"], "summary": "Solid-state devices such as multistep staircase avalanche photodiodes (APDs)\nare analogues to the photomultiplier tubes and are considered as a\ncascade-amplifier. The major source of internal noise in these APDs is due to\nthe randomness in their stepwise impact ionization. Recent literature on\nstaircase APDs by research groups such as Campbell and co-workers have reported\nthe theoretical estimates of total excess noise factors using Capasso's excess\nnoise factor formula. This formula is based on Friis' total noise factor\nformula for cascade networks. This article proves that Capasso's formula for a\nstaircase APD, erroneously considers the power gains in Friis' total noise\nfactor formula as the gains.", "comment": "Recent research articles that have used Capasso's formula have been\n  published in famous research journals, which include 'Nature Photonics\n  (2021)' and 'Optics (2023).' It is important that the research community is\n  aware that Capasso's formula and its equivalent forms are incorrect. This\n  article points out the errors in the formulas (5 pages, No figures, preprint\n  is under submission)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09649v1"}
{"id": "2506.09510", "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09510v1"}
{"id": "2506.09746", "title": "TikTok's Research API: Problems Without Explanations", "authors": ["Carlos Entrena-Serrano", "Martin Degeling", "Salvatore Romano", "Raziye Buse Çetin"], "summary": "Following the Digital Services Act of 2023, which requires Very Large Online\nPlatforms (VLOPs) and Very Large Online Search Engines (VLOSEs) to facilitate\ndata accessibility for independent research, TikTok augmented its Research API\naccess within Europe in July 2023. This action was intended to ensure\ncompliance with the DSA, bolster transparency, and address systemic risks.\nNonetheless, research findings reveal that despite this expansion, notable\nlimitations and inconsistencies persist within the data provided. Our\nexperiment reveals that the API fails to provide metadata for one in eight\nvideos provided through data donations, including official TikTok videos,\nadvertisements, videos from China, and content from specific accounts, without\nan apparent reason. The API data is incomplete, making it unreliable when\nworking with data donations, a prominent methodology for algorithm audits and\nresearch on platform accountability. To monitor the functionality of the API\nand eventual fixes implemented by TikTok, we publish a dashboard with a daily\ncheck of the availability of 10 videos that were not retrievable in the last\nmonth. The video list includes very well-known accounts, notably that of Taylor\nSwift. The current API lacks the necessary capabilities for thorough\nindependent research and scrutiny. It is crucial to support and safeguard\nresearchers who utilize data scraping to independently validate the platform's\ndata quality.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09746v1"}
{"id": "2506.09414", "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "comment": "13 pages, 7 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09414v1"}
{"id": "2506.09519", "title": "Segregated Runge-Kutta schemes for the time integration of the incompressible Navier-Stokes equations in presence of pressure stabilization", "authors": ["Pavel Bakhvalov"], "summary": "Segregated Runge-Kutta (SRK) schemes are time integration methods for the\nincompressible Navier-Stokes equations. In this approach, convection and\ndiffusion can be independently treated either explicitly or implicitly, which\nin particular allows to construct implicit-explicit (IMEX) methods. Original\nSRK schemes (Colomes, Badia, IJNME, 2015) are designed for finite-element\nmethods that satisfy the inf-sup condition. In this paper, the idea of SRK\nschemes is generalized to spatial discretizations with pressure stabilization.\nIn the numerical experiments, SRK schemes are demonstrated with both\nfinite-difference and finite element spatial discretizations. Numerical results\nshow that one of the SRK schemes outperforms the third-order multistep\nprojection-based method in terms of accuracy while preserving the computational\ncosts.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09519v1"}
{"id": "2506.09874", "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09874v1"}
{"id": "2506.09719", "title": "On the Virtues of Information Security in the UK Climate Movement", "authors": ["Mikaela Brough", "Rikke Bjerg Jensen", "Martin R. Albrecht"], "summary": "We report on an ethnographic study with members of the climate movement in\nthe United Kingdom (UK). We conducted participant observation and interviews at\nprotests and in various activist settings. Reporting on the findings as they\nrelate to information security, we show that members of the UK climate movement\nwrestled with (i) a fundamental tension between openness and secrecy; (ii)\ntensions between autonomy and collective interdependence in\ninformation-security decision-making; (iii) conflicting activist ideals that\nshape security discourses; and (iv) pressures from different social gazes --\nfrom each other, from people outside the movement and from their adversaries.\nOverall, our findings shed light on the social complexities of\ninformation-security research in activist settings and provoke methodological\nquestions about programmes that aim to design for activists.", "comment": "To appear at the USENIX Security Symposium 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09719v1"}
{"id": "2506.09491", "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09491v1"}
{"id": "2506.09237", "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09237v1"}
{"id": "2506.08672", "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "comment": "22 pages, 10 figures, 8 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08672v1"}
{"id": "2506.09104", "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "comment": "Preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09104v1"}
{"id": "2506.09873", "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09873v1"}
{"id": "2506.09054", "title": "Particle Builder -- Learn about the Standard Model while playing against an AI", "authors": ["Mohammad Attar", "Andrew Carse", "Yeming Chen", "Thomas Green", "Jeong-Yeon Ha", "Yanbai Jin", "Amy McWilliams", "Theirry Panggabean", "Zhengyu Peng", "Lujin Sun", "Jing Ru", "Jiacheng She", "Jialin Wang", "Zilun Wei", "Jiayuan Zhu", "Lachlan McGinness"], "summary": "Particle Builder Online is a web-based education game designed for high\nschool physics students. Students can play against an AI opponent or peers to\nfamiliarise themselves with the Standard Model of Particle Physics. The game is\naimed at a high school level and tailored to the International Baccalaureate\nand the Australian Curriculum. Students from four schools in Canberra took\npre/post-tests and a survey while completing a lesson where they played\nParticle Builder. Students' understanding of particle physics concepts improved\nsignificantly. Students found the game more enjoyable and effective than\nregular classroom lessons.", "comment": "This demo has been accepted for presentation at the AIED 2025\n  Interactive Events Track", "cate": "physics.ed-ph", "url": "http://arxiv.org/abs/2506.09054v1"}
{"id": "2506.09164", "title": "On Polynomial Stochastic Barrier Functions: Bernstein Versus Sum-of-Squares", "authors": ["Peter Amorese", "Morteza Lahijanian"], "summary": "Stochastic Barrier Functions (SBFs) certify the safety of stochastic systems\nby formulating a functional optimization problem, which state-of-the-art\nmethods solve using Sum-of-Squares (SoS) polynomials. This work focuses on\npolynomial SBFs and introduces a new formulation based on Bernstein polynomials\nand provides a comparative analysis of its theoretical and empirical\nperformance against SoS methods. We show that the Bernstein formulation leads\nto a linear program (LP), in contrast to the semi-definite program (SDP)\nrequired for SoS, and that its relaxations exhibit favorable theoretical\nconvergence properties. However, our empirical results reveal that the\nBernstein approach struggles to match SoS in practical performance, exposing an\nintriguing gap between theoretical advantages and real-world feasibility.", "comment": "To appear in IEEE Control Systems Letters (L-CSS) 2025", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09164v1"}
{"id": "2506.09218", "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "authors": ["Bruno Ferenc Šegedin"], "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09218v1"}
{"id": "2506.09199", "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "comment": "21 pages, 12 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09199v1"}
{"id": "2506.09726", "title": "Don't be Afraid of Cell Complexes! An Introduction from an Applied Perspective", "authors": ["Josef Hoppe", "Vincent P. Grande", "Michael T. Schaub"], "summary": "Cell complexes (CCs) are a higher-order network model deeply rooted in\nalgebraic topology that has gained interest in signal processing and network\nscience recently. However, while the processing of signals supported on CCs can\nbe described in terms of easily-accessible algebraic or combinatorial notions,\nthe commonly presented definition of CCs is grounded in abstract concepts from\ntopology and remains disconnected from the signal processing methods developed\nfor CCs. In this paper, we aim to bridge this gap by providing a simplified\ndefinition of CCs that is accessible to a wider audience and can be used in\npractical applications. Specifically, we first introduce a simplified notion of\nabstract regular cell complexes (ARCCs). These ARCCs only rely on notions from\nalgebra and can be shown to be equivalent to regular cell complexes for most\npractical applications. Second, using this new definition we provide an\naccessible introduction to (abstract) cell complexes from a perspective of\nnetwork science and signal processing. Furthermore, as many practical\napplications work with CCs of dimension 2 and below, we provide an even simpler\ndefinition for this case that significantly simplifies understanding and\nworking with CCs in practice.", "comment": "Preprint version, comments welcome!", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09726v1"}
{"id": "2506.09650", "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09650v1"}
{"id": "2506.09771", "title": "Where Journalism Silenced Voices: Exploring Discrimination in the Representation of Indigenous Communities in Bangladesh", "authors": ["Abhijit Paul", "Adity Khisa", "Zarif Masud", "Sharif Md. Abdullah", "Ahmedul Kabir", "Shebuti Rayana"], "summary": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09771v1"}
{"id": "2506.09645", "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "comment": "32 pages, 28 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09645v1"}
{"id": "2506.09584", "title": "Extensive Database of Spatial Ballistic Captures with Application to Lunar Trailblazer", "authors": ["Lorenzo Anoè", "Roberto Armellin", "Gregory Lantoine", "Claudio Bombardelli"], "summary": "For low-energy missions to the Moon and beyond, Ballistic Capture has proven\nto be a valuable technique for enabling orbital insertion while alleviating\npropulsion system requirements. This approach offers two key advantages. First,\nit extends the insertion window, allowing multiple maneuver opportunities to\nmitigate potential failures at the nominal insertion point. Second, it enables\nthe required insertion maneuver to be distributed across multiple revolutions,\nreducing propulsion system constraints in terms of single-burn thrust. Prior\nresearch introduced the concept of Energy Transition Domain to support the\ncreation of a comprehensive database of Ballistic Captures in the planar\nCircular Restricted Three-Body Problem. However, to apply these trajectories to\na real mission scenario, a three-dimensional, spatial analysis and transition\nto an ephemeris model are necessary. This paper first extends the Energy\nTransition Domain framework to the spatial case, constructing an extensive\ndatabase of spatial Ballistic Captures. Then, using Lunar Trailblazer as a case\nstudy, a subset of the trajectories is filtered using a mission-specific\ndistance metric, and transitioned into an ephemeris model. Finally, interesting\nfeatures of this subset are analyzed, and sample high-fidelity trajectories are\nselected as potential backup options for Lunar Trailblazer.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09584v1"}
{"id": "2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09175v1"}
{"id": "2506.09807", "title": "Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice", "authors": ["Junqing Zhang", "Francesco Ardizzon", "Mattia Piana", "Guanxiong Shen", "Stefano Tomasin"], "summary": "The identification of the devices from which a message is received is part of\nsecurity mechanisms to ensure authentication in wireless communications.\nConventional authentication approaches are cryptography-based, which, however,\nare usually computationally expensive and not adequate in the Internet of\nThings (IoT), where devices tend to be low-cost and with limited resources.\nThis paper provides a comprehensive survey of physical layer-based device\nfingerprinting, which is an emerging device authentication for wireless\nsecurity. In particular, this article focuses on hardware impairment-based\nidentity authentication and channel features-based authentication. They are\npassive techniques that are readily applicable to legacy IoT devices. Their\nintrinsic hardware and channel features, algorithm design methodologies,\napplication scenarios, and key research questions are extensively reviewed\nhere. The remaining research challenges are discussed, and future work is\nsuggested that can further enhance the physical layer-based device\nfingerprinting.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09807v1"}
{"id": "2506.09494", "title": "Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications", "authors": ["Alberto San-Miguel-Tello", "Gennaro Scarati", "Alejandro Hernández", "Mario Cavero-Vidal", "Aakash Maroti", "Néstor García"], "summary": "This paper presents advances on the Universal Manipulation Interface (UMI), a\nlow-cost hand-held gripper for robot Learning from Demonstration (LfD), for\ncomplex in-the-wild scenarios found in agricultural settings. The focus is on\nimproving the acquisition of suitable samples with minimal additional setup.\nFirstly, idle times and user's cognitive load are reduced through the\nextraction of individual samples from a continuous demonstration considering\ntask events. Secondly, reliability on the generation of task sample's\ntrajectories is increased through the combination on-board inertial\nmeasurements and external visual marker localization usage using Extended\nKalman Filtering (EKF). Results are presented for a fruit harvesting task,\noutperforming the default pipeline.", "comment": "7 pages, 2 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09494v1"}
{"id": "2506.09278", "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "comment": "Project Page: https://uniflowmatch.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09278v1"}
{"id": "2506.09052", "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "comment": "7 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09052v1"}
{"id": "2506.09105", "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09105v1"}
{"id": "2506.09929", "title": "Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation", "authors": ["Scott Schnelle", "Francesca Favaro", "Laura Fraade-Blanar", "David Wichner", "Holland Broce", "Justin Miranda"], "summary": "As Automated Driving Systems (ADS) technology advances, ensuring safety and\npublic trust requires robust assurance frameworks, with safety cases emerging\nas a critical tool toward such a goal. This paper explores an approach to\nassess how a safety case is supported by its claims and evidence, toward\nestablishing credibility for the overall case. Starting from a description of\nthe building blocks of a safety case (claims, evidence, and optional\nformat-dependent entries), this paper delves into the assessment of support of\neach claim through the provided evidence. Two domains of assessment are\noutlined for each claim: procedural support (formalizing process specification)\nand implementation support (demonstrating process application). Additionally,\nan assessment of evidence status is also undertaken, independently from the\nclaims support. Scoring strategies and evaluation guidelines are provided,\nincluding detailed scoring tables for claim support and evidence status\nassessment. The paper further discusses governance, continual improvement, and\ntiming considerations for safety case assessments. Reporting of results and\nfindings is contextualized within its primary use for internal decision-making\non continual improvement efforts. The presented approach builds on state of the\nart auditing practices, but specifically tackles the question of judging the\ncredibility of a safety case. While not conclusive on its own, it provides a\nstarting point toward a comprehensive \"Case Credibility Assessment\" (CCA),\nstarting from the evaluation of the support for each claim (individually and in\naggregate), as well as every piece of evidence provided. By delving into the\ntechnical intricacies of ADS safety cases, this work contributes to the ongoing\ndiscourse on safety assurance and aims to facilitate the responsible\nintegration of ADS technology into society.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09929v1"}
{"id": "2506.09073", "title": "Understanding and Improving Data Repurposing", "authors": ["J. Parsons", "R. Lukyanenko", "B. Greenwood", "C. Cooper"], "summary": "We live in an age of unprecedented opportunities to use existing data for\ntasks not anticipated when those data were collected, resulting in widespread\ndata repurposing. This commentary defines and maps the scope of data\nrepurposing to highlight its importance for organizations and society and the\nneed to study data repurposing as a frontier of data management. We explain how\nrepurposing differs from original data use and data reuse and then develop a\nframework for data repurposing consisting of concepts and activities for\nadapting existing data to new tasks. The framework and its implications are\nillustrated using two examples of repurposing, one in healthcare and one in\ncitizen science. We conclude by suggesting opportunities for research to better\nunderstand data repurposing and enable more effective data repurposing\npractices.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09073v1"}
{"id": "2506.09383", "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09383v1"}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1"}
{"id": "2506.09226", "title": "Terabyte-Scale Analytics in the Blink of an Eye", "authors": ["Bowen Wu", "Wei Cui", "Carlo Curino", "Matteo Interlandi", "Rathijit Sen"], "summary": "For the past two decades, the DB community has devoted substantial research\nto take advantage of cheap clusters of machines for distributed data analytics\n-- we believe that we are at the beginning of a paradigm shift. The scaling\nlaws and popularity of AI models lead to the deployment of incredibly powerful\nGPU clusters in commercial data centers. Compared to CPU-only solutions, these\nclusters deliver impressive improvements in per-node compute, memory bandwidth,\nand inter-node interconnect performance. In this paper, we study the problem of\nscaling analytical SQL queries on distributed clusters of GPUs, with the stated\ngoal of establishing an upper bound on the likely performance gains. To do so,\nwe build a prototype designed to maximize performance by leveraging ML/HPC best\npractices, such as group communication primitives for cross-device data\nmovements. This allows us to conduct thorough performance experimentation to\npoint our community towards a massive performance opportunity of at least\n60$\\times$. To make these gains more relatable, before you can blink twice, our\nsystem can run all 22 queries of TPC-H at a 1TB scale factor!", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.09226v1"}
{"id": "2506.09732", "title": "End-to-End Dynamic Metasurface Antenna Wireless System: Prototype, Opportunities, and Challenges", "authors": ["François Yven", "Jean Tapie", "Jérôme Sol", "Philipp del Hougne"], "summary": "Dynamic metasurface antennas (DMAs) are a promising hybrid analog/digital\nbeamforming technology to realize next-generation wireless systems with low\ncost, footprint, and power consumption. The research on DMA-empowered wireless\nsystems is still at an early stage, mostly limited to theoretical studies under\nsimplifying assumptions on the one hand and a few antenna-level experiments on\nthe other hand. Substantial knowledge gaps arise from the lack of complete\nend-to-end DMA-empowered wireless system prototypes. In addition, recently\nunveiled benefits of strong inter-element mutual coupling (MC) in DMAs remain\nuntapped. Here, we demonstrate a K-band prototype of an end-to-end wireless\nsystem based on a DMA with strong inter-element MC. To showcase the flexible\ncontrol over the DMA's radiation pattern, we present an experimental case study\nof simultaneously steering a beam to a desired transmitter and a null to an\nundesired jammer, achieving up to 43~dB discrimination. Using software-defined\nradios, we transmit and receive QPSK OFDM waveforms to evaluate the bit error\nrate. We also discuss algorithmic and technological challenges associated with\nenvisioned future evolutions of our end-to-end testbed and real-life DMA-based\nwireless systems.", "comment": "7 pages, 4 figures, submitted to an IEEE Journal", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09732v1"}
{"id": "2506.09789", "title": "Delegations as Adaptive Representation Patterns: Rethinking Influence in Liquid Democracy", "authors": ["Davide Grossi", "Andreas Nitsche"], "summary": "Liquid democracy is a mechanism for the division of labor in decision-making\nthrough the transitive delegation of influence. In essence, all individuals\npossess the autonomy to determine the issues with which they will engage\ndirectly, while for other matters, they may appoint a representative of their\nchoosing. So far, the literature has studied the delegation structures emerging\nin liquid democracy as static. As a result, transitivity defined as the\ncapacity to transfer acquired authority to another entity, has been identified\nas a concern as it would be conducive to unrestrained accumulation of power.\n  Focusing on the implementation of liquid democracy supported by the\nLiquidFeedback software, we propose a novel approach to assessing the influence\nof voting nodes in a transitive delegation graph, taking into account the\nprocess nature of real-world liquid democracy in which delegation and voting\nare distinct and increasingly independent activities. By introducing a novel\nmodel of delegations in liquid democracy, we show how transitivity may in fact\ncontribute to an effective regulation of deliberation influence and\ndecision-making power. While maintaining the one-person, one-vote paradigm for\nall votes cast, the anticipated influence of an agent, to the extent it is\nstemming from transitivity, experiences a precipitous decline following an\nexponential trajectory.\n  In general, it is our objective to move the first steps towards a rigorous\nanalysis of liquid democracy as an adaptive democratic representation process.\nThe adaptivity aspect of liquid democracy has not yet been explored within the\nexisting academic literature despite it being, we believe, one of its most\nimportant features. We therefore also outline a research agenda focusing on\nthis aspect of liquid democracy.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09789v1"}
{"id": "2506.09679", "title": "Geometric flow regularization in latent spaces for smooth dynamics with the efficient variations of curvature", "authors": ["Andrew Gracyk"], "summary": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity.", "comment": "First version", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09679v1"}
{"id": "2506.09218", "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "authors": ["Bruno Ferenc Šegedin"], "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09218v1"}
{"id": "2506.09950", "title": "Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher", "authors": ["La Scala Roberto", "Sharwan Kumar Tiwari"], "summary": "The multistep solving strategy consists in a divide-and-conquer approach:\nwhen a multivariate polynomial system is computationally infeasible to solve\ndirectly, one variable is assigned over the elements of the base finite field,\nand the procedure is recursively applied to the resulting simplified systems.\nIn a previous work by the same authors (among others), this approach proved\neffective in the algebraic cryptanalysis of the Trivium cipher. In this paper,\nwe present a new implementation of the corresponding algorithm based on a\nDepth-First Search strategy, along with a novel complexity analysis leveraging\ntree structures. We further introduce the notion of an \"oracle function\" as a\ngeneral predictive tool for deciding whether the evaluation of a new variable\nis necessary to simplify the current polynomial system. This notion allows us\nto unify all previously proposed variants of the multistep strategy, including\nthe classical hybrid approach, by appropriately selecting the oracle function.\nFinally, we apply the multistep solving strategy to the cryptanalysis of the\nlow-latency block cipher Aradi, recently introduced by the NSA. We present the\nfirst full round algebraic attack, raising concerns about the cipher's actual\nsecurity with respect to its key length.", "comment": "19 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09950v1"}
{"id": "2506.09548", "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "comment": "Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09548v1"}
{"id": "2506.09299", "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "comment": "6 Pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09299v1"}
{"id": "2506.09061", "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "comment": "4 figures, 7 pages, IEEE conference template", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09061v1"}
{"id": "2506.09108", "title": "SensorLM: Learning the Language of Wearable Sensors", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09108v1"}
{"id": "2506.09938", "title": "Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies", "authors": ["Aaditaa Vashisht", "Rekha B S"], "summary": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09938v1"}
{"id": "2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09160v1"}
{"id": "2506.09766", "title": "Vulnerability-Based Optimal Grid Defense Strategies for Enhancing Cyber-Physical Energy System Resilience", "authors": ["Eric Tönges", "Philipp Härtel", "Martin Braun"], "summary": "An approach is proposed to identify optimal asset protection strategies based\non vulnerability assessment outcomes. Traditional bilevel attacker-defender\nmodels emphasize worstcase scenarios but offer limited defensive guidance. In\ncontrast, trilevel models introduce high computational complexity and rely on\nfixed network configurations. The proposed critical-components method leverages\nvulnerability assessment results to determine protection strategies,\neffectively outsourcing the upper-level defense decision. This enables\nadaptability to diverse network topologies, assessment techniques, and\ncyber-physical energy systems without the overhead of multi-level optimization.\nCase studies demonstrate the potential for improved system resilience across\nvarying operational conditions.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09766v1"}
{"id": "2506.09375", "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09375v1"}
{"id": "2506.09438", "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09438v1"}
{"id": "2506.09773", "title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "authors": ["Taulant Koka", "Manolis C. Tsakiris", "Benjamín Béjar Haro", "Michael Muma"], "summary": "Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.", "comment": "Accepted to ICASSP 2025. \\copyright 2025 IEEE. Personal use of this\n  material is permitted", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09773v1"}
{"id": "2506.09947", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09947v1"}
{"id": "2506.09687", "title": "Matrix best approximation in the spectral norm", "authors": ["Vance Faber", "Jörg Liesen", "Petr Tichý"], "summary": "We derive, similar to Lau and Riha, a matrix formulation of a general best\napproximation theorem of Singer for the special case of spectral approximations\nof a given matrix from a given subspace. Using our matrix formulation we\ndescribe the relation of the spectral approximation problem to semidefinite\nprogramming, and we present a simple MATLAB code to solve the problem\nnumerically. We then obtain geometric characterizations of spectral\napproximations that are based on the $k$-dimensional field of $k$ matrices,\nwhich we illustrate with several numerical examples. The general spectral\napproximation problem is a min-max problem, whose value is bounded from below\nby the corresponding max-min problem. Using our geometric characterizations of\nspectral approximations, we derive several necessary and sufficient as well as\nsufficient conditions for equality of the max-min and min-max values. Finally,\nwe prove that the max-min and min-max values are always equal when we\n``double'' the problem. Several results in this paper generalize results that\nhave been obtained in the convergence analysis of the GMRES method for solving\nlinear algebraic systems.", "comment": "24 pages, 3 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09687v1"}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1"}
{"id": "2506.09956", "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "João Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09956v1"}
{"id": "2506.09552", "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "comment": "Preprint, Journal of Intelligent & Robotic Systems", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09552v1"}
{"id": "2506.09300", "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09300v1"}
{"id": "2506.09065", "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "comment": "6 pages, 8 figures, and 1 table", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09065v1"}
{"id": "2506.09110", "title": "CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model", "authors": ["Jingying Ma", "Feng Wu", "Qika Lin", "Yucheng Xing", "Chenyu Liu", "Ziyu Jia", "Mengling Feng"], "summary": "Electroencephalography (EEG) provides real-time insights into brain activity\nand is widely used in neuroscience. However, variations in channel\nconfigurations, sequence lengths, and task objectives limit the transferability\nof traditional task-specific models. Although recent EEG foundation models\n(EFMs) aim to learn generalizable representations, they struggle with limited\nheterogeneous representation capacity and inefficiency in capturing multi-scale\nbrain dependencies. To address these challenges, we propose CodeBrain, an\nefficient EFM structurally aligned with brain organization, trained in two\nstages. (1) We introduce a TFDual-Tokenizer that independently tokenizes\nheterogeneous temporal and frequency components, enabling a quadratic expansion\nof the discrete representation space. This also offers a degree of\ninterpretability through cross-domain token analysis. (2) We propose the\nEEGSSM, which combines a structured global convolution architecture and a\nsliding window attention mechanism to jointly model sparse long-range and local\ndependencies. Unlike fully connected Transformer models, EEGSSM better reflects\nthe brain's small-world topology and efficiently captures EEG's inherent\nmulti-scale structure. EEGSSM is trained with a masked self-supervised learning\nobjective to predict token indices obtained in TFDual-Tokenizer. Comprehensive\nexperiments on 10 public EEG datasets demonstrate the generalizability of\nCodeBrain with linear probing. By offering biologically informed and\ninterpretable EEG modeling, CodeBrain lays the foundation for future\nneuroscience research. Both code and pretraining weights will be released in\nthe future version.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09110v1"}
{"id": "2506.09790", "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09790v1"}
{"id": "2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09420v1"}
{"id": "2506.09776", "title": "A Saddle Point Algorithm for Robust Data-Driven Factor Model Problems", "authors": ["Shabnam Khodakaramzadeh", "Soroosh Shafiee", "Gabriel de Albuquerque Gleizer", "Peyman Mohajerin Esfahani"], "summary": "We study the factor model problem, which aims to uncover low-dimensional\nstructures in high-dimensional datasets. Adopting a robust data-driven\napproach, we formulate the problem as a saddle-point optimization. Our primary\ncontribution is a general first-order algorithm that solves this reformulation\nby leveraging a linear minimization oracle (LMO). We further develop\nsemi-closed form solutions (up to a scalar) for three specific LMOs,\ncorresponding to the Frobenius norm, Kullback-Leibler divergence, and Gelbrich\n(aka Wasserstein) distance. The analysis includes explicit quantification of\nthese LMOs' regularity conditions, notably the Lipschitz constants of the dual\nfunction, whthich govern the algorithm's convergence performance. Numerical\nexperiments confirm our meod's effectiveness in high-dimensional settings,\noutperforming standard off-the-shelf optimization solvers.", "comment": "Submitted to Automatica", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09776v1"}
{"id": "2506.09448", "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "comment": "Accepted to Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09448v1"}
{"id": "2506.09660", "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "authors": ["Baran Can Gül", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09660v1"}
{"id": "2506.09855", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "comment": "6 pages, 5 figures, PIMRC conference", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09855v1"}
{"id": "2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "comment": "in French language", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09089v1"}
{"id": "2506.09721", "title": "Generative Models for Parameter Space Reduction applied to Reduced Order Modelling", "authors": ["Guglielmo Padula", "Gianluigi Rozza"], "summary": "Solving and optimising Partial Differential Equations (PDEs) in geometrically\nparameterised domains often requires iterative methods, leading to high\ncomputational and time complexities. One potential solution is to learn a\ndirect mapping from the parameters to the PDE solution. Two prominent methods\nfor this are Data-driven Non-Intrusive Reduced Order Models (DROMs) and\nParametrised Physics Informed Neural Networks (PPINNs). However, their accuracy\ntends to degrade as the number of geometric parameters increases. To address\nthis, we propose adopting Generative Models to create new geometries,\neffectively reducing the number of parameters, and improving the performance of\nDROMs and PPINNs. The first section briefly reviews the general theory of\nGenerative Models and provides some examples, whereas the second focusses on\ntheir application to geometries with fixed or variable points, emphasising\ntheir integration with DROMs and PPINNs. DROMs trained on geometries generated\nby these models demonstrate enhanced accuracy due to reduced parameter\ndimensionality. For PPINNs, we introduce a methodology that leverages\nGenerative Models to reduce the parameter dimensions and improve convergence.\nThis approach is tested on a Poisson equation defined over deformed Stanford\nBunny domains.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09721v1"}
{"id": "2506.09375", "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09375v1"}
{"id": "2410.16222", "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2410.16222v2"}
{"id": "2506.09581", "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "David Sobrín-Hidalgo", "Ángel Manuel Guerrero-Higueras", "Vicente MatellÁn-Olivera"], "summary": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics.", "comment": "10 pages, 4 figures, Submitted to 3rd edition of the Workshop on\n  Ontologies and Standards for Robotics and Automation (WOSRA) at ICRA 2024", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09581v1"}
{"id": "2506.09327", "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09327v1"}
{"id": "2506.09066", "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09066v1"}
{"id": "2506.09114", "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09114v1"}
{"id": "2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "comment": "5 pages, 2 figures", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09707v1"}
{"id": "2506.09876", "title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization", "authors": ["Jisheng Xu", "Ding Lin", "Pangkit Fong", "Chongrong Fang", "Xiaoming Duan", "Jianping He"], "summary": "This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09876v1"}
{"id": "2506.09487", "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09487v1"}
{"id": "2506.09870", "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "authors": ["Maximilian Egger", "Rawad Bitar"], "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09870v1"}
{"id": "2506.09900", "title": "Corrections to Friis noise factor formulas for cascade networks", "authors": ["Ankitha E Bangera"], "summary": "The signal-to-noise ratio of a multistage cascade network is often estimated\nusing the well-known Friis' formulas for noise factors (or the noise figures in\ndecibels). However, this article addresses the major errors in Friis' noise\nfactor formulas for higher stages. Additionally, we re-derive the correct\nformulas to calculate the stage-wise noise factors for cascade networks from\nthe basic definition of noise factors. We then present a comparison of our\nderived formulas with Friis' noise factor formulas. Contrary to Friis' formula,\nwe define the total noise factor of an n-stage cascade network as the product\nof its stage-wise noise factors. We further validate our derived formulas for a\ncascade network by correlating them with the expressions for a staircase\navalanche photodiode.", "comment": "Friis' noise factor formulas for cascade networks are often used to\n  derive other related formulas for devices involving cascade mechanism (eg.\n  staircase APDs). However, Friis' equations for higher stages are themselves\n  incorrect. This article points out the existing mistakes and re-derives the\n  correct formulas for a cascade network's noise factor (10 pages, 3 figures,\n  preprint under submission)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09900v1"}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software.", "comment": "Accepted at CSCW 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09216v1"}
{"id": "2506.09830", "title": "Machine Learning-based quadratic closures for non-intrusive Reduced Order Models", "authors": ["Gabriele Codega", "Anna Ivagnes", "Nicola Demo", "Gianluigi Rozza"], "summary": "In the present work, we introduce a data-driven approach to enhance the\naccuracy of non-intrusive Reduced Order Models (ROMs). In particular, we focus\non ROMs built using Proper Orthogonal Decomposition (POD) in an under-resolved\nand marginally-resolved regime, i.e. when the number of modes employed is not\nenough to capture the system dynamics. We propose a method to re-introduce the\ncontribution of neglected modes through a quadratic correction term, given by\nthe action of a quadratic operator on the POD coefficients. Differently from\nthe state-of-the-art methodologies, where the operator is learned via\nleast-squares optimisation, we propose to parametrise the operator by a\nMulti-Input Operators Network (MIONet). This way, we are able to build models\nwith higher generalisation capabilities, where the operator itself is\ncontinuous in space -- thus agnostic of the domain discretisation -- and\nparameter-dependent. We test our model on two standard benchmarks in fluid\ndynamics and show that the correction term improves the accuracy of standard\nPOD-based ROMs.", "comment": "18 pages, 8 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09830v1"}
{"id": "2506.09549", "title": "A Study on Speech Assessment with Visual Cues", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Nasir Saleem", "Amir Hussain", "Hsin-Min Wang", "Yu Tsao"], "summary": "Non-intrusive assessment of speech quality and intelligibility is essential\nwhen clean reference signals are unavailable. In this work, we propose a\nmultimodal framework that integrates audio features and visual cues to predict\nPESQ and STOI scores. It employs a dual-branch architecture, where spectral\nfeatures are extracted using STFT, and visual embeddings are obtained via a\nvisual encoder. These features are then fused and processed by a CNN-BLSTM with\nattention, followed by multi-task learning to simultaneously predict PESQ and\nSTOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND\ncorpus, show that our model outperforms the audio-only baseline. Under seen\nnoise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%\n(0.7403->0.8253) for STOI. These results highlight the effectiveness of\nincorporating visual cues in enhancing the accuracy of non-intrusive speech\nassessment.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09549v1"}
{"id": "2506.09227", "title": "SoK: Machine Unlearning for Large Language Models", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09227v1"}
{"id": "2506.09583", "title": "VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "Vicente Matellán-Olivera"], "summary": "Localization plays a crucial role in the navigation capabilities of\nautonomous robots, and while indoor environments can rely on wheel odometry and\n2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,\npresent unique challenges that necessitate real-time localization and\nconsistent mapping. Addressing this need, this paper introduces the VAULT\nprototype, a ROS 2-based mobile mapping system (MMS) that combines various\nsensors to enable robust outdoor and indoor localization. The proposed solution\nharnesses the power of Global Navigation Satellite System (GNSS) data,\nvisual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the\nExtended Kalman Filter (EKF) to generate reliable 3D odometry. To further\nenhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting\nin the creation of a comprehensive 3D point cloud map. By leveraging these\nsensor technologies and advanced algorithms, the prototype offers a\ncomprehensive solution for outdoor localization in autonomous mobile robots,\nenabling them to navigate and map their surroundings with confidence and\nprecision.", "comment": "15 pages, 5 figures, Submitted to WAF 2023: Workshop de Agentes\n  Fisicos", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09583v1"}
{"id": "2506.09343", "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "comment": "CVPR 2025 Highlight", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09343v1"}
{"id": "2506.09067", "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09067v1"}
{"id": "2506.09163", "title": "Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes", "authors": ["Daniel Jenson", "Jhonathan Navott", "Piotr Grynfelder", "Mengyan Zhang", "Makkunda Sharma", "Elizaveta Semenova", "Seth Flaxman"], "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nWhile early architectures were developed primarily as a scalable alternative to\nGaussian Processes (GPs), modern NPs tackle far more complex and data hungry\napplications spanning geology, epidemiology, climate, and robotics. These\napplications have placed increasing pressure on the scalability of these\nmodels, with many architectures compromising accuracy for scalability. In this\npaper, we demonstrate that this tradeoff is often unnecessary, particularly\nwhen modeling fully or partially translation invariant processes. We propose a\nversatile new architecture, the Biased Scan Attention Transformer Neural\nProcess (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),\ngroup-invariant attention biases, and memory-efficient Biased Scan Attention\n(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models\nwhile often training in a fraction of the time, (2) exhibit translation\ninvariance, enabling learning at multiple resolutions simultaneously, (3)\ntransparently model processes that evolve in both space and time, (4) support\nhigh dimensional fixed effects, and (5) scale gracefully -- running inference\nwith over 1M test points with 100K context points in under a minute on a single\n24GB GPU.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09163v1"}
{"id": "2506.09873", "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09873v1"}
{"id": "2506.09709", "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "comment": "Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09709v1"}
{"id": "2506.09549", "title": "A Study on Speech Assessment with Visual Cues", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Nasir Saleem", "Amir Hussain", "Hsin-Min Wang", "Yu Tsao"], "summary": "Non-intrusive assessment of speech quality and intelligibility is essential\nwhen clean reference signals are unavailable. In this work, we propose a\nmultimodal framework that integrates audio features and visual cues to predict\nPESQ and STOI scores. It employs a dual-branch architecture, where spectral\nfeatures are extracted using STFT, and visual embeddings are obtained via a\nvisual encoder. These features are then fused and processed by a CNN-BLSTM with\nattention, followed by multi-task learning to simultaneously predict PESQ and\nSTOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND\ncorpus, show that our model outperforms the audio-only baseline. Under seen\nnoise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%\n(0.7403->0.8253) for STOI. These results highlight the effectiveness of\nincorporating visual cues in enhancing the accuracy of non-intrusive speech\nassessment.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09549v1"}
{"id": "2506.09221", "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation", "authors": ["Michael Soprano"], "summary": "The spread of online misinformation poses serious threats to democratic\nsocieties. Traditionally, expert fact-checkers verify the truthfulness of\ninformation through investigative processes. However, the volume and immediacy\nof online content present major scalability challenges. Crowdsourcing offers a\npromising alternative by leveraging non-expert judgments, but it introduces\nconcerns about bias, accuracy, and interpretability. This thesis investigates\nhow human intelligence can be harnessed to assess the truthfulness of online\ninformation, focusing on three areas: misinformation assessment, cognitive\nbiases, and automated fact-checking systems. Through large-scale crowdsourcing\nexperiments and statistical modeling, it identifies key factors influencing\nhuman judgments and introduces a model for the joint prediction and explanation\nof truthfulness. The findings show that non-expert judgments often align with\nexpert assessments, particularly when factors such as timing and experience are\nconsidered. By deepening our understanding of human judgment and bias in\ntruthfulness assessment, this thesis contributes to the development of more\ntransparent, trustworthy, and interpretable systems for combating\nmisinformation.", "comment": "PhD thesis, University of Udine, defended May 2023, 458 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09221v1"}
{"id": "2506.09913", "title": "A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals", "authors": ["Brian N. Granzow", "Stephen D. Bond", "D. Thomas Seidl", "Bernhard Endtmayer"], "summary": "We consider estimating the discretization error in a nonlinear functional\n$J(u)$ in the setting of an abstract variational problem: find $u \\in\n\\mathcal{V}$ such that $B(u,\\varphi) = L(\\varphi) \\; \\forall \\varphi \\in\n\\mathcal{V}$, as approximated by a Galerkin finite element method. Here,\n$\\mathcal{V}$ is a Hilbert space, $B(\\cdot,\\cdot)$ is a bilinear form, and\n$L(\\cdot)$ is a linear functional. We consider well-known error estimates\n$\\eta$ of the form $J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$, where $u_h$\ndenotes a finite element approximation to $u$, and $z$ denotes the solution to\nan auxiliary adjoint variational problem. We show that there exist nonlinear\nfunctionals for which error estimates of this form are not reliable, even in\nthe presence of an exact adjoint solution solution $z$. An estimate $\\eta$ is\nsaid to be reliable if there exists a constant $C \\in \\mathbb{R}_{>0}$\nindependent of $u_h$ such that $|J(u) - J(u_h)| \\leq C|\\eta|$. We present\nseveral example pairs of bilinear forms and nonlinear functionals where\nreliability of $\\eta$ is not achieved.", "comment": "6 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09913v1"}
{"id": "2506.09804", "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09804v1"}
{"id": "2506.09363", "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "comment": "Under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09363v1"}
{"id": "2506.09588", "title": "Attention-Based Map Encoding for Learning Generalized Legged Locomotion", "authors": ["Junzhe He", "Chong Zhang", "Fabian Jenelten", "Ruben Grandia", "Moritz BÄcher", "Marco Hutter"], "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in\nexpanding the operational range of mobile robots. It requires precise planning\nwhen possible footholds are sparse, robustness against uncertainties and\ndisturbances, and generalizability across diverse terrains. While traditional\nmodel-based controllers excel at planning on complex terrains, they struggle\nwith real-world uncertainties. Learning-based controllers offer robustness to\nsuch uncertainties but often lack precision on terrains with sparse steppable\nareas. Hybrid methods achieve enhanced robustness on sparse terrains by\ncombining both methods but are computationally demanding and constrained by the\ninherent limitations of model-based planners. To achieve generalized legged\nlocomotion on diverse terrains while preserving the robustness of\nlearning-based controllers, this paper proposes to learn an attention-based map\nencoding conditioned on robot proprioception, which is trained as part of the\nend-to-end controller using reinforcement learning. We show that the network\nlearns to focus on steppable areas for future footholds when the robot\ndynamically navigates diverse and challenging terrains. We synthesize behaviors\nthat exhibit robustness against uncertainties while enabling precise and agile\ntraversal of sparse terrains. Additionally, our method offers a way to\ninterpret the topographical perception of a neural network. We have trained two\ncontrollers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot\nrespectively and tested the resulting controllers in the real world under\nvarious challenging indoor and outdoor scenarios, including ones unseen during\ntraining.", "comment": "Original draft prior to peer review. Significant revisions and new\n  materials are expected after formal publication release", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09588v1"}
{"id": "2506.09345", "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09345v1"}
{"id": "2506.09070", "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09070v1"}
{"id": "2506.09171", "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09171v1"}
{"id": "2506.09792", "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "comment": "Accepted by Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09792v1"}
{"id": "2506.09570", "title": "Spectral Efficiency Maximization for DMA-enabled Multiuser MISO with Statistical CSI", "authors": ["Hao Xu", "Boyu Ning", "Chongjun Ouyang", "Hongwen Yang"], "summary": "Dynamic metasurface antennas (DMAs) offer the potential to achieve\nlarge-scale antenna arrays with low power consumption and reduced hardware\ncosts, making them a promising technology for future communication systems.\nThis paper investigates the spectral efficiency (SE) of DMA-enabled multiuser\nmultiple-input single-output (MISO) systems in both uplink and downlink\ntransmissions, using only statistical channel state information (CSI) to\nmaximize the ergodic sum rate of multiple users. For the uplink system, we\nconsider two decoding rules: minimum mean square error (MMSE) with and without\nsuccessive interference cancellation (SIC). For both decoders, we derive\nclosed-form surrogates to substitute the original expressions of ergodic sum\nrate and formulate tractable optimization problems for designing DMA weights.\nThen, a weighted MMSE (WMMSE)-based algorithm is proposed to maximize the\nergodic sum rate. For the downlink system, we derive an approximate expression\nfor the ergodic sum rate and formulate a hybrid analog/digital beamforming\noptimization problem that jointly optimizes the digital precoder and DMA\nweights. A penalty dual decomposition (PDD)-based algorithm is proposed by\nleveraging the fractional programming framework. Numerical results validate the\naccuracy of the derived surrogates and highlight the superiority of the\nproposed algorithms over baseline schemes. It is shown that these algorithms\nare effective across various DMA settings and are particularly well-suited for\nsystem design in fast time-varying channels.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09570v1"}
{"id": "2506.09259", "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09259v1"}
{"id": "2506.09933", "title": "Efficient multigrid solvers for mixed-degree local discontinuous Galerkin multiphase Stokes problems", "authors": ["Robert I. Saye"], "summary": "We design and investigate efficient multigrid solvers for multiphase Stokes\nproblems discretised via mixed-degree local discontinuous Galerkin methods.\nUsing the template of a standard multigrid V-cycle, we develop a smoother\nanalogous to element-wise block Gauss-Seidel, except the diagonal block\ninverses are replaced with an approximation that balances the smoothing of the\nvelocity and pressure variables, factoring in the unequal scaling of the\nvarious Stokes system operators, and optimised via two-grid local Fourier\nanalysis. We evaluate the performance of the multigrid solver across an\nextensive range of two- and three-dimensional test problems, including\nsteady-state and unsteady, standard-form and stress-form, single-phase and\nhigh-contrast multiphase Stokes problems, with multiple kinds of boundary\nconditions and various choices of polynomial degree. In the lowest-degree case,\ni.e., that of piecewise constant pressure fields, we observe reliable multigrid\nconvergence rates, though not especially fast. However, in every other case, we\nsee rapid convergence rates matching those of classical Poisson-style geometric\nmultigrid methods; e.g., 5 iterations reduce the Stokes system residual by 5 to\n10 orders of magnitude.", "comment": "25 pages, 10 figures, 4 algorithms, 1 table", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09933v1"}
{"id": "2506.09984", "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09984v1"}
{"id": "2506.09452", "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "comment": "Submitted to IEEE S&P 2026", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09452v1"}
{"id": "2506.09623", "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09623v1"}
{"id": "2506.09350", "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09350v1"}
{"id": "2506.09071", "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09071v1"}
{"id": "2506.09172", "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09172v1"}
{"id": "2506.09874", "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09874v1"}
{"id": "2506.09807", "title": "Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice", "authors": ["Junqing Zhang", "Francesco Ardizzon", "Mattia Piana", "Guanxiong Shen", "Stefano Tomasin"], "summary": "The identification of the devices from which a message is received is part of\nsecurity mechanisms to ensure authentication in wireless communications.\nConventional authentication approaches are cryptography-based, which, however,\nare usually computationally expensive and not adequate in the Internet of\nThings (IoT), where devices tend to be low-cost and with limited resources.\nThis paper provides a comprehensive survey of physical layer-based device\nfingerprinting, which is an emerging device authentication for wireless\nsecurity. In particular, this article focuses on hardware impairment-based\nidentity authentication and channel features-based authentication. They are\npassive techniques that are readily applicable to legacy IoT devices. Their\nintrinsic hardware and channel features, algorithm design methodologies,\napplication scenarios, and key research questions are extensively reviewed\nhere. The remaining research challenges are discussed, and future work is\nsuggested that can further enhance the physical layer-based device\nfingerprinting.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09807v1"}
{"id": "2506.09683", "title": "Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead", "authors": ["Priyavanshi Pathania", "Nikhil Bamby", "Rohit Mehra", "Samarth Sikand", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "The proliferation of software and AI comes with a hidden risk: its growing\nenergy and carbon footprint. As concerns regarding environmental sustainability\ncome to the forefront, understanding and optimizing how software impacts the\nenvironment becomes paramount. In this paper, we present a state-of-the-art\nreview of methods and tools that enable the measurement of software and\nAI-related energy and/or carbon emissions. We introduce a taxonomy to\ncategorize the existing work as Monitoring, Estimation, or Black-Box\napproaches. We delve deeper into the tools and compare them across different\ndimensions and granularity - for example, whether their measurement encompasses\nenergy and carbon emissions and the components considered (like CPU, GPU, RAM,\netc.). We present our observations on the practical use (component wise\nconsolidation of approaches) as well as the challenges that we have identified\nacross the current state-of-the-art. As we start an initiative to address these\nchallenges, we emphasize active collaboration across the community in this\nimportant field.", "comment": "8 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09683v1"}
{"id": "2506.09207", "title": "mLaSDI: Multi-stage latent space dynamics identification", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09207v1"}
{"id": "2506.09474", "title": "Covert Entanglement Generation over Bosonic Channels", "authors": ["Evan J. D. Anderson", "Michael S. Bullock", "Ohad Kimelfeld", "Christopher K. Eyre", "Filip Rozpędek", "Uzi Pereg", "Boulat A. Bash"], "summary": "We explore covert entanglement generation over the lossy thermal-noise\nbosonic channel, which is a quantum-mechanical model of many practical\nsettings, including optical, microwave, and radio-frequency (RF) channels.\nCovert communication ensures that an adversary is unable to detect the presence\nof transmissions, which are concealed in channel noise. We show that a\n$\\textit{square root law}$ (SRL) for covert entanglement generation similar to\nthat for classical: $L_{\\rm EG}\\sqrt{n}$ entangled bits (ebits) can be\ngenerated covertly and reliably over $n$ uses of a bosonic channel. We report a\nsingle-letter expression for optimal $L_{\\rm EG}$ as well as an achievable\nmethod. We additionally analyze the performance of covert entanglement\ngeneration using single- and dual-rail photonic qubits, which may be more\npractical for physical implementation.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09474v1"}
{"id": "2506.09629", "title": "R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing", "authors": ["Maurice Brunner", "Edoardo Ghignone", "Nicolas Baumann", "Michele Magno"], "summary": "Autonomous racing has emerged as a crucial testbed for autonomous driving\nalgorithms, necessitating a simulation environment for both vehicle dynamics\nand sensor behavior. Striking the right balance between vehicle dynamics and\nsensor accuracy is crucial for pushing vehicles to their performance limits.\nHowever, autonomous racing developers often face a trade-off between accurate\nvehicle dynamics and high-fidelity sensor simulations. This paper introduces\nR-CARLA, an enhancement of the CARLA simulator that supports holistic\nfull-stack testing, from perception to control, using a single system. By\nseamlessly integrating accurate vehicle dynamics with sensor simulations,\nopponents simulation as NPCs, and a pipeline for creating digital twins from\nreal-world robotic data, R-CARLA empowers researchers to push the boundaries of\nautonomous racing development. Furthermore, it is developed using CARLA's rich\nsuite of sensor simulations. Our results indicate that incorporating the\nproposed digital-twin framework into R-CARLA enables more realistic full-stack\ntesting, demonstrating a significant reduction in the Sim-to-Real gap of car\ndynamics simulation by 42% and by 82% in the case of sensor simulation across\nvarious testing scenarios.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09629v1"}
{"id": "2506.09357", "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "authors": ["Junchao Zhou"], "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09357v1"}
{"id": "2506.09079", "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09079v1"}
{"id": "2506.09173", "title": "The Curious Language Model: Strategic Test-Time Information Acquisition", "authors": ["Michael Cooper", "Rohan Wadhawan", "John Michael Giorgi", "Chenhao Tan", "Davis Liang"], "summary": "Decision-makers often possess insufficient information to render a confident\ndecision. In these cases, the decision-maker can often undertake actions to\nacquire the necessary information about the problem at hand, e.g., by\nconsulting knowledgeable authorities or by conducting experiments. Importantly,\ndifferent levers of information acquisition come with different costs, posing\nthe challenge of selecting the actions that are both informative and\ncost-effective. In this work, we propose CuriosiTree, a heuristic-based,\ntest-time policy for zero-shot information acquisition in large language models\n(LLMs). CuriosiTree employs a greedy tree search to estimate the expected\ninformation gain of each action and strategically chooses actions based on a\nbalance of anticipated information gain and associated cost. Empirical\nvalidation in a clinical diagnosis simulation shows that CuriosiTree enables\ncost-effective integration of heterogenous sources of information, and\noutperforms baseline action selection strategies in selecting action sequences\nthat enable accurate diagnosis.", "comment": "39 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09173v1"}
{"id": "2506.09931", "title": "Faster-than-Nyquist Signaling is Good for Single-Carrier ISAC: An Analytical Study", "authors": ["Shuangyang Li", "Fan Liu", "Yifeng Xiong", "Weijie Yuan", "Baoming Bai", "Christos Masouros", "Giuseppe Caire"], "summary": "In this paper, we provide an analytical study of single-carrier\nfaster-than-Nyquist (FTN) signaling for integrated sensing and communications\n(ISAC). Our derivations show that FTN is advantageous for ISAC, and reveal new\ninsights that these advantages come from the fact that FTN signaling can\neffectively avoid the spectral aliasing due to the mismatch between the symbol\nrate and the bandwidth of the shaping pulse. Specifically, the communication\nspectral efficiency advantages of FTN signaling over time-invariant multipath\nchannels are analytically shown, where both upper- and lower-bounds on the\nspectral efficiency are derived. We show that the gap between these two bounds\ncorresponds to the potential signal-to-noise ratio (SNR) variation due to the\npresence of multipath delay and spectral aliasing, which diminishes as the\nsymbol rate grows higher. Particularly, in the limiting case, this SNR\nvariation disappears while the degree of freedom (DoF) of the system attain the\nmaximum. Furthermore, the sensing advantages for FTN signals are verified in\nterms of the expected normalized squared ambiguity function. We show that FTN\nsignals generally enjoy a more robust ranging performance. More importantly, we\nprove that FTN signaling can effectively avoid the undesired peaks in the\nconsidered ambiguity function along the Doppler dimension, thereby reducing the\nambiguities in velocity estimation. All these conclusions are explicitly\nverified by numerical results.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09931v1"}
{"id": "2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09847v1"}
{"id": "2506.09280", "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09280v1"}
{"id": "2506.09600", "title": "Effective Red-Teaming of Policy-Adherent Agents", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09600v1"}
{"id": "2506.09697", "title": "Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling", "authors": ["Paolo Franceschi", "Andrea Bussolan", "Vincenzo Pomponi", "Oliver Avram", "Stefano Baraldo", "Anna Valente"], "summary": "Nowadays, industries are showing a growing interest in human-robot\ncollaboration, particularly for shared tasks. This requires intelligent\nstrategies to plan a robot's motions, considering both task constraints and\nhuman-specific factors such as height and movement preferences. This work\nintroduces a novel approach to generate personalized trajectories using Dynamic\nMovement Primitives (DMPs), enhanced with real-time velocity scaling based on\nhuman feedback. The method was rigorously tested in industrial-grade\nexperiments, focusing on the collaborative transport of an engine cowl lip\nsection. Comparative analysis between DMP-generated trajectories and a\nstate-of-the-art motion planner (BiTRRT) highlights their adaptability combined\nwith velocity scaling. Subjective user feedback further demonstrates a clear\npreference for DMP- based interactions. Objective evaluations, including\nphysiological measurements from brain and skin activity, reinforce these\nfindings, showcasing the advantages of DMPs in enhancing human-robot\ninteraction and improving user experience.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09697v1"}
{"id": "2506.09363", "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "comment": "Under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09363v1"}
{"id": "2506.09080", "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09080v1"}
{"id": "2506.09174", "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter", "authors": ["Chenheng Xu", "Dan Wu", "Yixin Zhu", "Ying Nian Wu"], "summary": "Multivariate long-term time series forecasting has been suffering from the\nchallenge of capturing both temporal dependencies within variables and spatial\ncorrelations across variables simultaneously. Current approaches predominantly\nrepurpose backbones from natural language processing or computer vision (e.g.,\nTransformers), which fail to adequately address the unique properties of time\nseries (e.g., periodicity). The research community lacks a dedicated backbone\nwith temporal-specific inductive biases, instead relying on domain-agnostic\nbackbones supplemented with auxiliary techniques (e.g., signal decomposition).\nWe introduce FNF as the backbone and DBD as the architecture to provide\nexcellent learning capabilities and optimal learning pathways for\nspatio-temporal modeling, respectively. Our theoretical analysis proves that\nFNF unifies local time-domain and global frequency-domain information\nprocessing within a single backbone that extends naturally to spatial modeling,\nwhile information bottleneck theory demonstrates that DBD provides superior\ngradient flow and representation capacity compared to existing unified or\nsequential architectures. Our empirical evaluation across 11 public benchmark\ndatasets spanning five domains (energy, meteorology, transportation,\nenvironment, and nature) confirms state-of-the-art performance with consistent\nhyperparameter settings. Notably, our approach achieves these results without\nany auxiliary techniques, suggesting that properly designed neural\narchitectures can capture the inherent properties of time series, potentially\ntransforming time series modeling in scientific and industrial applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09174v1"}
{"id": "2506.09929", "title": "Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation", "authors": ["Scott Schnelle", "Francesca Favaro", "Laura Fraade-Blanar", "David Wichner", "Holland Broce", "Justin Miranda"], "summary": "As Automated Driving Systems (ADS) technology advances, ensuring safety and\npublic trust requires robust assurance frameworks, with safety cases emerging\nas a critical tool toward such a goal. This paper explores an approach to\nassess how a safety case is supported by its claims and evidence, toward\nestablishing credibility for the overall case. Starting from a description of\nthe building blocks of a safety case (claims, evidence, and optional\nformat-dependent entries), this paper delves into the assessment of support of\neach claim through the provided evidence. Two domains of assessment are\noutlined for each claim: procedural support (formalizing process specification)\nand implementation support (demonstrating process application). Additionally,\nan assessment of evidence status is also undertaken, independently from the\nclaims support. Scoring strategies and evaluation guidelines are provided,\nincluding detailed scoring tables for claim support and evidence status\nassessment. The paper further discusses governance, continual improvement, and\ntiming considerations for safety case assessments. Reporting of results and\nfindings is contextualized within its primary use for internal decision-making\non continual improvement efforts. The presented approach builds on state of the\nart auditing practices, but specifically tackles the question of judging the\ncredibility of a safety case. While not conclusive on its own, it provides a\nstarting point toward a comprehensive \"Case Credibility Assessment\" (CCA),\nstarting from the evaluation of the support for each claim (individually and in\naggregate), as well as every piece of evidence provided. By delving into the\ntechnical intricacies of ADS safety cases, this work contributes to the ongoing\ndiscourse on safety assurance and aims to facilitate the responsible\nintegration of ADS technology into society.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09929v1"}
{"id": "2506.09297", "title": "A thorough study of Riemannian Newton's Method", "authors": ["Caio O. da Silva", "Yuri A. Aoto", "Felipe F. G. S. Costa", "Márcio F. da Silva"], "summary": "This work presents a thorough numerical study of Riemannian Newton's Method\n(RNM) for optimization problems, with a focus on the Grassmannian and on the\nStiefel manifold. We compare the Riemannian formulation of Newton's Method with\nits classical Euclidean counterpart based on Lagrange multipliers by applying\nboth approaches to the important and challenging Hartree--Fock energy\nminimization problem from Quantum Chemistry. Experiments on a dataset of 125\nmolecules show that the Riemannian approaches achieve higher convergence rates,\nrequire fewer iterations, and exhibit greater robustness to the choice of\ninitial guess. In this work we also analyze the numerical issues that arise\nfrom using Newton's Method on the total manifold when the cost function is\ndefined on the quotient manifold. We investigate the performance of a modified\nRNM in which we ignore the small eigenvalues of the Hessian and the results\nindicate that this modified method is stable and performs on par with the RNM\non the quotient manifold.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09297v1"}
{"id": "2506.09689", "title": "BF-Max: an Efficient Bit Flipping Decoder with Predictable Decoding Failure Rate", "authors": ["Alessio Baldelli", "Marco Baldi", "Franco Chiaraluce", "Paolo Santini"], "summary": "The Bit-Flipping (BF) decoder, thanks to its very low computational\ncomplexity, is widely employed in post-quantum cryptographic schemes based on\nModerate Density Parity Check codes in which, ultimately, decryption boils down\nto syndrome decoding. In such a setting, for security concerns, one must\nguarantee that the Decoding Failure Rate (DFR) is negligible. Such a condition,\nhowever, is very difficult to guarantee, because simulations are of little help\nand the decoder performance is difficult to model theoretically. In this paper,\nwe introduce a new version of the BF decoder, that we call BF-Max,\ncharacterized by the fact that in each iteration only one bit (the least\nreliable) is flipped. When the number of iterations is equal to the number of\nerrors to be corrected, we are able to develop a theoretical characterization\nof the DFR that tightly matches with numerical simulations. We also show how\nBF-Max can be implemented efficiently, achieving low complexity and making it\ninherently constant time. With our modeling, we are able to accurately predict\nvalues of DFR that are remarkably lower than those estimated by applying other\napproaches.", "comment": "5 pages plus 1 page that contains only bibliography, 2 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09689v1"}
{"id": "2506.09765", "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09765v1"}
{"id": "2506.09369", "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09369v1"}
{"id": "2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09081v1"}
{"id": "2506.09183", "title": "Multi-Task Reward Learning from Human Ratings", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09183v1"}
{"id": "2506.09996", "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "comment": "22 pages, 7 figures, and 9 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09996v1"}
{"id": "2506.09711", "title": "Non-Euclidean dual gradient ascent for entropically regularized linear and semidefinite programming", "authors": ["Yuhang Cai", "Michael Lindsey"], "summary": "We present an optimization framework that exhibits dimension-independent\nconvergence on a broad class of semidefinite programs (SDPs). Our approach\nfirst regularizes the primal problem with the von Neumann entropy, then solve\nthe regularized problem using dual gradient ascent with respect to a\nproblem-adapted norm. In particular, we show that the dual gradient norm\nconverges to zero at a rate independent of the ambient dimension and, via\nrounding arguments, construct primal-feasible solutions in certain special\ncases. We also derive explicit convergence rates for the objective. In order to\nachieve optimal computational scaling, we must accommodate the use of\nstochastic gradients constructed via randomized trace estimators. Throughout we\nillustrate the generality of our framework via three important special cases --\nthe Goemans-Williamson SDP relaxation of the Max-Cut problem, the optimal\ntransport linear program, and several SDP relaxations of the permutation\nsynchronization problem. Numerical experiments confirm that our methods achieve\ndimension-independent convergence in practice.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09711v1"}
{"id": "2506.09702", "title": "Mapping NVD Records to Their VFCs: How Hard is it?", "authors": ["Huu Hung Nguyen", "Duc Manh Tran", "Yiran Cheng", "Thanh Le-Cong", "Hong Jin Kang", "Ratnadira Widyasari", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "summary": "Mapping National Vulnerability Database (NVD) records to vulnerability-fixing\ncommits (VFCs) is crucial for vulnerability analysis but challenging due to\nsparse explicit links in NVD references.This study explores this mapping's\nfeasibility through an empirical approach. Manual analysis of NVD references\nshowed Git references enable over 86% success, while non-Git references achieve\nunder 14%. Using these findings, we built an automated pipeline extracting\n31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,\nmainly from Git references. To fill gaps, we mined six external security\ndatabases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,\nand GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%\nprecision. Combining these, we mapped 26,710 unique records (11.3% coverage)\nfrom 7,634 projects, with overlap between NVD and external databases, plus\nunique GitHub contributions. Despite success with Git references, 88.7% of\nrecords remain unmapped, highlighting the difficulty without Git links. This\nstudy offers insights for enhancing vulnerability datasets and guiding future\nautomated security research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09702v1"}
{"id": "2506.09800", "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving", "authors": ["Haochen Liu", "Tianyu Li", "Haohan Yang", "Li Chen", "Caojun Wang", "Ke Guo", "Haochen Tian", "Hongchen Li", "Hongyang Li", "Chen Lv"], "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09800v1"}
{"id": "2506.09378", "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09378v1"}
{"id": "2506.09082", "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "comment": "First two authors contribute equally", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09082v1"}
{"id": "2506.09193", "title": "LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting", "authors": ["Yilin Zhuang", "Karthik Duraisamy"], "summary": "Accurate probabilistic weather forecasting demands both high accuracy and\nefficient uncertainty quantification, challenges that overburden both ensemble\nnumerical weather prediction (NWP) and recent machine-learning methods. We\nintroduce LaDCast, the first global latent-diffusion framework for medium-range\nensemble forecasting, which generates hourly ensemble forecasts entirely in a\nlearned latent space. An autoencoder compresses high-dimensional ERA5\nreanalysis fields into a compact representation, and a transformer-based\ndiffusion model produces sequential latent updates with arbitrary hour\ninitialization. The model incorporates Geometric Rotary Position Embedding\n(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream\nattention mechanism for efficient conditioning, and sinusoidal temporal\nembeddings to capture seasonal patterns. LaDCast achieves deterministic and\nprobabilistic skill close to that of the European Centre for Medium-Range\nForecast IFS-ENS, without any explicit perturbations. Notably, LaDCast\ndemonstrates superior performance in tracking rare extreme events such as\ncyclones, capturing their trajectories more accurately than established models.\nBy operating in latent space, LaDCast reduces storage and compute by orders of\nmagnitude, demonstrating a practical path toward forecasting at kilometer-scale\nresolution in real time. We open-source our code and models and provide the\ntraining and evaluation pipelines at: https://github.com/tonyzyl/ladcast.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09193v1"}
{"id": "2506.09775", "title": "The Intrinsic Riemannian Proximal Gradient Method for Nonconvex Optimization", "authors": ["Ronny Bergmann", "Hajg Jasa", "Paula John", "Max Pfeffer"], "summary": "We consider the proximal gradient method on Riemannian manifolds for\nfunctions that are possibly not geodesically convex. Starting from the\nforward-backward-splitting, we define an intrinsic variant of the proximal\ngradient method that uses proximal maps defined on the manifold and therefore\ndoes not require or work in the embedding. We investigate its convergence\nproperties and illustrate its numerical performance, particularly for nonconvex\nor nonembedded problems that are hence out of reach for other methods.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09775v1"}
{"id": "2506.09803", "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09803v1"}
{"id": "2506.09859", "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints", "authors": ["Huajian Liu", "Yixuan Feng", "Wei Dong", "Kunpeng Fan", "Chao Wang", "Yongzhuo Gao"], "summary": "In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09859v1"}
{"id": "2506.09385", "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09385v1"}
{"id": "2506.09083", "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "authors": ["Jerry Lin", "Partick P. W. Chen"], "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "comment": "4 pages, 3 figures, 1 Table", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09083v1"}
{"id": "2506.09199", "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "comment": "21 pages, 12 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09199v1"}
{"id": "2506.09825", "title": "On the Impossibility of a Perfect Hypervisor", "authors": ["Mordechai Guri"], "summary": "We establish a fundamental impossibility result for a `perfect hypervisor',\none that (1) preserves every observable behavior of any program exactly as on\nbare metal and (2) adds zero timing or resource overhead.\n  Within this model we prove two theorems. (1) Indetectability Theorem. If such\na hypervisor existed, no guest-level program, measurement, or timing test could\ndistinguish it from native execution; all traces, outputs, and timings would be\nidentical.\n  (2) Impossibility Theorem. Despite that theoretical indetectability, a\nperfect hypervisor cannot exist on any machine with finite computational\nresources.\n  These results are architecture-agnostic and extend beyond hypervisors to any\nvirtualization layer emulators, sandboxes, containers, or\nruntime-instrumentation frameworks. Together they provide a formal foundation\nfor future work on the principles and limits of virtualization.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09825v1"}
{"id": "2506.09876", "title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization", "authors": ["Jisheng Xu", "Ding Lin", "Pangkit Fong", "Chongrong Fang", "Xiaoming Duan", "Jianping He"], "summary": "This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09876v1"}
{"id": "2506.09399", "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09399v1"}
{"id": "2506.09084", "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09084v1"}
{"id": "2506.09200", "title": "FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems", "authors": ["Val Andrei Fajardo", "David B. Emerson", "Amandeep Singh", "Veronica Chatrath", "Marcelo Lotif", "Ravi Theja", "Alex Cheung", "Izuki Matsubi"], "summary": "Retrieval-augmented generation (RAG) systems have been shown to be effective\nin addressing many of the drawbacks of relying solely on the parametric memory\nof large language models. Recent work has demonstrated that RAG systems can be\nimproved via fine-tuning of their retriever and generator models. In this work,\nwe introduce FedRAG, a framework for fine-tuning RAG systems across centralized\nand federated architectures. FedRAG supports state-of-the-art fine-tuning\nmethods, offering a simple and intuitive interface and a seamless conversion\nfrom centralized to federated training tasks. FedRAG is also deeply integrated\nwith the modern RAG ecosystem, filling a critical gap in available tools.", "comment": "9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at\n  ICML 2025. Framework code available at\n  https://github.com/VectorInstitute/fed-rag", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09200v1"}
{"id": "2506.09914", "title": "From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications", "authors": ["Teng Guo"], "summary": "The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots\nfrom start to goal configurations efficiently while avoiding collisions.\nDespite progress in solution quality and runtime, its complexity and industrial\nrelevance continue to drive research.\n  This dissertation introduces scalable MRPP methods with provable guarantees\nand practical heuristics. First, we study dense MRPP on 2D grids, relevant to\nwarehouse and parcel systems. We propose the Rubik Table method, achieving $(1\n+ \\delta)$-optimal makespan (with $\\delta \\in (0, 0.5]$) for up to $\\frac{m_1\nm_2}{2}$ robots, solving large instances efficiently and setting a new\ntheoretical benchmark.\n  Next, we address real-world MRPP. We design optimal layouts for structured\nenvironments (e.g., warehouses, parking systems) and propose a puzzle-based\nsystem for dense, deadlock-free autonomous vehicle parking. We also extend MRPP\nto Reeds-Shepp robots, introducing motion primitives and smoothing techniques\nto ensure feasible, efficient paths under nonholonomic constraints. Simulations\nand real-world tests validate the approach in urban driving and robotic\ntransport scenarios.", "comment": "Ph.D. thesis", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09914v1"}
{"id": "2506.09403", "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09403v1"}
{"id": "2506.09085", "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09085v1"}
{"id": "2506.09202", "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09202v1"}
{"id": "2506.09930", "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "comment": "Under review", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09930v1"}
{"id": "2506.09411", "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09411v1"}
{"id": "2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "comment": "in French language", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09089v1"}
{"id": "2506.09207", "title": "mLaSDI: Multi-stage latent space dynamics identification", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09207v1"}
{"id": "2506.09934", "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09934v1"}
{"id": "2506.09416", "title": "Noise Conditional Variational Score Distillation", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09416v1"}
{"id": "2506.09092", "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09092v1"}
{"id": "2506.09215", "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "authors": ["Greyson Brothers"], "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09215v1"}
{"id": "2506.09937", "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "comment": "Project Page: https://vla-safe.github.io/", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09937v1"}
{"id": "2506.09417", "title": "ODG: Occupancy Prediction Using Dual Gaussians", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09417v1"}
{"id": "2506.09093", "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09093v1"}
{"id": "2506.09227", "title": "SoK: Machine Unlearning for Large Language Models", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09227v1"}
{"id": "2506.09979", "title": "Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control", "authors": ["Zachary Olkin", "Aaron D. Ames"], "summary": "Computing stabilizing and optimal control actions for legged locomotion in\nreal time is difficult due to the nonlinear, hybrid, and high dimensional\nnature of these robots. The hybrid nature of the system introduces a\ncombination of discrete and continuous variables which causes issues for\nnumerical optimal control. To address these challenges, we propose a layered\narchitecture that separates the choice of discrete variables and a smooth Model\nPredictive Controller (MPC). The layered formulation allows for online\nflexibility and optimality without sacrificing real-time performance through a\ncombination of gradient-free and gradient-based methods. The architecture\nleverages a sampling-based method for determining discrete variables, and a\nclassical smooth MPC formulation using these fixed discrete variables. We\ndemonstrate the results on a quadrupedal robot stepping over gaps and onto\nterrain with varying heights. In simulation, we demonstrate the controller on a\nhumanoid robot for gap traversal. The layered approach is shown to be more\noptimal and reliable than common heuristic-based approaches and faster to\ncompute than pure sampling methods.", "comment": "Submitted to Humanoids 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09979v1"}
{"id": "2506.09427", "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09427v1"}
{"id": "2506.09095", "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Clárisa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09095v1"}
{"id": "2506.09247", "title": "Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation", "authors": ["Karl Löwenmark", "Daniel Strömbergsson", "Chang Liu", "Marcus Liwicki", "Fredrik Sandin"], "summary": "Condition monitoring (CM) plays a crucial role in ensuring reliability and\nefficiency in the process industry. Although computerised maintenance systems\neffectively detect and classify faults, tasks like fault severity estimation,\nand maintenance decisions still largely depend on human expert analysis. The\nanalysis and decision making automatically performed by current systems\ntypically exhibit considerable uncertainty and high false alarm rates, leading\nto increased workload and reduced efficiency.\n  This work integrates large language model (LLM)-based reasoning agents with\nCM workflows to address analyst and industry needs, namely reducing false\nalarms, enhancing fault severity estimation, improving decision support, and\noffering explainable interfaces. We propose MindRAG, a modular framework\ncombining multimodal retrieval-augmented generation (RAG) with novel vector\nstore structures designed specifically for CM data. The framework leverages\nexisting annotations and maintenance work orders as surrogates for labels in a\nsupervised learning protocol, addressing the common challenge of training\npredictive models on unlabelled and noisy real-world datasets.\n  The primary contributions include: (1) an approach for structuring industry\nCM data into a semi-structured multimodal vector store compatible with\nLLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM\ndata; (3) developing practical reasoning agents capable of addressing\nreal-world CM queries; and (4) presenting an experimental framework for\nintegrating and evaluating such agents in realistic industrial scenarios.\nPreliminary results, evaluated with the help of an experienced analyst,\nindicate that MindRAG provide meaningful decision support for more efficient\nmanagement of alarms, thereby improving the interpretability of CM systems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09247v1"}
{"id": "2506.09990", "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09990v1"}
{"id": "2506.09429", "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09429v1"}
{"id": "2506.09096", "title": "Intra-Trajectory Consistency for Reward Modeling", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "comment": "Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09096v1"}
{"id": "2506.09258", "title": "CFMI: Flow Matching for Missing Data Imputation", "authors": ["Vaidotas Simkus", "Michael U. Gutmann"], "summary": "We introduce conditional flow matching for imputation (CFMI), a new\ngeneral-purpose method to impute missing data. The method combines continuous\nnormalising flows, flow-matching, and shared conditional modelling to deal with\nintractabilities of traditional multiple imputation. Our comparison with nine\nclassical and state-of-the-art imputation methods on 24 small to\nmoderate-dimensional tabular data sets shows that CFMI matches or outperforms\nboth traditional and modern techniques across a wide range of metrics. Applying\nthe method to zero-shot imputation of time-series data, we find that it matches\nthe accuracy of a related diffusion-based method while outperforming it in\nterms of computational efficiency. Overall, CFMI performs at least as well as\ntraditional methods on lower-dimensional data while remaining scalable to\nhigh-dimensional settings, matching or exceeding the performance of other deep\nlearning-based approaches, making it a go-to imputation method for a wide range\nof data types and dimensionalities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09258v1"}
{"id": "2506.09994", "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09994v1"}
{"id": "2506.09445", "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09445v1"}
{"id": "2506.09099", "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "authors": ["Joshua Barron", "Devin White"], "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09099v1"}
{"id": "2506.09270", "title": "Uncertainty Prioritized Experience Replay", "authors": ["Rodrigo Carrasco-Davis", "Sebastian Lee", "Claudia Clopath", "Will Dabney"], "summary": "Prioritized experience replay, which improves sample efficiency by selecting\nrelevant transitions to update parameter estimates, is a crucial component of\ncontemporary value-based deep reinforcement learning models. Typically,\ntransitions are prioritized based on their temporal difference error. However,\nthis approach is prone to favoring noisy transitions, even when the value\nestimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which\nexploration-guided agents get stuck by mistaking noise for novelty. To mitigate\nthe disruptive effects of noise in value estimation, we propose using epistemic\nuncertainty estimation to guide the prioritization of transitions from the\nreplay buffer. Epistemic uncertainty quantifies the uncertainty that can be\nreduced by learning, hence reducing transitions sampled from the buffer\ngenerated by unpredictable random processes. We first illustrate the benefits\nof epistemic uncertainty prioritized replay in two tabular toy models: a simple\nmulti-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our\nprioritization scheme on the Atari suite, outperforming quantile regression\ndeep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.", "comment": "Accepted at Reinforcement Learning Conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09270v1"}
{"id": "2506.09068", "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09068v1"}
{"id": "2506.09446", "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09446v1"}
{"id": "2506.09101", "title": "Feature Shift Localization Network", "authors": ["Míriam Barrabés", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "comment": "9 pages, 2 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09101v1"}
{"id": "2506.09272", "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Antonin Berthon", "Mihaela van der Schaar"], "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09272v1"}
{"id": "2506.09176", "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "comment": "ICML 2025 Poster", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09176v1"}
{"id": "2506.09460", "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09460v1"}
{"id": "2506.09102", "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation", "authors": ["Mihaela van der Schaar", "Richard Peck", "Eoin McKinney", "Jim Weatherall", "Stuart Bailey", "Justine Rochon", "Chris Anagnostopoulos", "Pierre Marquet", "Anthony Wood", "Nicky Best", "Harry Amad", "Julianna Piskorz", "Krzysztof Kacprzyk", "Rafik Salama", "Christina Gunther", "Francesca Frau", "Antoine Pugeat", "Ramon Hernandez"], "summary": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09102v1"}
{"id": "2506.09276", "title": "Learning The Minimum Action Distance", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "Özgür Şimşek", "Anders Jonsson"], "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09276v1"}
{"id": "2506.09278", "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "comment": "Project Page: https://uniflowmatch.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09278v1"}
{"id": "2506.09469", "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09469v1"}
{"id": "2506.09104", "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "comment": "Preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09104v1"}
{"id": "2506.09279", "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09279v1"}
{"id": "2506.09343", "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "comment": "CVPR 2025 Highlight", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09343v1"}
{"id": "2506.09473", "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "comment": "10 pages, 6 figures, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09473v1"}
{"id": "2506.09105", "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09105v1"}
{"id": "2506.09286", "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09286v1"}
{"id": "2506.09508", "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09508v1"}
{"id": "2506.09476", "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09476v1"}
{"id": "2506.09107", "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines", "authors": ["Athena Vakali", "Ilias Dimitriadis"], "summary": "AI models have become active decision makers, often acting without human\nsupervision. The rapid advancement of AI technology has already caused harmful\nincidents that have hurt individuals and societies and AI unfairness in heavily\ncriticized. It is urgent to disrupt AI pipelines which largely neglect human\nprinciples and focus on computational biases exploration at the data (pre),\nmodel(in), and deployment (post) processing stages. We claim that by exploiting\nthe advances of agents technology, we will introduce cautious, prompt, and\nongoing fairness watch schemes, under realistic, systematic, and human-centric\nfairness expectations. We envision agents as fairness guardians, since agents\nlearn from their environment, adapt to new information, and solve complex\nproblems by interacting with external tools and other systems. To set the\nproper fairness guardrails in the overall AI pipeline, we introduce a\nfairness-by-design approach which embeds multi-role agents in an end-to-end\n(human to AI) synergetic scheme. Our position is that we may design adaptive\nand realistic AI fairness frameworks, and we introduce a generalized algorithm\nwhich can be customized to the requirements and goals of each AI decision\nmaking scenario. Our proposed, so called FAIRTOPIA framework, is structured\nover a three-layered architecture, which encapsulates the AI pipeline inside an\nagentic guardian and a knowledge-based, self-refining layered scheme. Based on\nour proposition, we enact fairness watch in all of the AI pipeline stages,\nunder robust multi-agent workflows, which will inspire new fairness research\nhypothesis, heuristics, and methods grounded in human-centric, systematic,\ninterdisciplinary, socio-technical principles.", "comment": "11 pages, 4 figures", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09107v1"}
{"id": "2506.09316", "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention", "authors": ["Yeonju Ro", "Zhenyu Zhang", "Souvik Kundu", "Zhangyang Wang", "Aditya Akella"], "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose \\textit{dual-state linear attention} (\\textbf{\\dsla}), a novel\ndesign that maintains two specialized hidden states-one for preserving\nhistorical context and one for tracking recency-thereby mitigating the\nshort-range bias typical of linear-attention architectures. To further balance\nefficiency and accuracy under dynamic workload conditions, we introduce\n\\textbf{\\serve}, an online \\textit{adaptive distillation} framework that\nprogressively replaces Transformer layers with DSLA layers at inference time,\nguided by a sensitivity-based layer ordering. \\serve\\ uses a chained\nfine-tuning strategy to ensure that each newly converted DSLA layer remains\nconsistent with previously replaced layers, preserving the overall quality.\nExtensive evaluations on commonsense reasoning, long-context QA, and text\nsummarization demonstrate that \\serve\\ yields \\textbf{2.3x} faster inference\nthan Llama2-7B and \\textbf{3.0x} faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09316v1"}
{"id": "2506.09520", "title": "How attention simplifies mental representations for planning", "authors": ["Jason da Silva Castanheira", "Nicholas Shea", "Stephen M. Fleming"], "summary": "Human planning is efficient -- it frugally deploys limited cognitive\nresources to accomplish difficult tasks -- and flexible -- adapting to novel\nproblems and environments. Computational approaches suggest that people\nconstruct simplified mental representations of their environment, balancing the\ncomplexity of a task representation with its utility. These models imply a\nnested optimisation in which planning shapes perception, and perception shapes\nplanning -- but the perceptual and attentional mechanisms governing how this\ninteraction unfolds remain unknown. Here, we harness virtual maze navigation to\ncharacterise how spatial attention controls which aspects of a task\nrepresentation enter subjective awareness and are available for planning. We\nfind that spatial proximity governs which aspects of a maze are available for\nplanning, and that when task-relevant information follows natural (lateralised)\ncontours of attention, people can more easily construct simplified and useful\nmaze representations. This influence of attention varies considerably across\nindividuals, explaining differences in people's task representations and\nbehaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the\neffects of visuospatial attention into existing computational accounts of\nvalue-guided construal. Together, our work bridges computational perspectives\non perception and decision-making to better understand how individuals\nrepresent their environments in aid of planning.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.09520v1"}
{"id": "2506.09479", "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09479v1"}
{"id": "2506.09108", "title": "SensorLM: Learning the Language of Wearable Sensors", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09108v1"}
{"id": "2506.09332", "title": "Natural Language Guided Ligand-Binding Protein Design", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09332v1"}
{"id": "2506.09523", "title": "Adaptive event-triggered robust tracking control of soft robots", "authors": ["Renjie Ma", "Ziyao Qu", "Zhijian Hu", "Dong Zhao", "Marios M. Polycarpou"], "summary": "Soft robots manufactured with flexible materials can be highly compliant and\nadaptive to their surroundings, which facilitates their application in areas\nsuch as dexterous manipulation and environmental exploration. This paper aims\nat investigating the tracking control problem for soft robots under uncertainty\nsuch as unmodeled dynamics and external disturbance. First, we establish a\nnovel switching function and design the compensated tracking error dynamics by\nvirtue of the command filter. Then, based on the backstepping methodology, the\nvirtual controllers and the adaptive logic estimating the supremum of\nuncertainty impacts are developed for synthesizing an event-triggered control\nstrategy. In addition, the uniformed finite-time stability certification is\nderived for different scenarios of the switching function. Finally, we perform\na case study of a soft robot to illustrate the effectiveness of the proposed\ncontrol algorithm.", "comment": "8 pages, 7 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09523v1"}
{"id": "2506.09482", "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09482v1"}
{"id": "2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09147v1"}
{"id": "2506.09347", "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "comment": "12 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09347v1"}
{"id": "2506.09650", "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09650v1"}
{"id": "2506.09510", "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09510v1"}
{"id": "2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09160v1"}
{"id": "2506.09348", "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "authors": ["Natalie S. Frank"], "summary": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest.", "comment": "37 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09348v1"}
{"id": "2506.09748", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "comment": "8 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09748v1"}
{"id": "2506.09518", "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09518v1"}
{"id": "2506.09167", "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "comment": "13 pages", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09167v1"}
{"id": "2506.09368", "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "comment": "20 pages, 11 figures, 13 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09368v1"}
{"id": "2506.09839", "title": "OctoNav: Towards Generalist Embodied Navigation", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "comment": "31 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09839v1"}
{"id": "2506.09522", "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "authors": ["Beomsik Cho", "Jaehyung Kim"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "comment": "Code available at https://github.com/bscho333/ReVisiT", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09522v1"}
{"id": "2506.09171", "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09171v1"}
{"id": "2506.09373", "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09373v1"}
{"id": "2506.09981", "title": "ReSim: Reliable World Simulation for Autonomous Driving", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "comment": "Project page: https://opendrivelab.com/ReSim", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09981v1"}
{"id": "2506.09534", "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "comment": "18 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09534v1"}
{"id": "2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09175v1"}
{"id": "2506.09376", "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation", "authors": ["Bowen Zheng", "Tianming Yang"], "summary": "Diffusion distillation is a widely used technique to reduce the sampling cost\nof diffusion models, yet it often requires extensive training, and the student\nperformance tends to be degraded. Recent studies show that incorporating a GAN\nobjective may alleviate these issues, yet the underlying mechanism remains\nunclear. In this work, we first identify a key limitation of distillation:\nmismatched step sizes and parameter numbers between the teacher and the student\nmodel lead them to converge to different local minima, rendering direct\nimitation suboptimal. We further demonstrate that a standalone GAN objective,\nwithout relying a distillation loss, overcomes this limitation and is\nsufficient to convert diffusion models into efficient one-step generators.\nBased on this finding, we propose that diffusion training may be viewed as a\nform of generative pre-training, equipping models with capabilities that can be\nunlocked through lightweight GAN fine-tuning. Supporting this view, we create a\none-step generation model by fine-tuning a pre-trained model with 85% of\nparameters frozen, achieving strong performance with only 0.2M images and\nnear-SOTA results with 5M images. We further present a frequency-domain\nanalysis that may explain the one-step generative capability gained in\ndiffusion training. Overall, our work provides a new perspective for diffusion\ntraining, highlighting its role as a powerful generative pre-training process,\nwhich can be the basis for building efficient one-step generation models.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09376v1"}
{"id": "2506.09985", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09985v1"}
{"id": "2506.09538", "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09538v1"}
{"id": "2506.09183", "title": "Multi-Task Reward Learning from Human Ratings", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09183v1"}
{"id": "2506.09398", "title": "Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames", "authors": ["Haiyang Yu", "Yuchao Lin", "Xuan Zhang", "Xiaofeng Qian", "Shuiwang Ji"], "summary": "We consider the task of predicting Hamiltonian matrices to accelerate\nelectronic structure calculations, which plays an important role in physics,\nchemistry, and materials science. Motivated by the inherent relationship\nbetween the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local\nframe, we propose a novel and efficient network, called QHNetV2, that achieves\nglobal SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor\nproducts. This is achieved by introducing a set of new efficient and powerful\nSO(2)-equivariant operations and performing all off-diagonal feature updates\nand message passing within SO(2) local frames, thereby eliminating the need of\nSO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed\nwithin the SO(2) local frame at each node to fuse node features, mimicking the\nsymmetric contraction operation. Extensive experiments on the large QH9 and\nMD17 datasets demonstrate that our model achieves superior performance across a\nwide range of molecular structures and trajectories, highlighting its strong\ngeneralization capability. The proposed SO(2) operations on SO(2) local frames\noffer a promising direction for scalable and symmetry-aware learning of\nelectronic structures. Our code will be released as part of the AIRS library\nhttps://github.com/divelab/AIRS.", "comment": "Code available at: https://github.com/divelab/AIRS", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09398v1"}
{"id": "2506.09541", "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "comment": "Accepted by IEEE Transactions on Multimedia", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09541v1"}
{"id": "2506.09194", "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks", "authors": ["Emirhan Bilgiç", "Neslihan Serap Şengör", "Namık Berk Yalabık", "Yavuz Selim İşler", "Aykut Görkem Gelen", "Rahmi Elibol"], "summary": "This study examines the integration of Contrastive Predictive Coding (CPC)\nwith Spiking Neural Networks (SNN). While CPC learns the predictive structure\nof data to generate meaningful representations, SNN mimics the computational\nprocesses of biological neural systems over time. In this study, the goal is to\ndevelop a predictive coding model with greater biological plausibility by\nprocessing inputs and outputs in a spike-based system. The proposed model was\ntested on the MNIST dataset and achieved a high classification rate in\ndistinguishing positive sequential samples from non-sequential negative\nsamples. The study demonstrates that CPC can be effectively combined with SNN,\nshowing that an SNN trained for classification tasks can also function as an\nencoding mechanism. Project codes and detailed results can be accessed on our\nGitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal\n  Processing and Communications Applications Conference (SIU)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09194v1"}
{"id": "2506.09404", "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09404v1"}
{"id": "2506.09553", "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09553v1"}
{"id": "2506.09195", "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09195v1"}
{"id": "2506.09433", "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training", "authors": ["Shurui Gui", "Shuiwang Ji"], "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09433v1"}
{"id": "2506.09557", "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09557v1"}
{"id": "2506.09199", "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "comment": "21 pages, 12 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09199v1"}
{"id": "2506.09438", "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09438v1"}
{"id": "2506.09565", "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09565v1"}
{"id": "2506.09202", "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09202v1"}
{"id": "2506.09451", "title": "Safe Screening Rules for Group SLOPE", "authors": ["Runxue Bao", "Quanchao Lu", "Yanfu Zhang"], "summary": "Variable selection is a challenging problem in high-dimensional sparse\nlearning, especially when group structures exist. Group SLOPE performs well for\nthe adaptive selection of groups of predictors. However, the block\nnon-separable group effects in Group SLOPE make existing methods either invalid\nor inefficient. Consequently, Group SLOPE tends to incur significant\ncomputational costs and memory usage in practical high-dimensional scenarios.\nTo overcome this issue, we introduce a safe screening rule tailored for the\nGroup SLOPE model, which efficiently identifies inactive groups with zero\ncoefficients by addressing the block non-separable group effects. By excluding\nthese inactive groups during training, we achieve considerable gains in\ncomputational efficiency and memory usage. Importantly, the proposed screening\nrule can be seamlessly integrated into existing solvers for both batch and\nstochastic algorithms. Theoretically, we establish that our screening rule can\nbe safely employed with existing optimization algorithms, ensuring the same\nresults as the original approaches. Experimental results confirm that our\nmethod effectively detects inactive feature groups and significantly boosts\ncomputational efficiency without compromising accuracy.", "comment": "Accepted by ECML PKDD 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09451v1"}
{"id": "2506.09612", "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "comment": "17 pages, 9. figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09612v1"}
{"id": "2506.09204", "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.09204v1"}
{"id": "2506.09452", "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "comment": "Submitted to IEEE S&P 2026", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09452v1"}
{"id": "2506.09626", "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "comment": "IJCNN 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09626v1"}
{"id": "2506.09206", "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09206v1"}
{"id": "2506.09454", "title": "NDCG-Consistent Softmax Approximation with Accelerated Convergence", "authors": ["Yuanhao Pu", "Defu Lian", "Xiaolong Chen", "Xu Huang", "Jin Chen", "Enhong Chen"], "summary": "Ranking tasks constitute fundamental components of extreme similarity\nlearning frameworks, where extremely large corpora of objects are modeled\nthrough relative similarity relationships adhering to predefined ordinal\nstructures. Among various ranking surrogates, Softmax (SM) Loss has been widely\nadopted due to its natural capability to handle listwise ranking via global\nnegative comparisons, along with its flexibility across diverse application\nscenarios. However, despite its effectiveness, SM Loss often suffers from\nsignificant computational overhead and scalability limitations when applied to\nlarge-scale object spaces. To address this challenge, we propose novel loss\nformulations that align directly with ranking metrics: the\nRanking-Generalizable \\textbf{squared} (RG$^2$) Loss and the\nRanking-Generalizable interactive (RG$^\\times$) Loss, both derived through\nTaylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic\nmechanisms underlying weighted squared losses (WSL) in ranking methods and\nuncovers fundamental connections between sampling-based and non-sampling-based\nloss paradigms. Furthermore, we integrate the proposed RG losses with the\nhighly efficient Alternating Least Squares (ALS) optimization method, providing\nboth generalization guarantees and convergence rate analyses. Empirical\nevaluations on real-world datasets demonstrate that our approach achieves\ncomparable or superior ranking performance relative to SM Loss, while\nsignificantly accelerating convergence. This framework offers the similarity\nlearning community both theoretical insights and practically efficient tools,\nwith methodologies applicable to a broad range of tasks where balancing ranking\nquality and computational efficiency is essential.", "comment": "35 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09454v1"}
{"id": "2506.09634", "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09634v1"}
{"id": "2506.09215", "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "authors": ["Greyson Brothers"], "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09215v1"}
{"id": "2506.09477", "title": "On a few pitfalls in KL divergence gradient estimation for RL", "authors": ["Yunhao Tang", "Rémi Munos"], "summary": "We point out a few pitfalls in implementing gradient estimation for KL\ndivergence in RL training for LLM, as seen in a number of open source projects\nand papers. The first major pitfall is to differentiate through the KL estimate\nas loss functions to minimize KL divergence. We show that such implementations\nare generally incorrect and do not produce the desired KL gradient. Secondly,\nwe show that some implementations do not account for the sequential nature of\nthe estimation problem and produce a partial gradient at best. We demonstrate\nthe impact of such issues with illustrative tabular and LLM experiments, and\nshow the correct way to implement the KL gradient.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09477v1"}
{"id": "2506.09644", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09644v1"}
{"id": "2506.09251", "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "comment": "23 pages, 20 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09251v1"}
{"id": "2506.09496", "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09496v1"}
{"id": "2506.09650", "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09650v1"}
{"id": "2506.09259", "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09259v1"}
{"id": "2506.09499", "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "comment": "12 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09499v1"}
{"id": "2506.09663", "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09663v1"}
{"id": "2506.09268", "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09268v1"}
{"id": "2506.09508", "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09508v1"}
{"id": "2506.09668", "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "comment": "Work currently under revision for IEEE TMI", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09668v1"}
{"id": "2506.09276", "title": "Learning The Minimum Action Distance", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "Özgür Şimşek", "Anders Jonsson"], "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09276v1"}
{"id": "2506.09526", "title": "Neural Functions for Learning Periodic Signal", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09526v1"}
{"id": "2506.09677", "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09677v1"}
{"id": "2506.09284", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09284v1"}
{"id": "2506.09532", "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09532v1"}
{"id": "2506.09691", "title": "Adding simple structure at inference improves Vision-Language Compositionality", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09691v1"}
{"id": "2506.09286", "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09286v1"}
{"id": "2506.09544", "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network", "authors": ["Yang Yang", "Du Yin", "Hao Xue", "Flora Salim"], "summary": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09544v1"}
{"id": "2506.09695", "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "comment": "11 pages, 5 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09695v1"}
{"id": "2506.09301", "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "comment": "Accepted to ACL 2025 (Main Conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09301v1"}
{"id": "2506.09574", "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning", "authors": ["Gaurav Chaudhary", "Wassim Uddin Mondal", "Laxmidhar Behera"], "summary": "Sample efficiency and exploration remain critical challenges in Deep\nReinforcement Learning (DRL), particularly in complex domains. Offline RL,\nwhich enables agents to learn optimal policies from static, pre-collected\ndatasets, has emerged as a promising alternative. However, offline RL is\nconstrained by issues such as out-of-distribution (OOD) actions that limit\npolicy performance and generalization. To overcome these limitations, we\npropose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework\nthat unifies offline and online RL for efficient and scalable learning. While\nprevious hybrid methods rely on extensive design components and added\ncomputational complexity to utilize offline data effectively, MOORL introduces\na meta-policy that seamlessly adapts across offline and online trajectories.\nThis enables the agent to leverage offline data for robust initialization while\nutilizing online interactions to drive efficient exploration. Our theoretical\nanalysis demonstrates that the hybrid approach enhances exploration by\neffectively combining the complementary strengths of offline and online data.\nFurthermore, we demonstrate that MOORL learns a stable Q-function without added\ncomplexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL\nbenchmarks validate its effectiveness, showing consistent improvements over\nstate-of-the-art offline and hybrid RL baselines. With minimal computational\noverhead, MOORL achieves strong performance, underscoring its potential for\npractical applications in real-world scenarios.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09574v1"}
{"id": "2506.09699", "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander González Tomé", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09699v1"}
{"id": "2506.09315", "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "comment": "To be published in the proceedings of Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09315v1"}
{"id": "2506.09593", "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09593v1"}
{"id": "2506.09718", "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09718v1"}
{"id": "2506.09331", "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "authors": ["Arjun Vaithilingam Sudhakar"], "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09331v1"}
{"id": "2506.09594", "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery", "authors": ["Wenjin Qin", "Hailin Wang", "Jingyao Hou", "Jianjun Wang"], "summary": "Currently, existing tensor recovery methods fail to recognize the impact of\ntensor scale variations on their structural characteristics. Furthermore,\nexisting studies face prohibitive computational costs when dealing with\nlarge-scale high-order tensor data. To alleviate these issue, assisted by the\nKrylov subspace iteration, block Lanczos bidiagonalization process, and random\nprojection strategies, this article first devises two fast and accurate\nrandomized algorithms for low-rank tensor approximation (LRTA) problem.\nTheoretical bounds on the accuracy of the approximation error estimate are\nestablished. Next, we develop a novel generalized nonconvex modeling framework\ntailored to large-scale tensor recovery, in which a new regularization paradigm\nis exploited to achieve insightful prior representation for large-scale\ntensors. On the basis of the above, we further investigate new unified\nnonconvex models and efficient optimization algorithms, respectively, for\nseveral typical high-order tensor recovery tasks in unquantized and quantized\nsituations. To render the proposed algorithms practical and efficient for\nlarge-scale tensor data, the proposed randomized LRTA schemes are integrated\ninto their central and time-intensive computations. Finally, we conduct\nextensive experiments on various large-scale tensors, whose results demonstrate\nthe practicability, effectiveness and superiority of the proposed method in\ncomparison with some state-of-the-art approaches.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09594v1"}
{"id": "2506.09724", "title": "The Four Color Theorem for Cell Instance Segmentation", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "comment": "Accepted at ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09724v1"}
{"id": "2506.09335", "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "authors": ["Moshi Wei", "Sparks Li"], "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "comment": "11 pages, 1 figures,", "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09335v1"}
{"id": "2506.09613", "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot", "authors": ["Kaiwen Tuo", "Huan Wang"], "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09613v1"}
{"id": "2506.09735", "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09735v1"}
{"id": "2506.09338", "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09338v1"}
{"id": "2506.09625", "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "authors": ["Ekaterina Filimoshina", "Dmitry Shirokov"], "summary": "We propose, implement, and compare with competitors a new architecture of\nequivariant neural networks based on geometric (Clifford) algebras: Generalized\nLipschitz Group Equivariant Neural Networks (GLGENN). These networks are\nequivariant to all pseudo-orthogonal transformations, including rotations and\nreflections, of a vector space with any non-degenerate or degenerate symmetric\nbilinear form. We propose a weight-sharing parametrization technique that takes\ninto account the fundamental structures and operations of geometric algebras.\nDue to this technique, GLGENN architecture is parameter-light and has less\ntendency to overfitting than baseline equivariant models. GLGENN outperforms or\nmatches competitors on several benchmarking equivariant tasks, including\nestimation of an equivariant function and a convex hull experiment, while using\nsignificantly fewer optimizable parameters.", "comment": "Accepted to ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09625v1"}
{"id": "2506.09736", "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "comment": "Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09736v1"}
{"id": "2506.09340", "title": "RePO: Replay-Enhanced Policy Optimization", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "comment": "Project Page: https://github.com/SihengLi99/RePO", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09340v1"}
{"id": "2506.09630", "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "authors": ["Pol G. Recasens", "Alberto Gutierrez", "Jordi Torres", "Josep. Ll Berral", "Anisa Halimi", "Kieran Fraser"], "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.", "comment": "Paper accepted at ICML 2025 workshop DIG-BUG", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09630v1"}
{"id": "2506.09740", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09740v1"}
{"id": "2506.09342", "title": "Latent Multi-Head Attention for Small Language Models", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "comment": "6 pages, 1 figure. 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09342v1"}
{"id": "2506.09638", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09638v1"}
{"id": "2506.09745", "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09745v1"}
{"id": "2506.09347", "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "comment": "12 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09347v1"}
{"id": "2506.09660", "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "authors": ["Baran Can Gül", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09660v1"}
{"id": "2506.09748", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "comment": "8 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09748v1"}
{"id": "2506.09350", "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09350v1"}
{"id": "2506.09674", "title": "Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning", "authors": ["Alessandro Licciardi", "Davide Leo", "Davide Carbone"], "summary": "Federated Learning (FL) enables the training of machine learning models\nacross decentralized clients while preserving data privacy. However, the\npresence of anomalous or corrupted clients - such as those with faulty sensors\nor non representative data distributions - can significantly degrade model\nperformance. Detecting such clients without accessing raw data remains a key\nchallenge. We propose WAFFLE (Wavelet and Fourier representations for Federated\nLearning) a detection algorithm that labels malicious clients {\\it before\ntraining}, using locally computed compressed representations derived from\neither the Wavelet Scattering Transform (WST) or the Fourier Transform. Both\napproaches provide low-dimensional, task-agnostic embeddings suitable for\nunsupervised client separation. A lightweight detector, trained on a\ndistillated public dataset, performs the labeling with minimal communication\nand computational overhead. While both transforms enable effective detection,\nWST offers theoretical advantages, such as non-invertibility and stability to\nlocal deformations, that make it particularly well-suited to federated\nscenarios. Experiments on benchmark datasets show that our method improves\ndetection accuracy and downstream classification performance compared to\nexisting FL anomaly detection algorithms, validating its effectiveness as a\npre-training alternative to online detection strategies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09674v1"}
{"id": "2506.09777", "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09777v1"}
{"id": "2506.09354", "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09354v1"}
{"id": "2506.09682", "title": "Wasserstein Hypergraph Neural Network", "authors": ["Iulia Duta", "Pietro Liò"], "summary": "The ability to model relational information using machine learning has driven\nadvancements across various domains, from medicine to social science. While\ngraph representation learning has become mainstream over the past decade,\nrepresenting higher-order relationships through hypergraphs is rapidly gaining\nmomentum. In the last few years, numerous hypergraph neural networks have\nemerged, most of them falling under a two-stage, set-based framework. The\nmessages are sent from nodes to edges and then from edges to nodes. However,\nmost of the advancement still takes inspiration from the graph counterpart,\noften simplifying the aggregations to basic pooling operations. In this paper\nwe are introducing Wasserstein Hypergraph Neural Network, a model that treats\nthe nodes and hyperedge neighbourhood as distributions and aggregate the\ninformation using Sliced Wasserstein Pooling. Unlike conventional aggregators\nsuch as mean or sum, which only capture first-order statistics, our approach\nhas the ability to preserve geometric properties like the shape and spread of\ndistributions. This enables the learned embeddings to reflect how easily one\nhyperedge distribution can be transformed into another, following principles of\noptimal transport. Experimental results demonstrate that applying Wasserstein\npooling in a hypergraph setting significantly benefits node classification\ntasks, achieving top performance on several real-world datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09682v1"}
{"id": "2506.09782", "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "comment": "20 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09782v1"}
{"id": "2506.09362", "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09362v1"}
{"id": "2506.09701", "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09701v1"}
{"id": "2506.09784", "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09784v1"}
{"id": "2506.09363", "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "comment": "Under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09363v1"}
{"id": "2506.09714", "title": "Auto-Compressing Networks", "authors": ["Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "summary": "Deep neural networks with short residual connections have demonstrated\nremarkable success across domains, but increasing depth often introduces\ncomputational redundancy without corresponding improvements in representation\nquality. In this work, we introduce Auto-Compressing Networks (ACNs), an\narchitectural variant where additive long feedforward connections from each\nlayer to the output replace traditional short residual connections. ACNs\nshowcase a unique property we coin as \"auto-compression\", the ability of a\nnetwork to organically compress information during training with gradient\ndescent, through architectural design alone. Through auto-compression,\ninformation is dynamically \"pushed\" into early layers during training,\nenhancing their representational quality and revealing potential redundancy in\ndeeper ones. We theoretically show that this property emerges from layer-wise\ntraining patterns present in ACNs, where layers are dynamically utilized during\ntraining based on task requirements. We also find that ACNs exhibit enhanced\nnoise robustness compared to residual networks, superior performance in\nlow-data settings, improved transfer learning capabilities, and mitigate\ncatastrophic forgetting suggesting that they learn representations that\ngeneralize better despite using fewer parameters. Our results demonstrate up to\n18% reduction in catastrophic forgetting and 30-80% architectural compression\nwhile maintaining accuracy across vision transformers, MLP-mixers, and BERT\narchitectures. Furthermore, we demonstrate that coupling ACNs with traditional\npruning techniques, enables significantly better sparsity-performance\ntrade-offs compared to conventional architectures. These findings establish\nACNs as a practical approach to developing efficient neural architectures that\nautomatically adapt their computational footprint to task complexity, while\nlearning robust representations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09714v1"}
{"id": "2506.09814", "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09814v1"}
{"id": "2506.09367", "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "comment": "BEA 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09367v1"}
{"id": "2506.09733", "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "authors": ["Minjong Cheon"], "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09733v1"}
{"id": "2506.09834", "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09834v1"}
{"id": "2506.09368", "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "comment": "20 pages, 11 figures, 13 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09368v1"}
{"id": "2506.09738", "title": "Towards Multi-modal Graph Large Language Model", "authors": ["Xin Wang", "Zeyang Zhang", "Linxin Xiao", "Haibo Chen", "Chendi Ge", "Wenwu Zhu"], "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09738v1"}
{"id": "2506.09836", "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09836v1"}
{"id": "2506.09373", "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09373v1"}
{"id": "2506.09742", "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "comment": "Accepted at AAMAS 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09742v1"}
{"id": "2506.09839", "title": "OctoNav: Towards Generalist Embodied Navigation", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "comment": "31 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09839v1"}
{"id": "2506.09383", "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09383v1"}
{"id": "2506.09769", "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09769v1"}
{"id": "2506.09846", "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "comment": "17 pages, 10 figures, Under Review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09846v1"}
{"id": "2506.09396", "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "authors": ["Zongjie Li", "Shuai Wang"], "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09396v1"}
{"id": "2506.09781", "title": "On the Similarities of Embeddings in Contrastive Learning", "authors": ["Chungpa Lee", "Sehee Lim", "Kibok Lee", "Jy-yong Sohn"], "summary": "Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.", "comment": "contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09781v1"}
{"id": "2506.09849", "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09849v1"}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "comment": "6 pages, 9 figures, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09397v1"}
{"id": "2506.09785", "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09785v1"}
{"id": "2506.09881", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09881v1"}
{"id": "2506.09408", "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09408v1"}
{"id": "2506.09803", "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09803v1"}
{"id": "2506.09883", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09883v1"}
{"id": "2506.09411", "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09411v1"}
{"id": "2506.09810", "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "authors": ["Minoh Jeong", "Alfred Hero"], "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09810v1"}
{"id": "2506.09885", "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09885v1"}
{"id": "2506.09427", "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09427v1"}
{"id": "2506.09813", "title": "Metritocracy: Representative Metrics for Lite Benchmarks", "authors": ["Ariel Procaccia", "Benjamin Schiffer", "Serena Wang", "Shirley Zhang"], "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09813v1"}
{"id": "2506.09895", "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "comment": "19 pages, 11 Figures, 13 Tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09895v1"}
{"id": "2506.09428", "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "authors": ["Fei Ding", "Baiqiao Wang"], "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09428v1"}
{"id": "2506.09816", "title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "authors": ["Cecilia Casolo", "Sören Becker", "Niki Kilbertus"], "summary": "Dynamical systems modeling is a core pillar of scientific inquiry across\nnatural and life sciences. Increasingly, dynamical system models are learned\nfrom data, rendering identifiability a paramount concept. For systems that are\nnot identifiable from data, no guarantees can be given about their behavior\nunder new conditions and inputs, or about possible control mechanisms to steer\nthe system. It is known in the community that \"linear ordinary differential\nequations (ODE) are almost surely identifiable from a single trajectory.\"\nHowever, this only holds for dense matrices. The sparse regime remains\nunderexplored, despite its practical relevance with sparsity arising naturally\nin many biological, social, and physical systems. In this work, we address this\ngap by characterizing the identifiability of sparse linear ODEs. Contrary to\nthe dense case, we show that sparse systems are unidentifiable with a positive\nprobability in practically relevant sparsity regimes and provide lower bounds\nfor this probability. We further study empirically how this theoretical\nunidentifiability manifests in state-of-the-art methods to estimate linear ODEs\nfrom data. Our results corroborate that sparse systems are also practically\nunidentifiable. Theoretical limitations are not resolved through inductive\nbiases or optimization dynamics. Our findings call for rethinking what can be\nexpected from data-driven dynamical system modeling and allows for quantitative\nassessments of how much to trust a learned linear ODE.", "comment": "9 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09816v1"}
{"id": "2506.09897", "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "authors": ["Tao Liu", "Zhenchao Cui"], "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09897v1"}
{"id": "2506.09434", "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09434v1"}
{"id": "2506.09824", "title": "Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity", "authors": ["Johan Erbani", "Sonia Ben Mokhtar", "Pierre-Edouard Portier", "Elod Egyed-Zsigmond", "Diana Nurbakova"], "summary": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09824v1"}
{"id": "2506.09916", "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09916v1"}
{"id": "2506.09440", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "comment": "ACL-2025 System Demo", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09440v1"}
{"id": "2506.09862", "title": "Guided Graph Compression for Quantum Graph Neural Networks", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarcón", "Sofia Vallecorsa", "Michele Grossi"], "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09862v1"}
{"id": "2506.09919", "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09919v1"}
{"id": "2506.09445", "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09445v1"}
{"id": "2506.09867", "title": "Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing", "authors": ["Amit Baran Dey", "Wasim Arif", "Rakhesh Singh Kshetrimayum"], "summary": "This paper proposes a machine learning-based methodology for the\nclassification of various oil samples based on their dielectric properties,\nutilizing a microwave resonant sensor. The dielectric behaviour of oils,\ngoverned by their molecular composition, induces distinct shifts in the\nsensor's resonant frequency and amplitude response. These variations are\nsystematically captured and processed to extract salient features, which serve\nas inputs for multiple machine learning classifiers. The microwave resonant\nsensor operates in a non-destructive, low-power manner, making it particularly\nwell-suited for real-time industrial applications. A comprehensive dataset is\ndeveloped by varying the permittivity of oil samples and acquiring the\ncorresponding sensor responses. Several classifiers are trained and evaluated\nusing the extracted resonant features to assess their capability in\ndistinguishing between oil types. Experimental results demonstrate that the\nproposed approach achieves a high classification accuracy of 99.41% with the\nrandom forest classifier, highlighting its strong potential for automated oil\nidentification. The system's compact form factor, efficiency, and high\nperformance underscore its viability for fast and reliable oil characterization\nin industrial environments.", "comment": "6 pages, 11 figures, Accepted to IEEE INDISCON 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09867v1"}
{"id": "2506.09920", "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09920v1"}
{"id": "2506.09450", "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09450v1"}
{"id": "2506.09870", "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "authors": ["Maximilian Egger", "Rawad Bitar"], "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09870v1"}
{"id": "2506.09932", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "comment": "4 Pages, 5 Figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09932v1"}
{"id": "2506.09455", "title": "Abstraction-Based Proof Production in Formal Verification of Neural Networks", "authors": ["Yizhak Yisrael Elboher", "Omri Isac", "Guy Katz", "Tobias Ladner", "Haoze Wu"], "summary": "Modern verification tools for deep neural networks (DNNs) increasingly rely\non abstraction to scale to realistic architectures. In parallel, proof\nproduction is becoming a critical requirement for increasing the reliability of\nDNN verification results. However, current proofproducing verifiers do not\nsupport abstraction-based reasoning, creating a gap between scalability and\nprovable guarantees. We address this gap by introducing a novel framework for\nproof-producing abstraction-based DNN verification. Our approach modularly\nseparates the verification task into two components: (i) proving the\ncorrectness of an abstract network, and (ii) proving the soundness of the\nabstraction with respect to the original DNN. The former can be handled by\nexisting proof-producing verifiers, whereas we propose the first method for\ngenerating formal proofs for the latter. This preliminary work aims to enable\nscalable and trustworthy verification by supporting common abstraction\ntechniques within a formal proof framework.", "comment": "To appear in SAIV 2025", "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.09455v1"}
{"id": "2506.09887", "title": "Learning single-index models via harmonic decomposition", "authors": ["Nirmit Joshi", "Hugo Koubbi", "Theodor Misiakiewicz", "Nathan Srebro"], "summary": "We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.", "comment": "80 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09887v1"}
{"id": "2506.09935", "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "comment": "Project page: https://leo-vl.github.io", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09935v1"}
{"id": "2506.09485", "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09485v1"}
{"id": "2506.09891", "title": "Causal Climate Emulation with Bayesian Filtering", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "comment": "32 pages, 21 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09891v1"}
{"id": "2506.09943", "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09943v1"}
{"id": "2506.09487", "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09487v1"}
{"id": "2506.09896", "title": "A look at adversarial attacks on radio waveforms from discrete latent space", "authors": ["Attanasia Garuso", "Silvija Kokalj-Filipovic", "Yagna Kaasaragadda"], "summary": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09896v1"}
{"id": "2506.09952", "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "comment": "Accepted to CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09952v1"}
{"id": "2506.09496", "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09496v1"}
{"id": "2506.09901", "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)", "authors": ["Noel Brindise", "Vijeth Hebbar", "Riya Shah", "Cedric Langbort"], "summary": "In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09901v1"}
{"id": "2506.09953", "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09953v1"}
{"id": "2506.09499", "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "comment": "12 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09499v1"}
{"id": "2506.09923", "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning", "authors": ["Liou Tang", "James Joshi", "Ashish Kundu"], "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09923v1"}
{"id": "2506.09954", "title": "Vision Generalist Model: A Survey", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "comment": "Accepted by International Journal of Computer Vision (IJCV)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09954v1"}
{"id": "2506.09507", "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09507v1"}
{"id": "2506.09928", "title": "Bayesian Probabilistic Matrix Factorization", "authors": ["Ruixuan Xu", "Xiangxiang Weng"], "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.", "comment": "11 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09928v1"}
{"id": "2506.09958", "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09958v1"}
{"id": "2506.09508", "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09508v1"}
{"id": "2506.09940", "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09940v1"}
{"id": "2506.09965", "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09965v1"}
{"id": "2506.09513", "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "comment": "24 pages, 6 figures, 7 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09513v1"}
{"id": "2506.09955", "title": "Canonical Latent Representations in Conditional Diffusion Models", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "comment": "45 pages,41 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09955v1"}
{"id": "2506.09969", "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09969v1"}
{"id": "2506.09520", "title": "How attention simplifies mental representations for planning", "authors": ["Jason da Silva Castanheira", "Nicholas Shea", "Stephen M. Fleming"], "summary": "Human planning is efficient -- it frugally deploys limited cognitive\nresources to accomplish difficult tasks -- and flexible -- adapting to novel\nproblems and environments. Computational approaches suggest that people\nconstruct simplified mental representations of their environment, balancing the\ncomplexity of a task representation with its utility. These models imply a\nnested optimisation in which planning shapes perception, and perception shapes\nplanning -- but the perceptual and attentional mechanisms governing how this\ninteraction unfolds remain unknown. Here, we harness virtual maze navigation to\ncharacterise how spatial attention controls which aspects of a task\nrepresentation enter subjective awareness and are available for planning. We\nfind that spatial proximity governs which aspects of a maze are available for\nplanning, and that when task-relevant information follows natural (lateralised)\ncontours of attention, people can more easily construct simplified and useful\nmaze representations. This influence of attention varies considerably across\nindividuals, explaining differences in people's task representations and\nbehaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the\neffects of visuospatial attention into existing computational accounts of\nvalue-guided construal. Together, our work bridges computational perspectives\non perception and decision-making to better understand how individuals\nrepresent their environments in aid of planning.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.09520v1"}
{"id": "2506.09991", "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09991v1"}
{"id": "2506.09980", "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09980v1"}
{"id": "2506.09522", "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "authors": ["Beomsik Cho", "Jaehyung Kim"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "comment": "Code available at https://github.com/bscho333/ReVisiT", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09522v1"}
{"id": "2506.09998", "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "comment": "Technical Report v1 (21 pages, 14 figures)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09998v1"}
{"id": "2506.09981", "title": "ReSim: Reliable World Simulation for Autonomous Driving", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "comment": "Project page: https://opendrivelab.com/ReSim", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09981v1"}
{"id": "2506.09526", "title": "Neural Functions for Learning Periodic Signal", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09526v1"}
{"id": "2505.14156", "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2505.14156v1"}
{"id": "2506.09982", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09982v1"}
{"id": "2506.09532", "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09532v1"}
{"id": "2506.06905", "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.06905v2"}
{"id": "2506.09984", "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09984v1"}
{"id": "2506.09548", "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "comment": "Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09548v1"}
{"id": "2506.08672", "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "comment": "22 pages, 10 figures, 8 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08672v1"}
{"id": "2506.09987", "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09987v1"}
{"id": "2506.09557", "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09557v1"}
{"id": "2506.09063", "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09063v1"}
{"id": "2506.09988", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09988v1"}
{"id": "2506.09566", "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "comment": "To-appear as a book chapter", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09566v1"}
{"id": "2506.09065", "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "comment": "6 pages, 8 figures, and 1 table", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09065v1"}
{"id": "2506.09989", "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09989v1"}
{"id": "2506.09600", "title": "Effective Red-Teaming of Policy-Adherent Agents", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09600v1"}
{"id": "2506.09068", "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09068v1"}
{"id": "2506.09993", "title": "Text-Aware Image Restoration with Diffusion Models", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09993v1"}
{"id": "2506.09634", "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09634v1"}
{"id": "2506.09069", "title": "Devanagari Digit Recognition using Quantum Machine Learning", "authors": ["Sahaj Raj Malla"], "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09069v1"}
{"id": "2506.09995", "title": "PlayerOne: Egocentric World Simulator", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "comment": "Project page: https://playerone-hku.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09995v1"}
{"id": "2506.09644", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09644v1"}
{"id": "2506.09075", "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09075v1"}
{"id": "2506.09063", "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09063v1"}
{"id": "2506.09662", "title": "Empirical Quantification of Spurious Correlations in Malware Detection", "authors": ["Bianca Perasso", "Ludovico Lozza", "Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Fabio Roli"], "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09662v1"}
{"id": "2506.09076", "title": "A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models", "authors": ["Haley Stone", "Jing Du", "Hao Xue", "Matthew Scotch", "David Heslop", "Andreas Züfle", "Chandini Raina MacIntyre", "Flora Salim"], "summary": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows.", "comment": "9 pages, 3 figures", "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.09076v1"}
{"id": "2506.09065", "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "comment": "6 pages, 8 figures, and 1 table", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09065v1"}
{"id": "2506.09672", "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09672v1"}
{"id": "2506.09082", "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "comment": "First two authors contribute equally", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09082v1"}
{"id": "2506.09069", "title": "Devanagari Digit Recognition using Quantum Machine Learning", "authors": ["Sahaj Raj Malla"], "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09069v1"}
{"id": "2506.09677", "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09677v1"}
{"id": "2506.09097", "title": "Detecting malignant dynamics on very few blood sample using signature coefficients", "authors": ["Rémi Vaucher", "Stéphane Chrétien"], "summary": "Recent discoveries have suggested that the promising avenue of using\ncirculating tumor DNA (ctDNA) levels in blood samples provides reasonable\naccuracy for cancer monitoring, with extremely low burden on the patient's\nside. It is known that the presence of ctDNA can result from various mechanisms\nleading to DNA release from cells, such as apoptosis, necrosis or active\nsecretion. One key idea in recent cancer monitoring studies is that monitoring\nthe dynamics of ctDNA levels might be sufficient for early multi-cancer\ndetection. This interesting idea has been turned into commercial products, e.g.\nin the company named GRAIL.\n  In the present work, we propose to explore the use of Signature theory for\ndetecting aggressive cancer tumors based on the analysis of blood samples. Our\napproach combines tools from continuous time Markov modelling for the dynamics\nof ctDNA levels in the blood, with Signature theory for building efficient\ntesting procedures. Signature theory is a topic of growing interest in the\nMachine Learning community (see Chevyrev2016 and Fermanian2021), which is now\nrecognised as a powerful feature extraction tool for irregularly sampled\nsignals. The method proposed in the present paper is shown to correctly address\nthe challenging problem of overcoming the inherent data scarsity due to the\nextremely small number of blood samples per patient. The relevance of our\napproach is illustrated with extensive numerical experiments that confirm the\nefficiency of the proposed pipeline.", "comment": "Under review", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09097v1"}
{"id": "2506.09075", "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09075v1"}
{"id": "2506.09695", "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "comment": "11 pages, 5 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09695v1"}
{"id": "2506.09100", "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09100v1"}
{"id": "2506.09095", "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Clárisa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09095v1"}
{"id": "2506.09701", "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09701v1"}
{"id": "2506.09106", "title": "Bias Analysis in Unconditional Image Generative Models", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09106v1"}
{"id": "2506.09098", "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "comment": "https://youtu.be/AQAgVdrx1DE", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09098v1"}
{"id": "2506.09718", "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09718v1"}
{"id": "2506.09176", "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "comment": "ICML 2025 Poster", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09176v1"}
{"id": "2506.09100", "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09100v1"}
{"id": "2506.09733", "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "authors": ["Minjong Cheon"], "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09733v1"}
{"id": "2506.09209", "title": "Revisiting Graph Projections for Effective Complementary Product Recommendation", "authors": ["Leandro Anghinoni", "Pablo Zivic", "Jorge Adrian Sanchez"], "summary": "Complementary product recommendation is a powerful strategy to improve\ncustomer experience and retail sales. However, recommending the right product\nis not a simple task because of the noisy and sparse nature of user-item\ninteractions. In this work, we propose a simple yet effective method to predict\na list of complementary products given a query item, based on the structure of\na directed weighted graph projected from the user-item bipartite graph. We\nrevisit bipartite graph projections for recommender systems and propose a novel\napproach for inferring complementarity relationships from historical user-item\ninteractions. We compare our model with recent methods from the literature and\nshow, despite the simplicity of our approach, an average improvement of +43%\nand +38% over sequential and graph-based recommenders, respectively, over\ndifferent benchmarks.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09209v1"}
{"id": "2506.09161", "title": "An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation", "authors": ["Rajan Das Gupta", "Md Imrul Hasan Showmick", "Mushfiqur Rahman Abir", "Shanjida Akter", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Early and accurate detection of brain abnormalities, such as tumors and\nstrokes, is essential for timely intervention and improved patient outcomes. In\nthis study, we present a deep learning-based system capable of identifying both\nbrain tumors and strokes from MRI images, along with their respective stages.\nWe have executed two groundbreaking strategies involving convolutional neural\nnetworks, MobileNet V2 and ResNet-50-optimized through transfer learning to\nclassify MRI scans into five diagnostic categories. Our dataset, aggregated and\naugmented from various publicly available MRI sources, was carefully curated to\nensure class balance and image diversity. To enhance model generalization and\nprevent overfitting, we applied dropout layers and extensive data augmentation.\nThe models achieved strong performance, with training accuracy reaching 93\\%\nand validation accuracy up to 88\\%. While ResNet-50 demonstrated slightly\nbetter results, Mobile Net V2 remains a promising option for real-time\ndiagnosis in low resource settings due to its lightweight architecture. This\nresearch offers a practical AI-driven solution for early brain abnormality\ndetection, with potential for clinical deployment and future enhancement\nthrough larger datasets and multi modal inputs.", "comment": "Accepted in MECON 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09161v1"}
{"id": "2506.09736", "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "comment": "Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09736v1"}
{"id": "2506.09237", "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09237v1"}
{"id": "2506.09162", "title": "The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset", "authors": ["Tyler J. Richards", "Adam E. Flanders", "Errol Colak", "Luciano M. Prevedello", "Robyn L. Ball", "Felipe Kitamura", "John Mongan", "Maryam Vazirabad", "Hui-Ming Lin", "Anne Kendell", "Thanat Kanthawang", "Salita Angkurawaranon", "Emre Altinmakas", "Hakan Dogan", "Paulo Eduardo de Aguiar Kuriki", "Arjuna Somasundaram", "Christopher Ruston", "Deniz Bulja", "Naida Spahovic", "Jennifer Sommer", "Sirui Jiang", "Eduardo Moreno Judice de Mattos Farina", "Eduardo Caminha Nunes", "Michael Brassil", "Megan McNamara", "Johanna Ortiz", "Jacob Peoples", "Vinson L. Uytana", "Anthony Kam", "Venkata N. S. Dola", "Daniel Murphy", "David Vu", "Dataset Contributor Group", "Dataset Annotator Group", "Competition Data Notebook Group", "Jason F. Talbott"], "summary": "The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging\nSpine Classification (LumbarDISC) dataset is the largest publicly available\ndataset of adult MRI lumbar spine examinations annotated for degenerative\nchanges. The dataset includes 2,697 patients with a total of 8,593 image series\nfrom 8 institutions across 6 countries and 5 continents. The dataset is\navailable for free for non-commercial use via Kaggle and RSNA Medical Imaging\nResource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine\nDegenerative Classification competition where competitors developed deep\nlearning models to grade degenerative changes in the lumbar spine. The degree\nof spinal canal, subarticular recess, and neural foraminal stenosis was graded\nat each intervertebral disc level in the lumbar spine. The images were\nannotated by expert volunteer neuroradiologists and musculoskeletal\nradiologists from the RSNA, American Society of Neuroradiology, and the\nAmerican Society of Spine Radiology. This dataset aims to facilitate research\nand development in machine learning and lumbar spine imaging to lead to\nimproved patient care and clinical efficiency.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09162v1"}
{"id": "2506.09740", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09740v1"}
{"id": "2506.09250", "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "authors": ["C. Opus", "A. Lawsen"], "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "comment": "Comment on: arXiv:2506.06941", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09250v1"}
{"id": "2506.09172", "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09172v1"}
{"id": "2506.09742", "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "comment": "Accepted at AAMAS 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09742v1"}
{"id": "2506.09255", "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization", "authors": ["Saeed Hashemi", "Genchang Peng", "Mehrdad Nourani", "Omar Nofal", "Jay Harvey"], "summary": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.", "comment": "Accepted to be presented at the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025). This\n  version is submitted to arXiv prior to final IEEE formatting and publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09255v1"}
{"id": "2506.09217", "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09217v1"}
{"id": "2506.09749", "title": "Large Language Models for Design Structure Matrix Optimization", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09749v1"}
{"id": "2506.09278", "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "comment": "Project Page: https://uniflowmatch.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09278v1"}
{"id": "2506.09284", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09284v1"}
{"id": "2506.09755", "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09755v1"}
{"id": "2506.09280", "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09280v1"}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1"}
{"id": "2506.09769", "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09769v1"}
{"id": "2506.09282", "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs", "authors": ["Dhruv Parikh", "Viktor Prasanna"], "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.", "comment": "IC3", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09282v1"}
{"id": "2506.09353", "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt", "authors": ["Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across\nvarious applications but remain vulnerable to malicious queries that exploit\nthe visual modality. Existing alignment approaches typically fail to resist\nmalicious queries while preserving utility on benign ones effectively. To\naddress these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),\nwhich is built upon two key innovations. First, we introduce the Visual Safety\nPrompt, which appends a trainable padding region around the input image. It\npreserves visual features and expands the optimization space. Second, we\npropose Deep Alignment, a novel approach to train the visual safety prompt\nthrough supervision in the model's activation space. It enhances the inherent\nability of LVLMs to perceive malicious queries, achieving deeper alignment than\nprior works. Extensive experiments across five benchmarks on two representative\nLVLMs demonstrate that DAVSP effectively resists malicious queries while\npreserving benign input utility. Furthermore, DAVSP exhibits great cross-model\ngeneration ability. Ablation studies further reveal that both the Visual Safety\nPrompt and Deep Alignment are essential components, jointly contributing to its\noverall effectiveness. The code is publicly available at\nhttps://github.com/zhangyitonggg/DAVSP.", "comment": "16 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09353v1"}
{"id": "2506.09777", "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09777v1"}
{"id": "2506.09299", "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "comment": "6 Pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09299v1"}
{"id": "2506.09373", "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09373v1"}
{"id": "2506.09782", "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "comment": "20 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09782v1"}
{"id": "2506.09312", "title": "What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?", "authors": ["Erik Buchholz", "Natasha Fernandes", "David D. Nguyen", "Alsharif Abuadbba", "Surya Nepal", "Salil S. Kanhere"], "summary": "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09312v1"}
{"id": "2506.09491", "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09491v1"}
{"id": "2506.09785", "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09785v1"}
{"id": "2506.09313", "title": "Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features", "authors": ["Angel Yanguas-Gil", "Jeffrey W. Elam"], "summary": "In this work we explore surrogate models to optimize plasma enhanced atomic\nlayer deposition (PEALD) in high aspect ratio features. In plasma-based\nprocesses such as PEALD and atomic layer etching, surface recombination can\ndominate the reactivity of plasma species with the surface, which can lead to\nunfeasibly long exposure times to achieve full conformality inside\nnanostructures like high aspect ratio vias. Using a synthetic dataset based on\nsimulations of PEALD, we train artificial neural networks to predict saturation\ntimes based on cross section thickness data obtained for partially coated\nconditions. The results obtained show that just two experiments in\nundersaturated conditions contain enough information to predict saturation\ntimes within 10% of the ground truth. A surrogate model trained to determine\nwhether surface recombination dominates the plasma-surface interactions in a\nPEALD process achieves 99% accuracy. This demonstrates that machine learning\ncan provide a new pathway to accelerate the optimization of PEALD processes in\nareas such as microelectronics. Our approach can be easily extended to atomic\nlayer etching and more complex structures.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.09313v1"}
{"id": "2506.09532", "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09532v1"}
{"id": "2506.09820", "title": "CoRT: Code-integrated Reasoning within Thinking", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "comment": "work in progress", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09820v1"}
{"id": "2506.09315", "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "comment": "To be published in the proceedings of Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09315v1"}
{"id": "2506.09552", "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "comment": "Preprint, Journal of Intelligent & Robotic Systems", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09552v1"}
{"id": "2506.09822", "title": "Superstudent intelligence in thermodynamics", "authors": ["Rebecca Loubet", "Pascal Zittlau", "Marco Hoffmann", "Luisa Vollmer", "Sophie Fellenz", "Heike Leitte", "Fabian Jirasek", "Johannes Lenhard", "Hans Hasse"], "summary": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.", "comment": "This document is the unedited Author's version of a yet to be\n  Submitted Work to Physical Review Physics Education Research. 15 pages, 2\n  figures, Graphical Abstract, Highlights and SI available (12 pages)", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09822v1"}
{"id": "2506.09338", "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09338v1"}
{"id": "2506.09638", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09638v1"}
{"id": "2506.09827", "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sören Auer"], "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09827v1"}
{"id": "2506.09340", "title": "RePO: Replay-Enhanced Policy Optimization", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "comment": "Project Page: https://github.com/SihengLi99/RePO", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09340v1"}
{"id": "2506.09643", "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09643v1"}
{"id": "2506.09836", "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09836v1"}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1"}
{"id": "2506.09661", "title": "A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma", "authors": ["Garima Jain", "Sanghamitra Pati", "Mona Duggal", "Amit Sethi", "Abhijeet Patil", "Gururaj Malekar", "Nilesh Kowe", "Jitender Kumar", "Jatin Kashyap", "Divyajeet Rout", "Deepali", "Hitesh", "Nishi Halduniya", "Sharat Kumar", "Heena Tabassum", "Rupinder Singh Dhaliwal", "Sucheta Devi Khuraijam", "Sushma Khuraijam", "Sharmila Laishram", "Simmi Kharb", "Sunita Singh", "K. Swaminadtan", "Ranjana Solanki", "Deepika Hemranjani", "Shashank Nath Singh", "Uma Handa", "Manveen Kaur", "Surinder Singhal", "Shivani Kalhan", "Rakesh Kumar Gupta", "Ravi. S", "D. Pavithra", "Sunil Kumar Mahto", "Arvind Kumar", "Deepali Tirkey", "Saurav Banerjee", "L. Sreelakshmi"], "summary": "Oral squamous cell carcinoma OSCC is a major global health burden,\nparticularly in several regions across Asia, Africa, and South America, where\nit accounts for a significant proportion of cancer cases. Early detection\ndramatically improves outcomes, with stage I cancers achieving up to 90 percent\nsurvival. However, traditional diagnosis based on histopathology has limited\naccessibility in low-resource settings because it is invasive,\nresource-intensive, and reliant on expert pathologists. On the other hand, oral\ncytology of brush biopsy offers a minimally invasive and lower cost\nalternative, provided that the remaining challenges, inter observer variability\nand unavailability of expert pathologists can be addressed using artificial\nintelligence. Development and validation of robust AI solutions requires access\nto large, labeled, and multi-source datasets to train high capacity models that\ngeneralize across domain shifts. We introduce the first large and multicenter\noral cytology dataset, comprising annotated slides stained with\nPapanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten\ntertiary medical centers in India. The dataset is labeled and annotated by\nexpert pathologists for cellular anomaly classification and detection, is\ndesigned to advance AI driven diagnostic methods. By filling the gap in\npublicly available oral cytology datasets, this resource aims to enhance\nautomated detection, reduce diagnostic errors, and improve early OSCC diagnosis\nin resource-constrained settings, ultimately contributing to reduced mortality\nand better patient outcomes worldwide.", "comment": "7 pages, 2 figurs", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09661v1"}
{"id": "2506.09839", "title": "OctoNav: Towards Generalist Embodied Navigation", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "comment": "31 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09839v1"}
{"id": "2506.09350", "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09350v1"}
{"id": "2506.09665", "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09665v1"}
{"id": "2506.09846", "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "comment": "17 pages, 10 figures, Under Review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09846v1"}
{"id": "2506.09366", "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09366v1"}
{"id": "2506.09709", "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "comment": "Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09709v1"}
{"id": "2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09847v1"}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "comment": "6 pages, 9 figures, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09397v1"}
{"id": "2506.09733", "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "authors": ["Minjong Cheon"], "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09733v1"}
{"id": "2506.09853", "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09853v1"}
{"id": "2506.09401", "title": "A theoretical basis for model collapse in recursive training", "authors": ["Vivek Shripad Borkar"], "summary": "It is known that recursive training from generative models can lead to the so\ncalled `collapse' of the simulated probability distribution. This note shows\nthat one in fact gets two different asymptotic behaviours depending on whether\nan external source, howsoever minor, is also contributing samples.", "comment": null, "cate": "math.PR", "url": "http://arxiv.org/abs/2506.09401v1"}
{"id": "2506.09790", "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09790v1"}
{"id": "2506.09862", "title": "Guided Graph Compression for Quantum Graph Neural Networks", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarcón", "Sofia Vallecorsa", "Michele Grossi"], "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09862v1"}
{"id": "2506.09406", "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09406v1"}
{"id": "2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09847v1"}
{"id": "2506.09873", "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09873v1"}
{"id": "2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09420v1"}
{"id": "2506.09930", "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "comment": "Under review", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09930v1"}
{"id": "2506.09883", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09883v1"}
{"id": "2506.09422", "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09422v1"}
{"id": "2506.09934", "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09934v1"}
{"id": "2506.09886", "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09886v1"}
{"id": "2506.09434", "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09434v1"}
{"id": "2506.09949", "title": "Sampling Theory for Super-Resolution with Implicit Neural Representations", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.", "comment": "arXiv admin note: text overlap with arXiv:2405.18410", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09949v1"}
{"id": "2506.09890", "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09890v1"}
{"id": "2506.09441", "title": "Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking", "authors": ["Piyush Mishra", "Philippe Roudot"], "summary": "Tracking multiple particles in noisy and cluttered scenes remains challenging\ndue to a combinatorial explosion of trajectory hypotheses, which scales\nsuper-exponentially with the number of particles and frames. The transformer\narchitecture has shown a significant improvement in robustness against this\nhigh combinatorial load. However, its performance still falls short of the\nconventional Bayesian filtering approaches in scenarios presenting a reduced\nset of trajectory hypothesis. This suggests that while transformers excel at\nnarrowing down possible associations, they may not be able to reach the\noptimality of the Bayesian approach in locally sparse scenario. Hence, we\nintroduce a hybrid tracking framework that combines the ability of\nself-attention to learn the underlying representation of particle behavior with\nthe reliability and interpretability of Bayesian filtering. We perform\ntrajectory-to-detection association by solving a label prediction problem,\nusing a transformer encoder to infer soft associations between detections\nacross frames. This prunes the hypothesis set, enabling efficient\nmultiple-particle tracking in Bayesian filtering framework. Our approach\ndemonstrates improved tracking accuracy and robustness against spurious\ndetections, offering a solution for high clutter multiple particle tracking\nscenarios.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09441v1"}
{"id": "2506.09955", "title": "Canonical Latent Representations in Conditional Diffusion Models", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "comment": "45 pages,41 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09955v1"}
{"id": "2506.09891", "title": "Causal Climate Emulation with Bayesian Filtering", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "comment": "32 pages, 21 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09891v1"}
{"id": "2506.09457", "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09457v1"}
{"id": "2506.09985", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09985v1"}
{"id": "2506.09902", "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "comment": "Accepted to ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09902v1"}
{"id": "2506.09487", "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09487v1"}
{"id": "2506.09990", "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09990v1"}
{"id": "2506.09932", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "comment": "4 Pages, 5 Figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09932v1"}
{"id": "2506.09495", "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09495v1"}
{"id": "2506.09997", "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09997v1"}
{"id": "2506.09937", "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "comment": "Project Page: https://vla-safe.github.io/", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09937v1"}
{"id": "2506.09512", "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "comment": "7 pages, 1 figure", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09512v1"}
{"id": "2506.09940", "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09940v1"}
{"id": "2506.09516", "title": "LLM-Powered CPI Prediction Inference with Online Text Time Series", "authors": ["Yingying Fan", "Jinchi Lv", "Ao Sun", "Yurou Wang"], "summary": "Forecasting the Consumer Price Index (CPI) is an important yet challenging\ntask in economics, where most existing approaches rely on low-frequency,\nsurvey-based data. With the recent advances of large language models (LLMs),\nthere is growing potential to leverage high-frequency online text data for\nimproved CPI prediction, an area still largely unexplored. This paper proposes\nLLM-CPI, an LLM-based approach for CPI prediction inference incorporating\nonline text time series. We collect a large set of high-frequency online texts\nfrom a popularly used Chinese social network site and employ LLMs such as\nChatGPT and the trained BERT models to construct continuous inflation labels\nfor posts that are related to inflation. Online text embeddings are extracted\nvia LDA and BERT. We develop a joint time series framework that combines\nmonthly CPI data with LLM-generated daily CPI surrogates. The monthly model\nemploys an ARX structure combining observed CPI data with text embeddings and\nmacroeconomic variables, while the daily model uses a VARX structure built on\nLLM-generated CPI surrogates and text embeddings. We establish the asymptotic\nproperties of the method and provide two forms of constructed prediction\nintervals. The finite-sample performance and practical advantages of LLM-CPI\nare demonstrated through both simulation and real data examples.", "comment": "73 pages, 13 figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09516v1"}
{"id": "2506.09942", "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "comment": "16 pages, 8 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09942v1"}
{"id": "2506.09548", "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "comment": "Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09548v1"}
{"id": "2506.09943", "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09943v1"}
{"id": "2506.09562", "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning", "authors": ["Songze Li", "Mingxuan Zhang", "Oubo Ma", "Kang Wei", "Shouling Ji"], "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09562v1"}
{"id": "2506.09952", "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "comment": "Accepted to CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09952v1"}
{"id": "2506.09566", "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "comment": "To-appear as a book chapter", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09566v1"}
{"id": "2506.09953", "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09953v1"}
{"id": "2506.09640", "title": "Evasion Attacks Against Bayesian Predictive Models", "authors": ["Pablo G. Arce", "Roi Naveiro", "David Ríos Insua"], "summary": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups.", "comment": "Accepted as an oral presentation at UAI'25", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09640v1"}
{"id": "2506.09954", "title": "Vision Generalist Model: A Survey", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "comment": "Accepted by International Journal of Computer Vision (IJCV)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09954v1"}
{"id": "2506.09645", "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "comment": "32 pages, 28 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09645v1"}
{"id": "2506.09956", "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "João Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09956v1"}
{"id": "2506.09647", "title": "Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach", "authors": ["Lei Deng", "Wenhan Xu", "Jingwei Li", "Danny H. K. Tsang"], "summary": "Real-time network traffic forecasting is crucial for network management and\nearly resource allocation. Existing network traffic forecasting approaches\noperate under the assumption that the network traffic data is fully observed.\nHowever, in practical scenarios, the collected data are often incomplete due to\nvarious human and natural factors. In this paper, we propose a generative model\napproach for real-time network traffic forecasting with missing data. Firstly,\nwe model the network traffic forecasting task as a tensor completion problem.\nSecondly, we incorporate a pre-trained generative model to achieve the low-rank\nstructure commonly associated with tensor completion. The generative model\neffectively captures the intrinsic low-rank structure of network traffic data\nduring pre-training and enables the mapping from a compact latent\nrepresentation to the tensor space. Thirdly, rather than directly optimizing\nthe high-dimensional tensor, we optimize its latent representation, which\nsimplifies the optimization process and enables real-time forecasting. We also\nestablish a theoretical recovery guarantee that quantifies the error bound of\nthe proposed approach. Experiments on real-world datasets demonstrate that our\napproach achieves accurate network traffic forecasting within 100 ms, with a\nmean absolute error (MAE) below 0.002, as validated on the Abilene dataset.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09647v1"}
{"id": "2506.09965", "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09965v1"}
{"id": "2506.09648", "title": "Scaling Laws for Uncertainty in Deep Learning", "authors": ["Mattia Rosso", "Simone Rossi", "Giulio Franzese", "Markus Heinonen", "Maurizio Filippone"], "summary": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09648v1"}
{"id": "2506.09984", "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09984v1"}
{"id": "2506.09650", "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09650v1"}
{"id": "2506.09988", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09988v1"}
{"id": "2506.09655", "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09655v1"}
{"id": "2506.09993", "title": "Text-Aware Image Restoration with Diffusion Models", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09993v1"}
{"id": "2506.09659", "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09659v1"}
{"id": "2506.09994", "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09994v1"}
{"id": "2506.09668", "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "comment": "Work currently under revision for IEEE TMI", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09668v1"}
{"id": "2506.09997", "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09997v1"}
{"id": "2506.09681", "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09681v1"}
{"id": "2506.09691", "title": "Adding simple structure at inference improves Vision-Language Compositionality", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09691v1"}
{"id": "2506.09709", "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "comment": "Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09709v1"}
{"id": "2506.09730", "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization", "authors": ["Pierre Vernimmen", "François Glineur"], "summary": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09730v1"}
{"id": "2506.09764", "title": "Alice and the Caterpillar: A more descriptive null model for assessing data mining results", "authors": ["Giulia Preti", "Gianmarco De Francisci Morales", "Matteo Riondato"], "summary": "We introduce novel null models for assessing the results obtained from\nobserved binary transactional and sequence datasets, using statistical\nhypothesis testing. Our null models maintain more properties of the observed\ndataset than existing ones. Specifically, they preserve the Bipartite Joint\nDegree Matrix of the bipartite (multi-)graph corresponding to the dataset,\nwhich ensures that the number of caterpillars, i.e., paths of length three, is\npreserved, in addition to other properties considered by other models. We\ndescribe Alice, a suite of Markov chain Monte Carlo algorithms for sampling\ndatasets from our null models, based on a carefully defined set of states and\nefficient operations to move between them. The results of our experimental\nevaluation show that Alice mixes fast and scales well, and that our null model\nfinds different significant results than ones previously considered in the\nliterature.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.09764v1"}
{"id": "2506.09765", "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09765v1"}
{"id": "2506.09773", "title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "authors": ["Taulant Koka", "Manolis C. Tsakiris", "Benjamín Béjar Haro", "Michael Muma"], "summary": "Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.", "comment": "Accepted to ICASSP 2025. \\copyright 2025 IEEE. Personal use of this\n  material is permitted", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09773v1"}
{"id": "2506.09792", "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "comment": "Accepted by Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09792v1"}
{"id": "2506.09804", "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09804v1"}
{"id": "2506.09805", "title": "Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy", "authors": ["Tonghe Wang", "Yining Feng", "Xiaofeng Yang"], "summary": "Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the\npattern of needle placement solely relies on physician experience. We\ninvestigated the feasibility of using reinforcement learning (RL) to provide\nneedle positions and dwell times based on patient anatomy during pre-planning\nstage. This approach would reduce procedure time and ensure consistent plan\nquality. Materials and Methods: We train a RL agent to adjust the position of\none selected needle and all the dwell times on it to maximize a pre-defined\nreward function after observing the environment. After adjusting, the RL agent\nthen moves on to the next needle, until all needles are adjusted. Multiple\nrounds are played by the agent until the maximum number of rounds is reached.\nPlan data from 11 prostate HDR boost patients (1 for training, and 10 for\ntesting) treated in our clinic were included in this study. The dosimetric\nmetrics and the number of used needles of RL plan were compared to those of the\nclinical results (ground truth). Results: On average, RL plans and clinical\nplans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no\nstatistical significance), while RL plans have less prostate hotspot (Prostate\nV150) and Urethra D20% plans with statistical significance. Moreover, RL plans\nuse 2 less needles than clinical plan on average. Conclusion: We present the\nfirst study demonstrating the feasibility of using reinforcement learning to\nautonomously generate clinically practical HDR prostate brachytherapy plans.\nThis RL-based method achieved equal or improved plan quality compared to\nconventional clinical approaches while requiring fewer needles. With minimal\ndata requirements and strong generalizability, this approach has substantial\npotential to standardize brachytherapy planning, reduce clinical variability,\nand enhance patient outcomes.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.09805v1"}
{"id": "2506.09820", "title": "CoRT: Code-integrated Reasoning within Thinking", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "comment": "work in progress", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09820v1"}
{"id": "2506.09832", "title": "A Deep Generative Model for the Simulation of Discrete Karst Networks", "authors": ["Dany Lauzon", "Julien Straubhaar", "Philippe Renard"], "summary": "The simulation of discrete karst networks presents a significant challenge\ndue to the complexity of the physicochemical processes occurring within various\ngeological and hydrogeological contexts over extended periods. This complex\ninterplay leads to a wide variety of karst network patterns, each intricately\nlinked to specific hydrogeological conditions. We explore a novel approach that\nrepresents karst networks as graphs and applies graph generative models (deep\nlearning techniques) to capture the intricate nature of karst environments. In\nthis representation, nodes retain spatial information and properties, while\nedges signify connections between nodes. Our generative process consists of two\nmain steps. First, we utilize graph recurrent neural networks (GraphRNN) to\nlearn the topological distribution of karst networks. GraphRNN decomposes the\ngraph simulation into a sequential generation of nodes and edges, informed by\npreviously generated structures. Second, we employ denoising diffusion\nprobabilistic models on graphs (G-DDPM) to learn node features (spatial\ncoordinates and other properties). G-DDPMs enable the generation of nodes\nfeatures on the graphs produced by the GraphRNN that adhere to the learned\nstatistical properties by sampling from the derived probability distribution,\nensuring that the generated graphs are realistic and capture the essential\nfeatures of the original data. We test our approach using real-world karst\nnetworks and compare generated subgraphs with actual subgraphs from the\ndatabase, by using geometry and topology metrics. Our methodology allows\nstochastic simulation of discrete karst networks across various types of\nformations, a useful tool for studying the behavior of physical processes such\nas flow and transport.", "comment": "26 pages, 15 figures, submitted to Earth and Space Science", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09832v1"}
{"id": "2506.09851", "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "comment": "Accepted in MECON 2025", "cate": "q-fin.ST", "url": "http://arxiv.org/abs/2506.09851v1"}
{"id": "2506.09874", "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09874v1"}
{"id": "2506.09902", "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "comment": "Accepted to ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09902v1"}
{"id": "2506.09958", "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09958v1"}
{"id": "2506.09985", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09985v1"}
{"id": "2506.09987", "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09987v1"}
{"id": "2506.09988", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09988v1"}
{"id": "2506.09990", "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09990v1"}
{"id": "2506.09993", "title": "Text-Aware Image Restoration with Diffusion Models", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09993v1"}
{"id": "2506.09997", "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09997v1"}
