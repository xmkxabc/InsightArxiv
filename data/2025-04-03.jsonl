{"id": "2504.01100", "pdf": "https://arxiv.org/pdf/2504.01100", "abs": "https://arxiv.org/abs/2504.01100", "authors": ["Matéo Mahaut", "Francesca Franzon"], "title": "Repetitions are not all alike: distinct mechanisms sustain repetition in language models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text generated by language models (LMs) can degrade into repetitive cycles,\nwhere identical word sequences are persistently repeated one after another.\nPrior research has typically treated repetition as a unitary phenomenon.\nHowever, repetitive sequences emerge under diverse tasks and contexts, raising\nthe possibility that it may be driven by multiple underlying factors. Here, we\nexperimentally explore the hypothesis that repetition in LMs can result from\ndistinct mechanisms, reflecting different text generation strategies used by\nthe model. We examine the internal working of LMs under two conditions that\nprompt repetition: one in which repeated sequences emerge naturally after\nhuman-written text, and another where repetition is explicitly induced through\nan in-context learning (ICL) setup. Our analysis reveals key differences\nbetween the two conditions: the model exhibits varying levels of confidence,\nrelies on different attention heads, and shows distinct pattens of change in\nresponse to controlled perturbations. These findings suggest that distinct\ninternal mechanisms can interact to drive repetition, with implications for its\ninterpretation and mitigation strategies. More broadly, our results highlight\nthat the same surface behavior in LMs may be sustained by different underlying\nprocesses, acting independently or in combination."}
{"id": "2504.01127", "pdf": "https://arxiv.org/pdf/2504.01127", "abs": "https://arxiv.org/abs/2504.01127", "authors": ["Ziyi Liu", "Priyanka Dey", "Zhenyu Zhao", "Jen-tse Huang", "Rahul Gupta", "Yang Liu", "Jieyu Zhao"], "title": "Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench", "categories": ["cs.CL"], "comment": null, "summary": "Cultural Intelligence (CQ) refers to the ability to understand unfamiliar\ncultural contexts-a crucial skill for large language models (LLMs) to\neffectively engage with globally diverse users. While existing research often\nfocuses on explicitly stated cultural norms, such approaches fail to capture\nthe subtle, implicit values that underlie real-world conversations. To address\nthis gap, we introduce CQ-Bench, a benchmark specifically designed to assess\nLLMs' capability to infer implicit cultural values from natural conversational\ncontexts. We generate a multi-character conversation-based stories dataset\nusing values from the World Value Survey and GlobalOpinions datasets, with\ntopics including ethical, religious, social, and political. Our dataset\nconstruction pipeline includes rigorous validation procedures-incorporation,\nconsistency, and implicitness checks-using GPT-4o, with 98.2% human-model\nagreement in the final validation. Our benchmark consists of three tasks of\nincreasing complexity: attitude detection, value selection, and value\nextraction. We find that while o1 and Deepseek-R1 models reach human-level\nperformance in value selection (0.809 and 0.814), they still fall short in\nnuanced attitude detection, with F1 scores of 0.622 and 0.635, respectively. In\nthe value extraction task, GPT-4o-mini and o3-mini score 0.602 and 0.598,\nhighlighting the difficulty of open-ended cultural reasoning. Notably,\nfine-tuning smaller models (e.g., LLaMA-3.2-3B) on only 500 culturally rich\nexamples improves performance by over 10%, even outperforming stronger\nbaselines (o3-mini) in some cases. Using CQ-Bench, we provide insights into the\ncurrent challenges in LLMs' CQ research and suggest practical pathways for\nenhancing LLMs' cross-cultural reasoning abilities."}
{"id": "2504.01132", "pdf": "https://arxiv.org/pdf/2504.01132", "abs": "https://arxiv.org/abs/2504.01132", "authors": ["Melanie Subbiah", "Akankshya Mishra", "Grace Kim", "Liyan Tang", "Greg Durrett", "Kathleen McKeown"], "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced."}
{"id": "2504.01137", "pdf": "https://arxiv.org/pdf/2504.01137", "abs": "https://arxiv.org/abs/2504.01137", "authors": ["Guy Kaplan", "Michael Toker", "Yuval Reif", "Yonatan Belinkov", "Roy Schwartz"], "title": "Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models", "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Image (T2I) models often suffer from issues such as semantic leakage,\nincorrect feature binding, and omissions of key concepts in the generated\nimage. This work studies these phenomena by looking into the role of\ninformation flow between textual token representations. To this end, we\ngenerate images by applying the diffusion component on a subset of contextual\ntoken representations in a given prompt and observe several interesting\nphenomena. First, in many cases, a word or multiword expression is fully\nrepresented by one or two tokens, while other tokens are redundant. For\nexample, in \"San Francisco's Golden Gate Bridge\", the token \"gate\" alone\ncaptures the full expression. We demonstrate the redundancy of these tokens by\nremoving them after textual encoding and generating an image from the resulting\nrepresentation. Surprisingly, we find that this process not only maintains\nimage generation performance but also reduces errors by 21\\% compared to\nstandard generation. We then show that information can also flow between\ndifferent expressions in a sentence, which often leads to semantic leakage.\nBased on this observation, we propose a simple, training-free method to\nmitigate semantic leakage: replacing the leaked item's representation after the\ntextual encoding with its uncontextualized representation. Remarkably, this\nsimple approach reduces semantic leakage by 85\\%. Overall, our work provides a\ncomprehensive analysis of information flow across textual tokens in T2I models,\noffering both novel insights and practical benefits."}
{"id": "2504.01023", "pdf": "https://arxiv.org/pdf/2504.01023", "abs": "https://arxiv.org/abs/2504.01023", "authors": ["Chaofan Wu", "Jiaheng Li", "Jinghao Cao", "Ming Li", "Yongkang Feng", "Jiayu Wu Shuwen Xu", "Zihang Gao", "Sidan Du", "Yang Li"], "title": "Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical Voxel for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate 3D perception is essential for autonomous driving. Traditional\nmethods often struggle with geometric ambiguity due to a lack of geometric\nprior. To address these challenges, we use omnidirectional depth estimation to\nintroduce geometric prior. Based on the depth information, we propose a\nSketch-Coloring framework OmniDepth-Occ. Additionally, our approach introduces\na cylindrical voxel representation based on polar coordinate to better align\nwith the radial nature of panoramic camera views. To address the lack of\nfisheye camera dataset in autonomous driving tasks, we also build a virtual\nscene dataset with six fisheye cameras, and the data volume has reached twice\nthat of SemanticKITTI. Experimental results demonstrate that our\nSketch-Coloring network significantly enhances 3D perception performance."}
{"id": "2504.01196", "pdf": "https://arxiv.org/pdf/2504.01196", "abs": "https://arxiv.org/abs/2504.01196", "authors": ["Zian Su", "Ziyang Huang", "Kaiyuan Zhang", "Xiangyu Zhang"], "title": "$μ$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Large language models (LLMs) have emerged as powerful knowledge bases yet are\nlimited by static training data, leading to issues such as hallucinations and\nsafety risks. Editing a model's internal knowledge through the locate-and-edit\nparadigm has proven a cost-effective alternative to retraining, though current\nunstructured approaches, especially window-based autoregressive methods, often\ndisrupt the causal dependency between early memory updates and later output\ntokens. In this work, we first theoretically analyze these limitations and then\nintroduce Matryoshka Unstructured Knowledge Editing ($\\mu$KE), a novel memory\nupdate mechanism that preserves such dependencies via a Matryoshka-style\nobjective and adaptive loss coefficients. Empirical evaluations on two models\nacross four benchmarks demonstrate that $\\mu$KE improves edit efficacy by up to\n12.33% over state-of-the-art methods, and remain robust when applied to diverse\nformatted edits, underscoring its potential for effective unstructured\nknowledge editing in LLMs."}
{"id": "2504.01024", "pdf": "https://arxiv.org/pdf/2504.01024", "abs": "https://arxiv.org/abs/2504.01024", "authors": ["Yufei He", "Xucong Zhang", "Arno H. A. Stienen"], "title": "Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Human intention detection with hand motion prediction is critical to drive\nthe upper-extremity assistive robots in neurorehabilitation applications.\nHowever, the traditional methods relying on physiological signal measurement\nare restrictive and often lack environmental context. We propose a novel\napproach that predicts future sequences of both hand poses and joint positions.\nThis method integrates gaze information, historical hand motion sequences, and\nenvironmental object data, adapting dynamically to the assistive needs of the\npatient without prior knowledge of the intended object for grasping.\nSpecifically, we use a vector-quantized variational autoencoder for robust hand\npose encoding with an autoregressive generative transformer for effective hand\nmotion sequence prediction. We demonstrate the usability of these novel\ntechniques in a pilot study with healthy subjects. To train and evaluate the\nproposed method, we collect a dataset consisting of various types of grasp\nactions on different objects from multiple subjects. Through extensive\nexperiments, we demonstrate that the proposed method can successfully predict\nsequential hand movement. Especially, the gaze information shows significant\nenhancements in prediction capabilities, particularly with fewer input frames,\nhighlighting the potential of the proposed method for real-world applications."}
{"id": "2504.01201", "pdf": "https://arxiv.org/pdf/2504.01201", "abs": "https://arxiv.org/abs/2504.01201", "authors": ["Krithik Vishwanath", "Anton Alyakin", "Daniel Alexander Alber", "Jin Vivian Lee", "Douglas Kondziolka", "Eric Karl Oermann"], "title": "Medical large language models are easily distracted", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "20 pages, 2 main figures, 6 extended figures", "summary": "Large language models (LLMs) have the potential to transform medicine, but\nreal-world clinical scenarios contain extraneous information that can hinder\nperformance. The rise of assistive technologies like ambient dictation, which\nautomatically generates draft notes from live patient encounters, has the\npotential to introduce additional noise making it crucial to assess the ability\nof LLM's to filter relevant data. To investigate this, we developed\nMedDistractQA, a benchmark using USMLE-style questions embedded with simulated\nreal-world distractions. Our findings show that distracting statements\n(polysemous words with clinical meanings used in a non-clinical context or\nreferences to unrelated health conditions) can reduce LLM accuracy by up to\n17.9%. Commonly proposed solutions to improve model performance such as\nretrieval-augmented generation (RAG) and medical fine-tuning did not change\nthis effect and in some cases introduced their own confounders and further\ndegraded performance. Our findings suggest that LLMs natively lack the logical\nmechanisms necessary to distinguish relevant from irrelevant clinical\ninformation, posing challenges for real-world applications. MedDistractQA and\nour results highlights the need for robust mitigation strategies to enhance LLM\nresilience to extraneous information."}
{"id": "2504.01028", "pdf": "https://arxiv.org/pdf/2504.01028", "abs": "https://arxiv.org/abs/2504.01028", "authors": ["Anket Mehra", "Malte Prieß", "Marian Himstedt"], "title": "Improving Applicability of Deep Learning based Token Classification models during Training", "categories": ["cs.CV", "cs.CL", "cs.IR"], "comment": null, "summary": "This paper shows that further evaluation metrics during model training are\nneeded to decide about its applicability in inference. As an example, a\nLayoutLM-based model is trained for token classification in documents. The\ndocuments are German receipts. We show that conventional classification\nmetrics, represented by the F1-Score in our experiments, are insufficient for\nevaluating the applicability of machine learning models in practice. To address\nthis problem, we introduce a novel metric, Document Integrity Precision (DIP),\nas a solution for visual document understanding and the token classification\ntask. To the best of our knowledge, nothing comparable has been introduced in\nthis context. DIP is a rigorous metric, describing how many documents of the\ntest dataset require manual interventions. It enables AI researchers and\nsoftware developers to conduct an in-depth investigation of the level of\nprocess automation in business software. In order to validate DIP, we conduct\nexperiments with our created models to highlight and analyze the impact and\nrelevance of DIP to evaluate if the model should be deployed or not in\ndifferent training settings. Our results demonstrate that existing metrics\nbarely change for isolated model impairments, whereas DIP indicates that the\nmodel requires substantial human interventions in deployment. The larger the\nset of entities being predicted, the less sensitive conventional metrics are,\nentailing poor automation quality. DIP, in contrast, remains a single value to\nbe interpreted for entire entity sets. This highlights the importance of having\nmetrics that focus on the business task for model training in production. Since\nDIP is created for the token classification task, more research is needed to\nfind suitable metrics for other training tasks."}
{"id": "2504.01216", "pdf": "https://arxiv.org/pdf/2504.01216", "abs": "https://arxiv.org/abs/2504.01216", "authors": ["Feng Chen", "Dror Ben-Zeev", "Gillian Sparks", "Arya Kadakia", "Trevor Cohen"], "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 4 tables, 1 figure", "summary": "Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical\nsettings, presenting opportunities for automated detection to identify\npatients. This study evaluates natural language processing approaches for\ndetecting PTSD from clinical interview transcripts. We compared general and\nmental health-specific transformer models (BERT/RoBERTa), embedding-based\nmethods (SentenceBERT/LLaMA), and large language model prompting strategies\n(zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset.\nDomain-specific models significantly outperformed general models\n(Mental-RoBERTa F1=0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural\nnetworks achieved the highest performance (F1=0.700). Zero-shot prompting using\nDSM-5 criteria yielded competitive results without training data (F1=0.657).\nPerformance varied significantly across symptom severity and comorbidity\nstatus, with higher accuracy for severe PTSD cases and patients with comorbid\ndepression. Our findings highlight the potential of domain-adapted embeddings\nand LLMs for scalable screening while underscoring the need for improved\ndetection of nuanced presentations and offering insights for developing\nclinically viable AI tools for PTSD assessment."}
{"id": "2504.01040", "pdf": "https://arxiv.org/pdf/2504.01040", "abs": "https://arxiv.org/abs/2504.01040", "authors": ["Ilir Tahiraj", "Jeremialie Swadiryus", "Felix Fent", "Markus Lienkamp"], "title": "Cal or No Cal? -- Real-Time Miscalibration Detection of LiDAR and Camera Sensors", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The goal of extrinsic calibration is the alignment of sensor data to ensure\nan accurate representation of the surroundings and enable sensor fusion\napplications. From a safety perspective, sensor calibration is a key enabler of\nautonomous driving. In the current state of the art, a trend from target-based\noffline calibration towards targetless online calibration can be observed.\nHowever, online calibration is subject to strict real-time and resource\nconstraints which are not met by state-of-the-art methods. This is mainly due\nto the high number of parameters to estimate, the reliance on geometric\nfeatures, or the dependence on specific vehicle maneuvers. To meet these\nrequirements and ensure the vehicle's safety at any time, we propose a\nmiscalibration detection framework that shifts the focus from the direct\nregression of calibration parameters to a binary classification of the\ncalibration state, i.e., calibrated or miscalibrated. Therefore, we propose a\ncontrastive learning approach that compares embedded features in a latent space\nto classify the calibration state of two different sensor modalities. Moreover,\nwe provide a comprehensive analysis of the feature embeddings and challenging\ncalibration errors that highlight the performance of our approach. As a result,\nour method outperforms the current state-of-the-art in terms of detection\nperformance, inference time, and resource demand. The code is open source and\navailable on https://github.com/TUMFTM/MiscalibrationDetection."}
{"id": "2504.01225", "pdf": "https://arxiv.org/pdf/2504.01225", "abs": "https://arxiv.org/abs/2504.01225", "authors": ["Gonçalo Gomes", "Chrysoula Zerva", "Bruno Martins"], "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessment for individual\nword misalignments within captions, and the reliance on single-point quality\nestimates without considering uncertainty. To address these limitations, we\npropose a simple yet effective strategy for generating and calibrating\nCLIPScore distributions. Leveraging a model-agnostic conformal risk control\nframework, we calibrate CLIPScore values for task-specific control variables,\nto tackle the aforementioned two limitations. Experimental results demonstrate\nthat using conformal risk control, over the distributions produced with simple\nmethods such as input masking, can achieve competitive performance compared to\nmore complex approaches. Our method effectively detects misaligned words, while\nproviding formal guarantees aligned with desired risk levels, and improving the\ncorrelation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics."}
{"id": "2504.01044", "pdf": "https://arxiv.org/pdf/2504.01044", "abs": "https://arxiv.org/abs/2504.01044", "authors": ["Lan Wei", "Gema Vera Gonzalez", "Phatsimo Kgwarae", "Alexander Timms", "Denis Zahorovsky", "Simon Schultz", "Dandan Zhang"], "title": "Coarse-to-Fine Learning for Multi-Pipette Localisation in Robot-Assisted In Vivo Patch-Clamp", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "In vivo image-guided multi-pipette patch-clamp is essential for studying\ncellular interactions and network dynamics in neuroscience. However, current\nprocedures mainly rely on manual expertise, which limits accessibility and\nscalability. Robotic automation presents a promising solution, but achieving\nprecise real-time detection of multiple pipettes remains a challenge. Existing\nmethods focus on ex vivo experiments or single pipette use, making them\ninadequate for in vivo multi-pipette scenarios. To address these challenges, we\npropose a heatmap-augmented coarse-to-fine learning technique to facilitate\nmulti-pipette real-time localisation for robot-assisted in vivo patch-clamp.\nMore specifically, we introduce a Generative Adversarial Network (GAN)-based\nmodule to remove background noise and enhance pipette visibility. We then\nintroduce a two-stage Transformer model that starts with predicting the coarse\nheatmap of the pipette tips, followed by the fine-grained coordination\nregression module for precise tip localisation. To ensure robust training, we\nuse the Hungarian algorithm for optimal matching between the predicted and\nactual locations of tips. Experimental results demonstrate that our method\nachieved > 98% accuracy within 10 {\\mu}m, and > 89% accuracy within 5 {\\mu}m\nfor the localisation of multi-pipette tips. The average MSE is 2.52 {\\mu}m."}
{"id": "2504.01241", "pdf": "https://arxiv.org/pdf/2504.01241", "abs": "https://arxiv.org/abs/2504.01241", "authors": ["Naimul Haque"], "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced Natural Language\nProcessing (NLP), particularly in Natural Language Understanding (NLU) tasks.\nAs we progress toward an agentic world where LLM-based agents autonomously\nhandle specialized tasks, it becomes crucial for these models to adapt to new\ntasks without forgetting previously learned information - a challenge known as\ncatastrophic forgetting. This study evaluates the continual fine-tuning of\nvarious open-source LLMs with different parameter sizes (specifically models\nunder 10 billion parameters) on key NLU tasks from the GLUE benchmark,\nincluding SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and\ntask-specific adjustments, we assess and compare the models' abilities to\nretain prior knowledge while learning new tasks. Our results indicate that\nmodels such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong\nlearning capabilities, making them well-suited for continual learning\nenvironments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate\nimpressive learning abilities and overall performance after fine-tuning. This\nwork contributes to understanding catastrophic forgetting in LLMs and\nhighlights prompting engineering to optimize model performance for continual\nlearning scenarios."}
{"id": "2504.01047", "pdf": "https://arxiv.org/pdf/2504.01047", "abs": "https://arxiv.org/abs/2504.01047", "authors": ["Asraa Muayed Abdalah", "Noor Redha Alkazaz"], "title": "Predicting Movie Production Years through Facial Recognition of Actors with Machine Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study used machine learning algorithms to identify actors and extract\nthe age of actors from images taken randomly from movies. The use of images\ntaken from Arab movies includes challenges such as non-uniform lighting,\ndifferent and multiple poses for the actors and multiple elements with the\nactor or a group of actors. Additionally, the use of make-up, wigs, beards, and\nwearing different accessories and costumes made it difficult for the system to\nidentify the personality of the same actor. The Arab Actors Dataset-AAD\ncomprises 574 images sourced from various movies, encompassing both black and\nwhite as well as color compositions. The images depict complete scenes or\nfragments thereof. Multiple models were employed for feature extraction, and\ndiverse machine learning algorithms were utilized during the classification and\nprediction stages to determine the most effective algorithm for handling such\nimage types. The study demonstrated the effectiveness of the Logistic\nRegression model exhibited the best performance compared to other models in the\ntraining phase, as evidenced by its AUC, precision, CA and F1score values of\n99%, 86%, 85.5% and 84.2% respectively. The findings of this study can be used\nto improve the precision and reliability of facial recognition technology for\nvarious uses as with movies search services, movie suggestion algorithms, and\ngenre classification of movies."}
{"id": "2504.01248", "pdf": "https://arxiv.org/pdf/2504.01248", "abs": "https://arxiv.org/abs/2504.01248", "authors": ["Rafael Giebisch", "Ken E. Friedl", "Lev Sorokin", "Andrea Stocco"], "title": "Automated Factual Benchmarking for In-Car Conversational Systems using Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted in IEEE Intelligent Vehicles Symposium Conference (IV 2025)", "summary": "In-car conversational systems bring the promise to improve the in-vehicle\nuser experience. Modern conversational systems are based on Large Language\nModels (LLMs), which makes them prone to errors such as hallucinations, i.e.,\ninaccurate, fictitious, and therefore factually incorrect information. In this\npaper, we present an LLM-based methodology for the automatic factual\nbenchmarking of in-car conversational systems. We instantiate our methodology\nwith five LLM-based methods, leveraging ensembling techniques and diverse\npersonae to enhance agreement and minimize hallucinations. We use our\nmethodology to evaluate CarExpert, an in-car retrieval-augmented conversational\nquestion answering system, with respect to the factual correctness to a\nvehicle's manual. We produced a novel dataset specifically created for the\nin-car domain, and tested our methodology against an expert evaluation. Our\nresults show that the combination of GPT-4 with the Input Output Prompting\nachieves over 90 per cent factual correctness agreement rate with expert\nevaluations, other than being the most efficient approach yielding an average\nresponse time of 4.5s. Our findings suggest that LLM-based testing constitutes\na viable approach for the validation of conversational systems regarding their\nfactual correctness."}
{"id": "2504.01048", "pdf": "https://arxiv.org/pdf/2504.01048", "abs": "https://arxiv.org/abs/2504.01048", "authors": ["Chunxue Xu", "Yiwei Wang", "Bryan Hooi", "Yujun Cai", "Songze Li"], "title": "How does Watermarking Affect Visual Language Models in Document Understanding?", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Visual Language Models (VLMs) have become foundational models for document\nunderstanding tasks, widely used in the processing of complex multimodal\ndocuments across domains such as finance, law, and academia. However, documents\noften contain noise-like information, such as watermarks, which inevitably\nleads us to inquire: \\emph{Do watermarks degrade the performance of VLMs in\ndocument understanding?} To address this, we propose a novel evaluation\nframework to investigate the effect of visible watermarks on VLMs performance.\nWe takes into account various factors, including different types of document\ndata, the positions of watermarks within documents and variations in watermark\ncontent. Our experimental results reveal that VLMs performance can be\nsignificantly compromised by watermarks, with performance drop rates reaching\nup to 36\\%. We discover that \\emph{scattered} watermarks cause stronger\ninterference than centralized ones, and that \\emph{semantic contents} in\nwatermarks creates greater disruption than simple visual occlusion. Through\nattention mechanism analysis and embedding similarity examination, we find that\nthe performance drops are mainly attributed to that watermarks 1) force\nwidespread attention redistribution, and 2) alter semantic representation in\nthe embedding space. Our research not only highlights significant challenges in\ndeploying VLMs for document understanding, but also provides insights towards\ndeveloping robust inference mechanisms on watermarked documents."}
{"id": "2504.01253", "pdf": "https://arxiv.org/pdf/2504.01253", "abs": "https://arxiv.org/abs/2504.01253", "authors": ["Niharika Dadu", "Harsh Vardhan Singh", "Romi Banerjee"], "title": "Grade Guard: A Smart System for Short Answer Automated Grading", "categories": ["cs.CL", "I.2.7"], "comment": "11 pages, 18 figures", "summary": "The advent of large language models (LLMs) in the education sector has\nprovided impetus to automate grading short answer questions. LLMs make\nevaluating short answers very efficient, thus addressing issues like staff\nshortage. However, in the task of Automated Short Answer Grading (ASAG), LLM\nresponses are influenced by diverse perspectives in their training dataset,\nleading to inaccuracies in evaluating nuanced or partially correct answers. To\naddress this challenge, we propose a novel framework, Grade Guard.\n  1. To enhance the task-based specialization of the LLMs, the temperature\nparameter has been fine-tuned using Root Mean Square Error (RMSE).\n  2. Unlike traditional approaches, LLMs in Grade Guard compute an\nIndecisiveness Score (IS) along with the grade to reflect uncertainty in\npredicted grades.\n  3. Introduced Confidence-Aware Loss (CAL) to generate an optimized\nIndecisiveness Score (IS).\n  4. To improve reliability, self-reflection based on the optimized IS has been\nintroduced into the framework, enabling human re-evaluation to minimize\nincorrect grade assignments.\n  Our experimentation shows that the best setting of Grade Guard outperforms\ntraditional methods by 19.16% RMSE in Upstage Solar Pro, 23.64% RMSE in Upstage\nSolar Mini, 4.00% RMSE in Gemini 1.5 Flash, and 10.20% RMSE in GPT 4-o Mini.\nFuture work includes improving interpretability by generating rationales for\ngrades to enhance accuracy. Expanding benchmark datasets and annotating them\nwith domain-specific nuances will enhance grading accuracy. Finally, analyzing\nfeedback to enhance confidence in predicted grades, reduce biases, optimize\ngrading criteria, and personalize learning while supporting multilingual\ngrading systems will make the solution more accurate, adaptable, fair, and\ninclusive."}
{"id": "2504.01049", "pdf": "https://arxiv.org/pdf/2504.01049", "abs": "https://arxiv.org/abs/2504.01049", "authors": ["Bingxin Li"], "title": "SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal models integrating speech and vision hold significant potential\nfor advancing human-computer interaction, particularly in Speech-Based Visual\nQuestion Answering (SBVQA) where spoken questions about images require direct\naudio-visual understanding. Existing approaches predominantly focus on\ntext-visual integration, leaving speech-visual modality gaps underexplored due\nto their inherent heterogeneity. To this end, we introduce SViQA, a unified\nspeech-vision model that directly processes spoken questions without text\ntranscription. Building upon the LLaVA architecture, our framework bridges\nauditory and visual modalities through two key innovations: (1) end-to-end\nspeech feature extraction eliminating intermediate text conversion, and (2)\ncross-modal alignment optimization enabling effective fusion of speech signals\nwith visual content. Extensive experimental results on the SBVQA benchmark\ndemonstrate the proposed SViQA's state-of-the-art performance, achieving 75.62%\naccuracy, and competitive multimodal generalization. Leveraging speech-text\nmixed input boosts performance to 78.85%, a 3.23% improvement over pure speech\ninput, highlighting SViQA's enhanced robustness and effective cross-modal\nattention alignment."}
{"id": "2504.01282", "pdf": "https://arxiv.org/pdf/2504.01282", "abs": "https://arxiv.org/abs/2504.01282", "authors": ["Jihyun Janice Ahn", "Wenpeng Yin"], "title": "Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing", "categories": ["cs.CL"], "comment": "9 pages", "summary": "While the inconsistency of LLMs is not a novel topic, prior research has\npredominantly addressed two types of generative inconsistencies: i) Randomness\nInconsistency: running the same LLM multiple trials, yielding varying\nresponses; ii) Paraphrase Inconsistency: paraphrased prompts result in\ndifferent responses from the same LLM. Randomness Inconsistency arises from the\ninherent randomness due to stochastic sampling in generative models, while\nParaphrase Inconsistency is a consequence of the language modeling objectives,\nwhere paraphrased prompts alter the distribution of vocabulary logits. This\nresearch discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM\nself-inconsistency: given a question and a couple of LLM-generated answer\ncandidates, the LLM often has conflicting responses when prompted \"Which are\ncorrect answers?\" and \"Which are incorrect answers?\". PRIN poses a big concern\nas it undermines the credibility of LLM-as-a-judge, and suggests a challenge\nfor LLMs to adhere to basic logical rules. We conduct a series of experiments\nto investigate PRIN, examining the extent of PRIN across different LLMs,\nmethods to mitigate it, potential applications, and its relationship with\nRandomness Inconsistency and Paraphrase Inconsistency. As the first study to\nexplore PRIN, our findings offer valuable insights into the inner workings of\nLLMs and contribute to advancing trustworthy AI."}
{"id": "2504.01053", "pdf": "https://arxiv.org/pdf/2504.01053", "abs": "https://arxiv.org/abs/2504.01053", "authors": ["Chongyang Li", "Yanmei He", "Tianqian Zhang", "Mingjian He", "Shouyin Liu"], "title": "Knowledge-Base based Semantic Image Transmission Using CLIP", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper proposes a novel knowledge-Base (KB) assisted semantic\ncommunication framework for image transmission. At the receiver, a Facebook AI\nSimilarity Search (FAISS) based vector database is constructed by extracting\nsemantic embeddings from images using the Contrastive Language-Image\nPre-Training (CLIP) model. During transmission, the transmitter first extracts\na 512-dimensional semantic feature using the CLIP model, then compresses it\nwith a lightweight neural network for transmission. After receiving the signal,\nthe receiver reconstructs the feature back to 512 dimensions and performs\nsimilarity matching from the KB to retrieve the most semantically similar\nimage. Semantic transmission success is determined by category consistency\nbetween the transmitted and retrieved images, rather than traditional metrics\nlike Peak Signal-to-Noise Ratio (PSNR). The proposed system prioritizes\nsemantic accuracy, offering a new evaluation paradigm for semantic-aware\ncommunication systems. Experimental validation on CIFAR100 demonstrates the\neffectiveness of the framework in achieving semantic image transmission."}
{"id": "2504.01296", "pdf": "https://arxiv.org/pdf/2504.01296", "abs": "https://arxiv.org/abs/2504.01296", "authors": ["Bairu Hou", "Yang Zhang", "Jiabao Ji", "Yujian Liu", "Kaizhi Qian", "Jacob Andreas", "Shiyu Chang"], "title": "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning", "categories": ["cs.CL"], "comment": "15 pages, 7 figures", "summary": "We present ThinkPrune, a simple yet effective method for pruning the thinking\nlength for long-thinking LLMs, which has been found to often produce\ninefficient and redundant thinking processes. Existing preliminary explorations\nof reducing thinking length primarily focus on forcing the thinking process to\nearly exit, rather than adapting the LLM to optimize and consolidate the\nthinking process, and therefore the length-performance tradeoff observed so far\nis sub-optimal. To fill this gap, ThinkPrune offers a simple solution that\ncontinuously trains the long-thinking LLMs via reinforcement learning (RL) with\nan added token limit, beyond which any unfinished thoughts and answers will be\ndiscarded, resulting in a zero reward. To further preserve model performance,\nwe introduce an iterative length pruning approach, where multiple rounds of RL\nare conducted, each with an increasingly more stringent token limit. We\nobserved that ThinkPrune results in a remarkable performance-length tradeoff --\non the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B\ncan be reduced by half with only 2% drop in performance. We also observed that\nafter pruning, the LLMs can bypass unnecessary steps while keeping the core\nreasoning process complete. Code is available at\nhttps://github.com/UCSB-NLP-Chang/ThinkPrune."}
{"id": "2504.01081", "pdf": "https://arxiv.org/pdf/2504.01081", "abs": "https://arxiv.org/abs/2504.01081", "authors": ["Wenjun Zeng", "Dana Kurniawan", "Ryan Mullins", "Yuchi Liu", "Tamoghna Saha", "Dirichi Ike-Njoku", "Jindong Gu", "Yiwen Song", "Cai Xu", "Jingjing Zhou", "Aparna Joshi", "Shravan Dheep", "Mani Malek", "Hamid Palangi", "Joon Baek", "Rick Pereira", "Karthik Narasimhan"], "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation", "categories": ["cs.CV", "cs.CL", "eess.IV"], "comment": null, "summary": "We introduce ShieldGemma 2, a 4B parameter image content moderation model\nbuilt on Gemma 3. This model provides robust safety risk predictions across the\nfollowing key harm categories: Sexually Explicit, Violence \\& Gore, and\nDangerous Content for synthetic images (e.g. output of any image generation\nmodel) and natural images (e.g. any image input to a Vision-Language Model). We\nevaluated on both internal and external benchmarks to demonstrate\nstate-of-the-art performance compared to LlavaGuard\n\\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base\nGemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we\npresent a novel adversarial data generation pipeline which enables a\ncontrolled, diverse, and robust image generation. ShieldGemma 2 provides an\nopen image moderation tool to advance multimodal safety and responsible AI\ndevelopment."}
{"id": "2504.01309", "pdf": "https://arxiv.org/pdf/2504.01309", "abs": "https://arxiv.org/abs/2504.01309", "authors": ["Lingxiao Guan", "Yuanhao Huang", "Jie Liu"], "title": "Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In Question Answering (QA), Retrieval Augmented Generation (RAG) has\nrevolutionized performance in various domains. However, how to effectively\ncapture multi-document relationships, particularly critical for biomedical\ntasks, remains an open question. In this work, we propose a novel method that\nutilizes propositional claims to construct a local knowledge graph from\nretrieved documents. Summaries are then derived via layerwise summarization\nfrom the knowledge graph to contextualize a small language model to perform QA.\nWe achieved comparable or superior performance with our method over RAG\nbaselines on several biomedical QA benchmarks. We also evaluated each\nindividual step of our methodology over a targeted set of metrics,\ndemonstrating its effectiveness."}
{"id": "2504.01128", "pdf": "https://arxiv.org/pdf/2504.01128", "abs": "https://arxiv.org/abs/2504.01128", "authors": ["Andrei Dumitriu", "Florin Tatui", "Florin Miron", "Aakash Ralhan", "Radu Tudor Ionescu", "Radu Timofte"], "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety", "categories": ["cs.CV", "cs.AI", "I.4.0, I.4.9"], "comment": null, "summary": "Rip currents are strong, localized and narrow currents of water that flow\noutwards into the sea, causing numerous beach-related injuries and fatalities\nworldwide. Accurate identification of rip currents remains challenging due to\ntheir amorphous nature and the lack of annotated data, which often requires\nexpert knowledge. To address these issues, we present RipVIS, a large-scale\nvideo instance segmentation benchmark explicitly designed for rip current\nsegmentation. RipVIS is an order of magnitude larger than previous datasets,\nfeaturing $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$\nframes) are with rip currents, collected from various sources, including\ndrones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse\nvisual contexts, such as wave-breaking patterns, sediment flows, and water\ncolor variations, across multiple global locations, including USA, Mexico,\nCosta Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New\nZealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic\nscenarios, supplemented by an additional $34$ videos ($48,800$ frames) without\nrip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade\nMask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip\ncurrent segmentation. Results are reported in terms of multiple metrics, with a\nparticular focus on the $F_2$ score to prioritize recall and reduce false\nnegatives. To enhance segmentation performance, we introduce a novel\npost-processing step based on Temporal Confidence Aggregation (TCA). RipVIS\naims to set a new standard for rip current segmentation, contributing towards\nsafer beach environments. We offer a benchmark website to share data, models,\nand results with the research community, encouraging ongoing collaboration and\nfuture contributions, at https://ripvis.ai."}
{"id": "2504.01317", "pdf": "https://arxiv.org/pdf/2504.01317", "abs": "https://arxiv.org/abs/2504.01317", "authors": ["Zhendong Tan", "Xingjun Zhang", "Chaoyi Hu", "Yancheng Pan", "Shaoxun Wang"], "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time\nscaling can significantly improve model performance, especially in complex\ntasks such as logical reasoning. Common test-time scaling methods involve\ngenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.\nHowever, while self-correction can improve performance, it may lead to\nsignificant token waste and reduce readability of the CoT if the reasoning\nsteps are already correct. To demonstrate that large language models (LLMs) can\nrectify errors at a more fine-grained level, we propose Adaptive Rectification\nSampling (AR-Sampling), which can guide the LLMs to self-correction at the\nappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)\nas a verifier and constructed trigger sentences to guide the model in adaptive\nstep-level rethinking. Through the experiments on GSM8K and MATH500, it\nindicate that our approach enables the models to rethink in more fine-grained\nlevel, improving the accuracy of solutions, while generating a reasonable\nnumber of additional tokens."}
{"id": "2504.01213", "pdf": "https://arxiv.org/pdf/2504.01213", "abs": "https://arxiv.org/abs/2504.01213", "authors": ["Banafsheh Adami", "Nima Karimian"], "title": "GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Although contactless fingerprints offer user comfort, they are more\nvulnerable to spoofing. The current solution for anti-spoofing in the area of\ncontactless fingerprints relies on domain adaptation learning, limiting their\ngeneralization and scalability. To address these limitations, we introduce\nGRU-AUNet, a domain adaptation approach that integrates a Swin\nTransformer-based UNet architecture with GRU-enhanced attention mechanisms, a\nDynamic Filter Network in the bottleneck, and a combined Focal and Contrastive\nLoss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet\ndemonstrates robust resilience against presentation attacks, achieving an\naverage BPCER of 0.09\\% and APCER of 1.2\\% in the CLARKSON, COLFISPOOF, and\nIIITD datasets, outperforming state-of-the-art domain adaptation methods."}
{"id": "2504.01342", "pdf": "https://arxiv.org/pdf/2504.01342", "abs": "https://arxiv.org/abs/2504.01342", "authors": ["Jungyeul Park"], "title": "Foundations and Evaluations in NLP", "categories": ["cs.CL"], "comment": null, "summary": "This memoir explores two fundamental aspects of Natural Language Processing\n(NLP): the creation of linguistic resources and the evaluation of NLP system\nperformance. Over the past decade, my work has focused on developing a\nmorpheme-based annotation scheme for the Korean language that captures\nlinguistic properties from morphology to semantics. This approach has achieved\nstate-of-the-art results in various NLP tasks, including part-of-speech\ntagging, dependency parsing, and named entity recognition. Additionally, this\nwork provides a comprehensive analysis of segmentation granularity and its\ncritical impact on NLP system performance. In parallel with linguistic resource\ndevelopment, I have proposed a novel evaluation framework, the jp-algorithm,\nwhich introduces an alignment-based method to address challenges in\npreprocessing tasks like tokenization and sentence boundary detection (SBD).\nTraditional evaluation methods assume identical tokenization and sentence\nlengths between gold standards and system outputs, limiting their applicability\nto real-world data. The jp-algorithm overcomes these limitations, enabling\nrobust end-to-end evaluations across a variety of NLP tasks. It enhances\naccuracy and flexibility by incorporating linear-time alignment while\npreserving the complexity of traditional evaluation metrics. This memoir\nprovides key insights into the processing of morphologically rich languages,\nsuch as Korean, while offering a generalizable framework for evaluating diverse\nend-to-end NLP systems. My contributions lay the foundation for future\ndevelopments, with broader implications for multilingual resource development\nand system evaluation."}
{"id": "2504.01214", "pdf": "https://arxiv.org/pdf/2504.01214", "abs": "https://arxiv.org/abs/2504.01214", "authors": ["Salim Khazem", "Jeremy Fix", "Cédric Pradalier"], "title": "PolygoNet: Leveraging Simplified Polygonal Representation for Effective Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning models have achieved significant success in various image\nrelated tasks. However, they often encounter challenges related to\ncomputational complexity and overfitting. In this paper, we propose an\nefficient approach that leverages polygonal representations of images using\ndominant points or contour coordinates. By transforming input images into these\ncompact forms, our method significantly reduces computational requirements,\naccelerates training, and conserves resources making it suitable for real time\nand resource constrained applications. These representations inherently capture\nessential image features while filtering noise, providing a natural\nregularization effect that mitigates overfitting. The resulting lightweight\nmodels achieve performance comparable to state of the art methods using full\nresolution images while enabling deployment on edge devices. Extensive\nexperiments on benchmark datasets validate the effectiveness of our approach in\nreducing complexity, improving generalization, and facilitating edge computing\napplications. This work demonstrates the potential of polygonal representations\nin advancing efficient and scalable deep learning solutions for real world\nscenarios. The code for the experiments of the paper is provided in\nhttps://github.com/salimkhazem/PolygoNet."}
{"id": "2504.01345", "pdf": "https://arxiv.org/pdf/2504.01345", "abs": "https://arxiv.org/abs/2504.01345", "authors": ["Akil Raj Subedi", "Taniya Shah", "Aswani Kumar Cherukuri", "Thanos Vasilakos"], "title": "Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social media platforms like Twitter have increasingly relied on Natural\nLanguage Processing NLP techniques to analyze and understand the sentiments\nexpressed in the user generated content. One such state of the art NLP model is\nBidirectional Encoder Representations from Transformers BERT which has been\nwidely adapted in sentiment analysis. BERT is susceptible to adversarial\nattacks. This paper aims to scrutinize the inherent vulnerabilities of such\nmodels in Twitter sentiment analysis. It aims to formulate a framework for\nconstructing targeted adversarial texts capable of deceiving these models,\nwhile maintaining stealth. In contrast to conventional methodologies, such as\nImportance Reweighting, this framework core idea resides in its reliance on\ngradients to prioritize the importance of individual words within the text. It\nuses a whitebox approach to attain fine grained sensitivity, pinpointing words\nthat exert maximal influence on the classification outcome. This paper is\norganized into three interdependent phases. It starts with fine-tuning a\npre-trained BERT model on Twitter data. It then analyzes gradients of the model\nto rank words on their importance, and iteratively replaces those with feasible\ncandidates until an acceptable solution is found. Finally, it evaluates the\neffectiveness of the adversarial text against the custom trained sentiment\nclassification model. This assessment would help in gauging the capacity of the\nadversarial text to successfully subvert classification without raising any\nalarm."}
{"id": "2504.01220", "pdf": "https://arxiv.org/pdf/2504.01220", "abs": "https://arxiv.org/abs/2504.01220", "authors": ["Banafsheh Adami", "Nima Karimian"], "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal."}
{"id": "2504.01346", "pdf": "https://arxiv.org/pdf/2504.01346", "abs": "https://arxiv.org/abs/2504.01346", "authors": ["Jiaru Zou", "Dongqi Fu", "Sirui Chen", "Xinrui He", "Zihao Li", "Yada Zhu", "Jiawei Han", "Jingrui He"], "title": "GTR: Graph-Table-RAG for Cross-Table Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Beyond pure text, a substantial amount of knowledge is stored in tables. In\nreal-world scenarios, user questions often require retrieving answers that are\ndistributed across multiple tables. GraphRAG has recently attracted much\nattention for enhancing LLMs' reasoning capabilities by organizing external\nknowledge to address ad-hoc and complex questions, exemplifying a promising\ndirection for cross-table question answering. In this paper, to address the\ncurrent gap in available data, we first introduce a multi-table benchmark,\nMutliTableQA, comprising 60k tables and 25k user queries collected from\nreal-world sources. Then, we propose the first Graph-Table-RAG framework,\nnamely GTR, which reorganizes table corpora into a heterogeneous graph, employs\na hierarchical coarse-to-fine retrieval process to extract the most relevant\ntables, and integrates graph-aware prompting for downstream LLMs' tabular\nreasoning. Extensive experiments show that GTR exhibits superior cross-table\nquestion-answering performance while maintaining high deployment efficiency,\ndemonstrating its real-world practical applicability."}
{"id": "2504.01228", "pdf": "https://arxiv.org/pdf/2504.01228", "abs": "https://arxiv.org/abs/2504.01228", "authors": ["Kimia haghjooei", "Mansoor Rezghi"], "title": "TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning models have achieved remarkable success in computer vision but\nremain vulnerable to adversarial attacks, particularly in black-box settings\nwhere model details are unknown. Existing adversarial attack methods(even those\nworks with key frames) often treat video data as simple vectors, ignoring their\ninherent multi-dimensional structure, and require a large number of queries,\nmaking them inefficient and detectable. In this paper, we propose\n\\textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages\nthe multi-dimensional properties of video data by representing videos as\nfourth-order tensors. By exploiting low-rank attack, our method significantly\nreduces the search space and the number of queries needed to generate\nadversarial examples in black-box settings. Experimental results on standard\nvideo classification datasets demonstrate that \\textbf{TenAd} effectively\ngenerates imperceptible adversarial perturbations while achieving higher attack\nsuccess rates and query efficiency compared to state-of-the-art methods. Our\napproach outperforms existing black-box adversarial attacks in terms of success\nrate, query efficiency, and perturbation imperceptibility, highlighting the\npotential of tensor-based methods for adversarial attacks on video models."}
{"id": "2504.01349", "pdf": "https://arxiv.org/pdf/2504.01349", "abs": "https://arxiv.org/abs/2504.01349", "authors": ["Allison Koenecke", "Jed Stiglitz", "David Mimno", "Matthew Wilkens"], "title": "Tasks and Roles in Legal AI: Data Curation, Annotation, and Verification", "categories": ["cs.CL"], "comment": null, "summary": "The application of AI tools to the legal field feels natural: large legal\ndocument collections could be used with specialized AI to improve workflow\nefficiency for lawyers and ameliorate the \"justice gap\" for underserved\nclients. However, legal documents differ from the web-based text that underlies\nmost AI systems. The challenges of legal AI are both specific to the legal\ndomain, and confounded with the expectation of AI's high performance in\nhigh-stakes settings. We identify three areas of special relevance to\npractitioners: data curation, data annotation, and output verification. First,\nit is difficult to obtain usable legal texts. Legal collections are\ninconsistent, analog, and scattered for reasons technical, economic, and\njurisdictional. AI tools can assist document curation efforts, but the lack of\nexisting data also limits AI performance. Second, legal data annotation\ntypically requires significant expertise to identify complex phenomena such as\nmodes of judicial reasoning or controlling precedents. We describe case studies\nof AI systems that have been developed to improve the efficiency of human\nannotation in legal contexts and identify areas of underperformance. Finally,\nAI-supported work in the law is valuable only if results are verifiable and\ntrustworthy. We describe both the abilities of AI systems to support evaluation\nof their outputs, as well as new approaches to systematic evaluation of\ncomputational systems in complex domains. We call on both legal and AI\npractitioners to collaborate across disciplines and to release open access\nmaterials to support the development of novel, high-performing, and reliable AI\ntools for legal applications."}
{"id": "2504.01243", "pdf": "https://arxiv.org/pdf/2504.01243", "abs": "https://arxiv.org/abs/2504.01243", "authors": ["Jaskaran Singh Walia", "Shravan Venkatraman", "Pavithra LK"], "title": "FUSION: Frequency-guided Underwater Spatial Image recOnstructioN", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "eess.IV"], "comment": null, "summary": "Underwater images suffer from severe degradations, including color\ndistortions, reduced visibility, and loss of structural details due to\nwavelength-dependent attenuation and scattering. Existing enhancement methods\nprimarily focus on spatial-domain processing, neglecting the frequency domain's\npotential to capture global color distributions and long-range dependencies. To\naddress these limitations, we propose FUSION, a dual-domain deep learning\nframework that jointly leverages spatial and frequency domain information.\nFUSION independently processes each RGB channel through multi-scale\nconvolutional kernels and adaptive attention mechanisms in the spatial domain,\nwhile simultaneously extracting global structural information via FFT-based\nfrequency attention. A Frequency Guided Fusion module integrates complementary\nfeatures from both domains, followed by inter-channel fusion and adaptive\nchannel recalibration to ensure balanced color distributions. Extensive\nexperiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION\nachieves state-of-the-art performance, consistently outperforming existing\nmethods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883\non UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual\nenhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly\nfewer parameters (0.28M) and lower computational complexity, demonstrating its\nsuitability for real-time underwater imaging applications."}
{"id": "2504.01369", "pdf": "https://arxiv.org/pdf/2504.01369", "abs": "https://arxiv.org/abs/2504.01369", "authors": ["Lin Zhang", "Zhouhong Gu", "Suhang Zheng", "Tao Wang", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "LITE: LLM-Impelled efficient Taxonomy Evaluation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper presents LITE, an LLM-based evaluation method designed for\nefficient and flexible assessment of taxonomy quality. To address challenges in\nlarge-scale taxonomy evaluation, such as efficiency, fairness, and consistency,\nLITE adopts a top-down hierarchical evaluation strategy, breaking down the\ntaxonomy into manageable substructures and ensuring result reliability through\ncross-validation and standardized input formats. LITE also introduces a penalty\nmechanism to handle extreme cases and provides both quantitative performance\nanalysis and qualitative insights by integrating evaluation metrics closely\naligned with task objectives. Experimental results show that LITE demonstrates\nhigh reliability in complex evaluation tasks, effectively identifying semantic\nerrors, logical contradictions, and structural flaws in taxonomies, while\noffering directions for improvement. Code is available at\nhttps://github.com/Zhang-l-i-n/TAXONOMY_DETECT ."}
{"id": "2504.01298", "pdf": "https://arxiv.org/pdf/2504.01298", "abs": "https://arxiv.org/abs/2504.01298", "authors": ["Shiyong Liu", "Zhihao Li", "Xiao Tang", "Jianzhuang Liu"], "title": "Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop", "summary": "Most model-based 3D hand pose and shape estimation methods directly regress\nthe parametric model parameters from an image to obtain 3D joints under weak\nsupervision. However, these methods involve solving a complex optimization\nproblem with many local minima, making training difficult. To address this\nchallenge, we propose learning direction-aware hybrid features (DaHyF) that\nfuse implicit image features and explicit 2D joint coordinate features. This\nfusion is enhanced by the pixel direction information in the camera coordinate\nsystem to estimate pose, shape, and camera viewpoint. Our method directly\npredicts 3D hand poses with DaHyF representation and reduces jittering during\nmotion capture using prediction confidence based on contrastive learning. We\nevaluate our method on the FreiHAND dataset and show that it outperforms\nexisting state-of-the-art methods by more than 33% in accuracy. DaHyF also\nachieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the\nmetric of Mean Joint Error (after scale and translation alignment). Compared to\nthe second-best results, the largest improvement observed is 10%. We also\ndemonstrate its effectiveness in real-time motion capture scenarios with hand\nposition variability, occlusion, and motion blur."}
{"id": "2504.01400", "pdf": "https://arxiv.org/pdf/2504.01400", "abs": "https://arxiv.org/abs/2504.01400", "authors": ["Xingshan Zeng", "Weiwen Liu", "Xu Huang", "Zezhong Wang", "Lingzhi Wang", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Ruiming Tang", "Qun Liu"], "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tool learning, which allows Large Language Models (LLMs) to leverage external\ntools for solving complex user tasks, has emerged as a promising avenue for\nextending model capabilities. However, current approaches primarily focus on\ndata synthesis for fine-tuning LLMs to invoke tools effectively, largely\nignoring how to fully stimulate the potential of the model. In this paper, we\npropose ToolACE-R, a novel method that introduces adaptive self-refinement for\ntool invocations. Our approach features a model-aware iterative training\nprocedure that progressively incorporates more training samples based on the\nmodel's evolving capabilities. Additionally, it allows LLMs to iteratively\nrefine their tool calls, optimizing performance without requiring external\nfeedback. To further enhance computational efficiency, we integrate an adaptive\nmechanism when scaling the inference time, enabling the model to autonomously\ndetermine when to stop the refinement process. We conduct extensive experiments\nacross several benchmark datasets, showing that ToolACE-R achieves competitive\nperformance compared to advanced API-based models, even without any refinement.\nFurthermore, its performance can be further improved efficiently through\nadaptive self-refinement. Our results demonstrate the effectiveness of the\nproposed method, which is compatible with base models of various sizes,\noffering a promising direction for more efficient tool learning."}
{"id": "2504.01308", "pdf": "https://arxiv.org/pdf/2504.01308", "abs": "https://arxiv.org/abs/2504.01308", "authors": ["Jiawei Wang", "Yushen Zuo", "Yuanjun Chai", "Zhendong Liu", "Yichen Fu", "Yichun Feng", "Kin-man Lam"], "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."}
{"id": "2504.01420", "pdf": "https://arxiv.org/pdf/2504.01420", "abs": "https://arxiv.org/abs/2504.01420", "authors": ["Athena Wen", "Tanush Patil", "Ansh Saxena", "Yicheng Fu", "Sean O'Brien", "Kevin Zhu"], "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git."}
{"id": "2504.01321", "pdf": "https://arxiv.org/pdf/2504.01321", "abs": "https://arxiv.org/abs/2504.01321", "authors": ["Chunhui Zhang", "Li Liu", "Jialin Gao", "Xin Sun", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint submitted to Elsevier.\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Transformer has recently demonstrated great potential in improving\nvision-language (VL) tracking algorithms. However, most of the existing VL\ntrackers rely on carefully designed mechanisms to perform the multi-stage\nmulti-modal fusion. Additionally, direct multi-modal fusion without alignment\nignores distribution discrepancy between modalities in feature space,\npotentially leading to suboptimal representations. In this work, we propose\nCOST, a contrastive one-stage transformer fusion framework for VL tracking,\naiming to learn semantically consistent and unified VL representations.\nSpecifically, we introduce a contrastive alignment strategy that maximizes\nmutual information (MI) between a video and its corresponding language\ndescription. This enables effective cross-modal alignment, yielding\nsemantically consistent features in the representation space. By leveraging a\nvisual-linguistic transformer, we establish an efficient multi-modal fusion and\nreasoning mechanism, empirically demonstrating that a simple stack of\ntransformer encoders effectively enables unified VL representations. Moreover,\nwe contribute a newly collected VL tracking benchmark dataset for small object\ntracking, named VL-SOT500, with bounding boxes and language descriptions. Our\ndataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated\nto evaluating generic and high-speed small object tracking, respectively. Small\nobject tracking is notoriously challenging due to weak appearance and limited\nfeatures, and this dataset is, to the best of our knowledge, the first to\nexplore the usage of language cues to enhance visual representation for small\nobject tracking. Extensive experiments demonstrate that COST achieves\nstate-of-the-art performance on five existing VL tracking datasets, as well as\non our proposed VL-SOT500 dataset. Source codes and dataset will be made\npublicly available."}
{"id": "2504.01429", "pdf": "https://arxiv.org/pdf/2504.01429", "abs": "https://arxiv.org/abs/2504.01429", "authors": ["Zhaoxing Li", "Xiaoming Zhang", "Haifeng Zhang", "Chengxiang Liu"], "title": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference."}
{"id": "2504.01324", "pdf": "https://arxiv.org/pdf/2504.01324", "abs": "https://arxiv.org/abs/2504.01324", "authors": ["Ke Zhu", "Yu Wang", "Jiangjiang Liu", "Qunyi Xie", "Shanshan Liu", "Gang Zhang"], "title": "On Data Synthesis and Post-training for Visual Abstract Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper is a pioneering work attempting to address abstract visual\nreasoning (AVR) problems for large vision-language models (VLMs). We make a\ncommon LLaVA-NeXT 7B model capable of perceiving and reasoning about specific\nAVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and\nclosed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a\ngreat breakthrough since almost all previous VLMs fail or show nearly random\nperformance on representative AVR benchmarks. Our key success is our innovative\ndata synthesis and post-training process, aiming to fully relieve the task\ndifficulty and elicit the model to learn, step by step. Our 7B model is also\nshown to be behave well on AVR without sacrificing common multimodal\ncomprehension abilities. We hope our paper could serve as an early effort in\nthis area and would inspire further research in abstract visual reasoning."}
{"id": "2504.01509", "pdf": "https://arxiv.org/pdf/2504.01509", "abs": "https://arxiv.org/abs/2504.01509", "authors": ["Zhengwei Tao", "Zhi Jin", "Bincheng Li", "Xiaoying Bai", "Haiyan Zhao", "Chengfeng Dou", "Xiancai Chen", "Jia Li", "Linyu Li", "Chongyang Tao"], "title": "PROPHET: An Inferable Future Forecasting Benchmark with Causal Intervened Likelihood Estimation", "categories": ["cs.CL"], "comment": null, "summary": "Predicting future events stands as one of the ultimate aspirations of\nartificial intelligence. Recent advances in large language model (LLM)-based\nsystems have shown remarkable potential in forecasting future events, thereby\ngarnering significant interest in the research community. Currently, several\nbenchmarks have been established to evaluate the forecasting capabilities by\nformalizing the event prediction as a retrieval-augmented generation (RAG) and\nreasoning task. In these benchmarks, each prediction question is answered with\nrelevant retrieved news articles. However, because there is no consideration on\nwhether the questions can be supported by valid or sufficient supporting\nrationales, some of the questions in these benchmarks may be inherently\nnoninferable. To address this issue, we introduce a new benchmark, PROPHET,\nwhich comprises inferable forecasting questions paired with relevant news for\nretrieval. To ensure the inferability of the benchmark, we propose Causal\nIntervened Likelihood (CIL), a statistical measure that assesses inferability\nthrough causal inference. In constructing this benchmark, we first collected\nrecent trend forecasting questions and then filtered the data using CIL,\nresulting in an inferable benchmark for event prediction. Through extensive\nexperiments, we first demonstrate the validity of CIL and in-depth\ninvestigations into event prediction with the aid of CIL. Subsequently, we\nevaluate several representative prediction systems on PROPHET, drawing valuable\ninsights for future directions."}
{"id": "2504.01326", "pdf": "https://arxiv.org/pdf/2504.01326", "abs": "https://arxiv.org/abs/2504.01326", "authors": ["Jin Lian", "Zhongyu Wan", "Ming Gao", "JunFeng Chen"], "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress\nin multi-scale feature fusion and boundary detail preservation for salient\nobject detection. However, traditional CFPNs still suffer from two core\nlimitations: (1) a computational bottleneck caused by complex feature weighting\noperations, and (2) degraded boundary accuracy due to feature blurring in the\nupsampling process. To address these challenges, we propose CFMD, a novel\ncross-layer feature pyramid network that introduces two key innovations. First,\nwe design a context-aware feature aggregation module (CFLMA), which\nincorporates the state-of-the-art Mamba architecture to construct a dynamic\nweight distribution mechanism. This module adaptively adjusts feature\nimportance based on image context, significantly improving both representation\nefficiency and generalization. Second, we introduce an adaptive dynamic\nupsampling unit (CFLMD) that preserves spatial details during resolution\nrecovery. By adjusting the upsampling range dynamically and initializing with a\nbilinear strategy, the module effectively reduces feature overlap and maintains\nfine-grained boundary structures. Extensive experiments on three standard\nbenchmarks using three mainstream backbone networks demonstrate that CFMD\nachieves substantial improvements in pixel-level accuracy and boundary\nsegmentation quality, especially in complex scenes. The results validate the\neffectiveness of CFMD in jointly enhancing computational efficiency and\nsegmentation performance, highlighting its strong potential in salient object\ndetection tasks."}
{"id": "2504.01519", "pdf": "https://arxiv.org/pdf/2504.01519", "abs": "https://arxiv.org/abs/2504.01519", "authors": ["Zhiyuan Tang", "Dong Wang", "Zhikai Zhou", "Yong Liu", "Shen Huang", "Shidong Shang"], "title": "Chain of Correction for Full-text Speech Recognition with Large Language Models", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Full-text error correction with Large Language Models (LLMs) for Automatic\nSpeech Recognition (ASR) has gained increased attention due to its potential to\ncorrect errors across long contexts and address a broader spectrum of error\ntypes, including punctuation restoration and inverse text normalization.\nNevertheless, many challenges persist, including issues related to stability,\ncontrollability, completeness, and fluency. To mitigate these challenges, this\npaper proposes the Chain of Correction (CoC) for full-text error correction\nwith LLMs, which corrects errors segment by segment using pre-recognized text\nas guidance within a regular multi-turn chat format. The CoC also uses\npre-recognized full text for context, allowing the model to better grasp global\nsemantics and maintain a comprehensive overview of the entire content.\nUtilizing the open-sourced full-text error correction dataset ChFT, we\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\nExperimental results demonstrate that the CoC effectively corrects errors in\nfull-text ASR outputs, significantly outperforming baseline and benchmark\nsystems. We further analyze how to set the correction threshold to balance\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\nlong ASR outputs, and investigate whether other types of information can be\nemployed to guide the error correction process."}
{"id": "2504.01328", "pdf": "https://arxiv.org/pdf/2504.01328", "abs": "https://arxiv.org/abs/2504.01328", "authors": ["Min Shi", "Shihao Wang", "Chieh-Yun Chen", "Jitesh Jain", "Kai Wang", "Junjun Xiong", "Guilin Liu", "Zhiding Yu", "Humphrey Shi"], "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Balancing temporal resolution and spatial detail under limited compute budget\nremains a key challenge for video-based multi-modal large language models\n(MLLMs). Existing methods typically compress video representations using\npredefined rules before feeding them into the LLM, resulting in irreversible\ninformation loss and often ignoring input instructions. To address this, we\npropose a novel slow-fast architecture that naturally circumvents this\ntrade-off, enabling the use of more input frames while preserving spatial\ndetails. Inspired by how humans first skim a video before focusing on relevant\nparts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual\ntokens -- a compact set of compressed video features -- are fed into the LLM\nalongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens\n-- uncompressed video features -- are cross-attended by text embeddings through\nspecially designed hybrid decoder layers, enabling instruction-aware extraction\nof relevant visual details with linear complexity. We conduct systematic\nexploration to optimize both the overall architecture and key components.\nExperiments show that our model significantly outperforms self-attention-only\nbaselines, extending the input capacity from 16 to 128 frames with just a 3%\nincrease in computation, and achieving a 16% average performance improvement\nacross five video understanding benchmarks. Our 7B model achieves\nstate-of-the-art performance among models of similar size. Furthermore, our\nslow-fast architecture is a plug-and-play design that can be integrated into\nother video MLLMs to improve efficiency and scalability."}
{"id": "2504.01534", "pdf": "https://arxiv.org/pdf/2504.01534", "abs": "https://arxiv.org/abs/2504.01534", "authors": ["Adrien Schurger-Foy", "Rafal Dariusz Kocielnik", "Caglar Gulcehre", "R. Michael Alvarez"], "title": "Context-Aware Toxicity Detection in Multiplayer Games: Integrating Domain-Adaptive Pretraining and Match Metadata", "categories": ["cs.CL"], "comment": null, "summary": "The detrimental effects of toxicity in competitive online video games are\nwidely acknowledged, prompting publishers to monitor player chat conversations.\nThis is challenging due to the context-dependent nature of toxicity, often\nspread across multiple messages or informed by non-textual interactions.\nTraditional toxicity detectors focus on isolated messages, missing the broader\ncontext needed for accurate moderation. This is especially problematic in video\ngames, where interactions involve specialized slang, abbreviations, and typos,\nmaking it difficult for standard models to detect toxicity, especially given\nits rarity. We adapted RoBERTa LLM to support moderation tailored to video\ngames, integrating both textual and non-textual context. By enhancing\npretrained embeddings with metadata and addressing the unique slang and\nlanguage quirks through domain adaptive pretraining, our method better captures\nthe nuances of player interactions. Using two gaming datasets - from Defense of\nthe Ancients 2 (DOTA 2) and Call of Duty$^\\circledR$: Modern\nWarfare$^\\circledR$III (MWIII) we demonstrate which sources of context\n(metadata, prior interactions...) are most useful, how to best leverage them to\nboost performance, and the conditions conducive to doing so. This work\nunderscores the importance of context-aware and domain-specific approaches for\nproactive moderation."}
{"id": "2504.01348", "pdf": "https://arxiv.org/pdf/2504.01348", "abs": "https://arxiv.org/abs/2504.01348", "authors": ["Yuji Nozawa", "Yu-Chieh Lin", "Kazumoto Nakamura", "Youyang Ng"], "title": "Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": "Accepted to CVPR 2025 PixFoundation Workshop", "summary": "The goal of this paper is to enhance pretrained Vision Transformer (ViT)\nmodels for focus-oriented image retrieval with visual prompting. In real-world\nimage retrieval scenarios, both query and database images often exhibit\ncomplexity, with multiple objects and intricate backgrounds. Users often want\nto retrieve images with specific object, which we define as the Focus-Oriented\nImage Retrieval (FOIR) task. While a standard image encoder can be employed to\nextract image features for similarity matching, it may not perform optimally in\nthe multi-object-based FOIR task. This is because each image is represented by\na single global feature vector. To overcome this, a prompt-based image\nretrieval solution is required. We propose an approach called Prompt-guided\nattention Head Selection (PHS) to leverage the head-wise potential of the\nmulti-head attention mechanism in ViT in a promptable manner. PHS selects\nspecific attention heads by matching their attention maps with user's visual\nprompts, such as a point, box, or segmentation. This empowers the model to\nfocus on specific object of interest while preserving the surrounding visual\ncontext. Notably, PHS does not necessitate model re-training and avoids any\nimage alteration. Experimental results show that PHS substantially improves\nperformance on multiple datasets, offering a practical and training-free\nsolution to enhance model performance in the FOIR task."}
{"id": "2504.01540", "pdf": "https://arxiv.org/pdf/2504.01540", "abs": "https://arxiv.org/abs/2504.01540", "authors": ["Mikkel Wildner Kildeberg", "Emil Allerslev Schledermann", "Nicolaj Larsen", "Rob van der Goot"], "title": "From Smør-re-brød to Subwords: Training LLMs on Danish, One Morpheme at a Time", "categories": ["cs.CL"], "comment": null, "summary": "The best performing transformer-based language models use subword\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\napproaches often overlook linguistic principles, such as morphological\nsegmentation, which we believe is fundamental for understanding\nlanguage-specific word structure. In this study, we leverage an annotated\nDanish morphological dataset to train a semisupervised model for morphological\nsegmentation, enabling the development of tokenizers optimized for Danish\nmorphology. We evaluate four distinct tokenizers, including two custom\nmorphological tokenizers, by analyzing their performance in morphologically\nsegmenting Danish words. Additionally, we train two generative transformer\nmodels, \\textit{CerebrasGPT-111M} and \\textit{LLaMA-3.2 1B}, using these\ntokenizers and evaluate their downstream performance. Our findings reveal that\nour custom-developed tokenizers substantially enhance morphological\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\nDanish BPE tokenizer. In downstream tasks, models trained with our\nmorphological tokenizers outperform those using BPE tokenizers across different\nevaluation metrics. These results highlight that incorporating Danish\nmorphological segmentation strategies into tokenizers leads to improved\nperformance in generative transformer models on Danish language"}
{"id": "2504.01383", "pdf": "https://arxiv.org/pdf/2504.01383", "abs": "https://arxiv.org/abs/2504.01383", "authors": ["Chang-Bin Zhang", "Jinhong Ni", "Yujie Zhong", "Kai Han"], "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, Project page:\n  https://visual-ai.github.io/vclr, Code: https://github.com/Visual-AI/vCLR", "summary": "In this paper, we address the challenging problem of open-world instance\nsegmentation. Existing works have shown that vanilla visual networks are biased\ntoward learning appearance information, \\eg texture, to recognize objects. This\nimplicit bias causes the model to fail in detecting novel objects with unseen\ntextures in the open-world setting. To address this challenge, we propose a\nlearning framework, called view-Consistent LeaRning (v-CLR), which aims to\nenforce the model to learn appearance-invariant representations for robust\ninstance segmentation. In v-CLR, we first introduce additional views for each\nimage, where the texture undergoes significant alterations while preserving the\nimage's underlying structure. We then encourage the model to learn the\nappearance-invariant representation by enforcing the consistency between object\nfeatures across different views, for which we obtain class-agnostic object\nproposals using off-the-shelf unsupervised models that possess strong\nobject-awareness. These proposals enable cross-view object feature matching,\ngreatly reducing the appearance dependency while enhancing the\nobject-awareness. We thoroughly evaluate our method on public benchmarks under\nboth cross-class and cross-dataset settings, achieving state-of-the-art\nperformance. Project page: https://visual-ai.github.io/vclr"}
{"id": "2504.01542", "pdf": "https://arxiv.org/pdf/2504.01542", "abs": "https://arxiv.org/abs/2504.01542", "authors": ["Amanda Myntti", "Erik Henriksson", "Veronika Laippala", "Sampo Pyysalo"], "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation", "categories": ["cs.CL"], "comment": null, "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labeling systems, datasets\nare divided into categories, frequently reducing to a binary: those passing the\nfilters deemed as valuable examples, others discarded as useless or\ndetrimental. However, a more detailed understanding of the contribution of\ndifferent kinds of texts to model performance is still largely lacking. In this\narticle, we present the first study utilizing registers (also known as genres)\n- a widely used standard in corpus linguistics to model linguistic variation -\nto curate pretraining datasets and investigate the effect of register on the\nperformance of LLMs. We perform comparative studies by training models with\nregister classified data and evaluating them using standard benchmarks, and\nshow that the register of pretraining data substantially affects model\nperformance. We uncover surprising relationships between the pretraining\nmaterial and the resulting models: using the News register results in subpar\nperformance, and on the contrary, including the Opinion class, covering texts\nsuch as reviews and opinion blogs, is highly beneficial. While a model trained\non the entire unfiltered dataset outperforms those trained on datasets limited\nto a single register, combining well-performing registers like\nHow-to-Instructions, Informational Description, and Opinion leads to major\nimprovements. Furthermore, analysis of individual benchmark results reveals key\ndifferences in the strengths and drawbacks of specific register classes as\npretraining data. These findings show that register is an important explainer\nof model variation and can facilitate more deliberate future data selection\npractices."}
{"id": "2504.01386", "pdf": "https://arxiv.org/pdf/2504.01386", "abs": "https://arxiv.org/abs/2504.01386", "authors": ["Junjie Wu", "Jiangtao Xie", "Zhaolin Zhang", "Qilong Wang", "Qinghua Hu", "Peihua Li", "Sen Xu"], "title": "DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Recently, Contrastive Language-Image Pre-training (CLIP) has shown promising\nperformance in domain-specific data (e.g., biology), and has attracted\nincreasing research attention. Existing works generally focus on collecting\nextensive domain-specific data and directly tuning the original CLIP models.\nIntuitively, such a paradigm takes no full consideration of the characteristics\nlying in domain-specific data (e.g., fine-grained nature of biological data)\nand so limits model capability, while mostly losing the original ability of\nCLIP in the general domain. In this paper, we propose a Distribution\nAlignment-based Language-Image Pre-Training (DALIP) method for biological data.\nSpecifically, DALIP optimizes CLIP models by matching the similarity between\nfeature distribution of image-text pairs instead of the original [cls] token,\nwhich can capture rich yet effective information inherent in image-text pairs\nas powerful representations, and so better cope with fine-grained nature of\nbiological data. Particularly, our DALIP efficiently approximates feature\ndistribution via its first- and second-order statistics, while presenting a\nMulti-head Brownian Distance Covariance (MBDC) module to acquire second-order\nstatistics of token features efficiently. Furthermore, we collect a new dataset\nfor plant domain (e.g., specific data in biological domain) comprising 10M\nplant data with 3M general-domain data (namely PlantMix-13M) according to data\nmixing laws. Extensive experiments show that DALIP clearly outperforms existing\nCLIP counterparts in biological domain, while well generalizing to remote\nsensing and medical imaging domains. Besides, our PlantMix-13M dataset further\nboosts performance of DALIP in plant domain, while preserving model ability in\ngeneral domain."}
{"id": "2504.01667", "pdf": "https://arxiv.org/pdf/2504.01667", "abs": "https://arxiv.org/abs/2504.01667", "authors": ["Cedric Lothritz", "Jordi Cabot"], "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish", "categories": ["cs.CL"], "comment": "18 pages, 2 figures, 11 tables", "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\nmodels show weak performances. We also find that the performances in such\nlanguage exams can be used to predict performances in other NLP tasks."}
{"id": "2504.01396", "pdf": "https://arxiv.org/pdf/2504.01396", "abs": "https://arxiv.org/abs/2504.01396", "authors": ["Zheng Yang", "Ruoxin Chen", "Zhiyuan Yan", "Ke-Yue Zhang", "Xinghe Fu", "Shuang Wu", "Xiujun Shu", "Taiping Yao", "Junchi Yan", "Shouhong Ding", "Xi Li"], "title": "All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of AI-generated images (AIGIs) underscores the urgent\nneed for robust and generalizable detection methods. In this paper, we\nestablish two key principles for AIGI detection through systematic analysis:\n\\textbf{(1) All Patches Matter:} Unlike conventional image classification where\ndiscriminative features concentrate on object-centric regions, each patch in\nAIGIs inherently contains synthetic artifacts due to the uniform generation\nprocess, suggesting that every patch serves as an important artifact source for\ndetection. \\textbf{(2) More Patches Better}: Leveraging distributed artifacts\nacross more patches improves detection robustness by capturing complementary\nforensic evidence and reducing over-reliance on specific patches, thereby\nenhancing robustness and generalization. However, our counterfactual analysis\nreveals an undesirable phenomenon: naively trained detectors often exhibit a\n\\textbf{Few-Patch Bias}, discriminating between real and synthetic images based\non minority patches. We identify \\textbf{Lazy Learner} as the root cause:\ndetectors preferentially learn conspicuous artifacts in limited patches while\nneglecting broader artifact distributions. To address this bias, we propose the\n\\textbf{P}anoptic \\textbf{P}atch \\textbf{L}earning (PPL) framework, involving:\n(1) Random Patch Replacement that randomly substitutes synthetic patches with\nreal counterparts to compel models to identify artifacts in underutilized\nregions, encouraging the broader use of more patches; (2) Patch-wise\nContrastive Learning that enforces consistent discriminative capability across\nall patches, ensuring uniform utilization of all patches. Extensive experiments\nacross two different settings on several benchmarks verify the effectiveness of\nour approach."}
{"id": "2504.01698", "pdf": "https://arxiv.org/pdf/2504.01698", "abs": "https://arxiv.org/abs/2504.01698", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Jiajun Song", "Lifeng Fan", "Wei Wang"], "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs."}
{"id": "2504.01399", "pdf": "https://arxiv.org/pdf/2504.01399", "abs": "https://arxiv.org/abs/2504.01399", "authors": ["Haibo Zhang", "Zhihua Yao", "Kouichi Sakurai", "Takeshi Saitoh"], "title": "Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In the rapidly evolving field of artificial intelligence, machine learning\nemerges as a key technology characterized by its vast potential and inherent\nrisks. The stability and reliability of these models are important, as they are\nfrequent targets of security threats. Adversarial attacks, first rigorously\ndefined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability:\nthey can trick machine learning models into making incorrect predictions by\napplying nearly invisible perturbations to images. Although many studies have\nfocused on constructing sophisticated defensive mechanisms to mitigate such\nattacks, they often overlook the substantial time and computational costs of\ntraining and maintaining these models. Ideally, a defense method should be able\nto generalize across various, even unseen, adversarial attacks with minimal\noverhead. Building on our previous work on image-to-image translation-based\ndefenses, this study introduces an improved model that incorporates residual\nblocks to enhance generalizability. The proposed method requires training only\na single model, effectively defends against diverse attack types, and is\nwell-transferable between different target models. Experiments show that our\nmodel can restore the classification accuracy from near zero to an average of\n72\\% while maintaining competitive performance compared to state-of-the-art\nmethods."}
{"id": "2504.01707", "pdf": "https://arxiv.org/pdf/2504.01707", "abs": "https://arxiv.org/abs/2504.01707", "authors": ["Bowen Cao", "Deng Cai", "Wai Lam"], "title": "InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) is critical for large language models (LLMs), but\nits effectiveness is constrained by finite context windows, particularly in\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\nthat parallels context and parameters in LLMs with short- and long-term memory\nin human cognitive systems, focusing on transforming temporary context\nknowledge into permanent parameter updates. This approach significantly reduces\nmemory usage, maintains robust performance across varying input lengths, and\ntheoretically enables infinite context integration through the principles of\ncontext knowledge elicitation, selection, and consolidation. Evaluations\ndemonstrate that our method reduces context length by 90% while achieving 103%\naverage performance of full-context prompting across fact recall, grounded\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\ntransformations on complex, real-world contexts (with length up to 2M tokens),\nour approach surpasses full-context prompting while using only 0.4% of the\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\nthe scalability and efficiency of LLMs by breaking the limitations of\nconventional context window sizes."}
{"id": "2504.01407", "pdf": "https://arxiv.org/pdf/2504.01407", "abs": "https://arxiv.org/abs/2504.01407", "authors": ["Junwen Pan", "Rui Zhang", "Xin Wan", "Yuan Zhang", "Ming Lu", "Qi She"], "title": "TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large video-language models (LVLMs) have shown remarkable performance across\nvarious video-language tasks. However, they encounter significant challenges\nwhen processing long videos because of the large number of video frames\ninvolved. Downsampling long videos in either space or time can lead to visual\nhallucinations, making it difficult to accurately interpret long videos.\nMotivated by human hierarchical temporal search strategies, we propose\n\\textbf{TimeSearch}, a novel framework enabling LVLMs to understand long videos\nin a human-like manner. TimeSearch integrates two human-like primitives into a\nunified autoregressive LVLM: 1) \\textbf{Spotlight} efficiently identifies\nrelevant temporal events through a Temporal-Augmented Frame Representation\n(TAFR), explicitly binding visual features with timestamps; 2)\n\\textbf{Reflection} evaluates the correctness of the identified events,\nleveraging the inherent temporal self-reflection capabilities of LVLMs.\nTimeSearch progressively explores key events and prioritizes temporal search\nbased on reflection confidence. Extensive experiments on challenging long-video\nbenchmarks confirm that TimeSearch substantially surpasses previous\nstate-of-the-art, improving the accuracy from 41.8\\% to 51.5\\% on the LVBench.\nAdditionally, experiments on temporal grounding demonstrate that appropriate\nTAFR is adequate to effectively stimulate the surprising temporal grounding\nability of LVLMs in a simpler yet versatile manner, which improves mIoU on\nCharades-STA by 11.8\\%. The code will be released."}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738", "abs": "https://arxiv.org/abs/2504.01738", "authors": ["Philip Lippmann", "Jie Yang"], "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families."}
{"id": "2504.01428", "pdf": "https://arxiv.org/pdf/2504.01428", "abs": "https://arxiv.org/abs/2504.01428", "authors": ["Zhuangzhuang Chen", "Hualiang Wang", "Chubin Ou", "Xiaomeng Li"], "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects."}
{"id": "2504.01789", "pdf": "https://arxiv.org/pdf/2504.01789", "abs": "https://arxiv.org/abs/2504.01789", "authors": ["Sumeth Yuenyong", "Thodsaporn Chay-intr", "Kobkrit Viriyayudhakorn"], "title": "OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\nLanguage Models (LLMs) developed through distinct methodologies to enhance\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\nlanguage tasks, achieving competitive results against larger-scale open-source\nThai LLMs. This paper details the proposed models, training processes,\nbenchmarks, and results, highlighting improvements over previous models and\nestablishing new performance standards for Thai-centric LLMs."}
{"id": "2504.01449", "pdf": "https://arxiv.org/pdf/2504.01449", "abs": "https://arxiv.org/abs/2504.01449", "authors": ["Zaipeng Duan", "Xuzhong Hu", "Pei An", "Jie Ma"], "title": "Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based 3D point cloud recognition has been proven beneficial in various\napplications. However, the sparsity and varying density pose a significant\nchallenge in capturing intricate details of objects, particularly for\nmedium-range and small targets. Therefore, we propose a multi-modal point cloud\nsemantic segmentation method based on Virtual Point Enhancement (VPE), which\nintegrates virtual points generated from images to address these issues. These\nvirtual points are dense but noisy, and directly incorporating them can\nincrease computational burden and degrade performance. Therefore, we introduce\na spatial difference-driven adaptive filtering module that selectively extracts\nvaluable pseudo points from these virtual points based on density and distance,\nenhancing the density of medium-range targets. Subsequently, we propose a\nnoise-robust sparse feature encoder that incorporates noise-robust feature\nextraction and fine-grained feature enhancement. Noise-robust feature\nextraction exploits the 2D image space to reduce the impact of noisy points,\nwhile fine-grained feature enhancement boosts sparse geometric features through\ninner-voxel neighborhood point aggregation and downsampled voxel aggregation.\nThe results on the SemanticKITTI and nuScenes, two large-scale benchmark data\nsets, have validated effectiveness, significantly improving 2.89\\% mIoU with\nthe introduction of 7.7\\% virtual points on nuScenes."}
{"id": "2504.01801", "pdf": "https://arxiv.org/pdf/2504.01801", "abs": "https://arxiv.org/abs/2504.01801", "authors": ["Zhijun Wang", "Jiahuan Li", "Hao Zhou", "Rongxiang Weng", "Jingang Wang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Shujian Huang"], "title": "Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities."}
{"id": "2504.01452", "pdf": "https://arxiv.org/pdf/2504.01452", "abs": "https://arxiv.org/abs/2504.01452", "authors": ["Encheng Su", "Hu Cao", "Alois Knoll"], "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)", "summary": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset."}
{"id": "2504.01833", "pdf": "https://arxiv.org/pdf/2504.01833", "abs": "https://arxiv.org/abs/2504.01833", "authors": ["Sumuk Shashidhar", "Clémentine Fourrier", "Alina Lozovskia", "Thomas Wolf", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "YourBench: Easy Custom Evaluation Sets for Everyone", "categories": ["cs.CL", "cs.AI", "I.2.1"], "comment": null, "summary": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation."}
{"id": "2504.01457", "pdf": "https://arxiv.org/pdf/2504.01457", "abs": "https://arxiv.org/abs/2504.01457", "authors": ["Ting Meng", "Chunyun Fu", "Xiangyan Yan", "Zheng Liang", "Pan Ji", "Jianwen Wang", "Tao Huang"], "title": "Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker", "categories": ["cs.CV"], "comment": "11 pages, 6 fugures", "summary": "Multi-object tracking plays a crucial role in various applications, such as\nautonomous driving and security surveillance. This study introduces Deep\nLG-Track, a novel multi-object tracker that incorporates three key enhancements\nto improve the tracking accuracy and robustness. First, an adaptive Kalman\nfilter is developed to dynamically update the covariance of measurement noise\nbased on detection confidence and trajectory disappearance. Second, a novel\ncost matrix is formulated to adaptively fuse motion and appearance information,\nleveraging localization confidence and detection confidence as weighting\nfactors. Third, a dynamic appearance feature updating strategy is introduced,\nadjusting the relative weighting of historical and current appearance features\nbased on appearance clarity and localization accuracy. Comprehensive\nevaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep\nLG-Track consistently outperforms state-of-the-art trackers across multiple\nperformance metrics, highlighting its effectiveness in multi-object tracking\ntasks."}
{"id": "2504.01840", "pdf": "https://arxiv.org/pdf/2504.01840", "abs": "https://arxiv.org/abs/2504.01840", "authors": ["Minhu Park", "Hongseok Oh", "Eunkyung Choi", "Wonseok Hwang"], "title": "LARGE: Legal Retrieval Augmented Generation Evaluation Tool", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE."}
{"id": "2504.01466", "pdf": "https://arxiv.org/pdf/2504.01466", "abs": "https://arxiv.org/abs/2504.01466", "authors": ["Kaiwei Zhang", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes", "categories": ["cs.CV"], "comment": "to be published in CVPR 2025", "summary": "Mesh saliency enhances the adaptability of 3D vision by identifying and\nemphasizing regions that naturally attract visual attention. To investigate the\ninteraction between geometric structure and texture in shaping visual\nattention, we establish a comprehensive mesh saliency dataset, which is the\nfirst to systematically capture the differences in saliency distribution under\nboth textured and non-textured visual conditions. Furthermore, we introduce\nmesh Mamba, a unified saliency prediction model based on a state space model\n(SSM), designed to adapt across various mesh types. Mesh Mamba effectively\nanalyzes the geometric structure of the mesh while seamlessly incorporating\ntexture features into the topological framework, ensuring coherence throughout\nappearance-enhanced modeling. More importantly, by subgraph embedding and a\nbidirectional SSM, the model enables global context modeling for both local\ngeometry and texture, preserving the topological structure and improving the\nunderstanding of visual details and structural complexity. Through extensive\ntheoretical and empirical validation, our model not only improves performance\nacross various mesh types but also demonstrates high scalability and\nversatility, particularly through cross validations of various visual features."}
{"id": "2504.01857", "pdf": "https://arxiv.org/pdf/2504.01857", "abs": "https://arxiv.org/abs/2504.01857", "authors": ["Zhiwei Yu", "Tuo Li", "Changhong Wang", "Hui Chen", "Lang Zhou"], "title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing\nreasoning capabilities in large language models (LLMs), with self-consistency\ndemonstrating notable promise in boosting performance. However, inherent\nlinguistic biases in multilingual training corpora frequently cause semantic\ndrift and logical inconsistencies, especially in sub-10B parameter LLMs\nhandling complex inference tasks. To overcome these constraints, we propose the\nCross-Lingual Consistency (CLC) framework, an innovative inference paradigm\nthat integrates multilingual reasoning paths through majority voting to elevate\nLLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal\nCLC's superiority over the conventional self-consistency method, delivering\n9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct,\nQwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's\nlinguistic scope to 11 diverse languages implies two synergistic benefits: 1)\nneutralizing linguistic biases in multilingual training corpora through\nmultilingual ensemble voting, 2) escaping monolingual reasoning traps by\nexploring the broader multilingual solution space. This dual benefits\nempirically enables more globally optimal reasoning paths compared to\nmonolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy\ngains using Gemma2-9B-Instruct on the MGSM dataset."}
{"id": "2504.01470", "pdf": "https://arxiv.org/pdf/2504.01470", "abs": "https://arxiv.org/abs/2504.01470", "authors": ["Soumyya Kanti Datta", "Shan Jia", "Siwei Lyu"], "title": "Detecting Lip-Syncing Deepfakes: Vision Temporal Transformer for Analyzing Mouth Inconsistencies", "categories": ["cs.CV"], "comment": null, "summary": "Deepfakes are AI-generated media in which the original content is digitally\naltered to create convincing but manipulated images, videos, or audio. Among\nthe various types of deepfakes, lip-syncing deepfakes are one of the most\nchallenging deepfakes to detect. In these videos, a person's lip movements are\nsynthesized to match altered or entirely new audio using AI models. Therefore,\nunlike other types of deepfakes, the artifacts in lip-syncing deepfakes are\nconfined to the mouth region, making them more subtle and, thus harder to\ndiscern. In this paper, we propose LIPINC-V2, a novel detection framework that\nleverages a combination of vision temporal transformer with multihead\ncross-attention to detect lip-syncing deepfakes by identifying spatiotemporal\ninconsistencies in the mouth region. These inconsistencies appear across\nadjacent frames and persist throughout the video. Our model can successfully\ncapture both short-term and long-term variations in mouth movement, enhancing\nits ability to detect these inconsistencies. Additionally, we created a new\nlip-syncing deepfake dataset, LipSyncTIMIT, which was generated using five\nstate-of-the-art lip-syncing models to simulate real-world scenarios. Extensive\nexperiments on our proposed LipSyncTIMIT dataset and two other benchmark\ndeepfake datasets demonstrate that our model achieves state-of-the-art\nperformance. The code and the dataset are available at\nhttps://github.com/skrantidatta/LIPINC-V2 ."}
{"id": "2504.01879", "pdf": "https://arxiv.org/pdf/2504.01879", "abs": "https://arxiv.org/abs/2504.01879", "authors": ["Abhilash Shankarampeta", "Harsh Mahajan", "Tushar Kataria", "Dan Roth", "Vivek Gupta"], "title": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables", "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "19 Pages. 21 Tables, 1 figure", "summary": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance."}
{"id": "2504.01472", "pdf": "https://arxiv.org/pdf/2504.01472", "abs": "https://arxiv.org/abs/2504.01472", "authors": ["Yuejiao Su", "Yi Wang", "Qiongyang Hu", "Chuang Yang", "Lap-Pui Chau"], "title": "ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction", "categories": ["cs.CV"], "comment": "Computer Vision and Pattern Recognition", "summary": "Egocentric interaction perception is one of the essential branches in\ninvestigating human-environment interaction, which lays the basis for\ndeveloping next-generation intelligent systems. However, existing egocentric\ninteraction understanding methods cannot yield coherent textual and pixel-level\nresponses simultaneously according to user queries, which lacks flexibility for\nvarying downstream application requirements. To comprehend egocentric\ninteractions exhaustively, this paper presents a novel task named Egocentric\nInteraction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image\nwith the query as input, Ego-IRG is the first task that aims to resolve the\ninteractions through three crucial steps: analyzing, answering, and pixel\ngrounding, which results in fluent textual and fine-grained pixel-level\nresponses. Another challenge is that existing datasets cannot meet the\nconditions for the Ego-IRG task. To address this limitation, this paper creates\nthe Ego-IRGBench dataset based on extensive manual efforts, which includes over\n20k egocentric images with 1.6 million queries and corresponding multimodal\nresponses about interactions. Moreover, we design a unified ANNEXE model to\ngenerate text- and pixel-level outputs utilizing multimodal large language\nmodels, which enables a comprehensive interpretation of egocentric\ninteractions. The experiments on the Ego-IRGBench exhibit the effectiveness of\nour ANNEXE model compared with other works."}
{"id": "2504.01902", "pdf": "https://arxiv.org/pdf/2504.01902", "abs": "https://arxiv.org/abs/2504.01902", "authors": ["Célia Nouri", "Jean-Philippe Cointet", "Chloé Clavel"], "title": "Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Detecting abusive language in social media conversations poses significant\nchallenges, as identifying abusiveness often depends on the conversational\ncontext, characterized by the content and topology of preceding comments.\nTraditional Abusive Language Detection (ALD) models often overlook this\ncontext, which can lead to unreliable performance metrics. Recent Natural\nLanguage Processing (NLP) methods that integrate conversational context often\ndepend on limited and simplified representations, and report inconsistent\nresults. In this paper, we propose a novel approach that utilize graph neural\nnetworks (GNNs) to model social media conversations as graphs, where nodes\nrepresent comments, and edges capture reply structures. We systematically\ninvestigate various graph representations and context windows to identify the\noptimal configuration for ALD. Our GNN model outperform both context-agnostic\nbaselines and linear context-aware methods, achieving significant improvements\nin F1 scores. These findings demonstrate the critical role of structured\nconversational context and establish GNNs as a robust framework for advancing\ncontext-aware abusive language detection."}
{"id": "2504.01476", "pdf": "https://arxiv.org/pdf/2504.01476", "abs": "https://arxiv.org/abs/2504.01476", "authors": ["Junlong Ren", "Hao Wang"], "title": "Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "Cross-modal 3D retrieval is a critical yet challenging task, aiming to\nachieve bi-directional retrieval between 3D and text modalities. Current\nmethods predominantly rely on a certain 3D representation (e.g., point cloud),\nwith few exploiting the 2D-3D consistency and complementary relationships,\nwhich constrains their performance. To bridge this gap, we propose to adopt\nmulti-view images and point clouds to jointly represent 3D shapes, facilitating\ntri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D\nretrieval. Notably, we introduce tri-modal reconstruction to improve the\ngeneralization ability of encoders. Given point features, we reconstruct image\nfeatures under the guidance of text features, and vice versa. With well-aligned\npoint cloud and multi-view image features, we aggregate them as multimodal\nembeddings through fine-grained 2D-3D fusion to enhance geometric and semantic\nunderstanding. Recognizing the significant noise in current datasets where many\n3D shapes and texts share similar semantics, we employ hard negative\ncontrastive training to emphasize harder negatives with greater significance,\nleading to robust discriminative embeddings. Extensive experiments on the\nText2Shape dataset demonstrate that our method significantly outperforms\nprevious state-of-the-art methods in both shape-to-text and text-to-shape\nretrieval tasks by a substantial margin."}
{"id": "2504.01903", "pdf": "https://arxiv.org/pdf/2504.01903", "abs": "https://arxiv.org/abs/2504.01903", "authors": ["Zijun Wang", "Haoqin Tu", "Yuhan Wang", "Juncheng Wu", "Jieru Mei", "Brian R. Bartoldson", "Bhavya Kailkhura", "Cihang Xie"], "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1."}
{"id": "2504.01503", "pdf": "https://arxiv.org/pdf/2504.01503", "abs": "https://arxiv.org/abs/2504.01503", "authors": ["Ziteng Cui", "Xuangeng Chu", "Tatsuya Harada"], "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment", "categories": ["cs.CV"], "comment": "CVPR 2025, project page:\n  https://cuiziteng.github.io/Luminance_GS_web/", "summary": "Capturing high-quality photographs under diverse real-world lighting\nconditions is challenging, as both natural lighting (e.g., low-light) and\ncamera exposure settings (e.g., exposure time) significantly impact image\nquality. This challenge becomes more pronounced in multi-view scenarios, where\nvariations in lighting and image signal processor (ISP) settings across\nviewpoints introduce photometric inconsistencies. Such lighting degradations\nand view-dependent variations pose substantial challenges to novel view\nsynthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel\napproach to achieving high-quality novel view synthesis results under diverse\nchallenging lighting conditions using 3DGS. By adopting per-view color matrix\nmapping and view-adaptive curve adjustments, Luminance-GS achieves\nstate-of-the-art (SOTA) results across various lighting conditions -- including\nlow-light, overexposure, and varying exposure -- while not altering the\noriginal 3DGS explicit representation. Compared to previous NeRF- and\n3DGS-based baselines, Luminance-GS provides real-time rendering speed with\nimproved reconstruction quality."}
{"id": "2504.01919", "pdf": "https://arxiv.org/pdf/2504.01919", "abs": "https://arxiv.org/abs/2504.01919", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Asif Ekbal"], "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\nthat enable effective adaptation to under-resourced settings. The paper also\nexplores synthetic data generation strategies using LLMs, including\nback-translation and lexical augmentation. Additionally, we compare LLM-based\ntranslation with traditional encoder-decoder models across diverse language\npairs, highlighting the strengths and limitations of each. We discuss\npersistent challenges such as hallucinations, evaluation inconsistencies, and\ninherited biases while also evaluating emerging LLM-driven metrics for\ntranslation quality. This survey offers practical insights and outlines future\ndirections for building robust, inclusive, and scalable MT systems in the era\nof large-scale generative models."}
{"id": "2504.01512", "pdf": "https://arxiv.org/pdf/2504.01512", "abs": "https://arxiv.org/abs/2504.01512", "authors": ["Yiyang Shen", "Kun Zhou", "He Wang", "Yin Yang", "Tianjia Shao"], "title": "High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Recently single-view 3D generation via Gaussian splatting has emerged and\ndeveloped quickly. They learn 3D Gaussians from 2D RGB images generated from\npre-trained multi-view diffusion (MVD) models, and have shown a promising\navenue for 3D generation through a single image. Despite the current progress,\nthese methods still suffer from the inconsistency jointly caused by the\ngeometric ambiguity in the 2D images, and the lack of structure of 3D\nGaussians, leading to distorted and blurry 3D object generation. In this paper,\nwe propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian\nReconstruction Model designed to generate high-fidelity 3D objects from\nsingle-view images. Our key insight is a structured 3D representation can\nsimultaneously mitigate the afore-mentioned two issues. To this end, we propose\na novel hybrid Voxel-Gaussian representation, where a 3D voxel representation\ncontains explicit 3D geometric information, eliminating the geometric ambiguity\nfrom 2D images. It also structures Gaussians during learning so that the\noptimization tends to find better local optima. Our 3D voxel representation is\nobtained by a fusion module that aligns RGB features and surface normal\nfeatures, both of which can be estimated from 2D images. Extensive experiments\ndemonstrate the superiority of our methods over prior works in terms of\nhigh-quality reconstruction results, robust generalization, and good\nefficiency."}
{"id": "2504.01928", "pdf": "https://arxiv.org/pdf/2504.01928", "abs": "https://arxiv.org/abs/2504.01928", "authors": ["Boshi Wang", "Huan Sun"], "title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure", "categories": ["cs.CL", "cs.LG"], "comment": "Code and data:\n  https://github.com/OSU-NLP-Group/reversal-curse-binding", "summary": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning."}
{"id": "2504.01515", "pdf": "https://arxiv.org/pdf/2504.01515", "abs": "https://arxiv.org/abs/2504.01515", "authors": ["Zixuan Wang", "Duo Peng", "Feng Chen", "Yuwei Yang", "Yinjie Lei"], "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Conditional image synthesis is a crucial task with broad applications, such\nas artistic creation and virtual reality. However, current generative methods\nare often task-oriented with a narrow scope, handling a restricted condition\nwith constrained applicability. In this paper, we propose a novel approach that\ntreats conditional image synthesis as the modular combination of diverse\nfundamental condition units. Specifically, we divide conditions into three\nprimary units: text, layout, and drag. To enable effective control over these\nconditions, we design a dedicated alignment module for each. For the text\ncondition, we introduce a Dense Concept Alignment (DCA) module, which achieves\ndense visual-text alignment by drawing on diverse textual concepts. For the\nlayout condition, we propose a Dense Geometry Alignment (DGA) module to enforce\ncomprehensive geometric constraints that preserve the spatial configuration.\nFor the drag condition, we introduce a Dense Motion Alignment (DMA) module to\napply multi-level motion regularization, ensuring that each pixel follows its\ndesired trajectory without visual artifacts. By flexibly inserting and\ncombining these alignment modules, our framework enhances the model's\nadaptability to diverse conditional generation tasks and greatly expands its\napplication range. Extensive experiments demonstrate the superior performance\nof our framework across a variety of conditions, including textual description,\nsegmentation mask (bounding box), drag manipulation, and their combinations.\nCode is available at https://github.com/ZixuanWang0525/DADG."}
{"id": "2504.01930", "pdf": "https://arxiv.org/pdf/2504.01930", "abs": "https://arxiv.org/abs/2504.01930", "authors": ["Washington Cunha", "Leonardo Rocha", "Marcos André Gonçalves"], "title": "A thorough benchmark of automatic text classification: From traditional approaches to large language models", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 2 figures, 3 tables", "summary": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off."}
{"id": "2504.01527", "pdf": "https://arxiv.org/pdf/2504.01527", "abs": "https://arxiv.org/abs/2504.01527", "authors": ["Olivier Rukundo"], "title": "Beyond Nearest Neighbor Interpolation in Data Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": "6 pages, 9 figures, 1 table", "summary": "Avoiding the risk of undefined categorical labels using nearest neighbor\ninterpolation overlooks the risk of exacerbating pixel level annotation errors\nin data augmentation. To simultaneously avoid these risks, the author modified\nconvolutional neural networks data transformation functions by incorporating a\nmodified geometric transformation function to improve the quality of augmented\ndata by removing the reliance on nearest neighbor interpolation and integrating\na mean based class filtering mechanism to handle undefined categorical labels\nwith alternative interpolation algorithms. Experiments on semantic segmentation\ntasks using three medical image datasets demonstrated both qualitative and\nquantitative improvements with alternative interpolation algorithms."}
{"id": "2504.01931", "pdf": "https://arxiv.org/pdf/2504.01931", "abs": "https://arxiv.org/abs/2504.01931", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Jindong Gu", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Hamid Palangi", "Tomas Pfister"], "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection", "categories": ["cs.CL"], "comment": null, "summary": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization."}
{"id": "2504.01547", "pdf": "https://arxiv.org/pdf/2504.01547", "abs": "https://arxiv.org/abs/2504.01547", "authors": ["Luca Ciampi", "Gabriele Lagani", "Giuseppe Amato", "Fabrizio Falchi"], "title": "Semi-Supervised Biomedical Image Segmentation via Diffusion Models and Teacher-Student Co-Training", "categories": ["cs.CV"], "comment": null, "summary": "Supervised deep learning for semantic segmentation has achieved excellent\nresults in accurately identifying anatomical and pathological structures in\nmedical images. However, it often requires large annotated training datasets,\nwhich limits its scalability in clinical settings. To address this challenge,\nsemi-supervised learning is a well-established approach that leverages both\nlabeled and unlabeled data. In this paper, we introduce a novel semi-supervised\nteacher-student framework for biomedical image segmentation, inspired by the\nrecent success of generative models. Our approach leverages denoising diffusion\nprobabilistic models (DDPMs) to generate segmentation masks by progressively\nrefining noisy inputs conditioned on the corresponding images. The teacher\nmodel is first trained in an unsupervised manner using a cycle-consistency\nconstraint based on noise-corrupted image reconstruction, enabling it to\ngenerate informative semantic masks. Subsequently, the teacher is integrated\ninto a co-training process with a twin-student network. The student learns from\nground-truth labels when available and from teacher-generated pseudo-labels\notherwise, while the teacher continuously improves its pseudo-labeling\ncapabilities. Finally, to further enhance performance, we introduce a\nmulti-round pseudo-label generation strategy that iteratively improves the\npseudo-labeling process. We evaluate our approach on multiple biomedical\nimaging benchmarks, spanning multiple imaging modalities and segmentation\ntasks. Experimental results show that our method consistently outperforms\nstate-of-the-art semi-supervised techniques, highlighting its effectiveness in\nscenarios with limited annotated data. The code to replicate our experiments\ncan be found at\nhttps://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation"}
{"id": "2504.01943", "pdf": "https://arxiv.org/pdf/2504.01943", "abs": "https://arxiv.org/abs/2504.01943", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."}
{"id": "2504.01559", "pdf": "https://arxiv.org/pdf/2504.01559", "abs": "https://arxiv.org/abs/2504.01559", "authors": ["Yahui Li", "Zhi Zeng", "Liming Pang", "Guixuan Zhang", "Shuwu Zhang"], "title": "RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars", "categories": ["cs.CV"], "comment": null, "summary": "Modeling animatable human avatars from monocular or multi-view videos has\nbeen widely studied, with recent approaches leveraging neural radiance fields\n(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in\nnovel-view and novel-pose synthesis. However, existing methods often struggle\nto accurately capture the dynamics of loose clothing, as they primarily rely on\nglobal pose conditioning or static per-frame representations, leading to\noversmoothing and temporal inconsistencies in non-rigid regions. To address\nthis, We propose RealityAvatar, an efficient framework for high-fidelity\ndigital human modeling, specifically targeting loosely dressed avatars. Our\nmethod leverages 3D Gaussian Splatting to capture complex clothing deformations\nand motion dynamics while ensuring geometric consistency. By incorporating a\nmotion trend module and a latentbone encoder, we explicitly model\npose-dependent deformations and temporal variations in clothing behavior.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nour approach in capturing fine-grained clothing deformations and motion-driven\nshape variations. Our method significantly enhances structural fidelity and\nperceptual quality in dynamic human reconstruction, particularly in non-rigid\nregions, while achieving better consistency across temporal frames."}
{"id": "2504.01028", "pdf": "https://arxiv.org/pdf/2504.01028", "abs": "https://arxiv.org/abs/2504.01028", "authors": ["Anket Mehra", "Malte Prieß", "Marian Himstedt"], "title": "Improving Applicability of Deep Learning based Token Classification models during Training", "categories": ["cs.CV", "cs.CL", "cs.IR"], "comment": null, "summary": "This paper shows that further evaluation metrics during model training are\nneeded to decide about its applicability in inference. As an example, a\nLayoutLM-based model is trained for token classification in documents. The\ndocuments are German receipts. We show that conventional classification\nmetrics, represented by the F1-Score in our experiments, are insufficient for\nevaluating the applicability of machine learning models in practice. To address\nthis problem, we introduce a novel metric, Document Integrity Precision (DIP),\nas a solution for visual document understanding and the token classification\ntask. To the best of our knowledge, nothing comparable has been introduced in\nthis context. DIP is a rigorous metric, describing how many documents of the\ntest dataset require manual interventions. It enables AI researchers and\nsoftware developers to conduct an in-depth investigation of the level of\nprocess automation in business software. In order to validate DIP, we conduct\nexperiments with our created models to highlight and analyze the impact and\nrelevance of DIP to evaluate if the model should be deployed or not in\ndifferent training settings. Our results demonstrate that existing metrics\nbarely change for isolated model impairments, whereas DIP indicates that the\nmodel requires substantial human interventions in deployment. The larger the\nset of entities being predicted, the less sensitive conventional metrics are,\nentailing poor automation quality. DIP, in contrast, remains a single value to\nbe interpreted for entire entity sets. This highlights the importance of having\nmetrics that focus on the business task for model training in production. Since\nDIP is created for the token classification task, more research is needed to\nfind suitable metrics for other training tasks."}
{"id": "2504.01589", "pdf": "https://arxiv.org/pdf/2504.01589", "abs": "https://arxiv.org/abs/2504.01589", "authors": ["Zhaochen Wang", "Yujun Cai", "Zi Huang", "Bryan Hooi", "Yiwei Wang", "Ming-Hsuan Yang"], "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Under review at COLM 2025", "summary": "Vision-language models (VLMs) have advanced rapidly in processing multimodal\ninformation, but their ability to reconcile conflicting signals across\nmodalities remains underexplored. This work investigates how VLMs process ASCII\nart, a unique medium where textual elements collectively form visual patterns,\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\nframework that systematically challenges five state-of-the-art models\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\ncharacter-level semantics deliberately contradict global visual patterns. Our\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\ntextual information over visual patterns, with visual recognition ability\ndeclining dramatically as semantic complexity increases. Various mitigation\nattempts through visual parameter tuning and prompt engineering yielded only\nmodest improvements, suggesting that this limitation requires\narchitectural-level solutions. These findings uncover fundamental flaws in how\ncurrent VLMs integrate multimodal information, providing important guidance for\nfuture model development while highlighting significant implications for\ncontent moderation systems vulnerable to adversarial examples."}
{"id": "2504.01081", "pdf": "https://arxiv.org/pdf/2504.01081", "abs": "https://arxiv.org/abs/2504.01081", "authors": ["Wenjun Zeng", "Dana Kurniawan", "Ryan Mullins", "Yuchi Liu", "Tamoghna Saha", "Dirichi Ike-Njoku", "Jindong Gu", "Yiwen Song", "Cai Xu", "Jingjing Zhou", "Aparna Joshi", "Shravan Dheep", "Mani Malek", "Hamid Palangi", "Joon Baek", "Rick Pereira", "Karthik Narasimhan"], "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation", "categories": ["cs.CV", "cs.CL", "eess.IV"], "comment": null, "summary": "We introduce ShieldGemma 2, a 4B parameter image content moderation model\nbuilt on Gemma 3. This model provides robust safety risk predictions across the\nfollowing key harm categories: Sexually Explicit, Violence \\& Gore, and\nDangerous Content for synthetic images (e.g. output of any image generation\nmodel) and natural images (e.g. any image input to a Vision-Language Model). We\nevaluated on both internal and external benchmarks to demonstrate\nstate-of-the-art performance compared to LlavaGuard\n\\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base\nGemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we\npresent a novel adversarial data generation pipeline which enables a\ncontrolled, diverse, and robust image generation. ShieldGemma 2 provides an\nopen image moderation tool to advance multimodal safety and responsible AI\ndevelopment."}
{"id": "2504.01591", "pdf": "https://arxiv.org/pdf/2504.01591", "abs": "https://arxiv.org/abs/2504.01591", "authors": ["Adriano Fragomeni", "Dima Damen", "Michael Wray"], "title": "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Video retrieval requires aligning visual content with corresponding natural\nlanguage descriptions. In this paper, we introduce Modality Auxiliary Concepts\nfor Video Retrieval (MAC-VR), a novel approach that leverages modality-specific\ntags -- automatically extracted from foundation models -- to enhance video\nretrieval. We propose to align modalities in a latent space, along with\nlearning and aligning auxiliary latent concepts, derived from the features of a\nvideo and its corresponding caption. We introduce these auxiliary concepts to\nimprove the alignment of visual and textual latent concepts, and so are able to\ndistinguish concepts from one other. We conduct extensive experiments on five\ndiverse datasets: MSR-VTT, DiDeMo, TGIF, Charades and YouCook2. The\nexperimental results consistently demonstrate that modality-specific tags\nimprove cross-modal alignment, outperforming current state-of-the-art methods\nacross three datasets and performing comparably or better across the other two."}
{"id": "2504.01094", "pdf": "https://arxiv.org/pdf/2504.01094", "abs": "https://arxiv.org/abs/2504.01094", "authors": ["Jaechul Roh", "Virat Shejwalkar", "Amir Houmansadr"], "title": "Multilingual and Multi-Accent Jailbreaking of Audio LLMs", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CR", "eess.AS"], "comment": "21 pages, 6 figures, 15 tables", "summary": "Large Audio Language Models (LALMs) have significantly advanced audio\nunderstanding but introduce critical security risks, particularly through audio\njailbreaks. While prior work has focused on English-centric attacks, we expose\na far more severe vulnerability: adversarial multilingual and multi-accent\naudio jailbreaks, where linguistic and acoustic variations dramatically amplify\nattack success. In this paper, we introduce Multi-AudioJail, the first\nsystematic framework to exploit these vulnerabilities through (1) a novel\ndataset of adversarially perturbed multilingual/multi-accent audio jailbreaking\nprompts, and (2) a hierarchical evaluation pipeline revealing that how acoustic\nperturbations (e.g., reverberation, echo, and whisper effects) interacts with\ncross-lingual phonetics to cause jailbreak success rates (JSRs) to surge by up\nto +57.25 percentage points (e.g., reverberated Kenyan-accented attack on\nMERaLiON). Crucially, our work further reveals that multimodal LLMs are\ninherently more vulnerable than unimodal systems: attackers need only exploit\nthe weakest link (e.g., non-English audio inputs) to compromise the entire\nmodel, which we empirically show by multilingual audio-only attacks achieving\n3.1x higher success rates than text-only attacks. We plan to release our\ndataset to spur research into cross-modal defenses, urging the community to\naddress this expanding attack surface in multimodality as LALMs evolve."}
{"id": "2504.01596", "pdf": "https://arxiv.org/pdf/2504.01596", "abs": "https://arxiv.org/abs/2504.01596", "authors": ["Jijun Xiang", "Xuan Zhu", "Xianqi Wang", "Yu Wang", "Hong Zhang", "Fei Guo", "Xin Yang"], "title": "DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image", "categories": ["cs.CV"], "comment": "10 pages, 8 figures, 7 tables", "summary": "Depth enhancement, which uses RGB images as guidance to convert raw signals\nfrom dToF into high-precision, dense depth maps, is a critical task in computer\nvision. Although existing super-resolution-based methods show promising results\non public datasets, they often rely on idealized assumptions like accurate\nregion correspondences and reliable dToF inputs, overlooking calibration errors\nthat cause misalignment and anomaly signals inherent to dToF imaging, limiting\nreal-world applicability. To address these challenges, we propose a novel\ncompletion-based method, named DEPTHOR, featuring advances in both the training\nstrategy and model architecture. First, we propose a method to simulate\nreal-world dToF data from the accurate ground truth in synthetic datasets to\nenable noise-robust training. Second, we design a novel network that\nincorporates monocular depth estimation (MDE), leveraging global depth\nrelationships and contextual information to improve prediction in challenging\nregions. On the ZJU-L5 dataset, our training strategy significantly enhances\ndepth completion models, achieving results comparable to depth super-resolution\nmethods, while our model achieves state-of-the-art results, improving Rel and\nRMSE by 27% and 18%, respectively. On a more challenging set of dToF samples we\ncollected, our method outperforms SOTA methods on preliminary stereo-based GT,\nimproving Rel and RMSE by 23% and 22%, respectively. Our Code is available at\nhttps://github.com/ShadowBbBb/Depthor"}
{"id": "2504.01205", "pdf": "https://arxiv.org/pdf/2504.01205", "abs": "https://arxiv.org/abs/2504.01205", "authors": ["Nicholas Clark", "Hua Shen", "Bill Howe", "Tanushree Mitra"], "title": "Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "LLMs increasingly serve as tools for knowledge acquisition, yet users cannot\neffectively specify how they want information presented. When users request\nthat LLMs \"cite reputable sources,\" \"express appropriate uncertainty,\" or\n\"include multiple perspectives,\" they discover that current interfaces provide\nno structured way to articulate these preferences. The result is prompt sharing\nfolklore: community-specific copied prompts passed through trust relationships\nrather than based on measured efficacy. We propose the Epistemic Alignment\nFramework, a set of ten challenges in knowledge transmission derived from the\nphilosophical literature of epistemology, concerning issues such as evidence\nquality assessment and calibration of testimonial reliance. The framework\nserves as a structured intermediary between user needs and system capabilities,\ncreating a common vocabulary to bridge the gap between what users want and what\nsystems deliver. Through a thematic analysis of custom prompts and\npersonalization strategies shared on online communities where these issues are\nactively discussed, we find users develop elaborate workarounds to address each\nof the challenges. We then apply our framework to two prominent model\nproviders, OpenAI and Anthropic, through content analysis of their documented\npolicies and product features. Our analysis shows that while these providers\nhave partially addressed the challenges we identified, they fail to establish\nadequate mechanisms for specifying epistemic preferences, lack transparency\nabout how preferences are implemented, and offer no verification tools to\nconfirm whether preferences were followed. For AI developers, the Epistemic\nAlignment Framework offers concrete guidance for supporting diverse approaches\nto knowledge; for users, it works toward information delivery that aligns with\ntheir specific needs rather than defaulting to one-size-fits-all approaches."}
{"id": "2504.01597", "pdf": "https://arxiv.org/pdf/2504.01597", "abs": "https://arxiv.org/abs/2504.01597", "authors": ["Yuehui Qiu", "Dandan Shan", "Yining Wang", "Pei Dong", "Dijia Wu", "Xinnian Yang", "Qingqi Hong", "Dinggang Shen"], "title": "A topology-preserving three-stage framework for fully-connected coronary artery extraction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Coronary artery extraction is a crucial prerequisite for computer-aided\ndiagnosis of coronary artery disease. Accurately extracting the complete\ncoronary tree remains challenging due to several factors, including presence of\nthin distal vessels, tortuous topological structures, and insufficient\ncontrast. These issues often result in over-segmentation and under-segmentation\nin current segmentation methods. To address these challenges, we propose a\ntopology-preserving three-stage framework for fully-connected coronary artery\nextraction. This framework includes vessel segmentation, centerline\nreconnection, and missing vessel reconstruction. First, we introduce a new\ncenterline enhanced loss in the segmentation process. Second, for the broken\nvessel segments, we further propose a regularized walk algorithm to integrate\ndistance, probabilities predicted by a centerline classifier, and directional\ncosine similarity, for reconnecting the centerlines. Third, we apply implicit\nneural representation and implicit modeling, to reconstruct the geometric model\nof the missing vessels. Experimental results show that our proposed framework\noutperforms existing methods, achieving Dice scores of 88.53\\% and 85.07\\%,\nwith Hausdorff Distances (HD) of 1.07mm and 1.63mm on ASOCA and PDSCA datasets,\nrespectively. Code will be available at https://github.com/YH-Qiu/CorSegRec."}
{"id": "2504.01281", "pdf": "https://arxiv.org/pdf/2504.01281", "abs": "https://arxiv.org/abs/2504.01281", "authors": ["Sakhinana Sagar Srinivas", "Venkataramana Runkana"], "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."}
{"id": "2504.01603", "pdf": "https://arxiv.org/pdf/2504.01603", "abs": "https://arxiv.org/abs/2504.01603", "authors": ["Yizhe Tang", "Zhimin Sun", "Yuzhen Du", "Ran Yi", "Guangben Lu", "Teng Hu", "Luying Li", "Lizhuang Ma", "Fangyuan Zou"], "title": "A$^\\text{T}$A: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Image inpainting aims to fill the missing region of an image. Recently, there\nhas been a surge of interest in foreground-conditioned background inpainting, a\nsub-task that fills the background of an image while the foreground subject and\nassociated text prompt are provided. Existing background inpainting methods\ntypically strictly preserve the subject's original position from the source\nimage, resulting in inconsistencies between the subject and the generated\nbackground. To address this challenge, we propose a new task, the \"Text-Guided\nSubject-Position Variable Background Inpainting\", which aims to dynamically\nadjust the subject position to achieve a harmonious relationship between the\nsubject and the inpainted background, and propose the Adaptive Transformation\nAgent (A$^\\text{T}$A) for this task. Firstly, we design a PosAgent Block that\nadaptively predicts an appropriate displacement based on given features to\nachieve variable subject-position. Secondly, we design the Reverse Displacement\nTransform (RDT) module, which arranges multiple PosAgent blocks in a reverse\nstructure, to transform hierarchical feature maps from deep to shallow based on\nsemantic information. Thirdly, we equip A$^\\text{T}$A with a Position Switch\nEmbedding to control whether the subject's position in the generated image is\nadaptively predicted or fixed. Extensive comparative experiments validate the\neffectiveness of our A$^\\text{T}$A approach, which not only demonstrates\nsuperior inpainting capabilities in subject-position variable inpainting, but\nalso ensures good performance on subject-position fixed inpainting."}
{"id": "2504.01324", "pdf": "https://arxiv.org/pdf/2504.01324", "abs": "https://arxiv.org/abs/2504.01324", "authors": ["Ke Zhu", "Yu Wang", "Jiangjiang Liu", "Qunyi Xie", "Shanshan Liu", "Gang Zhang"], "title": "On Data Synthesis and Post-training for Visual Abstract Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper is a pioneering work attempting to address abstract visual\nreasoning (AVR) problems for large vision-language models (VLMs). We make a\ncommon LLaVA-NeXT 7B model capable of perceiving and reasoning about specific\nAVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and\nclosed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a\ngreat breakthrough since almost all previous VLMs fail or show nearly random\nperformance on representative AVR benchmarks. Our key success is our innovative\ndata synthesis and post-training process, aiming to fully relieve the task\ndifficulty and elicit the model to learn, step by step. Our 7B model is also\nshown to be behave well on AVR without sacrificing common multimodal\ncomprehension abilities. We hope our paper could serve as an early effort in\nthis area and would inspire further research in abstract visual reasoning."}
{"id": "2504.01619", "pdf": "https://arxiv.org/pdf/2504.01619", "abs": "https://arxiv.org/abs/2504.01619", "authors": ["Hao Wu", "Hao Wang", "Ruochong Li", "Xuran Ma", "Hui Xiong"], "title": "3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Recent advancements in text-to-3D generation have shown remarkable results by\nleveraging 3D priors in combination with 2D diffusion. However, previous\nmethods utilize 3D priors that lack detailed and complex structural\ninformation, limiting them to generating simple objects and presenting\nchallenges for creating intricate structures such as bonsai. In this paper, we\npropose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with\ncomplex structures. Technically, we first design a trainable 3D space\ncolonization algorithm to produce bonsai structures, which are then enhanced\nthrough random sampling and point cloud augmentation to serve as the 3D\nGaussian priors. We introduce two bonsai generation pipelines with distinct\nstructural levels: fine structure conditioned generation, which initializes 3D\nGaussians using a 3D structure prior to produce detailed and complex bonsai,\nand coarse structure conditioned generation, which employs a multi-view\nstructure consistency module to align 2D and 3D structures. Moreover, we have\ncompiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental\nresults demonstrate that 3DBonsai significantly outperforms existing methods,\nproviding a new benchmark for structure-aware 3D bonsai generation."}
{"id": "2504.01337", "pdf": "https://arxiv.org/pdf/2504.01337", "abs": "https://arxiv.org/abs/2504.01337", "authors": ["Mohan Zhang", "Pingzhi Li", "Jie Peng", "Mufan Qiu", "Tianlong Chen"], "title": "Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": "NAACL 2025", "summary": "Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\nnearly constant computing costs. By employing a gating network to route input\ntokens, it selectively activates a subset of expert networks to process the\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\nchallenging to achieve due to two key reasons: imbalanced expert activation,\nwhich leads to substantial idle time during model or expert parallelism, and\ninsufficient capacity utilization; massive communication overhead, induced by\nnumerous expert routing combinations in expert parallelism at the system level.\nPrevious works typically formulate it as the load imbalance issue characterized\nby the gating network favoring certain experts over others or attribute it to\nstatic execution which fails to adapt to the dynamic expert workload at\nruntime. In this paper, we exploit it from a brand new perspective, a\nhigher-order view and analysis of MoE routing policies: expert collaboration\nand specialization where some experts tend to activate broadly with others\n(collaborative), while others are more likely to activate only with a specific\nsubset of experts (specialized). Our experiments reveal that most experts tend\nto be overly collaborative, leading to increased communication overhead from\nrepeatedly sending tokens to different accelerators. To this end, we propose a\nnovel collaboration-constrained routing (C2R) strategy to encourage more\nspecialized expert groups, as well as to improve expert utilization, and\npresent an efficient implementation of MoE that further leverages expert\nspecialization. We achieve an average performance improvement of 0.51% and\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\nMegaBlocks."}
{"id": "2504.01620", "pdf": "https://arxiv.org/pdf/2504.01620", "abs": "https://arxiv.org/abs/2504.01620", "authors": ["Haidong Wu", "Snehal Bhayani", "Janne Heikkilä"], "title": "A Conic Transformation Approach for Solving the Perspective-Three-Point Problem", "categories": ["cs.CV"], "comment": null, "summary": "We propose a conic transformation method to solve the Perspective-Three-Point\n(P3P) problem. In contrast to the current state-of-the-art solvers, which\nformulate the P3P problem by intersecting two conics and constructing a\ndegenerate conic to find the intersection, our approach builds upon a new\nformulation based on a transformation that maps the two conics to a new\ncoordinate system, where one of the conics becomes a standard parabola in a\ncanonical form. This enables expressing one variable in terms of the other\nvariable, and as a consequence, substantially simplifies the problem of finding\nthe conic intersection. Moreover, the polynomial coefficients are fast to\ncompute, and we only need to determine the real-valued intersection points,\nwhich avoids the requirement of using computationally expensive complex\narithmetic. While the current state-of-the-art methods reduce the conic\nintersection problem to solving a univariate cubic equation, our approach,\ndespite resulting in a quartic equation, is still faster thanks to this new\nsimplified formulation. Extensive evaluations demonstrate that our method\nachieves higher speed while maintaining robustness and stability comparable to\nstate-of-the-art methods."}
{"id": "2504.01382", "pdf": "https://arxiv.org/pdf/2504.01382", "abs": "https://arxiv.org/abs/2504.01382", "authors": ["Tianci Xue", "Weijian Qi", "Tianneng Shi", "Chan Hee Song", "Boyu Gou", "Dawn Song", "Huan Sun", "Yu Su"], "title": "An Illusion of Progress? Assessing the Current State of Web Agents", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 16 figures, 4 tables", "summary": "As digitalization and cloud technologies evolve, the web is becoming\nincreasingly important in the modern society. Autonomous web agents based on\nlarge language models (LLMs) hold a great potential in work automation. It is\ntherefore important to accurately measure and monitor the progression of their\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\nof the current state of web agents. Our results depict a very different picture\nof the competency of current agents, suggesting over-optimism in previously\nreported results. This gap can be attributed to shortcomings in existing\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\nus to evaluate web agents under a setting that approximates how real users use\nthese agents. To facilitate more scalable evaluation and development, we also\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\nachieve around 85% agreement with human judgment, substantially higher than\nexisting methods. Finally, we present the first comprehensive comparative\nanalysis of current web agents, highlighting both their strengths and\nlimitations to inspire future research."}
{"id": "2504.01632", "pdf": "https://arxiv.org/pdf/2504.01632", "abs": "https://arxiv.org/abs/2504.01632", "authors": ["Giulia Marchiori Pietrosanti", "Giulio Rossolini", "Alessandro Biondi", "Giorgio Buttazzo"], "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks."}
{"id": "2504.01403", "pdf": "https://arxiv.org/pdf/2504.01403", "abs": "https://arxiv.org/abs/2504.01403", "authors": ["Ming Pang", "Chunyuan Yuan", "Xiaoyu He", "Zheng Fang", "Donghao Xie", "Fanyi Qu", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by WWW2025", "summary": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality."}
{"id": "2504.01641", "pdf": "https://arxiv.org/pdf/2504.01641", "abs": "https://arxiv.org/abs/2504.01641", "authors": ["Zhixin Cheng", "Jiacheng Deng", "Xinjun Li", "Baoqun Yin", "Tianzhu Zhang"], "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI2025accept", "summary": "The method for image-to-point cloud registration typically determines the\nrigid transformation using a coarse-to-fine pipeline. However, directly and\nuniformly matching image patches with point cloud patches may lead to focusing\non incorrect noise patches during matching while ignoring key ones. Moreover,\ndue to the significant differences between image and point cloud modalities, it\nmay be challenging to bridge the domain gap without specific improvements in\ndesign. To address the above issues, we innovatively propose the\nUncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal\nAlignment Module (AMAM). Within the UHMM, we model the uncertainty of critical\ninformation in image patches and facilitate multi-level fusion interactions\nbetween image and point cloud features. In the AMAM, we design an adversarial\napproach to reduce the domain gap between image and point cloud. Extensive\nexperiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks\ndemonstrate the superiority of our method, making it a state-of-the-art\napproach for image-to-point cloud registration tasks."}
{"id": "2504.01450", "pdf": "https://arxiv.org/pdf/2504.01450", "abs": "https://arxiv.org/abs/2504.01450", "authors": ["Runlong Zhou", "Yi Zhang"], "title": "CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Language models often struggle with cross-mode knowledge retrieval -- the\nability to access knowledge learned in one format (mode) when queried in\nanother. We demonstrate that models trained on multiple data sources (e.g.,\nWikipedia and TinyStories) exhibit significantly reduced accuracy when\nretrieving knowledge in a format different from its original training mode.\nThis paper quantitatively investigates this phenomenon through a controlled\nstudy of random token sequence memorization across different modes. We first\nexplore dataset rewriting as a solution, revealing that effective cross-mode\nretrieval requires prohibitively extensive rewriting efforts that follow a\nsigmoid-like relationship. As an alternative, we propose CASCADE, a novel\npretraining algorithm that uses cascading datasets with varying sequence\nlengths to capture knowledge at different scales. Our experiments demonstrate\nthat CASCADE outperforms dataset rewriting approaches, even when compressed\ninto a single model with a unified loss function. This work provides both\nqualitative evidence of cross-mode retrieval limitations and a practical\nsolution to enhance language models' ability to access knowledge independently\nof its presentational format."}
{"id": "2504.01647", "pdf": "https://arxiv.org/pdf/2504.01647", "abs": "https://arxiv.org/abs/2504.01647", "authors": ["Tobias Fischer", "Samuel Rota Bulò", "Yung-Hsu Yang", "Nikhil Varma Keetha", "Lorenzo Porzi", "Norman Müller", "Katja Schwarz", "Jonathon Luiten", "Marc Pollefeys", "Peter Kontschieder"], "title": "FlowR: Flowing from Sparse to Dense 3D Reconstructions", "categories": ["cs.CV"], "comment": "Project page is available at https://tobiasfshr.github.io/pub/flowr", "summary": "3D Gaussian splatting enables high-quality novel view synthesis (NVS) at\nreal-time frame rates. However, its quality drops sharply as we depart from the\ntraining views. Thus, dense captures are needed to match the high-quality\nexpectations of some applications, e.g. Virtual Reality (VR). However, such\ndense captures are very laborious and expensive to obtain. Existing works have\nexplored using 2D generative models to alleviate this requirement by\ndistillation or generating additional training views. These methods are often\nconditioned only on a handful of reference input views and thus do not fully\nexploit the available 3D information, leading to inconsistent generation\nresults and reconstruction artifacts. To tackle this problem, we propose a\nmulti-view, flow matching model that learns a flow to connect novel view\nrenderings from possibly sparse reconstructions to renderings that we expect\nfrom dense reconstructions. This enables augmenting scene captures with novel,\ngenerated views to improve reconstruction quality. Our model is trained on a\nnovel dataset of 3.6M image pairs and can process up to 45 views at 540x960\nresolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline\nconsistently improves NVS in sparse- and dense-view scenarios, leading to\nhigher-quality reconstructions than prior works across multiple, widely-used\nNVS benchmarks."}
{"id": "2504.01522", "pdf": "https://arxiv.org/pdf/2504.01522", "abs": "https://arxiv.org/abs/2504.01522", "authors": ["Silvia Fernandez-Sabido", "Laura Peniche-Sabido"], "title": "Redefining technology for indigenous languages", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "in Spanish language", "summary": "In this paper, we offer an overview of indigenous languages, identifying the\ncauses of their devaluation and the need for legislation on language rights. We\nreview the technologies used to revitalize these languages, finding that when\nthey come from outside, they often have the opposite effect to what they seek;\nhowever, when developed from within communities, they become powerful\ninstruments of expression. We propose that the inclusion of Indigenous\nknowledge in large language models (LLMs) will enrich the technological\nlandscape, but must be done in a participatory environment that encourages the\nexchange of knowledge."}
{"id": "2504.01648", "pdf": "https://arxiv.org/pdf/2504.01648", "abs": "https://arxiv.org/abs/2504.01648", "authors": ["Haosheng Li", "Yuecong Xu", "Junjie Chen", "Kemi Ding"], "title": "ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D point cloud semantic segmentation technology has been widely used.\nHowever, in real-world scenarios, the environment is evolving. Thus,\noffline-trained segmentation models may lead to catastrophic forgetting of\npreviously seen classes. Class-incremental learning (CIL) is designed to\naddress the problem of catastrophic forgetting. While point clouds are common,\nwe observe high similarity and unclear boundaries between different classes.\nMeanwhile, they are known to be imbalanced in class distribution. These lead to\nissues including misclassification between similar classes and the long-tail\nproblem, which have not been adequately addressed in previous CIL methods. We\nthus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels).\nIn the base-class training phase, ProtoGuard maintains geometric and semantic\nprototypes for each class, which are combined into prototype features using an\nattention mechanism. In the novel-class training phase, PROPEL inherits the\nbase feature extractor and classifier, guiding pseudo-label propagation and\nupdates based on density distribution and semantic similarity. Extensive\nexperiments show that our approach achieves remarkable results on both the\nS3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation\nby a maximum of 20.39% under the 5-step CIL scenario on S3DIS."}
{"id": "2504.01550", "pdf": "https://arxiv.org/pdf/2504.01550", "abs": "https://arxiv.org/abs/2504.01550", "authors": ["Ashkan Yousefpour", "Taeheon Kim", "Ryan S. Kwon", "Seungbeen Lee", "Wonje Jeung", "Seungju Han", "Alvin Wan", "Harrison Ngan", "Youngjae Yu", "Jonghyun Choi"], "title": "Representation Bending for Large Language Model Safety", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities."}
{"id": "2504.01655", "pdf": "https://arxiv.org/pdf/2504.01655", "abs": "https://arxiv.org/abs/2504.01655", "authors": ["Yiting Lu", "Xin Li", "Haoning Wu", "Bingchen Li", "Weisi Lin", "Zhibo Chen"], "title": "Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive Instruction Tuning", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved\nthe way for the possible Explainable Image Quality Assessment (EIQA) with\ninstruction tuning from two perspectives: overall quality explanation, and\nattribute-wise perception answering. However, existing works usually overlooked\nthe conflicts between these two types of perception explanations during joint\ninstruction tuning, leading to insufficient perception understanding. To\nmitigate this, we propose a new paradigm for perception-oriented instruction\ntuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the\nsynergy between these two EIQA tasks when adapting LMM, resulting in enhanced\nmulti-faceted explanations of IQA. Particularly, we propose a progressive\ninstruction tuning strategy by dividing the adaption process of LMM for EIQA\ninto two stages, where the first stage empowers the LMM with universal\nperception knowledge tailored for two tasks using an efficient transfer\nlearning strategy, i.e., LoRA, and the second stage introduces the\ninstruction-adaptive visual prompt tuning to dynamically adapt visual features\nfor the different instructions from two tasks. In this way, our proposed\nQ-Adapt can achieve a lightweight visual quality evaluator, demonstrating\ncomparable performance and, in some instances, superior results across\nperceptual-related benchmarks and commonly-used IQA databases. The source code\nis publicly available at https://github.com/yeppp27/Q-Adapt."}
{"id": "2504.01627", "pdf": "https://arxiv.org/pdf/2504.01627", "abs": "https://arxiv.org/abs/2504.01627", "authors": ["Lena Schmidt", "Oshin Sharma", "Chris Marshall", "Sonia Garcia Gonzalez Moral"], "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Introduction: Horizon scanning in healthcare assesses early signals of\ninnovation, crucial for timely adoption. Current horizon scanning faces\nchallenges in efficient information retrieval and analysis, especially from\nunstructured sources like news, presenting a need for innovative tools.\nMethodology: The study introduces SCANAR and AIDOC, open-source Python-based\ntools designed to improve horizon scanning. SCANAR automates the retrieval and\nprocessing of news articles, offering functionalities such as de-duplication\nand unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to\nreorder textual data based on relevancy, employing neural networks for semantic\nsimilarity, and subsequently prioritizing likely relevant entries for human\nreview. Results: Twelve internal datasets from horizon scans and four external\nbenchmarking datasets were used. SCANAR improved retrieval efficiency by\nautomating processes previously dependent on manual labour. AIDOC displayed\nwork-saving potential, achieving around 62% reduction in manual review efforts\nat 95% recall. Comparative analysis with benchmarking data showed AIDOC's\nperformance was similar to existing systematic review automation tools, though\nperformance varied depending on dataset characteristics. A smaller case-study\non our news datasets shows the potential of ensembling large language models\nwithin the active-learning process for faster detection of relevant articles\nacross news datasets. Conclusion: The validation indicates that SCANAR and\nAIDOC show potential to enhance horizon scanning efficiency by streamlining\ndata retrieval and prioritisation. These tools may alleviate methodological\nlimitations and allow broader, swifter horizon scans. Further studies are\nsuggested to optimize these models and to design new workflows and validation\nprocesses that integrate large language models."}
{"id": "2504.01659", "pdf": "https://arxiv.org/pdf/2504.01659", "abs": "https://arxiv.org/abs/2504.01659", "authors": ["Haosheng Li", "Yuecong Xu", "Junjie Chen", "Kemi Ding"], "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised domain adaptation (UDA) frameworks have shown good\ngeneralization capabilities for 3D point cloud semantic segmentation models on\nclean data. However, existing works overlook adversarial robustness when the\nsource domain itself is compromised. To comprehensively explore the robustness\nof the UDA frameworks, we first design a stealthy adversarial point cloud\ngeneration attack that can significantly contaminate datasets with only minor\nperturbations to the point cloud surface. Based on that, we propose a novel\ndataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds.\nWith the generated corrupted data, we further develop the Adversarial\nAdaptation Framework (AAF) as the countermeasure. Specifically, by extending\nthe key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss)\nand utilizing a decoder branch, our approach enables the model to focus on\nlong-tail classes during the pre-training phase and leverages high-confidence\ndecoded point cloud information to restore point cloud structures during the\nadaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where\nthe results demonstrate that our AAF method can mitigate performance\ndegradation under source adversarial perturbations for UDA in the 3D point\ncloud segmentation application."}
{"id": "2504.01681", "pdf": "https://arxiv.org/pdf/2504.01681", "abs": "https://arxiv.org/abs/2504.01681", "authors": ["Maelyson R. F. Santos", "Marcelo A. F. Gomes"], "title": "Study of scaling laws in language families", "categories": ["physics.soc-ph", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "This article investigates scaling laws within language families using data\nfrom over six thousand languages and analyzing emergent patterns observed in\nZipf-like classification graphs. Both macroscopic (based on number of languages\nby family) and microscopic (based on numbers of speakers by language on a\nfamily) aspects of these classifications are examined. Particularly noteworthy\nis the discovery of a distinct division among the fourteen largest contemporary\nlanguage families, excluding Afro-Asiatic and Nilo-Saharan languages. These\nfamilies are found to be distributed across three language family quadruplets,\neach characterized by significantly different exponents in the Zipf graphs.\nThis finding sheds light on the underlying structure and organization of major\nlanguage families, revealing intriguing insights into the nature of linguistic\ndiversity and distribution."}
{"id": "2504.01662", "pdf": "https://arxiv.org/pdf/2504.01662", "abs": "https://arxiv.org/abs/2504.01662", "authors": ["Namhun Kim", "UiHyun Cho"], "title": "BioAtt: Anatomical Prior Driven Low-Dose CT Denoising", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages", "summary": "Deep-learning-based denoising methods have significantly improved Low-Dose CT\n(LDCT) image quality. However, existing models often over-smooth important\nanatomical details due to their purely data-driven attention mechanisms. To\naddress this challenge, we propose a novel LDCT denoising framework, BioAtt.\nThe key innovation lies in attending anatomical prior distributions extracted\nfrom the pretrained vision-language model BiomedCLIP. These priors guide the\ndenoising model to focus on anatomically relevant regions to suppress noise\nwhile preserving clinically relevant structures. We highlight three main\ncontributions: BioAtt outperforms baseline and attention-based models in SSIM,\nPSNR, and RMSE across multiple anatomical regions. The framework introduces a\nnew architectural paradigm by embedding anatomic priors directly into spatial\nattention. Finally, BioAtt attention maps provide visual confirmation that the\nimprovements stem from anatomical guidance rather than increased model\ncomplexity."}
{"id": "2504.01818", "pdf": "https://arxiv.org/pdf/2504.01818", "abs": "https://arxiv.org/abs/2504.01818", "authors": ["Sean MacAvaney", "Antonio Mallia", "Nicola Tonellotto"], "title": "Efficient Constant-Space Multi-Vector Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "ECIR 2025", "summary": "Multi-vector retrieval methods, exemplified by the ColBERT architecture, have\nshown substantial promise for retrieval by providing strong trade-offs in terms\nof retrieval latency and effectiveness. However, they come at a high cost in\nterms of storage since a (potentially compressed) vector needs to be stored for\nevery token in the input collection. To overcome this issue, we propose\nencoding documents to a fixed number of vectors, which are no longer\nnecessarily tied to the input tokens. Beyond reducing the storage costs, our\napproach has the advantage that document representations become of a fixed size\non disk, allowing for better OS paging management. Through experiments using\nthe MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a\nrepresentative multi-vector ranking model architecture, we find that passages\ncan be effectively encoded into a fixed number of vectors while retaining most\nof the original effectiveness."}
{"id": "2504.01666", "pdf": "https://arxiv.org/pdf/2504.01666", "abs": "https://arxiv.org/abs/2504.01666", "authors": ["Sarah Alyami", "Hamzah Luqman"], "title": "CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Continuous sign language recognition (CSLR) focuses on interpreting and\ntranscribing sequences of sign language gestures in videos. In this work, we\npropose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that\nleverages the powerful pre-trained visual encoder from the CLIP model to sign\nlanguage tasks through parameter-efficient fine-tuning (PEFT). We introduce two\nvariants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP\nvisual encoder, enabling fine-tuning with minimal trainable parameters. The\neffectiveness of the proposed frameworks is validated on four datasets:\nPhoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA\nvariants outperformed several SOTA models with fewer trainable parameters.\nExtensive ablation studies emphasize the effectiveness and flexibility of the\nproposed methods with different vision-language models for CSLR. These findings\nshowcase the potential of adapting large-scale pre-trained models for scalable\nand efficient CSLR, which pave the way for future advancements in sign language\nunderstanding."}
{"id": "2504.01848", "pdf": "https://arxiv.org/pdf/2504.01848", "abs": "https://arxiv.org/abs/2504.01848", "authors": ["Giulio Starace", "Oliver Jaffe", "Dane Sherburn", "James Aung", "Jun Shern Chan", "Leon Maksin", "Rachel Dias", "Evan Mays", "Benjamin Kinsella", "Wyatt Thompson", "Johannes Heidecke", "Amelia Glaese", "Tejal Patwardhan"], "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research", "categories": ["cs.AI", "cs.CL"], "comment": "30 pages, 14 figures", "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents."}
{"id": "2504.01668", "pdf": "https://arxiv.org/pdf/2504.01668", "abs": "https://arxiv.org/abs/2504.01668", "authors": ["Junjie Chen", "Yuecong Xu", "Haosheng Li", "Kemi Ding"], "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages,6 figures", "summary": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack."}
{"id": "2504.01883", "pdf": "https://arxiv.org/pdf/2504.01883", "abs": "https://arxiv.org/abs/2504.01883", "authors": ["Aashiq Muhamed", "Mona Diab", "Virginia Smith"], "title": "CoRAG: Collaborative Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "NAACL 2024", "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research."}
{"id": "2504.01689", "pdf": "https://arxiv.org/pdf/2504.01689", "abs": "https://arxiv.org/abs/2504.01689", "authors": ["Noam Elata", "Hyungjin Chung", "Jong Chul Ye", "Tomer Michaeli", "Michael Elad"], "title": "InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Models have demonstrated remarkable capabilities in handling\ninverse problems, offering high-quality posterior-sampling-based solutions.\nDespite significant advances, a fundamental trade-off persists, regarding the\nway the conditioned synthesis is employed: Training-based methods achieve high\nquality results, while zero-shot approaches trade this with flexibility. This\nwork introduces a framework that combines the best of both worlds -- the strong\nperformance of supervised approaches and the flexibility of zero-shot methods.\nThis is achieved through a novel architectural design that seamlessly\nintegrates the degradation operator directly into the denoiser. In each block,\nour proposed architecture applies the degradation operator on the network\nactivations and conditions the output using the attention mechanism, enabling\nadaptation to diverse degradation scenarios while maintaining high performance.\nOur work demonstrates the versatility of the proposed architecture, operating\nas a general MMSE estimator, a posterior sampler, or a Neural Posterior\nPrincipal Component estimator. This flexibility enables a wide range of\ndownstream tasks, highlighting the broad applicability of our framework. The\nproposed modification of the denoiser network offers a versatile, accurate, and\ncomputationally efficient solution, demonstrating the advantages of dedicated\nnetwork architectures for complex inverse problems. Experimental results on the\nFFHQ and ImageNet datasets demonstrate state-of-the-art posterior-sampling\nperformance, surpassing both training-based and zero-shot alternatives."}
{"id": "2504.01901", "pdf": "https://arxiv.org/pdf/2504.01901", "abs": "https://arxiv.org/abs/2504.01901", "authors": ["Haochen Wang", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiangyu Zhang", "Zhaoxiang Zhang"], "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data."}
{"id": "2504.01722", "pdf": "https://arxiv.org/pdf/2504.01722", "abs": "https://arxiv.org/abs/2504.01722", "authors": ["Kaan Karaman", "Yuchang Jiang", "Damien Robert", "Vivien Sainte Fare Garnot", "Maria João Santos", "Jan Dirk Wegner"], "title": "{GSR4B}: Biomass Map Super-Resolution with Sentinel-1/2 Guidance", "categories": ["cs.CV"], "comment": "Accepted for an oral presentation at the ISPRS Geospatial Week 2025", "summary": "Accurate Above-Ground Biomass (AGB) mapping at both large scale and high\nspatio-temporal resolution is essential for applications ranging from climate\nmodeling to biodiversity assessment, and sustainable supply chain monitoring.\nAt present, fine-grained AGB mapping relies on costly airborne laser scanning\nacquisition campaigns usually limited to regional scales. Initiatives such as\nthe ESA CCI map attempt to generate global biomass products from diverse\nspaceborne sensors but at a coarser resolution. To enable global,\nhigh-resolution (HR) mapping, several works propose to regress AGB from HR\nsatellite observations such as ESA Sentinel-1/2 images. We propose a novel way\nto address HR AGB estimation, by leveraging both HR satellite observations and\nexisting low-resolution (LR) biomass products. We cast this problem as Guided\nSuper-Resolution (GSR), aiming at upsampling LR biomass maps (sources) from\n$100$ to $10$ m resolution, using auxiliary HR co-registered satellite images\n(guides). We compare super-resolving AGB maps with and without guidance,\nagainst direct regression from satellite images, on the public BioMassters\ndataset. We observe that Multi-Scale Guidance (MSG) outperforms direct\nregression both for regression ($-780$ t/ha RMSE) and perception ($+2.0$ dB\nPSNR) metrics, and better captures high-biomass values, without significant\ncomputational overhead. Interestingly, unlike the RGB+Depth setting they were\noriginally designed for, our best-performing AGB GSR approaches are those that\nmost preserve the guide image texture. Our results make a strong case for\nadopting the GSR framework for accurate HR biomass mapping at scale. Our code\nand model weights are made publicly available\n(https://github.com/kaankaramanofficial/GSR4B)."}
{"id": "2504.01911", "pdf": "https://arxiv.org/pdf/2504.01911", "abs": "https://arxiv.org/abs/2504.01911", "authors": ["Yinggan Xu", "Hana Kimlee", "Yijia Xiao", "Di Luo"], "title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are playing an expanding role in physics\nresearch by enhancing reasoning, symbolic manipulation, and numerical\ncomputation. However, ensuring the reliability and interpretability of their\noutputs remains a significant challenge. In our framework, we conceptualize the\ncollaboration between AI and human scientists as a dynamic interplay among\nthree modules: the reasoning module, the interpretation module, and the\nAI-scientist interaction module. Recognizing that effective physics reasoning\ndemands rigorous logical consistency, quantitative precision, and deep\nintegration with established theoretical models, we introduce the\ninterpretation module to improve the understanding of AI-generated outputs,\nwhich is not previously explored in the literature. This module comprises\nmultiple specialized agents, including summarizers, model builders, UI\nbuilders, and testers, which collaboratively structure LLM outputs within a\nphysically grounded framework, by constructing a more interpretable science\nmodel. A case study demonstrates that our approach enhances transparency,\nfacilitates validation, and strengthens AI-augmented reasoning in scientific\ndiscovery."}
{"id": "2504.01724", "pdf": "https://arxiv.org/pdf/2504.01724", "abs": "https://arxiv.org/abs/2504.01724", "authors": ["Yuxuan Luo", "Zhengkun Rong", "Lizhen Wang", "Longhao Zhang", "Tianshu Hu", "Yongming Zhu"], "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/."}
{"id": "2504.01916", "pdf": "https://arxiv.org/pdf/2504.01916", "abs": "https://arxiv.org/abs/2504.01916", "authors": ["Mothilal Asokan", "Kebin Wu", "Fatima Albreiki"], "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP."}
{"id": "2504.01732", "pdf": "https://arxiv.org/pdf/2504.01732", "abs": "https://arxiv.org/abs/2504.01732", "authors": ["Ulas Gunes", "Matias Turkulainen", "Xuqian Ren", "Arno Solin", "Juho Kannala", "Esa Rahtu"], "title": "FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking", "categories": ["cs.CV"], "comment": "SCIA 2025", "summary": "The development of large-scale 3D scene reconstruction and novel view\nsynthesis methods mostly rely on datasets comprising perspective images with\nnarrow fields of view (FoV). While effective for small-scale scenes, these\ndatasets require large image sets and extensive structure-from-motion (SfM)\nprocessing, limiting scalability. To address this, we introduce a fisheye image\ndataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye\nlenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor\nscenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense\npoint clouds that can be used as geometric ground-truth, enabling robust\nbenchmarking under challenging conditions such as occlusions and reflections.\nWhile the baseline experiments focus on vanilla Gaussian Splatting and NeRF\nbased Nerfacto methods, the dataset supports diverse approaches for scene\nreconstruction, novel view synthesis, and image-based rendering."}
{"id": "2504.01951", "pdf": "https://arxiv.org/pdf/2504.01951", "abs": "https://arxiv.org/abs/2504.01951", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri", "Jacopo Staiano"], "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies."}
{"id": "2504.01735", "pdf": "https://arxiv.org/pdf/2504.01735", "abs": "https://arxiv.org/abs/2504.01735", "authors": ["Chaohu Liu", "Tianyi Gui", "Yu Liu", "Linli Xu"], "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."}
{"id": "2504.01739", "pdf": "https://arxiv.org/pdf/2504.01739", "abs": "https://arxiv.org/abs/2504.01739", "authors": ["Lukas Boehm", "Jonas Leo Mueller", "Christoffer Loeffler", "Leo Schwinn", "Bjoern Eskofier", "Dario Zanca"], "title": "Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the perceptual invariances of artificial neural networks is\nessential for improving explainability and aligning models with human vision.\nMetamers - stimuli that are physically distinct yet produce identical neural\nactivations - serve as a valuable tool for investigating these invariances. We\nintroduce a novel approach to metamer generation by leveraging ensembles of\nartificial neural networks, capturing shared representational subspaces across\ndiverse architectures, including convolutional neural networks and vision\ntransformers. To characterize the properties of the generated metamers, we\nemploy a suite of image-based metrics that assess factors such as semantic\nfidelity and naturalness. Our findings show that convolutional neural networks\ngenerate more recognizable and human-like metamers, while vision transformers\nproduce realistic but less transferable metamers, highlighting the impact of\narchitectural biases on representational invariances."}
{"id": "2504.01755", "pdf": "https://arxiv.org/pdf/2504.01755", "abs": "https://arxiv.org/abs/2504.01755", "authors": ["Xin Su", "Chen Wu", "Zhuoran Zheng"], "title": "Bridge the Gap between SNN and ANN for Image Restoration", "categories": ["cs.CV"], "comment": "Under review", "summary": "Models of dense prediction based on traditional Artificial Neural Networks\n(ANNs) require a lot of energy, especially for image restoration tasks.\nCurrently, neural networks based on the SNN (Spiking Neural Network) framework\nare beginning to make their mark in the field of image restoration, especially\nas they typically use less than 10\\% of the energy of ANNs with the same\narchitecture. However, training an SNN is much more expensive than training an\nANN, due to the use of the heuristic gradient descent strategy. In other words,\nthe process of SNN's potential membrane signal changing from sparse to dense is\nvery slow, which affects the convergence of the whole model.To tackle this\nproblem, we propose a novel distillation technique, called asymmetric framework\n(ANN-SNN) distillation, in which the teacher is an ANN and the student is an\nSNN. Specifically, we leverage the intermediate features (feature maps) learned\nby the ANN as hints to guide the training process of the SNN. This approach not\nonly accelerates the convergence of the SNN but also improves its final\nperformance, effectively bridging the gap between the efficiency of the SNN and\nthe superior learning capabilities of ANN. Extensive experimental results show\nthat our designed SNN-based image restoration model, which has only 1/300 the\nnumber of parameters of the teacher network and 1/50 the energy consumption of\nthe teacher network, is as good as the teacher network in some denoising tasks."}
{"id": "2504.01764", "pdf": "https://arxiv.org/pdf/2504.01764", "abs": "https://arxiv.org/abs/2504.01764", "authors": ["Mingrui Ye", "Lianping Yang", "Hegui Zhu", "Zenghao Zheng", "Xin Wang", "Yantao Lo"], "title": "Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a novel approach to monocular 3D human pose estimation\nusing contextualized representation learning with the Transformer-GCN\ndual-stream model. Monocular 3D human pose estimation is challenged by depth\nambiguity, limited 3D-labeled training data, imbalanced modeling, and\nrestricted model generalization. To address these limitations, our work\nintroduces a groundbreaking motion pre-training method based on contextualized\nrepresentation learning. Specifically, our method involves masking 2D pose\nfeatures and utilizing a Transformer-GCN dual-stream model to learn\nhigh-dimensional representations through a self-distillation setup. By focusing\non contextualized representation learning and spatial-temporal modeling, our\napproach enhances the model's ability to understand spatial-temporal\nrelationships between postures, resulting in superior generalization.\nFurthermore, leveraging the Transformer-GCN dual-stream model, our approach\neffectively balances global and local interactions in video pose estimation.\nThe model adaptively integrates information from both the Transformer and GCN\nstreams, where the GCN stream effectively learns local relationships between\nadjacent key points and frames, while the Transformer stream captures\ncomprehensive global spatial and temporal features. Our model achieves\nstate-of-the-art performance on two benchmark datasets, with an MPJPE of 38.0mm\nand P-MPJPE of 31.9mm on Human3.6M, and an MPJPE of 15.9mm on MPI-INF-3DHP.\nFurthermore, visual experiments on public datasets and in-the-wild videos\ndemonstrate the robustness and generalization capabilities of our approach."}
{"id": "2504.01774", "pdf": "https://arxiv.org/pdf/2504.01774", "abs": "https://arxiv.org/abs/2504.01774", "authors": ["Kegang Wang", "Jiankai Tang", "Yuxuan Fan", "Jiatong Ji", "Yuanchun Shi", "Yuntao Wang"], "title": "Memory-efficient Low-latency Remote Photoplethysmography through Temporal-Spatial State Space Duality", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG), enabling non-contact physiological\nmonitoring through facial light reflection analysis, faces critical\ncomputational bottlenecks as deep learning introduces performance gains at the\ncost of prohibitive resource demands. This paper proposes ME-rPPG, a\nmemory-efficient algorithm built on temporal-spatial state space duality, which\nresolves the trilemma of model scalability, cross-dataset generalization, and\nreal-time constraints. Leveraging a transferable state space, ME-rPPG\nefficiently captures subtle periodic variations across facial frames while\nmaintaining minimal computational overhead, enabling training on extended video\nsequences and supporting low-latency inference. Achieving cross-dataset MAEs of\n5.38 (MMPD), 0.70 (VitalVideo), and 0.25 (PURE), ME-rPPG outperforms all\nbaselines with improvements ranging from 21.3% to 60.2%. Our solution enables\nreal-time inference with only 3.6 MB memory usage and 9.46 ms latency --\nsurpassing existing methods by 19.5%-49.7% accuracy and 43.2% user satisfaction\ngains in real-world deployments. The code and demos are released for\nreproducibility on https://github.com/Health-HCI-Group/ME-rPPG-demo."}
{"id": "2504.01792", "pdf": "https://arxiv.org/pdf/2504.01792", "abs": "https://arxiv.org/abs/2504.01792", "authors": ["Limeng Qiao", "Yiyang Gan", "Bairui Wang", "Jie Qin", "Shuang Xu", "Siqi Yang", "Lin Ma"], "title": "UniViTAR: Unified Vision Transformer with Native Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Conventional Vision Transformer simplifies visual modeling by standardizing\ninput resolutions, often disregarding the variability of natural visual data\nand compromising spatial-contextual fidelity. While preliminary explorations\nhave superficially investigated native resolution modeling, existing approaches\nstill lack systematic analysis from a visual representation perspective. To\nbridge this gap, we introduce UniViTAR, a family of homogeneous vision\nfoundation models tailored for unified visual modality and native resolution\nscenario in the era of multimodal. Our framework first conducts architectural\nupgrades to the vanilla paradigm by integrating multiple advanced components.\nBuilding upon these improvements, a progressive training paradigm is\nintroduced, which strategically combines two core mechanisms: (1) resolution\ncurriculum learning, transitioning from fixed-resolution pretraining to native\nresolution tuning, thereby leveraging ViT's inherent adaptability to\nvariable-length sequences, and (2) visual modality adaptation via inter-batch\nimage-video switching, which balances computational efficiency with enhanced\ntemporal reasoning. In parallel, a hybrid training framework further synergizes\nsigmoid-based contrastive loss with feature distillation from a frozen teacher\nmodel, thereby accelerating early-stage convergence. Finally, trained\nexclusively on public datasets, externsive experiments across multiple model\nscales from 0.3B to 1B demonstrate its effectiveness."}
{"id": "2504.01805", "pdf": "https://arxiv.org/pdf/2504.01805", "abs": "https://arxiv.org/abs/2504.01805", "authors": ["Kun Ouyang"], "title": "Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Enhancing the spatial reasoning capabilities of Multi-modal Large Language\nModels (MLLMs) for video understanding is crucial yet challenging. We present\nSpatial-R1, a targeted approach involving two key contributions: the curation\nof SR, a new video spatial reasoning dataset from ScanNet with automatically\ngenerated QA pairs across seven task types, and the application of\nTask-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By\ntraining the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1\nsignificantly advances performance on the VSI-Bench benchmark, achieving a\n7.4\\% gain over the baseline and outperforming strong contemporary models. This\nwork validates the effectiveness of specialized data curation and optimization\ntechniques for improving complex spatial reasoning in video MLLMs."}
{"id": "2504.01819", "pdf": "https://arxiv.org/pdf/2504.01819", "abs": "https://arxiv.org/abs/2504.01819", "authors": ["Huayang Huang", "Xiangye Jin", "Jiaxu Miao", "Yu Wu"], "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accept to CVPR 2025", "summary": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks."}
{"id": "2504.01838", "pdf": "https://arxiv.org/pdf/2504.01838", "abs": "https://arxiv.org/abs/2504.01838", "authors": ["Nusrat Munia", "Abdullah-Al-Zubaer Imran"], "title": "Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images", "categories": ["cs.CV"], "comment": "Paper accepted at International Symposium on Biomedical Imaging (ISBI\n  2025)", "summary": "Artificial Intelligence (AI) in skin disease diagnosis has improved\nsignificantly, but a major concern is that these models frequently show biased\nperformance across subgroups, especially regarding sensitive attributes such as\nskin color. To address these issues, we propose a novel generative AI-based\nframework, namely, Dermatology Diffusion Transformer (DermDiT), which leverages\ntext prompts generated via Vision Language Models and multimodal text-image\nlearning to generate new dermoscopic images. We utilize large vision language\nmodels to generate accurate and proper prompts for each dermoscopic image which\nhelps to generate synthetic images to improve the representation of\nunderrepresented groups (patient, disease, etc.) in highly imbalanced datasets\nfor clinical diagnoses. Our extensive experimentation showcases the large\nvision language models providing much more insightful representations, that\nenable DermDiT to generate high-quality images. Our code is available at\nhttps://github.com/Munia03/DermDiT"}
{"id": "2504.01844", "pdf": "https://arxiv.org/pdf/2504.01844", "abs": "https://arxiv.org/abs/2504.01844", "authors": ["Stéphane Pateux", "Matthieu Gendrin", "Luce Morin", "Théo Ladune", "Xiaoran Jiang"], "title": "BOGausS: Better Optimized Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view\nsynthesis. Its framework provides fast and high-fidelity rendering. Although\nless complex than other solutions such as Neural Radiance Fields (NeRF), there\nare still some challenges building smaller models without sacrificing quality.\nIn this study, we perform a careful analysis of 3DGS training process and\npropose a new optimization methodology. Our Better Optimized Gaussian Splatting\n(BOGausS) solution is able to generate models up to ten times lighter than the\noriginal 3DGS with no quality degradation, thus significantly boosting the\nperformance of Gaussian Splatting compared to the state of the art."}
{"id": "2504.01872", "pdf": "https://arxiv.org/pdf/2504.01872", "abs": "https://arxiv.org/abs/2504.01872", "authors": ["Jintao Zhang", "Zimin Xia", "Mingyue Dong", "Shuhan Shen", "Linwei Yue", "Xianwei Zheng"], "title": "CoMatcher: Multi-View Collaborative Feature Matching", "categories": ["cs.CV", "I.4.8; I.2.10; I.5.4"], "comment": "15 pages, 7 figures, to be published in CVPR 2025", "summary": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm."}
{"id": "2504.01873", "pdf": "https://arxiv.org/pdf/2504.01873", "abs": "https://arxiv.org/abs/2504.01873", "authors": ["Zheng-Peng Duan", "Jiawei Zhang", "Siyu Liu", "Zheng Lin", "Chun-Le Guo", "Dongqing Zou", "Jimmy Ren", "Chongyi Li"], "title": "A Diffusion-Based Framework for Occluded Object Movement", "categories": ["cs.CV"], "comment": null, "summary": "Seamlessly moving objects within a scene is a common requirement for image\nediting, but it is still a challenge for existing editing methods. Especially\nfor real-world images, the occlusion situation further increases the\ndifficulty. The main difficulty is that the occluded portion needs to be\ncompleted before movement can proceed. To leverage the real-world knowledge\nembedded in the pre-trained diffusion models, we propose a Diffusion-based\nframework specifically designed for Occluded Object Movement, named DiffOOM.\nThe proposed DiffOOM consists of two parallel branches that perform object\nde-occlusion and movement simultaneously. The de-occlusion branch utilizes a\nbackground color-fill strategy and a continuously updated object mask to focus\nthe diffusion process on completing the obscured portion of the target object.\nConcurrently, the movement branch employs latent optimization to place the\ncompleted object in the target location and adopts local text-conditioned\nguidance to integrate the object into new surroundings appropriately. Extensive\nevaluations demonstrate the superior performance of our method, which is\nfurther validated by a comprehensive user study."}
{"id": "2504.01886", "pdf": "https://arxiv.org/pdf/2504.01886", "abs": "https://arxiv.org/abs/2504.01886", "authors": ["Yanzhou Su", "Tianbin Li", "Jiyao Liu", "Chenglong Ma", "Junzhi Ning", "Cheng Tang", "Sibo Ju", "Jin Ye", "Pengcheng Chen", "Ming Hu", "Shixiang Tang", "Lihao Liu", "Bin Fu", "Wenqi Shao", "Xiaowei Hu", "Xiangwen Liao", "Yuanfeng Ji", "Junjun He"], "title": "GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in general medical AI have made significant strides, but\nexisting models often lack the reasoning capabilities needed for complex\nmedical decision-making. This paper presents GMAI-VL-R1, a multimodal medical\nreasoning model enhanced by reinforcement learning (RL) to improve its\nreasoning abilities. Through iterative training, GMAI-VL-R1 optimizes\ndecision-making, significantly boosting diagnostic accuracy and clinical\nsupport. We also develop a reasoning data synthesis method, generating\nstep-by-step reasoning data via rejection sampling, which further enhances the\nmodel's generalization. Experimental results show that after RL training,\nGMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question\nanswering. While the model demonstrates basic memorization with supervised\nfine-tuning, RL is crucial for true generalization. Our work establishes new\nevaluation benchmarks and paves the way for future advancements in medical\nreasoning models. Code, data, and model will be released at\n\\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}."}
{"id": "2504.01890", "pdf": "https://arxiv.org/pdf/2504.01890", "abs": "https://arxiv.org/abs/2504.01890", "authors": ["Shreyank N Gowda", "Boyan Gao", "Xiao Gu", "Xiaobo Jin"], "title": "Is Temporal Prompting All We Need For Limited Labeled Action Recognition?", "categories": ["cs.CV"], "comment": "Accepted in CVPR-W 2025", "summary": "Video understanding has shown remarkable improvements in recent years,\nlargely dependent on the availability of large scaled labeled datasets. Recent\nadvancements in visual-language models, especially based on contrastive\npretraining, have shown remarkable generalization in zero-shot tasks, helping\nto overcome this dependence on labeled datasets. Adaptations of such models for\nvideos, typically involve modifying the architecture of vision-language models\nto cater to video data. However, this is not trivial, since such adaptations\nare mostly computationally intensive and struggle with temporal modeling. We\npresent TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting\nfor temporal adaptation without modifying the core CLIP architecture. This\npreserves its generalization abilities. TP-CLIP efficiently integrates into the\nCLIP architecture, leveraging its pre-trained capabilities for video data.\nExtensive experiments across various datasets demonstrate its efficacy in\nzero-shot and few-shot learning, outperforming existing approaches with fewer\nparameters and computational efficiency. In particular, we use just 1/3 the\nGFLOPs and 1/28 the number of tuneable parameters in comparison to recent\nstate-of-the-art and still outperform it by up to 15.8% depending on the task\nand dataset."}
{"id": "2504.01901", "pdf": "https://arxiv.org/pdf/2504.01901", "abs": "https://arxiv.org/abs/2504.01901", "authors": ["Haochen Wang", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiangyu Zhang", "Zhaoxiang Zhang"], "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "The rapid development of Large Multimodal Models (LMMs) for 2D images and\nvideos has spurred efforts to adapt these models for interpreting 3D scenes.\nHowever, the absence of large-scale 3D vision-language datasets has posed a\nsignificant obstacle. To address this issue, typical approaches focus on\ninjecting 3D awareness into 2D LMMs by designing 3D input-level scene\nrepresentations. This work provides a new perspective. We introduce\nreconstructive visual instruction tuning with 3D-awareness (Ross3D), which\nintegrates 3D-aware visual supervision into the training procedure.\nSpecifically, it incorporates cross-view and global-view reconstruction. The\nformer requires reconstructing masked views by aggregating overlapping\ninformation from other views. The latter aims to aggregate information from all\navailable views to recover Bird's-Eye-View images, contributing to a\ncomprehensive overview of the entire scene. Empirically, Ross3D achieves\nstate-of-the-art performance across various 3D scene understanding benchmarks.\nMore importantly, our semi-supervised experiments demonstrate significant\npotential in leveraging large amounts of unlabeled 3D vision-only data."}
{"id": "2504.01916", "pdf": "https://arxiv.org/pdf/2504.01916", "abs": "https://arxiv.org/abs/2504.01916", "authors": ["Mothilal Asokan", "Kebin Wu", "Fatima Albreiki"], "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP."}
{"id": "2504.01925", "pdf": "https://arxiv.org/pdf/2504.01925", "abs": "https://arxiv.org/abs/2504.01925", "authors": ["Haykel Snoussi", "Davood Karimi"], "title": "Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early and accurate assessment of brain microstructure using diffusion\nMagnetic Resonance Imaging (dMRI) is crucial for identifying neurodevelopmental\ndisorders in neonates, but remains challenging due to low signal-to-noise ratio\n(SNR), motion artifacts, and ongoing myelination. In this study, we propose a\nrotationally equivariant Spherical Convolutional Neural Network (sCNN)\nframework tailored for neonatal dMRI. We predict the Fiber Orientation\nDistribution (FOD) from multi-shell dMRI signals acquired with a reduced set of\ngradient directions (30% of the full protocol), enabling faster and more\ncost-effective acquisitions. We train and evaluate the performance of our sCNN\nusing real data from 43 neonatal dMRI datasets provided by the Developing Human\nConnectome Project (dHCP). Our results demonstrate that the sCNN achieves\nsignificantly lower mean squared error (MSE) and higher angular correlation\ncoefficient (ACC) compared to a Multi-Layer Perceptron (MLP) baseline,\nindicating improved accuracy in FOD estimation. Furthermore, tractography\nresults based on the sCNN-predicted FODs show improved anatomical plausibility,\ncoverage, and coherence compared to those from the MLP. These findings\nhighlight that sCNNs, with their inherent rotational equivariance, offer a\npromising approach for accurate and clinically efficient dMRI analysis, paving\nthe way for improved diagnostic capabilities and characterization of early\nbrain development."}
{"id": "2504.01934", "pdf": "https://arxiv.org/pdf/2504.01934", "abs": "https://arxiv.org/abs/2504.01934", "authors": ["Runhui Huang", "Chunwei Wang", "Junwei Yang", "Guansong Lu", "Yunlong Yuan", "Jianhua Han", "Lu Hou", "Wei Zhang", "Lanqing Hong", "Hengshuang Zhao", "Hang Xu"], "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement", "categories": ["cs.CV"], "comment": null, "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/."}
{"id": "2504.01941", "pdf": "https://arxiv.org/pdf/2504.01941", "abs": "https://arxiv.org/abs/2504.01941", "authors": ["Yingyan Li", "Yuqi Wang", "Yang Liu", "Jiawei He", "Lue Fan", "Zhaoxiang Zhang"], "title": "End-to-End Driving with Online Trajectory Evaluation via BEV World Model", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end autonomous driving has achieved remarkable progress by integrating\nperception, prediction, and planning into a fully differentiable framework.\nYet, to fully realize its potential, an effective online trajectory evaluation\nis indispensable to ensure safety. By forecasting the future outcomes of a\ngiven trajectory, trajectory evaluation becomes much more effective. This goal\ncan be achieved by employing a world model to capture environmental dynamics\nand predict future states. Therefore, we propose an end-to-end driving\nframework WoTE, which leverages a BEV World model to predict future BEV states\nfor Trajectory Evaluation. The proposed BEV world model is latency-efficient\ncompared to image-level world models and can be seamlessly supervised using\noff-the-shelf BEV-space traffic simulators. We validate our framework on both\nthe NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the\nCARLA simulator, achieving state-of-the-art performance. Code is released at\nhttps://github.com/liyingyanUCAS/WoTE."}
{"id": "2504.01952", "pdf": "https://arxiv.org/pdf/2504.01952", "abs": "https://arxiv.org/abs/2504.01952", "authors": ["Wenxuan Wang", "Zijia Zhao", "Yisi Zhang", "Yepeng Tang", "Erdong Hu", "Xinlong Wang", "Jing Liu"], "title": "Image Difference Grounding with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding (VG) typically focuses on locating regions of interest\nwithin an image using natural language, and most existing VG methods are\nlimited to single-image interpretations. This limits their applicability in\nreal-world scenarios like automatic surveillance, where detecting subtle but\nmeaningful visual differences across multiple images is crucial. Besides,\nprevious work on image difference understanding (IDU) has either focused on\ndetecting all change regions without cross-modal text guidance, or on providing\ncoarse-grained descriptions of differences. Therefore, to push towards\nfiner-grained vision-language perception, we propose Image Difference Grounding\n(IDG), a task designed to precisely localize visual differences based on user\ninstructions. We introduce DiffGround, a large-scale and high-quality dataset\nfor IDG, containing image pairs with diverse visual variations along with\ninstructions querying fine-grained differences. Besides, we present a baseline\nmodel for IDG, DiffTracker, which effectively integrates feature differential\nenhancement and common suppression to precisely locate differences. Experiments\non the DiffGround dataset highlight the importance of our IDG dataset in\nenabling finer-grained IDU. To foster future research, both DiffGround data and\nDiffTracker model will be publicly released."}
{"id": "2504.01953", "pdf": "https://arxiv.org/pdf/2504.01953", "abs": "https://arxiv.org/abs/2504.01953", "authors": ["Mohini Anand", "Xavier Tricoche"], "title": "Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "10 pages, 5 figures. Submitted to MICCAI 2025 (under review)", "summary": "Understanding the complex myocardial architecture is critical for diagnosing\nand treating heart disease. However, existing methods often struggle to\naccurately capture this intricate structure from Diffusion Tensor Imaging (DTI)\ndata, particularly due to the lack of ground truth labels and the ambiguous,\nintertwined nature of fiber trajectories. We present a novel deep learning\nframework for unsupervised clustering of myocardial fibers, providing a\ndata-driven approach to identifying distinct fiber bundles. We uniquely combine\na Bidirectional Long Short-Term Memory network to capture local sequential\ninformation along fibers, with a Transformer autoencoder to learn global shape\nfeatures, with pointwise incorporation of essential anatomical context.\nClustering these representations using a density-based algorithm identifies 33\nto 62 robust clusters, successfully capturing the subtle distinctions in fiber\ntrajectories with varying levels of granularity. Our framework offers a new,\nflexible, and quantitative way to analyze myocardial structure, achieving a\nlevel of delineation that, to our knowledge, has not been previously achieved,\nwith potential applications in improving surgical planning, characterizing\ndisease-related remodeling, and ultimately, advancing personalized cardiac\ncare."}
{"id": "2504.01954", "pdf": "https://arxiv.org/pdf/2504.01954", "abs": "https://arxiv.org/abs/2504.01954", "authors": ["Jing Liu", "Wenxuan Wang", "Yisi Zhang", "Yepeng Tang", "Xingjian He", "Longteng Guo", "Tongtian Yue", "Xinlong Wang"], "title": "Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities", "categories": ["cs.CV"], "comment": null, "summary": "Referring expression segmentation (RES) aims at segmenting the entities'\nmasks that match the descriptive language expression. While traditional RES\nmethods primarily address object-level grounding, real-world scenarios demand a\nmore versatile framework that can handle multiple levels of target granularity,\nsuch as multi-object, single object or part-level references. This introduces\ngreat challenges due to the diverse and nuanced ways users describe targets.\nHowever, existing datasets and models mainly focus on designing grounding\nspecialists for object-level target localization, lacking the necessary data\nresources and unified frameworks for the more practical multi-grained RES. In\nthis paper, we take a step further towards visual granularity unified RES task.\nTo overcome the limitation of data scarcity, we introduce a new\nmulti-granularity referring expression segmentation (MRES) task, alongside the\nRefCOCOm benchmark, which includes part-level annotations for advancing\nfiner-grained visual understanding. In addition, we create MRES-32M, the\nlargest visual grounding dataset, comprising over 32.2M masks and captions\nacross 1M images, specifically designed for part-level vision-language\ngrounding. To tackle the challenges of multi-granularity RES, we propose\nUniRES++, a unified multimodal large language model that integrates\nobject-level and part-level RES tasks. UniRES++ incorporates targeted designs\nfor fine-grained visual feature exploration. With the joint model architecture\nand parameters, UniRES++ achieves state-of-the-art performance across multiple\nbenchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and\nRefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into\nmulti-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and\nmodel UniRES++ will be publicly available at\nhttps://github.com/Rubics-Xuan/MRES."}
{"id": "2504.01955", "pdf": "https://arxiv.org/pdf/2504.01955", "abs": "https://arxiv.org/abs/2504.01955", "authors": ["Oliver Hahn", "Christoph Reich", "Nikita Araslanov", "Daniel Cremers", "Christian Rupprecht", "Stefan Roth"], "title": "Scene-Centric Unsupervised Panoptic Segmentation", "categories": ["cs.CV"], "comment": "To appear at CVPR 2025. Christoph Reich and Oliver Hahn - both\n  authors contributed equally. Code: https://github.com/visinf/cups Project\n  page: https://visinf.github.io/cups/", "summary": "Unsupervised panoptic segmentation aims to partition an image into\nsemantically meaningful regions and distinct object instances without training\non manually annotated data. In contrast to prior work on unsupervised panoptic\nscene understanding, we eliminate the need for object-centric training data,\nenabling the unsupervised understanding of complex scenes. To that end, we\npresent the first unsupervised panoptic method that directly trains on\nscene-centric imagery. In particular, we propose an approach to obtain\nhigh-resolution panoptic pseudo labels on complex scene-centric data, combining\nvisual representations, depth, and motion cues. Utilizing both pseudo-label\ntraining and a panoptic self-training strategy yields a novel approach that\naccurately predicts panoptic segmentation of complex scenes without requiring\nany human annotations. Our approach significantly improves panoptic quality,\ne.g., surpassing the recent state of the art in unsupervised panoptic\nsegmentation on Cityscapes by 9.4% points in PQ."}
{"id": "2504.01956", "pdf": "https://arxiv.org/pdf/2504.01956", "abs": "https://arxiv.org/abs/2504.01956", "authors": ["Hanyang Wang", "Fangfu Liu", "Jiawei Chi", "Yueqi Duan"], "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step", "categories": ["cs.CV"], "comment": "Project Page: https://hanyang-21.github.io/VideoScene", "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene"}
{"id": "2504.01957", "pdf": "https://arxiv.org/pdf/2504.01957", "abs": "https://arxiv.org/abs/2504.01957", "authors": ["Shu-Wei Lu", "Yi-Hsuan Tsai", "Yi-Ting Chen"], "title": "GaussianLSS -- Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Bird's-eye view (BEV) perception has gained significant attention because it\nprovides a unified representation to fuse multiple view images and enables a\nwide range of down-stream autonomous driving tasks, such as forecasting and\nplanning. Recent state-of-the-art models utilize projection-based methods which\nformulate BEV perception as query learning to bypass explicit depth estimation.\nWhile we observe promising advancements in this paradigm, they still fall short\nof real-world applications because of the lack of uncertainty modeling and\nexpensive computational requirement. In this work, we introduce GaussianLSS, a\nnovel uncertainty-aware BEV perception framework that revisits\nunprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm,\nand enhances them with depth un-certainty modeling. GaussianLSS represents\nspatial dispersion by learning a soft depth mean and computing the variance of\nthe depth distribution, which implicitly captures object extents. We then\ntransform the depth distribution into 3D Gaussians and rasterize them to\nconstruct uncertainty-aware BEV features. We evaluate GaussianLSS on the\nnuScenes dataset, achieving state-of-the-art performance compared to\nunprojection-based methods. In particular, it provides significant advantages\nin speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory\ncompared to projection-based methods, while achieving competitive performance\nwith only a 0.4% IoU difference."}
{"id": "2504.01960", "pdf": "https://arxiv.org/pdf/2504.01960", "abs": "https://arxiv.org/abs/2504.01960", "authors": ["Niluthpol Chowdhury Mithun", "Tuan Pham", "Qiao Wang", "Ben Southall", "Kshitij Minhas", "Bogdan Matei", "Stephan Mandt", "Supun Samarasekera", "Rakesh Kumar"], "title": "Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "WACV ULTRRA Workshop 2025", "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance\nFields (NeRF) have achieved impressive results in real-time 3D reconstruction\nand novel view synthesis. However, these methods struggle in large-scale,\nunconstrained environments where sparse and uneven input coverage, transient\nocclusions, appearance variability, and inconsistent camera settings lead to\ndegraded quality. We propose GS-Diff, a novel 3DGS framework guided by a\nmulti-view diffusion model to address these limitations. By generating\npseudo-observations conditioned on multi-view inputs, our method transforms\nunder-constrained 3D reconstruction problems into well-posed ones, enabling\nrobust optimization even with sparse data. GS-Diff further integrates several\nenhancements, including appearance embedding, monocular depth priors, dynamic\nobject modeling, anisotropy regularization, and advanced rasterization\ntechniques, to tackle geometric and photometric challenges in real-world\nsettings. Experiments on four benchmarks demonstrate that GS-Diff consistently\noutperforms state-of-the-art baselines by significant margins."}
{"id": "2504.01961", "pdf": "https://arxiv.org/pdf/2504.01961", "abs": "https://arxiv.org/abs/2504.01961", "authors": ["Tengda Han", "Dilara Gokay", "Joseph Heyward", "Chuhan Zhang", "Daniel Zoran", "Viorica Pătrăucean", "João Carreira", "Dima Damen", "Andrew Zisserman"], "title": "Learning from Streaming Video with Orthogonal Gradients", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "We address the challenge of representation learning from a continuous stream\nof video as input, in a self-supervised manner. This differs from the standard\napproaches to video learning where videos are chopped and shuffled during\ntraining in order to create a non-redundant batch that satisfies the\nindependently and identically distributed (IID) sample assumption expected by\nconventional training paradigms. When videos are only available as a continuous\nstream of input, the IID assumption is evidently broken, leading to poor\nperformance. We demonstrate the drop in performance when moving from shuffled\nto sequential learning on three tasks: the one-video representation learning\nmethod DoRA, standard VideoMAE on multi-video datasets, and the task of future\nvideo prediction. To address this drop, we propose a geometric modification to\nstandard optimizers, to decorrelate batches by utilising orthogonal gradients\nduring training. The proposed modification can be applied to any optimizer --\nwe demonstrate it with Stochastic Gradient Descent (SGD) and AdamW. Our\nproposed orthogonal optimizer allows models trained from streaming videos to\nalleviate the drop in representation learning performance, as evaluated on\ndownstream tasks. On three scenarios (DoRA, VideoMAE, future prediction), we\nshow our orthogonal optimizer outperforms the strong AdamW in all three\nscenarios."}
{"id": "2504.01025", "pdf": "https://arxiv.org/pdf/2504.01025", "abs": "https://arxiv.org/abs/2504.01025", "authors": ["Fubao Zhu", "Yang Zhang", "Gengmin Liang", "Jiaofen Nan", "Yanting Li", "Chuang Han", "Danyang Sun", "Zhiguo Wang", "Chen Zhao", "Wenxuan Zhou", "Jian He", "Yi Xu", "Iokfai Cheang", "Xu Zhu", "Yanli Zhou", "Weihua Zhou"], "title": "Diagnosis of Pulmonary Hypertension by Integrating Multimodal Data with a Hybrid Graph Convolutional and Transformer Network", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "23 pages, 8 figures, 4 tables", "summary": "Early and accurate diagnosis of pulmonary hypertension (PH) is essential for\noptimal patient management. Differentiating between pre-capillary and\npost-capillary PH is critical for guiding treatment decisions. This study\ndevelops and validates a deep learning-based diagnostic model for PH, designed\nto classify patients as non-PH, pre-capillary PH, or post-capillary PH. This\nretrospective study analyzed data from 204 patients (112 with pre-capillary PH,\n32 with post-capillary PH, and 60 non-PH controls) at the First Affiliated\nHospital of Nanjing Medical University. Diagnoses were confirmed through right\nheart catheterization. We selected 6 samples from each category for the test\nset (18 samples, 10%), with the remaining 186 samples used for the training\nset. This process was repeated 35 times for testing. This paper proposes a deep\nlearning model that combines Graph convolutional networks (GCN), Convolutional\nneural networks (CNN), and Transformers. The model was developed to process\nmultimodal data, including short-axis (SAX) sequences, four-chamber (4CH)\nsequences, and clinical parameters. Our model achieved a performance of Area\nunder the receiver operating characteristic curve (AUC) = 0.81 +- 0.06(standard\ndeviation) and Accuracy (ACC) = 0.73 +- 0.06 on the test set. The\ndiscriminative abilities were as follows: non-PH subjects (AUC = 0.74 +- 0.11),\npre-capillary PH (AUC = 0.86 +- 0.06), and post-capillary PH (AUC = 0.83 +-\n0.10). It has the potential to support clinical decision-making by effectively\nintegrating multimodal data to assist physicians in making accurate and timely\ndiagnoses."}
{"id": "2504.01027", "pdf": "https://arxiv.org/pdf/2504.01027", "abs": "https://arxiv.org/abs/2504.01027", "authors": ["Sai Karthikey Pentapati", "Gregoire Phillips", "Alan C. Bovik"], "title": "Mesh Compression with Quantized Neural Displacement Fields", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Implicit neural representations (INRs) have been successfully used to\ncompress a variety of 3D surface representations such as Signed Distance\nFunctions (SDFs), voxel grids, and also other forms of structured data such as\nimages, videos, and audio. However, these methods have been limited in their\napplication to unstructured data such as 3D meshes and point clouds. This work\npresents a simple yet effective method that extends the usage of INRs to\ncompress 3D triangle meshes. Our method encodes a displacement field that\nrefines the coarse version of the 3D mesh surface to be compressed using a\nsmall neural network. Once trained, the neural network weights occupy much\nlower memory than the displacement field or the original surface. We show that\nour method is capable of preserving intricate geometric textures and\ndemonstrates state-of-the-art performance for compression ratios ranging from\n4x to 380x."}
{"id": "2504.01035", "pdf": "https://arxiv.org/pdf/2504.01035", "abs": "https://arxiv.org/abs/2504.01035", "authors": ["Loc Hoang Tran", "Luong Anh Tuan Nguyen"], "title": "Novel sparse PCA method via Runge Kutta numerical method(s) for face recognition", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "3 tables", "summary": "Face recognition is a crucial topic in data science and biometric security,\nwith applications spanning military, finance, and retail industries. This paper\nexplores the implementation of sparse Principal Component Analysis (PCA) using\nthe Proximal Gradient method (also known as ISTA) and the Runge-Kutta numerical\nmethods. To address the face recognition problem, we integrate sparse PCA with\neither the k-nearest neighbor method or the kernel ridge regression method.\nExperimental results demonstrate that combining sparse PCA-solved via the\nProximal Gradient method or the Runge-Kutta numerical approach-with a\nclassification system yields higher accuracy compared to standard PCA.\nAdditionally, we observe that the Runge-Kutta-based sparse PCA computation\nconsistently outperforms the Proximal Gradient method in terms of speed."}
{"id": "2504.01038", "pdf": "https://arxiv.org/pdf/2504.01038", "abs": "https://arxiv.org/abs/2504.01038", "authors": ["Xian-Xian Liu", "Yuanyuan Wei", "Mingkun Xu", "Yongze Guo", "Hongwei Zhang", "Huicong Dong", "Qun Song", "Qi Zhao", "Wei Luo", "Feng Tien", "Juntao Gao", "Simon Fong"], "title": "An Integrated AI-Enabled System Using One Class Twin Cross Learning (OCT-X) for Early Gastric Cancer Detection", "categories": ["eess.IV", "cs.CV", "cs.HC"], "comment": "26 pages, 4 figures, 6 tables", "summary": "Early detection of gastric cancer, a leading cause of cancer-related\nmortality worldwide, remains hampered by the limitations of current diagnostic\ntechnologies, leading to high rates of misdiagnosis and missed diagnoses. To\naddress these challenges, we propose an integrated system that synergizes\nadvanced hardware and software technologies to balance speed-accuracy. Our\nstudy introduces the One Class Twin Cross Learning (OCT-X) algorithm.\nLeveraging a novel fast double-threshold grid search strategy (FDT-GS) and a\npatch-based deep fully convolutional network, OCT-X maximizes diagnostic\naccuracy through real-time data processing and seamless lesion surveillance.\nThe hardware component includes an all-in-one point-of-care testing (POCT)\ndevice with high-resolution imaging sensors, real-time data processing, and\nwireless connectivity, facilitated by the NI CompactDAQ and LabVIEW software.\nOur integrated system achieved an unprecedented diagnostic accuracy of 99.70%,\nsignificantly outperforming existing models by up to 4.47%, and demonstrated a\n10% improvement in multirate adaptability. These findings underscore the\npotential of OCT-X as well as the integrated system in clinical diagnostics,\noffering a path toward more accurate, efficient, and less invasive early\ngastric cancer detection. Future research will explore broader applications,\nfurther advancing oncological diagnostics. Code is available at\nhttps://github.com/liu37972/Multirate-Location-on-OCT-X-Learning.git."}
{"id": "2504.01204", "pdf": "https://arxiv.org/pdf/2504.01204", "abs": "https://arxiv.org/abs/2504.01204", "authors": ["Xuan Li", "Qianli Ma", "Tsung-Yi Lin", "Yongxin Chen", "Chenfanfu Jiang", "Ming-Yu Liu", "Donglai Xiang"], "title": "Articulated Kinematics Distillation from Video Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/"}
{"id": "2504.01208", "pdf": "https://arxiv.org/pdf/2504.01208", "abs": "https://arxiv.org/abs/2504.01208", "authors": ["Ian Mateos Gonzalez", "Estefani Jaramilla Nava", "Abraham Sánchez Morales", "Jesús García-Ramírez", "Ricardo Ramos-Aguilar"], "title": "Lightweight Deep Models for Dermatological Disease Detection: A Study on Instance Selection and Channel Optimization", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Submitted to Mexican Conference on Pattern Recognition 2025", "summary": "The identification of dermatological disease is an important problem in\nMexico according with different studies. Several works in literature use the\ndatasets of different repositories without applying a study of the data\nbehavior, especially in medical images domain. In this work, we propose a\nmethodology to preprocess dermaMNIST dataset in order to improve its quality\nfor the classification stage, where we use lightweight convolutional neural\nnetworks. In our results, we reduce the number of instances for the neural\nnetwork training obtaining a similar performance of models as ResNet."}
{"id": "2504.01218", "pdf": "https://arxiv.org/pdf/2504.01218", "abs": "https://arxiv.org/abs/2504.01218", "authors": ["Piyush Nagasubramaniam", "Neeraj Karamchandani", "Chen Wu", "Sencun Zhu"], "title": "Prompting Forgetting: Unlearning in GANs via Textual Guidance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "State-of-the-art generative models exhibit powerful image-generation\ncapabilities, introducing various ethical and legal challenges to service\nproviders hosting these models. Consequently, Content Removal Techniques (CRTs)\nhave emerged as a growing area of research to control outputs without\nfull-scale retraining. Recent work has explored the use of Machine Unlearning\nin generative models to address content removal. However, the focus of such\nresearch has been on diffusion models, and unlearning in Generative Adversarial\nNetworks (GANs) has remained largely unexplored. We address this gap by\nproposing Text-to-Unlearn, a novel framework that selectively unlearns concepts\nfrom pre-trained GANs using only text prompts, enabling feature unlearning,\nidentity unlearning, and fine-grained tasks like expression and multi-attribute\nremoval in models trained on human faces. Leveraging natural language\ndescriptions, our approach guides the unlearning process without requiring\nadditional datasets or supervised fine-tuning, offering a scalable and\nefficient solution. To evaluate its effectiveness, we introduce an automatic\nunlearning assessment method adapted from state-of-the-art image-text alignment\nmetrics, providing a comprehensive analysis of the unlearning methodology. To\nour knowledge, Text-to-Unlearn is the first cross-modal unlearning framework\nfor GANs, representing a flexible and efficient advancement in managing\ngenerative model behavior."}
{"id": "2504.01225", "pdf": "https://arxiv.org/pdf/2504.01225", "abs": "https://arxiv.org/abs/2504.01225", "authors": ["Gonçalo Gomes", "Chrysoula Zerva", "Bruno Martins"], "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessment for individual\nword misalignments within captions, and the reliance on single-point quality\nestimates without considering uncertainty. To address these limitations, we\npropose a simple yet effective strategy for generating and calibrating\nCLIPScore distributions. Leveraging a model-agnostic conformal risk control\nframework, we calibrate CLIPScore values for task-specific control variables,\nto tackle the aforementioned two limitations. Experimental results demonstrate\nthat using conformal risk control, over the distributions produced with simple\nmethods such as input masking, can achieve competitive performance compared to\nmore complex approaches. Our method effectively detects misaligned words, while\nproviding formal guarantees aligned with desired risk levels, and improving the\ncorrelation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics."}
{"id": "2504.01261", "pdf": "https://arxiv.org/pdf/2504.01261", "abs": "https://arxiv.org/abs/2504.01261", "authors": ["Thomas Pritchard", "Saifullah Ijaz", "Ronald Clark", "Basaran Bahadir Kocer"], "title": "ForestVO: Enhancing Visual Odometry in Forest Environments through ForestGlue", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to the IEEE Robotics and Automation Letters", "summary": "Recent advancements in visual odometry systems have improved autonomous\nnavigation; however, challenges persist in complex environments like forests,\nwhere dense foliage, variable lighting, and repetitive textures compromise\nfeature correspondence accuracy. To address these challenges, we introduce\nForestGlue, enhancing the SuperPoint feature detector through four\nconfigurations - grayscale, RGB, RGB-D, and stereo-vision - optimised for\nvarious sensing modalities. For feature matching, we employ LightGlue or\nSuperGlue, retrained with synthetic forest data. ForestGlue achieves comparable\npose estimation accuracy to baseline models but requires only 512 keypoints -\njust 25% of the baseline's 2048 - to reach an LO-RANSAC AUC score of 0.745 at a\n10{\\deg} threshold. With only a quarter of keypoints needed, ForestGlue\nsignificantly reduces computational overhead, demonstrating effectiveness in\ndynamic forest environments, and making it suitable for real-time deployment on\nresource-constrained platforms. By combining ForestGlue with a\ntransformer-based pose estimation model, we propose ForestVO, which estimates\nrelative camera poses using matched 2D pixel coordinates between frames. On\nchallenging TartanAir forest sequences, ForestVO achieves an average relative\npose error (RPE) of 1.09 m and a kitti_score of 2.33%, outperforming\ndirect-based methods like DSO by 40% in dynamic scenes. Despite using only 10%\nof the dataset for training, ForestVO maintains competitive performance with\nTartanVO while being a significantly lighter model. This work establishes an\nend-to-end deep learning pipeline specifically tailored for visual odometry in\nforested environments, leveraging forest-specific training data to optimise\nfeature correspondence and pose estimation, thereby enhancing the accuracy and\nrobustness of autonomous navigation systems."}
{"id": "2504.01274", "pdf": "https://arxiv.org/pdf/2504.01274", "abs": "https://arxiv.org/abs/2504.01274", "authors": ["Boseong Kim", "Debashis Das Chakladar", "Haejun Chung", "Ikbeom Jang"], "title": "BOLDSimNet: Examining Brain Network Similarity between Task and Resting-State fMRI", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "Traditional causal connectivity methods in task-based and resting-state\nfunctional magnetic resonance imaging (fMRI) face challenges in accurately\ncapturing directed information flow due to their sensitivity to noise and\ninability to model multivariate dependencies. These limitations hinder the\neffective comparison of brain networks between cognitive states, making it\ndifficult to analyze network reconfiguration during task and resting states. To\naddress these issues, we propose BOLDSimNet, a novel framework utilizing\nMultivariate Transfer Entropy (MTE) to measure causal connectivity and network\nsimilarity across different cognitive states. Our method groups functionally\nsimilar regions of interest (ROIs) rather than spatially adjacent nodes,\nimproving accuracy in network alignment. We applied BOLDSimNet to fMRI data\nfrom 40 healthy controls and found that children exhibited higher similarity\nscores between task and resting states compared to adolescents, indicating\nreduced variability in attention shifts. In contrast, adolescents showed more\ndifferences between task and resting states in the Dorsal Attention Network\n(DAN) and the Default Mode Network (DMN), reflecting enhanced network\nadaptability. These findings emphasize developmental variations in the\nreconfiguration of the causal brain network, showcasing BOLDSimNet's ability to\nquantify network similarity and identify attentional fluctuations between\ndifferent cognitive states."}
{"id": "2504.01358", "pdf": "https://arxiv.org/pdf/2504.01358", "abs": "https://arxiv.org/abs/2504.01358", "authors": ["Zirui Wu", "Jianteng Chen", "Laijian Li", "Shaoteng Wu", "Zhikai Zhu", "Kang Xu", "Martin R. Oswald", "Jie Song"], "title": "3D Gaussian Inverse Rendering with Approximated Global Illumination", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting shows great potential in reconstructing photo-realistic\n3D scenes. However, these methods typically bake illumination into their\nrepresentations, limiting their use for physically-based rendering and scene\nediting. Although recent inverse rendering approaches aim to decompose scenes\ninto material and lighting components, they often rely on simplifying\nassumptions that fail when editing. We present a novel approach that enables\nefficient global illumination for 3D Gaussians Splatting through screen-space\nray tracing. Our key insight is that a substantial amount of indirect light can\nbe traced back to surfaces visible within the current view frustum. Leveraging\nthis observation, we augment the direct shading computed by 3D Gaussians with\nMonte-Carlo screen-space ray-tracing to capture one-bounce indirect\nillumination. In this way, our method enables realistic global illumination\nwithout sacrificing the computational efficiency and editability benefits of 3D\nGaussians. Through experiments, we show that the screen-space approximation we\nutilize allows for indirect illumination and supports real-time rendering and\nediting. Code, data, and models will be made available at our project page:\nhttps://wuzirui.github.io/gs-ssr."}
{"id": "2504.01483", "pdf": "https://arxiv.org/pdf/2504.01483", "abs": "https://arxiv.org/abs/2504.01483", "authors": ["Siran Li", "Ruiyang Liu", "Chen Liu", "Zhendong Wang", "Gaofeng He", "Yong-Lu Li", "Xiaogang Jin", "Huamin Wang"], "title": "GarmageNet: A Dataset and Scalable Representation for Generic Garment Modeling", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "High-fidelity garment modeling remains challenging due to the lack of\nlarge-scale, high-quality datasets and efficient representations capable of\nhandling non-watertight, multi-layer geometries. In this work, we introduce\nGarmage, a neural-network-and-CG-friendly garment representation that\nseamlessly encodes the accurate geometry and sewing pattern of complex\nmulti-layered garments as a structured set of per-panel geometry images. As a\ndual-2D-3D representation, Garmage achieves an unprecedented integration of 2D\nimage-based algorithms with 3D modeling workflows, enabling high fidelity,\nnon-watertight, multi-layered garment geometries with direct compatibility for\nindustrial-grade simulations.Built upon this representation, we present\nGarmageNet, a novel generation framework capable of producing detailed\nmulti-layered garments with body-conforming initial geometries and intricate\nsewing patterns, based on user prompts or existing in-the-wild sewing patterns.\nFurthermore, we introduce a robust stitching algorithm that recovers per-vertex\nstitches, ensuring seamless integration into flexible simulation pipelines for\ndownstream editing of sewing patterns, material properties, and dynamic\nsimulations. Finally, we release an industrial-standard, large-scale,\nhigh-fidelity garment dataset featuring detailed annotations, vertex-wise\ncorrespondences, and a robust pipeline for converting unstructured production\nsewing patterns into GarmageNet standard structural assets, paving the way for\nlarge-scale, industrial-grade garment generation systems."}
{"id": "2504.01521", "pdf": "https://arxiv.org/pdf/2504.01521", "abs": "https://arxiv.org/abs/2504.01521", "authors": ["Jincheng Zhong", "Xiangcheng Zhang", "Jianmin Wang", "Mingsheng Long"], "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion models have revolutionized generative\nmodeling. However, the impressive and vivid outputs they produce often come at\nthe cost of significant model scaling and increased computational demands.\nConsequently, building personalized diffusion models based on off-the-shelf\nmodels has emerged as an appealing alternative. In this paper, we introduce a\nnovel perspective on conditional generation for transferring a pre-trained\nmodel. From this viewpoint, we propose *Domain Guidance*, a straightforward\ntransfer approach that leverages pre-trained knowledge to guide the sampling\nprocess toward the target domain. Domain Guidance shares a formulation similar\nto advanced classifier-free guidance, facilitating better domain alignment and\nhigher-quality generations. We provide both empirical and theoretical analyses\nof the mechanisms behind Domain Guidance. Our experimental results demonstrate\nits substantial effectiveness across various transfer benchmarks, achieving\nover a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$\ncompared to standard fine-tuning. Notably, existing fine-tuned models can\nseamlessly integrate Domain Guidance to leverage these benefits, without\nadditional training."}
{"id": "2504.01561", "pdf": "https://arxiv.org/pdf/2504.01561", "abs": "https://arxiv.org/abs/2504.01561", "authors": ["Dandan Shan", "Zihan Li", "Yunxiang Li", "Qingde Li", "Jie Tian", "Qingqi Hong"], "title": "STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of lesions plays a critical role in medical image\nanalysis and diagnosis. Traditional segmentation approaches that rely solely on\nvisual features often struggle with the inherent uncertainty in lesion\ndistribution and size. To address these issues, we propose STPNet, a\nScale-aware Text Prompt Network that leverages vision-language modeling to\nenhance medical image segmentation. Our approach utilizes multi-scale textual\ndescriptions to guide lesion localization and employs retrieval-segmentation\njoint learning to bridge the semantic gap between visual and linguistic\nmodalities. Crucially, STPNet retrieves relevant textual information from a\nspecialized medical text repository during training, eliminating the need for\ntext input during inference while retaining the benefits of cross-modal\nlearning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and\nKvasir-SEG. Experimental results show that our vision-language approach\noutperforms state-of-the-art segmentation methods, demonstrating the\neffectiveness of incorporating textual semantic knowledge into medical image\nanalysis. The code has been made publicly on\nhttps://github.com/HUANGLIZI/STPNet."}
{"id": "2504.01571", "pdf": "https://arxiv.org/pdf/2504.01571", "abs": "https://arxiv.org/abs/2504.01571", "authors": ["Aleksander Plocharski", "Jan Swidzinski", "Przemyslaw Musialski"], "title": "Pro-DG: Procedural Diffusion Guidance for Architectural Facade Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "I.3.7; I.4.9; I.2.10"], "comment": "12 pages, 13 figures", "summary": "We present Pro-DG, a framework for procedurally controllable photo-realistic\nfacade generation that combines a procedural shape grammar with diffusion-based\nimage synthesis. Starting from a single input image, we reconstruct its facade\nlayout using grammar rules, then edit that structure through user-defined\ntransformations. As facades are inherently multi-hierarchical structures, we\nintroduce hierarchical matching procedure that aligns facade structures at\ndifferent levels which is used to introduce control maps to guide a generative\ndiffusion pipeline. This approach retains local appearance fidelity while\naccommodating large-scale edits such as floor duplication or window\nrearrangement. We provide a thorough evaluation, comparing Pro-DG against\ninpainting-based baselines and synthetic ground truths. Our user study and\nquantitative measurements indicate improved preservation of architectural\nidentity and higher edit accuracy. Our novel method is the first to integrate\nneuro-symbolically derived shape-grammars for modeling with modern generative\nmodel and highlights the broader potential of such approaches for precise and\ncontrollable image manipulation."}
{"id": "2504.01577", "pdf": "https://arxiv.org/pdf/2504.01577", "abs": "https://arxiv.org/abs/2504.01577", "authors": ["Lirui Qi", "Hongliang He", "Tong Wang", "Siwei Feng", "Guohong Fu"], "title": "Instance Migration Diffusion for Nuclear Instance Segmentation in Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Nuclear instance segmentation plays a vital role in disease diagnosis within\ndigital pathology. However, limited labeled data in pathological images\nrestricts the overall performance of nuclear instance segmentation. To tackle\nthis challenge, we propose a novel data augmentation framework Instance\nMigration Diffusion Model (IM-Diffusion), IM-Diffusion designed to generate\nmore varied pathological images by constructing diverse nuclear layouts and\ninternuclear spatial relationships. In detail, we introduce a Nuclear Migration\nModule (NMM) which constructs diverse nuclear layouts by simulating the process\nof nuclear migration. Building on this, we further present an\nInternuclear-regions Inpainting Module (IIM) to generate diverse internuclear\nspatial relationships by structure-aware inpainting. On the basis of the above,\nIM-Diffusion generates more diverse pathological images with different layouts\nand internuclear spatial relationships, thereby facilitating downstream tasks.\nEvaluation on the CoNSeP and GLySAC datasets demonstrate that the images\ngenerated by IM-Diffusion effectively enhance overall instance segmentation\nperformance. Code will be made public later."}
{"id": "2504.01767", "pdf": "https://arxiv.org/pdf/2504.01767", "abs": "https://arxiv.org/abs/2504.01767", "authors": ["Abdelrahaman A. Hassan", "Abdelrahman A. Ali", "Aya E. Fouda", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for Mental Illness Assessment", "categories": ["eess.AS", "cs.AI", "cs.CV"], "comment": null, "summary": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools."}
{"id": "2504.01879", "pdf": "https://arxiv.org/pdf/2504.01879", "abs": "https://arxiv.org/abs/2504.01879", "authors": ["Abhilash Shankarampeta", "Harsh Mahajan", "Tushar Kataria", "Dan Roth", "Vivek Gupta"], "title": "TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables", "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "19 Pages. 21 Tables, 1 figure", "summary": "Humans continuously make new discoveries, and understanding temporal sequence\nof events leading to these breakthroughs is essential for advancing science and\nsociety. This ability to reason over time allows us to identify future steps\nand understand the effects of financial and political decisions on our lives.\nHowever, large language models (LLMs) are typically trained on static datasets,\nlimiting their ability to perform effective temporal reasoning. To assess the\ntemporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES\ndataset, which comprises 3,971 questions derived from over 14,000 tables,\nspanning 1,238 entities across multiple time periods. We introduce a\ntemplate-based question-generation pipeline that harnesses LLMs to refine both\ntemplates and questions. Additionally, we establish baseline results using\nstate-of-the-art LLMs to create a benchmark. We also introduce novel modeling\nstrategies centered around task decomposition, enhancing LLM performance."}
