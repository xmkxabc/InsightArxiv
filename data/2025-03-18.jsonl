{"id": "2503.11731", "pdf": "https://arxiv.org/pdf/2503.11731", "abs": "https://arxiv.org/abs/2503.11731", "authors": ["Xianming Zeng", "Sicong Du", "Qifeng Chen", "Lizhe Liu", "Haoyu Shu", "Jiaxuan Gao", "Jiarun Liu", "Jiulong Xu", "Jianyun Xu", "Mingxia Chen", "Yiru Zhao", "Peng Chen", "Yapeng Xue", "Chunming Zhao", "Sheng Yang", "Qiang Li"], "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor simulation is pivotal for scalable validation of autonomous driving\nsystems, yet existing Neural Radiance Fields (NeRF) based methods face\napplicability and efficiency challenges in industrial workflows. This paper\nintroduces a Gaussian Splatting (GS) based system to address these challenges:\nWe first break down sensor simulator components and analyze the possible\nadvantages of GS over NeRF. Then in practice, we refactor three crucial\ncomponents through GS, to leverage its explicit scene representation and\nreal-time rendering: (1) choosing the 2D neural Gaussian representation for\nphysics-compliant scene and sensor modeling, (2) proposing a scene editing\npipeline to leverage Gaussian primitives library for data augmentation, and (3)\ncoupling a controllable diffusion model for scene expansion and harmonization.\nWe implement this framework on a proprietary autonomous driving dataset\nsupporting cameras and LiDAR sensors. We demonstrate through ablation studies\nthat our approach reduces frame-wise simulation latency, achieves better\ngeometric and photometric consistency, and enables interpretable explicit scene\nediting and expansion. Furthermore, we showcase how integrating such a GS-based\nsensor simulator with traffic and dynamic simulators enables full-stack testing\nof end-to-end autonomy algorithms. Our work provides both algorithmic insights\nand practical validation, establishing GS as a cornerstone for industrial-grade\nsensor simulation."}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Inc√†", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones."}
{"id": "2503.11750", "pdf": "https://arxiv.org/pdf/2503.11750", "abs": "https://arxiv.org/abs/2503.11750", "authors": ["Shuyang Hao", "Yiwei Wang", "Bryan Hooi", "Jun Liu", "Muhao Chen", "Zi Huang", "Yujun Cai"], "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."}
{"id": "2503.11780", "pdf": "https://arxiv.org/pdf/2503.11780", "abs": "https://arxiv.org/abs/2503.11780", "authors": ["Tianyi Zhao", "Boyang Liu", "Yanglei Gao", "Yiming Sun", "Maoxun Yuan", "Xingxing Wei"], "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem that the\ndecreased feature extraction capability in multi-modal joint learning. This\nleads to an unreasonable but prevalent phenomenon--Fusion Degradation, which\nhinders the performance improvement of the MMOD model. Motivated by this, in\nthis paper, we introduce linear probing evaluation to the multi-modal detectors\nand rethink the multi-modal object detection task from the mono-modality\nlearning perspective. Therefore, we construct an novel framework called\nM$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method\nand the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework\nfacilitates the sufficient learning of mono-modality during multi-modal joint\ntraining and explores a lightweight yet effective feature fusion manner to\nachieve superior object detection performance. Extensive experiments conducted\non three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates\nthe Fusion Degradation phenomenon and outperforms the previous SOTA detectors."}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN"}
{"id": "2503.11787", "pdf": "https://arxiv.org/pdf/2503.11787", "abs": "https://arxiv.org/abs/2503.11787", "authors": ["Samuel W. Remedios", "Shuwen Wei", "Shuo Han", "Jinwei Zhang", "Aaron Carass", "Kurt G. Schilling", "Dzung L. Pham", "Jerry L. Prince", "Blake E. Dewey"], "title": "ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In clinical imaging, magnetic resonance (MR) image volumes are often acquired\nas stacks of 2D slices, permitting decreased scan times, improved\nsignal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.\nWhile this is sufficient for clinical evaluation, automated algorithms designed\nfor 3D analysis perform sub-optimally on 2D-acquired scans, especially those\nwith thick slices and gaps between slices. Super-resolution (SR) methods aim to\naddress this problem, but previous methods do not address all of the following:\nslice profile shape estimation, slice gap, domain shift, and non-integer /\narbitrary upsampling factors. In this paper, we propose ECLARE (Efficient\nCross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method\nthat addresses each of these factors. ECLARE estimates the slice profile from\nthe 2D-acquired multi-slice MR volume, trains a network to learn the mapping\nfrom low-resolution to high-resolution in-plane patches from the same volume,\nand performs SR with anti-aliasing. We compared ECLARE to cubic B-spline\ninterpolation, SMORE, and other contemporary SR methods. We used realistic and\nrepresentative simulations so that quantitative performance against a ground\ntruth could be computed, and ECLARE outperformed all other methods in both\nsignal recovery and downstream tasks. On real data for which there is no ground\ntruth, ECLARE demonstrated qualitative superiority over other methods as well.\nImportantly, as ECLARE does not use external training data it cannot suffer\nfrom domain shift between training and testing. Our code is open-source and\navailable at https://www.github.com/sremedios/eclare."}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus."}
{"id": "2503.11794", "pdf": "https://arxiv.org/pdf/2503.11794", "abs": "https://arxiv.org/abs/2503.11794", "authors": ["Bangzheng Li", "Fei Wang", "Wenxuan Zhou", "Nan Xu", "Ben Zhou", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*."}
{"id": "2503.11806", "pdf": "https://arxiv.org/pdf/2503.11806", "abs": "https://arxiv.org/abs/2503.11806", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "categories": ["cs.CV"], "comment": "Project page: https://www.projectaria.com/scenescript/", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts."}
{"id": "2503.11807", "pdf": "https://arxiv.org/pdf/2503.11807", "abs": "https://arxiv.org/abs/2503.11807", "authors": ["Sanayya A", "Amoolya Shetty", "Abhijeet Sharma", "Venkatesh Ravichandran", "Masthan Wali Gosuvarapalli", "Sarthak Jain", "Priyamvada Nanjundiah", "Ujjal Kr Dutta", "Divya Sharma"], "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024", "summary": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making."}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taix√©", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM."}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign."}
{"id": "2503.11893", "pdf": "https://arxiv.org/pdf/2503.11893", "abs": "https://arxiv.org/abs/2503.11893", "authors": ["Md Abu Bakr Siddique", "Junliang Liu", "Piyush Singh", "Md Jahidul Islam"], "title": "UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The concept of waterbody style transfer remains largely unexplored in the\nunderwater imaging and vision literature. Traditional image style transfer\n(STx) methods primarily focus on artistic and photorealistic blending, often\nfailing to preserve object and scene geometry in images captured in\nhigh-scattering mediums such as underwater. The wavelength-dependent nonlinear\nattenuation and depth-dependent backscattering artifacts further complicate\nlearning underwater image STx from unpaired data. This paper introduces UStyle,\nthe first data-driven learning framework for transferring waterbody styles\nacross underwater images without requiring prior reference images or scene\ninformation. We propose a novel depth-aware whitening and coloring transform\n(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure\nperceptually consistent stylization while preserving scene structure. To\nenhance style transfer quality, we incorporate carefully designed loss\nfunctions that guide UStyle to maintain colorfulness, lightness, structural\nintegrity, and frequency-domain characteristics, as well as high-level content\nin VGG and CLIP (contrastive language-image pretraining) feature spaces. By\naddressing domain-specific challenges, UStyle provides a robust framework for\nno-reference underwater image STx, surpassing state-of-the-art (SOTA) methods\nthat rely solely on end-to-end reconstruction loss. Furthermore, we introduce\nthe UF7D dataset, a curated collection of high-resolution underwater images\nspanning seven distinct waterbody styles, establishing a benchmark to support\nfuture research in underwater image STx. The UStyle inference pipeline and UF7D\ndataset are released at: https://github.com/uf-robopi/UStyle."}
{"id": "2503.11905", "pdf": "https://arxiv.org/pdf/2503.11905", "abs": "https://arxiv.org/abs/2503.11905", "authors": ["Ruchika Chavhan", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Luca Morreale", "Mehdi Noroozi", "Sourav Bhattacharya"], "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Text-to-image synthesis has witnessed remarkable advancements in recent\nyears. Many attempts have been made to adopt text-to-image models to support\nmultiple tasks. However, existing approaches typically require\nresource-intensive re-training or additional parameters to accommodate for the\nnew tasks, which makes the model inefficient for on-device deployment. We\npropose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends\nthe capabilities of a pre-trained text-to-image diffusion model to support a\nvariety of image-to-image generation tasks. MTU replaces Feed-Forward Network\n(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,\nand combines them with a dynamic routing mechanism. To the best of our\nknowledge, MTU is the first multi-task diffusion modeling approach that\nseamlessly blends multi-tasking with on-device compatibility, by mitigating the\nissue of parameter inflation. We show that the performance of MTU is on par\nwith the single-task fine-tuned diffusion models across several tasks including\nimage editing, super-resolution, and inpainting, while maintaining similar\nlatency and computational load (GFLOPs) as the single-task fine-tuned models."}
{"id": "2503.11906", "pdf": "https://arxiv.org/pdf/2503.11906", "abs": "https://arxiv.org/abs/2503.11906", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Emanuele Salerno"], "title": "A Survey on SAR ship classification using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to JSTARS journal", "summary": "Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture\nRadar (SAR) ship classification. This survey comprehensively analyzes the\ndiverse DL techniques employed in this domain. We identify critical trends and\nchallenges, highlighting the importance of integrating handcrafted features,\nutilizing public datasets, data augmentation, fine-tuning, explainability\ntechniques, and fostering interdisciplinary collaborations to improve DL model\nperformance. This survey establishes a first-of-its-kind taxonomy for\ncategorizing relevant research based on DL models, handcrafted feature use, SAR\nattribute utilization, and the impact of fine-tuning. We discuss the\nmethodologies used in SAR ship classification tasks and the impact of different\ntechniques. Finally, the survey explores potential avenues for future research,\nincluding addressing data scarcity, exploring novel DL architectures,\nincorporating interpretability techniques, and establishing standardized\nperformance metrics. By addressing these challenges and leveraging advancements\nin DL, researchers can contribute to developing more accurate and efficient\nship classification systems, ultimately enhancing maritime surveillance and\nrelated applications."}
{"id": "2503.11919", "pdf": "https://arxiv.org/pdf/2503.11919", "abs": "https://arxiv.org/abs/2503.11919", "authors": ["Jeonghwan Park", "Kang Li", "Huiyu Zhou"], "title": "k-fold Subsampling based Sequential Backward Feature Elimination", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy."}
{"id": "2503.11930", "pdf": "https://arxiv.org/pdf/2503.11930", "abs": "https://arxiv.org/abs/2503.11930", "authors": ["Jingxuan Zhang", "Robert J. Hart", "Ziqian Bi", "Shiaofen Fang", "Susan Walsh"], "title": "Generating a Biometrically Unique and Realistic Iris Database", "categories": ["cs.CV", "cs.LG"], "comment": "for associated iris database, see\n  https://huggingface.co/datasets/fatdove/Iris_Database", "summary": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity."}
{"id": "2503.11932", "pdf": "https://arxiv.org/pdf/2503.11932", "abs": "https://arxiv.org/abs/2503.11932", "authors": ["Dhruv Kudale", "Badri Vishal Kasuba", "Venkatapathy Subramanian", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "title": "SPRINT: Script-agnostic Structure Recognition in Tables", "categories": ["cs.CV"], "comment": "Accepted at ICDAR 2024", "summary": "Table Structure Recognition (TSR) is vital for various downstream tasks like\ninformation retrieval, table reconstruction, and document understanding. While\nmost state-of-the-art (SOTA) research predominantly focuses on TSR in English\ndocuments, the need for similar capabilities in other languages is evident,\nconsidering the global diversity of data. Moreover, creating substantial\nlabeled data in non-English languages and training these SOTA models from\nscratch is costly and time-consuming. We propose TSR as a language-agnostic\ncell arrangement prediction and introduce SPRINT, Script-agnostic Structure\nRecognition in Tables. SPRINT uses recently introduced Optimized Table\nStructure Language (OTSL) sequences to predict table structures. We show that\nwhen coupled with a pre-trained table grid estimator, SPRINT can improve the\noverall tree edit distance-based similarity structure scores of tables even for\nnon-English documents. We experimentally evaluate our performance across\nbenchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our\nfindings reveal that SPRINT not only matches SOTA models in performance on\nstandard datasets but also demonstrates lower latency. Additionally, SPRINT\nexcels in accurately identifying table structures in non-English documents,\nsurpassing current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predictions into\na widely used HTML-based table representation. To encourage further research,\nwe release our code and Multilingual Scanned and Scene Table Structure\nRecognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in\nthirteen languages encompassing several scripts at\nhttps://github.com/IITB-LEAP-OCR/SPRINT"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method."}
{"id": "2503.11937", "pdf": "https://arxiv.org/pdf/2503.11937", "abs": "https://arxiv.org/abs/2503.11937", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model."}
{"id": "2503.11945", "pdf": "https://arxiv.org/pdf/2503.11945", "abs": "https://arxiv.org/abs/2503.11945", "authors": ["Naresh Kumar Devulapally", "Mingzhen Huang", "Vishal Asnani", "Shruti Agarwal", "Siwei Lyu", "Vishnu Suresh Lokhande"], "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Invisible watermarking of AI-generated images can help with copyright\nprotection, enabling detection and identification of AI-generated media. In\nthis work, we present a novel approach to watermark images of T2I Latent\nDiffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we\nenable watermarking in selected objects or parts of the image, offering greater\nflexibility compared to traditional full-image watermarking. Our method\nleverages the text encoder's compatibility across various LDMs, allowing\nplug-and-play integration for different LDMs. Moreover, introducing the\nwatermark early in the encoding stage improves robustness to adversarial\nperturbations in later stages of the pipeline. Our approach achieves $99\\%$ bit\naccuracy ($48$ bits) with a $10^5 \\times$ reduction in model parameters,\nenabling efficient watermarking."}
{"id": "2503.11953", "pdf": "https://arxiv.org/pdf/2503.11953", "abs": "https://arxiv.org/abs/2503.11953", "authors": ["Priyanka Mandikal", "Tushar Nagarajan", "Alex Stoken", "Zihui Xue", "Kristen Grauman"], "title": "SPOC: Spatially-Progressing Object State Change Segmentation in Video", "categories": ["cs.CV"], "comment": null, "summary": "Object state changes in video reveal critical information about human and\nagent activity. However, existing methods are limited to temporal localization\nof when the object is in its initial state (e.g., the unchopped avocado) versus\nwhen it has completed a state change (e.g., the chopped avocado), which limits\napplicability for any task requiring detailed information about the progress of\nthe actions and its spatial localization. We propose to deepen the problem by\nintroducing the spatially-progressing object state change segmentation task.\nThe goal is to segment at the pixel-level those regions of an object that are\nactionable and those that are transformed. We introduce the first model to\naddress this task, designing a VLM-based pseudo-labeling approach, state-change\ndynamics constraints, and a novel WhereToChange benchmark built on in-the-wild\nInternet videos. Experiments on two datasets validate both the challenge of the\nnew task as well as the promise of our model for localizing exactly where and\nhow fast objects are changing in video. We further demonstrate useful\nimplications for tracking activity progress to benefit robotic agents. Project\npage: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz √ñztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations."}
{"id": "2503.11969", "pdf": "https://arxiv.org/pdf/2503.11969", "abs": "https://arxiv.org/abs/2503.11969", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "The registration between the pre-operative model and the intra-operative\nsurface is crucial in image-guided liver surgery, as it facilitates the\neffective use of pre-operative information during the procedure. However, the\nintra-operative surface, usually represented as a point cloud, often has\nlimited coverage, especially in laparoscopic surgery, and is prone to holes and\nnoise, posing significant challenges for registration methods. Point cloud\ncompletion methods have the potential to alleviate these issues. Thus, we\nexplore six state-of-the-art point cloud completion methods to identify the\noptimal completion method for liver surgery applications. We focus on a\npatient-specific approach for liver point cloud completion from a partial liver\nsurface under three cases: canonical pose, non-canonical pose, and canonical\npose with noise. The transformer-based method, AdaPoinTr, outperforms all other\nmethods to generate a complete point cloud from the given partial liver point\ncloud under the canonical pose. On the other hand, our findings reveal\nsubstantial performance degradation of these methods under non-canonical poses\nand noisy settings, highlighting the limitations of these methods, which\nsuggests the need for a robust point completion method for its application in\nimage-guided liver surgery."}
{"id": "2503.11979", "pdf": "https://arxiv.org/pdf/2503.11979", "abs": "https://arxiv.org/abs/2503.11979", "authors": ["Runfa Blark Li", "Mahdi Shaghaghi", "Keito Suzuki", "Xinshuang Liu", "Varun Moparthi", "Bang Du", "Walker Curtis", "Martin Renschler", "Ki Myung Brian Lee", "Nikolay Atanasov", "Truong Nguyen"], "title": "DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) is one of the most important\nenvironment-perception and navigation algorithms for computer vision, robotics,\nand autonomous cars/drones. Hence, high quality and fast mapping becomes a\nfundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an\nexplicit representation with excellent rendering quality and speed,\nstate-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical\npointcloud-SLAM, GS-SLAM generates photometric information by learning from\ninput camera views and synthesize unseen views with high-quality textures.\nHowever, these GS-SLAM fail when moving objects occupy the scene that violate\nthe static assumption of bundle adjustment. The failed updates of moving GS\naffects the static GS and contaminates the full map over long frames. Although\nsome efforts have been made by concurrent works to consider moving objects for\nGS-SLAM, they simply detect and remove the moving regions from GS rendering\n(\"anti'' dynamic GS-SLAM), where only the static background could benefit from\nGS. To this end, we propose the first real-time GS-SLAM, \"DynaGSLAM'', that\nachieves high-quality online GS rendering, tracking, motion predictions of\nmoving objects in dynamic scenes while jointly estimating accurate ego motion.\nOur DynaGSLAM outperforms SOTA static & \"Anti'' dynamic GS-SLAM on three\ndynamic real datasets, while keeping speed and memory efficiency in practice."}
{"id": "2503.11981", "pdf": "https://arxiv.org/pdf/2503.11981", "abs": "https://arxiv.org/abs/2503.11981", "authors": ["Utkarsh Nath", "Rajeev Goel", "Rahul Khurana", "Kyle Min", "Mark Ollila", "Pavan Turaga", "Varun Jampani", "Tejaswi Gowda"], "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation saw dramatic advances in recent years by leveraging\nText-to-Image models. However, most existing techniques struggle with\ncompositional prompts, which describe multiple objects and their spatial\nrelationships. They often fail to capture fine-grained inter-object\ninteractions. We introduce DecompDreamer, a Gaussian splatting-based training\nroutine designed to generate high-quality 3D compositions from such complex\nprompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose\nscenes into structured components and their relationships. We propose a\nprogressive optimization strategy that first prioritizes joint relationship\nmodeling before gradually shifting toward targeted object refinement. Our\nqualitative and quantitative evaluations against state-of-the-art text-to-3D\nmodels demonstrate that DecompDreamer effectively generates intricate 3D\ncompositions with superior object disentanglement, offering enhanced control\nand flexibility in 3D generation. Project page :\nhttps://decompdreamer3d.github.io"}
{"id": "2503.11995", "pdf": "https://arxiv.org/pdf/2503.11995", "abs": "https://arxiv.org/abs/2503.11995", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Zhihao Chen", "Guangwei Gao"], "title": "Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "In recent years, Transformer has witnessed significant progress in food\nrecognition. However, most existing approaches still face two critical\nchallenges in lightweight food recognition: (1) the quadratic complexity and\nredundant feature representation from interactions with irrelevant tokens; (2)\nstatic feature recognition and single-scale representation, which overlook the\nunstructured, non-fixed nature of food images and the need for multi-scale\nfeatures. To address these, we propose an adaptive and efficient sparse\nTransformer architecture (Fraesormer) with two core designs: Adaptive Top-k\nSparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature\nGating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator\n(GDTKO) to retain critical attention scores, filtering low query-key matches\nthat hinder feature aggregation. It also introduces a partial channel mechanism\nto reduce redundancy and promote expert information flow, enabling local-global\ncollaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale\nfeature representation, enhancing contextual semantic information. Extensive\nexperiments show that Fraesormer outperforms state-of-the-art methods. code is\navailable at https://zs1314.github.io/Fraesormer."}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM."}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset."}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets."}
{"id": "2503.12015", "pdf": "https://arxiv.org/pdf/2503.12015", "abs": "https://arxiv.org/abs/2503.12015", "authors": ["Donglin Yang", "Paul Vicol", "Xiaojuan Qi", "Renjie Liao", "Xiaofan Zhang"], "title": "QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for Efficient Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based super-resolution (SR) methods often perform pixel-wise\ncomputations uniformly across entire images, even in homogeneous regions where\nhigh-resolution refinement is redundant. We propose the Quadtree Diffusion\nModel (QDM), a region-adaptive diffusion framework that leverages a quadtree\nstructure to selectively enhance detail-rich regions while reducing\ncomputations in homogeneous areas. By guiding the diffusion with a quadtree\nderived from the low-quality input, QDM identifies key regions-represented by\nleaf nodes-where fine detail is essential and applies minimal refinement\nelsewhere. This mask-guided, two-stream architecture adaptively balances\nquality and efficiency, producing high-fidelity outputs with low computational\nredundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR\ntasks across diverse image types, particularly in medical imaging (e.g., CT\nscans), where large homogeneous regions are prevalent. Furthermore, QDM\noutperforms or is comparable to state-of-the-art SR methods on standard\nbenchmarks while significantly reducing computational costs, highlighting its\nefficiency and suitability for resource-limited environments. Our code is\navailable at https://github.com/linYDTHU/QDM."}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach."}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation."}
{"id": "2503.12026", "pdf": "https://arxiv.org/pdf/2503.12026", "abs": "https://arxiv.org/abs/2503.12026", "authors": ["Zihan Zhoua", "Changrui Daia", "Aibo Songa", "Xiaolin Fang"], "title": "Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised video correspondence learning depends on the ability to\naccurately associate pixels between video frames that correspond to the same\nvisual object. However, achieving reliable pixel matching without supervision\nremains a major challenge. To address this issue, recent research has focused\non feature learning techniques that aim to encode unique pixel representations\nfor matching. Despite these advances, existing methods still struggle to\nachieve exact pixel correspondences and often suffer from false matches,\nlimiting their effectiveness in self-supervised settings.\n  To this end, we explore an efficient self-supervised Video Correspondence\nLearning framework (MER) that aims to accurately extract object details from\nunlabeled videos. First, we design a dedicated Motion Enhancement Engine that\nemphasizes capturing the dynamic motion of objects in videos. In addition, we\nintroduce a flexible sampling strategy for inter-pixel correspondence\ninformation (Multi-Cluster Sampler) that enables the model to pay more\nattention to the pixel changes of important objects in motion. Through\nexperiments, our algorithm outperforms the state-of-the-art competitors on\nvideo correspondence learning tasks such as video object segmentation and video\nobject keypoint tracking."}
{"id": "2503.12028", "pdf": "https://arxiv.org/pdf/2503.12028", "abs": "https://arxiv.org/abs/2503.12028", "authors": ["F. √áengel", "V. Adanova", "S. Tari"], "title": "Challenges in Plane Symmetry: From Theory to Perception", "categories": ["cs.CV"], "comment": null, "summary": "The planar ornaments are created by repeating a base unit using a combination\nof four primitive geometric operations: translation, rotation, reflection, and\nglide reflection. According to group theory, different combinations of these\nfour geometric operations lead to different symmetry groups. In this work, we\nselect a single challenging ornament, and analyze it both from the theoretical\npoint of view and perceptual point of view. We present the perceptual\nexperiment results, where one can see that the symmetries that the participants\nperceived from the ornaments do not match to what the theory dictates."}
{"id": "2503.12034", "pdf": "https://arxiv.org/pdf/2503.12034", "abs": "https://arxiv.org/abs/2503.12034", "authors": ["Enes Erdogan", "Eren Erdal Aksoy", "Sanem Sariel"], "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures, 7 tables", "summary": "Recognition of human manipulation actions in real-time is essential for safe\nand effective human-robot interaction and collaboration. The challenge lies in\ndeveloping a model that is both lightweight enough for real-time execution and\ncapable of generalization. While some existing methods in the literature can\nrun in real-time, they struggle with temporal scalability, i.e., they fail to\nadapt to long-duration manipulations effectively. To address this, leveraging\nthe generalizable scene graph representations, we propose a new Factorized\nGraph Sequence Encoder network that not only runs in real-time but also scales\neffectively in the temporal dimension, thanks to its factorized encoder\narchitecture. Additionally, we introduce Hand Pooling operation, a simple\npooling operation for more focused extraction of the graph-level embeddings.\nOur model outperforms the previous state-of-the-art real-time approach,\nachieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual\nAction (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.\nMoreover, we conduct an extensive ablation study to validate our network design\nchoices. Finally, we compare our model with its architecturally similar\nRGB-based model on the Bimacs dataset and show the limitations of this model in\ncontrast to ours on such an object-centric manipulation dataset."}
{"id": "2503.12035", "pdf": "https://arxiv.org/pdf/2503.12035", "abs": "https://arxiv.org/abs/2503.12035", "authors": ["Zhengyuan Peng", "Jinpeng Ma", "Zhimin Sun", "Ran Yi", "Haichuan Song", "Xin Tan", "Lizhuang Ma"], "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is a classification task that aims to\nclassify both base and novel classes in unlabeled images, using knowledge from\na labeled dataset. In GCD, previous research overlooks scene information or\ntreats it as noise, reducing its impact during model training. However, in this\npaper, we argue that scene information should be viewed as a strong prior for\ninferring novel classes. We attribute the misinterpretation of scene\ninformation to a key factor: the Ambiguity Challenge inherent in GCD.\nSpecifically, novel objects in base scenes might be wrongly classified into\nbase categories, while base objects in novel scenes might be mistakenly\nrecognized as novel categories. Once the ambiguity challenge is addressed,\nscene information can reach its full potential, significantly enhancing the\nperformance of GCD models. To more effectively leverage scene information, we\npropose the Modeling Object-Scene Associations (MOS) framework, which utilizes\na simple MLP-based scene-awareness module to enhance GCD performance. It\nachieves an exceptional average accuracy improvement of 4% on the challenging\nfine-grained datasets compared to state-of-the-art methods, emphasizing its\nsuperior performance in fine-grained GCD. The code is publicly available at\nhttps://github.com/JethroPeng/MOS."}
{"id": "2503.12047", "pdf": "https://arxiv.org/pdf/2503.12047", "abs": "https://arxiv.org/abs/2503.12047", "authors": ["Hangrui Xu", "Chuanrui Zhang", "Zhengxian Wu", "Peng Jiao", "Haoqian Wang"], "title": "PSGait: Multimodal Gait Recognition using Parsing Skeleton", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the parsing skeleton\nrepresentation, we propose a novel parsing skeleton-based gait recognition\nframework, named PSGait, which takes parsing skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods. Furthermore,\nas a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in\nRank-1 accuracy across various gait recognition models. These results\ndemonstrate the effectiveness and versatility of parsing skeletons for gait\nrecognition in the wild, establishing PSGait as a new state-of-the-art approach\nfor multimodal gait recognition."}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO."}
{"id": "2503.12052", "pdf": "https://arxiv.org/pdf/2503.12052", "abs": "https://arxiv.org/abs/2503.12052", "authors": ["Zhiyao Sun", "Yu-Hui Wen", "Matthieu Lin", "Ho-Jui Fang", "Sheng Ye", "Tian Lv", "Yong-Jin Liu"], "title": "Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://human-tailor.github.com", "summary": "Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use."}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet."}
{"id": "2503.12063", "pdf": "https://arxiv.org/pdf/2503.12063", "abs": "https://arxiv.org/abs/2503.12063", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting", "categories": ["cs.CV"], "comment": null, "summary": "Cell counting remains a fundamental yet challenging task in medical and\nbiological research due to the diverse morphology of cells, their dense\ndistribution, and variations in image quality. We present DLA-Count, a\nbreakthrough approach to cell counting that introduces three key innovations:\n(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell\nmatching in dense regions, (2) Multi-scale Deformable Gaussian Convolution\n(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced\nFeature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive\nexperiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)\ndemonstrate that our method outperforms previous methods across diverse\ndatasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and\n42.5\\% on MBM datasets. Our code is available at\nhttps://anonymous.4open.science/r/DLA-Count."}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey"}
{"id": "2503.12068", "pdf": "https://arxiv.org/pdf/2503.12068", "abs": "https://arxiv.org/abs/2503.12068", "authors": ["Qingchen Tang", "Lei Fan", "Maurice Pagnucco", "Yang Song"], "title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised image segmentation with image-level labels has drawn\nattention due to the high cost of pixel-level annotations. Traditional methods\nusing Class Activation Maps (CAMs) often highlight only the most discriminative\nregions, leading to incomplete masks. Recent approaches that introduce textual\ninformation struggle with histopathological images due to inter-class\nhomogeneity and intra-class heterogeneity. In this paper, we propose a\nprototype-based image prompting framework for histopathological image\nsegmentation. It constructs an image bank from the training set using\nclustering, extracting multiple prototype features per class to capture\nintra-class heterogeneity. By designing a matching loss between input features\nand class-specific prototypes using contrastive learning, our method addresses\ninter-class homogeneity and guides the model to generate more accurate CAMs.\nExperiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show\nthat our method outperforms existing weakly supervised segmentation approaches,\nsetting new benchmarks in histopathological image segmentation."}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness."}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization."}
{"id": "2503.12086", "pdf": "https://arxiv.org/pdf/2503.12086", "abs": "https://arxiv.org/abs/2503.12086", "authors": ["Rui Qian", "Chenyangguang Zhang", "Yan Di", "Guangyao Zhai", "Ruida Zhang", "Jiayu Guo", "Benjamin Busam", "Jian Pu"], "title": "FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have exhibited highly effective performance for\nphotorealistic novel view synthesis recently. However, the key limitation it\nmeets is the reliance on a hand-crafted frequency annealing strategy to recover\n3D scenes with imperfect camera poses. The strategy exploits a temporal\nlow-pass filter to guarantee convergence while decelerating the joint\noptimization of implicit scene reconstruction and camera registration. In this\nwork, we introduce the Frequency Adapted Bundle Adjusting Radiance Field\n(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted\nspatial low-pass filter to address the decelerating problem. We establish a\ntheoretical framework to interpret the relationship between position encoding\nof NeRF and camera registration and show that our frequency-adapted filter can\nmitigate frequency fluctuation caused by the temporal filter. Furthermore, we\nshow that applying a spatial low-pass filter in NeRF can optimize camera poses\nproductively through radial uncertainty overlaps among various views. Extensive\nexperiments show that FA-BARF can accelerate the joint optimization process\nunder little perturbations in object-centric scenes and recover real-world\nscenes with unknown camera poses. This implies wider possibilities for NeRF\napplied in dense 3D mapping and reconstruction under real-time requirements.\nThe code will be released upon paper acceptance."}
{"id": "2503.12087", "pdf": "https://arxiv.org/pdf/2503.12087", "abs": "https://arxiv.org/abs/2503.12087", "authors": ["Gino E. Jansen", "Mark J. Schuuring", "Berto J. Bouma", "Ivana I≈°gum"], "title": "Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel approach to achieving temporally consistent mitral\nannulus landmark localization in echocardiography videos using sparse\nannotations. Our method introduces a self-supervised loss term that enforces\ntemporal consistency between neighboring frames, which smooths the position of\nlandmarks and enhances measurement accuracy over time. Additionally, we\nincorporate realistic field-of-view augmentations to improve the recognition of\nmissing anatomical landmarks. We evaluate our approach on both a public and\nprivate dataset, and demonstrate significant improvements in Mitral Annular\nPlane Systolic Excursion (MAPSE) calculations and overall landmark tracking\nstability. The method achieves a mean absolute MAPSE error of 1.81 $\\pm$ 0.14\nmm, an annulus size error of 2.46 $\\pm$ 0.31 mm, and a landmark localization\nerror of 2.48 $\\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for\nrecognition of missing landmarks."}
{"id": "2503.12093", "pdf": "https://arxiv.org/pdf/2503.12093", "abs": "https://arxiv.org/abs/2503.12093", "authors": ["Oren Shrout", "Ayellet Tal"], "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose SFMNet, a novel 3D sparse detector that combines the efficiency of\nsparse convolutions with the ability to model long-range dependencies. While\ntraditional sparse convolution techniques efficiently capture local structures,\nthey struggle with modeling long-range relationships. However, capturing\nlong-range dependencies is fundamental for 3D object detection. In contrast,\ntransformers are designed to capture these long-range dependencies through\nattention mechanisms. But, they come with high computational costs, due to\ntheir quadratic query-key-value interactions. Furthermore, directly applying\nattention to non-empty voxels is inefficient due to the sparse nature of 3D\nscenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,\nwhich integrates short- and long-range contexts with linear complexity by\nleveraging a new hierarchical sparse convolution design. This approach enables\nSFMNet to achieve high detection performance with improved efficiency, making\nit well-suited for large-scale LiDAR scenes. We show that our detector achieves\nstate-of-the-art performance on autonomous driving datasets."}
{"id": "2503.12094", "pdf": "https://arxiv.org/pdf/2503.12094", "abs": "https://arxiv.org/abs/2503.12094", "authors": ["Weiming Zhang", "Dingwen Xiao", "Lei Chen", "Lin Wang"], "title": "E-SAM: Training-Free Segment Every Entity Model", "categories": ["cs.CV"], "comment": "Under review", "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities\nwithin an image without the need for predefined class labels. This\ncharacteristic makes ES well-suited to open-world applications with adaptation\nto diverse and dynamically changing environments, where new and previously\nunseen entities may appear frequently. Existing ES methods either require large\nannotated datasets or high training costs, limiting their scalability and\nadaptability. Recently, the Segment Anything Model (SAM), especially in its\nAutomatic Mask Generation (AMG) mode, has shown potential for holistic image\nsegmentation. However, it struggles with over-segmentation and\nunder-segmentation, making it less effective for ES. In this paper, we\nintroduce E-SAM, a novel training-free framework that exhibits exceptional ES\ncapability. Specifically, we first propose Multi-level Mask Generation (MMG)\nthat hierarchically processes SAM's AMG outputs to generate reliable\nobject-level masks while preserving fine details at other levels. Entity-level\nMask Refinement (EMR) then refines these object-level masks into accurate\nentity-level masks. That is, it separates overlapping masks to address the\nredundancy issues inherent in SAM's outputs and merges similar masks by\nevaluating entity-level consistency. Lastly, Under-Segmentation Refinement\n(USR) addresses under-segmentation by generating additional high-confidence\nmasks fused with EMR outputs to produce the final ES map. These three modules\nare seamlessly optimized to achieve the best ES without additional training\noverhead. Extensive experiments demonstrate that E-SAM achieves\nstate-of-the-art performance compared to prior ES methods, demonstrating a\nsignificant improvement by +30.1 on benchmark metrics."}
{"id": "2503.12095", "pdf": "https://arxiv.org/pdf/2503.12095", "abs": "https://arxiv.org/abs/2503.12095", "authors": ["Walter Zimmer", "Ross Greer", "Daniel Lehmberg", "Marc Pavel", "Holger Caesar", "Xingcheng Zhou", "Ahmed Ghita", "Mohan Trivedi", "Rui Song", "Hu Cao", "Akshay Gopalkrishnan", "Alois C. Knoll"], "title": "Towards Vision Zero: The Accid3nD Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Even though a significant amount of work has been done to increase the safety\nof transportation networks, accidents still occur regularly. They must be\nunderstood as unavoidable and sporadic outcomes of traffic networks. No public\ndataset contains 3D annotations of real-world accidents recorded from roadside\nsensors. We present the Accid3nD dataset, a collection of real-world highway\naccidents in different weather and lighting conditions. It contains vehicle\ncrashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,\ninstance masks, and 3D bounding boxes with track IDs. In total, the dataset\ncontains 111,945 labeled frames recorded from four roadside cameras and LiDARs\nat 25 Hz. The dataset contains six object classes and is provided in the\nOpenLABEL format. We propose an accident detection model that combines a\nrule-based approach with a learning-based one. Experiments and ablation studies\non our dataset show the robustness of our proposed method. The dataset, model,\nand code are available on our website: https://accident-dataset.github.io."}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks."}
{"id": "2503.12102", "pdf": "https://arxiv.org/pdf/2503.12102", "abs": "https://arxiv.org/abs/2503.12102", "authors": ["Paula Andrea P√©rez-Toro", "Tom√°s Arias-Vergara", "Fangxu Xing", "Xiaofeng Liu", "Maureen Stone", "Jiachen Zhuo", "Juan Rafael Orozco-Arroyave", "Elmar N√∂th", "Jana Hutter", "Jerry L. Prince", "Andreas Maier", "Jonghye Woo"], "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the relationship between vocal tract motion during speech and\nthe resulting acoustic signal is crucial for aided clinical assessment and\ndeveloping personalized treatment and rehabilitation strategies. Toward this\ngoal, we introduce an audio-to-video generation framework for creating Real\nTime/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract\nfrom speech signals. Our framework first preprocesses RT-/cine-MRI sequences\nand speech samples to achieve temporal alignment, ensuring synchronization\nbetween visual and audio data. We then employ a modified stable diffusion\nmodel, integrating structural and temporal blocks, to effectively capture\nmovement characteristics and temporal dynamics in the synchronized data. This\nprocess enables the generation of MRI sequences from new speech inputs,\nimproving the conversion of audio into visual data. We evaluated our framework\non healthy controls and tongue cancer patients by analyzing and comparing the\nvocal tract movements in synthesized videos. Our framework demonstrated\nadaptability to new speech inputs and effective generalization. In addition,\npositive human evaluations confirmed its effectiveness, with realistic and\naccurate visualizations, suggesting its potential for outpatient therapy and\npersonalized simulation of vocal tract visualizations."}
{"id": "2503.12124", "pdf": "https://arxiv.org/pdf/2503.12124", "abs": "https://arxiv.org/abs/2503.12124", "authors": ["Yingying Deng", "Xiangyu He", "Fan Tang", "Weiming Dong"], "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "The customization of multiple attributes has gained popularity with the\nrising demand for personalized content creation. Despite promising empirical\nresults, the contextual coherence between different attributes has been largely\noverlooked. In this paper, we argue that subsequent attributes should follow\nthe multivariable conditional distribution introduced by former attribute\ncreation. In light of this, we reformulate multi-attribute creation from a\nconditional probability theory perspective and tackle the challenging zero-shot\nsetting. By explicitly modeling the dependencies between attributes, we further\nenhance the coherence of generated images across diverse attribute\ncombinations. Furthermore, we identify connections between multi-attribute\ncustomization and multi-task learning, effectively addressing the high\ncomputing cost encountered in multi-attribute synthesis. Extensive experiments\ndemonstrate that Z-Magic outperforms existing models in zero-shot image\ngeneration, with broad implications for AI-driven design and creative\napplications."}
{"id": "2503.12127", "pdf": "https://arxiv.org/pdf/2503.12127", "abs": "https://arxiv.org/abs/2503.12127", "authors": ["Tobia Poppi", "Tejaswi Kasarla", "Pascal Mettes", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Hyperbolic Safety-Aware Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "CVPR 2025", "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC."}
{"id": "2503.12131", "pdf": "https://arxiv.org/pdf/2503.12131", "abs": "https://arxiv.org/abs/2503.12131", "authors": ["Shentong Mo", "Zehua Chen", "Fan Bao", "Jun Zhu"], "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities."}
{"id": "2503.12150", "pdf": "https://arxiv.org/pdf/2503.12150", "abs": "https://arxiv.org/abs/2503.12150", "authors": ["Hongyu Sun", "Qiuhong Ke", "Ming Cheng", "Yongcai Wang", "Deying Li", "Chenhui Gou", "Jianfei Cai"], "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables", "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."}
{"id": "2503.12165", "pdf": "https://arxiv.org/pdf/2503.12165", "abs": "https://arxiv.org/abs/2503.12165", "authors": ["Zijian He", "Yuwei Ning", "Yipeng Qin", "Wangrun Wang", "Sibei Yang", "Liang Lin", "Guanbin Li"], "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360."}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettr√©", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes."}
{"id": "2503.12173", "pdf": "https://arxiv.org/pdf/2503.12173", "abs": "https://arxiv.org/abs/2503.12173", "authors": ["Yuchen Deng", "Haibin Ling", "Bingyao Huang"], "title": "LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization", "categories": ["cs.CV", "cs.MM"], "comment": "12 pages, 9 figures", "summary": "We propose LAPIG, a language guided projector image generation method with\nsurface adaptation and stylization. LAPIG consists of a projector-camera system\nand a target textured projection surface. LAPIG takes the user text prompt as\ninput and aims to transform the surface style using the projector. LAPIG's key\nchallenge is that due to the projector's physical brightness limitation and the\nsurface texture, the viewer's perceived projection may suffer from color\nsaturation and artifacts in both dark and bright regions, such that even with\nthe state-of-the-art projector compensation techniques, the viewer may see\nclear surface texture-related artifacts. Therefore, how to generate a projector\nimage that follows the user's instruction while also displaying minimum surface\nartifacts is an open problem. To address this issue, we propose projection\nsurface adaptation (PSA) that can generate compensable surface stylization. We\nfirst train two networks to simulate the projector compensation and\nproject-and-capture processes, this allows us to find a satisfactory projector\nimage without real project-and-capture and utilize gradient descent for fast\nconvergence. Then, we design content and saturation losses to guide the\nprojector image generation, such that the generated image shows no clearly\nperceivable artifacts when projected. Finally, the generated image is projected\nfor visually pleasing surface style morphing effects. The source code and video\nare available on the project page: https://Yu-chen-Deng.github.io/LAPIG/."}
{"id": "2503.12191", "pdf": "https://arxiv.org/pdf/2503.12191", "abs": "https://arxiv.org/abs/2503.12191", "authors": ["Ying Zang", "Yuncan Gao", "Jiangi Zhang", "Yuangi Hu", "Runlong Cao", "Lanyun Zhu", "Qi Zhu", "Deyi Ji", "Renjun Xu", "Tianrun Chen"], "title": "Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches", "categories": ["cs.CV"], "comment": null, "summary": "This work advances zero-shot interactive segmentation for remote sensing\nimagery through three key contributions. First, we propose a novel sketch-based\nprompting method, enabling users to intuitively outline objects, surpassing\ntraditional point or box prompts. Second, we introduce LTL-Sensing, the first\ndataset pairing human sketches with remote sensing imagery, setting a benchmark\nfor future research. Third, we present LTL-Net, a model featuring a multi-input\nprompting transport module tailored for freehand sketches. Extensive\nexperiments show our approach significantly improves segmentation accuracy and\nrobustness over state-of-the-art methods like SAM, fostering more intuitive\nhuman-AI collaboration in remote sensing analysis and enhancing its\napplications."}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks."}
{"id": "2503.12206", "pdf": "https://arxiv.org/pdf/2503.12206", "abs": "https://arxiv.org/abs/2503.12206", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Muhammad Haris Khan", "Mohsen Ali"], "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. We evaluated our\nmodels on 11 base-to-novel datasets and they achieved superior accuracy on 9 of\nthese, including benchmarks like ImageNet, SUN397 and Caltech101, while\nmaintaining a strictly training-free paradigm. Our overall accuracy of 83.44%\nsurpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.\nOur method achieved 83.6% average accuracy across 13 datasets, a 9.7%\nimprovement over the previous 73.9% state-of-the-art for training-free\napproaches. Our method improves domain generalization, with a 3.6% gain on\nImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot\nmethods."}
{"id": "2503.12213", "pdf": "https://arxiv.org/pdf/2503.12213", "abs": "https://arxiv.org/abs/2503.12213", "authors": ["Ruyu Wang", "Xuefeng Hou", "Sabrina Schmedding", "Marco F. Huber"], "title": "STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted by WACV2025", "summary": "In layout-to-image (L2I) synthesis, controlled complex scenes are generated\nfrom coarse information like bounding boxes. Such a task is exciting to many\ndownstream applications because the input layouts offer strong guidance to the\ngeneration process while remaining easily reconfigurable by humans. In this\npaper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based\nmodel that produces photo-realistic images and provides fine-grained control of\nstylized objects in scenes. Our approach learns a global condition for each\nlayout, and a self-supervised semantic map for weight modulation using a novel\nEdge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)\nis also introduced to cross-condition the global condition and image feature\nfor capturing the objects' relationships. These measures provide consistent\nguidance through the model, enabling more accurate and controllable image\ngeneration. Extensive benchmarking demonstrates that our STAY Diffusion\npresents high-quality images while surpassing previous state-of-the-art methods\nin generation diversity, accuracy, and controllability."}
{"id": "2503.12215", "pdf": "https://arxiv.org/pdf/2503.12215", "abs": "https://arxiv.org/abs/2503.12215", "authors": ["Amulya Reddy Maligireddy", "Manohar Reddy Uppula", "Nidhi Rastogi", "Yaswanth Reddy Parla"], "title": "Gun Detection Using Combined Human Pose and Weapon Appearance", "categories": ["cs.CV"], "comment": null, "summary": "The increasing frequency of firearm-related incidents has necessitated\nadvancements in security and surveillance systems, particularly in firearm\ndetection within public spaces. Traditional gun detection methods rely on\nmanual inspections and continuous human monitoring of CCTV footage, which are\nlabor-intensive and prone to high false positive and negative rates. To address\nthese limitations, we propose a novel approach that integrates human pose\nestimation with weapon appearance recognition using deep learning techniques.\nUnlike prior studies that focus on either body pose estimation or firearm\ndetection in isolation, our method jointly analyzes posture and weapon presence\nto enhance detection accuracy in real-world, dynamic environments. To train our\nmodel, we curated a diverse dataset comprising images from open-source\nrepositories such as IMFDB and Monash Guns, supplemented with AI-generated and\nmanually collected images from web sources. This dataset ensures robust\ngeneralization and realistic performance evaluation under various surveillance\nconditions. Our research aims to improve the precision and reliability of\nfirearm detection systems, contributing to enhanced public safety and threat\nmitigation in high-risk areas."}
{"id": "2503.12218", "pdf": "https://arxiv.org/pdf/2503.12218", "abs": "https://arxiv.org/abs/2503.12218", "authors": ["Chengxuan Qian", "Kai Han", "Siqi Ma", "Chongwen Lyu", "Zhenlong Yuan", "Jun Chen", "Zhe Liu"], "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods."}
{"id": "2503.12230", "pdf": "https://arxiv.org/pdf/2503.12230", "abs": "https://arxiv.org/abs/2503.12230", "authors": ["Yihao Wang", "Raphael Memmesheimer", "Sven Behnke"], "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps."}
{"id": "2503.12232", "pdf": "https://arxiv.org/pdf/2503.12232", "abs": "https://arxiv.org/abs/2503.12232", "authors": ["Yan Jiang", "Hao Yu", "Xu Cheng", "Haoyu Chen", "Zhaodong Sun", "Guoying Zhao"], "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community."}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos."}
{"id": "2503.12249", "pdf": "https://arxiv.org/pdf/2503.12249", "abs": "https://arxiv.org/abs/2503.12249", "authors": ["Boyu Chen", "Ameenat L. Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "categories": ["cs.CV"], "comment": null, "summary": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging\ntechnique with great potential for diagnosing anterior uveitis, a\nvision-threatening ocular inflammatory condition. A hallmark of this condition\nis the presence of inflammatory cells in the eye's anterior chamber, and\ndetecting these cells using AS-OCT images has attracted research interest.\nWhile recent efforts aim to replace manual cell detection with automated\ncomputer vision approaches, detecting extremely small (minuscule) objects in\nhigh-resolution images, such as AS-OCT, poses substantial challenges: (1) each\ncell appears as a minuscule particle, representing less than 0.005\\% of the\nimage, making the detection difficult, and (2) OCT imaging introduces\npixel-level noise that can be mistaken for cells, leading to false positive\ndetections. To overcome these challenges, we propose a minuscule cell detection\nframework through a progressive field-of-view focusing strategy. This strategy\nsystematically refines the detection scope from the whole image to a target\nregion where cells are likely to be present, and further to minuscule regions\npotentially containing individual cells. Our framework consists of two modules.\nFirst, a Field-of-Focus module uses a vision foundation model to segment the\ntarget region. Subsequently, a Fine-grained Object Detection module introduces\na specialized Minuscule Region Proposal followed by a Spatial Attention Network\nto distinguish individual cells from noise within the segmented region.\nExperimental results demonstrate that our framework outperforms\nstate-of-the-art methods for cell detection, providing enhanced efficacy for\nclinical applications. Our code is publicly available at:\nhttps://github.com/joeybyc/MCD."}
{"id": "2503.12260", "pdf": "https://arxiv.org/pdf/2503.12260", "abs": "https://arxiv.org/abs/2503.12260", "authors": ["Josep Cabacas-Maso", "Elena Ortega-Beltr√°n", "Ismael Benito-Altamirano", "Carles Ventura"], "title": "Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge", "categories": ["cs.CV", "I.4"], "comment": null, "summary": "We present our contribution to the 8th ABAW challenge at CVPR 2025, where we\ntackle valence-arousal estimation, emotion recognition, and facial action unit\ndetection as three independent challenges. Our approach leverages the\nwell-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all\nthree tasks, achieving results that surpass the proposed baselines.\nAdditionally, we explore the use of CLIP for the emotion recognition challenge\nas an additional experiment. We provide insights into the architectural choices\nthat contribute to the strong performance of our methods."}
{"id": "2503.12261", "pdf": "https://arxiv.org/pdf/2503.12261", "abs": "https://arxiv.org/abs/2503.12261", "authors": ["R. Gnana Praveen", "Jahangir Alam"], "title": "Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Submission to valence arousal track of 8th ABAW competition. arXiv\n  admin note: substantial text overlap with arXiv:2403.13659", "summary": "Multimodal emotion recognition has recently drawn a lot of interest in\naffective computing as it has immense potential to outperform isolated unimodal\napproaches. Audio and visual modalities are two predominant contact-free\nchannels in videos, which are often expected to carry a complementary\nrelationship with each other. However, audio and visual channels may not always\nbe complementary with each other, resulting in poor audio-visual feature\nrepresentations, thereby degrading the performance of the system. In this\npaper, we propose a flexible audio-visual fusion model that can adapt to weak\ncomplementary relationships using a gated attention mechanism. Specifically, we\nextend the recursive joint cross-attention model by introducing gating\nmechanism in every iteration to control the flow of information between the\ninput features and the attended features depending on the strength of their\ncomplementary relationship. For instance, if the modalities exhibit strong\ncomplementary relationships, the gating mechanism chooses cross-attended\nfeatures, otherwise non-attended features. To further improve the performance\nof the system, we further introduce stage gating mechanism, which is used to\ncontrol the flow of information across the gated outputs of each iteration.\nTherefore, the proposed model improves the performance of the system even when\nthe audio and visual modalities do not have a strong complementary relationship\nwith each other by adding more flexibility to the recursive joint cross\nattention mechanism. The proposed model has been evaluated on the challenging\nAffwild2 dataset and significantly outperforms the state-of-the-art fusion\napproaches."}
{"id": "2503.12267", "pdf": "https://arxiv.org/pdf/2503.12267", "abs": "https://arxiv.org/abs/2503.12267", "authors": ["Aziz Amari", "Mariem Makni", "Wissal Fnaich", "Akram Lahmar", "Fedi Koubaa", "Oumayma Charrad", "Mohamed Ali Zormati", "Rabaa Youssef Douss"], "title": "An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation", "categories": ["cs.CV"], "comment": null, "summary": "In large organizations, the number of financial transactions can grow\nrapidly, driving the need for fast and accurate multi-criteria invoice\nvalidation. Manual processing remains error-prone and time-consuming, while\ncurrent automated solutions are limited by their inability to support a variety\nof constraints, such as documents that are partially handwritten or\nphotographed with a mobile phone. In this paper, we propose to automate the\nvalidation of machine written invoices using document layout analysis and\nobject detection techniques based on recent deep learning (DL) models. We\nintroduce a novel dataset consisting of manually annotated real-world invoices\nand a multi-criteria validation process. We fine-tune and benchmark the most\nrelevant DL models on our dataset. Experimental results show the effectiveness\nof the proposed pipeline and selected DL models in terms of achieving fast and\naccurate validation of invoices."}
{"id": "2503.12269", "pdf": "https://arxiv.org/pdf/2503.12269", "abs": "https://arxiv.org/abs/2503.12269", "authors": ["Negar Shahamiri", "Moritz Rempe", "Lukas Heine", "Jens Kleesiek", "Fabian H√∂rst"], "title": "Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net", "categories": ["cs.CV"], "comment": null, "summary": "Automatic tissue segmentation and nuclei detection is an important task in\npathology, aiding in biomarker extraction and discovery. The panoptic\nsegmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to\nimprove tissue segmentation and nuclei detection in melanoma histopathology.\nUnlike many challenge submissions focusing on extensive model tuning, our\napproach emphasizes delivering a deployable solution within a 24-hour\ndevelopment timeframe, using out-of-the-box frameworks. The pipeline combines\ntwo models, namely CellViT++ for nuclei detection and nnU-Net for tissue\nsegmentation. Our results demonstrate a significant improvement in tissue\nsegmentation, achieving a Dice score of 0.750, surpassing the baseline score of\n0.629. For nuclei detection, we obtained results comparable to the baseline in\nboth challenge tracks. The code is publicly available at\nhttps://github.com/TIO-IKIM/PUMA."}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach."}
{"id": "2503.12281", "pdf": "https://arxiv.org/pdf/2503.12281", "abs": "https://arxiv.org/abs/2503.12281", "authors": ["Paola Natalia Ca√±as", "Marcos Nieto", "Oihana Otaegui", "Igor Rodr√≠guez"], "title": "Exploration of VLMs for Driver Monitoring Systems Applications", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 16th ITS European Congress, Seville, Spain, 19-21 May\n  2025", "summary": "In recent years, we have witnessed significant progress in emerging deep\nlearning models, particularly Large Language Models (LLMs) and Vision-Language\nModels (VLMs). These models have demonstrated promising results, indicating a\nnew era of Artificial Intelligence (AI) that surpasses previous methodologies.\nTheir extensive knowledge and zero-shot capabilities suggest a paradigm shift\nin developing deep learning solutions, moving from data capturing and algorithm\ntraining to just writing appropriate prompts. While the application of these\ntechnologies has been explored across various industries, including automotive,\nthere is a notable gap in the scientific literature regarding their use in\nDriver Monitoring Systems (DMS). This paper presents our initial approach to\nimplementing VLMs in this domain, utilising the Driver Monitoring Dataset to\nevaluate their performance and discussing their advantages and challenges when\nimplemented in real-world scenarios."}
{"id": "2503.12284", "pdf": "https://arxiv.org/pdf/2503.12284", "abs": "https://arxiv.org/abs/2503.12284", "authors": ["Krzysztof Byrski", "Grzegorz Wilczy≈Ñski", "Weronika Smolak-Dy≈ºewska", "Piotr Borycki", "Dawid Baran", "S≈Çawomir Tadeja", "Przemys≈Çaw Spurek"], "title": "REdiSplats: Ray Tracing for Editable Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has become one of the most important neural rendering\nalgorithms. GS represents 3D scenes using Gaussian components with trainable\ncolor and opacity. This representation achieves high-quality renderings with\nfast inference. Regrettably, it is challenging to integrate such a solution\nwith varying light conditions, including shadows and light reflections, manual\nadjustments, and a physical engine. Recently, a few approaches have appeared\nthat incorporate ray-tracing or mesh primitives into GS to address some of\nthese caveats. However, no such solution can simultaneously solve all the\nexisting limitations of the classical GS. Consequently, we introduce\nREdiSplats, which employs ray tracing and a mesh-based representation of flat\n3D Gaussians. In practice, we model the scene using flat Gaussian distributions\nparameterized by the mesh. We can leverage fast ray tracing and control\nGaussian modification by adjusting the mesh vertices. Moreover, REdiSplats\nallows modeling of light conditions, manual adjustments, and physical\nsimulation. Furthermore, we can render our models using 3D tools such as\nBlender or Nvdiffrast, which opens the possibility of integrating them with all\nexisting 3D graphics techniques dedicated to mesh representations."}
{"id": "2503.12303", "pdf": "https://arxiv.org/pdf/2503.12303", "abs": "https://arxiv.org/abs/2503.12303", "authors": ["Xiaoying Zhang", "Da Peng", "Yipeng Zhang", "Zonghao Guo", "Chengyue Wu", "Chi Chen", "Wei Ke", "Helen Meng", "Maosong Sun"], "title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs", "categories": ["cs.CV"], "comment": "38 pages", "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent pre-training approaches focus on enhancing perception by training on\nhigh-quality image captions due to the extremely high cost of collecting\nchain-of-thought (CoT) reasoning data for improving reasoning. While leveraging\nadvanced MLLMs for caption generation enhances scalability, the outputs often\nlack comprehensiveness and accuracy. In this paper, we introduce Self-Improving\nCognition (SIcog), a self-learning framework designed to construct\nnext-generation foundation MLLMs by enhancing their systematic cognitive\ncapabilities through multimodal pre-training with self-generated data.\nSpecifically, we propose chain-of-description, an approach that improves an\nMLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used to\nrefine the MLLM during multimodal pre-training, facilitating next-generation\nfoundation MLLM construction. Extensive experiments on both low- and\nhigh-resolution MLLMs across diverse benchmarks demonstrate that, with merely\n213K self-generated pre-training samples, SIcog produces next-generation\nfoundation MLLMs with significantly improved cognition, achieving\nbenchmark-leading performance compared to prevalent pre-training approaches."}
{"id": "2503.12307", "pdf": "https://arxiv.org/pdf/2503.12307", "abs": "https://arxiv.org/abs/2503.12307", "authors": ["Jiahao Wu", "Rui Peng", "Zhiyan Wang", "Lu Xiao", "Luyang Tang", "Jinbo Yan", "Kaiqiang Xiong", "Ronggang Wang"], "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps://github.com/WuJH2001/swift4d."}
{"id": "2503.12326", "pdf": "https://arxiv.org/pdf/2503.12326", "abs": "https://arxiv.org/abs/2503.12326", "authors": ["Maciej P. Polak", "Dane Morgan"], "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Automated data extraction from research texts has been steadily improving,\nwith the emergence of large language models (LLMs) accelerating progress even\nfurther. Extracting data from plots in research papers, however, has been such\na complex task that it has predominantly been confined to manual data\nextraction. We show that current multimodal large language models, with proper\ninstructions and engineered workflows, are capable of accurately extracting\ndata from plots. This capability is inherent to the pretrained models and can\nbe achieved with a chain-of-thought sequence of zero-shot engineered prompts we\ncall PlotExtract, without the need to fine-tune. We demonstrate PlotExtract\nhere and assess its performance on synthetic and published plots. We consider\nonly plots with two axes in this analysis. For plots identified as extractable,\nPlotExtract finds points with over 90% precision (and around 90% recall) and\nerrors in x and y position of around 5% or lower. These results prove that\nmultimodal LLMs are a viable path for high-throughput data extraction for plots\nand in many circumstances can replace the current manual methods of data\nextraction."}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io."}
{"id": "2503.12332", "pdf": "https://arxiv.org/pdf/2503.12332", "abs": "https://arxiv.org/abs/2503.12332", "authors": ["Yunze Liu", "Peiran Wu", "Cheng Liang", "Junxiao Shen", "Limin Wang", "Li Yi"], "title": "VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based architectures for video understanding demonstrate\npromising computational efficiency and competitive performance, yet struggle\nwith overfitting issues that hinder their scalability. To overcome this\nchallenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework\nfeaturing a novel pre-training approach. VideoMAP uses a 4:1\nMamba-to-Transformer ratio, effectively balancing computational cost and model\ncapacity. This architecture, combined with our proposed frame-wise masked\nautoregressive pre-training strategy, delivers significant performance gains\nwhen scaling to larger models. Additionally, VideoMAP exhibits impressive\nsample efficiency, significantly outperforming existing methods with less\ntraining data. Experiments show that VideoMAP outperforms existing models\nacross various datasets, including Kinetics-400, Something-Something V2,\nBreakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a\nvisual encoder for multimodal large language models, highlighting its ability\nto reduce memory usage and enable the processing of longer video sequences. The\ncode is open-source at https://github.com/yunzeliu/MAP"}
{"id": "2503.12335", "pdf": "https://arxiv.org/pdf/2503.12335", "abs": "https://arxiv.org/abs/2503.12335", "authors": ["Tengfei Wang", "Yongmao Hou", "Zhaoning Zhang", "Yiwei Xu", "Zongqian Zhan", "Xin Wang"], "title": "GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images", "categories": ["cs.CV"], "comment": "This paper has been submitted to IROS 2025", "summary": "Accurate geometric surface reconstruction, providing essential environmental\ninformation for navigation and manipulation tasks, is critical for enabling\nrobotic self-exploration and interaction. Recently, 3D Gaussian Splatting\n(3DGS) has gained significant attention in the field of surface reconstruction\ndue to its impressive geometric quality and computational efficiency. While\nrecent relevant advancements in novel view synthesis under inconsistent\nillumination using 3DGS have shown promise, the challenge of robust surface\nreconstruction under such conditions is still being explored. To address this\nchallenge, we propose a method called GS-3I. Specifically, to mitigate 3D\nGaussian optimization bias caused by underexposed regions in single-view\nimages, based on Convolutional Neural Network (CNN), a tone mapping correction\nframework is introduced. Furthermore, inconsistent lighting across multi-view\nimages, resulting from variations in camera settings and complex scene\nillumination, often leads to geometric constraint mismatches and deviations in\nthe reconstructed surface. To overcome this, we propose a normal compensation\nmechanism that integrates reference normals extracted from single-view image\nwith normals computed from multi-view observations to effectively constrain\ngeometric inconsistencies. Extensive experimental evaluations demonstrate that\nGS-3I can achieve robust and accurate surface reconstruction across complex\nillumination scenarios, highlighting its effectiveness and versatility in this\ncritical challenge. https://github.com/TFwang-9527/GS-3I"}
{"id": "2503.12343", "pdf": "https://arxiv.org/pdf/2503.12343", "abs": "https://arxiv.org/abs/2503.12343", "authors": ["Xiaoyu Xiong", "Changyu Hu", "Chunru Lin", "Pingchuan Ma", "Chuang Gan", "Tao Du"], "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues", "categories": ["cs.CV"], "comment": null, "summary": "We present TopoGaussian, a holistic, particle-based pipeline for inferring\nthe interior structure of an opaque object from easily accessible photos and\nvideos as input. Traditional mesh-based approaches require tedious and\nerror-prone mesh filling and fixing process, while typically output rough\nboundary surface. Our pipeline combines Gaussian Splatting with a novel,\nversatile particle-based differentiable simulator that simultaneously\naccommodates constitutive model, actuator, and collision, without interference\nwith mesh. Based on the gradients from this simulator, we provide flexible\nchoice of topology representation for optimization, including particle, neural\nimplicit surface, and quadratic surface. The resultant pipeline takes easily\naccessible photos and videos as input and outputs the topology that matches the\nphysical characteristics of the input. We demonstrate the efficacy of our\npipeline on a synthetic dataset and four real-world tasks with 3D-printed\nprototypes. Compared with existing mesh-based method, our pipeline is 5.26x\nfaster on average with improved shape quality. These results highlight the\npotential of our pipeline in 3D vision, soft robotics, and manufacturing\napplications."}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines."}
{"id": "2503.12350", "pdf": "https://arxiv.org/pdf/2503.12350", "abs": "https://arxiv.org/abs/2503.12350", "authors": ["Wenqing Kuang", "Xiongwei Zhao", "Yehui Shen", "Congcong Wen", "Huimin Lu", "Zongtan Zhou", "Xieyuanli Chen"], "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR."}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas."}
{"id": "2503.12356", "pdf": "https://arxiv.org/pdf/2503.12356", "abs": "https://arxiv.org/abs/2503.12356", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Se Young Chun"], "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025", "summary": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure."}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/."}
{"id": "2503.12381", "pdf": "https://arxiv.org/pdf/2503.12381", "abs": "https://arxiv.org/abs/2503.12381", "authors": ["Ruchika Sharma", "Rudresh Dwivedi"], "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN", "categories": ["cs.CV", "cs.MM"], "comment": "Submiited to journal", "summary": "Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision."}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO."}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community."}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000."}
{"id": "2503.12399", "pdf": "https://arxiv.org/pdf/2503.12399", "abs": "https://arxiv.org/abs/2503.12399", "authors": ["Jiangdong Cai", "Yan Chen", "Zhenrong Shen", "Haotian Jiang", "Honglin Xiong", "Kai Xuan", "Lichi Zhang", "Qian Wang"], "title": "Pathology Image Restoration via Mixture of Prompts", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In digital pathology, acquiring all-in-focus images is essential to\nhigh-quality imaging and high-efficient clinical workflow. Traditional scanners\nachieve this by scanning at multiple focal planes of varying depths and then\nmerging them, which is relatively slow and often struggles with complex tissue\ndefocus. Recent prevailing image restoration technique provides a means to\nrestore high-quality pathology images from scans of single focal planes.\nHowever, existing image restoration methods are inadequate, due to intricate\ndefocus patterns in pathology images and their domain-specific semantic\ncomplexities. In this work, we devise a two-stage restoration solution\ncascading a transformer and a diffusion model, to benefit from their powers in\npreserving image fidelity and perceptual quality, respectively. We particularly\npropose a novel mixture of prompts for the two-stage solution. Given initial\nprompt that models defocus in microscopic imaging, we design two prompts that\ndescribe the high-level image semantics from pathology foundation model and the\nfine-grained tissue structures via edge extraction. We demonstrate that, by\nfeeding the prompt mixture to our method, we can restore high-quality pathology\nimages from single-focal-plane scans, implying high potentials of the mixture\nof prompts to clinical usage. Code will be publicly available at\nhttps://github.com/caijd2000/MoP."}
{"id": "2503.12401", "pdf": "https://arxiv.org/pdf/2503.12401", "abs": "https://arxiv.org/abs/2503.12401", "authors": ["Jianwei Zhao", "Xin Li", "Fan Yang", "Qiang Zhai", "Ao Luo", "Yang Zhao", "Hong Cheng", "Huazhu Fu"], "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks."}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition."}
{"id": "2503.12418", "pdf": "https://arxiv.org/pdf/2503.12418", "abs": "https://arxiv.org/abs/2503.12418", "authors": ["Shuo Gao", "Jingyang Zhang", "Jun Xue", "Meng Yang", "Yang Chen", "Guangquan Zhou"], "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, conference", "summary": "Carotid atherosclerosis represents a significant health risk, with its early\ndiagnosis primarily dependent on ultrasound-based assessments of carotid\nintima-media thickening. However, during carotid ultrasound screening,\nsignificant view variations cause style shifts, impairing content cues related\nto thickening, such as lumen anatomy, which introduces spurious correlations\nthat hinder assessment. Therefore, we propose a novel causal-inspired method\nfor assessing carotid intima-media thickening in frame-wise ultrasound videos,\nwhich focuses on two aspects: eliminating spurious correlations caused by style\nand enhancing causal content correlations. Specifically, we introduce a novel\nSpurious Correlation Elimination (SCE) module to remove non-causal style\neffects by enforcing prediction invariance with style perturbations.\nSimultaneously, we propose a Causal Equivalence Consolidation (CEC) module to\nstrengthen causal content correlation through adversarial optimization during\ncontent randomization. Simultaneously, we design a Causal Transition\nAugmentation (CTA) module to ensure smooth causal flow by integrating an\nauxiliary pathway with text prompts and connecting it through contrastive\nlearning. The experimental results on our in-house carotid ultrasound video\ndataset achieved an accuracy of 86.93\\%, demonstrating the superior performance\nof the proposed method. Code is available at\n\\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}."}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture."}
{"id": "2503.12441", "pdf": "https://arxiv.org/pdf/2503.12441", "abs": "https://arxiv.org/abs/2503.12441", "authors": ["Yuda Zou", "Zelong Liu", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "title": "Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "Crowd counting and localization are important in applications such as public\nsecurity and traffic management. Existing methods have achieved impressive\nresults thanks to extensive laborious annotations. This paper propose a novel\npoint-localization-based semi-supervised crowd counting and localization method\ntermed Consistent-Point. We identify and address two inconsistencies of\npseudo-points, which have not been adequately explored. To enhance their\nposition consistency, we aggregate the positions of neighboring auxiliary\nproposal-points. Additionally, an instance-wise uncertainty calibration is\nproposed to improve the class consistency of pseudo-points. By generating more\nconsistent pseudo-points, Consistent-Point provides more stable supervision to\nthe training process, yielding improved results. Extensive experiments across\nfive widely used datasets and three different labeled ratio settings\ndemonstrate that our method achieves state-of-the-art performance in crowd\nlocalization while also attaining impressive crowd counting results. The code\nwill be available."}
{"id": "2503.12446", "pdf": "https://arxiv.org/pdf/2503.12446", "abs": "https://arxiv.org/abs/2503.12446", "authors": ["Tianle Li", "Yongming Rao", "Winston Hu", "Yu Cheng"], "title": "BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Encoder-free multimodal large language models(MLLMs) eliminate the need for a\nwell-trained vision encoder by directly processing image tokens before the\nlanguage model. While this approach reduces computational overhead and model\ncomplexity, it often requires large amounts of training data to effectively\ncapture the visual knowledge typically encoded by vision models like CLIP. The\nabsence of a vision encoder implies that the model is likely to rely on\nsubstantial data to learn the necessary visual-semantic alignments. In this\nwork, we present BREEN, a data-efficient encoder-free multimodal architecture\nthat mitigates this issue. BREEN leverages a learnable query and image experts\nto achieve comparable performance with significantly less training data. The\nlearnable query, positioned between image and text tokens, is supervised by the\noutput of a pretrained CLIP model to distill visual knowledge, bridging the gap\nbetween visual and textual modalities. Additionally, the image expert processes\nimage tokens and learnable queries independently, improving efficiency and\nreducing interference with the LLM's textual capabilities. BREEN achieves\ncomparable performance to prior encoder-free state-of-the-art models like\nMono-InternVL, using only 13 million text-image pairs in training about one\npercent of the data required by existing methods. Our work highlights a\npromising direction for data-efficient encoder-free multimodal learning,\noffering an alternative to traditional encoder-based approaches."}
{"id": "2503.12447", "pdf": "https://arxiv.org/pdf/2503.12447", "abs": "https://arxiv.org/abs/2503.12447", "authors": ["Li Yicong"], "title": "Causality Model for Semantic Understanding on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "PhD Thesis", "summary": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA)."}
{"id": "2503.12450", "pdf": "https://arxiv.org/pdf/2503.12450", "abs": "https://arxiv.org/abs/2503.12450", "authors": ["Feihong Yan", "Qingyan Wei", "Jiayi Tang", "Jiajun Li", "Yulin Wang", "Xuming Hu", "Huiqi Li", "Linfeng Zhang"], "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."}
{"id": "2503.12451", "pdf": "https://arxiv.org/pdf/2503.12451", "abs": "https://arxiv.org/abs/2503.12451", "authors": ["Hossein Ranjbar", "Alireza Taheri"], "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons."}
{"id": "2503.12453", "pdf": "https://arxiv.org/pdf/2503.12453", "abs": "https://arxiv.org/abs/2503.12453", "authors": ["Edgar Heinert", "Thomas Gottwald", "Annika M√ºtze", "Matthias Rottmann"], "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs."}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD."}
{"id": "2503.12461", "pdf": "https://arxiv.org/pdf/2503.12461", "abs": "https://arxiv.org/abs/2503.12461", "authors": ["Fanhu Zeng", "Hao Tang", "Yihua Shao", "Siyu Chen", "Ling Shao", "Yan Wang"], "title": "MambaIC: State Space Models for High-Performance Learned Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to CVPR 2025", "summary": "A high-performance image compression algorithm is crucial for real-time\ninformation transmission across numerous fields. Despite rapid progress in\nimage compression, computational inefficiency and poor redundancy modeling\nstill pose significant bottlenecks, limiting practical applications. Inspired\nby the effectiveness of state space models (SSMs) in capturing long-range\ndependencies, we leverage SSMs to address computational inefficiency in\nexisting methods and improve image compression from multiple perspectives. In\nthis paper, we integrate the advantages of SSMs for better\nefficiency-performance trade-off and propose an enhanced image compression\napproach through refined context modeling, which we term MambaIC. Specifically,\nwe explore context modeling to adaptively refine the representation of hidden\nstates. Additionally, we introduce window-based local attention into\nchannel-spatial entropy modeling to reduce potential spatial redundancy during\ncompression, thereby increasing efficiency. Comprehensive qualitative and\nquantitative results validate the effectiveness and efficiency of our approach,\nparticularly for high-resolution image compression. Code is released at\nhttps://github.com/AuroraZengfh/MambaIC."}
{"id": "2503.12464", "pdf": "https://arxiv.org/pdf/2503.12464", "abs": "https://arxiv.org/abs/2503.12464", "authors": ["Alessio Xompero", "Andrea Cavallaro"], "title": "Learning Privacy from Visual Entities", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages (13 for the main article, 8 for bibliography, acks,\n  appendixes), 9 figures, 12 tables. Article accepted and to appear in the\n  Proceedings on Privacy Enhancing Technologies, 2025 (3):\n  https://petsymposium.org/popets/2025/. To be presented at the Privacy\n  Enhancing Technologies Symposium 2025. Artifact (source code) under review:\n  https://github.com/graphnex/privacy-from-visual-entities", "summary": "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes."}
{"id": "2503.12470", "pdf": "https://arxiv.org/pdf/2503.12470", "abs": "https://arxiv.org/abs/2503.12470", "authors": ["Han Mei", "Kunqian Li", "Shuaixin Liu", "Chengzhi Ma", "Qianli Jiang"], "title": "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https://github.com/OUCVisionGroup/DPF-Net."}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE"}
{"id": "2503.12485", "pdf": "https://arxiv.org/pdf/2503.12485", "abs": "https://arxiv.org/abs/2503.12485", "authors": ["Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Hezhen Hu", "Wengang Zhou", "Houqiang Li"], "title": "Cross-Modal Consistency Learning for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminate background perturbation but\ninevitably suffer from insufficient semantic cues compared to raw RGB videos.\nNevertheless, direct representation learning only from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public."}
{"id": "2503.12490", "pdf": "https://arxiv.org/pdf/2503.12490", "abs": "https://arxiv.org/abs/2503.12490", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Bin Chen", "Zian Guan", "Yuhao Wang", "Xu Jia", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing."}
{"id": "2503.12492", "pdf": "https://arxiv.org/pdf/2503.12492", "abs": "https://arxiv.org/abs/2503.12492", "authors": ["Dapeng Zhao"], "title": "Geometry-Aware Face Reconstruction Under Occluded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods."}
{"id": "2503.12494", "pdf": "https://arxiv.org/pdf/2503.12494", "abs": "https://arxiv.org/abs/2503.12494", "authors": ["Dapeng Zhao"], "title": "Learning Contour-Guided 3D Face Reconstruction with Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods."}
{"id": "2503.12495", "pdf": "https://arxiv.org/pdf/2503.12495", "abs": "https://arxiv.org/abs/2503.12495", "authors": ["Xuan Ma", "Zewen Lv", "Chengcai Ma", "Tao Zhang", "Yuelan Xin", "Kun Zhan"], "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau", "categories": ["cs.CV"], "comment": "Journal of Applied Remote Sensing, 2025", "summary": "Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a\nsignificant environmental challenge due to overgrazing, climate change, and\nrodent activity, which degrade vegetation cover and soil quality. These\nextremely degraded grassland on QTP, commonly referred to as black-soil area,\nrequire accurate assessment to guide effective restoration efforts. In this\npaper, we present a newly created QTP black-soil dataset, annotated under\nexpert guidance. We introduce a novel neural network model, BS-Mamba,\nspecifically designed for the black-soil area detection using UAV remote\nsensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying\nblack-soil area across two independent test datasets than the state-of-the-art\nmodels. This research contributes to grassland restoration by providing an\nefficient method for assessing the extent of black-soil area on the QTP."}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain."}
{"id": "2503.12507", "pdf": "https://arxiv.org/pdf/2503.12507", "abs": "https://arxiv.org/abs/2503.12507", "authors": ["Guangqian Guo", "Yoong Guo", "Xuehui Yu", "Wenbo Li", "Yaoxing Wang", "Shan Gao"], "title": "Segment Any-Quality Images with Generative Latent Space Enhancement", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset."}
{"id": "2503.12515", "pdf": "https://arxiv.org/pdf/2503.12515", "abs": "https://arxiv.org/abs/2503.12515", "authors": ["Pan Du", "Delin An", "Chaoli Wang", "Jian-Xun Wang"], "title": "AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "comment": "42 pages, 8 figures", "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings."}
{"id": "2503.12519", "pdf": "https://arxiv.org/pdf/2503.12519", "abs": "https://arxiv.org/abs/2503.12519", "authors": ["Taein Kwon", "Zador Pataki", "Mahdi Rad", "Marc Pollefeys"], "title": "Multi Activity Sequence Alignment via Implicit Clustering", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Self-supervised temporal sequence alignment can provide rich and effective\nrepresentations for a wide range of applications. However, existing methods for\nachieving optimal performance are mostly limited to aligning sequences of the\nsame activity only and require separate models to be trained for each activity.\nWe propose a novel framework that overcomes these limitations using sequence\nalignment via implicit clustering. Specifically, our key idea is to perform\nimplicit clip-level clustering while aligning frames in sequences. This coupled\nwith our proposed dual augmentation technique enhances the network's ability to\nlearn generalizable and discriminative representations. Our experiments show\nthat our proposed method outperforms state-of-the-art results and highlight the\ngeneralization capability of our framework with multi activity and different\nmodalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will\nrelease our code upon acceptance."}
{"id": "2503.12526", "pdf": "https://arxiv.org/pdf/2503.12526", "abs": "https://arxiv.org/abs/2503.12526", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditID, a training-free approach based on the DiT architecture,\nwhich achieves highly editable customized IDs for text to image generation.\nExisting text-to-image models for customized IDs typically focus more on ID\nconsistency while neglecting editability. It is challenging to alter facial\norientation, character attributes, and other features through prompts. EditID\naddresses this by deconstructing the text-to-image model for customized IDs\ninto an image generation branch and a character feature branch. The character\nfeature branch is further decoupled into three modules: feature extraction,\nfeature fusion, and feature integration. By introducing a combination of\nmapping features and shift features, along with controlling the intensity of ID\nfeature integration, EditID achieves semantic compression of local features\nacross network depths, forming an editable feature space. This enables the\nsuccessful generation of high-quality images with editable IDs while\nmaintaining ID consistency, achieving excellent results in the IBench\nevaluation, which is an editability evaluation framework for the field of\ncustomized ID text-to-image generation that quantitatively demonstrates the\nsuperior performance of EditID. EditID is the first text-to-image solution to\npropose customizable ID editability on the DiT architecture, meeting the\ndemands of long prompts and high quality image generation."}
{"id": "2503.12527", "pdf": "https://arxiv.org/pdf/2503.12527", "abs": "https://arxiv.org/abs/2503.12527", "authors": ["Yang Yi", "Kunqing Wang", "Jinpu Zhang", "Zhen Tan", "Xiangke Wang", "Hui Shen", "Dewen Hu"], "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}."}
{"id": "2503.12531", "pdf": "https://arxiv.org/pdf/2503.12531", "abs": "https://arxiv.org/abs/2503.12531", "authors": ["Mehmet Kerem Turkcan", "Mattia Ballo", "Filippo Filicori", "Zoran Kostic"], "title": "Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks", "categories": ["cs.CV"], "comment": null, "summary": "We introduce specialized diffusion-based generative models that capture the\nspatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions\nthrough supervised learning on annotated laparoscopic surgery footage. The\nproposed models form a foundation for data-driven world models capable of\nsimulating the biomechanical interactions and procedural dynamics of surgical\nsuturing with high temporal fidelity. Annotating a dataset of $\\sim2K$ clips\nextracted from simulation videos, we categorize surgical actions into\nfine-grained sub-stitch classes including ideal and non-ideal executions of\nneedle positioning, targeting, driving, and withdrawal. We fine-tune two\nstate-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to\ngenerate high-fidelity surgical action sequences at $\\ge$768x512 resolution and\n$\\ge$49 frames. For training our models, we explore both Low-Rank Adaptation\n(LoRA) and full-model fine-tuning approaches. Our experimental results\ndemonstrate that these world models can effectively capture the dynamics of\nsuturing, potentially enabling improved training simulators, surgical skill\nassessment tools, and autonomous surgical systems. The models also display the\ncapability to differentiate between ideal and non-ideal technique execution,\nproviding a foundation for building surgical training and evaluation systems.\nWe release our models for testing and as a foundation for future research.\nProject Page: https://mkturkcan.github.io/suturingmodels/"}
{"id": "2503.12532", "pdf": "https://arxiv.org/pdf/2503.12532", "abs": "https://arxiv.org/abs/2503.12532", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Ziqin Wei", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE."}
{"id": "2503.12535", "pdf": "https://arxiv.org/pdf/2503.12535", "abs": "https://arxiv.org/abs/2503.12535", "authors": ["Guibiao Liao", "Qing Li", "Zhenyu Bao", "Guoping Qiu", "Kanglin Liu"], "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. The project page is available at\n  https://gbliao.github.io/SPC-GS.github.io/", "summary": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches\nhave shown significant performance with dense input images. However, they\nexhibit poor performance when confronted with sparse inputs, primarily due to\nthe sparse distribution of Gaussian points and insufficient view supervision.\nTo relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based\nGaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)\nRegularization for open-world free view synthesis with sparse inputs.\nSpecifically, SGI provides a dense, scene-layout-based Gaussian distribution by\nutilizing view-changed images generated from the video generation model and\nview-constraint Gaussian points densification. Additionally, SPC mitigates\nlimited view supervision by employing semantic-prompt-based consistency\nconstraints developed by SAM2. This approach leverages available semantics from\ntraining views, serving as instructive prompts, to optimize visually\noverlapping regions in novel views with 2D and 3D consistency constraints.\nExtensive experiments demonstrate the superior performance of SPC-GS across\nReplica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in\nPSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world\nsemantic segmentation."}
{"id": "2503.12539", "pdf": "https://arxiv.org/pdf/2503.12539", "abs": "https://arxiv.org/abs/2503.12539", "authors": ["Weiguang Zhao", "Rui Zhang", "Qiufeng Wang", "Guangliang Cheng", "Kaizhu Huang"], "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps://github.com/weiguangzhao/BFANet."}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research."}
{"id": "2503.12545", "pdf": "https://arxiv.org/pdf/2503.12545", "abs": "https://arxiv.org/abs/2503.12545", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Xiaojiang Peng", "Kai Wang", "Yang You", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs."}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindstr√∂m", "Peng Su", "Matthias Nie√üner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public."}
{"id": "2503.12559", "pdf": "https://arxiv.org/pdf/2503.12559", "abs": "https://arxiv.org/abs/2503.12559", "authors": ["Xiao Wang", "Qingyi Si", "Jianlong Wu", "Shiyu Zhu", "Li Cao", "Liqiang Nie"], "title": "AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized video\nunderstanding, yet are still limited by context length when processing long\nvideos. Recent methods compress videos by leveraging visual redundancy\nuniformly, yielding promising results. Nevertheless, our quantitative analysis\nshows that redundancy varies significantly across time and model layers,\nnecessitating a more flexible compression strategy. We propose AdaReTaKe, a\ntraining-free method that flexibly reduces visual redundancy by allocating\ncompression ratios among time and layers with theoretical guarantees.\nIntegrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity\nfrom 256 to 2048 frames while preserving critical information. Experiments on\nVideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe\noutperforms existing methods by 2.3% and 2.8% for 7B and 72B models,\nrespectively, with even greater improvements of 5.9% and 6.0% on the longest\nLVBench. Our code is available at\nhttps://github.com/SCZwangxiao/video-FlexReduc.git."}
{"id": "2503.12562", "pdf": "https://arxiv.org/pdf/2503.12562", "abs": "https://arxiv.org/abs/2503.12562", "authors": ["Ruopeng Gao", "Yuyao Wang", "Chunxu Liu", "Limin Wang"], "title": "History-Aware Transformation of ReID Features for Multiple Object Tracking", "categories": ["cs.CV"], "comment": "Tech report. Without bells and whistles, achieving 80.8 HOTA on\n  SportsMOT", "summary": "The aim of multiple object tracking (MOT) is to detect all objects in a video\nand bind them into multiple trajectories. Generally, this process is carried\nout in two steps: detecting objects and associating them across frames based on\nvarious cues and metrics. Many studies and applications adopt object\nappearance, also known as re-identification (ReID) features, for target\nmatching through straightforward similarity calculation. However, we argue that\nthis practice is overly naive and thus overlooks the unique characteristics of\nMOT tasks. Unlike regular re-identification tasks that strive to distinguish\nall potential targets in a general representation, multi-object tracking\ntypically immerses itself in differentiating similar targets within the same\nvideo sequence. Therefore, we believe that seeking a more suitable feature\nrepresentation space based on the different sample distributions of each\nsequence will enhance tracking performance. In this paper, we propose using\nhistory-aware transformations on ReID features to achieve more discriminative\nappearance representations. Specifically, we treat historical trajectory\nfeatures as conditions and employ a tailored Fisher Linear Discriminant (FLD)\nto find a spatial projection matrix that maximizes the differentiation between\ndifferent trajectories. Our extensive experiments reveal that this\ntraining-free projection can significantly boost feature-only trackers to\nachieve competitive, even superior tracking performance compared to\nstate-of-the-art methods while also demonstrating impressive zero-shot transfer\ncapabilities. This demonstrates the effectiveness of our proposal and further\nencourages future investigation into the importance and customization of ReID\nmodels in multiple object tracking. The code will be released at\nhttps://github.com/HELLORPG/HATReID-MOT."}
{"id": "2503.12567", "pdf": "https://arxiv.org/pdf/2503.12567", "abs": "https://arxiv.org/abs/2503.12567", "authors": ["Abyad Enan", "Mashrur Chowdhury"], "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) for possible publication", "summary": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel."}
{"id": "2503.12572", "pdf": "https://arxiv.org/pdf/2503.12572", "abs": "https://arxiv.org/abs/2503.12572", "authors": ["Francesco Girlanda", "Denys Rozumnyi", "Marc Pollefeys", "Martin R. Oswald"], "title": "Deblur Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp\nreconstructions from motion-blurred inputs. The proposed method bridges the\nstrengths of both frame-to-frame and frame-to-model approaches to model\nsub-frame camera trajectories that lead to high-fidelity reconstructions in\nmotion-blurred settings. Moreover, our pipeline incorporates techniques such as\nonline loop closure and global bundle adjustment to achieve a dense and precise\nglobal trajectory. We model the physical image formation process of\nmotion-blurred images and minimize the error between the observed blurry images\nand rendered blurry images obtained by averaging sharp virtual sub-frame\nimages. Additionally, by utilizing a monocular depth estimator alongside the\nonline deformation of Gaussians, we ensure precise mapping and enhanced image\ndeblurring. The proposed SLAM pipeline integrates all these components to\nimprove the results. We achieve state-of-the-art results for sharp map\nestimation and sub-frame trajectory recovery both on synthetic and real-world\nblurry input data."}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO."}
{"id": "2503.12588", "pdf": "https://arxiv.org/pdf/2503.12588", "abs": "https://arxiv.org/abs/2503.12588", "authors": ["Xiaoyu Han", "Shengping Zhang", "Qinglin Liu", "Zonglin Li", "Chenyang Wang"], "title": "Progressive Limb-Aware Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2022. The code is available at\n  https://github.com/xyhanHIT/PL-VTON", "summary": "Existing image-based virtual try-on methods directly transfer specific\nclothing to a human image without utilizing clothing attributes to refine the\ntransferred clothing geometry and textures, which causes incomplete and blurred\nclothing appearances. In addition, these methods usually mask the limb textures\nof the input for the clothing-agnostic person representation, which results in\ninaccurate predictions for human limb regions (i.e., the exposed arm skin),\nespecially when transforming between long-sleeved and short-sleeved garments.\nTo address these problems, we present a progressive virtual try-on framework,\nnamed PL-VTON, which performs pixel-level clothing warping based on multiple\nattributes of clothing and embeds explicit limb-aware features to generate\nphoto-realistic try-on results. Specifically, we design a Multi-attribute\nClothing Warping (MCW) module that adopts a two-stage alignment strategy based\non multiple attributes to progressively estimate pixel-level clothing\ndisplacements. A Human Parsing Estimator (HPE) is then introduced to\nsemantically divide the person into various regions, which provides structural\nconstraints on the human body and therefore alleviates texture bleeding between\nclothing and limb regions. Finally, we propose a Limb-aware Texture Fusion\n(LTF) module to estimate high-quality details in limb regions by fusing\ntextures of the clothing and the human body with the guidance of explicit\nlimb-aware features. Extensive experiments demonstrate that our proposed method\noutperforms the state-of-the-art virtual try-on methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON."}
{"id": "2503.12590", "pdf": "https://arxiv.org/pdf/2503.12590", "abs": "https://arxiv.org/abs/2503.12590", "authors": ["Haoran Feng", "Zehuan Huang", "Lin Li", "Hairong Lv", "Lu Sheng"], "title": "Personalize Anything for Free with Diffusion Transformer", "categories": ["cs.CV"], "comment": "https://fenghora.github.io/Personalize-Anything-Page/", "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose \\textbf{Personalize\nAnything}, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization."}
{"id": "2503.12595", "pdf": "https://arxiv.org/pdf/2503.12595", "abs": "https://arxiv.org/abs/2503.12595", "authors": ["Dan Halperin", "Niklas Eisl"], "title": "Point Cloud Based Scene Segmentation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency."}
{"id": "2503.12605", "pdf": "https://arxiv.org/pdf/2503.12605", "abs": "https://arxiv.org/abs/2503.12605", "authors": ["Yaoting Wang", "Shengqiong Wu", "Yuecheng Zhang", "William Wang", "Ziwei Liu", "Jiebo Luo", "Hao Fei"], "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "survey resource at\n  https://github.com/yaotingwangofficial/Awesome-MCoT; 12 figures, 4 tables, 44\n  pages", "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI."}
{"id": "2503.11731", "pdf": "https://arxiv.org/pdf/2503.11731", "abs": "https://arxiv.org/abs/2503.11731", "authors": ["Xianming Zeng", "Sicong Du", "Qifeng Chen", "Lizhe Liu", "Haoyu Shu", "Jiaxuan Gao", "Jiarun Liu", "Jiulong Xu", "Jianyun Xu", "Mingxia Chen", "Yiru Zhao", "Peng Chen", "Yapeng Xue", "Chunming Zhao", "Sheng Yang", "Qiang Li"], "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor simulation is pivotal for scalable validation of autonomous driving\nsystems, yet existing Neural Radiance Fields (NeRF) based methods face\napplicability and efficiency challenges in industrial workflows. This paper\nintroduces a Gaussian Splatting (GS) based system to address these challenges:\nWe first break down sensor simulator components and analyze the possible\nadvantages of GS over NeRF. Then in practice, we refactor three crucial\ncomponents through GS, to leverage its explicit scene representation and\nreal-time rendering: (1) choosing the 2D neural Gaussian representation for\nphysics-compliant scene and sensor modeling, (2) proposing a scene editing\npipeline to leverage Gaussian primitives library for data augmentation, and (3)\ncoupling a controllable diffusion model for scene expansion and harmonization.\nWe implement this framework on a proprietary autonomous driving dataset\nsupporting cameras and LiDAR sensors. We demonstrate through ablation studies\nthat our approach reduces frame-wise simulation latency, achieves better\ngeometric and photometric consistency, and enables interpretable explicit scene\nediting and expansion. Furthermore, we showcase how integrating such a GS-based\nsensor simulator with traffic and dynamic simulators enables full-stack testing\nof end-to-end autonomy algorithms. Our work provides both algorithmic insights\nand practical validation, establishing GS as a cornerstone for industrial-grade\nsensor simulation."}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Inc√†", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones."}
{"id": "2503.11750", "pdf": "https://arxiv.org/pdf/2503.11750", "abs": "https://arxiv.org/abs/2503.11750", "authors": ["Shuyang Hao", "Yiwei Wang", "Bryan Hooi", "Jun Liu", "Muhao Chen", "Zi Huang", "Yujun Cai"], "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."}
{"id": "2503.11780", "pdf": "https://arxiv.org/pdf/2503.11780", "abs": "https://arxiv.org/abs/2503.11780", "authors": ["Tianyi Zhao", "Boyang Liu", "Yanglei Gao", "Yiming Sun", "Maoxun Yuan", "Xingxing Wei"], "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem that the\ndecreased feature extraction capability in multi-modal joint learning. This\nleads to an unreasonable but prevalent phenomenon--Fusion Degradation, which\nhinders the performance improvement of the MMOD model. Motivated by this, in\nthis paper, we introduce linear probing evaluation to the multi-modal detectors\nand rethink the multi-modal object detection task from the mono-modality\nlearning perspective. Therefore, we construct an novel framework called\nM$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method\nand the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework\nfacilitates the sufficient learning of mono-modality during multi-modal joint\ntraining and explores a lightweight yet effective feature fusion manner to\nachieve superior object detection performance. Extensive experiments conducted\non three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates\nthe Fusion Degradation phenomenon and outperforms the previous SOTA detectors."}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN"}
{"id": "2503.11787", "pdf": "https://arxiv.org/pdf/2503.11787", "abs": "https://arxiv.org/abs/2503.11787", "authors": ["Samuel W. Remedios", "Shuwen Wei", "Shuo Han", "Jinwei Zhang", "Aaron Carass", "Kurt G. Schilling", "Dzung L. Pham", "Jerry L. Prince", "Blake E. Dewey"], "title": "ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In clinical imaging, magnetic resonance (MR) image volumes are often acquired\nas stacks of 2D slices, permitting decreased scan times, improved\nsignal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.\nWhile this is sufficient for clinical evaluation, automated algorithms designed\nfor 3D analysis perform sub-optimally on 2D-acquired scans, especially those\nwith thick slices and gaps between slices. Super-resolution (SR) methods aim to\naddress this problem, but previous methods do not address all of the following:\nslice profile shape estimation, slice gap, domain shift, and non-integer /\narbitrary upsampling factors. In this paper, we propose ECLARE (Efficient\nCross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method\nthat addresses each of these factors. ECLARE estimates the slice profile from\nthe 2D-acquired multi-slice MR volume, trains a network to learn the mapping\nfrom low-resolution to high-resolution in-plane patches from the same volume,\nand performs SR with anti-aliasing. We compared ECLARE to cubic B-spline\ninterpolation, SMORE, and other contemporary SR methods. We used realistic and\nrepresentative simulations so that quantitative performance against a ground\ntruth could be computed, and ECLARE outperformed all other methods in both\nsignal recovery and downstream tasks. On real data for which there is no ground\ntruth, ECLARE demonstrated qualitative superiority over other methods as well.\nImportantly, as ECLARE does not use external training data it cannot suffer\nfrom domain shift between training and testing. Our code is open-source and\navailable at https://www.github.com/sremedios/eclare."}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus."}
{"id": "2503.11794", "pdf": "https://arxiv.org/pdf/2503.11794", "abs": "https://arxiv.org/abs/2503.11794", "authors": ["Bangzheng Li", "Fei Wang", "Wenxuan Zhou", "Nan Xu", "Ben Zhou", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*."}
{"id": "2503.11806", "pdf": "https://arxiv.org/pdf/2503.11806", "abs": "https://arxiv.org/abs/2503.11806", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "categories": ["cs.CV"], "comment": "Project page: https://www.projectaria.com/scenescript/", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts."}
{"id": "2503.11807", "pdf": "https://arxiv.org/pdf/2503.11807", "abs": "https://arxiv.org/abs/2503.11807", "authors": ["Sanayya A", "Amoolya Shetty", "Abhijeet Sharma", "Venkatesh Ravichandran", "Masthan Wali Gosuvarapalli", "Sarthak Jain", "Priyamvada Nanjundiah", "Ujjal Kr Dutta", "Divya Sharma"], "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024", "summary": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making."}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taix√©", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM."}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign."}
{"id": "2503.11893", "pdf": "https://arxiv.org/pdf/2503.11893", "abs": "https://arxiv.org/abs/2503.11893", "authors": ["Md Abu Bakr Siddique", "Junliang Liu", "Piyush Singh", "Md Jahidul Islam"], "title": "UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The concept of waterbody style transfer remains largely unexplored in the\nunderwater imaging and vision literature. Traditional image style transfer\n(STx) methods primarily focus on artistic and photorealistic blending, often\nfailing to preserve object and scene geometry in images captured in\nhigh-scattering mediums such as underwater. The wavelength-dependent nonlinear\nattenuation and depth-dependent backscattering artifacts further complicate\nlearning underwater image STx from unpaired data. This paper introduces UStyle,\nthe first data-driven learning framework for transferring waterbody styles\nacross underwater images without requiring prior reference images or scene\ninformation. We propose a novel depth-aware whitening and coloring transform\n(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure\nperceptually consistent stylization while preserving scene structure. To\nenhance style transfer quality, we incorporate carefully designed loss\nfunctions that guide UStyle to maintain colorfulness, lightness, structural\nintegrity, and frequency-domain characteristics, as well as high-level content\nin VGG and CLIP (contrastive language-image pretraining) feature spaces. By\naddressing domain-specific challenges, UStyle provides a robust framework for\nno-reference underwater image STx, surpassing state-of-the-art (SOTA) methods\nthat rely solely on end-to-end reconstruction loss. Furthermore, we introduce\nthe UF7D dataset, a curated collection of high-resolution underwater images\nspanning seven distinct waterbody styles, establishing a benchmark to support\nfuture research in underwater image STx. The UStyle inference pipeline and UF7D\ndataset are released at: https://github.com/uf-robopi/UStyle."}
{"id": "2503.11905", "pdf": "https://arxiv.org/pdf/2503.11905", "abs": "https://arxiv.org/abs/2503.11905", "authors": ["Ruchika Chavhan", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Luca Morreale", "Mehdi Noroozi", "Sourav Bhattacharya"], "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Text-to-image synthesis has witnessed remarkable advancements in recent\nyears. Many attempts have been made to adopt text-to-image models to support\nmultiple tasks. However, existing approaches typically require\nresource-intensive re-training or additional parameters to accommodate for the\nnew tasks, which makes the model inefficient for on-device deployment. We\npropose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends\nthe capabilities of a pre-trained text-to-image diffusion model to support a\nvariety of image-to-image generation tasks. MTU replaces Feed-Forward Network\n(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,\nand combines them with a dynamic routing mechanism. To the best of our\nknowledge, MTU is the first multi-task diffusion modeling approach that\nseamlessly blends multi-tasking with on-device compatibility, by mitigating the\nissue of parameter inflation. We show that the performance of MTU is on par\nwith the single-task fine-tuned diffusion models across several tasks including\nimage editing, super-resolution, and inpainting, while maintaining similar\nlatency and computational load (GFLOPs) as the single-task fine-tuned models."}
{"id": "2503.11906", "pdf": "https://arxiv.org/pdf/2503.11906", "abs": "https://arxiv.org/abs/2503.11906", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Emanuele Salerno"], "title": "A Survey on SAR ship classification using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to JSTARS journal", "summary": "Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture\nRadar (SAR) ship classification. This survey comprehensively analyzes the\ndiverse DL techniques employed in this domain. We identify critical trends and\nchallenges, highlighting the importance of integrating handcrafted features,\nutilizing public datasets, data augmentation, fine-tuning, explainability\ntechniques, and fostering interdisciplinary collaborations to improve DL model\nperformance. This survey establishes a first-of-its-kind taxonomy for\ncategorizing relevant research based on DL models, handcrafted feature use, SAR\nattribute utilization, and the impact of fine-tuning. We discuss the\nmethodologies used in SAR ship classification tasks and the impact of different\ntechniques. Finally, the survey explores potential avenues for future research,\nincluding addressing data scarcity, exploring novel DL architectures,\nincorporating interpretability techniques, and establishing standardized\nperformance metrics. By addressing these challenges and leveraging advancements\nin DL, researchers can contribute to developing more accurate and efficient\nship classification systems, ultimately enhancing maritime surveillance and\nrelated applications."}
{"id": "2503.11919", "pdf": "https://arxiv.org/pdf/2503.11919", "abs": "https://arxiv.org/abs/2503.11919", "authors": ["Jeonghwan Park", "Kang Li", "Huiyu Zhou"], "title": "k-fold Subsampling based Sequential Backward Feature Elimination", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy."}
{"id": "2503.11930", "pdf": "https://arxiv.org/pdf/2503.11930", "abs": "https://arxiv.org/abs/2503.11930", "authors": ["Jingxuan Zhang", "Robert J. Hart", "Ziqian Bi", "Shiaofen Fang", "Susan Walsh"], "title": "Generating a Biometrically Unique and Realistic Iris Database", "categories": ["cs.CV", "cs.LG"], "comment": "for associated iris database, see\n  https://huggingface.co/datasets/fatdove/Iris_Database", "summary": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity."}
{"id": "2503.11932", "pdf": "https://arxiv.org/pdf/2503.11932", "abs": "https://arxiv.org/abs/2503.11932", "authors": ["Dhruv Kudale", "Badri Vishal Kasuba", "Venkatapathy Subramanian", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "title": "SPRINT: Script-agnostic Structure Recognition in Tables", "categories": ["cs.CV"], "comment": "Accepted at ICDAR 2024", "summary": "Table Structure Recognition (TSR) is vital for various downstream tasks like\ninformation retrieval, table reconstruction, and document understanding. While\nmost state-of-the-art (SOTA) research predominantly focuses on TSR in English\ndocuments, the need for similar capabilities in other languages is evident,\nconsidering the global diversity of data. Moreover, creating substantial\nlabeled data in non-English languages and training these SOTA models from\nscratch is costly and time-consuming. We propose TSR as a language-agnostic\ncell arrangement prediction and introduce SPRINT, Script-agnostic Structure\nRecognition in Tables. SPRINT uses recently introduced Optimized Table\nStructure Language (OTSL) sequences to predict table structures. We show that\nwhen coupled with a pre-trained table grid estimator, SPRINT can improve the\noverall tree edit distance-based similarity structure scores of tables even for\nnon-English documents. We experimentally evaluate our performance across\nbenchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our\nfindings reveal that SPRINT not only matches SOTA models in performance on\nstandard datasets but also demonstrates lower latency. Additionally, SPRINT\nexcels in accurately identifying table structures in non-English documents,\nsurpassing current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predictions into\na widely used HTML-based table representation. To encourage further research,\nwe release our code and Multilingual Scanned and Scene Table Structure\nRecognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in\nthirteen languages encompassing several scripts at\nhttps://github.com/IITB-LEAP-OCR/SPRINT"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method."}
{"id": "2503.11937", "pdf": "https://arxiv.org/pdf/2503.11937", "abs": "https://arxiv.org/abs/2503.11937", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model."}
{"id": "2503.11945", "pdf": "https://arxiv.org/pdf/2503.11945", "abs": "https://arxiv.org/abs/2503.11945", "authors": ["Naresh Kumar Devulapally", "Mingzhen Huang", "Vishal Asnani", "Shruti Agarwal", "Siwei Lyu", "Vishnu Suresh Lokhande"], "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Invisible watermarking of AI-generated images can help with copyright\nprotection, enabling detection and identification of AI-generated media. In\nthis work, we present a novel approach to watermark images of T2I Latent\nDiffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we\nenable watermarking in selected objects or parts of the image, offering greater\nflexibility compared to traditional full-image watermarking. Our method\nleverages the text encoder's compatibility across various LDMs, allowing\nplug-and-play integration for different LDMs. Moreover, introducing the\nwatermark early in the encoding stage improves robustness to adversarial\nperturbations in later stages of the pipeline. Our approach achieves $99\\%$ bit\naccuracy ($48$ bits) with a $10^5 \\times$ reduction in model parameters,\nenabling efficient watermarking."}
{"id": "2503.11953", "pdf": "https://arxiv.org/pdf/2503.11953", "abs": "https://arxiv.org/abs/2503.11953", "authors": ["Priyanka Mandikal", "Tushar Nagarajan", "Alex Stoken", "Zihui Xue", "Kristen Grauman"], "title": "SPOC: Spatially-Progressing Object State Change Segmentation in Video", "categories": ["cs.CV"], "comment": null, "summary": "Object state changes in video reveal critical information about human and\nagent activity. However, existing methods are limited to temporal localization\nof when the object is in its initial state (e.g., the unchopped avocado) versus\nwhen it has completed a state change (e.g., the chopped avocado), which limits\napplicability for any task requiring detailed information about the progress of\nthe actions and its spatial localization. We propose to deepen the problem by\nintroducing the spatially-progressing object state change segmentation task.\nThe goal is to segment at the pixel-level those regions of an object that are\nactionable and those that are transformed. We introduce the first model to\naddress this task, designing a VLM-based pseudo-labeling approach, state-change\ndynamics constraints, and a novel WhereToChange benchmark built on in-the-wild\nInternet videos. Experiments on two datasets validate both the challenge of the\nnew task as well as the promise of our model for localizing exactly where and\nhow fast objects are changing in video. We further demonstrate useful\nimplications for tracking activity progress to benefit robotic agents. Project\npage: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz √ñztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations."}
{"id": "2503.11969", "pdf": "https://arxiv.org/pdf/2503.11969", "abs": "https://arxiv.org/abs/2503.11969", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "The registration between the pre-operative model and the intra-operative\nsurface is crucial in image-guided liver surgery, as it facilitates the\neffective use of pre-operative information during the procedure. However, the\nintra-operative surface, usually represented as a point cloud, often has\nlimited coverage, especially in laparoscopic surgery, and is prone to holes and\nnoise, posing significant challenges for registration methods. Point cloud\ncompletion methods have the potential to alleviate these issues. Thus, we\nexplore six state-of-the-art point cloud completion methods to identify the\noptimal completion method for liver surgery applications. We focus on a\npatient-specific approach for liver point cloud completion from a partial liver\nsurface under three cases: canonical pose, non-canonical pose, and canonical\npose with noise. The transformer-based method, AdaPoinTr, outperforms all other\nmethods to generate a complete point cloud from the given partial liver point\ncloud under the canonical pose. On the other hand, our findings reveal\nsubstantial performance degradation of these methods under non-canonical poses\nand noisy settings, highlighting the limitations of these methods, which\nsuggests the need for a robust point completion method for its application in\nimage-guided liver surgery."}
{"id": "2503.11979", "pdf": "https://arxiv.org/pdf/2503.11979", "abs": "https://arxiv.org/abs/2503.11979", "authors": ["Runfa Blark Li", "Mahdi Shaghaghi", "Keito Suzuki", "Xinshuang Liu", "Varun Moparthi", "Bang Du", "Walker Curtis", "Martin Renschler", "Ki Myung Brian Lee", "Nikolay Atanasov", "Truong Nguyen"], "title": "DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) is one of the most important\nenvironment-perception and navigation algorithms for computer vision, robotics,\nand autonomous cars/drones. Hence, high quality and fast mapping becomes a\nfundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an\nexplicit representation with excellent rendering quality and speed,\nstate-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical\npointcloud-SLAM, GS-SLAM generates photometric information by learning from\ninput camera views and synthesize unseen views with high-quality textures.\nHowever, these GS-SLAM fail when moving objects occupy the scene that violate\nthe static assumption of bundle adjustment. The failed updates of moving GS\naffects the static GS and contaminates the full map over long frames. Although\nsome efforts have been made by concurrent works to consider moving objects for\nGS-SLAM, they simply detect and remove the moving regions from GS rendering\n(\"anti'' dynamic GS-SLAM), where only the static background could benefit from\nGS. To this end, we propose the first real-time GS-SLAM, \"DynaGSLAM'', that\nachieves high-quality online GS rendering, tracking, motion predictions of\nmoving objects in dynamic scenes while jointly estimating accurate ego motion.\nOur DynaGSLAM outperforms SOTA static & \"Anti'' dynamic GS-SLAM on three\ndynamic real datasets, while keeping speed and memory efficiency in practice."}
{"id": "2503.11981", "pdf": "https://arxiv.org/pdf/2503.11981", "abs": "https://arxiv.org/abs/2503.11981", "authors": ["Utkarsh Nath", "Rajeev Goel", "Rahul Khurana", "Kyle Min", "Mark Ollila", "Pavan Turaga", "Varun Jampani", "Tejaswi Gowda"], "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation saw dramatic advances in recent years by leveraging\nText-to-Image models. However, most existing techniques struggle with\ncompositional prompts, which describe multiple objects and their spatial\nrelationships. They often fail to capture fine-grained inter-object\ninteractions. We introduce DecompDreamer, a Gaussian splatting-based training\nroutine designed to generate high-quality 3D compositions from such complex\nprompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose\nscenes into structured components and their relationships. We propose a\nprogressive optimization strategy that first prioritizes joint relationship\nmodeling before gradually shifting toward targeted object refinement. Our\nqualitative and quantitative evaluations against state-of-the-art text-to-3D\nmodels demonstrate that DecompDreamer effectively generates intricate 3D\ncompositions with superior object disentanglement, offering enhanced control\nand flexibility in 3D generation. Project page :\nhttps://decompdreamer3d.github.io"}
{"id": "2503.11995", "pdf": "https://arxiv.org/pdf/2503.11995", "abs": "https://arxiv.org/abs/2503.11995", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Zhihao Chen", "Guangwei Gao"], "title": "Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "In recent years, Transformer has witnessed significant progress in food\nrecognition. However, most existing approaches still face two critical\nchallenges in lightweight food recognition: (1) the quadratic complexity and\nredundant feature representation from interactions with irrelevant tokens; (2)\nstatic feature recognition and single-scale representation, which overlook the\nunstructured, non-fixed nature of food images and the need for multi-scale\nfeatures. To address these, we propose an adaptive and efficient sparse\nTransformer architecture (Fraesormer) with two core designs: Adaptive Top-k\nSparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature\nGating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator\n(GDTKO) to retain critical attention scores, filtering low query-key matches\nthat hinder feature aggregation. It also introduces a partial channel mechanism\nto reduce redundancy and promote expert information flow, enabling local-global\ncollaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale\nfeature representation, enhancing contextual semantic information. Extensive\nexperiments show that Fraesormer outperforms state-of-the-art methods. code is\navailable at https://zs1314.github.io/Fraesormer."}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM."}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset."}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets."}
{"id": "2503.12015", "pdf": "https://arxiv.org/pdf/2503.12015", "abs": "https://arxiv.org/abs/2503.12015", "authors": ["Donglin Yang", "Paul Vicol", "Xiaojuan Qi", "Renjie Liao", "Xiaofan Zhang"], "title": "QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for Efficient Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based super-resolution (SR) methods often perform pixel-wise\ncomputations uniformly across entire images, even in homogeneous regions where\nhigh-resolution refinement is redundant. We propose the Quadtree Diffusion\nModel (QDM), a region-adaptive diffusion framework that leverages a quadtree\nstructure to selectively enhance detail-rich regions while reducing\ncomputations in homogeneous areas. By guiding the diffusion with a quadtree\nderived from the low-quality input, QDM identifies key regions-represented by\nleaf nodes-where fine detail is essential and applies minimal refinement\nelsewhere. This mask-guided, two-stream architecture adaptively balances\nquality and efficiency, producing high-fidelity outputs with low computational\nredundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR\ntasks across diverse image types, particularly in medical imaging (e.g., CT\nscans), where large homogeneous regions are prevalent. Furthermore, QDM\noutperforms or is comparable to state-of-the-art SR methods on standard\nbenchmarks while significantly reducing computational costs, highlighting its\nefficiency and suitability for resource-limited environments. Our code is\navailable at https://github.com/linYDTHU/QDM."}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach."}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation."}
{"id": "2503.12026", "pdf": "https://arxiv.org/pdf/2503.12026", "abs": "https://arxiv.org/abs/2503.12026", "authors": ["Zihan Zhoua", "Changrui Daia", "Aibo Songa", "Xiaolin Fang"], "title": "Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised video correspondence learning depends on the ability to\naccurately associate pixels between video frames that correspond to the same\nvisual object. However, achieving reliable pixel matching without supervision\nremains a major challenge. To address this issue, recent research has focused\non feature learning techniques that aim to encode unique pixel representations\nfor matching. Despite these advances, existing methods still struggle to\nachieve exact pixel correspondences and often suffer from false matches,\nlimiting their effectiveness in self-supervised settings.\n  To this end, we explore an efficient self-supervised Video Correspondence\nLearning framework (MER) that aims to accurately extract object details from\nunlabeled videos. First, we design a dedicated Motion Enhancement Engine that\nemphasizes capturing the dynamic motion of objects in videos. In addition, we\nintroduce a flexible sampling strategy for inter-pixel correspondence\ninformation (Multi-Cluster Sampler) that enables the model to pay more\nattention to the pixel changes of important objects in motion. Through\nexperiments, our algorithm outperforms the state-of-the-art competitors on\nvideo correspondence learning tasks such as video object segmentation and video\nobject keypoint tracking."}
{"id": "2503.12028", "pdf": "https://arxiv.org/pdf/2503.12028", "abs": "https://arxiv.org/abs/2503.12028", "authors": ["F. √áengel", "V. Adanova", "S. Tari"], "title": "Challenges in Plane Symmetry: From Theory to Perception", "categories": ["cs.CV"], "comment": null, "summary": "The planar ornaments are created by repeating a base unit using a combination\nof four primitive geometric operations: translation, rotation, reflection, and\nglide reflection. According to group theory, different combinations of these\nfour geometric operations lead to different symmetry groups. In this work, we\nselect a single challenging ornament, and analyze it both from the theoretical\npoint of view and perceptual point of view. We present the perceptual\nexperiment results, where one can see that the symmetries that the participants\nperceived from the ornaments do not match to what the theory dictates."}
{"id": "2503.12034", "pdf": "https://arxiv.org/pdf/2503.12034", "abs": "https://arxiv.org/abs/2503.12034", "authors": ["Enes Erdogan", "Eren Erdal Aksoy", "Sanem Sariel"], "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures, 7 tables", "summary": "Recognition of human manipulation actions in real-time is essential for safe\nand effective human-robot interaction and collaboration. The challenge lies in\ndeveloping a model that is both lightweight enough for real-time execution and\ncapable of generalization. While some existing methods in the literature can\nrun in real-time, they struggle with temporal scalability, i.e., they fail to\nadapt to long-duration manipulations effectively. To address this, leveraging\nthe generalizable scene graph representations, we propose a new Factorized\nGraph Sequence Encoder network that not only runs in real-time but also scales\neffectively in the temporal dimension, thanks to its factorized encoder\narchitecture. Additionally, we introduce Hand Pooling operation, a simple\npooling operation for more focused extraction of the graph-level embeddings.\nOur model outperforms the previous state-of-the-art real-time approach,\nachieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual\nAction (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.\nMoreover, we conduct an extensive ablation study to validate our network design\nchoices. Finally, we compare our model with its architecturally similar\nRGB-based model on the Bimacs dataset and show the limitations of this model in\ncontrast to ours on such an object-centric manipulation dataset."}
{"id": "2503.12035", "pdf": "https://arxiv.org/pdf/2503.12035", "abs": "https://arxiv.org/abs/2503.12035", "authors": ["Zhengyuan Peng", "Jinpeng Ma", "Zhimin Sun", "Ran Yi", "Haichuan Song", "Xin Tan", "Lizhuang Ma"], "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is a classification task that aims to\nclassify both base and novel classes in unlabeled images, using knowledge from\na labeled dataset. In GCD, previous research overlooks scene information or\ntreats it as noise, reducing its impact during model training. However, in this\npaper, we argue that scene information should be viewed as a strong prior for\ninferring novel classes. We attribute the misinterpretation of scene\ninformation to a key factor: the Ambiguity Challenge inherent in GCD.\nSpecifically, novel objects in base scenes might be wrongly classified into\nbase categories, while base objects in novel scenes might be mistakenly\nrecognized as novel categories. Once the ambiguity challenge is addressed,\nscene information can reach its full potential, significantly enhancing the\nperformance of GCD models. To more effectively leverage scene information, we\npropose the Modeling Object-Scene Associations (MOS) framework, which utilizes\na simple MLP-based scene-awareness module to enhance GCD performance. It\nachieves an exceptional average accuracy improvement of 4% on the challenging\nfine-grained datasets compared to state-of-the-art methods, emphasizing its\nsuperior performance in fine-grained GCD. The code is publicly available at\nhttps://github.com/JethroPeng/MOS."}
{"id": "2503.12047", "pdf": "https://arxiv.org/pdf/2503.12047", "abs": "https://arxiv.org/abs/2503.12047", "authors": ["Hangrui Xu", "Chuanrui Zhang", "Zhengxian Wu", "Peng Jiao", "Haoqian Wang"], "title": "PSGait: Multimodal Gait Recognition using Parsing Skeleton", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the parsing skeleton\nrepresentation, we propose a novel parsing skeleton-based gait recognition\nframework, named PSGait, which takes parsing skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods. Furthermore,\nas a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in\nRank-1 accuracy across various gait recognition models. These results\ndemonstrate the effectiveness and versatility of parsing skeletons for gait\nrecognition in the wild, establishing PSGait as a new state-of-the-art approach\nfor multimodal gait recognition."}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO."}
{"id": "2503.12052", "pdf": "https://arxiv.org/pdf/2503.12052", "abs": "https://arxiv.org/abs/2503.12052", "authors": ["Zhiyao Sun", "Yu-Hui Wen", "Matthieu Lin", "Ho-Jui Fang", "Sheng Ye", "Tian Lv", "Yong-Jin Liu"], "title": "Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://human-tailor.github.com", "summary": "Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use."}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet."}
{"id": "2503.12063", "pdf": "https://arxiv.org/pdf/2503.12063", "abs": "https://arxiv.org/abs/2503.12063", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting", "categories": ["cs.CV"], "comment": null, "summary": "Cell counting remains a fundamental yet challenging task in medical and\nbiological research due to the diverse morphology of cells, their dense\ndistribution, and variations in image quality. We present DLA-Count, a\nbreakthrough approach to cell counting that introduces three key innovations:\n(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell\nmatching in dense regions, (2) Multi-scale Deformable Gaussian Convolution\n(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced\nFeature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive\nexperiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)\ndemonstrate that our method outperforms previous methods across diverse\ndatasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and\n42.5\\% on MBM datasets. Our code is available at\nhttps://anonymous.4open.science/r/DLA-Count."}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey"}
{"id": "2503.12068", "pdf": "https://arxiv.org/pdf/2503.12068", "abs": "https://arxiv.org/abs/2503.12068", "authors": ["Qingchen Tang", "Lei Fan", "Maurice Pagnucco", "Yang Song"], "title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised image segmentation with image-level labels has drawn\nattention due to the high cost of pixel-level annotations. Traditional methods\nusing Class Activation Maps (CAMs) often highlight only the most discriminative\nregions, leading to incomplete masks. Recent approaches that introduce textual\ninformation struggle with histopathological images due to inter-class\nhomogeneity and intra-class heterogeneity. In this paper, we propose a\nprototype-based image prompting framework for histopathological image\nsegmentation. It constructs an image bank from the training set using\nclustering, extracting multiple prototype features per class to capture\nintra-class heterogeneity. By designing a matching loss between input features\nand class-specific prototypes using contrastive learning, our method addresses\ninter-class homogeneity and guides the model to generate more accurate CAMs.\nExperiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show\nthat our method outperforms existing weakly supervised segmentation approaches,\nsetting new benchmarks in histopathological image segmentation."}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness."}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization."}
{"id": "2503.12086", "pdf": "https://arxiv.org/pdf/2503.12086", "abs": "https://arxiv.org/abs/2503.12086", "authors": ["Rui Qian", "Chenyangguang Zhang", "Yan Di", "Guangyao Zhai", "Ruida Zhang", "Jiayu Guo", "Benjamin Busam", "Jian Pu"], "title": "FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have exhibited highly effective performance for\nphotorealistic novel view synthesis recently. However, the key limitation it\nmeets is the reliance on a hand-crafted frequency annealing strategy to recover\n3D scenes with imperfect camera poses. The strategy exploits a temporal\nlow-pass filter to guarantee convergence while decelerating the joint\noptimization of implicit scene reconstruction and camera registration. In this\nwork, we introduce the Frequency Adapted Bundle Adjusting Radiance Field\n(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted\nspatial low-pass filter to address the decelerating problem. We establish a\ntheoretical framework to interpret the relationship between position encoding\nof NeRF and camera registration and show that our frequency-adapted filter can\nmitigate frequency fluctuation caused by the temporal filter. Furthermore, we\nshow that applying a spatial low-pass filter in NeRF can optimize camera poses\nproductively through radial uncertainty overlaps among various views. Extensive\nexperiments show that FA-BARF can accelerate the joint optimization process\nunder little perturbations in object-centric scenes and recover real-world\nscenes with unknown camera poses. This implies wider possibilities for NeRF\napplied in dense 3D mapping and reconstruction under real-time requirements.\nThe code will be released upon paper acceptance."}
{"id": "2503.12087", "pdf": "https://arxiv.org/pdf/2503.12087", "abs": "https://arxiv.org/abs/2503.12087", "authors": ["Gino E. Jansen", "Mark J. Schuuring", "Berto J. Bouma", "Ivana I≈°gum"], "title": "Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel approach to achieving temporally consistent mitral\nannulus landmark localization in echocardiography videos using sparse\nannotations. Our method introduces a self-supervised loss term that enforces\ntemporal consistency between neighboring frames, which smooths the position of\nlandmarks and enhances measurement accuracy over time. Additionally, we\nincorporate realistic field-of-view augmentations to improve the recognition of\nmissing anatomical landmarks. We evaluate our approach on both a public and\nprivate dataset, and demonstrate significant improvements in Mitral Annular\nPlane Systolic Excursion (MAPSE) calculations and overall landmark tracking\nstability. The method achieves a mean absolute MAPSE error of 1.81 $\\pm$ 0.14\nmm, an annulus size error of 2.46 $\\pm$ 0.31 mm, and a landmark localization\nerror of 2.48 $\\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for\nrecognition of missing landmarks."}
{"id": "2503.12093", "pdf": "https://arxiv.org/pdf/2503.12093", "abs": "https://arxiv.org/abs/2503.12093", "authors": ["Oren Shrout", "Ayellet Tal"], "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose SFMNet, a novel 3D sparse detector that combines the efficiency of\nsparse convolutions with the ability to model long-range dependencies. While\ntraditional sparse convolution techniques efficiently capture local structures,\nthey struggle with modeling long-range relationships. However, capturing\nlong-range dependencies is fundamental for 3D object detection. In contrast,\ntransformers are designed to capture these long-range dependencies through\nattention mechanisms. But, they come with high computational costs, due to\ntheir quadratic query-key-value interactions. Furthermore, directly applying\nattention to non-empty voxels is inefficient due to the sparse nature of 3D\nscenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,\nwhich integrates short- and long-range contexts with linear complexity by\nleveraging a new hierarchical sparse convolution design. This approach enables\nSFMNet to achieve high detection performance with improved efficiency, making\nit well-suited for large-scale LiDAR scenes. We show that our detector achieves\nstate-of-the-art performance on autonomous driving datasets."}
{"id": "2503.12094", "pdf": "https://arxiv.org/pdf/2503.12094", "abs": "https://arxiv.org/abs/2503.12094", "authors": ["Weiming Zhang", "Dingwen Xiao", "Lei Chen", "Lin Wang"], "title": "E-SAM: Training-Free Segment Every Entity Model", "categories": ["cs.CV"], "comment": "Under review", "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities\nwithin an image without the need for predefined class labels. This\ncharacteristic makes ES well-suited to open-world applications with adaptation\nto diverse and dynamically changing environments, where new and previously\nunseen entities may appear frequently. Existing ES methods either require large\nannotated datasets or high training costs, limiting their scalability and\nadaptability. Recently, the Segment Anything Model (SAM), especially in its\nAutomatic Mask Generation (AMG) mode, has shown potential for holistic image\nsegmentation. However, it struggles with over-segmentation and\nunder-segmentation, making it less effective for ES. In this paper, we\nintroduce E-SAM, a novel training-free framework that exhibits exceptional ES\ncapability. Specifically, we first propose Multi-level Mask Generation (MMG)\nthat hierarchically processes SAM's AMG outputs to generate reliable\nobject-level masks while preserving fine details at other levels. Entity-level\nMask Refinement (EMR) then refines these object-level masks into accurate\nentity-level masks. That is, it separates overlapping masks to address the\nredundancy issues inherent in SAM's outputs and merges similar masks by\nevaluating entity-level consistency. Lastly, Under-Segmentation Refinement\n(USR) addresses under-segmentation by generating additional high-confidence\nmasks fused with EMR outputs to produce the final ES map. These three modules\nare seamlessly optimized to achieve the best ES without additional training\noverhead. Extensive experiments demonstrate that E-SAM achieves\nstate-of-the-art performance compared to prior ES methods, demonstrating a\nsignificant improvement by +30.1 on benchmark metrics."}
{"id": "2503.12095", "pdf": "https://arxiv.org/pdf/2503.12095", "abs": "https://arxiv.org/abs/2503.12095", "authors": ["Walter Zimmer", "Ross Greer", "Daniel Lehmberg", "Marc Pavel", "Holger Caesar", "Xingcheng Zhou", "Ahmed Ghita", "Mohan Trivedi", "Rui Song", "Hu Cao", "Akshay Gopalkrishnan", "Alois C. Knoll"], "title": "Towards Vision Zero: The Accid3nD Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Even though a significant amount of work has been done to increase the safety\nof transportation networks, accidents still occur regularly. They must be\nunderstood as unavoidable and sporadic outcomes of traffic networks. No public\ndataset contains 3D annotations of real-world accidents recorded from roadside\nsensors. We present the Accid3nD dataset, a collection of real-world highway\naccidents in different weather and lighting conditions. It contains vehicle\ncrashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,\ninstance masks, and 3D bounding boxes with track IDs. In total, the dataset\ncontains 111,945 labeled frames recorded from four roadside cameras and LiDARs\nat 25 Hz. The dataset contains six object classes and is provided in the\nOpenLABEL format. We propose an accident detection model that combines a\nrule-based approach with a learning-based one. Experiments and ablation studies\non our dataset show the robustness of our proposed method. The dataset, model,\nand code are available on our website: https://accident-dataset.github.io."}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks."}
{"id": "2503.12102", "pdf": "https://arxiv.org/pdf/2503.12102", "abs": "https://arxiv.org/abs/2503.12102", "authors": ["Paula Andrea P√©rez-Toro", "Tom√°s Arias-Vergara", "Fangxu Xing", "Xiaofeng Liu", "Maureen Stone", "Jiachen Zhuo", "Juan Rafael Orozco-Arroyave", "Elmar N√∂th", "Jana Hutter", "Jerry L. Prince", "Andreas Maier", "Jonghye Woo"], "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the relationship between vocal tract motion during speech and\nthe resulting acoustic signal is crucial for aided clinical assessment and\ndeveloping personalized treatment and rehabilitation strategies. Toward this\ngoal, we introduce an audio-to-video generation framework for creating Real\nTime/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract\nfrom speech signals. Our framework first preprocesses RT-/cine-MRI sequences\nand speech samples to achieve temporal alignment, ensuring synchronization\nbetween visual and audio data. We then employ a modified stable diffusion\nmodel, integrating structural and temporal blocks, to effectively capture\nmovement characteristics and temporal dynamics in the synchronized data. This\nprocess enables the generation of MRI sequences from new speech inputs,\nimproving the conversion of audio into visual data. We evaluated our framework\non healthy controls and tongue cancer patients by analyzing and comparing the\nvocal tract movements in synthesized videos. Our framework demonstrated\nadaptability to new speech inputs and effective generalization. In addition,\npositive human evaluations confirmed its effectiveness, with realistic and\naccurate visualizations, suggesting its potential for outpatient therapy and\npersonalized simulation of vocal tract visualizations."}
{"id": "2503.12124", "pdf": "https://arxiv.org/pdf/2503.12124", "abs": "https://arxiv.org/abs/2503.12124", "authors": ["Yingying Deng", "Xiangyu He", "Fan Tang", "Weiming Dong"], "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "The customization of multiple attributes has gained popularity with the\nrising demand for personalized content creation. Despite promising empirical\nresults, the contextual coherence between different attributes has been largely\noverlooked. In this paper, we argue that subsequent attributes should follow\nthe multivariable conditional distribution introduced by former attribute\ncreation. In light of this, we reformulate multi-attribute creation from a\nconditional probability theory perspective and tackle the challenging zero-shot\nsetting. By explicitly modeling the dependencies between attributes, we further\nenhance the coherence of generated images across diverse attribute\ncombinations. Furthermore, we identify connections between multi-attribute\ncustomization and multi-task learning, effectively addressing the high\ncomputing cost encountered in multi-attribute synthesis. Extensive experiments\ndemonstrate that Z-Magic outperforms existing models in zero-shot image\ngeneration, with broad implications for AI-driven design and creative\napplications."}
{"id": "2503.12127", "pdf": "https://arxiv.org/pdf/2503.12127", "abs": "https://arxiv.org/abs/2503.12127", "authors": ["Tobia Poppi", "Tejaswi Kasarla", "Pascal Mettes", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Hyperbolic Safety-Aware Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "CVPR 2025", "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC."}
{"id": "2503.12131", "pdf": "https://arxiv.org/pdf/2503.12131", "abs": "https://arxiv.org/abs/2503.12131", "authors": ["Shentong Mo", "Zehua Chen", "Fan Bao", "Jun Zhu"], "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities."}
{"id": "2503.12150", "pdf": "https://arxiv.org/pdf/2503.12150", "abs": "https://arxiv.org/abs/2503.12150", "authors": ["Hongyu Sun", "Qiuhong Ke", "Ming Cheng", "Yongcai Wang", "Deying Li", "Chenhui Gou", "Jianfei Cai"], "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables", "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."}
{"id": "2503.12165", "pdf": "https://arxiv.org/pdf/2503.12165", "abs": "https://arxiv.org/abs/2503.12165", "authors": ["Zijian He", "Yuwei Ning", "Yipeng Qin", "Wangrun Wang", "Sibei Yang", "Liang Lin", "Guanbin Li"], "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360."}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettr√©", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes."}
{"id": "2503.12173", "pdf": "https://arxiv.org/pdf/2503.12173", "abs": "https://arxiv.org/abs/2503.12173", "authors": ["Yuchen Deng", "Haibin Ling", "Bingyao Huang"], "title": "LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization", "categories": ["cs.CV", "cs.MM"], "comment": "12 pages, 9 figures", "summary": "We propose LAPIG, a language guided projector image generation method with\nsurface adaptation and stylization. LAPIG consists of a projector-camera system\nand a target textured projection surface. LAPIG takes the user text prompt as\ninput and aims to transform the surface style using the projector. LAPIG's key\nchallenge is that due to the projector's physical brightness limitation and the\nsurface texture, the viewer's perceived projection may suffer from color\nsaturation and artifacts in both dark and bright regions, such that even with\nthe state-of-the-art projector compensation techniques, the viewer may see\nclear surface texture-related artifacts. Therefore, how to generate a projector\nimage that follows the user's instruction while also displaying minimum surface\nartifacts is an open problem. To address this issue, we propose projection\nsurface adaptation (PSA) that can generate compensable surface stylization. We\nfirst train two networks to simulate the projector compensation and\nproject-and-capture processes, this allows us to find a satisfactory projector\nimage without real project-and-capture and utilize gradient descent for fast\nconvergence. Then, we design content and saturation losses to guide the\nprojector image generation, such that the generated image shows no clearly\nperceivable artifacts when projected. Finally, the generated image is projected\nfor visually pleasing surface style morphing effects. The source code and video\nare available on the project page: https://Yu-chen-Deng.github.io/LAPIG/."}
{"id": "2503.12191", "pdf": "https://arxiv.org/pdf/2503.12191", "abs": "https://arxiv.org/abs/2503.12191", "authors": ["Ying Zang", "Yuncan Gao", "Jiangi Zhang", "Yuangi Hu", "Runlong Cao", "Lanyun Zhu", "Qi Zhu", "Deyi Ji", "Renjun Xu", "Tianrun Chen"], "title": "Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches", "categories": ["cs.CV"], "comment": null, "summary": "This work advances zero-shot interactive segmentation for remote sensing\nimagery through three key contributions. First, we propose a novel sketch-based\nprompting method, enabling users to intuitively outline objects, surpassing\ntraditional point or box prompts. Second, we introduce LTL-Sensing, the first\ndataset pairing human sketches with remote sensing imagery, setting a benchmark\nfor future research. Third, we present LTL-Net, a model featuring a multi-input\nprompting transport module tailored for freehand sketches. Extensive\nexperiments show our approach significantly improves segmentation accuracy and\nrobustness over state-of-the-art methods like SAM, fostering more intuitive\nhuman-AI collaboration in remote sensing analysis and enhancing its\napplications."}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks."}
{"id": "2503.12206", "pdf": "https://arxiv.org/pdf/2503.12206", "abs": "https://arxiv.org/abs/2503.12206", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Muhammad Haris Khan", "Mohsen Ali"], "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. We evaluated our\nmodels on 11 base-to-novel datasets and they achieved superior accuracy on 9 of\nthese, including benchmarks like ImageNet, SUN397 and Caltech101, while\nmaintaining a strictly training-free paradigm. Our overall accuracy of 83.44%\nsurpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.\nOur method achieved 83.6% average accuracy across 13 datasets, a 9.7%\nimprovement over the previous 73.9% state-of-the-art for training-free\napproaches. Our method improves domain generalization, with a 3.6% gain on\nImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot\nmethods."}
{"id": "2503.12213", "pdf": "https://arxiv.org/pdf/2503.12213", "abs": "https://arxiv.org/abs/2503.12213", "authors": ["Ruyu Wang", "Xuefeng Hou", "Sabrina Schmedding", "Marco F. Huber"], "title": "STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted by WACV2025", "summary": "In layout-to-image (L2I) synthesis, controlled complex scenes are generated\nfrom coarse information like bounding boxes. Such a task is exciting to many\ndownstream applications because the input layouts offer strong guidance to the\ngeneration process while remaining easily reconfigurable by humans. In this\npaper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based\nmodel that produces photo-realistic images and provides fine-grained control of\nstylized objects in scenes. Our approach learns a global condition for each\nlayout, and a self-supervised semantic map for weight modulation using a novel\nEdge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)\nis also introduced to cross-condition the global condition and image feature\nfor capturing the objects' relationships. These measures provide consistent\nguidance through the model, enabling more accurate and controllable image\ngeneration. Extensive benchmarking demonstrates that our STAY Diffusion\npresents high-quality images while surpassing previous state-of-the-art methods\nin generation diversity, accuracy, and controllability."}
{"id": "2503.12215", "pdf": "https://arxiv.org/pdf/2503.12215", "abs": "https://arxiv.org/abs/2503.12215", "authors": ["Amulya Reddy Maligireddy", "Manohar Reddy Uppula", "Nidhi Rastogi", "Yaswanth Reddy Parla"], "title": "Gun Detection Using Combined Human Pose and Weapon Appearance", "categories": ["cs.CV"], "comment": null, "summary": "The increasing frequency of firearm-related incidents has necessitated\nadvancements in security and surveillance systems, particularly in firearm\ndetection within public spaces. Traditional gun detection methods rely on\nmanual inspections and continuous human monitoring of CCTV footage, which are\nlabor-intensive and prone to high false positive and negative rates. To address\nthese limitations, we propose a novel approach that integrates human pose\nestimation with weapon appearance recognition using deep learning techniques.\nUnlike prior studies that focus on either body pose estimation or firearm\ndetection in isolation, our method jointly analyzes posture and weapon presence\nto enhance detection accuracy in real-world, dynamic environments. To train our\nmodel, we curated a diverse dataset comprising images from open-source\nrepositories such as IMFDB and Monash Guns, supplemented with AI-generated and\nmanually collected images from web sources. This dataset ensures robust\ngeneralization and realistic performance evaluation under various surveillance\nconditions. Our research aims to improve the precision and reliability of\nfirearm detection systems, contributing to enhanced public safety and threat\nmitigation in high-risk areas."}
{"id": "2503.12218", "pdf": "https://arxiv.org/pdf/2503.12218", "abs": "https://arxiv.org/abs/2503.12218", "authors": ["Chengxuan Qian", "Kai Han", "Siqi Ma", "Chongwen Lyu", "Zhenlong Yuan", "Jun Chen", "Zhe Liu"], "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods."}
{"id": "2503.12230", "pdf": "https://arxiv.org/pdf/2503.12230", "abs": "https://arxiv.org/abs/2503.12230", "authors": ["Yihao Wang", "Raphael Memmesheimer", "Sven Behnke"], "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps."}
{"id": "2503.12232", "pdf": "https://arxiv.org/pdf/2503.12232", "abs": "https://arxiv.org/abs/2503.12232", "authors": ["Yan Jiang", "Hao Yu", "Xu Cheng", "Haoyu Chen", "Zhaodong Sun", "Guoying Zhao"], "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community."}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos."}
{"id": "2503.12249", "pdf": "https://arxiv.org/pdf/2503.12249", "abs": "https://arxiv.org/abs/2503.12249", "authors": ["Boyu Chen", "Ameenat L. Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "categories": ["cs.CV"], "comment": null, "summary": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging\ntechnique with great potential for diagnosing anterior uveitis, a\nvision-threatening ocular inflammatory condition. A hallmark of this condition\nis the presence of inflammatory cells in the eye's anterior chamber, and\ndetecting these cells using AS-OCT images has attracted research interest.\nWhile recent efforts aim to replace manual cell detection with automated\ncomputer vision approaches, detecting extremely small (minuscule) objects in\nhigh-resolution images, such as AS-OCT, poses substantial challenges: (1) each\ncell appears as a minuscule particle, representing less than 0.005\\% of the\nimage, making the detection difficult, and (2) OCT imaging introduces\npixel-level noise that can be mistaken for cells, leading to false positive\ndetections. To overcome these challenges, we propose a minuscule cell detection\nframework through a progressive field-of-view focusing strategy. This strategy\nsystematically refines the detection scope from the whole image to a target\nregion where cells are likely to be present, and further to minuscule regions\npotentially containing individual cells. Our framework consists of two modules.\nFirst, a Field-of-Focus module uses a vision foundation model to segment the\ntarget region. Subsequently, a Fine-grained Object Detection module introduces\na specialized Minuscule Region Proposal followed by a Spatial Attention Network\nto distinguish individual cells from noise within the segmented region.\nExperimental results demonstrate that our framework outperforms\nstate-of-the-art methods for cell detection, providing enhanced efficacy for\nclinical applications. Our code is publicly available at:\nhttps://github.com/joeybyc/MCD."}
{"id": "2503.12260", "pdf": "https://arxiv.org/pdf/2503.12260", "abs": "https://arxiv.org/abs/2503.12260", "authors": ["Josep Cabacas-Maso", "Elena Ortega-Beltr√°n", "Ismael Benito-Altamirano", "Carles Ventura"], "title": "Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge", "categories": ["cs.CV", "I.4"], "comment": null, "summary": "We present our contribution to the 8th ABAW challenge at CVPR 2025, where we\ntackle valence-arousal estimation, emotion recognition, and facial action unit\ndetection as three independent challenges. Our approach leverages the\nwell-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all\nthree tasks, achieving results that surpass the proposed baselines.\nAdditionally, we explore the use of CLIP for the emotion recognition challenge\nas an additional experiment. We provide insights into the architectural choices\nthat contribute to the strong performance of our methods."}
{"id": "2503.12261", "pdf": "https://arxiv.org/pdf/2503.12261", "abs": "https://arxiv.org/abs/2503.12261", "authors": ["R. Gnana Praveen", "Jahangir Alam"], "title": "Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Submission to valence arousal track of 8th ABAW competition. arXiv\n  admin note: substantial text overlap with arXiv:2403.13659", "summary": "Multimodal emotion recognition has recently drawn a lot of interest in\naffective computing as it has immense potential to outperform isolated unimodal\napproaches. Audio and visual modalities are two predominant contact-free\nchannels in videos, which are often expected to carry a complementary\nrelationship with each other. However, audio and visual channels may not always\nbe complementary with each other, resulting in poor audio-visual feature\nrepresentations, thereby degrading the performance of the system. In this\npaper, we propose a flexible audio-visual fusion model that can adapt to weak\ncomplementary relationships using a gated attention mechanism. Specifically, we\nextend the recursive joint cross-attention model by introducing gating\nmechanism in every iteration to control the flow of information between the\ninput features and the attended features depending on the strength of their\ncomplementary relationship. For instance, if the modalities exhibit strong\ncomplementary relationships, the gating mechanism chooses cross-attended\nfeatures, otherwise non-attended features. To further improve the performance\nof the system, we further introduce stage gating mechanism, which is used to\ncontrol the flow of information across the gated outputs of each iteration.\nTherefore, the proposed model improves the performance of the system even when\nthe audio and visual modalities do not have a strong complementary relationship\nwith each other by adding more flexibility to the recursive joint cross\nattention mechanism. The proposed model has been evaluated on the challenging\nAffwild2 dataset and significantly outperforms the state-of-the-art fusion\napproaches."}
{"id": "2503.12267", "pdf": "https://arxiv.org/pdf/2503.12267", "abs": "https://arxiv.org/abs/2503.12267", "authors": ["Aziz Amari", "Mariem Makni", "Wissal Fnaich", "Akram Lahmar", "Fedi Koubaa", "Oumayma Charrad", "Mohamed Ali Zormati", "Rabaa Youssef Douss"], "title": "An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation", "categories": ["cs.CV"], "comment": null, "summary": "In large organizations, the number of financial transactions can grow\nrapidly, driving the need for fast and accurate multi-criteria invoice\nvalidation. Manual processing remains error-prone and time-consuming, while\ncurrent automated solutions are limited by their inability to support a variety\nof constraints, such as documents that are partially handwritten or\nphotographed with a mobile phone. In this paper, we propose to automate the\nvalidation of machine written invoices using document layout analysis and\nobject detection techniques based on recent deep learning (DL) models. We\nintroduce a novel dataset consisting of manually annotated real-world invoices\nand a multi-criteria validation process. We fine-tune and benchmark the most\nrelevant DL models on our dataset. Experimental results show the effectiveness\nof the proposed pipeline and selected DL models in terms of achieving fast and\naccurate validation of invoices."}
{"id": "2503.12269", "pdf": "https://arxiv.org/pdf/2503.12269", "abs": "https://arxiv.org/abs/2503.12269", "authors": ["Negar Shahamiri", "Moritz Rempe", "Lukas Heine", "Jens Kleesiek", "Fabian H√∂rst"], "title": "Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net", "categories": ["cs.CV"], "comment": null, "summary": "Automatic tissue segmentation and nuclei detection is an important task in\npathology, aiding in biomarker extraction and discovery. The panoptic\nsegmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to\nimprove tissue segmentation and nuclei detection in melanoma histopathology.\nUnlike many challenge submissions focusing on extensive model tuning, our\napproach emphasizes delivering a deployable solution within a 24-hour\ndevelopment timeframe, using out-of-the-box frameworks. The pipeline combines\ntwo models, namely CellViT++ for nuclei detection and nnU-Net for tissue\nsegmentation. Our results demonstrate a significant improvement in tissue\nsegmentation, achieving a Dice score of 0.750, surpassing the baseline score of\n0.629. For nuclei detection, we obtained results comparable to the baseline in\nboth challenge tracks. The code is publicly available at\nhttps://github.com/TIO-IKIM/PUMA."}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach."}
{"id": "2503.12281", "pdf": "https://arxiv.org/pdf/2503.12281", "abs": "https://arxiv.org/abs/2503.12281", "authors": ["Paola Natalia Ca√±as", "Marcos Nieto", "Oihana Otaegui", "Igor Rodr√≠guez"], "title": "Exploration of VLMs for Driver Monitoring Systems Applications", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 16th ITS European Congress, Seville, Spain, 19-21 May\n  2025", "summary": "In recent years, we have witnessed significant progress in emerging deep\nlearning models, particularly Large Language Models (LLMs) and Vision-Language\nModels (VLMs). These models have demonstrated promising results, indicating a\nnew era of Artificial Intelligence (AI) that surpasses previous methodologies.\nTheir extensive knowledge and zero-shot capabilities suggest a paradigm shift\nin developing deep learning solutions, moving from data capturing and algorithm\ntraining to just writing appropriate prompts. While the application of these\ntechnologies has been explored across various industries, including automotive,\nthere is a notable gap in the scientific literature regarding their use in\nDriver Monitoring Systems (DMS). This paper presents our initial approach to\nimplementing VLMs in this domain, utilising the Driver Monitoring Dataset to\nevaluate their performance and discussing their advantages and challenges when\nimplemented in real-world scenarios."}
{"id": "2503.12284", "pdf": "https://arxiv.org/pdf/2503.12284", "abs": "https://arxiv.org/abs/2503.12284", "authors": ["Krzysztof Byrski", "Grzegorz Wilczy≈Ñski", "Weronika Smolak-Dy≈ºewska", "Piotr Borycki", "Dawid Baran", "S≈Çawomir Tadeja", "Przemys≈Çaw Spurek"], "title": "REdiSplats: Ray Tracing for Editable Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has become one of the most important neural rendering\nalgorithms. GS represents 3D scenes using Gaussian components with trainable\ncolor and opacity. This representation achieves high-quality renderings with\nfast inference. Regrettably, it is challenging to integrate such a solution\nwith varying light conditions, including shadows and light reflections, manual\nadjustments, and a physical engine. Recently, a few approaches have appeared\nthat incorporate ray-tracing or mesh primitives into GS to address some of\nthese caveats. However, no such solution can simultaneously solve all the\nexisting limitations of the classical GS. Consequently, we introduce\nREdiSplats, which employs ray tracing and a mesh-based representation of flat\n3D Gaussians. In practice, we model the scene using flat Gaussian distributions\nparameterized by the mesh. We can leverage fast ray tracing and control\nGaussian modification by adjusting the mesh vertices. Moreover, REdiSplats\nallows modeling of light conditions, manual adjustments, and physical\nsimulation. Furthermore, we can render our models using 3D tools such as\nBlender or Nvdiffrast, which opens the possibility of integrating them with all\nexisting 3D graphics techniques dedicated to mesh representations."}
{"id": "2503.12303", "pdf": "https://arxiv.org/pdf/2503.12303", "abs": "https://arxiv.org/abs/2503.12303", "authors": ["Xiaoying Zhang", "Da Peng", "Yipeng Zhang", "Zonghao Guo", "Chengyue Wu", "Chi Chen", "Wei Ke", "Helen Meng", "Maosong Sun"], "title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs", "categories": ["cs.CV"], "comment": "38 pages", "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent pre-training approaches focus on enhancing perception by training on\nhigh-quality image captions due to the extremely high cost of collecting\nchain-of-thought (CoT) reasoning data for improving reasoning. While leveraging\nadvanced MLLMs for caption generation enhances scalability, the outputs often\nlack comprehensiveness and accuracy. In this paper, we introduce Self-Improving\nCognition (SIcog), a self-learning framework designed to construct\nnext-generation foundation MLLMs by enhancing their systematic cognitive\ncapabilities through multimodal pre-training with self-generated data.\nSpecifically, we propose chain-of-description, an approach that improves an\nMLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used to\nrefine the MLLM during multimodal pre-training, facilitating next-generation\nfoundation MLLM construction. Extensive experiments on both low- and\nhigh-resolution MLLMs across diverse benchmarks demonstrate that, with merely\n213K self-generated pre-training samples, SIcog produces next-generation\nfoundation MLLMs with significantly improved cognition, achieving\nbenchmark-leading performance compared to prevalent pre-training approaches."}
{"id": "2503.12307", "pdf": "https://arxiv.org/pdf/2503.12307", "abs": "https://arxiv.org/abs/2503.12307", "authors": ["Jiahao Wu", "Rui Peng", "Zhiyan Wang", "Lu Xiao", "Luyang Tang", "Jinbo Yan", "Kaiqiang Xiong", "Ronggang Wang"], "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps://github.com/WuJH2001/swift4d."}
{"id": "2503.12326", "pdf": "https://arxiv.org/pdf/2503.12326", "abs": "https://arxiv.org/abs/2503.12326", "authors": ["Maciej P. Polak", "Dane Morgan"], "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Automated data extraction from research texts has been steadily improving,\nwith the emergence of large language models (LLMs) accelerating progress even\nfurther. Extracting data from plots in research papers, however, has been such\na complex task that it has predominantly been confined to manual data\nextraction. We show that current multimodal large language models, with proper\ninstructions and engineered workflows, are capable of accurately extracting\ndata from plots. This capability is inherent to the pretrained models and can\nbe achieved with a chain-of-thought sequence of zero-shot engineered prompts we\ncall PlotExtract, without the need to fine-tune. We demonstrate PlotExtract\nhere and assess its performance on synthetic and published plots. We consider\nonly plots with two axes in this analysis. For plots identified as extractable,\nPlotExtract finds points with over 90% precision (and around 90% recall) and\nerrors in x and y position of around 5% or lower. These results prove that\nmultimodal LLMs are a viable path for high-throughput data extraction for plots\nand in many circumstances can replace the current manual methods of data\nextraction."}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io."}
{"id": "2503.12332", "pdf": "https://arxiv.org/pdf/2503.12332", "abs": "https://arxiv.org/abs/2503.12332", "authors": ["Yunze Liu", "Peiran Wu", "Cheng Liang", "Junxiao Shen", "Limin Wang", "Li Yi"], "title": "VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based architectures for video understanding demonstrate\npromising computational efficiency and competitive performance, yet struggle\nwith overfitting issues that hinder their scalability. To overcome this\nchallenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework\nfeaturing a novel pre-training approach. VideoMAP uses a 4:1\nMamba-to-Transformer ratio, effectively balancing computational cost and model\ncapacity. This architecture, combined with our proposed frame-wise masked\nautoregressive pre-training strategy, delivers significant performance gains\nwhen scaling to larger models. Additionally, VideoMAP exhibits impressive\nsample efficiency, significantly outperforming existing methods with less\ntraining data. Experiments show that VideoMAP outperforms existing models\nacross various datasets, including Kinetics-400, Something-Something V2,\nBreakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a\nvisual encoder for multimodal large language models, highlighting its ability\nto reduce memory usage and enable the processing of longer video sequences. The\ncode is open-source at https://github.com/yunzeliu/MAP"}
{"id": "2503.12335", "pdf": "https://arxiv.org/pdf/2503.12335", "abs": "https://arxiv.org/abs/2503.12335", "authors": ["Tengfei Wang", "Yongmao Hou", "Zhaoning Zhang", "Yiwei Xu", "Zongqian Zhan", "Xin Wang"], "title": "GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images", "categories": ["cs.CV"], "comment": "This paper has been submitted to IROS 2025", "summary": "Accurate geometric surface reconstruction, providing essential environmental\ninformation for navigation and manipulation tasks, is critical for enabling\nrobotic self-exploration and interaction. Recently, 3D Gaussian Splatting\n(3DGS) has gained significant attention in the field of surface reconstruction\ndue to its impressive geometric quality and computational efficiency. While\nrecent relevant advancements in novel view synthesis under inconsistent\nillumination using 3DGS have shown promise, the challenge of robust surface\nreconstruction under such conditions is still being explored. To address this\nchallenge, we propose a method called GS-3I. Specifically, to mitigate 3D\nGaussian optimization bias caused by underexposed regions in single-view\nimages, based on Convolutional Neural Network (CNN), a tone mapping correction\nframework is introduced. Furthermore, inconsistent lighting across multi-view\nimages, resulting from variations in camera settings and complex scene\nillumination, often leads to geometric constraint mismatches and deviations in\nthe reconstructed surface. To overcome this, we propose a normal compensation\nmechanism that integrates reference normals extracted from single-view image\nwith normals computed from multi-view observations to effectively constrain\ngeometric inconsistencies. Extensive experimental evaluations demonstrate that\nGS-3I can achieve robust and accurate surface reconstruction across complex\nillumination scenarios, highlighting its effectiveness and versatility in this\ncritical challenge. https://github.com/TFwang-9527/GS-3I"}
{"id": "2503.12343", "pdf": "https://arxiv.org/pdf/2503.12343", "abs": "https://arxiv.org/abs/2503.12343", "authors": ["Xiaoyu Xiong", "Changyu Hu", "Chunru Lin", "Pingchuan Ma", "Chuang Gan", "Tao Du"], "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues", "categories": ["cs.CV"], "comment": null, "summary": "We present TopoGaussian, a holistic, particle-based pipeline for inferring\nthe interior structure of an opaque object from easily accessible photos and\nvideos as input. Traditional mesh-based approaches require tedious and\nerror-prone mesh filling and fixing process, while typically output rough\nboundary surface. Our pipeline combines Gaussian Splatting with a novel,\nversatile particle-based differentiable simulator that simultaneously\naccommodates constitutive model, actuator, and collision, without interference\nwith mesh. Based on the gradients from this simulator, we provide flexible\nchoice of topology representation for optimization, including particle, neural\nimplicit surface, and quadratic surface. The resultant pipeline takes easily\naccessible photos and videos as input and outputs the topology that matches the\nphysical characteristics of the input. We demonstrate the efficacy of our\npipeline on a synthetic dataset and four real-world tasks with 3D-printed\nprototypes. Compared with existing mesh-based method, our pipeline is 5.26x\nfaster on average with improved shape quality. These results highlight the\npotential of our pipeline in 3D vision, soft robotics, and manufacturing\napplications."}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines."}
{"id": "2503.12350", "pdf": "https://arxiv.org/pdf/2503.12350", "abs": "https://arxiv.org/abs/2503.12350", "authors": ["Wenqing Kuang", "Xiongwei Zhao", "Yehui Shen", "Congcong Wen", "Huimin Lu", "Zongtan Zhou", "Xieyuanli Chen"], "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR."}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas."}
{"id": "2503.12356", "pdf": "https://arxiv.org/pdf/2503.12356", "abs": "https://arxiv.org/abs/2503.12356", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Se Young Chun"], "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025", "summary": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure."}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/."}
{"id": "2503.12381", "pdf": "https://arxiv.org/pdf/2503.12381", "abs": "https://arxiv.org/abs/2503.12381", "authors": ["Ruchika Sharma", "Rudresh Dwivedi"], "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN", "categories": ["cs.CV", "cs.MM"], "comment": "Submiited to journal", "summary": "Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision."}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO."}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community."}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000."}
{"id": "2503.12399", "pdf": "https://arxiv.org/pdf/2503.12399", "abs": "https://arxiv.org/abs/2503.12399", "authors": ["Jiangdong Cai", "Yan Chen", "Zhenrong Shen", "Haotian Jiang", "Honglin Xiong", "Kai Xuan", "Lichi Zhang", "Qian Wang"], "title": "Pathology Image Restoration via Mixture of Prompts", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In digital pathology, acquiring all-in-focus images is essential to\nhigh-quality imaging and high-efficient clinical workflow. Traditional scanners\nachieve this by scanning at multiple focal planes of varying depths and then\nmerging them, which is relatively slow and often struggles with complex tissue\ndefocus. Recent prevailing image restoration technique provides a means to\nrestore high-quality pathology images from scans of single focal planes.\nHowever, existing image restoration methods are inadequate, due to intricate\ndefocus patterns in pathology images and their domain-specific semantic\ncomplexities. In this work, we devise a two-stage restoration solution\ncascading a transformer and a diffusion model, to benefit from their powers in\npreserving image fidelity and perceptual quality, respectively. We particularly\npropose a novel mixture of prompts for the two-stage solution. Given initial\nprompt that models defocus in microscopic imaging, we design two prompts that\ndescribe the high-level image semantics from pathology foundation model and the\nfine-grained tissue structures via edge extraction. We demonstrate that, by\nfeeding the prompt mixture to our method, we can restore high-quality pathology\nimages from single-focal-plane scans, implying high potentials of the mixture\nof prompts to clinical usage. Code will be publicly available at\nhttps://github.com/caijd2000/MoP."}
{"id": "2503.12401", "pdf": "https://arxiv.org/pdf/2503.12401", "abs": "https://arxiv.org/abs/2503.12401", "authors": ["Jianwei Zhao", "Xin Li", "Fan Yang", "Qiang Zhai", "Ao Luo", "Yang Zhao", "Hong Cheng", "Huazhu Fu"], "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks."}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition."}
{"id": "2503.12418", "pdf": "https://arxiv.org/pdf/2503.12418", "abs": "https://arxiv.org/abs/2503.12418", "authors": ["Shuo Gao", "Jingyang Zhang", "Jun Xue", "Meng Yang", "Yang Chen", "Guangquan Zhou"], "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, conference", "summary": "Carotid atherosclerosis represents a significant health risk, with its early\ndiagnosis primarily dependent on ultrasound-based assessments of carotid\nintima-media thickening. However, during carotid ultrasound screening,\nsignificant view variations cause style shifts, impairing content cues related\nto thickening, such as lumen anatomy, which introduces spurious correlations\nthat hinder assessment. Therefore, we propose a novel causal-inspired method\nfor assessing carotid intima-media thickening in frame-wise ultrasound videos,\nwhich focuses on two aspects: eliminating spurious correlations caused by style\nand enhancing causal content correlations. Specifically, we introduce a novel\nSpurious Correlation Elimination (SCE) module to remove non-causal style\neffects by enforcing prediction invariance with style perturbations.\nSimultaneously, we propose a Causal Equivalence Consolidation (CEC) module to\nstrengthen causal content correlation through adversarial optimization during\ncontent randomization. Simultaneously, we design a Causal Transition\nAugmentation (CTA) module to ensure smooth causal flow by integrating an\nauxiliary pathway with text prompts and connecting it through contrastive\nlearning. The experimental results on our in-house carotid ultrasound video\ndataset achieved an accuracy of 86.93\\%, demonstrating the superior performance\nof the proposed method. Code is available at\n\\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}."}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture."}
{"id": "2503.12441", "pdf": "https://arxiv.org/pdf/2503.12441", "abs": "https://arxiv.org/abs/2503.12441", "authors": ["Yuda Zou", "Zelong Liu", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "title": "Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "Crowd counting and localization are important in applications such as public\nsecurity and traffic management. Existing methods have achieved impressive\nresults thanks to extensive laborious annotations. This paper propose a novel\npoint-localization-based semi-supervised crowd counting and localization method\ntermed Consistent-Point. We identify and address two inconsistencies of\npseudo-points, which have not been adequately explored. To enhance their\nposition consistency, we aggregate the positions of neighboring auxiliary\nproposal-points. Additionally, an instance-wise uncertainty calibration is\nproposed to improve the class consistency of pseudo-points. By generating more\nconsistent pseudo-points, Consistent-Point provides more stable supervision to\nthe training process, yielding improved results. Extensive experiments across\nfive widely used datasets and three different labeled ratio settings\ndemonstrate that our method achieves state-of-the-art performance in crowd\nlocalization while also attaining impressive crowd counting results. The code\nwill be available."}
{"id": "2503.12446", "pdf": "https://arxiv.org/pdf/2503.12446", "abs": "https://arxiv.org/abs/2503.12446", "authors": ["Tianle Li", "Yongming Rao", "Winston Hu", "Yu Cheng"], "title": "BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Encoder-free multimodal large language models(MLLMs) eliminate the need for a\nwell-trained vision encoder by directly processing image tokens before the\nlanguage model. While this approach reduces computational overhead and model\ncomplexity, it often requires large amounts of training data to effectively\ncapture the visual knowledge typically encoded by vision models like CLIP. The\nabsence of a vision encoder implies that the model is likely to rely on\nsubstantial data to learn the necessary visual-semantic alignments. In this\nwork, we present BREEN, a data-efficient encoder-free multimodal architecture\nthat mitigates this issue. BREEN leverages a learnable query and image experts\nto achieve comparable performance with significantly less training data. The\nlearnable query, positioned between image and text tokens, is supervised by the\noutput of a pretrained CLIP model to distill visual knowledge, bridging the gap\nbetween visual and textual modalities. Additionally, the image expert processes\nimage tokens and learnable queries independently, improving efficiency and\nreducing interference with the LLM's textual capabilities. BREEN achieves\ncomparable performance to prior encoder-free state-of-the-art models like\nMono-InternVL, using only 13 million text-image pairs in training about one\npercent of the data required by existing methods. Our work highlights a\npromising direction for data-efficient encoder-free multimodal learning,\noffering an alternative to traditional encoder-based approaches."}
{"id": "2503.12447", "pdf": "https://arxiv.org/pdf/2503.12447", "abs": "https://arxiv.org/abs/2503.12447", "authors": ["Li Yicong"], "title": "Causality Model for Semantic Understanding on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "PhD Thesis", "summary": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA)."}
{"id": "2503.12450", "pdf": "https://arxiv.org/pdf/2503.12450", "abs": "https://arxiv.org/abs/2503.12450", "authors": ["Feihong Yan", "Qingyan Wei", "Jiayi Tang", "Jiajun Li", "Yulin Wang", "Xuming Hu", "Huiqi Li", "Linfeng Zhang"], "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."}
{"id": "2503.12451", "pdf": "https://arxiv.org/pdf/2503.12451", "abs": "https://arxiv.org/abs/2503.12451", "authors": ["Hossein Ranjbar", "Alireza Taheri"], "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons."}
{"id": "2503.12453", "pdf": "https://arxiv.org/pdf/2503.12453", "abs": "https://arxiv.org/abs/2503.12453", "authors": ["Edgar Heinert", "Thomas Gottwald", "Annika M√ºtze", "Matthias Rottmann"], "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs."}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD."}
{"id": "2503.12461", "pdf": "https://arxiv.org/pdf/2503.12461", "abs": "https://arxiv.org/abs/2503.12461", "authors": ["Fanhu Zeng", "Hao Tang", "Yihua Shao", "Siyu Chen", "Ling Shao", "Yan Wang"], "title": "MambaIC: State Space Models for High-Performance Learned Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to CVPR 2025", "summary": "A high-performance image compression algorithm is crucial for real-time\ninformation transmission across numerous fields. Despite rapid progress in\nimage compression, computational inefficiency and poor redundancy modeling\nstill pose significant bottlenecks, limiting practical applications. Inspired\nby the effectiveness of state space models (SSMs) in capturing long-range\ndependencies, we leverage SSMs to address computational inefficiency in\nexisting methods and improve image compression from multiple perspectives. In\nthis paper, we integrate the advantages of SSMs for better\nefficiency-performance trade-off and propose an enhanced image compression\napproach through refined context modeling, which we term MambaIC. Specifically,\nwe explore context modeling to adaptively refine the representation of hidden\nstates. Additionally, we introduce window-based local attention into\nchannel-spatial entropy modeling to reduce potential spatial redundancy during\ncompression, thereby increasing efficiency. Comprehensive qualitative and\nquantitative results validate the effectiveness and efficiency of our approach,\nparticularly for high-resolution image compression. Code is released at\nhttps://github.com/AuroraZengfh/MambaIC."}
{"id": "2503.12464", "pdf": "https://arxiv.org/pdf/2503.12464", "abs": "https://arxiv.org/abs/2503.12464", "authors": ["Alessio Xompero", "Andrea Cavallaro"], "title": "Learning Privacy from Visual Entities", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages (13 for the main article, 8 for bibliography, acks,\n  appendixes), 9 figures, 12 tables. Article accepted and to appear in the\n  Proceedings on Privacy Enhancing Technologies, 2025 (3):\n  https://petsymposium.org/popets/2025/. To be presented at the Privacy\n  Enhancing Technologies Symposium 2025. Artifact (source code) under review:\n  https://github.com/graphnex/privacy-from-visual-entities", "summary": "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes."}
{"id": "2503.12470", "pdf": "https://arxiv.org/pdf/2503.12470", "abs": "https://arxiv.org/abs/2503.12470", "authors": ["Han Mei", "Kunqian Li", "Shuaixin Liu", "Chengzhi Ma", "Qianli Jiang"], "title": "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https://github.com/OUCVisionGroup/DPF-Net."}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE"}
{"id": "2503.12485", "pdf": "https://arxiv.org/pdf/2503.12485", "abs": "https://arxiv.org/abs/2503.12485", "authors": ["Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Hezhen Hu", "Wengang Zhou", "Houqiang Li"], "title": "Cross-Modal Consistency Learning for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminate background perturbation but\ninevitably suffer from insufficient semantic cues compared to raw RGB videos.\nNevertheless, direct representation learning only from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public."}
{"id": "2503.12490", "pdf": "https://arxiv.org/pdf/2503.12490", "abs": "https://arxiv.org/abs/2503.12490", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Bin Chen", "Zian Guan", "Yuhao Wang", "Xu Jia", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing."}
{"id": "2503.12492", "pdf": "https://arxiv.org/pdf/2503.12492", "abs": "https://arxiv.org/abs/2503.12492", "authors": ["Dapeng Zhao"], "title": "Geometry-Aware Face Reconstruction Under Occluded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods."}
{"id": "2503.12494", "pdf": "https://arxiv.org/pdf/2503.12494", "abs": "https://arxiv.org/abs/2503.12494", "authors": ["Dapeng Zhao"], "title": "Learning Contour-Guided 3D Face Reconstruction with Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods."}
{"id": "2503.12495", "pdf": "https://arxiv.org/pdf/2503.12495", "abs": "https://arxiv.org/abs/2503.12495", "authors": ["Xuan Ma", "Zewen Lv", "Chengcai Ma", "Tao Zhang", "Yuelan Xin", "Kun Zhan"], "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau", "categories": ["cs.CV"], "comment": "Journal of Applied Remote Sensing, 2025", "summary": "Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a\nsignificant environmental challenge due to overgrazing, climate change, and\nrodent activity, which degrade vegetation cover and soil quality. These\nextremely degraded grassland on QTP, commonly referred to as black-soil area,\nrequire accurate assessment to guide effective restoration efforts. In this\npaper, we present a newly created QTP black-soil dataset, annotated under\nexpert guidance. We introduce a novel neural network model, BS-Mamba,\nspecifically designed for the black-soil area detection using UAV remote\nsensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying\nblack-soil area across two independent test datasets than the state-of-the-art\nmodels. This research contributes to grassland restoration by providing an\nefficient method for assessing the extent of black-soil area on the QTP."}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain."}
{"id": "2503.12507", "pdf": "https://arxiv.org/pdf/2503.12507", "abs": "https://arxiv.org/abs/2503.12507", "authors": ["Guangqian Guo", "Yoong Guo", "Xuehui Yu", "Wenbo Li", "Yaoxing Wang", "Shan Gao"], "title": "Segment Any-Quality Images with Generative Latent Space Enhancement", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset."}
{"id": "2503.12515", "pdf": "https://arxiv.org/pdf/2503.12515", "abs": "https://arxiv.org/abs/2503.12515", "authors": ["Pan Du", "Delin An", "Chaoli Wang", "Jian-Xun Wang"], "title": "AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "comment": "42 pages, 8 figures", "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings."}
{"id": "2503.12519", "pdf": "https://arxiv.org/pdf/2503.12519", "abs": "https://arxiv.org/abs/2503.12519", "authors": ["Taein Kwon", "Zador Pataki", "Mahdi Rad", "Marc Pollefeys"], "title": "Multi Activity Sequence Alignment via Implicit Clustering", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Self-supervised temporal sequence alignment can provide rich and effective\nrepresentations for a wide range of applications. However, existing methods for\nachieving optimal performance are mostly limited to aligning sequences of the\nsame activity only and require separate models to be trained for each activity.\nWe propose a novel framework that overcomes these limitations using sequence\nalignment via implicit clustering. Specifically, our key idea is to perform\nimplicit clip-level clustering while aligning frames in sequences. This coupled\nwith our proposed dual augmentation technique enhances the network's ability to\nlearn generalizable and discriminative representations. Our experiments show\nthat our proposed method outperforms state-of-the-art results and highlight the\ngeneralization capability of our framework with multi activity and different\nmodalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will\nrelease our code upon acceptance."}
{"id": "2503.12526", "pdf": "https://arxiv.org/pdf/2503.12526", "abs": "https://arxiv.org/abs/2503.12526", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditID, a training-free approach based on the DiT architecture,\nwhich achieves highly editable customized IDs for text to image generation.\nExisting text-to-image models for customized IDs typically focus more on ID\nconsistency while neglecting editability. It is challenging to alter facial\norientation, character attributes, and other features through prompts. EditID\naddresses this by deconstructing the text-to-image model for customized IDs\ninto an image generation branch and a character feature branch. The character\nfeature branch is further decoupled into three modules: feature extraction,\nfeature fusion, and feature integration. By introducing a combination of\nmapping features and shift features, along with controlling the intensity of ID\nfeature integration, EditID achieves semantic compression of local features\nacross network depths, forming an editable feature space. This enables the\nsuccessful generation of high-quality images with editable IDs while\nmaintaining ID consistency, achieving excellent results in the IBench\nevaluation, which is an editability evaluation framework for the field of\ncustomized ID text-to-image generation that quantitatively demonstrates the\nsuperior performance of EditID. EditID is the first text-to-image solution to\npropose customizable ID editability on the DiT architecture, meeting the\ndemands of long prompts and high quality image generation."}
{"id": "2503.12527", "pdf": "https://arxiv.org/pdf/2503.12527", "abs": "https://arxiv.org/abs/2503.12527", "authors": ["Yang Yi", "Kunqing Wang", "Jinpu Zhang", "Zhen Tan", "Xiangke Wang", "Hui Shen", "Dewen Hu"], "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}."}
{"id": "2503.12531", "pdf": "https://arxiv.org/pdf/2503.12531", "abs": "https://arxiv.org/abs/2503.12531", "authors": ["Mehmet Kerem Turkcan", "Mattia Ballo", "Filippo Filicori", "Zoran Kostic"], "title": "Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks", "categories": ["cs.CV"], "comment": null, "summary": "We introduce specialized diffusion-based generative models that capture the\nspatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions\nthrough supervised learning on annotated laparoscopic surgery footage. The\nproposed models form a foundation for data-driven world models capable of\nsimulating the biomechanical interactions and procedural dynamics of surgical\nsuturing with high temporal fidelity. Annotating a dataset of $\\sim2K$ clips\nextracted from simulation videos, we categorize surgical actions into\nfine-grained sub-stitch classes including ideal and non-ideal executions of\nneedle positioning, targeting, driving, and withdrawal. We fine-tune two\nstate-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to\ngenerate high-fidelity surgical action sequences at $\\ge$768x512 resolution and\n$\\ge$49 frames. For training our models, we explore both Low-Rank Adaptation\n(LoRA) and full-model fine-tuning approaches. Our experimental results\ndemonstrate that these world models can effectively capture the dynamics of\nsuturing, potentially enabling improved training simulators, surgical skill\nassessment tools, and autonomous surgical systems. The models also display the\ncapability to differentiate between ideal and non-ideal technique execution,\nproviding a foundation for building surgical training and evaluation systems.\nWe release our models for testing and as a foundation for future research.\nProject Page: https://mkturkcan.github.io/suturingmodels/"}
{"id": "2503.12532", "pdf": "https://arxiv.org/pdf/2503.12532", "abs": "https://arxiv.org/abs/2503.12532", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Ziqin Wei", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE."}
{"id": "2503.12535", "pdf": "https://arxiv.org/pdf/2503.12535", "abs": "https://arxiv.org/abs/2503.12535", "authors": ["Guibiao Liao", "Qing Li", "Zhenyu Bao", "Guoping Qiu", "Kanglin Liu"], "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. The project page is available at\n  https://gbliao.github.io/SPC-GS.github.io/", "summary": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches\nhave shown significant performance with dense input images. However, they\nexhibit poor performance when confronted with sparse inputs, primarily due to\nthe sparse distribution of Gaussian points and insufficient view supervision.\nTo relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based\nGaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)\nRegularization for open-world free view synthesis with sparse inputs.\nSpecifically, SGI provides a dense, scene-layout-based Gaussian distribution by\nutilizing view-changed images generated from the video generation model and\nview-constraint Gaussian points densification. Additionally, SPC mitigates\nlimited view supervision by employing semantic-prompt-based consistency\nconstraints developed by SAM2. This approach leverages available semantics from\ntraining views, serving as instructive prompts, to optimize visually\noverlapping regions in novel views with 2D and 3D consistency constraints.\nExtensive experiments demonstrate the superior performance of SPC-GS across\nReplica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in\nPSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world\nsemantic segmentation."}
{"id": "2503.12539", "pdf": "https://arxiv.org/pdf/2503.12539", "abs": "https://arxiv.org/abs/2503.12539", "authors": ["Weiguang Zhao", "Rui Zhang", "Qiufeng Wang", "Guangliang Cheng", "Kaizhu Huang"], "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps://github.com/weiguangzhao/BFANet."}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research."}
{"id": "2503.12545", "pdf": "https://arxiv.org/pdf/2503.12545", "abs": "https://arxiv.org/abs/2503.12545", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Xiaojiang Peng", "Kai Wang", "Yang You", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs."}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindstr√∂m", "Peng Su", "Matthias Nie√üner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public."}
{"id": "2503.12559", "pdf": "https://arxiv.org/pdf/2503.12559", "abs": "https://arxiv.org/abs/2503.12559", "authors": ["Xiao Wang", "Qingyi Si", "Jianlong Wu", "Shiyu Zhu", "Li Cao", "Liqiang Nie"], "title": "AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized video\nunderstanding, yet are still limited by context length when processing long\nvideos. Recent methods compress videos by leveraging visual redundancy\nuniformly, yielding promising results. Nevertheless, our quantitative analysis\nshows that redundancy varies significantly across time and model layers,\nnecessitating a more flexible compression strategy. We propose AdaReTaKe, a\ntraining-free method that flexibly reduces visual redundancy by allocating\ncompression ratios among time and layers with theoretical guarantees.\nIntegrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity\nfrom 256 to 2048 frames while preserving critical information. Experiments on\nVideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe\noutperforms existing methods by 2.3% and 2.8% for 7B and 72B models,\nrespectively, with even greater improvements of 5.9% and 6.0% on the longest\nLVBench. Our code is available at\nhttps://github.com/SCZwangxiao/video-FlexReduc.git."}
{"id": "2503.12562", "pdf": "https://arxiv.org/pdf/2503.12562", "abs": "https://arxiv.org/abs/2503.12562", "authors": ["Ruopeng Gao", "Yuyao Wang", "Chunxu Liu", "Limin Wang"], "title": "History-Aware Transformation of ReID Features for Multiple Object Tracking", "categories": ["cs.CV"], "comment": "Tech report. Without bells and whistles, achieving 80.8 HOTA on\n  SportsMOT", "summary": "The aim of multiple object tracking (MOT) is to detect all objects in a video\nand bind them into multiple trajectories. Generally, this process is carried\nout in two steps: detecting objects and associating them across frames based on\nvarious cues and metrics. Many studies and applications adopt object\nappearance, also known as re-identification (ReID) features, for target\nmatching through straightforward similarity calculation. However, we argue that\nthis practice is overly naive and thus overlooks the unique characteristics of\nMOT tasks. Unlike regular re-identification tasks that strive to distinguish\nall potential targets in a general representation, multi-object tracking\ntypically immerses itself in differentiating similar targets within the same\nvideo sequence. Therefore, we believe that seeking a more suitable feature\nrepresentation space based on the different sample distributions of each\nsequence will enhance tracking performance. In this paper, we propose using\nhistory-aware transformations on ReID features to achieve more discriminative\nappearance representations. Specifically, we treat historical trajectory\nfeatures as conditions and employ a tailored Fisher Linear Discriminant (FLD)\nto find a spatial projection matrix that maximizes the differentiation between\ndifferent trajectories. Our extensive experiments reveal that this\ntraining-free projection can significantly boost feature-only trackers to\nachieve competitive, even superior tracking performance compared to\nstate-of-the-art methods while also demonstrating impressive zero-shot transfer\ncapabilities. This demonstrates the effectiveness of our proposal and further\nencourages future investigation into the importance and customization of ReID\nmodels in multiple object tracking. The code will be released at\nhttps://github.com/HELLORPG/HATReID-MOT."}
{"id": "2503.12567", "pdf": "https://arxiv.org/pdf/2503.12567", "abs": "https://arxiv.org/abs/2503.12567", "authors": ["Abyad Enan", "Mashrur Chowdhury"], "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) for possible publication", "summary": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel."}
{"id": "2503.12572", "pdf": "https://arxiv.org/pdf/2503.12572", "abs": "https://arxiv.org/abs/2503.12572", "authors": ["Francesco Girlanda", "Denys Rozumnyi", "Marc Pollefeys", "Martin R. Oswald"], "title": "Deblur Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp\nreconstructions from motion-blurred inputs. The proposed method bridges the\nstrengths of both frame-to-frame and frame-to-model approaches to model\nsub-frame camera trajectories that lead to high-fidelity reconstructions in\nmotion-blurred settings. Moreover, our pipeline incorporates techniques such as\nonline loop closure and global bundle adjustment to achieve a dense and precise\nglobal trajectory. We model the physical image formation process of\nmotion-blurred images and minimize the error between the observed blurry images\nand rendered blurry images obtained by averaging sharp virtual sub-frame\nimages. Additionally, by utilizing a monocular depth estimator alongside the\nonline deformation of Gaussians, we ensure precise mapping and enhanced image\ndeblurring. The proposed SLAM pipeline integrates all these components to\nimprove the results. We achieve state-of-the-art results for sharp map\nestimation and sub-frame trajectory recovery both on synthetic and real-world\nblurry input data."}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO."}
{"id": "2503.12588", "pdf": "https://arxiv.org/pdf/2503.12588", "abs": "https://arxiv.org/abs/2503.12588", "authors": ["Xiaoyu Han", "Shengping Zhang", "Qinglin Liu", "Zonglin Li", "Chenyang Wang"], "title": "Progressive Limb-Aware Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2022. The code is available at\n  https://github.com/xyhanHIT/PL-VTON", "summary": "Existing image-based virtual try-on methods directly transfer specific\nclothing to a human image without utilizing clothing attributes to refine the\ntransferred clothing geometry and textures, which causes incomplete and blurred\nclothing appearances. In addition, these methods usually mask the limb textures\nof the input for the clothing-agnostic person representation, which results in\ninaccurate predictions for human limb regions (i.e., the exposed arm skin),\nespecially when transforming between long-sleeved and short-sleeved garments.\nTo address these problems, we present a progressive virtual try-on framework,\nnamed PL-VTON, which performs pixel-level clothing warping based on multiple\nattributes of clothing and embeds explicit limb-aware features to generate\nphoto-realistic try-on results. Specifically, we design a Multi-attribute\nClothing Warping (MCW) module that adopts a two-stage alignment strategy based\non multiple attributes to progressively estimate pixel-level clothing\ndisplacements. A Human Parsing Estimator (HPE) is then introduced to\nsemantically divide the person into various regions, which provides structural\nconstraints on the human body and therefore alleviates texture bleeding between\nclothing and limb regions. Finally, we propose a Limb-aware Texture Fusion\n(LTF) module to estimate high-quality details in limb regions by fusing\ntextures of the clothing and the human body with the guidance of explicit\nlimb-aware features. Extensive experiments demonstrate that our proposed method\noutperforms the state-of-the-art virtual try-on methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON."}
{"id": "2503.12590", "pdf": "https://arxiv.org/pdf/2503.12590", "abs": "https://arxiv.org/abs/2503.12590", "authors": ["Haoran Feng", "Zehuan Huang", "Lin Li", "Hairong Lv", "Lu Sheng"], "title": "Personalize Anything for Free with Diffusion Transformer", "categories": ["cs.CV"], "comment": "https://fenghora.github.io/Personalize-Anything-Page/", "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose \\textbf{Personalize\nAnything}, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization."}
{"id": "2503.12595", "pdf": "https://arxiv.org/pdf/2503.12595", "abs": "https://arxiv.org/abs/2503.12595", "authors": ["Dan Halperin", "Niklas Eisl"], "title": "Point Cloud Based Scene Segmentation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency."}
{"id": "2503.12605", "pdf": "https://arxiv.org/pdf/2503.12605", "abs": "https://arxiv.org/abs/2503.12605", "authors": ["Yaoting Wang", "Shengqiong Wu", "Yuecheng Zhang", "William Wang", "Ziwei Liu", "Jiebo Luo", "Hao Fei"], "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "survey resource at\n  https://github.com/yaotingwangofficial/Awesome-MCoT; 12 figures, 4 tables, 44\n  pages", "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI."}
{"id": "2503.12615", "pdf": "https://arxiv.org/pdf/2503.12615", "abs": "https://arxiv.org/abs/2503.12615", "authors": ["Alessio Spagnoletti", "Jean Prost", "Andr√©s Almansa", "Nicolas Papadakis", "Marcelo Pereyra"], "title": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization", "categories": ["cs.CV", "cs.LG"], "comment": "27 pages, 20 figures", "summary": "Text-to-image latent diffusion models (LDMs) have recently emerged as\npowerful generative models with great potential for solving inverse problems in\nimaging. However, leveraging such models in a Plug & Play (PnP), zero-shot\nmanner remains challenging because it requires identifying a suitable text\nprompt for the unknown image of interest. Also, existing text-to-image PnP\napproaches are highly computationally expensive. We herein address these\nchallenges by proposing a novel PnP inference paradigm specifically designed\nfor embedding generative models within stochastic inverse solvers, with special\nattention to Latent Consistency Models (LCMs), which distill LDMs into fast\ngenerators. We leverage our framework to propose LAtent consisTency INverse\nsOlver (LATINO), the first zero-shot PnP framework to solve inverse problems\nwith priors encoded by LCMs. Our conditioning mechanism avoids automatic\ndifferentiation and reaches SOTA quality in as little as 8 neural function\nevaluations. As a result, LATINO delivers remarkably accurate solutions and is\nsignificantly more memory and computationally efficient than previous\napproaches. We then embed LATINO within an empirical Bayesian framework that\nautomatically calibrates the text prompt from the observed measurements by\nmarginal maximum likelihood estimation. Extensive experiments show that prompt\nself-calibration greatly improves estimation, allowing LATINO with PRompt\nOptimization to define new SOTAs in image reconstruction quality and\ncomputational efficiency."}
{"id": "2503.12617", "pdf": "https://arxiv.org/pdf/2503.12617", "abs": "https://arxiv.org/abs/2503.12617", "authors": ["Anthony Lamelas", "Harrison Muchnic"], "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)", "summary": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs."}
{"id": "2503.12627", "pdf": "https://arxiv.org/pdf/2503.12627", "abs": "https://arxiv.org/abs/2503.12627", "authors": ["Rui Cao"], "title": "Online Misinformation Detection in Live Streaming Videos", "categories": ["cs.CV", "cs.CL"], "comment": "First prize winner in the Smart City Challenge in the 16th ACM\n  international WSDM conference(WSDM), 2023", "summary": "Online misinformation detection is an important issue and methods are\nproposed to detect and curb misinformation in various forms. However, previous\nstudies are conducted in an offline manner. We claim a realistic misinformation\ndetection setting that has not been studied yet is online misinformation\ndetection in live streaming videos (MDLS). In the proposal, we formulate the\nproblem of MDLS and illustrate the importance and the challenge of the task.\nBesides, we propose feasible ways of developing the problem into AI challenges\nas well as potential solutions to the problem."}
{"id": "2503.12652", "pdf": "https://arxiv.org/pdf/2503.12652", "abs": "https://arxiv.org/abs/2503.12652", "authors": ["Tsu-Jui Fu", "Yusu Qian", "Chen Chen", "Wenze Hu", "Zhe Gan", "Yinfei Yang"], "title": "UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models have shown impressive results in\ngenerating visually compelling images following user prompts. Building on this,\nvarious methods further fine-tune the pre-trained T2I model for specific tasks.\nHowever, this requires separate model architectures, training designs, and\nmultiple parameter sets to handle different tasks. In this paper, we introduce\nUniVG, a generalist diffusion model capable of supporting a diverse range of\nimage generation tasks with a single set of weights. UniVG treats multi-modal\ninputs as unified conditions to enable various downstream applications, ranging\nfrom T2I generation, inpainting, instruction-based editing, identity-preserving\ngeneration, and layout-guided generation, to depth estimation and referring\nsegmentation. Through comprehensive empirical studies on data mixing and\nmulti-task training, we provide detailed insights into the training processes\nand decisions that inform our final designs. For example, we show that T2I\ngeneration and other tasks, such as instruction-based editing, can coexist\nwithout performance trade-offs, while auxiliary tasks like depth estimation and\nreferring segmentation enhance image editing. Notably, our model can even\noutperform some task-specific models on their respective benchmarks, marking a\nsignificant step towards a unified image generation model."}
{"id": "2503.12663", "pdf": "https://arxiv.org/pdf/2503.12663", "abs": "https://arxiv.org/abs/2503.12663", "authors": ["Imran Kabir", "Md Alimoor Reza", "Syed Billah"], "title": "Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "Large multimodal models (LMMs) are increasingly integrated into autonomous\ndriving systems for user interaction. However, their limitations in\nfine-grained spatial reasoning pose challenges for system interpretability and\nuser trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation\n(RAG) framework that improves LMMs' spatial understanding in driving scenarios.\nLogic-RAG constructs a dynamic knowledge base (KB) about object-object\nrelationships in first-order logic (FOL) using a perception module, a\nquery-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG\non visual-spatial queries using both synthetic and real-world driving videos.\nWhen using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous\ndriving system, these models achieved only 55% accuracy on synthetic driving\nscenes and under 75% on real-world driving scenes. Augmenting them with\nLogic-RAG increased their accuracies to over 80% and 90%, respectively. An\nablation study showed that even without logical inference, the fact-based\ncontext constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is\nextensible: it allows seamless replacement of individual components with\nimproved versions and enables domain experts to compose new knowledge in both\nFOL and natural language. In sum, Logic-RAG addresses critical spatial\nreasoning deficiencies in LMMs for autonomous driving applications. Code and\ndata are available at https://github.com/Imran2205/LogicRAG."}
{"id": "2503.12678", "pdf": "https://arxiv.org/pdf/2503.12678", "abs": "https://arxiv.org/abs/2503.12678", "authors": ["Partho Ghosh", "Raisa Bentay Hossain", "Mohammad Zunaed", "Taufiq Hasan"], "title": "Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing", "categories": ["cs.CV"], "comment": null, "summary": "Automatic video activity recognition is crucial across numerous domains like\nsurveillance, healthcare, and robotics. However, recognizing human activities\nfrom video data becomes challenging when training and test data stem from\ndiverse domains. Domain generalization, adapting to unforeseen domains, is thus\nessential. This paper focuses on office activity recognition amidst\nenvironmental variability. We propose three pre-processing techniques\napplicable to any video encoder, enhancing robustness against environmental\nvariations. Our study showcases the efficacy of MViT, a leading\nstate-of-the-art video classification model, and other video encoders combined\nwith our techniques, outperforming state-of-the-art domain adaptation methods.\nOur approach significantly boosts accuracy, precision, recall and F1 score on\nunseen domains, emphasizing its adaptability in real-world scenarios with\ndiverse video data sources. This method lays a foundation for more reliable\nvideo activity recognition systems across heterogeneous data domains."}
{"id": "2503.12688", "pdf": "https://arxiv.org/pdf/2503.12688", "abs": "https://arxiv.org/abs/2503.12688", "authors": ["Tianyuan Wang"], "title": "Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In industrial X-ray Computed Tomography (CT), the need for rapid in-line\ninspection is critical. Sparse-angle tomography plays a significant role in\nthis by reducing the required number of projections, thereby accelerating\nprocessing and conserving resources. Most existing methods aim to balance\nreconstruction quality and scanning time, typically relying on fixed scan\ndurations. Adaptive adjustment of the number of angles is essential; for\ninstance, more angles may be required for objects with complex geometries or\nnoisier projections. The concept of optimal stopping, which dynamically adjusts\nthis balance according to varying industrial needs, remains underutilized.\nBuilding on our previous work, we integrate optimal stopping into sequential\nOptimal Experimental Design (OED). We propose a novel method for computing the\npolicy gradient within the Actor-Critic framework, enabling the development of\nadaptive policies for informative angle selection and scan termination.\nAdditionally, we investigated the gap between simulation and real-world\napplications in the context of the developed learning-based method. Our trained\nmodel, developed using synthetic data, demonstrates reliable performance when\napplied to real-world data. This approach enhances the flexibility of CT\noperations and expands the applicability of sparse-angle tomography in\nindustrial settings."}
{"id": "2503.12689", "pdf": "https://arxiv.org/pdf/2503.12689", "abs": "https://arxiv.org/abs/2503.12689", "authors": ["Hengjia Li", "Lifan Jiang", "Xi Xiao", "Tianyang Wang", "Hongwei Yi", "Boxi Wu", "Deng Cai"], "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization", "categories": ["cs.CV"], "comment": null, "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\n$\\textbf{MagicID}$, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics."}
{"id": "2503.12701", "pdf": "https://arxiv.org/pdf/2503.12701", "abs": "https://arxiv.org/abs/2503.12701", "authors": ["Javier Tirado-Gar√≠n", "Javier Civera"], "title": "AnyCalib: On-Manifold Learning for Model-Agnostic Single-View Camera Calibration", "categories": ["cs.CV"], "comment": null, "summary": "We present AnyCalib, a method for calibrating the intrinsic parameters of a\ncamera from a single in-the-wild image, that is agnostic to the camera model.\nCurrent methods are predominantly tailored to specific camera models and/or\nrequire extrinsic cues, such as the direction of gravity, to be visible in the\nimage. In contrast, we argue that the perspective and distortion cues inherent\nin images are sufficient for model-agnostic camera calibration. To demonstrate\nthis, we frame the calibration process as the regression of the rays\ncorresponding to each pixel. We show, for the first time, that this\nintermediate representation allows for a closed-form recovery of the intrinsics\nfor a wide range of camera models, including but not limited to: pinhole,\nBrown-Conrady and Kannala-Brandt. Our approach also applies to edited --\ncropped and stretched -- images. Experimentally, we demonstrate that AnyCalib\nconsistently outperforms alternative methods, including 3D foundation models,\ndespite being trained on orders of magnitude less data. Code is available at\nhttps://github.com/javrtg/AnyCalib."}
{"id": "2503.12706", "pdf": "https://arxiv.org/pdf/2503.12706", "abs": "https://arxiv.org/abs/2503.12706", "authors": ["Rahul Deshmukh", "Avinash Kak"], "title": "SatDepth: A Novel Dataset for Satellite Image Matching", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep-learning based methods for image matching have\ndemonstrated their superiority over traditional algorithms, enabling\ncorrespondence estimation in challenging scenes with significant differences in\nviewing angles, illumination and weather conditions. However, the existing\ndatasets, learning frameworks, and evaluation metrics for the deep-learning\nbased methods are limited to ground-based images recorded with pinhole cameras\nand have not been explored for satellite images. In this paper, we present\n``SatDepth'', a novel dataset that provides dense ground-truth correspondences\nfor training image matching frameworks meant specifically for satellite images.\nSatellites capture images from various viewing angles and tracks through\nmultiple revisits over a region. To manage this variability, we propose a\ndataset balancing strategy through a novel image rotation augmentation\nprocedure. This procedure allows for the discovery of corresponding pixels even\nin the presence of large rotational differences between the images. We\nbenchmark four existing image matching frameworks using our dataset and carry\nout an ablation study that confirms that the models trained with our dataset\nwith rotation augmentation outperform (up to 40% increase in precision) the\nmodels trained with other datasets, especially when there exist large\nrotational differences between the images."}
{"id": "2503.12720", "pdf": "https://arxiv.org/pdf/2503.12720", "abs": "https://arxiv.org/abs/2503.12720", "authors": ["Feng Qiao", "Zhexiao Xiong", "Eric Xing", "Nathan Jacobs"], "title": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching", "categories": ["cs.CV"], "comment": "Project page is available at https://qjizhi.github.io/genstereo", "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo"}
{"id": "2503.12731", "pdf": "https://arxiv.org/pdf/2503.12731", "abs": "https://arxiv.org/abs/2503.12731", "authors": ["Haoran Ma", "Kaihan Zhang", "Jiannan Cai"], "title": "Navigating Heat Exposure: Simulation of Route Planning Based on Visual Language Model Agents", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Heat exposure significantly influences pedestrian routing behaviors. Existing\nmethods such as agent-based modeling (ABM) and empirical measurements fail to\naccount for individual physiological variations and environmental perception\nmechanisms under thermal stress. This results in a lack of human-centred,\nheat-adaptive routing suggestions. To address these limitations, we propose a\nnovel Vision Language Model (VLM)-driven Persona-Perception-Planning-Memory\n(PPPM) framework that integrating street view imagery and urban network\ntopology to simulate heat-adaptive pedestrian routing. Through structured\nprompt engineering on Gemini-2.0 model, eight distinct heat-sensitive personas\nwere created to model mobility behaviors during heat exposure, with empirical\nvalidation through questionnaire survey. Results demonstrate that simulation\noutputs effectively capture inter-persona variations, achieving high\nsignificant congruence with observed route preferences and highlighting\ndifferences in the factors driving agents decisions. Our framework is highly\ncost-effective, with simulations costing 0.006USD and taking 47.81s per route.\nThis Artificial Intelligence-Generated Content (AIGC) methodology advances\nurban climate adaptation research by enabling high-resolution simulation of\nthermal-responsive mobility patterns, providing actionable insights for\nclimate-resilient urban planning."}
{"id": "2503.12732", "pdf": "https://arxiv.org/pdf/2503.12732", "abs": "https://arxiv.org/abs/2503.12732", "authors": ["Zibin Liu", "Banglei Guan", "Yang Shang", "Yifei Bian", "Pengju Sun", "Qifeng Yu"], "title": "Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Geoscience and Remote Sensing", "summary": "Pose tracking of uncooperative spacecraft is an essential technology for\nspace exploration and on-orbit servicing, which remains an open problem. Event\ncameras possess numerous advantages, such as high dynamic range, high temporal\nresolution, and low power consumption. These attributes hold the promise of\novercoming challenges encountered by conventional cameras, including motion\nblur and extreme illumination, among others. To address the standard on-orbit\nobservation missions, we propose a line-based pose tracking method for\nuncooperative spacecraft utilizing a stereo event camera. To begin with, we\nestimate the wireframe model of uncooperative spacecraft, leveraging the\nspatio-temporal consistency of stereo event streams for line-based\nreconstruction. Then, we develop an effective strategy to establish\ncorrespondences between events and projected lines of uncooperative spacecraft.\nUsing these correspondences, we formulate the pose tracking as a continuous\noptimization process over 6-DOF motion parameters, achieved by minimizing\nevent-line distances. Moreover, we construct a stereo event-based uncooperative\nspacecraft motion dataset, encompassing both simulated and real events. The\nproposed method is quantitatively evaluated through experiments conducted on\nour self-collected dataset, demonstrating an improvement in terms of\neffectiveness and accuracy over competing methods. The code will be\nopen-sourced at https://github.com/Zibin6/SE6PT."}
{"id": "2503.12745", "pdf": "https://arxiv.org/pdf/2503.12745", "abs": "https://arxiv.org/abs/2503.12745", "authors": ["Patrick Rim", "Hyoungseob Park", "S. Gangopadhyay", "Ziyao Zeng", "Younjoon Chung", "Alex Wong"], "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We present ProtoDepth, a novel prototype-based approach for continual\nlearning of unsupervised depth completion, the multimodal 3D reconstruction\ntask of predicting dense depth maps from RGB images and sparse point clouds.\nThe unsupervised learning paradigm is well-suited for continual learning, as\nground truth is not needed. However, when training on new non-stationary\ndistributions, depth completion models will catastrophically forget previously\nlearned information. We address forgetting by learning prototype sets that\nadapt the latent features of a frozen pretrained model to new domains. Since\nthe original weights are not modified, ProtoDepth does not forget when\ntest-time domain identity is known. To extend ProtoDepth to the challenging\nsetting where the test-time domain identity is withheld, we propose to learn\ndomain descriptors that enable the model to select the appropriate prototype\nset for inference. We evaluate ProtoDepth on benchmark dataset sequences, where\nwe reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for\noutdoor to achieve the state of the art."}
{"id": "2503.12751", "pdf": "https://arxiv.org/pdf/2503.12751", "abs": "https://arxiv.org/abs/2503.12751", "authors": ["Yifan Zhan", "Wangze Xu", "Qingtian Zhu", "Muyao Niu", "Mingze Ma", "Yifei Liu", "Zhihang Zhong", "Xiao Sun", "Yinqiang Zheng"], "title": "R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars", "categories": ["cs.CV"], "comment": null, "summary": "We present R3-Avatar, incorporating a temporal codebook, to overcome the\ninability of human avatars to be both animatable and of high-fidelity rendering\nquality. Existing video-based reconstruction of 3D human avatars either focuses\nsolely on rendering, lacking animation support, or learns a pose-appearance\nmapping for animating, which degrades under limited training poses or complex\nclothing. In this paper, we adopt a \"record-retrieve-reconstruct\" strategy that\nensures high-quality rendering from novel views while mitigating degradation in\nnovel poses. Specifically, disambiguating timestamps record temporal appearance\nvariations in a codebook, ensuring high-fidelity novel-view rendering, while\nnovel poses retrieve corresponding timestamps by matching the most similar\ntraining poses for augmented appearance. Our R3-Avatar outperforms cutting-edge\nvideo-based human avatar reconstruction, particularly in overcoming visual\nquality degradation in extreme scenarios with limited training human poses and\ncomplex clothing."}
{"id": "2503.12758", "pdf": "https://arxiv.org/pdf/2503.12758", "abs": "https://arxiv.org/abs/2503.12758", "authors": ["Zhifeng Wang", "Renjiao Yi", "Xin Wen", "Chenyang Zhu", "Kai Xu"], "title": "VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for Angiography Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Angiography imaging is a medical imaging technique that enhances the\nvisibility of blood vessels within the body by using contrast agents.\nAngiographic images can effectively assist in the diagnosis of vascular\ndiseases. However, contrast agents may bring extra radiation exposure which is\nharmful to patients with health risks. To mitigate these concerns, in this\npaper, we aim to automatically generate angiography from non-angiographic\ninputs, by leveraging and enhancing the inherent physical properties of\nvascular structures. Previous methods relying on 2D slice-based angiography\nsynthesis struggle with maintaining continuity in 3D vascular structures and\nexhibit limited effectiveness across different imaging modalities. We propose\nVasTSD, a 3D vascular tree-state space diffusion model to synthesize\nangiography from 3D non-angiographic volumes, with a novel state space\nserialization approach that dynamically constructs vascular tree topologies,\nintegrating these with a diffusion-based generative model to ensure the\ngeneration of anatomically continuous vasculature in 3D volumes. A pre-trained\nvision embedder is employed to construct vascular state space representations,\nenabling consistent modeling of vascular structures across multiple modalities.\nExtensive experiments on various angiographic datasets demonstrate the\nsuperiority of VasTSD over prior works, achieving enhanced continuity of blood\nvessels in synthesized angiographic synthesis for multiple modalities and\nanatomical regions."}
{"id": "2503.12763", "pdf": "https://arxiv.org/pdf/2503.12763", "abs": "https://arxiv.org/abs/2503.12763", "authors": ["Kewei Sui", "Anindita Ghosh", "Inwoo Hwang", "Jian Wang", "Chuan Guo"], "title": "A Survey on Human Interaction Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": "The repository listing relevant papers is accessible at:\n  https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation", "summary": "Humans inhabit a world defined by interactions -- with other humans, objects,\nand environments. These interactive movements not only convey our relationships\nwith our surroundings but also demonstrate how we perceive and communicate with\nthe real world. Therefore, replicating these interaction behaviors in digital\nsystems has emerged as an important topic for applications in robotics, virtual\nreality, and animation. While recent advances in deep generative models and new\ndatasets have accelerated progress in this field, significant challenges remain\nin modeling the intricate human dynamics and their interactions with entities\nin the external world. In this survey, we present, for the first time, a\ncomprehensive overview of the literature in human interaction motion\ngeneration. We begin by establishing foundational concepts essential for\nunderstanding the research background. We then systematically review existing\nsolutions and datasets across three primary interaction tasks -- human-human,\nhuman-object, and human-scene interactions -- followed by evaluation metrics.\nFinally, we discuss open research directions and future opportunities."}
{"id": "2503.12764", "pdf": "https://arxiv.org/pdf/2503.12764", "abs": "https://arxiv.org/abs/2503.12764", "authors": ["Yidi Liu", "Dong Li", "Yuxin Ma", "Jie Huang", "Wenlong Zhang", "Xueyang Fu", "Zheng-jun Zha"], "title": "Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-definition (UHD) image restoration often faces computational\nbottlenecks and information loss due to its extremely high resolution. Existing\nstudies based on Variational Autoencoders (VAE) improve efficiency by\ntransferring the image restoration process from pixel space to latent space.\nHowever, degraded components are inherently coupled with background elements in\ndegraded images, both information loss during compression and information gain\nduring compensation remain uncontrollable. These lead to restored images often\nexhibiting image detail loss and incomplete degradation removal. To address\nthis issue, we propose a Controlled Differential Disentangled VAE, which\nutilizes Hierarchical Contrastive Disentanglement Learning and an Orthogonal\nGated Projection Module to guide the VAE to actively discard easily recoverable\nbackground information while encoding more difficult-to-recover degraded\ninformation into the latent space. Additionally, we design a Complex Invertible\nMultiscale Fusion Network to handle background features, ensuring their\nconsistency, and utilize a latent space restoration network to transform the\ndegraded latent features, leading to more accurate restoration results.\nExtensive experimental results demonstrate that our method effectively\nalleviates the information loss problem in VAE models while ensuring\ncomputational efficiency, significantly improving the quality of UHD image\nrestoration, and achieves state-of-the-art results in six UHD restoration tasks\nwith only 1M parameters."}
{"id": "2503.12769", "pdf": "https://arxiv.org/pdf/2503.12769", "abs": "https://arxiv.org/abs/2503.12769", "authors": ["Shenghao Fu", "Qize Yang", "Yuan-Ming Li", "Yi-Xing Peng", "Kun-Yu Lin", "Xihan Wei", "Jian-Fang Hu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research."}
{"id": "2503.12772", "pdf": "https://arxiv.org/pdf/2503.12772", "abs": "https://arxiv.org/abs/2503.12772", "authors": ["Sung-Yeon Park", "Can Cui", "Yunsheng Ma", "Ahmadreza Moradipari", "Rohit Gupta", "Kyungtae Han", "Ziran Wang"], "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA."}
{"id": "2503.12778", "pdf": "https://arxiv.org/pdf/2503.12778", "abs": "https://arxiv.org/abs/2503.12778", "authors": ["Gul Sheeraz", "Qun Chen", "Liu Feiyu", "Zhou Fengjin MD"], "title": "Adaptive Deep Learning for Multiclass Breast Cancer Classification via Misprediction Risk Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer remains one of the leading causes of cancer-related deaths\nworldwide. Early detection is crucial for improving patient outcomes, yet the\ndiagnostic process is often complex and prone to inconsistencies among\npathologists. Computer-aided diagnostic approaches have significantly enhanced\nbreast cancer detection, particularly in binary classification (benign vs.\nmalignant). However, these methods face challenges in multiclass\nclassification, leading to frequent mispredictions. In this work, we propose a\nnovel adaptive learning approach for multiclass breast cancer classification\nusing H&E-stained histopathology images. First, we introduce a misprediction\nrisk analysis framework that quantifies and ranks the likelihood of an image\nbeing mislabeled by a classifier. This framework leverages an interpretable\nrisk model that requires only a small number of labeled samples for training.\nNext, we present an adaptive learning strategy that fine-tunes classifiers\nbased on the specific characteristics of a given dataset. This approach\nminimizes misprediction risk, allowing the classifier to adapt effectively to\nthe target workload. We evaluate our proposed solutions on real benchmark\ndatasets, demonstrating that our risk analysis framework more accurately\nidentifies mispredictions compared to existing methods. Furthermore, our\nadaptive learning approach significantly improves the performance of\nstate-of-the-art deep neural network classifiers."}
{"id": "2503.12779", "pdf": "https://arxiv.org/pdf/2503.12779", "abs": "https://arxiv.org/abs/2503.12779", "authors": ["Haoxiao Wang", "Kaichen Zhou", "Binrui Gu", "Zhiyuan Feng", "Weijie Wang", "Peilin Sun", "Yicheng Xiao", "Jianhua Zhang", "Hao Dong"], "title": "TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Manipulating transparent objects presents significant challenges due to the\ncomplexities introduced by their reflection and refraction properties, which\nconsiderably hinder the accurate estimation of their 3D shapes. To address\nthese challenges, we propose a single-view RGB-D-based depth completion\nframework, TransDiff, that leverages the Denoising Diffusion Probabilistic\nModels(DDPM) to achieve material-agnostic object grasping in desktop.\nSpecifically, we leverage features extracted from RGB images, including\nsemantic segmentation, edge maps, and normal maps, to condition the depth map\ngeneration process. Our method learns an iterative denoising process that\ntransforms a random depth distribution into a depth map, guided by initially\nrefined depth information, ensuring more accurate depth estimation in scenarios\ninvolving transparent objects. Additionally, we propose a novel training method\nto better align the noisy depth and RGB image features, which are used as\nconditions to refine depth estimation step by step. Finally, we utilized an\nimproved inference process to accelerate the denoising procedure. Through\ncomprehensive experimental validation, we demonstrate that our method\nsignificantly outperforms the baselines in both synthetic and real-world\nbenchmarks with acceptable inference time. The demo of our method can be found\non https://wang-haoxiao.github.io/TransDiff/"}
{"id": "2503.12780", "pdf": "https://arxiv.org/pdf/2503.12780", "abs": "https://arxiv.org/abs/2503.12780", "authors": ["Chang Liu", "Bavesh Balaji", "Saad Hossain", "C Thomas", "Kwei-Herng Lai", "Raviteja Vemulapalli", "Alexander Wong", "Sirisha Rambhatla"], "title": "LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML", "68Txx", "I.2.1"], "comment": null, "summary": "Unsupervised domain adaptation for semantic segmentation (DASS) aims to\ntransfer knowledge from a label-rich source domain to a target domain with no\nlabels. Two key approaches in DASS are (1) vision-only approaches using masking\nor multi-resolution crops, and (2) language-based approaches that use generic\nclass-wise prompts informed by target domain (e.g. \"a {snowy} photo of a\n{class}\"). However, the former is susceptible to noisy pseudo-labels that are\nbiased to the source domain. The latter does not fully capture the intricate\nspatial relationships of objects -- key for dense prediction tasks. To this\nend, we propose LangDA. LangDA addresses these challenges by, first, learning\ncontextual relationships between objects via VLM-generated scene descriptions\n(e.g. \"a pedestrian is on the sidewalk, and the street is lined with\nbuildings.\"). Second, LangDA aligns the entire image features with text\nrepresentation of this context-aware scene caption and learns generalized\nrepresentations via text. With this, LangDA sets the new state-of-the-art\nacross three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and\n3.9%."}
{"id": "2503.12781", "pdf": "https://arxiv.org/pdf/2503.12781", "abs": "https://arxiv.org/abs/2503.12781", "authors": ["Zhang Jiaxing", "Tang Hao"], "title": "SAM2 for Image and Video Segmentation: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 7 Tables", "summary": "Despite significant advances in deep learning for image and video\nsegmentation, existing models continue to face challenges in cross-domain\nadaptability and generalization. Image and video segmentation are fundamental\ntasks in computer vision with wide-ranging applications in healthcare,\nagriculture, industrial inspection, and autonomous driving. With the advent of\nlarge-scale foundation models, SAM2 - an improved version of SAM (Segment\nAnything Model)has been optimized for segmentation tasks, demonstrating\nenhanced performance in complex scenarios. However, SAM2's adaptability and\nlimitations in specific domains require further investigation. This paper\nsystematically analyzes the application of SAM2 in image and video segmentation\nand evaluates its performance in various fields. We begin by introducing the\nfoundational concepts of image segmentation, categorizing foundation models,\nand exploring the technical characteristics of SAM and SAM2. Subsequently, we\ndelve into SAM2's applications in static image and video segmentation,\nemphasizing its performance in specialized areas such as medical imaging and\nthe challenges of cross-domain adaptability. As part of our research, we\nreviewed over 200 related papers to provide a comprehensive analysis of the\ntopic. Finally, the paper highlights the strengths and weaknesses of SAM2 in\nsegmentation tasks, identifies the technical challenges it faces, and proposes\nfuture development directions. This review provides valuable insights and\npractical recommendations for optimizing and applying SAM2 in real-world\nscenarios."}
{"id": "2503.12783", "pdf": "https://arxiv.org/pdf/2503.12783", "abs": "https://arxiv.org/abs/2503.12783", "authors": ["Jianan Li", "Huan Chen", "Wangcai Zhao", "Rui Chen", "Tingfa Xu"], "title": "Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by TNNLS", "summary": "Hyperspectral Images (HSIs) are crucial across numerous fields but are\nhindered by the long acquisition times associated with traditional\nspectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system\nmitigates this issue through a compression technique that accelerates the\nacquisition process. However, reconstructing HSIs from compressed data presents\nchallenges due to fixed spatial and spectral resolution constraints. This study\nintroduces a novel method using implicit neural representation for continuous\nhyperspectral image reconstruction. We propose the Mixed Granularity Implicit\nRepresentation (MGIR) framework, which includes a Hierarchical Spectral-Spatial\nImplicit Encoder for efficient multi-scale implicit feature extraction. This is\ncomplemented by a Mixed-Granularity Local Feature Aggregator that adaptively\nintegrates local features across scales, combined with a decoder that merges\ncoordinate information for precise reconstruction. By leveraging implicit\nneural representations, the MGIR framework enables reconstruction at any\ndesired spatial-spectral resolution, significantly enhancing the flexibility\nand adaptability of the CASSI system. Extensive experimental evaluations\nconfirm that our model produces reconstructed images at arbitrary resolutions\nand matches state-of-the-art methods across varying spectral-spatial\ncompression ratios. The code will be released at https://github.com/chh11/MGIR."}
{"id": "2503.12786", "pdf": "https://arxiv.org/pdf/2503.12786", "abs": "https://arxiv.org/abs/2503.12786", "authors": ["Peirong Zhang", "Yuliang Liu", "Songxuan Lai", "Hongliang Li", "Lianwen Jin"], "title": "Privacy-Preserving Biometric Verification with Handwritten Random Digit String", "categories": ["cs.CV"], "comment": null, "summary": "Handwriting verification has stood as a steadfast identity authentication\nmethod for decades. However, this technique risks potential privacy breaches\ndue to the inclusion of personal information in handwritten biometrics such as\nsignatures. To address this concern, we propose using the Random Digit String\n(RDS) for privacy-preserving handwriting verification. This approach allows\nusers to authenticate themselves by writing an arbitrary digit sequence,\neffectively ensuring privacy protection. To evaluate the effectiveness of RDS,\nwe construct a new HRDS4BV dataset composed of online naturally handwritten\nRDS. Unlike conventional handwriting, RDS encompasses unconstrained and\nvariable content, posing significant challenges for modeling consistent\npersonal writing style. To surmount this, we propose the Pattern Attentive\nVErification Network (PAVENet), along with a Discriminative Pattern Mining\n(DPM) module. DPM adaptively enhances the recognition of consistent and\ndiscriminative writing patterns, thus refining handwriting style\nrepresentation. Through comprehensive evaluations, we scrutinize the\napplicability of online RDS verification and showcase a pronounced\noutperformance of our model over existing methods. Furthermore, we discover a\nnoteworthy forgery phenomenon that deviates from prior findings and discuss its\npositive impact in countering malicious impostor attacks. Substantially, our\nwork underscores the feasibility of privacy-preserving biometric verification\nand propels the prospects of its broader acceptance and application."}
{"id": "2503.12797", "pdf": "https://arxiv.org/pdf/2503.12797", "abs": "https://arxiv.org/abs/2503.12797", "authors": ["Xinyu Ma", "Ziyang Ding", "Zhicong Luo", "Chi Chen", "Zonghao Guo", "Derek F. Wong", "Xiaoyi Feng", "Maosong Sun"], "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception."}
{"id": "2503.12799", "pdf": "https://arxiv.org/pdf/2503.12799", "abs": "https://arxiv.org/abs/2503.12799", "authors": ["Qiong Wu", "Xiangcong Yang", "Yiyi Zhou", "Chenxin Fang", "Baiyang Song", "Xiaoshuai Sun", "Rongrong Ji"], "title": "Grounded Chain-of-Thought for Multimodal Large Language Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC."}
{"id": "2503.12800", "pdf": "https://arxiv.org/pdf/2503.12800", "abs": "https://arxiv.org/abs/2503.12800", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Chunping Qiu", "Luoxi Jing", "Mengzhu Wang"], "title": "Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "With fully leveraging the value of unlabeled data, semi-supervised medical\nimage segmentation algorithms significantly reduces the limitation of limited\nlabeled data, achieving a significant improvement in accuracy. However, the\ndistributional shift between labeled and unlabeled data weakens the utilization\nof information from the labeled data. To alleviate the problem, we propose a\ngraph network feature alignment method based on pairwise similarity\nregularization (PaSR) for semi-supervised medical image segmentation. PaSR\naligns the graph structure of images in different domains by maintaining\nconsistency in the pairwise structural similarity of feature graphs between the\ntarget domain and the source domain, reducing distribution shift issues in\nmedical images. Meanwhile, further improving the accuracy of pseudo-labels in\nthe teacher network by aligning graph clustering information to enhance the\nsemi-supervised efficiency of the model. The experimental part was verified on\nthree medical image segmentation benchmark datasets, with results showing\nimprovements over advanced methods in various metrics. On the ACDC dataset, it\nachieved an average improvement of more than 10.66%."}
{"id": "2503.12820", "pdf": "https://arxiv.org/pdf/2503.12820", "abs": "https://arxiv.org/abs/2503.12820", "authors": ["Kailin Li", "Zhenxin Li", "Shiyi Lan", "Yuan Xie", "Zhizhong Zhang", "Jiayi Liu", "Zuxuan Wu", "Zhiding Yu", "Jose M. Alvarez"], "title": "Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Hydra-MDP++ introduces a novel teacher-student knowledge distillation\nframework with a multi-head decoder that learns from human demonstrations and\nrule-based experts. Using a lightweight ResNet-34 network without complex\ncomponents, the framework incorporates expanded evaluation metrics, including\ntraffic light compliance (TL), lane-keeping ability (LK), and extended comfort\n(EC) to address unsafe behaviors not captured by traditional NAVSIM-derived\nteachers. Like other end-to-end autonomous driving approaches, \\hydra processes\nraw images directly without relying on privileged perception signals.\nHydra-MDP++ achieves state-of-the-art performance by integrating these\ncomponents with a 91.0% drive score on NAVSIM through scaling to a V2-99 image\nencoder, demonstrating its effectiveness in handling diverse driving scenarios\nwhile maintaining computational efficiency."}
{"id": "2503.12821", "pdf": "https://arxiv.org/pdf/2503.12821", "abs": "https://arxiv.org/abs/2503.12821", "authors": ["Mingyang Song", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an $\\textbf{A}$daptive\n$\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which\nconsists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$)\nand $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume."}
{"id": "2503.12827", "pdf": "https://arxiv.org/pdf/2503.12827", "abs": "https://arxiv.org/abs/2503.12827", "authors": ["Md Farhamdur Reza", "Richeng Jin", "Tianfu Wu", "Huaiyu Dai"], "title": "GSBAK$^K$: $top$-$K$ Geometric Score-based Black-box Attack", "categories": ["cs.CV"], "comment": "This article has been accepted for publication at ICLR 2025", "summary": "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBAK$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBAK$^K$ can be used to attack against classifiers with\n$top$-$K$ multi-label learning. Extensive experimental results on ImageNet and\nPASCAL VOC datasets validate the effectiveness of GSBAK$^K$ in crafting\n$top$-$K$ adversarial examples."}
{"id": "2503.12834", "pdf": "https://arxiv.org/pdf/2503.12834", "abs": "https://arxiv.org/abs/2503.12834", "authors": ["Seunggwan Lee", "Hwanhee Jung", "Byoungsoo Koh", "Qixing Huang", "Sangho Yoon", "Sangpil Kim"], "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 18 figures", "summary": "A fundamental challenge in conditional 3D shape generation is to minimize the\ninformation loss and maximize the intention of user input. Existing approaches\nhave predominantly focused on two types of isolated conditional signals, i.e.,\nuser sketches and text descriptions, each of which does not offer flexible\ncontrol of the generated shape. In this paper, we introduce PASTA, the flexible\napproach that seamlessly integrates a user sketch and a text description for 3D\nshape generation. The key idea is to use text embeddings from a vision-language\nmodel to enrich the semantic representation of sketches. Specifically, these\ntext-derived priors specify the part components of the object, compensating for\nmissing visual cues from ambiguous sketches. In addition, we introduce ISG-Net\nwhich employs two types of graph convolutional networks: IndivGCN, which\nprocesses fine-grained details, and PartGCN, which aggregates these details\ninto parts and refines the structure of objects. Extensive experiments\ndemonstrate that PASTA outperforms existing methods in part-level editing and\nachieves state-of-the-art results in sketch-to-3D shape generation."}
{"id": "2503.12836", "pdf": "https://arxiv.org/pdf/2503.12836", "abs": "https://arxiv.org/abs/2503.12836", "authors": ["Sumin In", "Youngdong Jang", "Utae Jeong", "MinHyuk Jang", "Hyeongcheol Park", "Eunbyung Park", "Sangpil Kim"], "title": "CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 17 figures", "summary": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model."}
{"id": "2503.12838", "pdf": "https://arxiv.org/pdf/2503.12838", "abs": "https://arxiv.org/abs/2503.12838", "authors": ["Junjia Huang", "Pengxiang Yan", "Jinhang Cai", "Jiyang Liu", "Zhao Wang", "Yitong Wang", "Xinglong Wu", "Guanbin Li"], "title": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode", "categories": ["cs.CV"], "comment": "Under submission", "summary": "Text-driven image generation using diffusion models has recently gained\nsignificant attention. To enable more flexible image manipulation and editing,\nrecent research has expanded from single image generation to transparent layer\ngeneration and multi-layer compositions. However, existing approaches often\nfail to provide a thorough exploration of multi-layer structures, leading to\ninconsistent inter-layer interactions, such as occlusion relationships, spatial\nlayout, and shadowing. In this paper, we introduce DreamLayer, a novel\nframework that enables coherent text-driven generation of multiple image\nlayers, by explicitly modeling the relationship between transparent foreground\nand background layers. DreamLayer incorporates three key components, i.e.,\nContext-Aware Cross-Attention (CACA) for global-local information exchange,\nLayer-Shared Self-Attention (LSSA) for establishing robust inter-layer\nconnections, and Information Retained Harmonization (IRH) for refining fusion\ndetails at the latent level. By leveraging a coherent full-image context,\nDreamLayer builds inter-layer connections through attention mechanisms and\napplies a harmonization step to achieve seamless layer fusion. To facilitate\nresearch in multi-layer generation, we construct a high-quality, diverse\nmulti-layer dataset including 400k samples. Extensive experiments and user\nstudies demonstrate that DreamLayer generates more coherent and well-aligned\nlayers, with broad applicability, including latent-space image editing and\nimage-to-layer decomposition."}
{"id": "2503.12843", "pdf": "https://arxiv.org/pdf/2503.12843", "abs": "https://arxiv.org/abs/2503.12843", "authors": ["Haozhe Si", "Yuxuan Wan", "Minh Do", "Deepak Vasisht", "Han Zhao", "Hendrik F. Hamann"], "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Geospatial raster (imagery) data, such as that collected by satellite-based\nimaging systems at different times and spectral bands, hold immense potential\nfor enabling a wide range of high-impact applications. This potential stems\nfrom the rich information that is spatially and temporally contextualized\nacross multiple channels and sensing modalities. Recent work has adapted\nexisting self-supervised learning approaches for such geospatial data. However,\nthey fall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer (LESS ViT) with three key innovations: i)\nthe LESS Attention Block that approximates high-dimensional spatial-spectral\nattention through Kronecker's product of the low-dimensional spatial and\nspectral attention components; ii) the Continuous Positional-Channel Embedding\nLayer that preserves both spatial and spectral continuity and physical\ncharacteristics of each patch; and iii) the Perception Field Mask that exploits\nlocal spatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct a benchmark, GFM-Bench, which\nserves as a comprehensive benchmark for such geospatial raster data. We\npretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with\nintegrated positional and channel masking strategies. Experimental results\ndemonstrate that our proposed method surpasses current state-of-the-art\nmulti-modal geospatial foundation models, achieving superior performance with\nless computation and fewer parameters. The flexibility and extensibility of our\nframework make it a promising direction for future geospatial data analysis\ntasks that involve a wide range of modalities and channels."}
{"id": "2503.12844", "pdf": "https://arxiv.org/pdf/2503.12844", "abs": "https://arxiv.org/abs/2503.12844", "authors": ["Junhyeok Kim", "Jaewoo Park", "Junhee Park", "Sangeyl Lee", "Jiwan Chung", "Jisung Kim", "Ji Hoon Joung", "Youngjae Yu"], "title": "GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and Low-Vision Accessibility-Aware Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Mobility remains a significant challenge for the 2.2 billion people worldwide\naffected by blindness and low vision (BLV), with 7% of visually impaired\nindividuals experiencing falls at least once a month. While recent advances in\nMultimodal Large Language Models (MLLMs) offer promising opportunities for BLV\nassistance, their development has been hindered by limited datasets. This\nlimitation stems from the fact that BLV-aware annotation requires specialized\ndomain knowledge and intensive labor. To address this gap, we introduce\nGuideDog, a novel accessibility-aware guide dataset containing 22K\nimage-description pairs (including 2K human-annotated pairs) that capture\ndiverse real-world scenes from a pedestrian's viewpoint. Our approach shifts\nthe annotation burden from generation to verification through a collaborative\nhuman-AI framework grounded in established accessibility standards,\nsignificantly improving efficiency while maintaining high-quality annotations.\nWe also develop GuideDogQA, a subset of 818 samples featuring multiple-choice\nquestions designed to evaluate fine-grained visual perception capabilities,\nspecifically object recognition and relative depth perception. Our experimental\nresults highlight the importance of accurate spatial understanding for\neffective BLV guidance. GuideDog and GuideDogQA will advance research in\nMLLM-based assistive technologies for BLV individuals while contributing to\nbroader applications in understanding egocentric scenes for robotics and\naugmented reality. The code and dataset will be publicly available."}
{"id": "2503.12852", "pdf": "https://arxiv.org/pdf/2503.12852", "abs": "https://arxiv.org/abs/2503.12852", "authors": ["Aditi Tiwari", "Klara Nahrstedt"], "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing", "categories": ["cs.CV", "cs.MM"], "comment": "9 pages, 8 figures", "summary": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis."}
{"id": "2503.12853", "pdf": "https://arxiv.org/pdf/2503.12853", "abs": "https://arxiv.org/abs/2503.12853", "authors": ["Yanlin Xiang", "Qingyuan He", "Ting Xu", "Ran Hao", "Jiacheng Hu", "Hanchao Zhang"], "title": "Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis."}
{"id": "2503.12855", "pdf": "https://arxiv.org/pdf/2503.12855", "abs": "https://arxiv.org/abs/2503.12855", "authors": ["Yujie Lu", "Yale Song", "William Wang", "Lorenzo Torresani", "Tushar Nagarajan"], "title": "VITED: Video Temporal Evidence Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities."}
{"id": "2503.12862", "pdf": "https://arxiv.org/pdf/2503.12862", "abs": "https://arxiv.org/abs/2503.12862", "authors": ["Yu-Ting Zhan", "He-bi Yang", "Cheng-Yuan Ho", "Jui-Chiu Chiang", "Wen-Hsiao Peng"], "title": "CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has shown immense potential for novel view\nsynthesis. However, achieving rate-distortion-optimized compression of 3DGS\nrepresentations for transmission and/or storage applications remains a\nchallenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for\nend-to-end optimized compression, delivering state-of-the-art coding\nperformance. Despite this, it requires prolonged training and decoding time. To\naddress these limitations, we propose CAT-3DGS Pro, an enhanced version of\nCAT-3DGS that improves both compression performance and computational\nefficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which\nreplaces the triplane-based hyperprior to reduce redundant parameters. To\nachieve a more balanced rate-distortion trade-off and faster encoding, we\npropose an alternate optimization strategy (A-RDO). Additionally, we refine the\nsampling rate optimization method in CAT-3DGS, leading to significant\nimprovements in rate-distortion performance. These enhancements result in a\n46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while\nachieving 5x acceleration in decoding speed for the Amsterdam scene compared to\nCAT-3DGS."}
{"id": "2503.12866", "pdf": "https://arxiv.org/pdf/2503.12866", "abs": "https://arxiv.org/abs/2503.12866", "authors": ["Chenyu Zhang", "Kunlun Xu", "Zichen Liu", "Yuxin Peng", "Jiahuan Zhou"], "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Vision-language models (VLMs) encounter considerable challenges when adapting\nto domain shifts stemming from changes in data distribution. Test-time\nadaptation (TTA) has emerged as a promising approach to enhance VLM performance\nunder such conditions. In practice, test data often arrives in batches, leading\nto increasing interest in the transductive TTA setting. However, existing TTA\nmethods primarily focus on individual test samples, overlooking crucial\ncross-sample correlations within a batch. While recent ViT-based TTA methods\nhave introduced batch-level adaptation, they remain suboptimal for VLMs due to\ninadequate integration of the text modality. To address these limitations, we\npropose a novel transductive TTA framework, Supportive Clique-based Attribute\nPrompting (SCAP), which effectively combines visual and textual information to\nenhance adaptation by generating fine-grained attribute prompts across test\nbatches. SCAP first forms supportive cliques of test samples in an unsupervised\nmanner based on visual similarity and learns an attribute prompt for each\nclique, capturing shared attributes critical for adaptation. For each test\nsample, SCAP aggregates attribute prompts from its associated cliques,\nproviding enriched contextual information. To ensure adaptability over time, we\nincorporate a retention module that dynamically updates attribute prompts and\ntheir associated attributes as new data arrives. Comprehensive experiments\nacross multiple benchmarks demonstrate that SCAP outperforms existing\nstate-of-the-art methods, significantly advancing VLM generalization under\ndomain shifts. Our code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-SCAP."}
{"id": "2503.12868", "pdf": "https://arxiv.org/pdf/2503.12868", "abs": "https://arxiv.org/abs/2503.12868", "authors": ["Zi Li", "Jianpeng Zhang", "Tai Ma", "Tony C. W. Mok", "Yan-Jie Zhou", "Zeli Chen", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "UniReg: Foundation Model for Controllable Medical Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based medical image registration has achieved performance parity\nwith conventional methods while demonstrating a substantial advantage in\ncomputational efficiency. However, learning-based registration approaches lack\ngeneralizability across diverse clinical scenarios, requiring the laborious\ndevelopment of multiple isolated networks for specific registration tasks,\ne.g., inter-/intra-subject registration or organ-specific alignment. % To\novercome this limitation, we propose \\textbf{UniReg}, the first interactive\nfoundation model for medical image registration, which combines the precision\nadvantages of task-specific learning methods with the generalization of\ntraditional optimization methods. Our key innovation is a unified framework for\ndiverse registration scenarios, achieved through a conditional deformation\nfield estimation within a unified registration model. This is realized through\na dynamic learning paradigm that explicitly encodes: (1) anatomical structure\npriors, (2) registration type constraints (inter/intra-subject), and (3)\ninstance-specific features, enabling the generation of scenario-optimal\ndeformation fields. % Through comprehensive experiments encompassing $90$\nanatomical structures at different body regions, our UniReg model demonstrates\ncomparable performance with contemporary state-of-the-art methodologies while\nachieving ~50\\% reduction in required training iterations relative to the\nconventional learning-based paradigm. This optimization contributes to a\nsignificant reduction in computational resources, such as training time. Code\nand model will be available."}
{"id": "2503.12874", "pdf": "https://arxiv.org/pdf/2503.12874", "abs": "https://arxiv.org/abs/2503.12874", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Ke Ma", "Xinfeng Li", "Yihao Huang", "Wei Dong", "Yang Liu", "Xiaochun Cao"], "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT."}
{"id": "2503.12875", "pdf": "https://arxiv.org/pdf/2503.12875", "abs": "https://arxiv.org/abs/2503.12875", "authors": ["Evelyn J. Mannix", "Bartholomew A. Woodham"], "title": "An interpretable approach to automating the assessment of biofouling in video footage", "categories": ["cs.CV"], "comment": null, "summary": "Biofouling$\\unicode{x2013}$communities of organisms that grow on hard\nsurfaces immersed in water$\\unicode{x2013}$provides a pathway for the spread of\ninvasive marine species and diseases. To address this risk, international\nvessels are increasingly being obligated to provide evidence of their\nbiofouling management practices. Verification that these activities are\neffective requires underwater inspections, using divers or underwater remotely\noperated vehicles (ROVs), and the collection and analysis of large amounts of\nimagery and footage. Automated assessment using computer vision techniques can\nsignificantly streamline this process, and this work shows how this challenge\ncan be addressed efficiently and effectively using the interpretable Component\nFeatures (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation\nmodel. ComFe is able to obtain improved performance in comparison to previous\nnon-interpretable Convolutional Neural Network (CNN) methods, with\nsignificantly fewer weights and greater transparency$\\unicode{x2013}$through\nidentifying which regions of the image contribute to the classification, and\nwhich images in the training data lead to that conclusion. All code, data and\nmodel weights are publicly released."}
{"id": "2503.12885", "pdf": "https://arxiv.org/pdf/2503.12885", "abs": "https://arxiv.org/abs/2503.12885", "authors": ["Dewei Zhou", "Mingwei Li", "Zongxin Yang", "Yi Yang"], "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/."}
{"id": "2503.12886", "pdf": "https://arxiv.org/pdf/2503.12886", "abs": "https://arxiv.org/abs/2503.12886", "authors": ["Linzhou Li", "Yumeng Li", "Yanlin Weng", "Youyi Zheng", "Kun Zhou"], "title": "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for\nreconstructing photorealistic, animatable head avatars at speeds sufficient for\non-the-fly reconstruction. Unlike prior approaches that utilize linear bases\nfrom 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps\ntracked 3DMM parameters into reduced blendshape weights with an MLP, leading to\na compact set of blendshape bases. The learned compact base composition\neffectively captures essential facial details for specific individuals, and\ndoes not rely on the fixed base composition weights of 3DMM, leading to\nenhanced reconstruction quality and higher efficiency. To further expedite the\nreconstruction process, we develop a novel color initialization estimation\nmethod and a batch-parallel Gaussian rasterization process, achieving\nstate-of-the-art quality with training throughput of about 630 images per\nsecond. Moreover, we propose a local-global sampling strategy that enables\ndirect on-the-fly reconstruction, immediately reconstructing the model as video\nstreams in real time while achieving quality comparable to offline settings.\nOur source code is available at https://github.com/gapszju/RGBAvatar."}
{"id": "2503.12888", "pdf": "https://arxiv.org/pdf/2503.12888", "abs": "https://arxiv.org/abs/2503.12888", "authors": ["Siyuan Yao", "Yang Guo", "Yanyang Yan", "Wenqi Ren", "Xiaochun Cao"], "title": "UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network", "categories": ["cs.CV"], "comment": "14 pages,11 figures,references added", "summary": "Transformer-based trackers have achieved promising success and become the\ndominant tracking paradigm due to their accuracy and efficiency. Despite the\nsubstantial progress, most of the existing approaches tackle object tracking as\na deterministic coordinate regression problem, while the target localization\nuncertainty has been greatly overlooked, which hampers trackers' ability to\nmaintain reliable target state prediction in challenging scenarios. To address\nthis issue, we propose UncTrack, a novel uncertainty-aware transformer tracker\nthat predicts the target localization uncertainty and incorporates this\nuncertainty information for accurate target state inference. Specifically,\nUncTrack utilizes a transformer encoder to perform feature interaction between\ntemplate and search images. The output features are passed into an\nuncertainty-aware localization decoder (ULD) to coarsely predict the\ncorner-based localization and the corresponding localization uncertainty. Then\nthe localization uncertainty is sent into a prototype memory network (PMN) to\nexcavate valuable historical information to identify whether the target state\nprediction is reliable or not. To enhance the template representation, the\nsamples with high confidence are fed back into the prototype memory bank for\nmemory updating, making the tracker more robust to challenging appearance\nvariations. Extensive experiments demonstrate that our method outperforms other\nstate-of-the-art methods. Our code is available at\nhttps://github.com/ManOfStory/UncTrack."}
{"id": "2503.12905", "pdf": "https://arxiv.org/pdf/2503.12905", "abs": "https://arxiv.org/abs/2503.12905", "authors": ["Yuanbin Qian", "Shuhan Ye", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian", "Jiafei Wu"], "title": "UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by AAAI 2025", "summary": "Video anomaly detection plays a significant role in intelligent surveillance\nsystems. To enhance model's anomaly recognition ability, previous works have\ntypically involved RGB, optical flow, and text features. Recently, dynamic\nvision sensors (DVS) have emerged as a promising technology, which capture\nvisual information as discrete events with a very high dynamic range and\ntemporal resolution. It reduces data redundancy and enhances the capture\ncapacity of moving objects compared to conventional camera. To introduce this\nrich dynamic information into the surveillance field, we created the first DVS\nvideo anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this\nnew data modality, a multi-scale spiking fusion network (MSF) is designed based\non spiking neural networks (SNNs). This work explores the potential application\nof dynamic information from event data in video anomaly detection. Our\nexperiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and\nits superior performance compared to other models, establishing a new baseline\nfor SNN-based weakly supervised video anomaly detection."}
{"id": "2503.12910", "pdf": "https://arxiv.org/pdf/2503.12910", "abs": "https://arxiv.org/abs/2503.12910", "authors": ["Jingyi Yuan", "Pengyu Jie", "Junyin Zhang", "Ziao Li", "Chenqiang Gao"], "title": "MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal\nparadigm for identifying defects in unseen categories without requiring target\nsamples in training phase. However, existing ZSAD methods struggle with the\nboundary of small and complex defects due to insufficient representations. Most\nof them use the single manually designed prompts, failing to work for diverse\nobjects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based\nCLIP framework which explores the efficacy of multi-form prompts for zero-shot\nindustrial anomaly detection. We employ an image to text prompting(I2TP)\nmechanism to better represent the object in the image. MFP-CLIP enhances\nperception to multi-scale and complex anomalies by self prompting(SP) and a\nmulti-patch feature aggregation(MPFA) module. To precisely localize defects, we\nintroduce the mask prompting(MP) module to guide model to focus on potential\nanomaly regions. Extensive experiments are conducted on two wildly used\nindustrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating\nMFP-CLIP's superiority in ZSAD."}
{"id": "2503.12912", "pdf": "https://arxiv.org/pdf/2503.12912", "abs": "https://arxiv.org/abs/2503.12912", "authors": ["Bin Tang", "Keqi Pan", "Miao Zheng", "Ning Zhou", "Jialu Sui", "Dandan Zhu", "Cheng-Long Deng", "Shu-Guang Kuai"], "title": "Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 6 figures, AAAI 2025 Oral", "summary": "In recent years, predicting Big Five personality traits from multimodal data\nhas received significant attention in artificial intelligence (AI). However,\nexisting computational models often fail to achieve satisfactory performance.\nPsychological research has shown a strong correlation between pose and\npersonality traits, yet previous research has largely ignored pose data in\ncomputational models. To address this gap, we develop a novel multimodal\ndataset that incorporates full-body pose data. The dataset includes video\nrecordings of 287 participants completing a virtual interview with 36\nquestions, along with self-reported Big Five personality scores as labels. To\neffectively utilize this multimodal data, we introduce the Psychology-Inspired\nNetwork (PINet), which consists of three key modules: Multimodal Feature\nAwareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed\nModality Correlation Loss (PIMC Loss). The MFA module leverages the Vision\nMamba Block to capture comprehensive visual features related to personality,\nwhile the MFI module efficiently fuses the multimodal features. The PIMC Loss,\ngrounded in psychological theory, guides the model to emphasize different\nmodalities for different personality dimensions. Experimental results show that\nthe PINet outperforms several state-of-the-art baseline models. Furthermore,\nthe three modules of PINet contribute almost equally to the model's overall\nperformance. Incorporating pose data significantly enhances the model's\nperformance, with the pose modality ranking mid-level in importance among the\nfive modalities. These findings address the existing gap in personality-related\ndatasets that lack full-body pose data and provide a new approach for improving\nthe accuracy of personality prediction models, highlighting the importance of\nintegrating psychological insights into AI frameworks."}
{"id": "2503.12914", "pdf": "https://arxiv.org/pdf/2503.12914", "abs": "https://arxiv.org/abs/2503.12914", "authors": ["Zhuoqun Su", "Huimin Lu", "Shuaifeng Jiao", "Junhao Xiao", "Yaonan Wang", "Xieyuanli Chen"], "title": "Efficient Multimodal 3D Object Detector via Instance-Level Contrastive Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal 3D object detectors leverage the strengths of both geometry-aware\nLiDAR point clouds and semantically rich RGB images to enhance detection\nperformance. However, the inherent heterogeneity between these modalities,\nincluding unbalanced convergence and modal misalignment, poses significant\nchallenges. Meanwhile, the large size of the detection-oriented feature also\nconstrains existing fusion strategies to capture long-range dependencies for\nthe 3D detection tasks. In this work, we introduce a fast yet effective\nmultimodal 3D object detector, incorporating our proposed Instance-level\nContrastive Distillation (ICD) framework and Cross Linear Attention Fusion\nModule (CLFM). ICD aligns instance-level image features with LiDAR\nrepresentations through object-aware contrastive distillation, ensuring\nfine-grained cross-modal consistency. Meanwhile, CLFM presents an efficient and\nscalable fusion strategy that enhances cross-modal global interactions within\nsizable multimodal BEV features. Extensive experiments on the KITTI and\nnuScenes 3D object detection benchmarks demonstrate the effectiveness of our\nmethods. Notably, our 3D object detector outperforms state-of-the-art (SOTA)\nmethods while achieving superior efficiency. The implementation of our method\nhas been released as open-source at: https://github.com/nubot-nudt/ICD-Fusion."}
{"id": "2503.12927", "pdf": "https://arxiv.org/pdf/2503.12927", "abs": "https://arxiv.org/abs/2503.12927", "authors": ["Huangwei Chen", "Zhu Zhu", "Zhenyu Yan", "Yifei Chen", "Mingyang Ding", "Chenlei Li", "Feiwei Qin"], "title": "MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification Assisted with Textual Description Generation", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB."}
{"id": "2503.12929", "pdf": "https://arxiv.org/pdf/2503.12929", "abs": "https://arxiv.org/abs/2503.12929", "authors": ["Xuying Zhang", "Yupeng Zhou", "Kai Wang", "Yikai Wang", "Zhen Li", "Xiuli Shao", "Daquan Zhou", "Qibin Hou", "Ming-Ming Cheng"], "title": "AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.\nHowever, existing works still struggle to maintain consistency between the\ngenerated views and the input views, especially when there is a significant\ncamera pose difference, leading to poor-quality 3D geometries and textures. We\nattribute this issue to their treatment of all target views with equal priority\naccording to our empirical observation that the target views closer to the\ninput views exhibit higher fidelity. With this inspiration, we propose\nAR-1-to-3, a novel next-view prediction paradigm based on diffusion models that\nfirst generates views close to the input views, which are then utilized as\ncontextual information to progressively synthesize farther views. To encode the\ngenerated view subsequences as local and global conditions for the next-view\nprediction, we accordingly develop a stacked local feature encoding strategy\n(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).\nExtensive experiments demonstrate that our method significantly improves the\nconsistency between the generated views and the input views, producing\nhigh-fidelity 3D assets."}
{"id": "2503.12935", "pdf": "https://arxiv.org/pdf/2503.12935", "abs": "https://arxiv.org/abs/2503.12935", "authors": ["Guoliang Xu", "Jianqin Yin", "Ren Zhang", "Yonghao Dang", "Feng Zhou", "Bo Yu"], "title": "L2HCount:Generalizing Crowd Counting from Low to High Crowd Density via Density Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Since COVID-19, crowd-counting tasks have gained wide applications. While\nsupervised methods are reliable, annotation is more challenging in high-density\nscenes due to small head sizes and severe occlusion, whereas it's simpler in\nlow-density scenes. Interestingly, can we train the model in low-density scenes\nand generalize it to high-density scenes? Therefore, we propose a low- to\nhigh-density generalization framework (L2HCount) that learns the pattern\nrelated to high-density scenes from low-density ones, enabling it to generalize\nwell to high-density scenes. Specifically, we first introduce a High-Density\nSimulation Module and a Ground-Truth Generation Module to construct fake\nhigh-density images along with their corresponding ground-truth crowd\nannotations respectively by image-shifting technique, effectively simulating\nhigh-density crowd patterns. However, the simulated images have two issues:\nimage blurring and loss of low-density image characteristics. Therefore, we\nsecond propose a Head Feature Enhancement Module to extract clear features in\nthe simulated high-density scene. Third, we propose a Dual-Density Memory\nEncoding Module that uses two crowd memories to learn scene-specific patterns\nfrom low- and simulated high-density scenes, respectively. Extensive\nexperiments on four challenging datasets have shown the promising performance\nof L2HCount."}
{"id": "2503.12944", "pdf": "https://arxiv.org/pdf/2503.12944", "abs": "https://arxiv.org/abs/2503.12944", "authors": ["Jianzheng Huang", "Xianyu Mo", "Ziling Liu", "Jinyu Yang", "Feng Zheng"], "title": "GIFT: Generated Indoor video frames for Texture-less point tracking", "categories": ["cs.CV"], "comment": null, "summary": "Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking."}
{"id": "2503.12947", "pdf": "https://arxiv.org/pdf/2503.12947", "abs": "https://arxiv.org/abs/2503.12947", "authors": ["Ingyun Lee", "Jae Won Jang", "Seunghyeon Seo", "Nojun Kwak"], "title": "DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency for Few-shot View Synthesis", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view\nsynthesis but requires many multiview images, making it impractical for\nfew-shot scenarios. Ray augmentation was proposed to prevent overfitting for\nsparse training data by generating additional rays. However, existing methods,\nwhich generate augmented rays only near the original rays, produce severe\nfloaters and appearance distortion due to limited viewpoints and inconsistent\nrays obstructed by nearby obstacles and complex surfaces. To address these\nproblems, we propose DivCon-NeRF, which significantly enhances both diversity\nand consistency. It employs surface-sphere augmentation, which preserves the\ndistance between the original camera and the predicted surface point. This\nallows the model to compare the order of high-probability surface points and\nfilter out inconsistent rays easily without requiring the exact depth. By\nintroducing inner-sphere augmentation, DivCon-NeRF randomizes angles and\ndistances for diverse viewpoints, further increasing diversity. Consequently,\nour method significantly reduces floaters and visual distortions, achieving\nstate-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code\nwill be publicly available."}
{"id": "2503.12953", "pdf": "https://arxiv.org/pdf/2503.12953", "abs": "https://arxiv.org/abs/2503.12953", "authors": ["Zheyuan Liu", "Junyan Wang", "Zicheng Duan", "Cristian Rodriguez-Opazo", "Anton van den Hengel"], "title": "Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in Text-to-Video Prediction", "categories": ["cs.CV"], "comment": "20 pages, 15 figures", "summary": "Text-video prediction (TVP) is a downstream video generation task that\nrequires a model to produce subsequent video frames given a series of initial\nvideo frames and text describing the required motion. In practice TVP methods\nfocus on a particular category of videos depicting manipulations of objects\ncarried out by human beings or robot arms. Previous methods adapt models\npre-trained on text-to-image tasks, and thus tend to generate video that lacks\nthe required continuity. A natural progression would be to leverage more recent\npre-trained text-to-video (T2V) models. This approach is rendered more\nchallenging by the fact that the most common fine-tuning technique, low-rank\nadaptation (LoRA), yields undesirable results. In this work, we propose an\nadaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).\nWithin the module, we devise a sub-module that produces frame-wise text\nembeddings from the input text, which acts as an additional text condition to\naid generation. We use FCA to fine-tune the T2V model, which incorporates the\ninitial frame(s) as an extra condition. We compare and discuss the more\neffective strategy for injecting such embeddings into the T2V model. We conduct\nextensive ablation studies on our design choices with quantitative and\nqualitative performance analysis. Our approach establishes a new\nstate-of-the-art for the task of TVP. The project page is at\nhttps://github.com/Cuberick-Orion/FCA ."}
{"id": "2503.12955", "pdf": "https://arxiv.org/pdf/2503.12955", "abs": "https://arxiv.org/abs/2503.12955", "authors": ["Jiahe Zhao", "Ruibing Hou", "Zejie Tian", "Hong Chang", "Shiguang Shan"], "title": "HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new task to benchmark human-in-scene understanding for embodied\nagents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within\na 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,\nreason about its surrounding environment, and answer human-related questions\nwithin the scene. To support this new task, we present HIS-Bench, a multimodal\nbenchmark that systematically evaluates HIS understanding across a broad\nspectrum, from basic perception to commonsense reasoning and planning. Our\nevaluation of various vision-language models on HIS-Bench reveals significant\nlimitations in their ability to handle HIS-QA tasks. To this end, we propose\nHIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates\n3D scene context and human motion dynamics into large language models while\nincorporating specialized mechanisms to capture human-scene interactions.\nExtensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on\nHIS-QA tasks. We hope this work inspires future research on human behavior\nanalysis in 3D scenes, advancing embodied AI and world models."}
{"id": "2503.12963", "pdf": "https://arxiv.org/pdf/2503.12963", "abs": "https://arxiv.org/abs/2503.12963", "authors": ["Chaolong Yang", "Kai Yao", "Yuyao Yan", "Chenru Jiang", "Weiguang Zhao", "Jie Sun", "Guangliang Cheng", "Yifei Zhang", "Bin Dong", "Kaizhu Huang"], "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker."}
{"id": "2503.12964", "pdf": "https://arxiv.org/pdf/2503.12964", "abs": "https://arxiv.org/abs/2503.12964", "authors": ["Zeeshan Patel", "Ethan He", "Parth Mannan", "Xiaowei Ren", "Ryan Wolf", "Niket Agarwal", "Jacob Huffman", "Zhuoyao Wang", "Carl Wang", "Jack Chang", "Yan Bai", "Tommy Huang", "Linnan Wang", "Sahil Jain", "Shanmugam Ramasamy", "Joseph Jennings", "Ekaterina Sirazitdinova", "Oleg Sudakov", "Mingyuan Ma", "Bobby Chen", "Forrest Lin", "Hao Wang", "Vasanth Rao Naik Sabavat", "Sriharsha Niverty", "Rong Ou", "Pallab Bhattacharya", "David Page", "Nima Tajbakhsh", "Ashwath Aithal"], "title": "Training Video Foundation Models with NVIDIA NeMo", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference."}
{"id": "2503.12968", "pdf": "https://arxiv.org/pdf/2503.12968", "abs": "https://arxiv.org/abs/2503.12968", "authors": ["Guanhua Ding", "Yuxuan Xia", "Runwei Guan", "Qinchen Wu", "Tao Huang", "Weiping Ding", "Jinping Sun", "Guoqiang Mao"], "title": "OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as\nit enables robust perception, navigation, and planning in complex environments.\nWhile deep learning-based solutions have demonstrated impressive 3D MOT\nperformance, model-based approaches remain appealing for their simplicity,\ninterpretability, and data efficiency. Conventional model-based trackers\ntypically rely on random vector-based Bayesian filters within the\ntracking-by-detection (TBD) framework but face limitations due to heuristic\ndata association and track management schemes. In contrast, random finite set\n(RFS)-based Bayesian filtering handles object birth, survival, and death in a\ntheoretically sound manner, facilitating interpretability and parameter tuning.\nIn this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs\nan optimized Poisson multi-Bernoulli (PMB) filter while incorporating several\nkey innovative designs within the TBD framework. Specifically, we propose a\nmeasurement-driven hybrid adaptive birth model for improved track\ninitialization, employ adaptive detection probability parameters to effectively\nmaintain tracks for occluded objects, and optimize density pruning and track\nextraction modules to further enhance overall tracking performance. Extensive\nevaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior\ntracking accuracy compared with state-of-the-art methods, thereby establishing\na new benchmark for model-based 3D MOT and offering valuable insights for\nfuture research on RFS-based trackers in autonomous driving."}
{"id": "2503.12969", "pdf": "https://arxiv.org/pdf/2503.12969", "abs": "https://arxiv.org/abs/2503.12969", "authors": ["Kazuki Omi", "Jion Oshima", "Toru Tamaki"], "title": "Action tube generation by person query matching for spatio-temporal action detection", "categories": ["cs.CV"], "comment": "extended version of VISAPP2025", "summary": "This paper proposes a method for spatio-temporal action detection (STAD) that\ndirectly generates action tubes from the original video without relying on\npost-processing steps such as IoU-based linking and clip splitting. Our\napproach applies query-based detection (DETR) to each frame and matches DETR\nqueries to link the same person across frames. We introduce the Query Matching\nModule (QMM), which uses metric learning to bring queries for the same person\ncloser together across frames compared to queries for different people. Action\nclasses are predicted using the sequence of queries obtained from QMM matching,\nallowing for variable-length inputs from videos longer than a single clip.\nExperimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our\nmethod performs well for large position changes of people while offering\nsuperior computational efficiency and lower resource requirements."}
{"id": "2503.12972", "pdf": "https://arxiv.org/pdf/2503.12972", "abs": "https://arxiv.org/abs/2503.12972", "authors": ["Junming Liu", "Siyuan Meng", "Yanting Gao", "Song Mao", "Pinlong Cai", "Guohang Yan", "Yirong Chen", "Zilin Bian", "Botian Shi", "Ding Wang"], "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 7 figures, 6 tables", "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."}
{"id": "2503.12973", "pdf": "https://arxiv.org/pdf/2503.12973", "abs": "https://arxiv.org/abs/2503.12973", "authors": ["Colin Prieur", "Nassim Ait Ali Braham", "Paul Tresson", "Gr√©goire Vincent", "Jocelyn Chanussot"], "title": "Prospects for Mitigating Spectral Variability in Tropical Species Classification Using Self-Supervised Learning", "categories": ["cs.CV"], "comment": "5 pages, 3 figures, published as proceeding of the \"2024 14th\n  Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote\n  Sensing (WHISPERS)\"", "summary": "Airborne hyperspectral imaging is a promising method for identifying tropical\nspecies, but spectral variability between acquisitions hinders consistent\nresults. This paper proposes using Self-Supervised Learning (SSL) to encode\nspectral features that are robust to abiotic variability and relevant for\nspecies identification. By employing the state-of-the-art Barlow-Twins approach\non repeated spectral acquisitions, we demonstrate the ability to develop stable\nfeatures. For the classification of 40 tropical species, experiments show that\nthese features can outperform typical reflectance products in terms of\nrobustness to spectral variability by 10 points of accuracy across dates."}
{"id": "2503.12974", "pdf": "https://arxiv.org/pdf/2503.12974", "abs": "https://arxiv.org/abs/2503.12974", "authors": ["Xueying Jiang", "Wenhao Li", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released."}
{"id": "2503.12981", "pdf": "https://arxiv.org/pdf/2503.12981", "abs": "https://arxiv.org/abs/2503.12981", "authors": ["Thu Tran", "Kenny Tsu Wei Choo", "Shaohui Foong", "Hitesh Bhardwaj", "Shane Kyi Hla Win", "Wei Jun Ang", "Kenneth Goh", "Rajesh Krishna Balan"], "title": "Analyzing Swimming Performance Using Drone Captured Aerial Videos", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, published to ACM Dronet'24", "summary": "Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m/s for stroke duration and velocity, respectively."}
{"id": "2503.12982", "pdf": "https://arxiv.org/pdf/2503.12982", "abs": "https://arxiv.org/abs/2503.12982", "authors": ["Yunshuang Yuan", "Yan Xia", "Daniel Cremers", "Monika Sester"], "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception can increase the view field and decrease the occlusion\nof an ego vehicle, hence improving the perception performance and safety of\nautonomous driving. Despite the success of previous works on cooperative object\ndetection, they mostly operate on dense Bird's Eye View (BEV) feature maps,\nwhich are computationally demanding and can hardly be extended to long-range\ndetection problems. More efficient fully sparse frameworks are rarely explored.\nIn this work, we design a fully sparse framework, SparseAlign, with three key\nfeatures: an enhanced sparse 3D backbone, a query-based temporal context\nlearning module, and a robust detection head specially tailored for sparse\nfeatures. Extensive experimental results on both OPV2V and DairV2X datasets\nshow that our framework, despite its sparsity, outperforms the state of the art\nwith less communication bandwidth requirements. In addition, experiments on the\nOPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also\nshow a significant performance gain compared to the baseline works."}
{"id": "2503.12999", "pdf": "https://arxiv.org/pdf/2503.12999", "abs": "https://arxiv.org/abs/2503.12999", "authors": ["Ruichuan An", "Kai Zeng", "Ming Lu", "Sihan Yang", "Renrui Zhang", "Huitong Ji", "Qizhe Zhang", "Yulin Luo", "Hao Liang", "Wentao Zhang"], "title": "Concept-as-Tree: Synthetic Data is All You Need for VLM Personalization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated exceptional performance in\nvarious multi-modal tasks. Recently, there has been an increasing interest in\nimproving the personalization capabilities of VLMs. To better integrate\nuser-provided concepts into VLMs, many methods use positive and negative\nsamples to fine-tune these models. However, the scarcity of user-provided\npositive samples and the low quality of retrieved negative samples pose\nchallenges for fine-tuning. To reveal the relationship between sample and model\nperformance, we systematically investigate the impact of positive and negative\nsamples (easy and hard) and their diversity on VLM personalization tasks. Based\non the detailed analysis, we introduce Concept-as-Tree (CaT), which represents\na concept as a tree structure, thereby enabling the data generation of positive\nand negative samples with varying difficulty and diversity for VLM\npersonalization. With a well-designed data filtering strategy, our CaT\nframework can ensure the quality of generated data, constituting a powerful\npipeline. We perform thorough experiments with various VLM personalization\nbaselines to assess the effectiveness of the pipeline, alleviating the lack of\npositive samples and the low quality of negative samples. Our results\ndemonstrate that CaT equipped with the proposed data filter significantly\nenhances the personalization capabilities of VLMs across the MyVLM, Yo'LLaVA,\nand MC-LLaVA datasets. To our knowledge, this work is the first controllable\nsynthetic data pipeline for VLM personalization. The code is released at\n\\href{https://github.com/zengkaiya/CaT}{https://github.com/zengkaiya/CaT}."}
{"id": "2503.13004", "pdf": "https://arxiv.org/pdf/2503.13004", "abs": "https://arxiv.org/abs/2503.13004", "authors": ["Jiaxu Liu", "Li Li", "Hubert P. H. Shum", "Toby P. Breckon"], "title": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance."}
{"id": "2503.13012", "pdf": "https://arxiv.org/pdf/2503.13012", "abs": "https://arxiv.org/abs/2503.13012", "authors": ["Xingguo Lv", "Xingbo Dong", "Liwen Wang", "Jiewen Yang", "Lei Zhao", "Bin Pu", "Zhe Jin", "Xuejun Li"], "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM."}
{"id": "2503.13016", "pdf": "https://arxiv.org/pdf/2503.13016", "abs": "https://arxiv.org/abs/2503.13016", "authors": ["Zijia Zhao", "Yuqi Huo", "Tongtian Yue", "Longteng Guo", "Haoyu Lu", "Bingning Wang", "Weipeng Chen", "Jing Liu"], "title": "Efficient Motion-Aware Video MLLM", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks."}
{"id": "2503.13023", "pdf": "https://arxiv.org/pdf/2503.13023", "abs": "https://arxiv.org/abs/2503.13023", "authors": ["Michal Danilowicz", "Tomasz Kryjak"], "title": "Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA", "categories": ["cs.CV"], "comment": "Accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025", "summary": "Multi-object tracking (MOT) is one of the most important problems in computer\nvision and a key component of any vision-based perception system used in\nadvanced autonomous mobile robotics. Therefore, its implementation on low-power\nand real-time embedded platforms is highly desirable. Modern MOT algorithms\nshould be able to track objects of a given class (e.g. people or vehicles). In\naddition, the number of objects to be tracked is not known in advance, and they\nmay appear and disappear at any time, as well as be obscured. For these\nreasons, the most popular and successful approaches have recently been based on\nthe tracking paradigm. Therefore, the presence of a high quality object\ndetector is essential, which in practice accounts for the vast majority of the\ncomputational and memory complexity of the whole MOT system. In this paper, we\npropose an FPGA (Field-Programmable Gate Array) implementation of an embedded\nMOT system based on a quantized YOLOv8 detector and the SORT (Simple Online\nRealtime Tracker) tracker. We use a modified version of the FINN framework to\nutilize external memory for model parameters and to support operations\nnecessary required by YOLOv8. We discuss the evaluation of detection and\ntracking performance using the COCO and MOT15 datasets, where we achieve 0.21\nmAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC\nsystem (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed\nin reprogrammable logic and the tracking algorithm is implemented in the\nprocessor system."}
{"id": "2503.13025", "pdf": "https://arxiv.org/pdf/2503.13025", "abs": "https://arxiv.org/abs/2503.13025", "authors": ["ChangHee Yang", "Hyeonseop Song", "Seokhun Choi", "Seungwoo Lee", "Jaechul Kim", "Hoseok Do"], "title": "PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data", "categories": ["cs.CV", "cs.AI"], "comment": "The first three authors contributed equally to this work", "summary": "Despite considerable efforts to enhance the generalization of 3D pose\nestimators without costly 3D annotations, existing data augmentation methods\nstruggle in real world scenarios with diverse human appearances and complex\nposes. We propose PoseSyn, a novel data synthesis framework that transforms\nabundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn\ncomprises two key components: Error Extraction Module (EEM), which identifies\nchallenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),\nwhich synthesizes motion sequences around the challenging poses. Then, by\ngenerating realistic 3D training data via a human animation model aligned with\nchallenging poses and appearances PoseSyn boosts the accuracy of various 3D\npose estimators by up to 14% across real world benchmarks including various\nbackgrounds and occlusions, challenging poses, and multi view scenarios.\nExtensive experiments further confirm that PoseSyn is a scalable and effective\napproach for improving generalization without relying on expensive 3D\nannotations, regardless of the pose estimator's model size or design."}
{"id": "2503.13026", "pdf": "https://arxiv.org/pdf/2503.13026", "abs": "https://arxiv.org/abs/2503.13026", "authors": ["Tao Wang", "Changxu Cheng", "Lingfeng Wang", "Senda Chen", "Wuyue Zhao"], "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model", "categories": ["cs.CV"], "comment": "technical report", "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding."}
{"id": "2503.13028", "pdf": "https://arxiv.org/pdf/2503.13028", "abs": "https://arxiv.org/abs/2503.13028", "authors": ["Tony Danjun Wang", "Lennart Bastian", "Tobias Czempiel", "Christian Heiliger", "Nassir Navab"], "title": "Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room", "categories": ["cs.CV", "J.3"], "comment": "26 pages, 14 figures, Submitted to Medical Image Analysis", "summary": "Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination."}
{"id": "2503.13045", "pdf": "https://arxiv.org/pdf/2503.13045", "abs": "https://arxiv.org/abs/2503.13045", "authors": ["Gabriele Berton", "Kevin Musgrave", "Carlo Masone"], "title": "All You Need to Know About Training Image Retrieval Models", "categories": ["cs.CV"], "comment": null, "summary": "Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https://github.com/gmberton/image-retrieval"}
{"id": "2503.13047", "pdf": "https://arxiv.org/pdf/2503.13047", "abs": "https://arxiv.org/abs/2503.13047", "authors": ["Ruiqi Song", "Xianda Guo", "Hangbin Wu", "Qinggong Wei", "Long Chen"], "title": "InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Directly generating planning results from raw sensors has become increasingly\nprevalent due to its adaptability and robustness in complex scenarios. Scene\nrepresentation, as a key module in the pipeline, has traditionally relied on\nconventional perception, which focus on the global scene. However, in driving\nscenarios, human drivers typically focus only on regions that directly impact\ndriving, which often coincide with those required for end-to-end autonomous\ndriving. In this paper, a novel end-to-end autonomous driving method called\nInsightDrive is proposed, which organizes perception by language-guided scene\nrepresentation. We introduce an instance-centric scene tokenizer that\ntransforms the surrounding environment into map- and object-aware instance\ntokens. Scene attention language descriptions, which highlight key regions and\nobstacles affecting the ego vehicle's movement, are generated by a\nvision-language model that leverages the cognitive reasoning capabilities of\nfoundation models. We then align scene descriptions with visual features using\nthe vision-language model, guiding visual attention through these descriptions\nto give effectively scene representation. Furthermore, we employ self-attention\nand cross-attention mechanisms to model the ego-agents and ego-map\nrelationships to comprehensively build the topological relationships of the\nscene. Finally, based on scene understanding, we jointly perform motion\nprediction and planning. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate that the proposed InsightDrive achieves state-of-the-art\nperformance in end-to-end autonomous driving. The code is available at\nhttps://github.com/songruiqi/InsightDrive"}
{"id": "2503.13053", "pdf": "https://arxiv.org/pdf/2503.13053", "abs": "https://arxiv.org/abs/2503.13053", "authors": ["Nassim Ali Ousalah", "Anis Kacem", "Enjie Ghorbel", "Emmanuel Koumandakis", "Djamila Aouada"], "title": "Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios."}
{"id": "2503.13058", "pdf": "https://arxiv.org/pdf/2503.13058", "abs": "https://arxiv.org/abs/2503.13058", "authors": ["Zeyi Huang", "Utkarsh Ojha", "Yuyang Ji", "Donghyun Lee", "Yong Jae Lee"], "title": "Do Vision Models Develop Human-Like Progressive Difficulty Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "When a human undertakes a test, their responses likely follow a pattern: if\nthey answered an easy question $(2 \\times 3)$ incorrectly, they would likely\nanswer a more difficult one $(2 \\times 3 \\times 4)$ incorrectly; and if they\nanswered a difficult question correctly, they would likely answer the easy one\ncorrectly. Anything else hints at memorization. Do current visual recognition\nmodels exhibit a similarly structured learning capacity? In this work, we\nconsider the task of image classification and study if those models' responses\nfollow that pattern. Since real images aren't labeled with difficulty, we first\ncreate a dataset of 100 categories, 10 attributes, and 3 difficulty levels\nusing recent generative models: for each category (e.g., dog) and attribute\n(e.g., occlusion), we generate images of increasing difficulty (e.g., a dog\nwithout occlusion, a dog only partly visible). We find that most of the models\ndo in fact behave similarly to the aforementioned pattern around 80-90% of the\ntime. Using this property, we then explore a new way to evaluate those models.\nInstead of testing the model on every possible test image, we create an\nadaptive test akin to GRE, in which the model's performance on the current\nround of images determines the test images in the next round. This allows the\nmodel to skip over questions too easy/hard for itself, and helps us get its\noverall performance in fewer steps."}
{"id": "2503.13060", "pdf": "https://arxiv.org/pdf/2503.13060", "abs": "https://arxiv.org/abs/2503.13060", "authors": ["Harshal Kausadikar", "Tanvi Kale", "Onkar Susladkar", "Sparsh Mittal"], "title": "Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework for Transliteration of Modi Script to Devanagari", "categories": ["cs.CV"], "comment": "Under submission at a conference", "summary": "In medieval India, the Marathi language was written using the Modi script.\nThe texts written in Modi script include extensive knowledge about medieval\nsciences, medicines, land records and authentic evidence about Indian history.\nAround 40 million documents are in poor condition and have not yet been\ntransliterated. Furthermore, only a few experts in this domain can\ntransliterate this script into English or Devanagari. Most of the past research\npredominantly focuses on individual character recognition. A system that can\ntransliterate Modi script documents to Devanagari script is needed. We propose\nthe MoDeTrans dataset, comprising 2,043 images of Modi script documents\naccompanied by their corresponding textual transliterations in Devanagari. We\nfurther introduce MoScNet (\\textbf{Mo}di \\textbf{Sc}ript \\textbf{Net}work), a\nnovel Vision-Language Model (VLM) framework for transliterating Modi script\nimages into Devanagari text. MoScNet leverages Knowledge Distillation, where a\nstudent model learns from a teacher model to enhance transliteration\nperformance. The final student model of MoScNet has better performance than the\nteacher model while having 163$\\times$ lower parameters. Our work is the first\nto perform direct transliteration from the handwritten Modi script to the\nDevanagari script. MoScNet also shows competitive results on the optical\ncharacter recognition (OCR) task."}
{"id": "2503.13063", "pdf": "https://arxiv.org/pdf/2503.13063", "abs": "https://arxiv.org/abs/2503.13063", "authors": ["Zheng Wang", "Zihui Wang", "Zheng Wang", "Xiaoliang Fan", "Cheng Wang"], "title": "Federated Learning with Domain Shift Eraser", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Federated learning (FL) is emerging as a promising technique for\ncollaborative learning without local data leaving their devices. However,\nclients' data originating from diverse domains may degrade model performance\ndue to domain shifts, preventing the model from learning consistent\nrepresentation space. In this paper, we propose a novel FL framework, Federated\nDomain Shift Eraser (FDSE), to improve model performance by differently erasing\neach client's domain skew and enhancing their consensus. First, we formulate\nthe model forward passing as an iterative deskewing process that extracts and\nthen deskews features alternatively. This is efficiently achieved by\ndecomposing each original layer in the neural network into a Domain-agnostic\nFeature Extractor (DFE) and a Domain-specific Skew Eraser (DSE). Then, a\nregularization term is applied to promise the effectiveness of feature\ndeskewing by pulling local statistics of DSE's outputs close to the globally\nconsistent ones. Finally, DFE modules are fairly aggregated and broadcast to\nall the clients to maximize their consensus, and DSE modules are personalized\nfor each client via similarity-aware aggregation to erase their domain skew\ndifferently. Comprehensive experiments were conducted on three datasets to\nconfirm the advantages of our method in terms of accuracy, efficiency, and\ngeneralizability."}
{"id": "2503.13068", "pdf": "https://arxiv.org/pdf/2503.13068", "abs": "https://arxiv.org/abs/2503.13068", "authors": ["Henghui Du", "Guangyao Li", "Chang Zhou", "Chunjie Zhang", "Alan Zhao", "Di Hu"], "title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, numerous tasks have been proposed to encourage model to\ndevelop specified capability in understanding audio-visual scene, primarily\ncategorized into temporal localization, spatial localization, spatio-temporal\nreasoning, and pixel-level understanding. Instead, human possesses a unified\nunderstanding ability for diversified tasks. Therefore, designing an\naudio-visual model with general capability to unify these tasks is of great\nvalue. However, simply joint training for all tasks can lead to interference\ndue to the heterogeneity of audiovisual data and complex relationship among\ntasks. We argue that this problem can be solved through explicit cooperation\namong tasks. To achieve this goal, we propose a unified learning method which\nachieves explicit inter-task cooperation from both the perspectives of data and\nmodel thoroughly. Specifically, considering the labels of existing datasets are\nsimple words, we carefully refine these datasets and construct an Audio-Visual\nUnified Instruction-tuning dataset with Explicit reasoning process (AV-UIE),\nwhich clarifies the cooperative relationship among tasks. Subsequently, to\nfacilitate concrete cooperation in learning stage, an interaction-aware LoRA\nstructure with multiple LoRA heads is designed to learn different aspects of\naudiovisual data interaction. By unifying the explicit cooperation across the\ndata and model aspect, our method not only surpasses existing unified\naudio-visual model on multiple tasks, but also outperforms most specialized\nmodels for certain tasks. Furthermore, we also visualize the process of\nexplicit cooperation and surprisingly find that each LoRA head has certain\naudio-visual understanding ability. Code and dataset:\nhttps://github.com/GeWu-Lab/Crab"}
{"id": "2503.13070", "pdf": "https://arxiv.org/pdf/2503.13070", "abs": "https://arxiv.org/abs/2503.13070", "authors": ["Yihong Luo", "Tianyang Hu", "Weijian Luo", "Kenji Kawaguchi", "Jing Tang"], "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0."}
{"id": "2503.13073", "pdf": "https://arxiv.org/pdf/2503.13073", "abs": "https://arxiv.org/abs/2503.13073", "authors": ["Zhicheng Zhao", "Jinquan Yan", "Chenglong Li", "Xiao Wang", "Jin Tang"], "title": "DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Optical remote sensing image dehazing presents significant challenges due to\nits extensive spatial scale and highly non-uniform haze distribution, which\ntraditional single-image dehazing methods struggle to address effectively.\nWhile Synthetic Aperture Radar (SAR) imagery offers inherently haze-free\nreference information for large-scale scenes, existing SAR-guided dehazing\napproaches face two critical limitations: the integration of SAR information\noften diminishes the quality of haze-free regions, and the instability of\nfeature quality further exacerbates cross-modal domain shift. To overcome these\nchallenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built\non a progressive haze decoupling fusion strategy. Our approach incorporates two\nkey innovations: a Haze Perception and Decoupling Module (HPDM) that\ndynamically identifies haze-affected regions through optical-SAR difference\nanalysis, and a Progressive Fusion Module (PFM) that mitigates domain shift\nthrough a two-stage fusion process based on feature quality assessment. To\nfacilitate research in this domain, we present MRSHaze, a large-scale benchmark\ndataset comprising 8,000 pairs of temporally synchronized, precisely\ngeo-registered SAR-optical images with high resolution and diverse haze\nconditions. Extensive experiments demonstrate that DehazeMamba significantly\noutperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR\nand substantial enhancements in downstream tasks such as semantic segmentation.\nThe dataset is available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code."}
{"id": "2503.13074", "pdf": "https://arxiv.org/pdf/2503.13074", "abs": "https://arxiv.org/abs/2503.13074", "authors": ["Shaolin Su", "Josep M. Rocafort", "Danna Xue", "David Serrano-Lozano", "Lei Sun", "Javier Vazquez-Corral"], "title": "Rethinking Image Evaluation in Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100\\%\ntrustworthy for model evaluations? How does GT quality affect this evaluation?\nAnd how to make fair evaluations if there exist imperfect GTs? To answer these\nquestions, this paper presents two main contributions. First, by systematically\nanalyzing seven state-of-the-art SR models across three real-world SR datasets,\nwe show that SR performances can be consistently affected across models by\nlow-quality GTs, and models can perform quite differently when GT quality is\ncontrolled. Second, we propose a novel perceptual quality metric, Relative\nQuality Index (RQI), that measures the relative quality discrepancy of image\npairs, thus issuing the biased evaluations caused by unreliable GTs. Our\nproposed model achieves significantly better consistency with human opinions.\nWe expect our work to provide insights for the SR community on how future\ndatasets, models, and metrics should be developed."}
{"id": "2503.13086", "pdf": "https://arxiv.org/pdf/2503.13086", "abs": "https://arxiv.org/abs/2503.13086", "authors": ["Yiwei Xu", "Yifei Yu", "Wentian Gan", "Tengfei Wang", "Zongqian Zhan", "Hao Cheng", "Xin Wang"], "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction."}
{"id": "2503.13107", "pdf": "https://arxiv.org/pdf/2503.13107", "abs": "https://arxiv.org/abs/2503.13107", "authors": ["Hao Yin", "Guangzong Si", "Zilei Wang"], "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs."}
{"id": "2503.13108", "pdf": "https://arxiv.org/pdf/2503.13108", "abs": "https://arxiv.org/abs/2503.13108", "authors": ["Hao Yin", "Guangzong Si", "Zilei Wang"], "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference."}
{"id": "2503.13110", "pdf": "https://arxiv.org/pdf/2503.13110", "abs": "https://arxiv.org/abs/2503.13110", "authors": ["Jing Li", "Yihang Fu", "Falai Chen"], "title": "DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry", "categories": ["cs.CV"], "comment": null, "summary": "Boundary representation (B-rep) of geometric models is a fundamental format\nin Computer-Aided Design (CAD). However, automatically generating valid and\nhigh-quality B-rep models remains challenging due to the complex\ninterdependence between the topology and geometry of the models. Existing\nmethods tend to prioritize geometric representation while giving insufficient\nattention to topological constraints, making it difficult to maintain\nstructural validity and geometric accuracy. In this paper, we propose\nDTGBrepGen, a novel topology-geometry decoupled framework for B-rep generation\nthat explicitly addresses both aspects. Our approach first generates valid\ntopological structures through a two-stage process that independently models\nedge-face and edge-vertex adjacency relationships. Subsequently, we employ\nTransformer-based diffusion models for sequential geometry generation,\nprogressively generating vertex coordinates, followed by edge geometries and\nface geometries which are represented as B-splines. Extensive experiments on\ndiverse CAD datasets show that DTGBrepGen significantly outperforms existing\nmethods in both topological validity and geometric accuracy, achieving higher\nvalidity rates and producing more diverse and realistic B-reps. Our code is\npublicly available at https://github.com/jinli99/DTGBrepGen."}
{"id": "2503.13111", "pdf": "https://arxiv.org/pdf/2503.13111", "abs": "https://arxiv.org/abs/2503.13111", "authors": ["Erik Daxberger", "Nina Wenzel", "David Griffiths", "Haiming Gang", "Justin Lazarow", "Gefen Kohavi", "Kai Kang", "Marcin Eichner", "Yinfei Yang", "Afshin Dehghan", "Peter Grasch"], "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark."}
{"id": "2503.13120", "pdf": "https://arxiv.org/pdf/2503.13120", "abs": "https://arxiv.org/abs/2503.13120", "authors": ["Siyuan Fan", "Wenke Huang", "Xiantao Cai", "Bo Du"], "title": "3D Human Interaction Generation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "3D human interaction generation has emerged as a key research area, focusing\non producing dynamic and contextually relevant interactions between humans and\nvarious interactive entities. Recent rapid advancements in 3D model\nrepresentation methods, motion capture technologies, and generative models have\nlaid a solid foundation for the growing interest in this domain. Existing\nresearch in this field can be broadly categorized into three areas: human-scene\ninteraction, human-object interaction, and human-human interaction. Despite the\nrapid advancements in this area, challenges remain due to the need for\nnaturalness in human motion generation and the accurate interaction between\nhumans and interactive entities. In this survey, we present a comprehensive\nliterature review of human interaction generation, which, to the best of our\nknowledge, is the first of its kind. We begin by introducing the foundational\ntechnologies, including model representations, motion capture methods, and\ngenerative models. Subsequently, we introduce the approaches proposed for the\nthree sub-tasks, along with their corresponding datasets and evaluation\nmetrics. Finally, we discuss potential future research directions in this area\nand conclude the survey. Through this survey, we aim to offer a comprehensive\noverview of the current advancements in the field, highlight key challenges,\nand inspire future research works."}
{"id": "2503.13125", "pdf": "https://arxiv.org/pdf/2503.13125", "abs": "https://arxiv.org/abs/2503.13125", "authors": ["Pan Liu"], "title": "Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser Chips Based On Consistent Texture Entropy Recursive Optimization Semi-Supervised Network", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Laser chips, the core components of semiconductor lasers, are extensively\nutilized in various industries, showing great potential for future application.\nSmoothness emitting surfaces are crucial in chip production, as even\nimperceptible scratches can significantly degrade performance and lifespan,\nthus impeding production efficiency and yield. Therefore, non-destructively\ndetecting these imperceptible scratches on the emitting surfaces is essential\nfor enhancing yield and reducing costs. These sub-micron level scratches,\nbarely visible against the background, are extremely difficult to detect with\nconventional methods, compounded by a lack of labeled datasets. To address this\nchallenge, this paper introduces TexRecNet, a consistent texture entropy\nrecursive optimization semi-supervised network. The network, based on a\nrecursive optimization architecture, iteratively improves the detection\naccuracy of imperceptible scratch edges, using outputs from previous cycles to\ninform subsequent inputs and guide the network's positional encoding. It also\nintroduces image texture entropy, utilizing a substantial amount of unlabeled\ndata to expand the training set while maintaining training signal reliability.\nUltimately, by analyzing the inconsistency of the network output sequences\nobtained during the recursive process, a semi-supervised training strategy with\nrecursive consistency constraints is proposed, using outputs from the recursive\nprocess for non-destructive signal augmentation and consistently optimizes the\nloss function for efficient end-to-end training. Experimental results show that\nthis method, utilizing a substantial amount of unsupervised data, achieves\n75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5%\nand 33.6% improvement over conventional Unet, enhancing quality control in\nlaser chips."}
{"id": "2503.13130", "pdf": "https://arxiv.org/pdf/2503.13130", "abs": "https://arxiv.org/abs/2503.13130", "authors": ["Ling-An Zeng", "Guohong Huang", "Yi-Lin Wei", "Shengbo Gu", "Yu-Ming Tang", "Jingke Meng", "Wei-Shi Zheng"], "title": "ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We propose ChainHOI, a novel approach for text-driven human-object\ninteraction (HOI) generation that explicitly models interactions at both the\njoint and kinetic chain levels. Unlike existing methods that implicitly model\ninteractions using full-body poses as tokens, we argue that explicitly modeling\njoint-level interactions is more natural and effective for generating realistic\nHOIs, as it directly captures the geometric and semantic relationships between\njoints, rather than modeling interactions in the latent pose space. To this\nend, ChainHOI introduces a novel joint graph to capture potential interactions\nwith objects, and a Generative Spatiotemporal Graph Convolution Network to\nexplicitly model interactions at the joint level. Furthermore, we propose a\nKinematics-based Interaction Module that explicitly models interactions at the\nkinetic chain level, ensuring more realistic and biomechanically coherent\nmotions. Evaluations on two public datasets demonstrate that ChainHOI\nsignificantly outperforms previous methods, generating more realistic, and\nsemantically consistent HOIs. Code is available\n\\href{https://github.com/qinghuannn/ChainHOI}{here}."}
{"id": "2503.13131", "pdf": "https://arxiv.org/pdf/2503.13131", "abs": "https://arxiv.org/abs/2503.13131", "authors": ["Yaxi Chen", "Simin Ni", "Aleksandra Ivanova", "Shaheer U. Saeed", "Rikin Hargunani", "Jie Huang", "Chaozong Liu", "Yipeng Hu"], "title": "Patient-specific radiomic feature selection with reconstructed healthy persona of knee MR images", "categories": ["cs.CV"], "comment": null, "summary": "Classical radiomic features have been designed to describe image appearance\nand intensity patterns. These features are directly interpretable and readily\nunderstood by radiologists. Compared with end-to-end deep learning (DL) models,\nlower dimensional parametric models that use such radiomic features offer\nenhanced interpretability but lower comparative performance in clinical tasks.\nIn this study, we propose an approach where a standard logistic regression\nmodel performance is substantially improved by learning to select radiomic\nfeatures for individual patients, from a pool of candidate features. This\napproach has potentials to maintain the interpretability of such approaches\nwhile offering comparable performance to DL. We also propose to expand the\nfeature pool by generating a patient-specific healthy persona via\nmask-inpainting using a denoising diffusion model trained on healthy subjects.\nSuch a pathology-free baseline feature set allows further opportunity in novel\nfeature discovery and improved condition classification. We demonstrate our\nmethod on multiple clinical tasks of classifying general abnormalities,\nanterior cruciate ligament tears, and meniscus tears. Experimental results\ndemonstrate that our approach achieved comparable or even superior performance\nthan state-of-the-art DL approaches while offering added interpretability by\nusing radiomic features extracted from images and supplemented by generating\nhealthy personas. Example clinical cases are discussed in-depth to demonstrate\nthe intepretability-enabled utilities such as human-explainable feature\ndiscovery and patient-specific location/view selection. These findings\nhighlight the potentials of the combination of subject-specific feature\nselection with generative models in augmenting radiomic analysis for more\ninterpretable decision-making. The codes are available at:\nhttps://github.com/YaxiiC/RadiomicsPersona.git"}
{"id": "2503.13134", "pdf": "https://arxiv.org/pdf/2503.13134", "abs": "https://arxiv.org/abs/2503.13134", "authors": ["Prakhar Bhardwaj", "Sheethal Bhat", "Andreas Maier"], "title": "Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis", "categories": ["cs.CV"], "comment": null, "summary": "Due to the large volume of medical imaging data, advanced AI methodologies\nare needed to assist radiologists in diagnosing thoracic diseases from chest\nX-rays (CXRs). Existing deep learning models often require large, labeled\ndatasets, which are scarce in medical imaging due to the time-consuming and\nexpert-driven annotation process. In this paper, we extend the existing\napproach to enhance zero-shot learning in medical imaging by integrating\nContrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),\nresulting in our proposed model, MoCoCLIP. Our method addresses challenges\nposed by class-imbalanced and unlabeled datasets, enabling improved detection\nof pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset\ndemonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,\nachieving relative improvement of approximately 6.5%. Furthermore, on the\nCheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,\nachieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,\nhighlighting its enhanced generalization capabilities on unseen data."}
{"id": "2503.13139", "pdf": "https://arxiv.org/pdf/2503.13139", "abs": "https://arxiv.org/abs/2503.13139", "authors": ["Weiyu Guo", "Ziyang Chen", "Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Jinhui Ye", "Ying Sun", "Hui Xiong"], "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": "18 pages, under review", "summary": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable."}
{"id": "2503.13147", "pdf": "https://arxiv.org/pdf/2503.13147", "abs": "https://arxiv.org/abs/2503.13147", "authors": ["Jiayi Fu", "Siyu Liu", "Zikun Liu", "Chun-Le Guo", "Hyunhee Park", "Ruiqi Wu", "Guoqing Wang", "Chongyi Li"], "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Acceptted by CVPR 2025", "summary": "We propose a novel Iterative Predictor-Critic Code Decoding framework for\nreal-world image dehazing, abbreviated as IPC-Dehaze, which leverages the\nhigh-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from\nprevious codebook-based methods that rely on one-shot decoding, our method\nutilizes high-quality codes obtained in the previous iteration to guide the\nprediction of the Code-Predictor in the subsequent iteration, improving code\nprediction accuracy and ensuring stable dehazing performance. Our idea stems\nfrom the observations that 1) the degradation of hazy images varies with haze\ndensity and scene depth, and 2) clear regions play crucial cues in restoring\ndense haze regions. However, it is non-trivial to progressively refine the\nobtained codes in subsequent iterations, owing to the difficulty in determining\nwhich codes should be retained or replaced at each iteration. Another key\ninsight of our study is to propose Code-Critic to capture interrelations among\ncodes. The Code-Critic is used to evaluate code correlations and then resample\na set of codes with the highest mask scores, i.e., a higher score indicates\nthat the code is more likely to be rejected, which helps retain more accurate\ncodes and predict difficult ones. Extensive experiments demonstrate the\nsuperiority of our method over state-of-the-art methods in real-world dehazing."}
{"id": "2503.13156", "pdf": "https://arxiv.org/pdf/2503.13156", "abs": "https://arxiv.org/abs/2503.13156", "authors": ["Zakariae Zrimek", "Youssef Mourchid", "Mohammed El Hassouni"], "title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph Knowledge Distillation for Gait Disorders Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment."}
{"id": "2503.13160", "pdf": "https://arxiv.org/pdf/2503.13160", "abs": "https://arxiv.org/abs/2503.13160", "authors": ["Zihao Liu", "Xiaoyu Wu", "Jianqin Wu", "Xuxu Wang", "Linlin Yang"], "title": "Language-guided Open-world Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released."}
{"id": "2503.13163", "pdf": "https://arxiv.org/pdf/2503.13163", "abs": "https://arxiv.org/abs/2503.13163", "authors": ["Shani Gamrian", "Hila Barel", "Feiran Li", "Masakazu Yoshimura", "Daisuke Iso"], "title": "Beyond RGB: Adaptive Parallel Processing for RAW Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Object detection models are typically applied to standard RGB images\nprocessed through Image Signal Processing (ISP) pipelines, which are designed\nto enhance sensor-captured RAW images for human vision. However, these ISP\nfunctions can lead to a loss of critical information that may be essential in\noptimizing for computer vision tasks, such as object detection. In this work,\nwe introduce Raw Adaptation Module (RAM), a module designed to replace the\ntraditional ISP, with parameters optimized specifically for RAW object\ndetection. Inspired by the parallel processing mechanisms of the human visual\nsystem, RAM departs from existing learned ISP methods by applying multiple ISP\nfunctions in parallel rather than sequentially, allowing for a more\ncomprehensive capture of image features. These processed representations are\nthen fused in a specialized module, which dynamically integrates and optimizes\nthe information for the target task. This novel approach not only leverages the\nfull potential of RAW sensor data but also enables task-specific\npre-processing, resulting in superior object detection performance. Our\napproach outperforms RGB-based methods and achieves state-of-the-art results\nacross diverse RAW image datasets under varying lighting conditions and dynamic\nranges."}
{"id": "2503.13165", "pdf": "https://arxiv.org/pdf/2503.13165", "abs": "https://arxiv.org/abs/2503.13165", "authors": ["Chen Zhao", "Zhizhou Chen", "Yunzhe Xu", "Enxuan Gu", "Jian Li", "Zili Yi", "Qian Wang", "Jian Yang", "Ying Tai"], "title": "From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Ultra-high-definition (UHD) image restoration faces significant challenges\ndue to its high resolution, complex content, and intricate details. To cope\nwith these challenges, we analyze the restoration process in depth through a\nprogressive spectral perspective, and deconstruct the complex UHD restoration\nproblem into three progressive stages: zero-frequency enhancement,\nlow-frequency restoration, and high-frequency refinement. Building on this\ninsight, we propose a novel framework, ERR, which comprises three collaborative\nsub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer\n(LFR), and the high-frequency refiner (HFR). Specifically, the ZFE integrates\nglobal priors to learn global mapping, while the LFR restores low-frequency\ninformation, emphasizing reconstruction of coarse-grained content. Finally, the\nHFR employs our designed frequency-windowed kolmogorov-arnold networks (FW-KAN)\nto refine textures and details, producing high-quality image restoration. Our\napproach significantly outperforms previous UHD methods across various tasks,\nwith extensive ablation studies validating the effectiveness of each component.\nThe code is available at \\href{https://github.com/NJU-PCALab/ERR}{here}."}
{"id": "2503.13176", "pdf": "https://arxiv.org/pdf/2503.13176", "abs": "https://arxiv.org/abs/2503.13176", "authors": ["Rui Wang", "Quentin Lohmeyer", "Mirko Meboldt", "Siyu Tang"], "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing clean, distractor-free 3D scenes from real-world captures\nremains a significant challenge, particularly in highly dynamic and cluttered\nsettings such as egocentric videos. To tackle this problem, we introduce\nDeGauss, a simple and robust self-supervised framework for dynamic scene\nreconstruction based on a decoupled dynamic-static Gaussian Splatting design.\nDeGauss models dynamic elements with foreground Gaussians and static content\nwith background Gaussians, using a probabilistic mask to coordinate their\ncomposition and enable independent yet complementary optimization. DeGauss\ngeneralizes robustly across a wide range of real-world scenarios, from casual\nimage collections to long, dynamic egocentric videos, without relying on\ncomplex heuristics or extensive supervision. Experiments on benchmarks\nincluding NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that\nDeGauss consistently outperforms existing methods, establishing a strong\nbaseline for generalizable, distractor-free 3D reconstructionin highly dynamic,\ninteraction-rich environments."}
{"id": "2503.13179", "pdf": "https://arxiv.org/pdf/2503.13179", "abs": "https://arxiv.org/abs/2503.13179", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin"], "title": "A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network", "categories": ["cs.CV"], "comment": null, "summary": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models."}
{"id": "2503.13184", "pdf": "https://arxiv.org/pdf/2503.13184", "abs": "https://arxiv.org/abs/2503.13184", "authors": ["Yuanze Li", "Shihao Yuan", "Haolin Wang", "Qizhang Li", "Ming Liu", "Chen Xu", "Guangming Shi", "Wangmeng Zuo"], "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process", "categories": ["cs.CV"], "comment": null, "summary": "Although recent methods have tried to introduce large multimodal models\n(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD\nfield is far inferior to that for general purposes. We summarize the main\nreasons for this gap into two aspects. On one hand, general-purpose LMMs lack\ncognition of defects in the visual modality, thereby failing to sufficiently\nfocus on defect areas. Therefore, we propose to modify the AnyRes structure of\nthe LLaVA model, providing the potential anomalous areas identified by existing\nIAD models to the LMMs. On the other hand, existing methods mainly focus on\nidentifying defects by learning defect patterns or comparing with normal\nsamples, yet they fall short of understanding the causes of these defects.\nConsidering that the generation of defects is closely related to the\nmanufacturing process, we propose a manufacturing-driven IAD paradigm. An\ninstruction-tuning dataset for IAD (InstructIAD) and a data organization\napproach for Chain-of-Thought with manufacturing (CoT-M) are designed to\nleverage the manufacturing process for IAD. Based on the above two\nmodifications, we present Triad, a novel LMM-based method incorporating an\nexpert-guided region-of-interest tokenizer and manufacturing process for\nindustrial anomaly detection. Extensive experiments show that our Triad not\nonly demonstrates competitive performance against current LMMs but also\nachieves further improved accuracy when equipped with manufacturing processes.\nSource code, training data, and pre-trained models will be publicly available\nat https://github.com/tzjtatata/Triad."}
{"id": "2503.13185", "pdf": "https://arxiv.org/pdf/2503.13185", "abs": "https://arxiv.org/abs/2503.13185", "authors": ["Dingning Liu", "Cheng Wang", "Peng Gao", "Renrui Zhang", "Xinzhu Ma", "Yuan Meng", "Zhihui Wang"], "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt\nengineering techniques."}
{"id": "2503.13188", "pdf": "https://arxiv.org/pdf/2503.13188", "abs": "https://arxiv.org/abs/2503.13188", "authors": ["Matteo Sodano", "Federico Magistri", "Elias Marks", "Fares Hosn", "Aibek Zurbayev", "Rodrigo Marcuzzi", "Meher V. R. Malladi", "Jens Behley", "Cyrill Stachniss"], "title": "3D Hierarchical Panoptic Segmentation in Real Orchard Environments Across Different Sensors", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to IROS", "summary": "Crop yield estimation is a relevant problem in agriculture, because an\naccurate crop yield estimate can support farmers' decisions on harvesting or\nprecision intervention. Robots can help to automate this process. To do so,\nthey need to be able to perceive the surrounding environment to identify target\nobjects. In this paper, we introduce a novel approach to address the problem of\nhierarchical panoptic segmentation of apple orchards on 3D data from different\nsensors. Our approach is able to simultaneously provide semantic segmentation,\ninstance segmentation of trunks and fruits, and instance segmentation of plants\n(a single trunk with its fruits). This allows us to identify relevant\ninformation such as individual plants, fruits, and trunks, and capture the\nrelationship among them, such as precisely estimate the number of fruits\nassociated to each tree in an orchard. Additionally, to efficiently evaluate\nour approach for hierarchical panoptic segmentation, we provide a dataset\ndesigned specifically for this task. Our dataset is recorded in Bonn in a real\napple orchard with a variety of sensors, spanning from a terrestrial laser\nscanner to a RGB-D camera mounted on different robotic platforms. The\nexperiments show that our approach surpasses state-of-the-art approaches in 3D\npanoptic segmentation in the agricultural domain, while also providing full\nhierarchical panoptic segmentation. Our dataset has been made publicly\navailable at https://www.ipb.uni-bonn.de/data/hops/. We will provide the\nopen-source implementation of our approach and public competiton for\nhierarchical panoptic segmentation on the hidden test sets upon paper\nacceptance."}
{"id": "2503.13203", "pdf": "https://arxiv.org/pdf/2503.13203", "abs": "https://arxiv.org/abs/2503.13203", "authors": ["Corentin Sautier", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Vincent Lepetit"], "title": "Clustering is back: Reaching state-of-the-art LiDAR instance segmentation without training", "categories": ["cs.CV"], "comment": null, "summary": "Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\nachieves performance comparable to current state-of-the-art supervised methods\non standard benchmarks including SemanticKITTI and nuScenes, and outperforms\nevery publicly available method on SemanticKITTI as a drop-in instance head\nreplacement, while running in real-time on a single-threaded CPU and requiring\nno instance labels. Our method is fully explainable, and requires no learning\nor parameter tuning. Code is available at https://github.com/valeoai/Alpine/"}
{"id": "2503.13211", "pdf": "https://arxiv.org/pdf/2503.13211", "abs": "https://arxiv.org/abs/2503.13211", "authors": ["Marvin Seyfarth", "Salman Ul Hassan Dar", "Isabelle Ayx", "Matthias Alexander Fink", "Stefan O. Schoenberg", "Hans-Ulrich Kauczor", "Sandy Engelhardt"], "title": "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments."}
{"id": "2503.13214", "pdf": "https://arxiv.org/pdf/2503.13214", "abs": "https://arxiv.org/abs/2503.13214", "authors": ["Jie Huang", "Haorui Chen", "Jiaxuan Ren", "Siran Peng", "Liangjian Deng"], "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": "This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025", "summary": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM."}
{"id": "2503.13229", "pdf": "https://arxiv.org/pdf/2503.13229", "abs": "https://arxiv.org/abs/2503.13229", "authors": ["Yongkang Cheng", "Shaoli Huang"], "title": "HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures", "categories": ["cs.CV"], "comment": "Accepted by 3DV 2025", "summary": "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/."}
{"id": "2503.13241", "pdf": "https://arxiv.org/pdf/2503.13241", "abs": "https://arxiv.org/abs/2503.13241", "authors": ["Zhifu Tian", "Tao Hu", "Chaoyang Niu", "Di Wu", "Shu Wang"], "title": "Sampling Innovation-Based Adaptive Compressive Sensing", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR2025 accepted", "summary": "Scene-aware Adaptive Compressive Sensing (ACS) has attracted significant\ninterest due to its promising capability for efficient and high-fidelity\nacquisition of scene images. ACS typically prescribes adaptive sampling\nallocation (ASA) based on previous samples in the absence of ground truth.\nHowever, when confronting unknown scenes, existing ACS methods often lack\naccurate judgment and robust feedback mechanisms for ASA, thus limiting the\nhigh-fidelity sensing of the scene. In this paper, we introduce a Sampling\nInnovation-Based ACS (SIB-ACS) method that can effectively identify and\nallocate sampling to challenging image reconstruction areas, culminating in\nhigh-fidelity image reconstruction. An innovation criterion is proposed to\njudge ASA by predicting the decrease in image reconstruction error attributable\nto sampling increments, thereby directing more samples towards regions where\nthe reconstruction error diminishes significantly. A sampling innovation-guided\nmulti-stage adaptive sampling (AS) framework is proposed, which iteratively\nrefines the ASA through a multi-stage feedback process. For image\nreconstruction, we propose a Principal Component Compressed Domain Network\n(PCCD-Net), which efficiently and faithfully reconstructs images under AS\nscenarios. Extensive experiments demonstrate that the proposed SIB-ACS method\nsignificantly outperforms the state-of-the-art methods in terms of image\nreconstruction fidelity and visual effects. Codes are available at\nhttps://github.com/giant-pandada/SIB-ACS_CVPR2025."}
{"id": "2503.13260", "pdf": "https://arxiv.org/pdf/2503.13260", "abs": "https://arxiv.org/abs/2503.13260", "authors": ["Amit Zalcher", "Navve Wasserman", "Roman Beliy", "Oliver Heinimann", "Michal Irani"], "title": "Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual perceptual tasks aim to predict human judgment of images (e.g.,\nemotions invoked by images, image quality assessment). Unlike objective tasks\nsuch as object/scene recognition, perceptual tasks rely on subjective human\nassessments, making its data-labeling difficult. The scarcity of such\nhuman-annotated data results in small datasets leading to poor generalization.\nTypically, specialized models were designed for each perceptual task, tailored\nto its unique characteristics and its own training dataset. We propose a\nunified architectural framework for solving multiple different perceptual tasks\nleveraging CLIP as a prior. Our approach is based on recent cognitive findings\nwhich indicate that CLIP correlates well with human judgment. While CLIP was\nexplicitly trained to align images and text, it implicitly also learned human\ninclinations. We attribute this to the inclusion of human-written image\ncaptions in CLIP's training data, which contain not only factual image\ndescriptions, but inevitably also human sentiments and emotions. This makes\nCLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest\nthat minimal adaptation of CLIP suffices for solving a variety of perceptual\ntasks. Our simple unified framework employs a lightweight adaptation to\nfine-tune CLIP to each task, without requiring any task-specific architectural\nchanges. We evaluate our approach on three tasks: (i) Image Memorability\nPrediction, (ii) No-reference Image Quality Assessment, and (iii) Visual\nEmotion Analysis. Our model achieves state-of-the-art results on all three\ntasks, while demonstrating improved generalization across different datasets."}
{"id": "2503.13265", "pdf": "https://arxiv.org/pdf/2503.13265", "abs": "https://arxiv.org/abs/2503.13265", "authors": ["Luxi Chen", "Zihan Zhou", "Min Zhao", "Yikai Wang", "Ge Zhang", "Wenhao Huang", "Hao Sun", "Ji-Rong Wen", "Chongxuan Li"], "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld."}
{"id": "2503.13272", "pdf": "https://arxiv.org/pdf/2503.13272", "abs": "https://arxiv.org/abs/2503.13272", "authors": ["Katja Schwarz", "Norman Mueller", "Peter Kontschieder"], "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing consistent and photorealistic 3D scenes is an open problem in\ncomputer vision. Video diffusion models generate impressive videos but cannot\ndirectly synthesize 3D representations, i.e., lack 3D consistency in the\ngenerated sequences. In addition, directly training generative 3D models is\nchallenging due to a lack of 3D training data at scale. In this work, we\npresent Generative Gaussian Splatting (GGS) -- a novel approach that integrates\na 3D representation with a pre-trained latent video diffusion model.\nSpecifically, our model synthesizes a feature field parameterized via 3D\nGaussian primitives. The feature field is then either rendered to feature maps\nand decoded into multi-view images, or directly upsampled into a 3D radiance\nfield. We evaluate our approach on two common benchmark datasets for scene\nsynthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model\nsignificantly improves both the 3D consistency of the generated multi-view\nimages, and the quality of the generated 3D scenes over all relevant baselines.\nCompared to a similar model without 3D representation, GGS improves FID on the\ngenerated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:\nhttps://katjaschwarz.github.io/ggs/"}
{"id": "2503.13300", "pdf": "https://arxiv.org/pdf/2503.13300", "abs": "https://arxiv.org/abs/2503.13300", "authors": ["Ling-An Zeng", "Gaojie Wu", "Ancong Wu", "Jian-Fang Hu", "Wei-Shi Zheng"], "title": "Progressive Human Motion Generation Based on Text and Few Motion Frames", "categories": ["cs.CV"], "comment": null, "summary": "Although existing text-to-motion (T2M) methods can produce realistic human\nmotion from text description, it is still difficult to align the generated\nmotion with the desired postures since using text alone is insufficient for\nprecisely describing diverse postures. To achieve more controllable generation,\nan intuitive way is to allow the user to input a few motion frames describing\nprecise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M)\ngeneration task that aims to generate motions from text and very few given\nframes. Intuitively, the closer a frame is to a given frame, the lower the\nuncertainty of this frame is when conditioned on this given frame. Hence, we\npropose a novel Progressive Motion Generation (PMG) method to progressively\ngenerate a motion from the frames with low uncertainty to those with high\nuncertainty in multiple stages. During each stage, new frames are generated by\na Text-Frame Guided Generator conditioned on frame-aware semantics of the text,\ngiven frames, and frames generated in previous stages. Additionally, to\nalleviate the train-test gap caused by multi-stage accumulation of incorrectly\ngenerated frames during testing, we propose a Pseudo-frame Replacement Strategy\nfor training. Experimental results show that our PMG outperforms existing T2M\ngeneration methods by a large margin with even one given frame, validating the\neffectiveness of our PMG. Code will be released."}
{"id": "2503.13303", "pdf": "https://arxiv.org/pdf/2503.13303", "abs": "https://arxiv.org/abs/2503.13303", "authors": ["Yinqiao Wang", "Hao Xu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures, 7 tables", "summary": "Estimating the 3D pose of hand and potential hand-held object from monocular\nimages is a longstanding challenge. Yet, existing methods are specialized,\nfocusing on either bare-hand or hand interacting with object. No method can\nflexibly handle both scenarios and their performance degrades when applied to\nthe other scenario. In this paper, we propose UniHOPE, a unified approach for\ngeneral 3D hand-object pose estimation, flexibly adapting both scenarios.\nTechnically, we design a grasp-aware feature fusion module to integrate\nhand-object features with an object switcher to dynamically control the\nhand-object pose estimation according to grasping status. Further, to uplift\nthe robustness of hand pose estimation regardless of object presence, we\ngenerate realistic de-occluded image pairs to train the model to learn\nobject-induced hand occlusions, and formulate multi-level feature enhancement\ntechniques for learning occlusion-invariant features. Extensive experiments on\nthree commonly-used benchmarks demonstrate UniHOPE's SOTA performance in\naddressing hand-only and hand-object scenarios. Code will be released on\nhttps://github.com/JoyboyWang/UniHOPE_Pytorch."}
{"id": "2503.13319", "pdf": "https://arxiv.org/pdf/2503.13319", "abs": "https://arxiv.org/abs/2503.13319", "authors": ["Shitong Shao", "Hongwei Yi", "Hanzhong Guo", "Tian Ye", "Daquan Zhou", "Michael Lingelbach", "Zhiqiang Xu", "Zeke Xie"], "title": "MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Fine-tuning open-source large-scale VDMs for the portrait video synthesis\ntask can result in significant improvements across multiple dimensions, such as\nvisual quality and natural facial motion dynamics. Despite their advancements,\nhow to achieve step distillation and reduce the substantial computational\noverhead of large-scale VDMs remains unexplored. To fill this gap, this paper\nproposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue\nof insufficient training memory and the problem of training collapse observed\nin vanilla DMD during the training process. Specifically, we first leverage\nLoRA to fine-tune the fake diffusion transformer (DiT) to address the\nout-of-memory issue. Then, we employ the W2S distribution matching to adjust\nthe real DiT's parameter, subtly shifting it toward the fake DiT's parameter.\nThis adjustment is achieved by utilizing the weak weight of the low-rank\nbranch, effectively alleviate the conundrum where the video synthesized by the\nfew-step generator deviates from the real data distribution, leading to\ninaccuracies in the KL divergence approximation. Additionally, we minimize the\ndistance between the fake data distribution and the ground truth distribution\nto further enhance the visual quality of the synthesized videos. As\nexperimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard\nEuler, LCM, DMD and even the 28-step standard sampling in FID/FVD and VBench in\n1/4-step video synthesis. The project page is in\nhttps://w2svd.github.io/W2SVD/."}
{"id": "2503.13327", "pdf": "https://arxiv.org/pdf/2503.13327", "abs": "https://arxiv.org/abs/2503.13327", "authors": ["Lan Chen", "Qi Mao", "Yuchao Gu", "Mike Zheng Shou"], "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning."}
{"id": "2503.13344", "pdf": "https://arxiv.org/pdf/2503.13344", "abs": "https://arxiv.org/abs/2503.13344", "authors": ["Shashikant Verma", "Harish Katti", "Soumyaratna Debnath", "Yamuna Swamy", "Shanmuganathan Raman"], "title": "STEP: Simultaneous Tracking and Estimation of Pose for Animals and Humans", "categories": ["cs.CV"], "comment": null, "summary": "We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state\ninitialized through a pre-trained detector or manual initialization in the\ninitial frame of a given video sequence. It then seamlessly tracks the target\nand estimates keypoints of anatomical importance as output for subsequent\nframes. Unlike prevalent top-down pose estimation methods, our approach doesn't\nrely on per-frame target detections due to its tracking capability. This\nfacilitates a significant advancement in inference efficiency and potential\napplications. We train and validate our approach on datasets encompassing\ndiverse species. Our experiments demonstrate superior results compared to\nexisting methods, opening doors to various applications, including but not\nlimited to action recognition and behavioral analysis."}
{"id": "2503.13347", "pdf": "https://arxiv.org/pdf/2503.13347", "abs": "https://arxiv.org/abs/2503.13347", "authors": ["Jiaming Kang", "Keyan Chen", "Zhengxia Zou", "Zhenwei Shi"], "title": "TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing novel view synthesis (NVS) offers significant potential for 3D\ninterpretation of remote sensing scenes, with important applications in urban\nplanning and environmental monitoring. However, remote sensing scenes\nfrequently lack sufficient multi-view images due to acquisition constraints.\nWhile existing NVS methods tend to overfit when processing limited input views,\nadvanced few-shot NVS methods are computationally intensive and perform\nsub-optimally in remote sensing scenes. This paper presents TriDF, an efficient\nhybrid 3D representation for fast remote sensing NVS from as few as 3 input\nviews. Our approach decouples color and volume density information, modeling\nthem independently to reduce the computational burden on implicit radiance\nfields and accelerate reconstruction. We explore the potential of the triplane\nrepresentation in few-shot NVS tasks by mapping high-frequency color\ninformation onto this compact structure, and the direct optimization of feature\nplanes significantly speeds up convergence. Volume density is modeled as\ncontinuous density fields, incorporating reference features from neighboring\nviews through image-based rendering to compensate for limited input data.\nAdditionally, we introduce depth-guided optimization based on point clouds,\nwhich effectively mitigates the overfitting problem in few-shot NVS.\nComprehensive experiments across multiple remote sensing scenes demonstrate\nthat our hybrid representation achieves a 30x speed increase compared to\nNeRF-based methods, while simultaneously improving rendering quality metrics\nover advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%\nin LPIPS). The code is publicly available at https://github.com/kanehub/TriDF"}
{"id": "2503.13354", "pdf": "https://arxiv.org/pdf/2503.13354", "abs": "https://arxiv.org/abs/2503.13354", "authors": ["Laura Girometti", "Jean-Fran√ßois Aujol", "Antoine Guennec", "Yann Traonmilin"], "title": "Parameter-free structure-texture image decomposition by unrolling", "categories": ["cs.CV", "cs.NA", "eess.IV", "math.NA", "68U10, 90C26"], "comment": "To be published in Conference Proceedings: Scale Space and\n  Variational Method in Computer Vision, 2025", "summary": "In this work, we propose a parameter-free and efficient method to tackle the\nstructure-texture image decomposition problem. In particular, we present a\nneural network LPR-NET based on the unrolling of the Low Patch Rank model. On\nthe one hand, this allows us to automatically learn parameters from data, and\non the other hand to be computationally faster while obtaining qualitatively\nsimilar results compared to traditional iterative model-based methods.\nMoreover, despite being trained on synthetic images, numerical experiments show\nthe ability of our network to generalize well when applied to natural images."}
{"id": "2503.13358", "pdf": "https://arxiv.org/pdf/2503.13358", "abs": "https://arxiv.org/abs/2503.13358", "authors": ["Daniil Selikhanovych", "David Li", "Aleksei Leonov", "Nikita Gushchin", "Sergei Kushneriuk", "Alexander Filippov", "Evgeny Burnaev", "Iaroslav Koshelev", "Alexander Korotin"], "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K."}
{"id": "2503.13360", "pdf": "https://arxiv.org/pdf/2503.13360", "abs": "https://arxiv.org/abs/2503.13360", "authors": ["Hai-Long Sun", "Zhun Sun", "Houwen Peng", "Han-Jia Ye"], "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems."}
{"id": "2503.13377", "pdf": "https://arxiv.org/pdf/2503.13377", "abs": "https://arxiv.org/abs/2503.13377", "authors": ["Ye Wang", "Boshen Xu", "Zihao Yue", "Zihan Xiao", "Ziheng Wang", "Liang Zhang", "Dingyi Yang", "Wenxuan Wang", "Qin Jin"], "title": "TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code: https://github.com/www-Ye/TimeZero", "summary": "We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero."}
{"id": "2503.13383", "pdf": "https://arxiv.org/pdf/2503.13383", "abs": "https://arxiv.org/abs/2503.13383", "authors": ["Mengyao Lyu", "Yan Li", "Huasong Zhong", "Wenhao Yang", "Hui Chen", "Jungong Han", "Guiguang Ding", "Zhenheng Yang"], "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "update comparison with sota and analysis", "summary": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data."}
{"id": "2503.13385", "pdf": "https://arxiv.org/pdf/2503.13385", "abs": "https://arxiv.org/abs/2503.13385", "authors": ["Qing Zhou", "Junyu Gao", "Qi Wang"], "title": "Scale Efficient Training for Large Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa."}
{"id": "2503.13399", "pdf": "https://arxiv.org/pdf/2503.13399", "abs": "https://arxiv.org/abs/2503.13399", "authors": ["James Burgess", "Jeffrey J Nirschl", "Laura Bravo-S√°nchez", "Alejandro Lozano", "Sanket Rajan Gupte", "Jesus G. Galaz-Montoya", "Yuhui Zhang", "Yuchang Su", "Disha Bhowmik", "Zachary Coman", "Sarina M. Hasan", "Alexandra Johannesson", "William D. Leineweber", "Malvika G Nair", "Ridhi Yarlagadda", "Connor Zuraski", "Wah Chiu", "Sarah Cohen", "Jan N. Hansen", "Manuel D Leonetti", "Chad Liu", "Emma Lundberg", "Serena Yeung-Levy"], "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.CB"], "comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa", "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa."}
{"id": "2503.13424", "pdf": "https://arxiv.org/pdf/2503.13424", "abs": "https://arxiv.org/abs/2503.13424", "authors": ["Xinyu Lian", "Zichao Yu", "Ruiming Liang", "Yitong Wang", "Li Ray Luo", "Kaixu Chen", "Yuanzhen Zhou", "Qihong Tang", "Xudong Xu", "Zhaoyang Lyu", "Bo Dai", "Jiangmiao Pang"], "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation", "categories": ["cs.CV"], "comment": "Project page: https://infinite-mobility.github.io 10 pages,12 figures", "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility"}
{"id": "2503.13429", "pdf": "https://arxiv.org/pdf/2503.13429", "abs": "https://arxiv.org/abs/2503.13429", "authors": ["Nhi Pham", "Bernt Schiele", "Adam Kortylewski", "Jonas Fischer"], "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes", "categories": ["cs.CV"], "comment": null, "summary": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness."}
{"id": "2503.13430", "pdf": "https://arxiv.org/pdf/2503.13430", "abs": "https://arxiv.org/abs/2503.13430", "authors": ["Thomas Monninger", "Md Zafar Anwar", "Stanislaw Antol", "Steffen Staab", "Sihao Ding"], "title": "AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Autonomous driving requires an understanding of the infrastructure elements,\nsuch as lanes and crosswalks. To navigate safely, this understanding must be\nderived from sensor data in real-time and needs to be represented in vectorized\nform. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set\nof camera images from multiple views into one joint latent BEV grid.\nTraditionally, from this latent space, an intermediate raster map is predicted,\nproviding dense spatial supervision but requiring post-processing into the\ndesired vectorized form. More recent models directly derive infrastructure\nelements as polylines using vectorized map decoders, providing instance-level\ninformation. Our approach, Augmentation Map Network (AugMapNet), proposes\nlatent BEV grid augmentation, a novel technique that significantly enhances the\nlatent BEV representation. AugMapNet combines vector decoding and dense spatial\nsupervision more effectively than existing architectures while remaining as\nstraightforward to integrate and as generic as auxiliary supervision.\nExperiments on nuScenes and Argoverse2 datasets demonstrate significant\nimprovements in vectorized map prediction performance up to 13.3% over the\nStreamMapNet baseline on 60m range and greater improvements on larger ranges.\nWe confirm transferability by applying our method to another baseline and find\nsimilar improvements. A detailed analysis of the latent BEV grid confirms a\nmore structured latent space of AugMapNet and shows the value of our novel\nconcept beyond pure performance improvement. The code will be released soon."}
{"id": "2503.13433", "pdf": "https://arxiv.org/pdf/2503.13433", "abs": "https://arxiv.org/abs/2503.13433", "authors": ["Johan Edstedt"], "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC", "categories": ["cs.CV"], "comment": null, "summary": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1."}
{"id": "2503.13434", "pdf": "https://arxiv.org/pdf/2503.13434", "abs": "https://arxiv.org/abs/2503.13434", "authors": ["Yaowei Li", "Lingen Li", "Zhaoyang Zhang", "Xiaoyu Li", "Guangzhi Wang", "Hongxiang Li", "Xiaodong Cun", "Ying Shan", "Yuexian Zou"], "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/", "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/"}
{"id": "2503.13435", "pdf": "https://arxiv.org/pdf/2503.13435", "abs": "https://arxiv.org/abs/2503.13435", "authors": ["Ling Yang", "Kaixin Zhu", "Juanxi Tian", "Bohan Zeng", "Mingbao Lin", "Hongjuan Pei", "Wentao Zhang", "Shuicheng Yan"], "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/WideRange4D", "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D"}
{"id": "2503.13436", "pdf": "https://arxiv.org/pdf/2503.13436", "abs": "https://arxiv.org/abs/2503.13436", "authors": ["Lijie Fan", "Luming Tang", "Siyang Qin", "Tianhong Li", "Xuan Yang", "Siyuan Qiao", "Andreas Steiner", "Chen Sun", "Yuanzhen Li", "Tao Zhu", "Michael Rubinstein", "Michalis Raptis", "Deqing Sun", "Radu Soricut"], "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Tech report", "summary": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding."}
{"id": "2503.13439", "pdf": "https://arxiv.org/pdf/2503.13439", "abs": "https://arxiv.org/abs/2503.13439", "authors": ["Tianhao Wu", "Chuanxia Zheng", "Frank Guan", "Andrea Vedaldi", "Tat-Jen Cham"], "title": "Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images", "categories": ["cs.CV"], "comment": "Project Page: https://sm0kywu.github.io/Amodal3R/", "summary": "Most image-based 3D object reconstructors assume that objects are fully\nvisible, ignoring occlusions that commonly occur in real-world scenarios. In\nthis paper, we introduce Amodal3R, a conditional 3D generative model designed\nto reconstruct 3D objects from partial observations. We start from a\n\"foundation\" 3D generative model and extend it to recover plausible 3D geometry\nand appearance from occluded objects. We introduce a mask-weighted multi-head\ncross-attention mechanism followed by an occlusion-aware attention layer that\nexplicitly leverages occlusion priors to guide the reconstruction process. We\ndemonstrate that, by training solely on synthetic data, Amodal3R learns to\nrecover full 3D objects even in the presence of occlusions in real scenes. It\nsubstantially outperforms existing methods that independently perform 2D amodal\ncompletion followed by 3D reconstruction, thereby establishing a new benchmark\nfor occlusion-aware 3D reconstruction."}
{"id": "2503.13440", "pdf": "https://arxiv.org/pdf/2503.13440", "abs": "https://arxiv.org/abs/2503.13440", "authors": ["Yingyue Li", "Bencheng Liao", "Wenyu Liu", "Xinggang Wang"], "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling", "categories": ["cs.CV"], "comment": "Code and model are available at http://github.com/hustvl/MaTVLM", "summary": "With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http://github.com/hustvl/MaTVLM."}
{"id": "2503.13443", "pdf": "https://arxiv.org/pdf/2503.13443", "abs": "https://arxiv.org/abs/2503.13443", "authors": ["Haoyang Li", "Liang Wang", "Chao Wang", "Jing Jiang", "Yan Peng", "Guodong Long"], "title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 (CVPR 2025)", "summary": "The Base-New Trade-off (BNT) problem universally exists during the\noptimization of CLIP-based prompt tuning, where continuous fine-tuning on base\n(target) classes leads to a simultaneous decrease of generalization ability on\nnew (unseen) classes. Existing approaches attempt to regulate the prompt tuning\nprocess to balance BNT by appending constraints. However, imposed on the same\ntarget prompt, these constraints fail to fully avert the mutual exclusivity\nbetween the optimization directions for base and new. As a novel solution to\nthis challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)\nframework, the first that decoupling the optimization processes of base and new\ntasks at the prompt level. Specifically, we clone a learnable parallel prompt\nbased on the backbone prompt, and introduce a variable Weighting-Decoupling\nframework to independently control the optimization directions of dual prompts\nspecific to base or new tasks, thus avoiding the conflict in generalization.\nMeanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts\nto construct a more challenging optimization task on base classes for\nenhancement. For interpretability, we prove the feature channel invariance of\nthe prompt vector during the optimization process, providing theoretical\nsupport for the Weighting-Decoupling of DPC. Extensive experiments on multiple\nbackbones demonstrate that DPC can significantly improve base performance\nwithout introducing any external knowledge beyond the base classes, while\nmaintaining generalization to new classes. Code is available at:\nhttps://github.com/JREion/DPC."}
{"id": "2503.13444", "pdf": "https://arxiv.org/pdf/2503.13444", "abs": "https://arxiv.org/abs/2503.13444", "authors": ["Ye Liu", "Kevin Qinghong Lin", "Chang Wen Chen", "Mike Zheng Shou"], "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://videomind.github.io/", "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning."}
{"id": "2503.11677", "pdf": "https://arxiv.org/pdf/2503.11677", "abs": "https://arxiv.org/abs/2503.11677", "authors": ["Jungyeon Park", "Anna Kochnev Goldstein", "Yueming Zhou", "Nathan Jensen", "Daniel Palanker"], "title": "Simulation of prosthetic vision with PRIMA system and enhancement of face representation", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Objective. Patients implanted with the PRIMA photovoltaic subretinal\nprosthesis in geographic atrophy report form vision with the average acuity\nmatching the 100um pixel size. Although this remarkable outcome enables them to\nread and write, they report difficulty with perceiving faces. This paper\nprovides a novel, non-pixelated algorithm for simulating prosthetic vision the\nway it is experienced by PRIMA patients, compares the algorithm's predictions\nto clinical perceptual outcomes, and offers computer vision and machine\nlearning (ML) methods to improve face representation. Approach. Our simulation\nalgorithm integrates a grayscale filter, spatial resolution filter, and\ncontrast filter. This accounts for the limited sampling density of the retinal\nimplant, as well as the reduced contrast sensitivity of prosthetic vision.\nPatterns of Landolt C and faces created using this simulation algorithm are\ncompared to reports from actual PRIMA users. To recover the facial features\nlost in prosthetic vision, we apply an ML facial landmarking model as well as\ncontrast adjusting tone curves to the face image prior to its projection onto\nthe implant. Main results. Simulated prosthetic vision matches the maximum\nletter acuity observed in clinical studies as well as patients' subjective\ndescriptions. Application of the inversed contrast filter helps preserve the\ncontrast in prosthetic vision. Identification of the facial features using an\nML facial landmarking model and accentuating them further improve face\nrepresentation. Significance. Spatial and contrast constraints of prosthetic\nvision limit resolvable features and degrade natural images. ML based methods\nand contrast adjustments mitigate some limitations and improve face\nrepresentation. Even though higher spatial resolution can be expected with\nimplants having smaller pixels, contrast enhancement still remains essential\nfor face recognition."}
{"id": "2503.11685", "pdf": "https://arxiv.org/pdf/2503.11685", "abs": "https://arxiv.org/abs/2503.11685", "authors": ["Omkar Kokane", "Adam Teman", "Anushka Jha", "Guru Prasath SL", "Gopal Raut", "Mukul Lokhande", "S. V. Jaya Chand", "Tanushree Dewangan", "Santosh Kumar Vishvakarma"], "title": "CORDIC Is All You Need", "categories": ["cs.AR", "cs.CV", "eess.IV"], "comment": null, "summary": "Artificial intelligence necessitates adaptable hardware accelerators for\nefficient high-throughput million operations. We present pipelined architecture\nwith CORDIC block for linear MAC computations and nonlinear iterative\nActivation Functions (AF) such as $tanh$, $sigmoid$, and $softmax$. This\napproach focuses on a Reconfigurable Processing Engine (RPE) based systolic\narray, with 40\\% pruning rate, enhanced throughput up to 4.64$\\times$, and\nreduction in power and area by 5.02 $\\times$ and 4.06 $\\times$ at CMOS 28 nm,\nwith minor accuracy loss. FPGA implementation achieves a reduction of up to 2.5\n$\\times$ resource savings and 3 $\\times$ power compared to prior works. The\nSystolic CORDIC engine for Reconfigurability and Enhanced throughput (SYCore)\ndeploys an output stationary dataflow with the CAESAR control engine for\ndiverse AI workloads such as Transformers, RNNs/LSTMs, and DNNs for\napplications like image detection, LLMs, and speech recognition. The\nenergy-efficient and flexible approach extends the enhanced approach for edge\nAI accelerators supporting emerging workloads."}
{"id": "2503.11692", "pdf": "https://arxiv.org/pdf/2503.11692", "abs": "https://arxiv.org/abs/2503.11692", "authors": ["Rashik Shrestha", "Madhav Rijal", "Trevor Smith", "Yu Gu"], "title": "FloPE: Flower Pose Estimation for Precision Pollination", "categories": ["cs.RO", "cs.CV"], "comment": "IROS2025 under review", "summary": "This study presents Flower Pose Estimation (FloPE), a real-time flower pose\nestimation framework for computationally constrained robotic pollination\nsystems. Robotic pollination has been proposed to supplement natural\npollination to ensure global food security due to the decreased population of\nnatural pollinators. However, flower pose estimation for pollination is\nchallenging due to natural variability, flower clusters, and high accuracy\ndemands due to the flowers' fragility when pollinating. This method leverages\n3D Gaussian Splatting to generate photorealistic synthetic datasets with\nprecise pose annotations, enabling effective knowledge distillation from a\nhigh-capacity teacher model to a lightweight student model for efficient\ninference. The approach was evaluated on both single and multi-arm robotic\nplatforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees\nwithin a low computational cost. Our experiments validate the effectiveness of\nFloPE, achieving up to 78.75% pollination success rate and outperforming prior\nrobotic pollination techniques."}
{"id": "2503.11846", "pdf": "https://arxiv.org/pdf/2503.11846", "abs": "https://arxiv.org/abs/2503.11846", "authors": ["Alexander Weers", "Alexander H. Berger", "Laurin Lux", "Peter Sch√ºffler", "Daniel Rueckert", "Johannes C. Paetzold"], "title": "From Pixels to Histopathology: A Graph-Based Framework for Interpretable Whole Slide Image Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": "11 pages, 2 figures", "summary": "The histopathological classification of whole-slide images (WSIs) is a\nfundamental task in digital pathology; yet it requires extensive time and\nexpertise from specialists. While deep learning methods show promising results,\nthey typically process WSIs by dividing them into artificial patches, which\ninherently prevents a network from learning from the entire image context,\ndisregards natural tissue structures and compromises interpretability. Our\nmethod overcomes this limitation through a novel graph-based framework that\nconstructs WSI graph representations. The WSI-graph efficiently captures\nessential histopathological information in a compact form. We build tissue\nrepresentations (nodes) that follow biological boundaries rather than arbitrary\npatches all while providing interpretable features for explainability. Through\nadaptive graph coarsening guided by learned embeddings, we progressively merge\nregions while maintaining discriminative local features and enabling efficient\nglobal information exchange. In our method's final step, we solve the\ndiagnostic task through a graph attention network. We empirically demonstrate\nstrong performance on multiple challenging tasks such as cancer stage\nclassification and survival prediction, while also identifying predictive\nfactors using Integrated Gradients. Our implementation is publicly available at\nhttps://github.com/HistoGraph31/pix2pathology"}
{"id": "2503.11851", "pdf": "https://arxiv.org/pdf/2503.11851", "abs": "https://arxiv.org/abs/2503.11851", "authors": ["Jutika Borah", "Hidam Kumarjit Singh"], "title": "DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "18 pages, 8 figures, 5 tables", "summary": "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis."}
{"id": "2503.11954", "pdf": "https://arxiv.org/pdf/2503.11954", "abs": "https://arxiv.org/abs/2503.11954", "authors": ["Ahcen Aliouat", "Elsa Dupraz"], "title": "Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.IT", "cs.LG", "math.IT", "94A29, 94A08, 94B05, 68T01, 68P30", "I.4.2; E.4; I.2.10; I.5.4; I.5.1; I.4.1"], "comment": "11 pages, 13 figures, Submitted to IEEE Transactions on\n  Communications (Under Review)", "summary": "In the emerging field of goal-oriented communications, the focus has shifted\nfrom reconstructing data to directly performing specific learning tasks, such\nas classification, segmentation, or pattern recognition, on the received coded\ndata. In the commonly studied scenario of classification from compressed\nimages, a key objective is to enable learning directly on entropy-coded data,\nthereby bypassing the computationally intensive step of data reconstruction.\nConventional entropy-coding methods, such as Huffman and Arithmetic coding, are\neffective for compression but disrupt the data structure, making them less\nsuitable for direct learning without decoding. This paper investigates the use\nof low-density parity-check (LDPC) codes -- originally designed for channel\ncoding -- as an alternative entropy-coding approach. It is hypothesized that\nthe structured nature of LDPC codes can be leveraged more effectively by deep\nlearning models for tasks like classification. At the receiver side, gated\nrecurrent unit (GRU) models are trained to perform image classification\ndirectly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST,\nand CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in\nclassification tasks, while requiring significantly smaller learning models.\nFurthermore, the paper analyzes why LDPC codes preserve data structure more\neffectively than traditional entropy-coding techniques and explores the impact\nof key code parameters on classification performance. These results suggest\nthat LDPC-based entropy coding offers an optimal balance between learning\nefficiency and model complexity, eliminating the need for prior decoding."}
{"id": "2503.11978", "pdf": "https://arxiv.org/pdf/2503.11978", "abs": "https://arxiv.org/abs/2503.11978", "authors": ["Eric M. Chen", "Di Liu", "Sizhuo Ma", "Michael Vasilkovsky", "Bing Zhou", "Qiang Gao", "Wenzhou Wang", "Jiahao Luo", "Dimitris N. Metaxas", "Vincent Sitzmann", "Jian Wang"], "title": "Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars", "categories": ["cs.GR", "cs.CV"], "comment": "N/A", "summary": "The increasing popularity of personalized avatar systems, such as Snapchat\nBitmojis and Apple Memojis, highlights the growing demand for digital\nself-representation. Despite their widespread use, existing avatar platforms\nface significant limitations, including restricted expressivity due to\npredefined assets, tedious customization processes, or inefficient rendering\nrequirements. Addressing these shortcomings, we introduce Snapmoji, an avatar\ngeneration system that instantly creates animatable, dual-stylized avatars from\na selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on\nlarge-scale Gaussian models using 3D data from sources such as Objaverse and\nfine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This\nenables Snapmoji to transform a selfie into a primary stylized avatar, like the\nBitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all\nwhile preserving the user's identity and the primary style's integrity. Our\nsystem is capable of producing 3D Gaussian avatars that support dynamic\nanimation, including accurate facial expression transfer. Designed for\nefficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds\nand supports real-time interactions on mobile devices at 30 to 40 frames per\nsecond. Extensive testing confirms that Snapmoji outperforms existing methods\nin versatility and speed, making it a convenient tool for automatic avatar\ncreation in various styles."}
{"id": "2503.11999", "pdf": "https://arxiv.org/pdf/2503.11999", "abs": "https://arxiv.org/abs/2503.11999", "authors": ["Tongxuan Tian", "Haoyang Li", "Bo Ai", "Xiaodi Yuan", "Zhiao Huang", "Hao Su"], "title": "Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Manipulating deformable objects like cloth is challenging due to their\ncomplex dynamics, near-infinite degrees of freedom, and frequent\nself-occlusions, which complicate state estimation and dynamics modeling. Prior\nwork has struggled with robust cloth state estimation, while dynamics models,\nprimarily based on Graph Neural Networks (GNNs), are limited by their locality.\nInspired by recent advances in generative models, we hypothesize that these\nexpressive models can effectively capture intricate cloth configurations and\ndeformation patterns from data. Building on this insight, we propose a\ndiffusion-based generative approach for both perception and dynamics modeling.\nSpecifically, we formulate state estimation as reconstructing the full cloth\nstate from sparse RGB-D observations conditioned on a canonical cloth mesh and\ndynamics modeling as predicting future states given the current state and robot\nactions. Leveraging a transformer-based diffusion model, our method achieves\nhigh-fidelity state reconstruction while reducing long-horizon dynamics\nprediction errors by an order of magnitude compared to GNN-based approaches.\nIntegrated with model-predictive control (MPC), our framework successfully\nexecutes cloth folding on a real robotic system, demonstrating the potential of\ngenerative models for manipulation tasks with partial observability and complex\ndynamics."}
{"id": "2503.12030", "pdf": "https://arxiv.org/pdf/2503.12030", "abs": "https://arxiv.org/abs/2503.12030", "authors": ["Zhenxin Li", "Shihao Wang", "Shiyi Lan", "Zhiding Yu", "Zuxuan Wu", "Jose M. Alvarez"], "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt."}
{"id": "2503.12042", "pdf": "https://arxiv.org/pdf/2503.12042", "abs": "https://arxiv.org/abs/2503.12042", "authors": ["Zhedong Zhang", "Liang Li", "Chenggang Yan", "Chunshan Liu", "Anton van den Hengel", "Yuankai Qi"], "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Movie dubbing describes the process of transforming a script into speech that\naligns temporally and emotionally with a given movie clip while exemplifying\nthe speaker's voice demonstrated in a short reference audio clip. This task\ndemands the model bridge character performances and complicated prosody\nstructures to build a high-quality video-synchronized dubbing track. The\nlimited scale of movie dubbing datasets, along with the background noise\ninherent in audio data, hinder the acoustic modeling performance of trained\nmodels. To address these issues, we propose an acoustic-prosody disentangled\ntwo-stage method to achieve high-quality dubbing generation with precise\nprosody alignment. First, we propose a prosody-enhanced acoustic pre-training\nto develop robust acoustic modeling capabilities. Then, we freeze the\npre-trained acoustic system and design a disentangled framework to model\nprosodic text features and dubbing style while maintaining acoustic quality.\nAdditionally, we incorporate an in-domain emotion analysis module to reduce the\nimpact of visual domain shifts across different movies, thereby enhancing\nemotion-prosody alignment. Extensive experiments show that our method performs\nfavorably against the state-of-the-art models on two primary benchmarks. The\ndemos are available at https://zzdoog.github.io/ProDubber/."}
{"id": "2503.12141", "pdf": "https://arxiv.org/pdf/2503.12141", "abs": "https://arxiv.org/abs/2503.12141", "authors": ["Shayan Rokhva", "Babak Teimourpour", "Romina Babaei"], "title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries."}
{"id": "2503.12170", "pdf": "https://arxiv.org/pdf/2503.12170", "abs": "https://arxiv.org/abs/2503.12170", "authors": ["Tao Wang", "Cong Zhang", "Xingguang Qu", "Kun Li", "Weiwei Liu", "Chang Huang"], "title": "DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 6 figures", "summary": "End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising\napproach toward achieving full autonomy. However, existing E2E-AD systems\ntypically adopt a traditional multi-task framework, addressing perception,\nprediction, and planning tasks through separate task-specific heads. Despite\nbeing trained in a fully differentiable manner, they still encounter issues\nwith task coordination, and the system complexity remains high. In this work,\nwe introduce DiffAD, a novel diffusion probabilistic model that redefines\nautonomous driving as a conditional image generation task. By rasterizing\nheterogeneous targets onto a unified bird's-eye view (BEV) and modeling their\nlatent distribution, DiffAD unifies various driving objectives and jointly\noptimizes all driving tasks in a single framework, significantly reducing\nsystem complexity and harmonizing task coordination. The reverse process\niteratively refines the generated BEV image, resulting in more robust and\nrealistic driving behaviors. Closed-loop evaluations in Carla demonstrate the\nsuperiority of the proposed method, achieving a new state-of-the-art Success\nRate and Driving Score. The code will be made publicly available."}
{"id": "2503.12172", "pdf": "https://arxiv.org/pdf/2503.12172", "abs": "https://arxiv.org/abs/2503.12172", "authors": ["Kasra Arabi", "R. Teal Witter", "Chinmay Hegde", "Niv Cohen"], "title": "SEAL: Semantic Aware Image Watermarking", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels."}
{"id": "2503.12174", "pdf": "https://arxiv.org/pdf/2503.12174", "abs": "https://arxiv.org/abs/2503.12174", "authors": ["Jijiang Li", "Qingyue Deng", "Haibin Ling", "Bingyao Huang"], "title": "DPCS: Path Tracing-Based Differentiable Projector-Camera Systems", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages,16 figures", "summary": "Projector-camera systems (ProCams) simulation aims to model the physical\nproject-and-capture process and associated scene parameters of a ProCams, and\nis crucial for spatial augmented reality (SAR) applications such as ProCams\nrelighting and projector compensation. Recent advances use an end-to-end neural\nnetwork to learn the project-and-capture process. However, these neural\nnetwork-based methods often implicitly encapsulate scene parameters, such as\nsurface material, gamma, and white balance in the network parameters, and are\nless interpretable and hard for novel scene simulation. Moreover, neural\nnetworks usually learn the indirect illumination implicitly in an\nimage-to-image translation way which leads to poor performance in simulating\ncomplex projection effects such as soft-shadow and interreflection. In this\npaper, we introduce a novel path tracing-based differentiable projector-camera\nsystems (DPCS), offering a differentiable ProCams simulation method that\nexplicitly integrates multi-bounce path tracing. Our DPCS models the physical\nproject-and-capture process using differentiable physically-based rendering\n(PBR), enabling the scene parameters to be explicitly decoupled and learned\nusing much fewer samples. Moreover, our physically-based method not only\nenables high-quality downstream ProCams tasks, such as ProCams relighting and\nprojector compensation, but also allows novel scene simulation using the\nlearned scene parameters. In experiments, DPCS demonstrates clear advantages\nover previous approaches in ProCams simulation, offering better\ninterpretability, more efficient handling of complex interreflection and\nshadow, and requiring fewer training samples."}
{"id": "2503.12180", "pdf": "https://arxiv.org/pdf/2503.12180", "abs": "https://arxiv.org/abs/2503.12180", "authors": ["Yuhang Peng", "Sidong Wang", "Jihaoyu Yang", "Shilong Li", "Han Wang", "Jiangtao Gong"], "title": "Bench2FreeAD: A Benchmark for Vision-based End-to-end Navigation in Unstructured Robotic Environments", "categories": ["cs.RO", "cs.CV", "68T45"], "comment": "7 pages, 9 figures", "summary": "Most current end-to-end (E2E) autonomous driving algorithms are built on\nstandard vehicles in structured transportation scenarios, lacking exploration\nof robot navigation for unstructured scenarios such as auxiliary roads, campus\nroads, and indoor settings. This paper investigates E2E robot navigation in\nunstructured road environments. First, we introduce two data collection\npipelines - one for real-world robot data and another for synthetic data\ngenerated using the Isaac Sim simulator, which together produce an unstructured\nrobotics navigation dataset -- FreeWorld Dataset. Second, we fine-tuned an\nefficient E2E autonomous driving model -- VAD -- using our datasets to validate\nthe performance and adaptability of E2E autonomous driving models in these\nenvironments. Results demonstrate that fine-tuning through our datasets\nsignificantly enhances the navigation potential of E2E autonomous driving\nmodels in unstructured robotic environments. Thus, this paper presents the\nfirst dataset targeting E2E robot navigation tasks in unstructured scenarios,\nand provides a benchmark based on vision-based E2E autonomous driving\nalgorithms to facilitate the development of E2E navigation technology for\nlogistics and service robots. The project is available on Github."}
{"id": "2503.12229", "pdf": "https://arxiv.org/pdf/2503.12229", "abs": "https://arxiv.org/abs/2503.12229", "authors": ["William Louis Rothman", "Yasuyuki Matsushita"], "title": "Shadow Art Kanji: Inverse Rendering Application", "categories": ["cs.GR", "cs.CV"], "comment": "7 pages, 10 figures, 8 references", "summary": "Finding a balance between artistic beauty and machine-generated imagery is\nalways a difficult task. This project seeks to create 3D models that, when\nilluminated, cast shadows resembling Kanji characters. It aims to combine\nartistic expression with computational techniques, providing an accurate and\nefficient approach to visualizing these Japanese characters through shadows."}
{"id": "2503.12365", "pdf": "https://arxiv.org/pdf/2503.12365", "abs": "https://arxiv.org/abs/2503.12365", "authors": ["Xiangfei Fang", "Boying Wang", "Chengying Huan", "Shaonan Ma", "Heng Zhang", "Chen Zhao"], "title": "HyperKAN: Hypergraph Representation Learning with Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.CV", "cs.SI"], "comment": "Accepted by ICASSP2025", "summary": "Hypergraph representation learning has garnered increasing attention across\nvarious domains due to its capability to model high-order relationships.\nTraditional methods often rely on hypergraph neural networks (HNNs) employing\nmessage passing mechanisms to aggregate vertex and hyperedge features. However,\nthese methods are constrained by their dependence on hypergraph topology,\nleading to the challenge of imbalanced information aggregation, where\nhigh-degree vertices tend to aggregate redundant features, while low-degree\nvertices often struggle to capture sufficient structural features. To overcome\nthe above challenges, we introduce HyperKAN, a novel framework for hypergraph\nrepresentation learning that transcends the limitations of message-passing\ntechniques. HyperKAN begins by encoding features for each vertex and then\nleverages Kolmogorov-Arnold Networks (KANs) to capture complex nonlinear\nrelationships. By adjusting structural features based on similarity, our\napproach generates refined vertex representations that effectively addresses\nthe challenge of imbalanced information aggregation. Experiments conducted on\nthe real-world datasets demonstrate that HyperKAN significantly outperforms\nstate of-the-art HNN methods, achieving nearly a 9% performance improvement on\nthe Senate dataset."}
{"id": "2503.12466", "pdf": "https://arxiv.org/pdf/2503.12466", "abs": "https://arxiv.org/abs/2503.12466", "authors": ["Jiahang Cao", "Qiang Zhang", "Hanzhong Guo", "Jiaxu Wang", "Hao Cheng", "Renjing Xu"], "title": "Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ICLR 2025 Generative Models for Robot Learning Workshop", "summary": "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps://github.com/AndyCao1125/MCDP."}
{"id": "2503.12505", "pdf": "https://arxiv.org/pdf/2503.12505", "abs": "https://arxiv.org/abs/2503.12505", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Jiaxin Ai", "Wangbo Zhao", "Kai Wang", "Xiaojiang Peng", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs."}
{"id": "2503.12536", "pdf": "https://arxiv.org/pdf/2503.12536", "abs": "https://arxiv.org/abs/2503.12536", "authors": ["Lin-Chun Huang", "Ching Chieh Tsao", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model", "categories": ["cs.LG", "cs.CV", "cs.CY"], "comment": null, "summary": "Image generative models, particularly diffusion-based models, have surged in\npopularity due to their remarkable ability to synthesize highly realistic\nimages. However, since these models are data-driven, they inherit biases from\nthe training datasets, frequently leading to disproportionate group\nrepresentations that exacerbate societal inequities. Traditionally, efforts to\ndebiase these models have relied on predefined sensitive attributes,\nclassifiers trained on such attributes, or large language models to steer\noutputs toward fairness. However, these approaches face notable drawbacks:\npredefined attributes do not adequately capture complex and continuous\nvariations among groups. To address these issues, we introduce the Debiasing\nDiffusion Model (DDM), which leverages an indicator to learn latent\nrepresentations during training, promoting fairness through balanced\nrepresentations without requiring predefined sensitive attributes. This\napproach not only demonstrates its effectiveness in scenarios previously\naddressed by conventional techniques but also enhances fairness without relying\non predefined sensitive attributes as conditions. In this paper, we discuss the\nlimitations of prior bias mitigation techniques in diffusion-based models,\nelaborate on the architecture of the DDM, and validate the effectiveness of our\napproach through experiments."}
{"id": "2503.12549", "pdf": "https://arxiv.org/pdf/2503.12549", "abs": "https://arxiv.org/abs/2503.12549", "authors": ["Alexander Koebler", "Ralf Gross", "Florian Buettner", "Ingo Thon"], "title": "Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Published at ECML PKDD 2022", "summary": "Flexible industrial production systems will play a central role in the future\nof manufacturing due to higher product individualization and customization. A\nkey component in such systems is the robotic grasping of known or unknown\nobjects in random positions. Real-world applications often come with challenges\nthat might not be considered in grasping solutions tested in simulation or lab\nsettings. Partial occlusion of the target object is the most prominent.\nExamples of occlusion can be supporting structures in the camera's field of\nview, sensor imprecision, or parts occluding each other due to the production\nprocess. In all these cases, the resulting lack of information leads to\nshortcomings in calculating grasping points. In this paper, we present an\nalgorithm to reconstruct the missing information. Our inpainting solution\nfacilitates the real-world utilization of robust object matching approaches for\ngrasping point calculation. We demonstrate the benefit of our solution by\nenabling an existing grasping system embedded in a real-world industrial\napplication to handle occlusions in the input. With our solution, we\ndrastically decrease the number of objects discarded by the process."}
{"id": "2503.12553", "pdf": "https://arxiv.org/pdf/2503.12553", "abs": "https://arxiv.org/abs/2503.12553", "authors": ["Xianzu Wu", "Zhenxin Ai", "Harry Yang", "Ser-Nam Lim", "Jun Liu", "Huan Wang"], "title": "Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in single-view 3D scene reconstruction have highlighted the\nchallenges in capturing fine geometric details and ensuring structural\nconsistency, particularly in high-fidelity outdoor scene modeling. This paper\npresents Niagara, a new single-view 3D scene reconstruction framework that can\nfaithfully reconstruct challenging outdoor scenes from a single input image for\nthe first time.\n  Our approach integrates monocular depth and normal estimation as input, which\nsubstantially improves its ability to capture fine details, mitigating common\nissues like geometric detail loss and deformation.\n  Additionally, we introduce a geometric affine field (GAF) and 3D\nself-attention as geometry-constraint, which combines the structural properties\nof explicit geometry with the adaptability of implicit feature fields, striking\na balance between efficient rendering and high-fidelity reconstruction.\n  Our framework finally proposes a specialized encoder-decoder architecture,\nwhere a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian\nparameters, which can be used for novel view synthesis. Extensive results and\nanalyses suggest that our Niagara surpasses prior SoTA approaches such as\nFlash3D in both single-view and dual-view settings, significantly enhancing the\ngeometric accuracy and visual fidelity, especially in outdoor scenes."}
{"id": "2503.12609", "pdf": "https://arxiv.org/pdf/2503.12609", "abs": "https://arxiv.org/abs/2503.12609", "authors": ["Yitian Shi", "Di Wen", "Guanqi Chen", "Edgar Welte", "Sheng Liu", "Kunyu Peng", "Rainer Stiefelhagen", "Rania Rayyes"], "title": "VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active View Planning and Grasping in Clutter and Invisibility", "categories": ["cs.RO", "cs.CV"], "comment": "Under review", "summary": "We propose VISO-Grasp, a novel vision-language-informed system designed to\nsystematically address visibility constraints for grasping in severely occluded\nenvironments. By leveraging Foundation Models (FMs) for spatial reasoning and\nactive view planning, our framework constructs and updates an instance-centric\nrepresentation of spatial relationships, enhancing grasp success under\nchallenging occlusions. Furthermore, this representation facilitates active\nNext-Best-View (NBV) planning and optimizes sequential grasping strategies when\ndirect grasping is infeasible. Additionally, we introduce a multi-view\nuncertainty-driven grasp fusion mechanism that refines grasp confidence and\ndirectional uncertainty in real-time, ensuring robust and stable grasp\nexecution. Extensive real-world experiments demonstrate that VISO-Grasp\nachieves a success rate of $87.5\\%$ in target-oriented grasping with the fewest\ngrasp attempts outperforming baselines. To the best of our knowledge,\nVISO-Grasp is the first unified framework integrating FMs into target-aware\nactive view planning and 6-DoF grasping in environments with severe occlusions\nand entire invisibility constraints."}
{"id": "2503.12623", "pdf": "https://arxiv.org/pdf/2503.12623", "abs": "https://arxiv.org/abs/2503.12623", "authors": ["Vrushank Ahire", "Kunal Shah", "Mudasir Nazir Khan", "Nikhil Pakhale", "Lownish Rai Sookha", "M. A. Ganaie", "Abhinav Dhall"], "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal\nEmotion Network), a novel architecture for dynamic emotion recognition through\ndimensional modeling of affect. The model uniquely integrates visual, audio,\nand textual modalities via a bi-directional cross-modal attention mechanism\nwith six distinct attention pathways, enabling comprehensive interactions\nbetween all modality pairs. Our proposed approach employs modality-specific\nencoders to extract rich feature representations from synchronized video\nframes, audio segments, and transcripts. The architecture's novelty lies in its\ncross-modal enhancement strategy, where each modality representation is refined\nthrough weighted attention from other modalities, followed by self-attention\nrefinement through modality-specific encoders. Rather than directly predicting\nvalence-arousal values, MAVEN predicts emotions in a polar coordinate form,\naligning with psychological models of the emotion circumplex. Experimental\nevaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our\napproach, with performance measured using Concordance Correlation Coefficient\n(CCC). The multi-stage architecture demonstrates superior ability to capture\nthe complex, nuanced nature of emotional expressions in conversational videos,\nadvancing the state-of-the-art (SOTA) in continuous emotion recognition\nin-the-wild. Code can be found at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW."}
{"id": "2503.12642", "pdf": "https://arxiv.org/pdf/2503.12642", "abs": "https://arxiv.org/abs/2503.12642", "authors": ["Anjali Dharmik"], "title": "COVID 19 Diagnosis Analysis using Transfer Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Coronaviruses transmit COVID-19, a rapidly spreading disease. A Coronavirus\ninfection (COVID-19) was first discovered in December 2019 in Wuhan, China, and\nspread rapidly throughout the planet in exactly some months. because of this,\nthe virus can cause severe symptoms and even death, especially within the\nelderly and in people with medical conditions. The virus causes acute\nrespiratory infections in humans. the primary case was diagnosed in China in\n2019 and the pandemic started in 2020. Since the quantity of cases of COVID-19\nis increasing daily, there are only a limited number of test kits available in\nhospitals. So, to stop COVID-19 from spreading among people, an automatic\ndiagnosis system must be implemented. during this study, three pre-trained\nneural networks supported convolutional neural networks (VGG16, VGG19,\nResNet50) are proposed for detecting Coronavirus pneumonia infected patients\nthrough X-rays and computerized tomography (CT). By using cross-validation,\nwe've got implemented binary classifications with two classes (COVID-19, Normal\n(healthy)). Taking into consideration the results obtained, the pre-trained\nResNet50 model provides the simplest classification performance (97.77%\naccuracy, 100% sensitivity, 93.33% specificity, 98.00% F1-score) among the\nopposite three used models over 6259 images."}
{"id": "2503.12698", "pdf": "https://arxiv.org/pdf/2503.12698", "abs": "https://arxiv.org/abs/2503.12698", "authors": ["Dazhou Guo", "Zhanghexuan Ji", "Yanzhou Su", "Dandan Zheng", "Heng Guo", "Puyang Wang", "Ke Yan", "Yirui Wang", "Qinji Yu", "Zi Li", "Minfeng Xu", "Jianfeng Zhang", "Haoshen Li", "Jia Ge", "Tsung-Ying Ho", "Bing-Shen Huang", "Tashan Ai", "Kuaile Zhao", "Na Shen", "Qifeng Wang", "Yun Bian", "Tingyu Wu", "Peng Du", "Hua Zhang", "Feng-Ming Kong", "Alan L. Yuille", "Cher Heng Tan", "Chunyan Miao", "Perry J. Pickhardt", "Senxiang Yan", "Ronald M. Summers", "Le Lu", "Dakai Jin", "Xianghua Ye"], "title": "A Continual Learning-driven Model for Accurate and Generalizable Segmentation of Clinically Comprehensive and Fine-grained Whole-body Anatomies in CT", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Precision medicine in the quantitative management of chronic diseases and\noncology would be greatly improved if the Computed Tomography (CT) scan of any\npatient could be segmented, parsed and analyzed in a precise and detailed way.\nHowever, there is no such fully annotated CT dataset with all anatomies\ndelineated for training because of the exceptionally high manual cost, the need\nfor specialized clinical expertise, and the time required to finish the task.\nTo this end, we proposed a novel continual learning-driven CT model that can\nsegment complete anatomies presented using dozens of previously partially\nlabeled datasets, dynamically expanding its capacity to segment new ones\nwithout compromising previously learned organ knowledge. Existing multi-dataset\napproaches are not able to dynamically segment new anatomies without\ncatastrophic forgetting and would encounter optimization difficulty or\ninfeasibility when segmenting hundreds of anatomies across the whole range of\nbody regions. Our single unified CT segmentation model, CL-Net, can highly\naccurately segment a clinically comprehensive set of 235 fine-grained\nwhole-body anatomies. Composed of a universal encoder, multiple optimized and\npruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and\n16 private high-quality partially labeled CT datasets of various vendors,\ndifferent contrast phases, and pathologies. Extensive evaluation demonstrates\nthat CL-Net consistently outperforms the upper limit of an ensemble of 36\nspecialist nnUNets trained per dataset with the complexity of 5% model size and\nsignificantly surpasses the segmentation accuracy of recent leading Segment\nAnything-style medical image foundation models by large margins. Our continual\nlearning-driven CL-Net model would lay a solid foundation to facilitate many\ndownstream tasks of oncology and chronic diseases using the most widely adopted\nCT imaging."}
{"id": "2503.12768", "pdf": "https://arxiv.org/pdf/2503.12768", "abs": "https://arxiv.org/abs/2503.12768", "authors": ["Tatsuro Sakai", "Kanji Tanaka", "Jonathan Tay Yu Liang", "Muhammad Adil Luqman", "Daiki Iwata"], "title": "Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 4 figures, technical report", "summary": "In robot vision, thermal cameras have significant potential for recognizing\nhumans even in complete darkness. However, their application to multi-person\ntracking (MPT) has lagged due to data scarcity and difficulties in individual\nidentification. In this study, we propose a cooperative MPT system that\nutilizes co-located RGB and thermal cameras, using pseudo-annotations (bounding\nboxes + person IDs) to train RGB and T trackers. Evaluation experiments\ndemonstrate that the T tracker achieves remarkable performance in both bright\nand dark scenes. Furthermore, results suggest that a tracker-switching approach\nusing a binary brightness classifier is more suitable than a tracker-fusion\napproach for information integration. This study marks a crucial first step\ntoward ``Dynamic-Dark SLAM,\" enabling effective recognition, understanding, and\nreconstruction of individuals, occluding objects, and traversable areas in\ndynamic environments, both bright and dark."}
{"id": "2503.12793", "pdf": "https://arxiv.org/pdf/2503.12793", "abs": "https://arxiv.org/abs/2503.12793", "authors": ["Yechao Zhang", "Yingzhe Xu", "Junyu Shi", "Leo Yu Zhang", "Shengshan Hu", "Minghui Li", "Yanjun Zhang"], "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted in AAAI 2025", "summary": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%."}
{"id": "2503.12806", "pdf": "https://arxiv.org/pdf/2503.12806", "abs": "https://arxiv.org/abs/2503.12806", "authors": ["Hadam Baek", "Hannie Shin", "Jiyoung Seo", "Chanwoo Kim", "Saerom Kim", "Hyeongbok Kim", "Sangpil Kim"], "title": "AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Accurately modeling sound propagation with complex real-world environments is\nessential for Novel View Acoustic Synthesis (NVAS). While previous studies have\nleveraged visual perception to estimate spatial acoustics, the combined use of\nsurface normal and structural details from 3D representations in acoustic\nmodeling has been underexplored. Given their direct impact on sound wave\nreflections and propagation, surface normals should be jointly modeled with\nstructural details to achieve accurate spatial acoustics. In this paper, we\npropose a surface-enhanced geometry-aware approach for NVAS to improve spatial\nacoustic modeling. To achieve this, we exploit geometric priors, such as image,\ndepth map, surface normals, and point clouds obtained using a 3D Gaussian\nSplatting (3DGS) based framework. We introduce a dual cross-attention-based\ntransformer integrating geometrical constraints into frequency query to\nunderstand the surroundings of the emitter. Additionally, we design a\nConvNeXt-based spectral features processing network called Spectral Refinement\nNetwork (SRN) to synthesize realistic binaural audio. Experimental results on\nthe RWAVS and SoundSpace datasets highlight the necessity of our approach, as\nit surpasses existing methods in novel view acoustic synthesis."}
{"id": "2503.12840", "pdf": "https://arxiv.org/pdf/2503.12840", "abs": "https://arxiv.org/abs/2503.12840", "authors": ["Chen Liu", "Liying Yang", "Peike Li", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Sound-guided object segmentation has drawn considerable attention for its\npotential to enhance multimodal perception. Previous methods primarily focus on\ndeveloping advanced architectures to facilitate effective audio-visual\ninteractions, without fully addressing the inherent challenges posed by audio\nnatures, \\emph{\\ie}, (1) feature confusion due to the overlapping nature of\naudio signals, and (2) audio-visual matching difficulty from the varied sounds\nproduced by the same object. To address these challenges, we propose Dynamic\nDerivation and Elimination (DDESeg): a novel audio-visual segmentation\nframework. Specifically, to mitigate feature confusion, DDESeg reconstructs the\nsemantic content of the mixed audio signal by enriching the distinct semantic\ninformation of each individual source, deriving representations that preserve\nthe unique characteristics of each sound. To reduce the matching difficulty, we\nintroduce a discriminative feature learning module, which enhances the semantic\ndistinctiveness of generated audio representations. Considering that not all\nderived audio representations directly correspond to visual features (e.g.,\noff-screen sounds), we propose a dynamic elimination module to filter out\nnon-matching elements. This module facilitates targeted interaction between\nsounding regions and relevant audio semantics. By scoring the interacted\nfeatures, we identify and filter out irrelevant audio information, ensuring\naccurate audio-visual alignment. Comprehensive experiments demonstrate that our\nframework achieves superior performance in AVS datasets."}
{"id": "2503.12847", "pdf": "https://arxiv.org/pdf/2503.12847", "abs": "https://arxiv.org/abs/2503.12847", "authors": ["Chen Liu", "Peike Li", "Liying Yang", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation."}
{"id": "2503.12926", "pdf": "https://arxiv.org/pdf/2503.12926", "abs": "https://arxiv.org/abs/2503.12926", "authors": ["Cheng Yuan", "Zhening Liu", "Jiashu Lv", "Jiawei Shao", "Yufei Jiang", "Jun Zhang", "Xuelong Li"], "title": "Task-Oriented Feature Compression for Multimodal Understanding via Device-Edge Co-Inference", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "With the rapid development of large multimodal models (LMMs), multimodal\nunderstanding applications are emerging. As most LMM inference requests\noriginate from edge devices with limited computational capabilities, the\npredominant inference pipeline involves directly forwarding the input data to\nan edge server which handles all computations. However, this approach\nintroduces high transmission latency due to limited uplink bandwidth of edge\ndevices and significant computation latency caused by the prohibitive number of\nvisual tokens, thus hindering delay-sensitive tasks and degrading user\nexperience. To address this challenge, we propose a task-oriented feature\ncompression (TOFC) method for multimodal understanding in a device-edge\nco-inference framework, where visual features are merged by clustering and\nencoded by a learnable and selective entropy model before feature projection.\nSpecifically, we employ density peaks clustering based on K nearest neighbors\nto reduce the number of visual features, thereby minimizing both data\ntransmission and computational complexity. Subsequently, a learnable entropy\nmodel with hyperprior is utilized to encode and decode merged features, further\nreducing transmission overhead. To enhance compression efficiency, multiple\nentropy models are adaptively selected based on the characteristics of the\nvisual features, enabling a more accurate estimation of the probability\ndistribution. Comprehensive experiments on seven visual question answering\nbenchmarks validate the effectiveness of the proposed TOFC method. Results show\nthat TOFC achieves up to 60% reduction in data transmission overhead and 50%\nreduction in system latency while maintaining identical task performance,\ncompared with traditional image compression methods."}
{"id": "2503.12937", "pdf": "https://arxiv.org/pdf/2503.12937", "abs": "https://arxiv.org/abs/2503.12937", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Huanjin Yao", "Shunyu Liu", "Xikun Zhang", "Shijian Lu", "Dacheng Tao"], "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods."}
{"id": "2503.12990", "pdf": "https://arxiv.org/pdf/2503.12990", "abs": "https://arxiv.org/abs/2503.12990", "authors": ["Roba Al Majzoub", "Hashmat Malik", "Muzammal Naseer", "Zaigham Zaheer", "Tariq Mahmood", "Salman Khan", "Fahad Khan"], "title": "How Good is my Histopathology Vision-Language Foundation Model? A Holistic Benchmark", "categories": ["eess.IV", "cs.CV", "I.4.0; J.3"], "comment": null, "summary": "Recently, histopathology vision-language foundation models (VLMs) have gained\npopularity due to their enhanced performance and generalizability across\ndifferent downstream tasks. However, most existing histopathology benchmarks\nare either unimodal or limited in terms of diversity of clinical tasks, organs,\nand acquisition instruments, as well as their partial availability to the\npublic due to patient data privacy. As a consequence, there is a lack of\ncomprehensive evaluation of existing histopathology VLMs on a unified benchmark\nsetting that better reflects a wide range of clinical scenarios. To address\nthis gap, we introduce HistoVL, a fully open-source comprehensive benchmark\ncomprising images acquired using up to 11 various acquisition tools that are\npaired with specifically crafted captions by incorporating class names and\ndiverse pathology descriptions. Our Histo-VL includes 26 organs, 31 cancer\ntypes, and a wide variety of tissue obtained from 14 heterogeneous patient\ncohorts, totaling more than 5 million patches obtained from over 41K WSIs\nviewed under various magnification levels. We systematically evaluate existing\nhistopathology VLMs on Histo-VL to simulate diverse tasks performed by experts\nin real-world clinical scenarios. Our analysis reveals interesting findings,\nincluding large sensitivity of most existing histopathology VLMs to textual\nchanges with a drop in balanced accuracy of up to 25% in tasks such as\nMetastasis detection, low robustness to adversarial attacks, as well as\nimproper calibration of models evident through high ECE values and low model\nprediction confidence, all of which can affect their clinical implementation."}
{"id": "2503.13008", "pdf": "https://arxiv.org/pdf/2503.13008", "abs": "https://arxiv.org/abs/2503.13008", "authors": ["David E. Hernandez", "Jose Ramon Chang", "Torbj√∂rn E. M. Nordling"], "title": "Knowledge Distillation: Enhancing Neural Network Compression with Integrated Gradients", "categories": ["cs.LG", "cs.CV", "68T05, 68T07", "I.2.6; I.4.2; I.4.9"], "comment": "15 pages, 3 figures, conference", "summary": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount."}
{"id": "2503.13021", "pdf": "https://arxiv.org/pdf/2503.13021", "abs": "https://arxiv.org/abs/2503.13021", "authors": ["Omri Suissa", "Muhiim Ali", "Ariana Azarbal", "Hui Shen", "Shekhar Pradhan"], "title": "Dynamic Relation Inference via Verb Embeddings", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data."}
{"id": "2503.13051", "pdf": "https://arxiv.org/pdf/2503.13051", "abs": "https://arxiv.org/abs/2503.13051", "authors": ["Kai Uwe Barthel", "Florian Barthel", "Peter Eisert"], "title": "Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical."}
{"id": "2503.13057", "pdf": "https://arxiv.org/pdf/2503.13057", "abs": "https://arxiv.org/abs/2503.13057", "authors": ["Robin Zbinden", "Nina van Tiel", "Gencer Sumbul", "Chiara Vanalli", "Benjamin Kellenberger", "Devis Tuia"], "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications."}
{"id": "2503.13080", "pdf": "https://arxiv.org/pdf/2503.13080", "abs": "https://arxiv.org/abs/2503.13080", "authors": ["Hubert Szolc", "Mateusz Wasala", "Remigiusz Mietla", "Kacper Iwicki", "Tomasz Kryjak"], "title": "Vision-based automatic fruit counting with UAV", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Accepted for the 29th Conference on Automation - Innovations and\n  Future Perspectives Automation 2025, May 7 - 9, 2025, Warsaw, Poland", "summary": "The use of unmanned aerial vehicles (UAVs) for smart agriculture is becoming\nincreasingly popular. This is evidenced by recent scientific works, as well as\nthe various competitions organised on this topic. Therefore, in this work we\npresent a system for automatic fruit counting using UAVs. To detect them, our\nsolution uses a vision algorithm that processes streams from an RGB camera and\na depth sensor using classical image operations. Our system also allows the\nplanning and execution of flight trajectories, taking into account the\nminimisation of flight time and distance covered. We tested the proposed\nsolution in simulation and obtained an average score of 87.27/100 points from a\ntotal of 500 missions. We also submitted it to the UAV Competition organised as\npart of the ICUAS 2024 conference, where we achieved an average score of\n84.83/100 points, placing 6th in a field of 23 teams and advancing to the\nfinals."}
{"id": "2503.13082", "pdf": "https://arxiv.org/pdf/2503.13082", "abs": "https://arxiv.org/abs/2503.13082", "authors": ["Runyu Jiao", "Alice Fasoli", "Francesco Giuliari", "Matteo Bortolon", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Free-form language-based robotic reasoning and grasping", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://tev-fbk.github.io/FreeGrasp/", "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/."}
{"id": "2503.13090", "pdf": "https://arxiv.org/pdf/2503.13090", "abs": "https://arxiv.org/abs/2503.13090", "authors": ["V√°clav Truhla≈ô√≠k", "Tom√°≈° Pivo≈àka", "Michal Kasarda", "Libor P≈ôeuƒçil"], "title": "Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 5 figures", "summary": "Uniform and variable environments still remain a challenge for stable visual\nlocalization and mapping in mobile robot navigation. One of the possible\napproaches suitable for such environments is appearance-based teach-and-repeat\nnavigation, relying on simplified localization and reactive robot motion\ncontrol - all without a need for standard mapping. This work brings an\ninnovative solution to such a system based on visual place recognition\ntechniques. Here, the major contributions stand in the employment of a new\nvisual place recognition technique, a novel horizontal shift computation\napproach, and a multi-platform system design for applications across various\ntypes of mobile robots. Secondly, a new public dataset for experimental testing\nof appearance-based navigation methods is introduced. Moreover, the work also\nprovides real-world experimental testing and performance comparison of the\nintroduced navigation system against other state-of-the-art methods. The\nresults confirm that the new system outperforms existing methods in several\ntesting scenarios, is capable of operation indoors and outdoors, and exhibits\nrobustness to day and night scene variations."}
{"id": "2503.13205", "pdf": "https://arxiv.org/pdf/2503.13205", "abs": "https://arxiv.org/abs/2503.13205", "authors": ["Zhen Chen", "Zhihao Peng", "Xusheng Liang", "Cheng Wang", "Peigan Liang", "Linsheng Zeng", "Minjie Ju", "Yixuan Yuan"], "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MA"], "comment": null, "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems."}
{"id": "2503.13217", "pdf": "https://arxiv.org/pdf/2503.13217", "abs": "https://arxiv.org/abs/2503.13217", "authors": ["Yue Su", "Xinyu Zhan", "Hongjie Fang", "Han Xue", "Hao-Shu Fang", "Yong-Lu Li", "Cewu Lu", "Lixin Yang"], "title": "Dense Policy: Bidirectional Autoregressive Learning of Actions", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/."}
{"id": "2503.13227", "pdf": "https://arxiv.org/pdf/2503.13227", "abs": "https://arxiv.org/abs/2503.13227", "authors": ["Yijie Liu", "Xinyi Shang", "Yiqun Zhang", "Yang Lu", "Chen Gong", "Jing-Hao Xue", "Hanzi Wang"], "title": "Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data\nacross clients with limited labeled data to train a global model with strong\ngeneralization ability. Most FSSL methods rely on consistency regularization\nwith pseudo-labels, converting predictions from local or global models into\nhard pseudo-labels as supervisory signals. However, we discover that the\nquality of pseudo-label is largely deteriorated by data heterogeneity, an\nintrinsic facet of federated learning. In this paper, we study the problem of\nFSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label\nmismatches, further degrading model performance and convergence, and (2) local\nand global models' predictive tendencies diverge as heterogeneity increases.\nMotivated by these findings, we propose a simple and effective method called\nSemi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can\nflexibly correct pseudo-labels based on confidence discrepancies. This strategy\neffectively mitigates performance degradation caused by incorrect pseudo-labels\nand enhances consensus between local and global models. Experimental results\ndemonstrate that SAGE outperforms existing FSSL methods in both performance and\nconvergence. Our code is available at https://github.com/Jay-Codeman/SAGE"}
{"id": "2503.13236", "pdf": "https://arxiv.org/pdf/2503.13236", "abs": "https://arxiv.org/abs/2503.13236", "authors": ["Ihab Asaad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Gradient Extrapolation for Debiased Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine learning classification models trained with empirical risk\nminimization (ERM) often inadvertently rely on spurious correlations. When\nabsent in the test data, these unintended associations between non-target\nattributes and target labels lead to poor generalization. This paper addresses\nthis problem from a model optimization perspective and proposes a novel method,\nGradient Extrapolation for Debiased Representation Learning (GERNE), designed\nto learn debiased representations in both known and unknown attribute training\ncases. GERNE uses two distinct batches with different amounts of spurious\ncorrelations to define the target gradient as the linear extrapolation of two\ngradients computed from each batch's loss. It is demonstrated that the\nextrapolated gradient, if directed toward the gradient of the batch with fewer\namount of spurious correlation, can guide the training process toward learning\na debiased model. GERNE can serve as a general framework for debiasing with\nmethods, such as ERM, reweighting, and resampling, being shown as special\ncases. The theoretical upper and lower bounds of the extrapolation factor are\nderived to ensure convergence. By adjusting this factor, GERNE can be adapted\nto maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The\nproposed approach is validated on five vision and one NLP benchmarks,\ndemonstrating competitive and often superior performance compared to\nstate-of-the-art baseline methods."}
{"id": "2503.13277", "pdf": "https://arxiv.org/pdf/2503.13277", "abs": "https://arxiv.org/abs/2503.13277", "authors": ["Alfred Simbun", "Suresh Kumar"], "title": "Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "27 pages, 6 figures, 10 tables", "summary": "Background: The COVID-19 pandemic has overwhelmed healthcare systems,\nemphasizing the need for AI-driven tools to assist in rapid and accurate\npatient prognosis. Chest X-ray imaging is a widely available diagnostic tool,\nbut existing methods for prognosis classification lack scalability and\nefficiency. Objective: This study presents a high-accuracy deep learning model\nfor classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest\nX-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a\ndataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained\nand validated a deep learning model leveraging Convolutional Neural Networks\n(CNNs). The model was evaluated on an unseen dataset to measure accuracy,\nprecision, and recall. Results: Our model achieved an average accuracy of 97%,\nwith specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When\nclassifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),\n95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's\npotential for real-world clinical applications, aiding in faster\ndecision-making and improved resource allocation. Conclusion: AI-driven\nprognosis classification using deep learning can significantly enhance COVID-19\npatient management, enabling early intervention and efficient triaging. Our\nstudy provides a scalable, high-accuracy AI framework for integrating deep\nlearning into routine clinical workflows. Future work should focus on expanding\ndatasets, external validation, and regulatory compliance to facilitate clinical\nadoption."}
{"id": "2503.13309", "pdf": "https://arxiv.org/pdf/2503.13309", "abs": "https://arxiv.org/abs/2503.13309", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Milad Amir Toutounchian", "Arash Mohammadi"], "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available."}
{"id": "2503.13330", "pdf": "https://arxiv.org/pdf/2503.13330", "abs": "https://arxiv.org/abs/2503.13330", "authors": ["Ricardo Bigolin Lanfredi", "Yan Zhuang", "Mark Finkelstein", "Praveen Thoppey Srinivasan Balamuralikrishna", "Luke Krembs", "Brandon Khoury", "Arthi Reddy", "Pritam Mukherjee", "Neil M. Rofsky", "Ronald M. Summers"], "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes."}
{"id": "2503.13369", "pdf": "https://arxiv.org/pdf/2503.13369", "abs": "https://arxiv.org/abs/2503.13369", "authors": ["Wan Ju Kang", "Eunki Kim", "Na Min An", "Sangryul Kim", "Haemin Choi", "Ki Hoon Kwak", "James Thorne"], "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "37 pages, 10 figures, 21 tables", "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks."}
{"id": "2503.13400", "pdf": "https://arxiv.org/pdf/2503.13400", "abs": "https://arxiv.org/abs/2503.13400", "authors": ["Qi Zhang", "Xiuyuan Chen", "Ziyi He", "Kun Wang", "Lianming Wu", "Hongxing Shen", "Jianqi Sun"], "title": "U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "T2 hyperintensities in spinal cord MR images are crucial biomarkers for\nconditions such as degenerative cervical myelopathy. However, current clinical\ndiagnoses primarily rely on manual evaluation. Deep learning methods have shown\npromise in lesion detection, but most supervised approaches are heavily\ndependent on large, annotated datasets. Unsupervised anomaly detection (UAD)\noffers a compelling alternative by eliminating the need for abnormal data\nannotations. However, existing UAD methods rely on curated normal datasets and\ntheir performance frequently deteriorates when applied to clinical datasets due\nto domain shifts. We propose an Uncertainty-based Unsupervised Anomaly\nDetection framework, termed U2AD, to address these limitations. Unlike\ntraditional methods, U2AD is designed to be trained and tested within the same\nclinical dataset, following a \"mask-and-reconstruction\" paradigm built on a\nVision Transformer-based architecture. We introduce an uncertainty-guided\nmasking strategy to resolve task conflicts between normal reconstruction and\nanomaly detection to achieve an optimal balance. Specifically, we employ a\nMonte-Carlo sampling technique to estimate reconstruction uncertainty mappings\nduring training. By iteratively optimizing reconstruction training under the\nguidance of both epistemic and aleatoric uncertainty, U2AD reduces overall\nreconstruction variance while emphasizing regions. Experimental results\ndemonstrate that U2AD outperforms existing supervised and unsupervised methods\nin patient-level identification and segment-level localization tasks. This\nframework establishes a new benchmark for incorporating uncertainty guidance\ninto UAD, highlighting its clinical utility in addressing domain shifts and\ntask conflicts in medical image anomaly detection. Our code is available:\nhttps://github.com/zhibaishouheilab/U2AD"}
{"id": "2503.13441", "pdf": "https://arxiv.org/pdf/2503.13441", "abs": "https://arxiv.org/abs/2503.13441", "authors": ["Ri-Zhao Qiu", "Shiqi Yang", "Xuxin Cheng", "Chaitanya Chawla", "Jialong Li", "Tairan He", "Ge Yan", "Lars Paulsen", "Ge Yang", "Sha Yi", "Guanya Shi", "Xiaolong Wang"], "title": "Humanoid Policy ~ Human Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Code and data: https://human-as-robot.github.io/", "summary": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/"}
{"id": "2503.13446", "pdf": "https://arxiv.org/pdf/2503.13446", "abs": "https://arxiv.org/abs/2503.13446", "authors": ["Zhenyu Wu", "Yuheng Zhou", "Xiuwei Xu", "Ziwei Wang", "Haibin Yan"], "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://gary3410.github.io/momanipVLA/", "summary": "Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models."}
