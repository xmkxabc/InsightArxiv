{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport."}
{"id": "2505.21946", "pdf": "https://arxiv.org/pdf/2505.21946", "abs": "https://arxiv.org/abs/2505.21946", "authors": ["Sinan Wang", "Junwei Zhou", "Fan Feng", "Zhiqi Li", "Yuchen Sun", "Duowen Chen", "Greg Turk", "Bo Zhu"], "title": "Fluid Simulation on Vortex Particle Flow Maps", "categories": ["cs.GR", "physics.flu-dyn"], "comment": "ACM Transactions on Graphics (SIGGRAPH 2025), 24 pages", "summary": "We propose the Vortex Particle Flow Map (VPFM) method to simulate\nincompressible flow with complex vortical evolution in the presence of dynamic\nsolid boundaries. The core insight of our approach is that vorticity is an\nideal quantity for evolution on particle flow maps, enabling significantly\nlonger flow map distances compared to other fluid quantities like velocity or\nimpulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian\nrepresentation that evolves vorticity and flow map quantities on vortex\nparticles, while reconstructing velocity on a background grid. The method\nintegrates three key components: (1) a vorticity-based particle flow map\nframework, (2) an accurate Hessian evolution scheme on particles, and (3) a\nsolid boundary treatment for no-through and no-slip conditions in VPFM. These\ncomponents collectively allow a substantially longer flow map length (3-12\ntimes longer) than the state-of-the-art, enhancing vorticity preservation over\nextended spatiotemporal domains. We validated the performance of VPFM through\ndiverse simulations, demonstrating its effectiveness in capturing complex\nvortex dynamics and turbulence phenomena."}
{"id": "2505.22400", "pdf": "https://arxiv.org/pdf/2505.22400", "abs": "https://arxiv.org/abs/2505.22400", "authors": ["Zehao Li", "Hao Jiang", "Yujun Cai", "Jianing Chen", "Baolong Bi", "Shuqin Gao", "Honglong Zhao", "Yiwei Wang", "Tianlu Mao", "Zhaoqi Wang"], "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although dynamic scene reconstruction has long been a fundamental challenge\nin 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a\npromising direction by enabling high-quality, real-time rendering through\nexplicit Gaussian primitives. However, existing 3DGS-based methods for dynamic\nreconstruction often suffer from \\textit{spatio-temporal incoherence} during\ninitialization, where canonical Gaussians are constructed by aggregating\nobservations from multiple frames without temporal distinction. This results in\nspatio-temporally entangled representations, making it difficult to model\ndynamic motion accurately. To overcome this limitation, we propose\n\\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a\nplug-and-play module that learns spatio-temporal probability distributions for\neach Gaussian. STDR introduces a spatio-temporal mask, a separated deformation\nfield, and a consistency regularization to jointly disentangle spatial and\ntemporal patterns. Extensive experiments demonstrate that incorporating our\nmodule into existing 3DGS-based dynamic scene reconstruction frameworks leads\nto notable improvements in both reconstruction quality and spatio-temporal\nconsistency across synthetic and real-world benchmarks."}
{"id": "2505.22416", "pdf": "https://arxiv.org/pdf/2505.22416", "abs": "https://arxiv.org/abs/2505.22416", "authors": ["Sihun Cha", "Serin Yoon", "Kwanggyoon Seo", "Junyong Noh"], "title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurately retargeting facial expressions to a face mesh while enabling\nmanipulation is a key challenge in facial animation retargeting. Recent\ndeep-learning methods address this by encoding facial expressions into a global\nlatent code, but they often fail to capture fine-grained details in local\nregions. While some methods improve local accuracy by transferring deformations\nlocally, this often complicates overall control of the facial expression. To\naddress this, we propose a method that combines the strengths of both global\nand local deformation models. Our approach enables intuitive control and\ndetailed expression cloning across diverse face meshes, regardless of their\nunderlying structures. The core idea is to localize the influence of the global\nlatent code on the target mesh. Our model learns to predict skinning weights\nfor each vertex of the target face mesh through indirect supervision from\npredefined segmentation labels. These predicted weights localize the global\nlatent code, enabling precise and region-specific deformations even for meshes\nwith unseen shapes. We supervise the latent code using Facial Action Coding\nSystem (FACS)-based blendshapes to ensure interpretability and allow\nstraightforward editing of the generated animation. Through extensive\nexperiments, we demonstrate improved performance over state-of-the-art methods\nin terms of expression fidelity, deformation transfer accuracy, and\nadaptability across diverse mesh structures."}
{"id": "2505.21513", "pdf": "https://arxiv.org/pdf/2505.21513", "abs": "https://arxiv.org/abs/2505.21513", "authors": ["Nicolas Echevarrieta-Catalan", "Ana Ribas-Rodriguez", "Francisco Cedron", "Odelia Schwartz", "Vanessa Aguiar-Pulido"], "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition\n  Conference (CVPR) 2025", "summary": "Machine learning models achieve high precision, but their decision-making\nprocesses often lack explainability. Furthermore, as model complexity\nincreases, explainability typically decreases. Existing efforts to improve\nexplainability primarily involve developing new eXplainable artificial\nintelligence (XAI) techniques or incorporating explainability constraints\nduring training. While these approaches yield specific improvements, their\napplicability remains limited. In this work, we propose the Vision Transformer\nwith artificial Astrocytes (ViTA). This training-free approach is inspired by\nneuroscience and enhances the reasoning of a pretrained deep neural network to\ngenerate more human-aligned explanations. We evaluated our approach employing\ntwo well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a\nstandard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the\nsimilarity between the heatmaps produced by the XAI techniques and a\n(human-aligned) ground truth. Our results consistently demonstrate that\nincorporating artificial astrocytes enhances the alignment of model\nexplanations with human perception, leading to statistically significant\nimprovements across all XAI techniques and metrics utilized."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.22313", "pdf": "https://arxiv.org/pdf/2505.22313", "abs": "https://arxiv.org/abs/2505.22313", "authors": ["Kaixuan Wei", "Hector A. Jimenez-Romero", "Hadi Amata", "Jipeng Sun", "Qiang Fu", "Felix Heide", "Wolfgang Heidrich"], "title": "Large-Area Fabrication-aware Computational Diffractive Optics", "categories": ["physics.optics", "cs.CV", "cs.ET", "cs.GR"], "comment": null, "summary": "Differentiable optics, as an emerging paradigm that jointly optimizes optics\nand (optional) image processing algorithms, has made innovative optical designs\npossible across a broad range of applications. Many of these systems utilize\ndiffractive optical components (DOEs) for holography, PSF engineering, or\nwavefront shaping. Existing approaches have, however, mostly remained limited\nto laboratory prototypes, owing to a large quality gap between simulation and\nmanufactured devices. We aim at lifting the fundamental technical barriers to\nthe practical use of learned diffractive optical systems. To this end, we\npropose a fabrication-aware design pipeline for diffractive optics fabricated\nby direct-write grayscale lithography followed by nano-imprinting replication,\nwhich is directly suited for inexpensive mass production of large area designs.\nWe propose a super-resolved neural lithography model that can accurately\npredict the 3D geometry generated by the fabrication process. This model can be\nseamlessly integrated into existing differentiable optics frameworks, enabling\nfabrication-aware, end-to-end optimization of computational optical systems. To\ntackle the computational challenges, we also devise tensor-parallel compute\nframework centered on distributing large-scale FFT computation across many\nGPUs. As such, we demonstrate large scale diffractive optics designs up to\n32.16 mm $\\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760\nfeature points. We find adequate agreement between simulation and fabricated\nprototypes for applications such as holography and PSF engineering. We also\nachieve high image quality from an imaging system comprised only of a single\nDOE, with images processed only by a Wiener filter utilizing the simulation\nPSF. We believe our findings lift the fabrication limitations for real-world\napplications of diffractive optics and differentiable optical design."}
{"id": "2505.21520", "pdf": "https://arxiv.org/pdf/2505.21520", "abs": "https://arxiv.org/abs/2505.21520", "authors": ["Spiros Baxavanakis", "Manos Schinas", "Symeon Papadopoulos"], "title": "Do DeepFake Attribution Models Generalize?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in DeepFake generation, along with the proliferation of\nopen-source tools, have significantly lowered the barrier for creating\nsynthetic media. This trend poses a serious threat to the integrity and\nauthenticity of online information, undermining public trust in institutions\nand media. State-of-the-art research on DeepFake detection has primarily\nfocused on binary detection models. A key limitation of these models is that\nthey treat all manipulation techniques as equivalent, despite the fact that\ndifferent methods introduce distinct artifacts and visual cues. Only a limited\nnumber of studies explore DeepFake attribution models, although such models are\ncrucial in practical settings. By providing the specific manipulation method\nemployed, these models could enhance both the perceived trustworthiness and\nexplainability for end users. In this work, we leverage five state-of-the-art\nbackbone models and conduct extensive experiments across six DeepFake datasets.\nFirst, we compare binary and multi-class models in terms of cross-dataset\ngeneralization. Second, we examine the accuracy of attribution models in\ndetecting seen manipulation methods in unknown datasets, hence uncovering data\ndistribution shifts on the same DeepFake manipulations. Last, we assess the\neffectiveness of contrastive methods in improving cross-dataset generalization\nperformance. Our findings indicate that while binary models demonstrate better\ngeneralization abilities, larger models, contrastive methods, and higher data\nquality can lead to performance improvements in attribution models. The code of\nthis work is available on GitHub."}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578", "abs": "https://arxiv.org/abs/2505.21578", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios."}
{"id": "2505.22489", "pdf": "https://arxiv.org/pdf/2505.22489", "abs": "https://arxiv.org/abs/2505.22489", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "MICCAI2025 Submitted version", "summary": "We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications."}
{"id": "2505.21522", "pdf": "https://arxiv.org/pdf/2505.21522", "abs": "https://arxiv.org/abs/2505.21522", "authors": ["Shan Gao", "Zhiqiang Wu", "Yawen Niu", "Xiaotao Li", "Qingqing Xu"], "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB"}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598", "abs": "https://arxiv.org/abs/2505.21598", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture."}
{"id": "2505.21524", "pdf": "https://arxiv.org/pdf/2505.21524", "abs": "https://arxiv.org/abs/2505.21524", "authors": ["Amitai Yacobi", "Nir Ben-Ari", "Ronen Talmon", "Uri Shaham"], "title": "Learning Shared Representations from Unpaired Data", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our code IS publicly available at\nhttps://github.com/shaham-lab/SUE."}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."}
{"id": "2505.21528", "pdf": "https://arxiv.org/pdf/2505.21528", "abs": "https://arxiv.org/abs/2505.21528", "authors": ["Mokai Pan", "Kaizhen Zhu", "Yuexin Ma", "Yanwei Fu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus."}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646", "abs": "https://arxiv.org/abs/2505.21646", "authors": ["Lei Zhang", "Markus Stricker"], "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent."}
{"id": "2505.21532", "pdf": "https://arxiv.org/pdf/2505.21532", "abs": "https://arxiv.org/abs/2505.21532", "authors": ["Ismail Erbas", "Ferhat Demirkiran", "Karthik Swaminathan", "Naigang Wang", "Navid Ibtehaj Nizam", "Stefan T. Radev", "Kaoutar El Maghraoui", "Xavier Intes", "Vikas Pandey"], "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.optics"], "comment": "18 pages, 4 figures", "summary": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.21533", "pdf": "https://arxiv.org/pdf/2505.21533", "abs": "https://arxiv.org/abs/2505.21533", "authors": ["Thalles Silva", "Helio Pedrini", "Adín Ramírez Rivera"], "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025, code at https://github.com/sthalles/sop", "summary": "We present Self-Organizing Visual Prototypes (SOP), a new training technique\nfor unsupervised visual feature learning. Unlike existing prototypical\nself-supervised learning (SSL) methods that rely on a single prototype to\nencode all relevant features of a hidden cluster in the data, we propose the\nSOP strategy. In this strategy, a prototype is represented by many semantically\nsimilar representations, or support embeddings (SEs), each containing a\ncomplementary set of features that together better characterize their region in\nspace and maximize training performance. We reaffirm the feasibility of\nnon-parametric SSL by introducing novel non-parametric adaptations of two loss\nfunctions that implement the SOP strategy. Notably, we introduce the SOP Masked\nImage Modeling (SOP-MIM) task, where masked representations are reconstructed\nfrom the perspective of multiple non-parametric local SEs. We comprehensively\nevaluate the representations learned using the SOP strategy on a range of\nbenchmarks, including retrieval, linear evaluation, fine-tuning, and object\ndetection. Our pre-trained encoders achieve state-of-the-art performance on\nmany retrieval benchmarks and demonstrate increasing performance gains with\nmore complex encoders."}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy."}
{"id": "2505.21535", "pdf": "https://arxiv.org/pdf/2505.21535", "abs": "https://arxiv.org/abs/2505.21535", "authors": ["Yuxin Ren", "Maxwell D Collins", "Miao Hu", "Huanrui Yang"], "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages main paper + 6 pages appendix, 14 figures; submitted to\n  NeurIPS 2025", "summary": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module."}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization."}
{"id": "2505.21538", "pdf": "https://arxiv.org/pdf/2505.21538", "abs": "https://arxiv.org/abs/2505.21538", "authors": ["Zihan Weng", "Lucas Gomez", "Taylor Whittington Webb", "Pouya Bashivan"], "title": "Caption This, Reason That: VLMs Caught in the Middle", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution."}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693", "abs": "https://arxiv.org/abs/2505.21693", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.21539", "pdf": "https://arxiv.org/pdf/2505.21539", "abs": "https://arxiv.org/abs/2505.21539", "authors": ["Ziming Wang", "Nan Xue", "Rebecka Jörnsten"], "title": "Equivariant Flow Matching for Point Cloud Assembly", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701", "abs": "https://arxiv.org/abs/2505.21701", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose."}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710", "abs": "https://arxiv.org/abs/2505.21710", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2505.21545", "pdf": "https://arxiv.org/pdf/2505.21545", "abs": "https://arxiv.org/abs/2505.21545", "authors": ["Chika Maduabuchi", "Hao Chen", "Yujin Han", "Jindong Wang"], "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/chikap421/catlvdm Models:\n  https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main", "summary": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm"}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757", "abs": "https://arxiv.org/abs/2505.21757", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents."}
{"id": "2505.21547", "pdf": "https://arxiv.org/pdf/2505.21547", "abs": "https://arxiv.org/abs/2505.21547", "authors": ["Weixing Wang", "Zifeng Ding", "Jindong Gu", "Rui Cao", "Christoph Meinel", "Gerard de Melo", "Haojin Yang"], "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main"}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772", "abs": "https://arxiv.org/abs/2505.21772", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781", "abs": "https://arxiv.org/abs/2505.21781", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful."}
{"id": "2505.21556", "pdf": "https://arxiv.org/pdf/2505.21556", "abs": "https://arxiv.org/abs/2505.21556", "authors": ["Hee-Seon Kim", "Minbeom Kim", "Wonjun Lee", "Kihyun Kim", "Changick Kim"], "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts", "categories": ["cs.CV", "cs.AI"], "comment": "LVLM, Jailbreak", "summary": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches."}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets."}
{"id": "2505.21557", "pdf": "https://arxiv.org/pdf/2505.21557", "abs": "https://arxiv.org/abs/2505.21557", "authors": ["Polad Geidarov"], "title": "Analytical Calculation of Weights Convolutional Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights."}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816", "abs": "https://arxiv.org/abs/2505.21816", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks."}
{"id": "2505.21558", "pdf": "https://arxiv.org/pdf/2505.21558", "abs": "https://arxiv.org/abs/2505.21558", "authors": ["Elhoucine Elfatimia", "Recep Eryigitb", "Lahcen Elfatimi"], "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "na"], "comment": "11 Figure", "summary": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent."}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819", "abs": "https://arxiv.org/abs/2505.21819", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "title": "Representative Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models."}
{"id": "2505.21561", "pdf": "https://arxiv.org/pdf/2505.21561", "abs": "https://arxiv.org/abs/2505.21561", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Marouane Tliba", "Lauren Mills", "Ahmet Enis Cetin", "Mohammed H Elnagar"], "title": "Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment", "categories": ["cs.CV", "cs.LG"], "comment": "This paper has been accepted to the CVPR Workshop 2025, to be held in\n  Nashville, Tennessee", "summary": "We introduce a novel deep learning framework for the automated staging of\nspheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in\nboth orthodontics and forensic anthropology. Our approach leverages a\ndual-model architecture wherein a teacher model, trained on manually cropped\nimages, transfers its precise spatial understanding to a student model that\noperates on full, uncropped images. This knowledge distillation is facilitated\nby a newly formulated loss function that aligns spatial logits as well as\nincorporates gradient-based attention spatial mapping, ensuring that the\nstudent model internalizes the anatomically relevant features without relying\non external cropping or YOLO-based segmentation. By leveraging expert-curated\ndata and feedback at each step, our framework attains robust diagnostic\naccuracy, culminating in a clinically viable end-to-end pipeline. This\nstreamlined approach obviates the need for additional pre-processing tools and\naccelerates deployment, thereby enhancing both the efficiency and consistency\nof skeletal maturation assessment in diverse clinical settings."}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859", "abs": "https://arxiv.org/abs/2505.21859", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage."}
{"id": "2505.21564", "pdf": "https://arxiv.org/pdf/2505.21564", "abs": "https://arxiv.org/abs/2505.21564", "authors": ["Koki Matsuishi", "Tsuyoshi Okita"], "title": "Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "In deep multi-instance learning, the number of applicable instances depends\non the data set. In histopathology images, deep learning multi-instance\nlearners usually assume there are hundreds to thousands instances in a bag.\nHowever, when the number of instances in a bag increases to 256 in brain\nhematoma CT, learning becomes extremely difficult. In this paper, we address\nthis drawback. To overcome this problem, we propose using a pre-trained model\nwith self-supervised learning for the multi-instance learner as a downstream\ntask. With this method, even when the original target task suffers from the\nspurious correlation problem, we show improvements of 5% to 13% in accuracy and\n40% to 55% in the F1 measure for the hypodensity marker classification of brain\nhematoma CT."}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."}
{"id": "2505.21566", "pdf": "https://arxiv.org/pdf/2505.21566", "abs": "https://arxiv.org/abs/2505.21566", "authors": ["Gao Huayu", "Huang Tengjiu", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Diffusion Model-based Activity Completion for AI Motion Capture from Videos", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages, 16 figures", "summary": "AI-based motion capture is an emerging technology that offers a\ncost-effective alternative to traditional motion capture systems. However,\ncurrent AI motion capture methods rely entirely on observed video sequences,\nsimilar to conventional motion capture. This means that all human actions must\nbe predefined, and movements outside the observed sequences are not possible.\nTo address this limitation, we aim to apply AI motion capture to virtual\nhumans, where flexible actions beyond the observed sequences are required. We\nassume that while many action fragments exist in the training data, the\ntransitions between them may be missing. To bridge these gaps, we propose a\ndiffusion-model-based action completion technique that generates complementary\nhuman motion sequences, ensuring smooth and continuous movements. By\nintroducing a gate module and a position-time embedding module, our approach\nachieves competitive results on the Human3.6M dataset. Our experimental results\nshow that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but\nis slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size\n(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural\nand coherent motion sequences. Additionally, we propose a method for extracting\nsensor data, including acceleration and angular velocity, from human motion\nsequences."}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889", "abs": "https://arxiv.org/abs/2505.21889", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."}
{"id": "2505.21567", "pdf": "https://arxiv.org/pdf/2505.21567", "abs": "https://arxiv.org/abs/2505.21567", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods."}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%."}
{"id": "2505.21572", "pdf": "https://arxiv.org/pdf/2505.21572", "abs": "https://arxiv.org/abs/2505.21572", "authors": ["Sungwon Kim", "Namkyeong Lee", "Yunyoung Doh", "Seungmin Shin", "Guimok Cho", "Seung-Won Jeon", "Sangkook Kim", "Chanyoung Park"], "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Mesh-based 3D static analysis methods have recently emerged as efficient\nalternatives to traditional computational numerical solvers, significantly\nreducing computational costs and runtime for various physics-based analyses.\nHowever, these methods primarily focus on surface topology and geometry, often\noverlooking the inherent thickness of real-world 3D objects, which exhibits\nhigh correlations and similar behavior between opposing surfaces. This\nlimitation arises from the disconnected nature of these surfaces and the\nabsence of internal edge connections within the mesh. In this work, we propose\na novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network\n(T-EMNN), that effectively integrates the thickness of 3D objects while\nmaintaining the computational efficiency of surface meshes. Additionally, we\nintroduce data-driven coordinates that encode spatial information while\npreserving E(3)-equivariance or invariance properties, ensuring consistent and\nrobust analysis. Evaluations on a real-world industrial dataset demonstrate the\nsuperior performance of T-EMNN in accurately predicting node-level 3D\ndeformations, effectively capturing thickness effects while maintaining\ncomputational efficiency."}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA."}
{"id": "2505.21574", "pdf": "https://arxiv.org/pdf/2505.21574", "abs": "https://arxiv.org/abs/2505.21574", "authors": ["Dang Nguyen", "Jiping Li", "Jinghao Zheng", "Baharan Mirzasoleiman"], "title": "Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase\nthe size of the data by up to 10-30x to improve the in-distribution\nperformance. In this work, we show that synthetically augmenting part of the\ndata that is not learned early in training outperforms augmenting the entire\ndataset. By analyzing a two-layer CNN, we prove that this strategy improves\ngeneralization by promoting homogeneity in feature learning speed without\namplifying noise. Our extensive experiments show that by augmenting only\n30%-40% of the data, our method boosts the performance by up to 2.8% in a\nvariety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,\nCIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.\nNotably, our method applied with SGD outperforms the SOTA optimizer, SAM, on\nCIFAR-100 and TinyImageNet. It can also easily stack with existing weak and\nstrong augmentation strategies to further boost the performance."}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936", "abs": "https://arxiv.org/abs/2505.21936", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment."}
{"id": "2505.21589", "pdf": "https://arxiv.org/pdf/2505.21589", "abs": "https://arxiv.org/abs/2505.21589", "authors": ["Carina Newen", "Luca Hinkamp", "Maria Ntonti", "Emmanuel Müller"], "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 18 figures", "summary": "From uncertainty quantification to real-world object detection, we recognize\nthe importance of machine learning algorithms, particularly in safety-critical\ndomains such as autonomous driving or medical diagnostics. In machine learning,\nambiguous data plays an important role in various machine learning domains.\nOptical illusions present a compelling area of study in this context, as they\noffer insight into the limitations of both human and machine perception.\nDespite this relevance, optical illusion datasets remain scarce. In this work,\nwe introduce a novel dataset of optical illusions featuring intermingled animal\npairs designed to evoke perceptual ambiguity. We identify generalizable visual\nconcepts, particularly gaze direction and eye cues, as subtle yet impactful\nfeatures that significantly influence model accuracy. By confronting models\nwith perceptual ambiguity, our findings underscore the importance of concepts\nin visual learning and provide a foundation for studying bias and alignment\nbetween human and machine vision. To make this dataset useful for general\npurposes, we generate optical illusions systematically with different concepts\ndiscussed in our bias mitigation section. The dataset is accessible in Kaggle\nvia\nhttps://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.\nOur source code can be found at\nhttps://github.com/KDD-OpenSource/Ambivision.git."}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937", "abs": "https://arxiv.org/abs/2505.21937", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages."}
{"id": "2505.21593", "pdf": "https://arxiv.org/pdf/2505.21593", "abs": "https://arxiv.org/abs/2505.21593", "authors": ["Yang Yang", "Siming Zheng", "Jinwei Chen", "Boxi Wu", "Xiaofei He", "Deng Cai", "Bo Li", "Peng-Tao Jiang"], "title": "Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/", "summary": "Recent advances in diffusion based editing models have enabled realistic\ncamera simulation and image-based bokeh, but video bokeh remains largely\nunexplored. Existing video editing models cannot explicitly control focus\nplanes or adjust bokeh intensity, limiting their applicability for controllable\noptical effects. Moreover, naively extending image-based bokeh methods to video\noften results in temporal flickering and unsatisfactory edge blur transitions\ndue to the lack of temporal modeling and generalization capability. To address\nthese challenges, we propose a novel one-step video bokeh framework that\nconverts arbitrary input videos into temporally coherent, depth-aware bokeh\neffects. Our method leverages a multi-plane image (MPI) representation\nconstructed through a progressively widening depth sampling function, providing\nexplicit geometric guidance for depth-dependent blur synthesis. By conditioning\na single-step video diffusion model on MPI layers and utilizing the strong 3D\npriors from pre-trained models such as Stable Video Diffusion, our approach\nachieves realistic and consistent bokeh effects across diverse scenes.\nAdditionally, we introduce a progressive training strategy to enhance temporal\nconsistency, depth robustness, and detail preservation. Extensive experiments\ndemonstrate that our method produces high-quality, controllable bokeh effects\nand achieves state-of-the-art performance on multiple evaluation benchmarks."}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940", "abs": "https://arxiv.org/abs/2505.21940", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance."}
{"id": "2505.21635", "pdf": "https://arxiv.org/pdf/2505.21635", "abs": "https://arxiv.org/abs/2505.21635", "authors": ["Haoqian Liang", "Xiaohui Wang", "Zhichao Li", "Ya Yang", "Naiyan Wang"], "title": "Object Concepts Emerge from Motion", "categories": ["cs.CV"], "comment": null, "summary": "Object concepts play a foundational role in human visual cognition, enabling\nperception, memory, and interaction in the physical world. Inspired by findings\nin developmental neuroscience - where infants are shown to acquire object\nunderstanding through observation of motion - we propose a biologically\ninspired framework for learning object-centric visual representations in an\nunsupervised manner. Our key insight is that motion boundary serves as a strong\nsignal for object-level grouping, which can be used to derive pseudo instance\nsupervision from raw videos. Concretely, we generate motion-based instance\nmasks using off-the-shelf optical flow and clustering algorithms, and use them\nto train visual encoders via contrastive learning. Our framework is fully\nlabel-free and does not rely on camera calibration, making it scalable to\nlarge-scale unstructured video data. We evaluate our approach on three\ndownstream tasks spanning both low-level (monocular depth estimation) and\nhigh-level (3D object detection and occupancy prediction) vision. Our models\noutperform previous supervised and self-supervised baselines and demonstrate\nstrong generalization to unseen scenes. These results suggest that\nmotion-induced object representations offer a compelling alternative to\nexisting vision foundation models, capturing a crucial but overlooked level of\nabstraction: the visual instance. The corresponding code will be released upon\npaper acceptance."}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941", "abs": "https://arxiv.org/abs/2505.21941", "authors": ["Ashim Gupta", "Vivek Srikumar"], "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task."}
{"id": "2505.21637", "pdf": "https://arxiv.org/pdf/2505.21637", "abs": "https://arxiv.org/abs/2505.21637", "authors": ["Xiaole Tang", "Xiaoyi He", "Xiang Gu", "Jian Sun"], "title": "BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Despite remarkable advances made in all-in-one image restoration (AIR) for\nhandling different types of degradations simultaneously, existing methods\nremain vulnerable to out-of-distribution degradations and images, limiting\ntheir real-world applicability. In this paper, we propose a multi-source\nrepresentation learning framework BaryIR, which decomposes the latent space of\nmulti-source degraded images into a continuous barycenter space for unified\nfeature encoding and source-specific subspaces for specific semantic encoding.\nSpecifically, we seek the multi-source unified representation by introducing a\nmulti-source latent optimal transport barycenter problem, in which a continuous\nbarycenter map is learned to transport the latent representations to the\nbarycenter space. The transport cost is designed such that the representations\nfrom source-specific subspaces are contrasted with each other while maintaining\northogonality to those from the barycenter space. This enables BaryIR to learn\ncompact representations with unified degradation-agnostic information from the\nbarycenter space, as well as degradation-specific semantics from\nsource-specific subspaces, capturing the inherent geometry of multi-source data\nmanifold for generalizable AIR. Extensive experiments demonstrate that BaryIR\nachieves competitive performance compared to state-of-the-art all-in-one\nmethods. Particularly, BaryIR exhibits superior generalization ability to\nreal-world data and unseen degradations. The code will be publicly available at\nhttps://github.com/xl-tang3/BaryIR."}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958", "abs": "https://arxiv.org/abs/2505.21958", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem."}
{"id": "2505.21644", "pdf": "https://arxiv.org/pdf/2505.21644", "abs": "https://arxiv.org/abs/2505.21644", "authors": ["Kenneth Ball", "Erin Taylor", "Nirav Patel", "Andrew Bartels", "Gary Koplik", "James Polly", "Jay Hineman"], "title": "Geometric Feature Prompting of Image Segmentation Models", "categories": ["cs.CV"], "comment": null, "summary": "Advances in machine learning, especially the introduction of transformer\narchitectures and vision transformers, have led to the development of highly\ncapable computer vision foundation models. The segment anything model (known\ncolloquially as SAM and more recently SAM 2), is a highly capable foundation\nmodel for segmentation of natural images and has been further applied to\nmedical and scientific image segmentation tasks. SAM relies on prompts --\npoints or regions of interest in an image -- to generate associated\nsegmentations.\n  In this manuscript we propose the use of a geometrically motivated prompt\ngenerator to produce prompt points that are colocated with particular features\nof interest. Focused prompting enables the automatic generation of sensitive\nand specific segmentations in a scientific image analysis task using SAM with\nrelatively few point prompts. The image analysis task examined is the\nsegmentation of plant roots in rhizotron or minirhizotron images, which has\nhistorically been a difficult task to automate. Hand annotation of rhizotron\nimages is laborious and often subjective; SAM, initialized with GeomPrompt\nlocal ridge prompts has the potential to dramatically improve rhizotron image\nprocessing.\n  The authors have concurrently released an open source software suite called\ngeomprompt https://pypi.org/project/geomprompt/ that can produce point prompts\nin a format that enables direct integration with the segment-anything package."}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery."}
{"id": "2505.21647", "pdf": "https://arxiv.org/pdf/2505.21647", "abs": "https://arxiv.org/abs/2505.21647", "authors": ["Eric Xing", "Abby Stylianou", "Robert Pless", "Nathan Jacobs"], "title": "QuARI: Query Adaptive Retrieval Improvement", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures, 4 tables", "summary": "Massive-scale pretraining has made vision-language models increasingly\npopular for image-to-image and text-to-image retrieval across a broad\ncollection of domains. However, these models do not perform well when used for\nchallenging retrieval tasks, such as instance retrieval in very large-scale\nimage collections. Recent work has shown that linear transformations of VLM\nfeatures trained for instance retrieval can improve performance by emphasizing\nsubspaces that relate to the domain of interest. In this paper, we explore a\nmore extreme version of this specialization by learning to map a given query to\na query-specific feature space transformation. Because this transformation is\nlinear, it can be applied with minimal computational cost to millions of image\nembeddings, making it effective for large-scale retrieval or re-ranking.\nResults show that this method consistently outperforms state-of-the-art\nalternatives, including those that require many orders of magnitude more\ncomputation at query time."}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967", "abs": "https://arxiv.org/abs/2505.21967", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems."}
{"id": "2505.21649", "pdf": "https://arxiv.org/pdf/2505.21649", "abs": "https://arxiv.org/abs/2505.21649", "authors": ["Keanu Nichols", "Nazia Tasnim", "Yan Yuting", "Nicholas Ikechukwu", "Elva Zou", "Deepti Ghadiyaram", "Bryan Plummer"], "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark"}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979", "abs": "https://arxiv.org/abs/2505.21979", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable."}
{"id": "2505.21653", "pdf": "https://arxiv.org/pdf/2505.21653", "abs": "https://arxiv.org/abs/2505.21653", "authors": ["Ke Zhang", "Cihan Xiao", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation", "categories": ["cs.CV"], "comment": "19 pages, 8 figures", "summary": "Recent video diffusion models have demonstrated their great capability in\ngenerating visually-pleasing results, while synthesizing the correct physical\neffects in generated videos remains challenging. The complexity of real-world\nmotions, interactions, and dynamics introduce great difficulties when learning\nphysics from data. In this work, we propose DiffPhy, a generic framework that\nenables physically-correct and photo-realistic video generation by fine-tuning\na pre-trained video diffusion model. Our method leverages large language models\n(LLMs) to explicitly reason a comprehensive physical context from the text\nprompt and use it to guide the generation. To incorporate physical context into\nthe diffusion model, we leverage a Multimodal large language model (MLLM) as a\nsupervisory signal and introduce a set of novel training objectives that\njointly enforce physical correctness and semantic consistency with the input\ntext. We also establish a high-quality physical video dataset containing\ndiverse phyiscal actions and events to facilitate effective finetuning.\nExtensive experiments on public benchmarks demonstrate that DiffPhy is able to\nproduce state-of-the-art results across diverse physics-related scenarios. Our\nproject page is available at https://bwgzk-keke.github.io/DiffPhy/"}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997", "abs": "https://arxiv.org/abs/2505.21997", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research."}
{"id": "2505.21697", "pdf": "https://arxiv.org/pdf/2505.21697", "abs": "https://arxiv.org/abs/2505.21697", "authors": ["Xiaoling Hu", "Peirong Liu", "Dina Zemlyanker", "Jonathan Williams Ramirez", "Oula Puonti", "Juan Eugenio Iglesias"], "title": "Scalable Segmentation for Ultra-High-Resolution Brain MR Images", "categories": ["cs.CV"], "comment": "15 pages, 4 figures", "summary": "Although deep learning has shown great success in 3D brain MRI segmentation,\nachieving accurate and efficient segmentation of ultra-high-resolution brain\nimages remains challenging due to the lack of labeled training data for\nfine-scale anatomical structures and high computational demands. In this work,\nwe propose a novel framework that leverages easily accessible, low-resolution\ncoarse labels as spatial references and guidance, without incurring additional\nannotation cost. Instead of directly predicting discrete segmentation maps, our\napproach regresses per-class signed distance transform maps, enabling smooth,\nboundary-aware supervision. Furthermore, to enhance scalability,\ngeneralizability, and efficiency, we introduce a scalable class-conditional\nsegmentation strategy, where the model learns to segment one class at a time\nconditioned on a class-specific input. This novel design not only reduces\nmemory consumption during both training and testing, but also allows the model\nto generalize to unseen anatomical classes. We validate our method through\ncomprehensive experiments on both synthetic and real-world datasets,\ndemonstrating its superior performance and scalability compared to conventional\nsegmentation approaches."}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999", "abs": "https://arxiv.org/abs/2505.21999", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking."}
{"id": "2505.21698", "pdf": "https://arxiv.org/pdf/2505.21698", "abs": "https://arxiv.org/abs/2505.21698", "authors": ["Yitong Li", "Morteza Ghahremani", "Christian Wachinger"], "title": "MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Recent vision-language foundation models deliver state-of-the-art results on\nnatural image classification but falter on medical images due to pronounced\ndomain shifts. At the same time, training a medical foundation model requires\nsubstantial resources, including extensive annotated data and high\ncomputational capacity. To bridge this gap with minimal overhead, we introduce\nMedBridge, a lightweight multimodal adaptation framework that re-purposes\npretrained VLMs for accurate medical image diagnosis. MedBridge comprises three\nkey components. First, a Focal Sampling module that extracts high-resolution\nlocal regions to capture subtle pathological features and compensate for the\nlimited input resolution of general-purpose VLMs. Second, a Query Encoder\n(QEncoder) injects a small set of learnable queries that attend to the frozen\nfeature maps of VLM, aligning them with medical semantics without retraining\nthe entire backbone. Third, a Mixture of Experts mechanism, driven by learnable\nqueries, harnesses the complementary strength of diverse VLMs to maximize\ndiagnostic performance. We evaluate MedBridge on five medical imaging\nbenchmarks across three key adaptation tasks, demonstrating its superior\nperformance in both cross-domain and in-domain adaptation settings, even under\nvarying levels of training data availability. Notably, MedBridge achieved over\n6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in\nmulti-label thoracic disease diagnosis, underscoring its effectiveness in\nleveraging foundation models for accurate and data-efficient medical diagnosis.\nOur code is available at https://github.com/ai-med/MedBridge."}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell."}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724", "abs": "https://arxiv.org/abs/2505.21724", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality."}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017", "abs": "https://arxiv.org/abs/2505.22017", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs."}
{"id": "2505.21736", "pdf": "https://arxiv.org/pdf/2505.21736", "abs": "https://arxiv.org/abs/2505.21736", "authors": ["Zachary Schlamowitz", "Andrew Bennecke", "Daniel J. Tward"], "title": "Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The principle of translation equivariance (if an input image is translated an\noutput image should be translated by the same amount), led to the development\nof convolutional neural networks that revolutionized machine vision. Other\nsymmetries, like rotations and reflections, play a similarly critical role,\nespecially in biomedical image analysis, but exploiting these symmetries has\nnot seen wide adoption. We hypothesize that this is partially due to the\nmathematical complexity of methods used to exploit these symmetries, which\noften rely on representation theory, a bespoke concept in differential geometry\nand group theory. In this work, we show that the same equivariance can be\nachieved using a simple form of convolution kernels that we call ``moment\nkernels,'' and prove that all equivariant kernels must take this form. These\nare a set of radially symmetric functions of a spatial position $x$, multiplied\nby powers of the components of $x$ or the identity matrix. We implement\nequivariant neural networks using standard convolution modules, and provide\narchitectures to execute several biomedical image analysis tasks that depend on\nequivariance principles: classification (outputs are invariant under orthogonal\ntransforms), 3D image registration (outputs transform like a vector), and cell\nsegmentation (quadratic forms defining ellipses transform like a matrix)."}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018", "abs": "https://arxiv.org/abs/2505.22018", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Improving Continual Pre-training Through Seamless Data Packing", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing."}
{"id": "2505.21742", "pdf": "https://arxiv.org/pdf/2505.21742", "abs": "https://arxiv.org/abs/2505.21742", "authors": ["Briglia Maria Rosaria", "Mujtaba Hussain Mirza", "Giuseppe Lisanti", "Iacopo Masi"], "title": "What is Adversarial Training for Diffusion Models?", "categories": ["cs.CV", "cs.LG"], "comment": "40 pages", "summary": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.21746", "pdf": "https://arxiv.org/pdf/2505.21746", "abs": "https://arxiv.org/abs/2505.21746", "authors": ["Arif Masrur", "Peder A. Olsen", "Paul R. Adler", "Carlan Jackson", "Matthew W. Myers", "Nathan Sedghi", "Ray R. Weil"], "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for\nprecision agriculture, yet each presents trade-offs. Satellite data offer broad\nspatial, temporal, and spectral coverage but lack the resolution needed for\nmany precision farming applications, while UAS provide high spatial detail but\nare limited by coverage and cost, especially for hyperspectral data. This study\npresents a novel framework that fuses satellite and UAS imagery using\nsuper-resolution methods. By integrating data across spatial, spectral, and\ntemporal domains, we leverage the strengths of both platforms cost-effectively.\nWe use estimation of cover crop biomass and nitrogen (N) as a case study to\nevaluate our approach. By spectrally extending UAS RGB data to the vegetation\nred edge and near-infrared regions, we generate high-resolution Sentinel-2\nimagery and improve biomass and N estimation accuracy by 18% and 31%,\nrespectively. Our results show that UAS data need only be collected from a\nsubset of fields and time points. Farmers can then 1) enhance the spectral\ndetail of UAS RGB imagery; 2) increase the spatial resolution by using\nsatellite data; and 3) extend these enhancements spatially and across the\ngrowing season at the frequency of the satellite flights. Our SRCNN-based\nspectral extension model shows considerable promise for model transferability\nover other cropping systems in the Upper and Lower Chesapeake Bay regions.\nAdditionally, it remains effective even when cloud-free satellite data are\nunavailable, relying solely on the UAS RGB input. The spatial extension model\nproduces better biomass and N predictions than models built on raw UAS RGB\nimages. Once trained with targeted UAS RGB data, the spatial extension model\nallows farmers to stop repeated UAS flights. While we introduce\nsuper-resolution advances, the core contribution is a lightweight and scalable\nsystem for affordable on-farm use."}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037", "abs": "https://arxiv.org/abs/2505.22037", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation."}
{"id": "2505.21754", "pdf": "https://arxiv.org/pdf/2505.21754", "abs": "https://arxiv.org/abs/2505.21754", "authors": ["Martin Büchner", "Liza Dahiya", "Simon Dorer", "Vipul Ramtekkar", "Kenji Nishimiya", "Daniele Cattaneo", "Abhinav Valada"], "title": "Visual Loop Closure Detection Through Deep Graph Consensus", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Visual loop closure detection traditionally relies on place recognition\nmethods to retrieve candidate loops that are validated using computationally\nexpensive RANSAC-based geometric verification. As false positive loop closures\nsignificantly degrade downstream pose graph estimates, verifying a large number\nof candidates in online simultaneous localization and mapping scenarios is\nconstrained by limited time and compute resources. While most deep loop closure\ndetection approaches only operate on pairs of keyframes, we relax this\nconstraint by considering neighborhoods of multiple keyframes when detecting\nloops. In this work, we introduce LoopGNN, a graph neural network architecture\nthat estimates loop closure consensus by leveraging cliques of visually similar\nkeyframes retrieved through place recognition. By propagating deep feature\nencodings among nodes of the clique, our method yields high-precision estimates\nwhile maintaining high recall. Extensive experimental evaluations on the\nTartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms\ntraditional baselines. Additionally, an ablation study across various keypoint\nextractors demonstrates that our method is robust, regardless of the type of\ndeep feature encodings used, and exhibits higher computational efficiency\ncompared to classical geometric verification baselines. We release our code,\nsupplementary material, and keyframe data at\nhttps://loopgnn.cs.uni-freiburg.de."}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054", "abs": "https://arxiv.org/abs/2505.22054", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "title": "Voice Adaptation for Swiss German", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061", "abs": "https://arxiv.org/abs/2505.22061", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems."}
{"id": "2505.21771", "pdf": "https://arxiv.org/pdf/2505.21771", "abs": "https://arxiv.org/abs/2505.21771", "authors": ["Prasham Yatinkumar Titiya", "Jainil Trivedi", "Chitta Baral", "Vivek Gupta"], "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal tables those that integrate semi structured data with visual\nelements such as charts and maps are ubiquitous across real world domains, yet\nthey pose a formidable challenge to current vision language models (VLMs).\nWhile Large Language models (LLMs) and VLMs have demonstrated strong\ncapabilities in text and image understanding, their performance on complex,\nreal world multimodal table reasoning remains unexplored. To bridge this gap,\nwe introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of\n500 real world multimodal tables drawn from diverse real world sources, with a\ntotal of 4021 question answer pairs. MMTBENCH questions cover four question\ntypes (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning\ntypes (Mathematical, Extrema Identification, Fact Verification, Vision Based,\nand Others), and eight table types (Single/Multiple Entity, Maps and Charts\nwith Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive\nevaluation of state of the art models on all types reveals substantial\nperformance gaps, particularly on questions requiring visual-based reasoning\nand multi-step inference. These findings show the urgent need for improved\narchitectures that more tightly integrate vision and language processing. By\nproviding a challenging, high-quality resource that mirrors the complexity of\nreal-world tasks, MMTBENCH underscores its value as a resource for future\nresearch on multimodal tables."}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO."}
{"id": "2505.21780", "pdf": "https://arxiv.org/pdf/2505.21780", "abs": "https://arxiv.org/abs/2505.21780", "authors": ["Yanbo Wang", "Justin Dauwels", "Yilun Du"], "title": "Compositional Scene Understanding through Inverse Generative Modeling", "categories": ["cs.CV"], "comment": "ICML 2025, Webpage:\n  https://energy-based-model.github.io/compositional-inference/", "summary": "Generative models have demonstrated remarkable abilities in generating\nhigh-fidelity visual content. In this work, we explore how generative models\ncan further be used not only to synthesize visual content but also to\nunderstand the properties of a scene given a natural image. We formulate scene\nunderstanding as an inverse generative modeling problem, where we seek to find\nconditional parameters of a visual generative model to best fit a given natural\nimage. To enable this procedure to infer scene structure from images\nsubstantially different than those seen during training, we further propose to\nbuild this visual generative model compositionally from smaller models over\npieces of a scene. We illustrate how this procedure enables us to infer the set\nof objects in a scene, enabling robust generalization to new test scenes with\nan increased number of objects of new shapes. We further illustrate how this\nenables us to infer global scene factors, likewise enabling robust\ngeneralization to new scenes. Finally, we illustrate how this approach can be\ndirectly applied to existing pretrained text-to-image generative models for\nzero-shot multi-object perception. Code and visualizations are at\n\\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}."}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076", "abs": "https://arxiv.org/abs/2505.22076", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable."}
{"id": "2505.21795", "pdf": "https://arxiv.org/pdf/2505.21795", "abs": "https://arxiv.org/abs/2505.21795", "authors": ["Claudia Cuttano", "Gabriele Trivigno", "Giuseppe Averta", "Carlo Masone"], "title": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": "Code: https://github.com/ClaudiaCuttano/SANSA", "summary": "Few-shot segmentation aims to segment unseen object categories from just a\nhandful of annotated examples. This requires mechanisms that can both identify\nsemantically related objects across images and accurately produce segmentation\nmasks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate\nmechanism, offers both strong segmentation capabilities and a built-in feature\nmatching process. However, we show that its representations are entangled with\ntask-specific cues optimized for object tracking, which impairs its use for\ntasks requiring higher level semantic understanding. Our key insight is that,\ndespite its class-agnostic pretraining, SAM2 already encodes rich semantic\nstructure in its features. We propose SANSA (Semantically AligNed Segment\nAnything 2), a framework that makes this latent structure explicit, and\nrepurposes SAM2 for few-shot segmentation through minimal task-specific\nmodifications. SANSA achieves state-of-the-art performance on few-shot\nsegmentation benchmarks specifically designed to assess generalization,\noutperforms generalist methods in the popular in-context setting, supports\nvarious prompts flexible interaction via points, boxes, or scribbles, and\nremains significantly faster and more compact than prior approaches. Code is\navailable at https://github.com/ClaudiaCuttano/SANSA."}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095", "abs": "https://arxiv.org/abs/2505.22095", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy."}
{"id": "2505.21817", "pdf": "https://arxiv.org/pdf/2505.21817", "abs": "https://arxiv.org/abs/2505.21817", "authors": ["Xiaomeng Yang", "Lei Lu", "Qihui Fan", "Changdi Yang", "Juyi Lin", "Yanzhi Wang", "Xuan Zhang", "Shangqian Gao"], "title": "ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated exceptional capabilities in generating\nhigh-fidelity images. However, their iterative denoising process results in\nsignificant computational overhead during inference, limiting their practical\ndeployment in resource-constrained environments. Existing acceleration methods\noften adopt uniform strategies that fail to capture the temporal variations\nduring diffusion generation, while the commonly adopted sequential\npruning-then-fine-tuning strategy suffers from sub-optimality due to the\nmisalignment between pruning decisions made on pretrained weights and the\nmodel's final parameters. To address these limitations, we introduce ALTER:\nAll-in-One Layer Pruning and Temporal Expert Routing, a unified framework that\ntransforms diffusion models into a mixture of efficient temporal experts. ALTER\nachieves a single-stage optimization that unifies layer pruning, expert\nrouting, and model fine-tuning by employing a trainable hypernetwork, which\ndynamically generates layer pruning decisions and manages timestep routing to\nspecialized, pruned expert sub-networks throughout the ongoing fine-tuning of\nthe UNet. This unified co-optimization strategy enables significant efficiency\ngains while preserving high generative quality. Specifically, ALTER achieves\nsame-level visual fidelity to the original 50-step Stable Diffusion v2.1 model\nwhile utilizing only 25.9% of its total MACs with just 20 inference steps and\ndelivering a 3.64x speedup through 35% sparsity."}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially."}
{"id": "2505.21831", "pdf": "https://arxiv.org/pdf/2505.21831", "abs": "https://arxiv.org/abs/2505.21831", "authors": ["Bowen Chen", "Cheng-han Lee", "Yixu Chen", "Zaixi Shang", "Hai Wei", "Alan C. Bovik"], "title": "HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HDRSDR-VQA, a large-scale video quality assessment dataset\ndesigned to facilitate comparative analysis between High Dynamic Range (HDR)\nand Standard Dynamic Range (SDR) content under realistic viewing conditions.\nThe dataset comprises 960 videos generated from 54 diverse source sequences,\neach presented in both HDR and SDR formats across nine distortion levels. To\nobtain reliable perceptual quality scores, we conducted a comprehensive\nsubjective study involving 145 participants and six consumer-grade HDR-capable\ntelevisions. A total of over 22,000 pairwise comparisons were collected and\nscaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets\nthat focus on a single dynamic range format or use limited evaluation\nprotocols, HDRSDR-VQA enables direct content-level comparison between HDR and\nSDR versions, supporting detailed investigations into when and why one format\nis preferred over the other. The open-sourced part of the dataset is publicly\navailable to support further research in video quality assessment,\ncontent-adaptive streaming, and perceptual model development."}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101", "abs": "https://arxiv.org/abs/2505.22101", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems."}
{"id": "2505.21837", "pdf": "https://arxiv.org/pdf/2505.21837", "abs": "https://arxiv.org/abs/2505.21837", "authors": ["Aliasghar Khani", "Arianna Rampini", "Evan Atherton", "Bruno Roy"], "title": "UniMoGen: Universal Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion generation is a cornerstone of computer graphics, animation, gaming,\nand robotics, enabling the creation of realistic and varied character\nmovements. A significant limitation of existing methods is their reliance on\nspecific skeletal structures, which restricts their versatility across\ndifferent characters. To overcome this, we introduce UniMoGen, a novel\nUNet-based diffusion model designed for skeleton-agnostic motion generation.\nUniMoGen can be trained on motion data from diverse characters, such as humans\nand animals, without the need for a predefined maximum number of joints. By\ndynamically processing only the necessary joints for each character, our model\nachieves both skeleton agnosticism and computational efficiency. Key features\nof UniMoGen include controllability via style and trajectory inputs, and the\nability to continue motions from past frames. We demonstrate UniMoGen's\neffectiveness on the 100style dataset, where it outperforms state-of-the-art\nmethods in diverse character motion generation. Furthermore, when trained on\nboth the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen\nachieves high performance and improved efficiency across both skeletons. These\nresults highlight UniMoGen's potential to advance motion generation by\nproviding a flexible, efficient, and controllable solution for a wide range of\ncharacter animations."}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107", "abs": "https://arxiv.org/abs/2505.22107", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."}
{"id": "2505.21844", "pdf": "https://arxiv.org/pdf/2505.21844", "abs": "https://arxiv.org/abs/2505.21844", "authors": ["Mehrdad Noori", "David Osowiechi", "Gustavo Adolfo Vargas Hakim", "Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Farzad Beizaee", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, test-time adaptation has attracted wide interest in the context of\nvision-language models for image classification. However, to the best of our\nknowledge, the problem is completely overlooked in dense prediction tasks such\nas Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a\nnovel TTA method tailored to adapting VLMs for segmentation during test time.\nUnlike TTA methods for image classification, our Multi-Level and Multi-Prompt\n(MLMP) entropy minimization integrates features from intermediate\nvision-encoder layers and is performed with different text-prompt templates at\nboth the global CLS token and local pixel-wise levels. Our approach could be\nused as plug-and-play for any segmentation network, does not require additional\ntraining data or labels, and remains effective even with a single test sample.\nFurthermore, we introduce a comprehensive OVSS TTA benchmark suite, which\nintegrates a rigorous evaluation protocol, seven segmentation datasets, and 15\ncommon corruptions, with a total of 82 distinct test scenarios, establishing a\nstandardized and comprehensive testbed for future TTA research in\nopen-vocabulary segmentation. Our experiments on this suite demonstrate that\nour segmentation-tailored method consistently delivers significant gains over\ndirect adoption of TTA classification baselines."}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113", "abs": "https://arxiv.org/abs/2505.22113", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs."}
{"id": "2505.21847", "pdf": "https://arxiv.org/pdf/2505.21847", "abs": "https://arxiv.org/abs/2505.21847", "authors": ["Xuwei Xu", "Yang Li", "Yudong Chen", "Jiajun Liu", "Sen Wang"], "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.21848", "pdf": "https://arxiv.org/pdf/2505.21848", "abs": "https://arxiv.org/abs/2505.21848", "authors": ["Jingqi Xu", "Chenghao Li", "Yuke Zhang", "Peter A. Beerel"], "title": "FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable potential in generating\nhigh-quality images. However, their tendency to replicate training data raises\nserious privacy concerns, particularly when the training datasets contain\nsensitive or private information. Existing mitigation strategies primarily\nfocus on reducing image duplication, modifying the cross-attention mechanism,\nand altering the denoising backbone architecture of diffusion models. Moreover,\nrecent work has shown that adding a consistent small amount of noise to text\nembeddings can reduce replication to some degree. In this work, we begin by\nanalyzing the impact of adding varying amounts of noise. Based on our analysis,\nwe propose a fine-grained noise injection technique that probabilistically adds\na larger amount of noise to token embeddings. We refer to our method as\nFine-grained Probabilistic Addition of Noise (FPAN). Through our extensive\nexperiments, we show that our proposed FPAN can reduce replication by an\naverage of 28.78% compared to the baseline diffusion model without\nsignificantly impacting image quality, and outperforms the prior\nconsistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined\nwith other existing mitigation methods, our FPAN approach can further reduce\nreplication by up to 16.82% with similar, if not improved, image quality."}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118", "abs": "https://arxiv.org/abs/2505.22118", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages."}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120", "abs": "https://arxiv.org/abs/2505.22120", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}."}
{"id": "2505.21854", "pdf": "https://arxiv.org/pdf/2505.21854", "abs": "https://arxiv.org/abs/2505.21854", "authors": ["Jun Chen", "Xinke Li", "Mingyue Xu", "Tianrui Li", "Chongshou Li"], "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131", "abs": "https://arxiv.org/abs/2505.22131", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER."}
{"id": "2505.21862", "pdf": "https://arxiv.org/pdf/2505.21862", "abs": "https://arxiv.org/abs/2505.21862", "authors": ["Chenhui Zhao", "Yiwei Lyu", "Asadur Chowdury", "Edward Harake", "Akhil Kondepudi", "Akshay Rao", "Xinhai Hou", "Honglak Lee", "Todd Hollon"], "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Language-image pre-training has demonstrated strong performance in 2D medical\nimaging, but its success in 3D modalities such as CT and MRI remains limited\ndue to the high computational demands of volumetric data, which pose a\nsignificant barrier to training on large-scale, uncurated clinical studies. In\nthis study, we introduce Hierarchical attention for Language-Image Pre-training\n(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a\nlightweight hierarchical attention mechanism inspired by the natural hierarchy\nof radiology data: slice, scan, and study. This mechanism exhibits strong\ngeneralizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when\npre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables\ndirect training on uncurated datasets. Trained on 220K patients with 3.13\nmillion scans for brain MRI and 240K patients with 1.44 million scans for head\nCT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on\nthe proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and\n+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These\nresults demonstrate that, with HLIP, directly pre-training on uncurated\nclinical datasets is a scalable and effective direction for language-image\npre-training in 3D medical imaging. The code is available at\nhttps://github.com/Zch0414/hlip"}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135", "abs": "https://arxiv.org/abs/2505.22135", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization."}
{"id": "2505.21868", "pdf": "https://arxiv.org/pdf/2505.21868", "abs": "https://arxiv.org/abs/2505.21868", "authors": ["Guiping Cao", "Wenjian Huang", "Xiangyuan Lan", "Jianguo Zhang", "Dongmei Jiang", "Yaowei Wang"], "title": "Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection", "categories": ["cs.CV"], "comment": "IEEE TRANSACTIONS ON MULTIMEDIA", "summary": "Small Object Detection (SOD) poses significant challenges due to limited\ninformation and the model's low class prediction score. While Transformer-based\ndetectors have shown promising performance, their potential for SOD remains\nlargely unexplored. In typical DETR-like frameworks, the CNN backbone network,\nspecialized in aggregating local information, struggles to capture the\nnecessary contextual information for SOD. The multiple attention layers in the\nTransformer Encoder face difficulties in effectively attending to small objects\nand can also lead to blurring of features. Furthermore, the model's lower class\nprediction score of small objects compared to large objects further increases\nthe difficulty of SOD. To address these challenges, we introduce a novel\napproach called Cross-DINO. This approach incorporates the deep MLP network to\naggregate initial feature representations with both short and long range\ninformation for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to\nintegrate these initial representations to the Transformer Encoder feature,\nenhancing the details of small objects. Additionally, we introduce a new kind\nof soft label named Category-Size (CS), integrating the Category and Size of\nobjects. By treating CS as new ground truth, we propose a new loss function\ncalled Boost Loss to improve the class prediction score of the model. Extensive\nexperimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D\ndatasets demonstrate that Cross-DINO efficiently improves the performance of\nDETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for\nSOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.\n32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The\nsource codes will be available at https://github.com/Med-Process/Cross-DINO."}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156", "abs": "https://arxiv.org/abs/2505.22156", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."}
{"id": "2505.21876", "pdf": "https://arxiv.org/pdf/2505.21876", "abs": "https://arxiv.org/abs/2505.21876", "authors": ["Zun Wang", "Jaemin Cho", "Jialu Li", "Han Lin", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://zunwang1.github.io/Epic", "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios."}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157", "abs": "https://arxiv.org/abs/2505.22157", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian Küch"], "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead."}
{"id": "2505.21890", "pdf": "https://arxiv.org/pdf/2505.21890", "abs": "https://arxiv.org/abs/2505.21890", "authors": ["Sunil Kumar Narayanan", "Lingjun Zhao", "Lu Gan", "Yongsheng Chen"], "title": "Hyperspectral Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral imaging (HSI) has been widely used in agricultural applications\nfor non-destructive estimation of plant nutrient composition and precise\ndetermination of nutritional elements in samples. Recently, 3D reconstruction\nmethods have been used to create implicit neural representations of HSI scenes,\nwhich can help localize the target object's nutrient composition spatially and\nspectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit\nrepresentation that can render hyperspectral channel compositions of each\nspatial location from any viewing direction. However, it faces limitations in\ntraining time and rendering speed. In this paper, we propose Hyperspectral\nGaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian\nSplatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of\nthe hyperspectral scenes and novel view synthesis for the entire spectral\nrange. To enhance the model's ability to capture fine-grained reflectance\nvariations across the light spectrum and leverage correlations between adjacent\nwavelengths for denoising, we introduce a wavelength encoder to generate\nwavelength-specific spherical harmonics offsets. We also introduce a novel\nKullback--Leibler divergence-based loss to mitigate the spectral distribution\ngap between the rendered image and the ground truth. A diffusion model is\nfurther applied for denoising the rendered images and generating photorealistic\nhyperspectral images. We present extensive evaluations on five diverse\nhyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of\nour proposed HS-GS framework. The results demonstrate that HS-GS achieves new\nstate-of-the-art performance among all previously published methods. Code will\nbe released upon publication."}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation."}
{"id": "2505.21897", "pdf": "https://arxiv.org/pdf/2505.21897", "abs": "https://arxiv.org/abs/2505.21897", "authors": ["Jianchao Jiang", "Haofeng Zhang"], "title": "Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation", "categories": ["cs.CV"], "comment": "12 pages, 9 figures, 9 tables, accepted by IJCAI 2025", "summary": "Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a\nmodel that can perform segmentation from only a few annotated images. However,\nmost existing prototype-based FSMIS methods generate multiple prototypes from\nthe support image solely by random sampling or local averaging, which can cause\nparticularly severe boundary blurring due to the tendency for normal features\naccounting for the majority of features of a specific category. Consequently,\nwe propose to focus more attention to those weaker features that are crucial\nfor clear segmentation boundary. Specifically, we design a Support\nSelf-Prediction (SSP) module to identify such weak features by comparing true\nsupport mask with one predicted by global support prototype. Then, a Hard\nPrototypes Generation (HPG) module is employed to generate multiple hard\nprototypes based on these weak features. Subsequently, a Multiple Similarity\nMaps Fusion (MSMF) module is devised to generate final segmenting mask in a\ndual-path fashion to mitigate the imbalance between foreground and background\nin medical images. Furthermore, we introduce a boundary loss to further\nconstraint the edge of segmentation. Extensive experiments on three publicly\navailable medical image datasets demonstrate that our method achieves\nstate-of-the-art performance. Code is available at\nhttps://github.com/jcjiang99/CoW."}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169", "abs": "https://arxiv.org/abs/2505.22169", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation."}
{"id": "2505.21904", "pdf": "https://arxiv.org/pdf/2505.21904", "abs": "https://arxiv.org/abs/2505.21904", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches."}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172", "abs": "https://arxiv.org/abs/2505.22172", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "title": "Reverse Preference Optimization for Complex Instruction Following", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o."}
{"id": "2505.21905", "pdf": "https://arxiv.org/pdf/2505.21905", "abs": "https://arxiv.org/abs/2505.21905", "authors": ["Mo Zhou", "Keren Ye", "Viraj Shah", "Kangfu Mei", "Mauricio Delbracio", "Peyman Milanfar", "Vishal M. Patel", "Hossein Talebi"], "title": "Reference-Guided Identity Preserving Face Restoration", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Preserving face identity is a critical yet persistent challenge in\ndiffusion-based image restoration. While reference faces offer a path forward,\nexisting reference-based methods often fail to fully exploit their potential.\nThis paper introduces a novel approach that maximizes reference face utility\nfor improved face restoration and identity preservation. Our method makes three\nkey contributions: 1) Composite Context, a comprehensive representation that\nfuses multi-level (high- and low-level) information from the reference face,\noffering richer guidance than prior singular representations. 2) Hard Example\nIdentity Loss, a novel loss function that leverages the reference face to\naddress the identity learning inefficiencies found in the existing identity\nloss. 3) A training-free method to adapt the model to multi-reference inputs\nduring inference. The proposed method demonstrably restores high-quality faces\nand achieves state-of-the-art identity preserving restoration on benchmarks\nsuch as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work."}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176", "abs": "https://arxiv.org/abs/2505.22176", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation."}
{"id": "2505.21911", "pdf": "https://arxiv.org/pdf/2505.21911", "abs": "https://arxiv.org/abs/2505.21911", "authors": ["Yiheng Lin", "Shifang Zhao", "Ting Liu", "Xiaochao Qu", "Luoqi Liu", "Yao Zhao", "Yunchao Wei"], "title": "AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Personalized image generation aims to integrate user-provided concepts into\ntext-to-image models, enabling the generation of customized content based on a\ngiven prompt. Recent zero-shot approaches, particularly those leveraging\ndiffusion transformers, incorporate reference image information through\nmulti-modal attention mechanism. This integration allows the generated output\nto be influenced by both the textual prior from the prompt and the visual prior\nfrom the reference image. However, we observe that when the prompt and\nreference image are misaligned, the generated results exhibit a stronger bias\ntoward the textual prior, leading to a significant loss of reference content.\nTo address this issue, we propose AlignGen, a Cross-Modality Prior Alignment\nmechanism that enhances personalized image generation by: 1) introducing a\nlearnable token to bridge the gap between the textual and visual priors, 2)\nincorporating a robust training strategy to ensure proper prior alignment, and\n3) employing a selective cross-modal attention mask within the multi-modal\nattention mechanism to further align the priors. Experimental results\ndemonstrate that AlignGen outperforms existing zero-shot methods and even\nsurpasses popular test-time optimization approaches."}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant."}
{"id": "2505.21914", "pdf": "https://arxiv.org/pdf/2505.21914", "abs": "https://arxiv.org/abs/2505.21914", "authors": ["Chenfeng Wei", "Qi Wu", "Si Zuo", "Jiahua Xu", "Boyang Zhao", "Zeyu Yang", "Guotao Xie", "Shenhong Wang"], "title": "LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous driving datasets are essential for validating the progress of\nintelligent vehicle algorithms, which include localization, perception, and\nprediction. However, existing datasets are predominantly focused on structured\nurban environments, which limits the exploration of unstructured and\nspecialized scenarios, particularly those characterized by significant dust\nlevels. This paper introduces the LiDARDustX dataset, which is specifically\ndesigned for perception tasks under high-dust conditions, such as those\nencountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR\nframes captured by six different LiDAR sensors, each accompanied by 3D bounding\nbox annotations and point cloud semantic segmentation. Notably, over 80% of the\ndataset comprises dust-affected scenes. By utilizing this dataset, we have\nestablished a benchmark for evaluating the performance of state-of-the-art 3D\ndetection and segmentation algorithms. Additionally, we have analyzed the\nimpact of dust on perception accuracy and delved into the causes of these\neffects. The data and further information can be accessed at:\nhttps://github.com/vincentweikey/LiDARDustX."}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively."}
{"id": "2505.21915", "pdf": "https://arxiv.org/pdf/2505.21915", "abs": "https://arxiv.org/abs/2505.21915", "authors": ["Mir Sazzat Hossain", "Ovi Paul", "Md Akil Raihan Iftee", "Rakibul Hasan Rajib", "Abu Bakar Siddik Nayem", "Anis Sarker", "Arshad Momen", "Md. Ashraful Amin", "Amin Ahsan Ali", "AKM Mahbubur Rahman"], "title": "BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 3 tables, Accepted In ICIP 2025", "summary": "Land Use Land Cover (LULC) mapping using deep learning significantly enhances\nthe reliability of LULC classification, aiding in understanding geography,\nsocioeconomic conditions, poverty levels, and urban sprawl. However, the\nscarcity of annotated satellite data, especially in South/East Asian developing\ncountries, poses a major challenge due to limited funding, diverse\ninfrastructures, and dense populations. In this work, we introduce the BD Open\nLULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes\n(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka\nmetropolitan city and its surroundings using high-resolution Bing satellite\nimagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with\nground truth validated through a three-stage process involving GIS experts. We\nbenchmark LULC segmentation using DeepLab V3+ across five major classes and\ncompare performance on Bing and Sentinel-2A imagery. BOLM aims to support\nreliable deep models and domain adaptation tasks, addressing critical LULC\ndataset gaps in South/East Asia."}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces."}
{"id": "2505.21920", "pdf": "https://arxiv.org/pdf/2505.21920", "abs": "https://arxiv.org/abs/2505.21920", "authors": ["Yuanhong Zhang", "Muyao Yuan", "Weizhan Zhang", "Tieliang Gong", "Wen Wen", "Jiangyong Ying", "Weijie Shi"], "title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025 (Highlight)", "summary": "The Segment Anything Model (SAM), a vision foundation model, exhibits\nimpressive zero-shot capabilities in general tasks but struggles in specialized\ndomains. Parameter-efficient fine-tuning (PEFT) is a promising approach to\nunleash the potential of SAM in novel scenarios. However, existing PEFT methods\nfor SAM neglect the domain-invariant relations encoded in the pre-trained\nmodel. To bridge this gap, we propose InfoSAM, an information-theoretic\napproach that enhances SAM fine-tuning by distilling and preserving its\npre-trained segmentation knowledge. Specifically, we formulate the knowledge\ntransfer process as two novel mutual information-based objectives: (i) to\ncompress the domain-invariant relation extracted from pre-trained SAM,\nexcluding pseudo-invariant information as possible, and (ii) to maximize mutual\ninformation between the relational knowledge learned by the teacher\n(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM\nestablishes a robust distillation framework for PEFT of SAM. Extensive\nexperiments across diverse benchmarks validate InfoSAM's effectiveness in\nimproving SAM family's performance on real-world tasks, demonstrating its\nadaptability and superiority in handling specialized scenarios."}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.21943", "pdf": "https://arxiv.org/pdf/2505.21943", "abs": "https://arxiv.org/abs/2505.21943", "authors": ["Wei Lin", "Chenyang Zhao", "Antoni B. Chan"], "title": "Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting", "categories": ["cs.CV"], "comment": "accepted by CVPR-2025(highlight)", "summary": "Point detection has been developed to locate pedestrians in crowded scenes by\ntraining a counter through a point-to-point (P2P) supervision scheme. Despite\nits excellent localization and counting performance, training a point-based\ncounter still faces challenges concerning annotation labor: hundreds to\nthousands of points are required to annotate a single sample capturing a dense\ncrowd. In this paper, we integrate point-based methods into a semi-supervised\ncounting framework based on pseudo-labeling, enabling the training of a counter\nwith only a few annotated samples supplemented by a large volume of\npseudo-labeled data. However, during implementation, the training encounters\nissues as the confidence for pseudo-labels fails to be propagated to background\npixels via the P2P. To tackle this challenge, we devise a point-specific\nactivation map (PSAM) to visually interpret the phenomena occurring during the\nill-posed training. Observations from the PSAM suggest that the feature map is\nexcessively activated by the loss for unlabeled data, causing the decoder to\nmisinterpret these over-activations as pedestrians. To mitigate this issue, we\npropose a point-to-region (P2R) scheme to substitute P2P, which segments out\nlocal regions rather than detects a point corresponding to a pedestrian for\nsupervision. Consequently, pixels in the local region can share the same\nconfidence with the corresponding pseudo points. Experimental results in both\nsemi-supervised counting and unsupervised domain adaptation highlight the\nadvantages of our method, illustrating P2R can resolve issues identified in\nPSAM. The code is available at https://github.com/Elin24/P2RLoss."}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236", "abs": "https://arxiv.org/abs/2505.22236", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure."}
{"id": "2505.21954", "pdf": "https://arxiv.org/pdf/2505.21954", "abs": "https://arxiv.org/abs/2505.21954", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Khoa Quang Nhat Cao", "Yuwei Guo", "Tu Ho Manh Pham", "Tuan Tai Nguyen", "Toan Ngo Duc Vo", "Lucas Poon", "Soochahn Lee", "Yong Jae Lee"], "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present UniTalk, a novel dataset specifically designed for the task of\nactive speaker detection, emphasizing challenging scenarios to enhance model\ngeneralization. Unlike previously established benchmarks such as AVA, which\npredominantly features old movies and thus exhibits significant domain gaps,\nUniTalk focuses explicitly on diverse and difficult real-world conditions.\nThese include underrepresented languages, noisy backgrounds, and crowded scenes\n- such as multiple visible speakers speaking concurrently or in overlapping\nturns. It contains over 44.5 hours of video with frame-level active speaker\nannotations across 48,693 speaking identities, and spans a broad range of video\ntypes that reflect real-world conditions. Through rigorous evaluation, we show\nthat state-of-the-art models, while achieving nearly perfect scores on AVA,\nfail to reach saturation on UniTalk, suggesting that the ASD task remains far\nfrom solved under realistic conditions. Nevertheless, models trained on UniTalk\ndemonstrate stronger generalization to modern \"in-the-wild\" datasets like\nTalkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark\nfor active speaker detection, providing researchers with a valuable resource\nfor developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code"}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240", "abs": "https://arxiv.org/abs/2505.22240", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs."}
{"id": "2505.21955", "pdf": "https://arxiv.org/pdf/2505.21955", "abs": "https://arxiv.org/abs/2505.21955", "authors": ["Insu Lee", "Wooje Park", "Jaeyun Jang", "Minyoung Noh", "Kyuhong Shim", "Byonghyo Shim"], "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive\napplications such as virtual and augmented reality, where first-person\n(egocentric) view captured by head-mounted cameras serves as key input. While\nthis view offers fine-grained cues about user attention and hand-object\ninteractions, their narrow field of view and lack of global context often lead\nto failures on spatially or contextually demanding queries. To address this, we\nintroduce a framework that augments egocentric inputs with third-person\n(exocentric) views, providing complementary information such as global scene\nlayout and object visibility to LVLMs. We present E3VQA, the first benchmark\nfor multi-view question answering with 4K high-quality question-answer pairs\ngrounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a\ntraining-free prompting technique that constructs a unified scene\nrepresentation by integrating scene graphs from three complementary\nperspectives. M3CoT enables LVLMs to reason more effectively across views,\nyielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini\n2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key\nstrengths and limitations of LVLMs in multi-view reasoning and highlights the\nvalue of leveraging both egocentric and exocentric inputs."}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273", "abs": "https://arxiv.org/abs/2505.22273", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency."}
{"id": "2505.21960", "pdf": "https://arxiv.org/pdf/2505.21960", "abs": "https://arxiv.org/abs/2505.21960", "authors": ["Senmao Li", "Lei Wang", "Kai Wang", "Tao Liu", "Jiehang Xie", "Joost van de Weijer", "Fahad Shahbaz Khan", "Shiqi Yang", "Yaxing Wang", "Jian Yang"], "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR2025, Code: https://github.com/sen-mao/Loopfree", "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency."}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow."}
{"id": "2505.21962", "pdf": "https://arxiv.org/pdf/2505.21962", "abs": "https://arxiv.org/abs/2505.21962", "authors": ["Mengjingcheng Mo", "Xinyang Tong", "Jiaxu Leng", "Mingpi Tan", "Jiankang Zheng", "Yiran Liu", "Haosheng Chen", "Ji Gan", "Weisheng Li", "Xinbo Gao"], "title": "A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding", "categories": ["cs.CV"], "comment": null, "summary": "While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage\nfor anomaly detection, they face challenges such as dynamic viewpoints, scale\nvariations, and complex scenes. Existing datasets and methods, mainly designed\nfor fixed ground-level views, struggle to adapt to these conditions, leading to\nsignificant performance drops in drone-view scenarios. To bridge this gap, we\nintroduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric\nbenchmark dataset for aerial anomaly understanding. This dataset covers various\nscenarios and environmental conditions, providing high-resolution real-world\naerial videos with detailed annotations, including anomaly categories,\nframe-level timestamps, region-level bounding boxes, and natural language\nexplanations for causal reasoning. Building on this dataset, we propose\nA2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to\naerial anomaly understanding, enabling a deeper understanding of \"Where\"\nanomalies occur and \"Why\" they happen in aerial frames. To this end, A2Seek-R1\nfirst employs a graph-of-thought (GoT)-guided supervised fine-tuning approach\nto activate the model's latent reasoning capabilities on A2Seek. Then, we\nintroduce Aerial Group Relative Policy Optimization (A-GRPO) to design\nrule-based reward functions tailored to aerial scenarios. Furthermore, we\npropose a novel \"seeking\" mechanism that simulates UAV flight behavior by\ndirecting the model's attention to informative regions. Extensive experiments\ndemonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for\nprediction accuracy and a 13.9% gain in mIoU for anomaly localization,\nexhibiting strong generalization across complex environments and\nout-of-distribution scenarios. Our dataset and code will be released at\nhttps://hayneyday.github.io/A2Seek/."}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293", "abs": "https://arxiv.org/abs/2505.22293", "authors": ["Samuel Frontull", "Thomas Ströhle"], "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora."}
{"id": "2505.21975", "pdf": "https://arxiv.org/pdf/2505.21975", "abs": "https://arxiv.org/abs/2505.21975", "authors": ["Weiguang Zhang", "Huangcheng Lu", "Maizhen Ning", "Xiaowei Huang", "Wei Wang", "Kaizhu Huang", "Qiufeng Wang"], "title": "DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Document dewarping aims to rectify deformations in photographic document\nimages, thus improving text readability, which has attracted much attention and\nmade great progress, but it is still challenging to preserve document\nstructures. Given recent advances in diffusion models, it is natural for us to\nconsider their potential applicability to document dewarping. However, it is\nfar from straightforward to adopt diffusion models in document dewarping due to\ntheir unfaithful control on highly complex document images (e.g.,\n2000$\\times$3000 resolution). In this paper, we propose DvD, the first\ngenerative model to tackle document \\textbf{D}ewarping \\textbf{v}ia a\n\\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level\ndenoising instead of typical pixel-level denoising, generating a mapping for\ndeformation rectification. In addition, we further propose a time-variant\ncondition refinement mechanism to enhance the preservation of document\nstructures. In experiments, we find that current document dewarping benchmarks\ncan not evaluate dewarping models comprehensively. To this end, we present\nAnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark\ncomprising 6,300 real image pairs across three distinct domains, enabling\nfine-grained evaluation of dewarping models. Comprehensive experiments\ndemonstrate that our proposed DvD can achieve state-of-the-art performance with\nacceptable computational efficiency on multiple metrics across various\nbenchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark\nand code will be publicly available."}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296", "abs": "https://arxiv.org/abs/2505.22296", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights."}
{"id": "2505.21996", "pdf": "https://arxiv.org/pdf/2505.21996", "abs": "https://arxiv.org/abs/2505.21996", "authors": ["Taiye Chen", "Xun Hu", "Zihan Ding", "Chi Jin"], "title": "Learning World Models for Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities."}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298", "abs": "https://arxiv.org/abs/2505.22298", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs."}
{"id": "2505.22002", "pdf": "https://arxiv.org/pdf/2505.22002", "abs": "https://arxiv.org/abs/2505.22002", "authors": ["Zijing Hu", "Fengda Zhang", "Kun Kuang"], "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples", "categories": ["cs.CV"], "comment": "Accepted to ICML 2025", "summary": "The practical applications of diffusion models have been limited by the\nmisalignment between generated images and corresponding text prompts. Recent\nstudies have introduced direct preference optimization (DPO) to enhance the\nalignment of these models. However, the effectiveness of DPO is constrained by\nthe issue of visual inconsistency, where the significant visual disparity\nbetween well-aligned and poorly-aligned images prevents diffusion models from\nidentifying which factors contribute positively to alignment during\nfine-tuning. To address this issue, this paper introduces D-Fusion, a method to\nconstruct DPO-trainable visually consistent samples. On one hand, by performing\nmask-guided self-attention fusion, the resulting images are not only\nwell-aligned, but also visually consistent with given poorly-aligned images. On\nthe other hand, D-Fusion can retain the denoising trajectories of the resulting\nimages, which are essential for DPO training. Extensive experiments demonstrate\nthe effectiveness of D-Fusion in improving prompt-image alignment when applied\nto different reinforcement learning algorithms."}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318", "abs": "https://arxiv.org/abs/2505.22318", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge."}
{"id": "2505.22007", "pdf": "https://arxiv.org/pdf/2505.22007", "abs": "https://arxiv.org/abs/2505.22007", "authors": ["Wataru Ikeda", "Masashi Hatano", "Ryosei Hara", "Mariko Isogawa"], "title": "Event-based Egocentric Human Pose Estimation in Dynamic Environment", "categories": ["cs.CV"], "comment": "Accepted at ICIP 2025, Project Page:\n  https://wataru823.github.io/D-EventEgo/", "summary": "Estimating human pose using a front-facing egocentric camera is essential for\napplications such as sports motion analysis, VR/AR, and AI for wearable\ndevices. However, many existing methods rely on RGB cameras and do not account\nfor low-light environments or motion blur. Event-based cameras have the\npotential to address these challenges. In this work, we introduce a novel task\nof human pose estimation using a front-facing event-based camera mounted on the\nhead and propose D-EventEgo, the first framework for this task. The proposed\nmethod first estimates the head poses, and then these are used as conditions to\ngenerate body poses. However, when estimating head poses, the presence of\ndynamic objects mixed with background events may reduce head pose estimation\naccuracy. Therefore, we introduce the Motion Segmentation Module to remove\ndynamic objects and extract background information. Extensive experiments on\nour synthetic event-based dataset derived from EgoBody, demonstrate that our\napproach outperforms our baseline in four out of five evaluation metrics in\ndynamic environments."}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323", "abs": "https://arxiv.org/abs/2505.22323", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "title": "Advancing Expert Specialization for Better MoE", "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."}
{"id": "2505.22011", "pdf": "https://arxiv.org/pdf/2505.22011", "abs": "https://arxiv.org/abs/2505.22011", "authors": ["Menghui Zhang", "Jing Zhang", "Lin Chen", "Li Zhuo"], "title": "Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming", "categories": ["cs.CV"], "comment": null, "summary": "Livestreaming often involves interactions between streamers and objects,\nwhich is critical for understanding and regulating web content. While\nhuman-object interaction (HOI) detection has made some progress in\ngeneral-purpose video downstream tasks, when applied to recognize the\ninteraction behaviors between a streamer and different objects in\nlivestreaming, it tends to focuses too much on the objects and neglects their\ninteractions with the streamer, which leads to object bias. To solve this\nissue, we propose a prototype embedding optimization for human-object\ninteraction detection (PeO-HOI). First, the livestreaming is preprocessed using\nobject detection and tracking techniques to extract features of the\nhuman-object (HO) pairs. Then, prototype embedding optimization is adopted to\nmitigate the effect of object bias on HOI. Finally, after modelling the\nspatio-temporal context between HO pairs, the HOI detection results are\nobtained by the prediction head. The experimental results show that the\ndetection accuracy of the proposed PeO-HOI method has detection accuracies of\n37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset\nVidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset\nBJUT-HOI, which effectively improves the HOI detection performance in\nlivestreaming."}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327", "abs": "https://arxiv.org/abs/2505.22327", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tomás", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders Søgaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research."}
{"id": "2505.22016", "pdf": "https://arxiv.org/pdf/2505.22016", "abs": "https://arxiv.org/abs/2505.22016", "authors": ["Yifei Xia", "Shuchen Weng", "Siqi Yang", "Jingqi Liu", "Chengxuan Zhu", "Minggui Teng", "Zijian Jia", "Han Jiang", "Boxin Shi"], "title": "PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms", "categories": ["cs.CV"], "comment": null, "summary": "Panoramic video generation enables immersive 360{\\deg} content creation,\nvaluable in applications that demand scene-consistent world exploration.\nHowever, existing panoramic video generation models struggle to leverage\npre-trained generative priors from conventional text-to-video models for\nhigh-quality and diverse panoramic videos generation, due to limited dataset\nscale and the gap in spatial feature representations. In this paper, we\nintroduce PanoWan to effectively lift pre-trained text-to-video models to the\npanoramic domain, equipped with minimal modules. PanoWan employs latitude-aware\nsampling to avoid latitudinal distortion, while its rotated semantic denoising\nand padded pixel-wise decoding ensure seamless transitions at longitude\nboundaries. To provide sufficient panoramic videos for learning these lifted\nrepresentations, we contribute PanoVid, a high-quality panoramic video dataset\nwith captions and diverse scenarios. Consequently, PanoWan achieves\nstate-of-the-art performance in panoramic video generation and demonstrates\nrobustness for zero-shot downstream tasks."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22021", "pdf": "https://arxiv.org/pdf/2505.22021", "abs": "https://arxiv.org/abs/2505.22021", "authors": ["Zhihong Tang", "Yang Li"], "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad"}
{"id": "2505.22025", "pdf": "https://arxiv.org/pdf/2505.22025", "abs": "https://arxiv.org/abs/2505.22025", "authors": ["Manchao Bao", "Shengjiang Fang", "Tao Yue", "Xuemei Hu"], "title": "Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Long-distance depth imaging holds great promise for applications such as\nautonomous driving and robotics. Direct time-of-flight (dToF) imaging offers\nhigh-precision, long-distance depth sensing, yet demands ultra-short pulse\nlight sources and high-resolution time-to-digital converters. In contrast,\nindirect time-of-flight (iToF) imaging often suffers from phase wrapping and\nlow signal-to-noise ratio (SNR) as the sensing distance increases. In this\npaper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable\nTime-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth\nimaging. Specifically, the BE-ToF system emits light pulses in burst mode and\nestimates the phase delay of the reflected signal over the entire burst period,\nthereby effectively avoiding the phase wrapping inherent to conventional iToF\nsystems. Moreover, to address the low SNR caused by light attenuation over\nincreasing distances, we propose an end-to-end learnable framework that jointly\noptimizes the coding functions and the depth reconstruction network. A\nspecialized double well function and first-order difference term are\nincorporated into the framework to ensure the hardware implementability of the\ncoding functions. The proposed approach is rigorously validated through\ncomprehensive simulations and real-world prototype experiments, demonstrating\nits effectiveness and practical applicability."}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354", "abs": "https://arxiv.org/abs/2505.22354", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrieß"], "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses."}
{"id": "2505.22031", "pdf": "https://arxiv.org/pdf/2505.22031", "abs": "https://arxiv.org/abs/2505.22031", "authors": ["Hasan Yucedag", "Adam Jatowt"], "title": "Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation", "categories": ["cs.CV"], "comment": "4 Pages,4 figures and 1 system architect", "summary": "This paper introduces Guess the Age of Photos, a web platform engaging users\nin estimating the years of historical photographs through two gamified modes:\nGuess the Year (predicting a single image's year) and Timeline Challenge\n(comparing two images to identify the older). Built with Python, Flask,\nBootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation\nin the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards\nboost engagement. Evaluated with 113 users and 15,473 gameplays, the platform\nearned a 4.25/5 satisfaction rating. Users excelled in relative comparisons\n(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older\ndecades easier to identify. The platform serves as an educational tool,\nfostering historical awareness and analytical skills via interactive\nexploration of visual heritage. Furthermore, the platform provides a valuable\nresource for studying human perception of temporal cues in images and could be\nused to generate annotated data for training and evaluating computer vision\nmodels."}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375", "abs": "https://arxiv.org/abs/2505.22375", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."}
{"id": "2505.22038", "pdf": "https://arxiv.org/pdf/2505.22038", "abs": "https://arxiv.org/abs/2505.22038", "authors": ["Kaiyuan Li", "Xiaoyue Chen", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive performance across\nmulti-modal tasks by encoding images into thousands of tokens. However, the\nlarge number of image tokens results in significant computational overhead, and\nthe use of dynamic high-resolution inputs further increases this burden.\nPrevious approaches have attempted to reduce the number of image tokens through\ntoken pruning, typically by selecting tokens based on attention scores or image\ntoken diversity. Through empirical studies, we observe that existing methods\noften overlook the joint impact of pruning on both the current layer's output\n(local) and the outputs of subsequent layers (global), leading to suboptimal\npruning decisions. To address this challenge, we propose Balanced Token Pruning\n(BTP), a plug-and-play method for pruning vision tokens. Specifically, our\nmethod utilizes a small calibration set to divide the pruning process into\nmultiple stages. In the early stages, our method emphasizes the impact of\npruning on subsequent layers, whereas in the deeper stages, the focus shifts\ntoward preserving the consistency of local outputs. Extensive experiments\nacross various LVLMs demonstrate the broad effectiveness of our approach on\nmultiple benchmarks. Our method achieves a 78% compression rate while\npreserving 96.7% of the original models' performance on average."}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430", "abs": "https://arxiv.org/abs/2505.22430", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."}
{"id": "2505.22039", "pdf": "https://arxiv.org/pdf/2505.22039", "abs": "https://arxiv.org/abs/2505.22039", "authors": ["Shifang Zhao", "Yiheng Lin", "Lu Han", "Yao Zhao", "Yunchao Wei"], "title": "OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "While anomaly detection has made significant progress, generating detailed\nanalyses that incorporate industrial knowledge remains a challenge. To address\nthis gap, we introduce OmniAD, a novel framework that unifies anomaly detection\nand understanding for fine-grained analysis. OmniAD is a multimodal reasoner\nthat combines visual and textual reasoning processes. The visual reasoning\nprovides detailed inspection by leveraging Text-as-Mask Encoding to perform\nanomaly detection through text generation without manually selected thresholds.\nFollowing this, Visual Guided Textual Reasoning conducts comprehensive analysis\nby integrating visual perception. To enhance few-shot generalization, we employ\nan integrated training strategy that combines supervised fine-tuning (SFT) with\nreinforcement learning (GRPO), incorporating three sophisticated reward\nfunctions. Experimental results demonstrate that OmniAD achieves a performance\nof 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and\nGPT-4o. It also shows strong results across multiple anomaly detection\nbenchmarks. These results highlight the importance of enhancing visual\nperception for effective reasoning in anomaly understanding. All codes and\nmodels will be publicly available."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22046", "pdf": "https://arxiv.org/pdf/2505.22046", "abs": "https://arxiv.org/abs/2505.22046", "authors": ["Ashkan Taghipour", "Morteza Ghahremani", "Mohammed Bennamoun", "Farid Boussaid", "Aref Miri Rekavandi", "Zinuo Li", "Qiuhong Ke", "Hamid Laga"], "title": "LatentMove: Towards Complex Human Movement Video Generation", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Image-to-video (I2V) generation seeks to produce realistic motion sequences\nfrom a single reference image. Although recent methods exhibit strong temporal\nconsistency, they often struggle when dealing with complex, non-repetitive\nhuman movements, leading to unnatural deformations. To tackle this issue, we\npresent LatentMove, a DiT-based framework specifically tailored for highly\ndynamic human animation. Our architecture incorporates a conditional control\nbranch and learnable face/body tokens to preserve consistency as well as\nfine-grained details across frames. We introduce Complex-Human-Videos (CHV), a\ndataset featuring diverse, challenging human motions designed to benchmark the\nrobustness of I2V systems. We also introduce two metrics to assess the flow and\nsilhouette consistency of generated videos with their ground truth.\nExperimental results indicate that LatentMove substantially improves human\nanimation quality--particularly when handling rapid, intricate\nmovements--thereby pushing the boundaries of I2V generation. The code, the CHV\ndataset, and the evaluation metrics will be available at https://github.com/\n--."}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501", "abs": "https://arxiv.org/abs/2505.22501", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."}
{"id": "2505.22065", "pdf": "https://arxiv.org/pdf/2505.22065", "abs": "https://arxiv.org/abs/2505.22065", "authors": ["Mikko Impiö", "Philipp M. Rehsen", "Tiina Laamanen", "Arne J. Beermann", "Florian Leese", "Jenni Raitoharju"], "title": "AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents the AquaMonitor dataset, the first large computer vision\ndataset of aquatic invertebrates collected during routine environmental\nmonitoring. While several large species identification datasets exist, they are\nrarely collected using standardized collection protocols, and none focus on\naquatic invertebrates, which are particularly laborious to collect. For\nAquaMonitor, we imaged all specimens from two years of monitoring whenever\nimaging was possible given practical limitations. The dataset enables the\nevaluation of automated identification methods for real-life monitoring\npurposes using a realistically challenging and unbiased setup. The dataset has\n2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry\nmass and size measurements for 1494 specimens, making it also one of the\nlargest biological multi-view and multimodal datasets to date. We define three\nbenchmark tasks and provide strong baselines for these: 1) Monitoring\nbenchmark, reflecting real-life deployment challenges such as open-set\nrecognition, distribution shift, and extreme class imbalance, 2) Classification\nbenchmark, which follows a standard fine-grained visual categorization setup,\nand 3) Few-shot benchmark, which targets classes with only few training\nexamples from very fine-grained categories. Advancements on the Monitoring\nbenchmark can directly translate to improvement of aquatic biodiversity\nmonitoring, which is an important component of regular legislative water\nquality assessment in many countries."}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517", "abs": "https://arxiv.org/abs/2505.22517", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data."}
{"id": "2505.22067", "pdf": "https://arxiv.org/pdf/2505.22067", "abs": "https://arxiv.org/abs/2505.22067", "authors": ["Xinyu Xia", "Xingjun Ma", "Yunfeng Hu", "Ting Qu", "Hong Chen", "Xun Gong"], "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Ensuring robust and generalizable autonomous driving requires not only broad\nscenario coverage but also efficient repair of failure cases, particularly\nthose related to challenging and safety-critical scenarios. However, existing\nscenario generation and selection methods often lack adaptivity and semantic\nrelevance, limiting their impact on performance improvement. In this paper, we\npropose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving\nsystems to self-evolve by repairing failure cases through targeted scenario\nrecommendation. By analyzing performance logs, SERA identifies failure patterns\nand dynamically retrieves semantically aligned scenarios from a structured\nbank. An LLM-based reflection mechanism further refines these recommendations\nto maximize relevance and diversity. The selected scenarios are used for\nfew-shot fine-tuning, enabling targeted adaptation with minimal data.\nExperiments on the benchmark show that SERA consistently improves key metrics\nacross multiple autonomous driving baselines, demonstrating its effectiveness\nand generalizability under safety-critical conditions."}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548", "abs": "https://arxiv.org/abs/2505.22548", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."}
{"id": "2505.22079", "pdf": "https://arxiv.org/pdf/2505.22079", "abs": "https://arxiv.org/abs/2505.22079", "authors": ["Hanbin Ko", "Chang-Min Park"], "title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis", "categories": ["cs.CV"], "comment": "16 pages (8 main, 2 references, 6 appendix), 13 figures. Accepted to\n  CVPR 2025. This author-accepted manuscript includes an expanded ethics/data\n  user agreement section. The final version will appear in the Proceedings of\n  CVPR 2025", "summary": "The development of large-scale image-text pair datasets has significantly\nadvanced self-supervised learning in Vision-Language Processing (VLP). However,\ndirectly applying general-domain architectures such as CLIP to medical data\npresents challenges, particularly in handling negations and addressing the\ninherent data imbalance of medical datasets. To address these issues, we\npropose a novel approach that integrates clinically-enhanced dynamic soft\nlabels and medical graphical alignment, thereby improving clinical\ncomprehension and the applicability of contrastive loss in medical contexts.\nFurthermore, we introduce negation-based hard negatives to deepen the model's\nunderstanding of the complexities of clinical language. Our approach is easily\nintegrated into the medical CLIP training pipeline and achieves\nstate-of-the-art performance across multiple tasks, including zero-shot,\nfine-tuned classification, and report retrieval. To comprehensively evaluate\nour model's capacity for understanding clinical language, we introduce\nCXR-Align, a benchmark uniquely designed to evaluate the understanding of\nnegation and clinical information within chest X-ray (CXR) datasets.\nExperimental results demonstrate that our proposed methods are straightforward\nto implement and generalize effectively across contrastive learning frameworks,\nenhancing medical VLP capabilities and advancing clinical language\nunderstanding in medical imaging."}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."}
{"id": "2505.22084", "pdf": "https://arxiv.org/pdf/2505.22084", "abs": "https://arxiv.org/abs/2505.22084", "authors": ["Julie Tores", "Elisa Ancarani", "Lucile Sassatelli", "Hui-Yin Wu", "Clement Bergman", "Lea Andolfi", "Victor Ecrement", "Remy Sun", "Frederic Precioso", "Thierry Devars", "Magali Guaresi", "Virginie Julliard", "Sarah Lecossais"], "title": "MObyGaze: a film dataset of multimodal objectification densely annotated by experts", "categories": ["cs.CV"], "comment": null, "summary": "Characterizing and quantifying gender representation disparities in\naudiovisual storytelling contents is necessary to grasp how stereotypes may\nperpetuate on screen. In this article, we consider the high-level construct of\nobjectification and introduce a new AI task to the ML community: characterize\nand quantify complex multimodal (visual, speech, audio) temporal patterns\nproducing objectification in films. Building on film studies and psychology, we\ndefine the construct of objectification in a structured thesaurus involving 5\nsub-constructs manifesting through 11 concepts spanning 3 modalities. We\nintroduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20\nmovies annotated densely by experts for objectification levels and concepts\nover freely delimited segments: it amounts to 6072 segments over 43 hours of\nvideo with fine-grained localization and categorization. We formulate different\nlearning tasks, propose and investigate best ways to learn from the diversity\nof labels among a low number of annotators, and benchmark recent vision, text\nand audio models, showing the feasibility of the task. We make our code and our\ndataset available to the community and described in the Croissant format:\nhttps://anonymous.4open.science/r/MObyGaze-F600/."}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563", "abs": "https://arxiv.org/abs/2505.22563", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."}
{"id": "2505.22089", "pdf": "https://arxiv.org/pdf/2505.22089", "abs": "https://arxiv.org/abs/2505.22089", "authors": ["San Jiang", "Kan You", "Wanshou Jiang", "Qingquan Li"], "title": "Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching dominats the time costs in structure from motion (SfM). The\nprimary contribution of this study is a GPU data schedule algorithm for\nefficient feature matching of Unmanned aerial vehicle (UAV) images. The core\nidea is to divide the whole dataset into blocks based on the matrix band\nreduction (MBR) and achieve efficient feature matching via GPU-accelerated\ncascade hashing. First, match pairs are selected by using an image retrieval\ntechnique, which converts images into global descriptors and searches\nhigh-dimension nearest neighbors with graph indexing. Second, compact image\nblocks are iteratively generated from a MBR-based data schedule strategy, which\nexploits image connections to avoid redundant data IO (input/output) burden and\nincreases the usage of GPU computing power. Third, guided by the generated\nimage blocks, feature matching is executed sequentially within the framework of\nGPU-accelerated cascade hashing, and initial candidate matches are refined by\ncombining a local geometric constraint and RANSAC-based global verification.\nFor further performance improvement, these two seps are designed to execute\nparallelly in GPU and CPU. Finally, the performance of the proposed solution is\nevaluated by using large-scale UAV datasets. The results demonstrate that it\nincreases the efficiency of feature matching with speedup ratios ranging from\n77.0 to 100.0 compared with KD-Tree based matching methods, and achieves\ncomparable accuracy in relative and absolute bundle adjustment (BA). The\nproposed algorithm is an efficient solution for feature matching of UAV images."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22098", "pdf": "https://arxiv.org/pdf/2505.22098", "abs": "https://arxiv.org/abs/2505.22098", "authors": ["Junhuan Liu", "San Jiang", "Wei Ge", "Wei Huang", "Bingxuan Guo", "Qingquan Li"], "title": "UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images", "categories": ["cs.CV"], "comment": null, "summary": "The primary contribution of this paper is a challenging benchmark dataset,\nUAVPairs, and a training pipeline designed for match pair retrieval of\nlarge-scale UAV images. First, the UAVPairs dataset, comprising 21,622\nhigh-resolution images across 30 diverse scenes, is constructed; the 3D points\nand tracks generated by SfM-based 3D reconstruction are employed to define the\ngeometric similarity of image pairs, ensuring genuinely matchable image pairs\nare used for training. Second, to solve the problem of expensive mining cost\nfor global hard negative mining, a batched nontrivial sample mining strategy is\nproposed, leveraging the geometric similarity and multi-scene structure of the\nUAVPairs to generate training samples as to accelerate training. Third,\nrecognizing the limitation of pair-based losses, the ranked list loss is\ndesigned to improve the discrimination of image retrieval models, which\noptimizes the global similarity structure constructed from the positive set and\nnegative set. Finally, the effectiveness of the UAVPairs dataset and training\npipeline is validated through comprehensive experiments on three distinct\nlarge-scale UAV datasets. The experiment results demonstrate that models\ntrained with the UAVPairs dataset and the ranked list loss achieve\nsignificantly improved retrieval accuracy compared to models trained on\nexisting datasets or with conventional losses. Furthermore, these improvements\ntranslate to enhanced view graph connectivity and higher quality of\nreconstructed 3D models. The models trained by the proposed approach perform\nmore robustly compared with hand-crafted global features, particularly in\nchallenging repetitively textured scenes and weakly textured scenes. For match\npair retrieval of large-scale UAV images, the trained image retrieval models\noffer an effective solution. The dataset would be made publicly available at\nhttps://github.com/json87/UAVPairs."}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."}
{"id": "2505.22099", "pdf": "https://arxiv.org/pdf/2505.22099", "abs": "https://arxiv.org/abs/2505.22099", "authors": ["Wenwen Qiang", "Ziyin Gu", "Lingyu Si", "Jiangmeng Li", "Changwen Zheng", "Fuchun Sun", "Hui Xiong"], "title": "On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we addressed the limitation of relying solely on distribution\nalignment and source-domain empirical risk minimization in Unsupervised Domain\nAdaptation (UDA). Our information-theoretic analysis showed that this standard\nadversarial-based framework neglects the discriminability of target-domain\nfeatures, leading to suboptimal performance. To bridge this\ntheoretical-practical gap, we defined \"good representation learning\" as\nguaranteeing both transferability and discriminability, and proved that an\nadditional loss term targeting target-domain discriminability is necessary.\nBuilding on these insights, we proposed a novel adversarial-based UDA framework\nthat explicitly integrates a domain alignment objective with a\ndiscriminability-enhancing constraint. Instantiated as Domain-Invariant\nRepresentation Learning with Global and Local Consistency (RLGLC), our method\nleverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)\nto address class imbalance and semantic dimension weighting, and employs a\nlocal consistency mechanism to preserve fine-grained target-domain\ndiscriminative information. Extensive experiments across multiple benchmark\ndatasets demonstrate that RLGLC consistently surpasses state-of-the-art\nmethods, confirming the value of our theoretical perspective and underscoring\nthe necessity of enforcing both transferability and discriminability in\nadversarial-based UDA."}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582", "abs": "https://arxiv.org/abs/2505.22582", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."}
{"id": "2505.22105", "pdf": "https://arxiv.org/pdf/2505.22105", "abs": "https://arxiv.org/abs/2505.22105", "authors": ["Hang Chen", "Maoyuan Ye", "Peng Yang", "Haibin He", "Juhua Liu", "Bo Du"], "title": "Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Power transmission corridor hazard segmentation (PTCHS) aims to separate\ntransmission equipment and surrounding hazards from complex background,\nconveying great significance to maintaining electric power transmission safety.\nRecently, the Segment Anything Model (SAM) has emerged as a foundational vision\nmodel and pushed the boundaries of segmentation tasks. However, SAM struggles\nto deal with the target objects in complex transmission corridor scenario,\nespecially those with fine structure. In this paper, we propose ELE-SAM,\nadapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt\nAdapter to achieve better prompt tokens via incorporating global-local features\nand focusing more on key regions. Subsequently, to tackle the hazard objects\nwith fine structure in complex background, we design a High-Fidelity Mask\nDecoder by leveraging multi-granularity mask features and then scaling them to\na higher resolution. Moreover, to train ELE-SAM and advance this field, we\nconstruct the ELE-40K benchmark, the first large-scale and real-world dataset\nfor PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K\ndemonstrate the superior performance that ELE-SAM outperforms the baseline\nmodel with the average 16.8% mIoU and 20.6% mBIoU performance improvement.\nMoreover, compared with the state-of-the-art method on HQSeg-44K, the average\n2.9% mIoU and 3.8% mBIoU absolute improvements further validate the\neffectiveness of our method on high-quality generic object segmentation. The\nsource code and dataset are available at https://github.com/Hhaizee/ELE-SAM."}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586", "abs": "https://arxiv.org/abs/2505.22586", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "title": "Precise In-Parameter Concept Erasure in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."}
{"id": "2505.22111", "pdf": "https://arxiv.org/pdf/2505.22111", "abs": "https://arxiv.org/abs/2505.22111", "authors": ["Woonho Ko", "Jin Bok Park", "Il Yong Chun"], "title": "Autoregression-free video prediction using diffusion model for mitigating error propagation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 2 tables", "summary": "Existing long-term video prediction methods often rely on an autoregressive\nvideo prediction mechanism. However, this approach suffers from error\npropagation, particularly in distant future frames. To address this limitation,\nthis paper proposes the first AutoRegression-Free (ARFree) video prediction\nframework using diffusion models. Different from an autoregressive video\nprediction mechanism, ARFree directly predicts any future frame tuples from the\ncontext frame tuple. The proposed ARFree consists of two key components: 1) a\nmotion prediction module that predicts a future motion using motion feature\nextracted from the context frame tuple; 2) a training method that improves\nmotion continuity and contextual consistency between adjacent future frame\ntuples. Our experiments with two benchmark datasets show that the proposed\nARFree video prediction framework outperforms several state-of-the-art video\nprediction methods."}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."}
{"id": "2505.22126", "pdf": "https://arxiv.org/pdf/2505.22126", "abs": "https://arxiv.org/abs/2505.22126", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities."}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618", "abs": "https://arxiv.org/abs/2505.22618", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2505.22128", "pdf": "https://arxiv.org/pdf/2505.22128", "abs": "https://arxiv.org/abs/2505.22128", "authors": ["Alejandro D. Mousist"], "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work addresses mechanical defocus in Earth observation images from the\nIMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted\nto space-based edge computing constraints. Leveraging Sentinel-2 data, our\nmethod estimates the defocus kernel and trains a restoration model within a GAN\nframework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and\nPSNR by 25.00%, confirming the model's ability to recover lost details when the\noriginal clean image is known. On IMAGIN-e, where no reference images exist,\nperceptual quality metrics indicate a substantial enhancement, with NIQE\nimproving by 60.66% and BRISQUE by 48.38%, validating real-world onboard\nrestoration. The approach is currently deployed aboard the IMAGIN-e mission,\ndemonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing\nconstraints, the method enables applications such as water body segmentation\nand contour detection while maintaining processing viability despite resource\nlimitations."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22129", "pdf": "https://arxiv.org/pdf/2505.22129", "abs": "https://arxiv.org/abs/2505.22129", "authors": ["Jinhong Ni", "Chang-Bin Zhang", "Qiang Zhang", "Jing Zhang"], "title": "What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?", "categories": ["cs.CV"], "comment": null, "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630", "abs": "https://arxiv.org/abs/2505.22630", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.22141", "pdf": "https://arxiv.org/pdf/2505.22141", "abs": "https://arxiv.org/abs/2505.22141", "authors": ["Guanwen Feng", "Zhiyuan Ma", "Yunan Li", "Junwei Jing", "Jiahao Yang", "Qiguang Miao"], "title": "FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is crucial for achieving deep personalization and expanding the\nrange of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, the flexible adjustment of visual attributes-such as\nhairstyle, accessories, and subtle facial features is essential for aligning\nwith user preferences, reflecting diverse brand identities, and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method\noutperforms state-of-the-art approaches in lip-sync accuracy, video quality,\nand attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/"}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22143", "pdf": "https://arxiv.org/pdf/2505.22143", "abs": "https://arxiv.org/abs/2505.22143", "authors": ["Fengyun Wang", "Sicheng Yu", "Jiawei Wu", "Jinhui Tang", "Hanwang Zhang", "Qianru Sun"], "title": "3D Question Answering via only 2D Vision-Language Models", "categories": ["cs.CV"], "comment": "ICML2025", "summary": "Large vision-language models (LVLMs) have significantly advanced numerous\nfields. In this work, we explore how to harness their potential to address 3D\nscene understanding tasks, using 3D question answering (3D-QA) as a\nrepresentative example. Due to the limited training data in 3D, we do not train\nLVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a\n3D point cloud and feed them into 2D models to answer a given question. When\nthe 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters\nthe most. We propose cdViews, a novel approach to automatically selecting\ncritical and diverse Views for 3D-QA. cdViews consists of two key components:\nviewSelector prioritizing critical views based on their potential to provide\nanswer-specific information, and viewNMS enhancing diversity by removing\nredundant views based on spatial overlap. We evaluate cdViews on the\nwidely-used ScanQA and SQA benchmarks, demonstrating that it achieves\nstate-of-the-art performance in 3D-QA while relying solely on 2D models without\nfine-tuning. These findings support our belief that 2D LVLMs are currently the\nmost effective alternative (of the resource-intensive 3D LVLMs) for addressing\n3D tasks."}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645", "abs": "https://arxiv.org/abs/2505.22645", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648", "abs": "https://arxiv.org/abs/2505.22648", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebDancer: Towards Autonomous Information Seeking Agency", "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.22154", "pdf": "https://arxiv.org/pdf/2505.22154", "abs": "https://arxiv.org/abs/2505.22154", "authors": ["Chao Tian", "Chao Yang", "Guoqing Zhu", "Qiang Wang", "Zhenyu He"], "title": "Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance", "categories": ["cs.CV"], "comment": null, "summary": "RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653", "abs": "https://arxiv.org/abs/2505.22653", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."}
{"id": "2505.22167", "pdf": "https://arxiv.org/pdf/2505.22167", "abs": "https://arxiv.org/abs/2505.22167", "authors": ["Weilun Feng", "Chuanguang Yang", "Haotong Qin", "Xiangqi Li", "Yu Wang", "Zhulin An", "Libo Huang", "Boyu Diao", "Zixiang Zhao", "Yongjun Xu", "Michele Magno"], "title": "Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted to ICML2025", "summary": "Diffusion transformers (DiT) have demonstrated exceptional performance in\nvideo generation. However, their large number of parameters and high\ncomputational complexity limit their deployment on edge devices. Quantization\ncan reduce storage requirements and accelerate inference by lowering the\nbit-width of model parameters. Yet, existing quantization methods for image\ngeneration models do not generalize well to video generation tasks. We identify\ntwo primary challenges: the loss of information during quantization and the\nmisalignment between optimization objectives and the unique requirements of\nvideo generation. To address these challenges, we present Q-VDiT, a\nquantization framework specifically designed for video DiT models. From the\nquantization perspective, we propose the Token-aware Quantization Estimator\n(TQE), which compensates for quantization errors in both the token and feature\ndimensions. From the optimization perspective, we introduce Temporal\nMaintenance Distillation (TMD), which preserves the spatiotemporal correlations\nbetween frames and enables the optimization of each frame with respect to the\noverall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,\nsetting a new benchmark and outperforming current state-of-the-art quantization\nmethods by 1.9$\\times$. Code will be available at\nhttps://github.com/cantbebetter2/Q-VDiT."}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661", "abs": "https://arxiv.org/abs/2505.22661", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."}
{"id": "2505.22195", "pdf": "https://arxiv.org/pdf/2505.22195", "abs": "https://arxiv.org/abs/2505.22195", "authors": ["Guoan Xu", "Wenfeng Huang", "Wenjing Jia", "Jiamao Li", "Guangwei Gao", "Guo-Jun Qi"], "title": "S2AFormer: Strip Self-Attention for Efficient Vision Transformer", "categories": ["cs.CV"], "comment": "12 pages, 6 figures, 8 tables", "summary": "Vision Transformer (ViT) has made significant advancements in computer\nvision, thanks to its token mixer's sophisticated ability to capture global\ndependencies between all tokens. However, the quadratic growth in computational\ndemands as the number of tokens increases limits its practical efficiency.\nAlthough recent methods have combined the strengths of convolutions and\nself-attention to achieve better trade-offs, the expensive pairwise token\naffinity and complex matrix operations inherent in self-attention remain a\nbottleneck. To address this challenge, we propose S2AFormer, an efficient\nVision Transformer architecture featuring novel Strip Self-Attention (SSA). We\ndesign simple yet effective Hybrid Perception Blocks (HPBs) to effectively\nintegrate the local perception capabilities of CNNs with the global context\nmodeling of Transformer's attention mechanisms. A key innovation of SSA lies in\nits reducing the spatial dimensions of $K$ and $V$ while compressing the\nchannel dimensions of $Q$ and $K$. This design significantly reduces\ncomputational overhead while preserving accuracy, striking an optimal balance\nbetween efficiency and effectiveness. We evaluate the robustness and efficiency\nof S2AFormer through extensive experiments on multiple vision benchmarks,\nincluding ImageNet-1k for image classification, ADE20k for semantic\nsegmentation, and COCO for object detection and instance segmentation. Results\ndemonstrate that S2AFormer achieves significant accuracy gains with superior\nefficiency in both GPU and non-GPU environments, making it a strong candidate\nfor efficient vision Transformers."}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662", "abs": "https://arxiv.org/abs/2505.22662", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."}
{"id": "2505.22200", "pdf": "https://arxiv.org/pdf/2505.22200", "abs": "https://arxiv.org/abs/2505.22200", "authors": ["Darshana Saravanan", "Makarand Tapaswi", "Vineet Gandhi"], "title": "Investigating Mechanisms for In-Context Vision Language Binding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MIV at CVPRW 2025 (Oral)", "summary": "To understand a prompt, Vision-Language models (VLMs) must perceive the\nimage, comprehend the text, and build associations within and across both\nmodalities. For instance, given an 'image of a red toy car', the model should\nassociate this image to phrases like 'car', 'red toy', 'red object', etc. Feng\nand Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the\nentity and its corresponding attribute tokens share a Binding ID in the model\nactivations. We investigate this for image-text binding in VLMs using a\nsynthetic dataset and task that requires models to associate 3D objects in an\nimage with their descriptions in the text. Our experiments demonstrate that\nVLMs assign a distinct Binding ID to an object's image tokens and its textual\nreferences, enabling in-context association."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.22209", "pdf": "https://arxiv.org/pdf/2505.22209", "abs": "https://arxiv.org/abs/2505.22209", "authors": ["Naomi Kombol", "Ivan Martinović", "Siniša Šegvić"], "title": "A Survey on Training-free Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation is one of the most fundamental tasks in image\nunderstanding with a long history of research, and subsequently a myriad of\ndifferent approaches. Traditional methods strive to train models up from\nscratch, requiring vast amounts of computational resources and training data.\nIn the advent of moving to open-vocabulary semantic segmentation, which asks\nmodels to classify beyond learned categories, large quantities of finely\nannotated data would be prohibitively expensive. Researchers have instead\nturned to training-free methods where they leverage existing models made for\ntasks where data is more easily acquired. Specifically, this survey will cover\nthe history, nuance, idea development and the state-of-the-art in training-free\nopen-vocabulary semantic segmentation that leverages existing multi-modal\nclassification models. We will first give a preliminary on the task definition\nfollowed by an overview of popular model archetypes and then spotlight over 30\napproaches split into broader research branches: purely CLIP-based, those\nleveraging auxiliary visual foundation models and ones relying on generative\nmethods. Subsequently, we will discuss the limitations and potential problems\nof current research, as well as provide some underexplored ideas for future\nstudy. We believe this survey will serve as a good onboarding read to new\nresearchers and spark increased interest in the area."}
{"id": "2505.21510", "pdf": "https://arxiv.org/pdf/2505.21510", "abs": "https://arxiv.org/abs/2505.21510", "authors": ["Chundra Cathcart"], "title": "Complexity counts: global and local perspectives on Indo-Aryan numeral systems", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and\nBengali are highly unusual in that unlike most numeral systems (e.g., those of\nEnglish, Chinese, etc.), forms referring to 1--99 are highly non-transparent\nand are cannot be constructed using straightforward rules. As an example,\nHindi/Urdu *iky\\=anve* `91' is not decomposable into the composite elements\n*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This\npaper situates Indo-Aryan languages within the typology of cross-linguistic\nnumeral systems, and explores the linguistic and non-linguistic factors that\nmay be responsible for the persistence of complex systems in these languages.\nUsing cross-linguistic data from multiple databases, we develop and employ a\nnumber of cross-linguistically applicable metrics to quantifies the complexity\nof languages' numeral systems, and demonstrate that Indo-Aryan languages have\ndecisively more complex numeral systems than the world's languages as a whole,\nthough individual Indo-Aryan languages differ from each other in terms of the\ncomplexity of the patterns they display. We investigate the factors (e.g.,\nreligion, geographic isolation, etc.) that underlie complexity in numeral\nsystems, with a focus on South Asia, in an attempt to develop an account of why\ncomplex numeral systems developed and persisted in certain Indo-Aryan languages\nbut not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems\nadhere to certain general pressures toward efficient communication found\ncross-linguistically, despite their high complexity. We call for this somewhat\noverlooked dimension of complexity to be taken seriously when discussing\ngeneral variation in cross-linguistic numeral systems."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR."}
{"id": "2505.22226", "pdf": "https://arxiv.org/pdf/2505.22226", "abs": "https://arxiv.org/abs/2505.22226", "authors": ["Xuyang Zhang", "Xi Zhang", "Liang Chen", "Hao Shi", "Qingshan Guo"], "title": "Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "Recent studies have revealed the immense potential of Hadamard product in\nenhancing network representational capacity and dimensional compression.\nHowever, despite its theoretical promise, this technique has not been\nsystematically explored or effectively applied in practice, leaving its full\ncapabilities underdeveloped. In this work, we first analyze and identify the\nadvantages of Hadamard product over standard convolutional operations in\ncross-channel interaction and channel expansion. Building upon these insights,\nwe propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),\nwhich leverages adaptive cross-channel Hadamard products for high-dimensional\nchannel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive\nNetwork), a lightweight network backbone for visual tasks, which is\ndemonstrated through experiments that it achieves an unprecedented balance\nbetween inference speed and accuracy through our proposed module."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.22228", "pdf": "https://arxiv.org/pdf/2505.22228", "abs": "https://arxiv.org/abs/2505.22228", "authors": ["Haibin He", "Jing Zhang", "Maoyuan Ye", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2401.07080", "summary": "Video text spotting (VTS) extends image text spotting (ITS) by adding text\ntracking, significantly increasing task complexity. Despite progress in VTS,\nexisting methods still fall short of the performance seen in ITS. This paper\nidentifies a key limitation in current video text spotters: limited recognition\ncapability, even after extensive end-to-end training. To address this, we\npropose GoMatching++, a parameter- and data-efficient method that transforms an\noff-the-shelf image text spotter into a video specialist. The core idea lies in\nfreezing the image text spotter and introducing a lightweight, trainable\ntracker, which can be optimized efficiently with minimal training data. Our\napproach includes two key components: (1) a rescoring mechanism to bridge the\ndomain gap between image and video data, and (2) the LST-Matcher, which\nenhances the frozen image text spotter's ability to handle video text. We\nexplore various architectures for LST-Matcher to ensure efficiency in both\nparameters and training data. As a result, GoMatching++ sets new performance\nrecords on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,\nwhile significantly reducing training costs. To address the lack of curved text\ndatasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%\ncurved text with detailed annotations. We also provide a comprehensive\nstatistical analysis and experimental results for ArTVideo. We believe that\nGoMatching++ and the ArTVideo benchmark will drive future advancements in video\ntext spotting. The source code, models and dataset are publicly available at\nhttps://github.com/Hxyz-123/GoMatching."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.22230", "pdf": "https://arxiv.org/pdf/2505.22230", "abs": "https://arxiv.org/abs/2505.22230", "authors": ["Zhisong Wang", "Yiwen Ye", "Ziyang Chen", "Yong Xia"], "title": "Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, MICCAI 2025 (Early Accept)", "summary": "Weakly supervised semantic segmentation (WSSS) in medical imaging struggles\nwith effectively using sparse annotations. One promising direction for WSSS\nleverages gaze annotations, captured via eye trackers that record regions of\ninterest during diagnostic procedures. However, existing gaze-based methods,\nsuch as GazeMedSeg, do not fully exploit the rich information embedded in gaze\ndata. In this paper, we propose GradTrack, a framework that utilizes\nphysicians' gaze track, including fixation points, durations, and temporal\norder, to enhance WSSS performance. GradTrack comprises two key components:\nGaze Track Map Generation and Track Attention, which collaboratively enable\nprogressive feature refinement through multi-level gaze supervision during the\ndecoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets\ndemonstrate that GradTrack consistently outperforms existing gaze-based\nmethods, achieving Dice score improvements of 3.21\\% and 2.61\\%, respectively.\nMoreover, GradTrack significantly narrows the performance gap with fully\nsupervised models such as nnUNet."}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs."}
{"id": "2505.22246", "pdf": "https://arxiv.org/pdf/2505.22246", "abs": "https://arxiv.org/abs/2505.22246", "authors": ["Nedko Savov", "Naser Kazemi", "Deheng Zhang", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models", "categories": ["cs.CV"], "comment": null, "summary": "World models have recently become promising tools for predicting realistic\nvisuals based on actions in complex environments. However, their reliance on a\nshort sequence of observations causes them to quickly lose track of context. As\na result, visual consistency breaks down after just a few steps, and generated\nscenes no longer reflect information seen earlier. This limitation of the\nstate-of-the-art diffusion-based world models comes from their lack of a\nlasting environment state. To address this problem, we introduce\nStateSpaceDiffuser, where a diffusion model is enabled to perform on\nlong-context tasks by integrating a sequence representation from a state-space\nmodel (Mamba), representing the entire interaction history. This design\nrestores long-term memory without sacrificing the high-fidelity synthesis of\ndiffusion models. To rigorously measure temporal consistency, we develop an\nevaluation protocol that probes a model's ability to reinstantiate seen content\nin extended rollouts. Comprehensive experiments show that StateSpaceDiffuser\nsignificantly outperforms a strong diffusion-only baseline, maintaining a\ncoherent visual context for an order of magnitude more steps. It delivers\nconsistent views in both a 2D maze navigation and a complex 3D environment.\nThese results establish that bringing state-space representations into\ndiffusion models is highly effective in demonstrating both visual details and\nlong-term memory."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.22250", "pdf": "https://arxiv.org/pdf/2505.22250", "abs": "https://arxiv.org/abs/2505.22250", "authors": ["Mingzhuang Wang", "Yvyang Li", "Xiyang Zhang", "Fei Tan", "Qi Shi", "Guotao Zhang", "Siqi Chen", "Yufei Liu", "Lei Lei", "Ming Zhou", "Qiang Lin", "Hongqiang Yang"], "title": "YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Coral reefs, crucial for sustaining marine biodiversity and ecological\nprocesses (e.g., nutrient cycling, habitat provision), face escalating threats,\nunderscoring the need for efficient monitoring. Coral reef ecological\nmonitoring faces dual challenges of low efficiency in manual analysis and\ninsufficient segmentation accuracy in complex underwater scenarios. This study\ndevelops the YH-OSI system, establishing an intelligent framework centered on\nthe Multimodal Large Model (MLLM) for \"object detection-semantic\nsegmentation-prior input\". The system uses the object detection module\n(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the\nsegment module to complete pixel-level segmentation in low-light and densely\noccluded scenarios. The segmentation masks and finetuned classification\ninstructions are fed into the Qwen2-VL-based multimodal model as prior inputs,\nachieving a genus-level classification accuracy of 88% and simultaneously\nextracting core ecological metrics. Meanwhile, the system retains the\nscalability of the multimodal model through standardized interfaces, laying a\nfoundation for future integration into multimodal agent-based underwater robots\nand supporting the full-process automation of \"image acquisition-prior\ngeneration-real-time analysis.\""}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.22259", "pdf": "https://arxiv.org/pdf/2505.22259", "abs": "https://arxiv.org/abs/2505.22259", "authors": ["Kiyoon Jeong", "Jaehyuk Heo", "Junyeong Son", "Pilsung Kang"], "title": "Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) in images is an approach that can detect\nanomalies without access to normal samples, which can be beneficial in various\nrealistic scenarios where model training is not possible. However, existing\nZSAD research has shown limitations by either not considering domain adaptation\nof general-purpose backbone models to anomaly detection domains or by\nimplementing only partial adaptation to some model components. In this paper,\nwe propose HeadCLIP to overcome these limitations by effectively adapting both\ntext and image encoders to the domain. HeadCLIP generalizes the concepts of\nnormality and abnormality through learnable prompts in the text encoder, and\nintroduces learnable head weights to the image encoder to dynamically adjust\nthe features held by each attention head according to domain characteristics.\nAdditionally, we maximize the effect of domain adaptation by introducing a\njoint anomaly score that utilizes domain-adapted pixel-level information for\nimage-level anomaly detection. Experimental results using multiple real\ndatasets in both industrial and medical domains show that HeadCLIP outperforms\nexisting ZSAD techniques at both pixel and image levels. In the industrial\ndomain, improvements of up to 4.9%p in pixel-level mean anomaly detection score\n(mAD) and up to 3.0%p in image-level mAD were achieved, with similar\nimprovements (3.2%p, 3.1%p) in the medical domain."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.22279", "pdf": "https://arxiv.org/pdf/2505.22279", "abs": "https://arxiv.org/abs/2505.22279", "authors": ["Wenjun Lu", "Haodong Chen", "Anqi Yi", "Yuk Ying Chung", "Zhiyong Wang", "Kun Hu"], "title": "Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis is a fundamental task in 3D computer vision that aims to\nreconstruct realistic images from a set of posed input views. However,\nreconstruction quality degrades significantly under sparse-view conditions due\nto limited geometric cues. Existing methods, such as Neural Radiance Fields\n(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from\nblurred details and structural artifacts when trained with insufficient views.\nRecent works have identified the quality of rendered depth as a key factor in\nmitigating these artifacts, as it directly affects geometric accuracy and view\nconsistency. In this paper, we address these challenges by introducing\nHierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that\nprogressively refines geometry from coarse to fine levels. Central to HDGS is a\nnovel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and\nestimated monocular depths across multiple spatial scales. By enforcing\nmulti-scale depth consistency, our method substantially improves structural\nfidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU\nbenchmarks demonstrate that HDGS achieves state-of-the-art performance under\nsparse-view settings while maintaining efficient and high-quality rendering"}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy."}
{"id": "2505.22284", "pdf": "https://arxiv.org/pdf/2505.22284", "abs": "https://arxiv.org/abs/2505.22284", "authors": ["Junyu Fan", "Chuanlin Liao", "Yi Lin"], "title": "From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to\nachieve image restoration caused by multiple degradation patterns via a single\nmodel with unified parameters. Although existing AiOIR approaches obtain\npromising performance in closed and controlled scenarios, they still suffered\nfrom considerable performance reduction in real-world scenarios since the gap\nof data distributions between the training samples (source domain) and\nreal-world test samples (target domain) can lead inferior degradation awareness\nability. To address this issue, a Unified Domain-Adaptive Image Restoration\n(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the\nlearned knowledge from source domain to target domain. To improve the\ndegradation identification, a codebook is designed to learn a group of discrete\nembeddings to denote the degradation patterns, and the cross-sample contrastive\nlearning mechanism is further proposed to capture shared features from\ndifferent samples of certain degradation. To bridge the data gap, a domain\nadaptation strategy is proposed to build the feature projection between the\nsource and target domains by dynamically aligning their codebook embeddings,\nand a correlation alignment-based test-time adaptation mechanism is designed to\nfine-tune the alignment discrepancies by tightening the degradation embeddings\nto the corresponding cluster center in the source domain. Experimental results\non 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art\nperformance for the AiOIR task. Most importantly, the feature cluster validate\nthe degradation identification under unknown conditions, and qualitative\ncomparisons showcase robust generalization to real-world scenarios."}
{"id": "2505.21753", "pdf": "https://arxiv.org/pdf/2505.21753", "abs": "https://arxiv.org/abs/2505.21753", "authors": ["Roberto Ulloa", "Eve M. Zucker", "Daniel Bultmann", "David J. Simon", "Mykola Makhortykh"], "title": "From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) can influence how\nhistorical narratives are disseminated and perceived. This study explores the\nimplications of LLMs' responses on the representation of mass atrocity memory,\nexamining whether generative AI systems contribute to prosthetic memory, i.e.,\nmediated experiences of historical events, or to what we term \"prosthetic\ndenial,\" the AI-mediated erasure or distortion of atrocity memories. We argue\nthat LLMs function as interfaces that can elicit prosthetic memories and,\ntherefore, act as experiential sites for memory transmission, but also\nintroduce risks of denialism, particularly when their outputs align with\ncontested or revisionist narratives. To empirically assess these risks, we\nconducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and\nGemini) across four historical case studies: the Holodomor, the Holocaust, the\nCambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model\nwas prompted with questions addressing common denialist claims in English and\nan alternative language relevant to each case (Ukrainian, German, Khmer, and\nFrench). Our findings reveal that while LLMs generally produce accurate\nresponses for widely documented events like the Holocaust, significant\ninconsistencies and susceptibility to denialist framings are observed for more\nunderrepresented cases like the Cambodian Genocide. The disparities highlight\nthe influence of training data availability and the probabilistic nature of LLM\nresponses on memory integrity. We conclude that while LLMs extend the concept\nof prosthetic memory, their unmoderated use risks reinforcing historical\ndenialism, raising ethical concerns for (digital) memory preservation, and\npotentially challenging the advantageous role of technology associated with the\noriginal values of prosthetic memory."}
{"id": "2505.22291", "pdf": "https://arxiv.org/pdf/2505.22291", "abs": "https://arxiv.org/abs/2505.22291", "authors": ["Saptarshi Neil Sinha", "P. Julius Kuehn", "Johannes Koppe", "Arjan Kuijper", "Michael Weinmann"], "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The preservation of early visual arts, particularly color photographs, is\nchallenged by deterioration caused by aging and improper storage, leading to\nissues like blurring, scratches, color bleeding, and fading defects. In this\npaper, we present the first approach for the automatic removal of greening\ncolor defects in digitized autochrome photographs. Our main contributions\ninclude a method based on synthetic dataset generation and the use of\ngenerative AI with a carefully designed loss function for the restoration of\nvisual arts. To address the lack of suitable training datasets for analyzing\ngreening defects in damaged autochromes, we introduce a novel approach for\naccurately simulating such defects in synthetic data. We also propose a\nmodified weighted loss function for the ChaIR method to account for color\nimbalances between defected and non-defected areas. While existing methods\nstruggle with accurately reproducing original colors and may require\nsignificant manual effort, our method allows for efficient restoration with\nreduced time requirements."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.22304", "pdf": "https://arxiv.org/pdf/2505.22304", "abs": "https://arxiv.org/abs/2505.22304", "authors": ["Jiali Chen", "Xusen Hei", "HongFei Liu", "Yuancheng Wei", "Zikun Deng", "Jiayuan Xie", "Yi Cai", "Li Qing"], "title": "CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction", "categories": ["cs.CV"], "comment": "ACL 2025 main conference", "summary": "Computer-aided design (CAD) is crucial in prototyping 3D objects through\ngeometric instructions (i.e., CAD programs). In practical design workflows,\ndesigners often engage in time-consuming reviews and refinements of these\nprototypes by comparing them with reference images. To bridge this gap, we\nintroduce the CAD review task to automatically detect and correct potential\nerrors, ensuring consistency between the constructed 3D objects and reference\nimages. However, recent advanced multimodal large language models (MLLMs)\nstruggle to recognize multiple geometric components and perform spatial\ngeometric operations within the CAD program, leading to inaccurate reviews. In\nthis paper, we propose the CAD program repairer (ReCAD) framework to\neffectively detect program errors and provide helpful feedback on error\ncorrection. Additionally, we create a dataset, CADReview, consisting of over\n20K program-image pairs, with diverse errors for the CAD review task. Extensive\nexperiments demonstrate that our ReCAD significantly outperforms existing\nMLLMs, which shows great potential in design applications."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.22305", "pdf": "https://arxiv.org/pdf/2505.22305", "abs": "https://arxiv.org/abs/2505.22305", "authors": ["Md Touhidul Islam", "Imran Kabir", "Md Alimoor Reza", "Syed Masum Billah"], "title": "IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth", "categories": ["cs.CV"], "comment": "Accepted at DIS'25 (Funchal, Portugal)", "summary": "We present IKIWISI (\"I Know It When I See It\"), an interactive visual pattern\ngenerator for assessing vision-language models in video object recognition when\nground truth is unavailable. IKIWISI transforms model outputs into a binary\nheatmap where green cells indicate object presence and red cells indicate\nobject absence. This visualization leverages humans' innate pattern recognition\nabilities to evaluate model reliability. IKIWISI introduces \"spy objects\":\nadversarial instances users know are absent, to discern models hallucinating on\nnonexistent items. The tool functions as a cognitive audit mechanism, surfacing\nmismatches between human and machine perception by visualizing where models\ndiverge from human understanding.\n  Our study with 15 participants found that users considered IKIWISI easy to\nuse, made assessments that correlated with objective metrics when available,\nand reached informed conclusions by examining only a small fraction of heatmap\ncells. This approach not only complements traditional evaluation methods\nthrough visual assessment of model behavior with custom object sets, but also\nreveals opportunities for improving alignment between human perception and\nmachine understanding in vision-language systems."}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.22337", "pdf": "https://arxiv.org/pdf/2505.22337", "abs": "https://arxiv.org/abs/2505.22337", "authors": ["Samara Ghrer", "Christophe Godin", "Stefanie Wuhrer"], "title": "Learning to Infer Parameterized Representations of Plants from 3D Scans", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing faithfully the 3D architecture of plants from unstructured\nobservations is a challenging task. Plants frequently contain numerous organs,\norganized in branching systems in more or less complex spatial networks,\nleading to specific computational issues due to self-occlusion or spatial\nproximity between organs. Existing works either consider inverse modeling where\nthe aim is to recover the procedural rules that allow to simulate virtual\nplants, or focus on specific tasks such as segmentation or skeletonization. We\npropose a unified approach that, given a 3D scan of a plant, allows to infer a\nparameterized representation of the plant. This representation describes the\nplant's branching structure, contains parametric information for each plant\norgan, and can therefore be used directly in a variety of tasks. In this\ndata-driven approach, we train a recursive neural network with virtual plants\ngenerated using an L-systems-based procedural model. After training, the\nnetwork allows to infer a parametric tree-like representation based on an input\n3D point cloud. Our method is applicable to any plant that can be represented\nas binary axial tree. We evaluate our approach on Chenopodium Album plants,\nusing experiments on synthetic plants to show that our unified framework allows\nfor different tasks including reconstruction, segmentation and skeletonization,\nwhile achieving results on-par with state-of-the-art for each task."}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors."}
{"id": "2505.22342", "pdf": "https://arxiv.org/pdf/2505.22342", "abs": "https://arxiv.org/abs/2505.22342", "authors": ["Shriram M S", "Xinyue Hao", "Shihao Hou", "Yang Lu", "Laura Sevilla-Lara", "Anurag Arnab", "Shreyank N Gowda"], "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision"}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient."}
{"id": "2505.22344", "pdf": "https://arxiv.org/pdf/2505.22344", "abs": "https://arxiv.org/abs/2505.22344", "authors": ["Nikhil Behari", "Aaron Young", "Akshat Dave", "Ramesh Raskar"], "title": "Task-Driven Implicit Representations for Automated Design of LiDAR Systems", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Imaging system design is a complex, time-consuming, and largely manual\nprocess; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and\naerial imaging platforms, adds further complexity through unique spatial and\ntemporal sampling requirements. In this work, we propose a framework for\nautomated, task-driven LiDAR system design under arbitrary constraints. To\nachieve this, we represent LiDAR configurations in a continuous six-dimensional\ndesign space and learn task-specific implicit densities in this space via\nflow-based generative modeling. We then synthesize new LiDAR systems by\nmodeling sensors as parametric distributions in 6D space and fitting these\ndistributions to our learned implicit density using expectation-maximization,\nenabling efficient, constraint-aware LiDAR system design. We validate our\nmethod on diverse tasks in 3D vision, enabling automated LiDAR system design\nacross real-world-inspired applications in face scanning, robotic tracking, and\nobject detection."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.22353", "pdf": "https://arxiv.org/pdf/2505.22353", "abs": "https://arxiv.org/abs/2505.22353", "authors": ["Noora Al-Emadi", "Ingmar Weber", "Yin Yang", "Ferda Ofli"], "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.22360", "pdf": "https://arxiv.org/pdf/2505.22360", "abs": "https://arxiv.org/abs/2505.22360", "authors": ["Kewen Chen", "Xiaobin Hu", "Wenqi Ren"], "title": "Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large-scale text-to-image generation models have led to a\nsurge in subject-driven text-to-image generation, which aims to produce\ncustomized images that align with textual descriptions while preserving the\nidentity of specific subjects. Despite significant progress, current methods\nstruggle to disentangle identity-relevant information from identity-irrelevant\ndetails in the input images, resulting in overfitting or failure to maintain\nsubject identity. In this work, we propose a novel framework that improves the\nseparation of identity-related and identity-unrelated features and introduces\nan innovative feature fusion mechanism to improve the quality and text\nalignment of generated images. Our framework consists of two key components: an\nImplicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature\nFusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines\nlearnable adapters for implicit decoupling at the feature level with inpainting\ntechniques for explicit foreground-background separation at the image level.\nFFM dynamically integrates identity-irrelevant features with identity-related\nfeatures, enabling refined feature representations even in cases of incomplete\ndecoupling. In addition, we introduce three complementary loss functions to\nguide the decoupling process. Extensive experiments demonstrate the\neffectiveness of our proposed method in enhancing image generation quality,\nimproving flexibility in scene adaptation, and increasing the diversity of\ngenerated outputs across various textual descriptions."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2505.22387", "pdf": "https://arxiv.org/pdf/2505.22387", "abs": "https://arxiv.org/abs/2505.22387", "authors": ["Jaehyun Choi", "Gyojin Han", "Dong-Jae Lee", "Sunghyun Baek", "Junmo Kim"], "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the\ncomputational and storage burdens associated with training deep learning\nmodels. However, existing DC methods largely overlook the multi-domain nature\nof modern datasets, which are increasingly composed of heterogeneous images\nspanning multiple domains. In this paper, we extend DC and introduce\nMulti-Domain Dataset Condensation (MDDC), which aims to condense data that\ngeneralizes across both single-domain and multi-domain settings. To this end,\nwe propose the Domain-Aware Module (DAM), a training-time module that embeds\ndomain-related features into each synthetic image via learnable spatial masks.\nAs explicit domain labels are mostly unavailable in real-world datasets, we\nemploy frequency-based pseudo-domain labeling, which leverages low-frequency\namplitude statistics. DAM is only active during the condensation process, thus\npreserving the same images per class (IPC) with prior methods. Experiments show\nthat DAM consistently improves in-domain, out-of-domain, and cross-architecture\nperformance over baseline dataset condensation methods."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.22394", "pdf": "https://arxiv.org/pdf/2505.22394", "abs": "https://arxiv.org/abs/2505.22394", "authors": ["Fan Fei", "Jiajun Tang", "Fei-Peng Tian", "Boxin Shi", "Ping Tan"], "title": "PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models", "categories": ["cs.CV"], "comment": "20 pages, 7 figures", "summary": "We present PacTure, a novel framework for generating physically-based\nrendering (PBR) material textures from an untextured 3D mesh, a text\ndescription, and an optional image prompt. Early 2D generation-based texturing\napproaches generate textures sequentially from different views, resulting in\nlong inference times and globally inconsistent textures. More recent approaches\nadopt multi-view generation with cross-view attention to enhance global\nconsistency, which, however, limits the resolution for each view. In response\nto these weaknesses, we first introduce view packing, a novel technique that\nsignificantly increases the effective resolution for each view during\nmulti-view generation without imposing additional inference cost, by\nformulating the arrangement of multi-view maps as a 2D rectangle bin packing\nproblem. In contrast to UV mapping, it preserves the spatial proximity\nessential for image generation and maintains full compatibility with current 2D\ngenerative models. To further reduce the inference cost, we enable fine-grained\ncontrol and multi-domain generation within the next-scale prediction\nautoregressive framework to create an efficient multi-view multi-domain\ngenerative backbone. Extensive experiments show that PacTure outperforms\nstate-of-the-art methods in both quality of generated PBR textures and\nefficiency in training and inference."}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs."}
{"id": "2505.22396", "pdf": "https://arxiv.org/pdf/2505.22396", "abs": "https://arxiv.org/abs/2505.22396", "authors": ["Xudong Li", "Mengdan Zhang", "Peixian Chen", "Xiawu Zheng", "Yan Zhang", "Jingyuan Zheng", "Yunhang Shen", "Ke Li", "Chaoyou Fu", "Xing Sun", "Rongrong Ji"], "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) excel at single-image tasks but\nstruggle with multi-image understanding due to cross-modal misalignment,\nleading to hallucinations (context omission, conflation, and\nmisinterpretation). Existing methods using Direct Preference Optimization (DPO)\nconstrain optimization to a solitary image reference within the input sequence,\nneglecting holistic context modeling. We propose Context-to-Cue Direct\nPreference Optimization (CcDPO), a multi-level preference optimization\nframework that enhances per-image perception in multi-image settings by zooming\ninto visual clues -- from sequential context to local details. It features: (i)\nContext-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'\nmulti-image context comprehension and integrates a spectrum of low-cost global\nsequence preferences for bias mitigation. (ii) Needle-Level Optimization :\nDirects attention to fine-grained visual details through region-targeted visual\nprompts and multimodal preference supervision. To support scalable\noptimization, we also construct MultiScope-42k, an automatically generated\ndataset with high-quality multi-level preference pairs. Experiments show that\nCcDPO significantly reduces hallucinations and yields consistent performance\ngains across general single- and multi-image tasks."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.22407", "pdf": "https://arxiv.org/pdf/2505.22407", "abs": "https://arxiv.org/abs/2505.22407", "authors": ["Jiadong Pan", "Zhiyuan Ma", "Kaiyan Zhang", "Ning Ding", "Bowen Zhou"], "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/."}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization."}
{"id": "2505.22408", "pdf": "https://arxiv.org/pdf/2505.22408", "abs": "https://arxiv.org/abs/2505.22408", "authors": ["Victor Enescu", "Hichem Sahbi"], "title": "Frugal Incremental Generative Modeling using Variational Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "Continual or incremental learning holds tremendous potential in deep learning\nwith different challenges including catastrophic forgetting. The advent of\npowerful foundation and generative models has propelled this paradigm even\nfurther, making it one of the most viable solution to train these models.\nHowever, one of the persisting issues lies in the increasing volume of data\nparticularly with replay-based methods. This growth introduces challenges with\nscalability since continuously expanding data becomes increasingly demanding as\nthe number of tasks grows. In this paper, we attenuate this issue by devising a\nnovel replay-free incremental learning model based on Variational Autoencoders\n(VAEs). The main contribution of this work includes (i) a novel incremental\ngenerative modelling, built upon a well designed multi-modal latent space, and\nalso (ii) an orthogonality criterion that mitigates catastrophic forgetting of\nthe learned VAEs. The proposed method considers two variants of these VAEs:\nstatic and dynamic with no (or at most a controlled) growth in the number of\nparameters. Extensive experiments show that our method is (at least) an order\nof magnitude more ``memory-frugal'' compared to the closely related works while\nachieving SOTA accuracy scores."}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964", "abs": "https://arxiv.org/abs/2505.21964", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability."}
{"id": "2505.22421", "pdf": "https://arxiv.org/pdf/2505.22421", "abs": "https://arxiv.org/abs/2505.22421", "authors": ["Anthony Chen", "Wenzhao Zheng", "Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Kurt Keutzer", "Shangbang Zhang"], "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control", "categories": ["cs.CV", "cs.RO"], "comment": "code will be released at https://github.com/antonioo-c/GeoDrive", "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.22427", "pdf": "https://arxiv.org/pdf/2505.22427", "abs": "https://arxiv.org/abs/2505.22427", "authors": ["Van-Tin Luu", "Yon-Lin Cai", "Vu-Hoang Tran", "Wei-Chen Chiu", "Yi-Ting Chen", "Ching-Chun Huang"], "title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "This paper presents a groundbreaking approach - the first online automatic\ngeometric calibration method for radar and camera systems. Given the\nsignificant data sparsity and measurement uncertainty in radar height data,\nachieving automatic calibration during system operation has long been a\nchallenge. To address the sparsity issue, we propose a Dual-Perspective\nrepresentation that gathers features from both frontal and bird's-eye views.\nThe frontal view contains rich but sensitive height information, whereas the\nbird's-eye view provides robust features against height uncertainty. We thereby\npropose a novel Selective Fusion Mechanism to identify and fuse reliable\nfeatures from both perspectives, reducing the effect of height uncertainty.\nMoreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism\nto explicitly find location correspondences through cross-modal matching.\nDuring the training phase, we also design a Noise-Resistant Matcher to provide\nbetter supervision and enhance the robustness of the matching mechanism against\nsparsity and height uncertainty. Our experimental results, tested on the\nnuScenes dataset, demonstrate that our method significantly outperforms\nprevious radar-camera auto-calibration methods, as well as existing\nstate-of-the-art LiDAR-camera calibration techniques, establishing a new\nbenchmark for future research. The code is available at\nhttps://github.com/nycu-acm/RC-AutoCalib."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22429", "pdf": "https://arxiv.org/pdf/2505.22429", "abs": "https://arxiv.org/abs/2505.22429", "authors": ["Rong Li", "Shijie Li", "Lingdong Kong", "Xulei Yang", "Junwei Liang"], "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": "3D-LLM/VLA @ CVPR 2025; Project Page at https://seeground.github.io/", "summary": "3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using\nnatural language descriptions, enabling downstream applications such as\naugmented reality and robotics. Existing approaches typically rely on labeled\n3D data and predefined categories, limiting scalability to open-world settings.\nWe present SeeGround, a zero-shot 3DVG framework that leverages 2D\nVision-Language Models (VLMs) to bypass the need for 3D-specific training. To\nbridge the modality gap, we introduce a hybrid input format that pairs\nquery-aligned rendered views with spatially enriched textual descriptions. Our\nframework incorporates two core components: a Perspective Adaptation Module\nthat dynamically selects optimal viewpoints based on the query, and a Fusion\nAlignment Module that integrates visual and spatial signals to enhance\nlocalization precision. Extensive evaluations on ScanRefer and Nr3D confirm\nthat SeeGround achieves substantial improvements over existing zero-shot\nbaselines -- outperforming them by 7.7% and 7.1%, respectively -- and even\nrivals fully supervised alternatives, demonstrating strong generalization under\nchallenging conditions."}
{"id": "2505.22088", "pdf": "https://arxiv.org/pdf/2505.22088", "abs": "https://arxiv.org/abs/2505.22088", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "title": "Visual Cues Support Robust Turn-taking Prediction in Noise", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "5 pages", "summary": "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic\nhuman-robot interaction. However, little is known about their performance in\nnoise. This study therefore explores PTTM performance in types of noise likely\nto be encountered once deployed. Our analyses reveal PTTMs are highly sensitive\nto noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10\ndB music noise. Training with noisy data enables a multimodal PTTM, which\nincludes visual features to better exploit visual cues, with 72% accuracy in 10\ndB music noise. The multimodal PTTM outperforms the audio-only PTTM across all\nnoise types and SNRs, highlighting its ability to exploit visual cues; however,\nthis does not always generalise to new types of noise. Analysis also reveals\nthat successful training relies on accurate transcription, limiting the use of\nASR-derived transcriptions to clean conditions. We make code publicly available\nfor future research."}
{"id": "2505.22434", "pdf": "https://arxiv.org/pdf/2505.22434", "abs": "https://arxiv.org/abs/2505.22434", "authors": ["Zobia Batool", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Distance Transform Guided Mixup for Alzheimer's Detection", "categories": ["cs.CV"], "comment": null, "summary": "Alzheimer's detection efforts aim to develop accurate models for early\ndisease diagnosis. Significant advances have been achieved with convolutional\nneural networks and vision transformer based approaches. However, medical\ndatasets suffer heavily from class imbalance, variations in imaging protocols,\nand limited dataset diversity, which hinder model generalization. To overcome\nthese challenges, this study focuses on single-domain generalization by\nextending the well-known mixup method. The key idea is to compute the distance\ntransform of MRI scans, separate them spatially into multiple layers and then\ncombine layers stemming from distinct samples to produce augmented images. The\nproposed approach generates diverse data while preserving the brain's\nstructure. Experimental results show generalization performance improvement\nacross both ADNI and AIBL datasets."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.22441", "pdf": "https://arxiv.org/pdf/2505.22441", "abs": "https://arxiv.org/abs/2505.22441", "authors": ["Chaitanya Amballa", "Sattwik Basu", "Yu-Lin Wei", "Zhijian Yang", "Mehmet Ergezer", "Romit Roy Choudhury"], "title": "Can NeRFs See without Cameras?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.22444", "pdf": "https://arxiv.org/pdf/2505.22444", "abs": "https://arxiv.org/abs/2505.22444", "authors": ["Liyao Tang", "Zhe Chen", "Dacheng Tao"], "title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The emergence of large-scale pre-trained point cloud models has significantly\nadvanced 3D scene understanding, but adapting these models to specific\ndownstream tasks typically demands full fine-tuning, incurring high\ncomputational and storage costs. Parameter-efficient fine-tuning (PEFT)\ntechniques, successful in natural language processing and 2D vision tasks,\nwould underperform when naively applied to 3D point cloud models due to\nsignificant geometric and spatial distribution shifts. Existing PEFT methods\ncommonly treat points as orderless tokens, neglecting important local spatial\nstructures and global geometric contexts in 3D modeling. To bridge this gap, we\nintroduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT\nmodule specifically designed for 3D point cloud transformers. GEM explicitly\nintegrates fine-grained local positional encodings with a lightweight latent\nattention mechanism to capture comprehensive global context, thereby\neffectively addressing the spatial and geometric distribution mismatch.\nExtensive experiments demonstrate that GEM achieves performance comparable to\nor sometimes even exceeding full fine-tuning, while only updating 1.6% of the\nmodel's parameters, fewer than other PEFT methods. With significantly reduced\ntraining time and memory requirements, our approach thus sets a new benchmark\nfor efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point\ncloud models. Code will be released."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.22445", "pdf": "https://arxiv.org/pdf/2505.22445", "abs": "https://arxiv.org/abs/2505.22445", "authors": ["Puhua Jiang", "Zhangquan Chen", "Mingze Sun", "Ruqi Huang"], "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.6"], "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2311.04494", "summary": "In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22231", "pdf": "https://arxiv.org/pdf/2505.22231", "abs": "https://arxiv.org/abs/2505.22231", "authors": ["Stefan Bleeck"], "title": "Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often fails to fully characterize the functional\nimpact of hearing loss on speech understanding, particularly supra-threshold\ndeficits and frequency-specific perception challenges in conditions like\npresbycusis. This paper presents the development and simulated evaluation of a\nnovel Automatic Speech Recognition (ASR)-based frequency-specific speech test\ndesigned to provide granular diagnostic insights. Our approach leverages ASR to\nsimulate the perceptual effects of moderate sloping hearing loss by processing\nspeech stimuli under controlled acoustic degradation and subsequently analyzing\nphoneme-level confusion patterns. Key findings indicate that simulated hearing\nloss introduces specific phoneme confusions, predominantly affecting\nhigh-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)\nand leading to significant phoneme deletions, consistent with the acoustic cues\ndegraded in presbycusis. A test battery curated from these ASR-derived\nconfusions demonstrated diagnostic value, effectively differentiating between\nsimulated normal-hearing and hearing-impaired listeners in a comprehensive\nsimulation. This ASR-driven methodology offers a promising avenue for\ndeveloping objective, granular, and frequency-specific hearing assessment tools\nthat complement traditional audiometry. Future work will focus on validating\nthese findings with human participants and exploring the integration of\nadvanced AI models for enhanced diagnostic precision."}
{"id": "2505.22458", "pdf": "https://arxiv.org/pdf/2505.22458", "abs": "https://arxiv.org/abs/2505.22458", "authors": ["Seun-An Choe", "Keon-Hee Park", "Jinwoo Choi", "Gyeong-Moon Park"], "title": "Universal Domain Adaptation for Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to\ntransfer knowledge from labeled source data to unlabeled target data. However,\ntraditional UDA-SS methods assume that category settings between source and\ntarget domains are known, which is unrealistic in real-world scenarios. This\nleads to performance degradation if private classes exist. To address this\nlimitation, we propose Universal Domain Adaptation for Semantic Segmentation\n(UniDA-SS), achieving robust adaptation even without prior knowledge of\ncategory settings. We define the problem in the UniDA-SS scenario as low\nconfidence scores of common classes in the target domain, which leads to\nconfusion with private classes. To solve this problem, we propose UniMAP:\nUniDA-SS with Image Matching and Prototype-based Distinction, a novel framework\ncomposed of two key components. First, Domain-Specific Prototype-based\nDistinction (DSPD) divides each class into two domain-specific prototypes,\nenabling finer separation of domain-specific features and enhancing the\nidentification of common classes across domains. Second, Target-based Image\nMatching (TIM) selects a source image containing the most common-class pixels\nbased on the target pseudo-label and pairs it in a batch to promote effective\nlearning of common classes. We also introduce a new UniDA-SS benchmark and\ndemonstrate through various experiments that UniMAP significantly outperforms\nbaselines. The code is available at\n\\href{https://github.com/KU-VGI/UniMAP}{this https URL}."}
{"id": "2505.22251", "pdf": "https://arxiv.org/pdf/2505.22251", "abs": "https://arxiv.org/abs/2505.22251", "authors": ["Yuan Tseng", "Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Bhattacharya"], "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data."}
{"id": "2505.22461", "pdf": "https://arxiv.org/pdf/2505.22461", "abs": "https://arxiv.org/abs/2505.22461", "authors": ["Qiucheng Yu", "Yuan Xie", "Xin Tan"], "title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc"}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework."}
{"id": "2505.22465", "pdf": "https://arxiv.org/pdf/2505.22465", "abs": "https://arxiv.org/abs/2505.22465", "authors": ["Zobia Batool", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Although Alzheimer's disease detection via MRIs has advanced significantly\nthanks to contemporary deep learning models, challenges such as class\nimbalance, protocol variations, and limited dataset diversity often hinder\ntheir generalization capacity. To address this issue, this article focuses on\nthe single domain generalization setting, where given the data of one domain, a\nmodel is designed and developed with maximal performance w.r.t. an unseen\ndomain of distinct distribution. Since brain morphology is known to play a\ncrucial role in Alzheimer's diagnosis, we propose the use of learnable\npseudo-morphological modules aimed at producing shape-aware, anatomically\nmeaningful class-specific augmentations in combination with a supervised\ncontrastive learning module to extract robust class-specific representations.\nExperiments conducted across three datasets show improved performance and\ngeneralization capacity, especially under class imbalance and imaging protocol\nvariations. The source code will be made available upon acceptance at\nhttps://github.com/zobia111/SDG-Alzheimer."}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."}
{"id": "2505.22490", "pdf": "https://arxiv.org/pdf/2505.22490", "abs": "https://arxiv.org/abs/2505.22490", "authors": ["Ke Zhang", "Tianyu Ding", "Jiachen Jiang", "Tianyi Chen", "Ilya Zharkov", "Vishal M. Patel", "Luming Liang"], "title": "ProCrop: Learning Aesthetic Image Cropping from Professional Compositions", "categories": ["cs.CV"], "comment": "16 pages, 15 figures", "summary": "Image cropping is crucial for enhancing the visual appeal and narrative\nimpact of photographs, yet existing rule-based and data-driven approaches often\nlack diversity or require annotated training data. We introduce ProCrop, a\nretrieval-based method that leverages professional photography to guide\ncropping decisions. By fusing features from professional photographs with those\nof the query image, ProCrop learns from professional compositions,\nsignificantly boosting performance. Additionally, we present a large-scale\ndataset of 242K weakly-annotated images, generated by out-painting professional\nimages and iteratively refining diverse crop proposals. This composition-aware\ndataset generation offers diverse high-quality crop proposals guided by\naesthetic principles and becomes the largest publicly available dataset for\nimage cropping. Extensive experiments show that ProCrop significantly\noutperforms existing methods in both supervised and weakly-supervised settings.\nNotably, when trained on the new dataset, our ProCrop surpasses previous\nweakly-supervised methods and even matches fully supervised approaches. Both\nthe code and dataset will be made publicly available to advance research in\nimage aesthetics and composition analysis."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.22499", "pdf": "https://arxiv.org/pdf/2505.22499", "abs": "https://arxiv.org/abs/2505.22499", "authors": ["Aixuan Li", "Mochu Xiang", "Jing Zhang", "Yuchao Dai"], "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector", "categories": ["cs.CV"], "comment": null, "summary": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g., putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22522", "pdf": "https://arxiv.org/pdf/2505.22522", "abs": "https://arxiv.org/abs/2505.22522", "authors": ["Yuan Zhang", "Feng Chen", "Yaolei Qi", "Guanyu Yang", "Huazhu Fu"], "title": "PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 5 figures; Accepted by MedIA", "summary": "Pathology image segmentation across multiple centers encounters significant\nchallenges due to diverse sources of heterogeneity including imaging\nmodalities, organs, and scanning equipment, whose variability brings\nrepresentation bias and impedes the development of generalizable segmentation\nmodels. In this paper, we propose PathFL, a novel multi-alignment Federated\nLearning framework for pathology image segmentation that addresses these\nchallenges through three-level alignment strategies of image, feature, and\nmodel aggregation. Firstly, at the image level, a collaborative style\nenhancement module aligns and diversifies local data by facilitating style\ninformation exchange across clients. Secondly, at the feature level, an\nadaptive feature alignment module ensures implicit alignment in the\nrepresentation space by infusing local features with global insights, promoting\nconsistency across heterogeneous client features learning. Finally, at the\nmodel aggregation level, a stratified similarity aggregation strategy\nhierarchically aligns and aggregates models on the server, using layer-specific\nsimilarity to account for client discrepancies and enhance global\ngeneralization. Comprehensive evaluations on four sets of heterogeneous\npathology image datasets, encompassing cross-source, cross-modality,\ncross-organ, and cross-scanner variations, validate the effectiveness of our\nPathFL in achieving better performance and robustness against data\nheterogeneity."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22523", "pdf": "https://arxiv.org/pdf/2505.22523", "abs": "https://arxiv.org/abs/2505.22523", "authors": ["Junwen Chen", "Heyang Jiang", "Yanbin Wang", "Keming Wu", "Ji Li", "Chao Zhang", "Keiji Yanai", "Dong Chen", "Yuhui Yuan"], "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models", "categories": ["cs.CV"], "comment": "Homepage: https://prism-layers.github.io/", "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22535", "pdf": "https://arxiv.org/pdf/2505.22535", "abs": "https://arxiv.org/abs/2505.22535", "authors": ["Mohamad Hakam Shams Eddin", "Yikui Zhang", "Stefan Kollet", "Juergen Gall"], "title": "RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting", "categories": ["cs.CV", "cs.LG"], "comment": "Main paper 10 pages, Appendix 53 pages", "summary": "Recent deep learning approaches for river discharge forecasting have improved\nthe accuracy and efficiency in flood forecasting, enabling more reliable early\nwarning systems for risk management. Nevertheless, existing deep learning\napproaches in hydrology remain largely confined to local-scale applications and\ndo not leverage the inherent spatial connections of bodies of water. Thus,\nthere is a strong need for new deep learning methodologies that are capable of\nmodeling spatio-temporal relations to improve river discharge and flood\nforecasting for scientific and operational applications. To address this, we\npresent RiverMamba, a novel deep learning model that is pretrained with\nlong-term reanalysis data and that can forecast global river discharge and\nfloods on a $0.05^\\circ$ grid up to 7 days lead time, which is of high\nrelevance in early warning. To achieve this, RiverMamba leverages efficient\nMamba blocks that enable the model to capture global-scale channel network\nrouting and enhance its forecast capability for longer lead times. The forecast\nblocks integrate ECMWF HRES meteorological forecasts, while accounting for\ntheir inaccuracies through spatio-temporal modeling. Our analysis demonstrates\nthat RiverMamba delivers reliable predictions of river discharge, including\nextreme floods across return periods and lead times, surpassing both\noperational AI- and physics-based models."}
{"id": "2505.22487", "pdf": "https://arxiv.org/pdf/2505.22487", "abs": "https://arxiv.org/abs/2505.22487", "authors": ["Yen Meng", "Sharon Goldwater", "Hao Tang"], "title": "Effective Context in Neural Speech Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Modern neural speech models benefit from having longer context, and many\napproaches have been proposed to increase the maximum context a model can use.\nHowever, few have attempted to measure how much context these models actually\nuse, i.e., the effective context. Here, we propose two approaches to measuring\nthe effective context, and use them to analyze different speech Transformers.\nFor supervised models, we find that the effective context correlates well with\nthe nature of the task, with fundamental frequency tracking, phone\nclassification, and word classification requiring increasing amounts of\neffective context. For self-supervised models, we find that effective context\nincreases mainly in the early layers, and remains relatively short -- similar\nto the supervised phone model. Given that these models do not use a long\ncontext during prediction, we show that HuBERT can be run in streaming mode\nwithout modification to the architecture and without further fine-tuning."}
{"id": "2505.22543", "pdf": "https://arxiv.org/pdf/2505.22543", "abs": "https://arxiv.org/abs/2505.22543", "authors": ["Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Yingji Liang", "Xiaorong Zhu", "Chunyi Li", "Jinliang Han", "Haoning Wu", "Bin Wang", "Haoran Zhang", "Guanyu Zhu", "Qiyong Zhao", "Xiaohong Liu", "Guangtao Zhai", "Xiongkuo Min"], "title": "Scaling-up Perceptual Video Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22551", "pdf": "https://arxiv.org/pdf/2505.22551", "abs": "https://arxiv.org/abs/2505.22551", "authors": ["Long Hui", "Wai Lok Yeung"], "title": "Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Limited DXA access hinders osteoporosis screening. This proof-of-concept\nstudy proposes using widely available knee X-rays for opportunistic Bone\nMineral Density (BMD) estimation via deep learning, emphasizing robust\nuncertainty quantification essential for clinical use. An EfficientNet model\nwas trained on the OAI dataset to predict BMD from bilateral knee radiographs.\nTwo Test-Time Augmentation (TTA) methods were compared: traditional averaging\nand a multi-sample approach. Crucially, Split Conformal Prediction was\nimplemented to provide statistically rigorous, patient-specific prediction\nintervals with guaranteed coverage. Results showed a Pearson correlation of\n0.68 (traditional TTA). While traditional TTA yielded better point predictions,\nthe multi-sample approach produced slightly tighter confidence intervals (90%,\n95%, 99%) while maintaining coverage. The framework appropriately expressed\nhigher uncertainty for challenging cases. Although anatomical mismatch between\nknee X-rays and standard DXA limits immediate clinical use, this method\nestablishes a foundation for trustworthy AI-assisted BMD screening using\nroutine radiographs, potentially improving early osteoporosis detection."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22555", "pdf": "https://arxiv.org/pdf/2505.22555", "abs": "https://arxiv.org/abs/2505.22555", "authors": ["Yanyi Qu", "Haoyang Ma", "Wenhui Xiong"], "title": "MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Human pose estimation based on Channel State Information (CSI) has emerged as\na promising approach for non-intrusive and precise human activity monitoring,\nyet faces challenges including accurate multi-person pose recognition and\neffective CSI feature learning. This paper presents MultiFormer, a wireless\nsensing system that accurately estimates human pose through CSI. The proposed\nsystem adopts a Transformer based time-frequency dual-token feature extractor\nwith multi-head self-attention. This feature extractor is able to model\ninter-subcarrier correlations and temporal dependencies of the CSI. The\nextracted CSI features and the pose probability heatmaps are then fused by\nMulti-Stage Feature Fusion Network (MSFN) to enforce the anatomical\nconstraints. Extensive experiments conducted on on the public MM-Fi dataset and\nour self-collected dataset show that the MultiFormer achieves higher accuracy\nover state-of-the-art approaches, especially for high-mobility keypoints\n(wrists, elbows) that are particularly difficult for previous methods to\naccurately estimate."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22564", "pdf": "https://arxiv.org/pdf/2505.22564", "abs": "https://arxiv.org/abs/2505.22564", "authors": ["Jaehyun Choi", "Jiwan Hur", "Gyojin Han", "Jaemyung Yu", "Junmo Kim"], "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video dataset condensation has emerged as a critical technique for addressing\nthe computational challenges associated with large-scale video data processing\nin deep learning applications. While significant progress has been made in\nimage dataset condensation, the video domain presents unique challenges due to\nthe complex interplay between spatial content and temporal dynamics. This paper\nintroduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for\nvideo dataset condensation, a novel approach that fundamentally reconsiders how\nvideo data should be condensed. Unlike the previous method that separates\nstatic content from dynamic motion, our method preserves the essential\ninterdependence between these elements. Our approach progressively refines and\ninserts frames to fully accommodate the motion in an action while achieving\nbetter performance but less storage, considering the relation of gradients for\neach frame. Extensive experiments across standard video action recognition\nbenchmarks demonstrate that PRISM outperforms existing disentangled approaches\nwhile maintaining compact representations suitable for resource-constrained\nenvironments."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22566", "pdf": "https://arxiv.org/pdf/2505.22566", "abs": "https://arxiv.org/abs/2505.22566", "authors": ["Yifan Xie", "Mingyang Li", "Shoujie Li", "Xingting Li", "Guangyu Chen", "Fei Ma", "Fei Richard Yu", "Wenbo Ding"], "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22569", "pdf": "https://arxiv.org/pdf/2505.22569", "abs": "https://arxiv.org/abs/2505.22569", "authors": ["Dmitrii Sorokin", "Maksim Nakhodnov", "Andrey Kuznetsov", "Aibek Alanov"], "title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models", "categories": ["cs.CV"], "comment": "The source code can be found at\n  https://github.com/ControlGenAI/ImageReFL", "summary": "Recent advances in diffusion models have led to impressive image generation\ncapabilities, but aligning these models with human preferences remains\nchallenging. Reward-based fine-tuning using models trained on human feedback\nimproves alignment but often harms diversity, producing less varied outputs. In\nthis work, we address this trade-off with two contributions. First, we\nintroduce \\textit{combined generation}, a novel sampling strategy that applies\na reward-tuned diffusion model only in the later stages of the generation\nprocess, while preserving the base model for earlier steps. This approach\nmitigates early-stage overfitting and helps retain global structure and\ndiversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that\nimproves image diversity with minimal loss in quality by training on real\nimages and incorporating multiple regularizers, including diffusion and ReFL\nlosses. Our approach outperforms conventional reward tuning methods on standard\nquality and diversity metrics. A user study further confirms that our method\nbetter balances human preference alignment and visual diversity. The source\ncode can be found at https://github.com/ControlGenAI/ImageReFL ."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2505.22581", "pdf": "https://arxiv.org/pdf/2505.22581", "abs": "https://arxiv.org/abs/2505.22581", "authors": ["Kartik Kuckreja", "Parul Gupta", "Injy Hamed", "Thamar Solorio", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Tell me Habibi, is it Real or Fake?", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 2 figures, 12 tables", "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}."}
{"id": "2505.22596", "pdf": "https://arxiv.org/pdf/2505.22596", "abs": "https://arxiv.org/abs/2505.22596", "authors": ["Jiaqi Huang", "Zunnan Xu", "Jun Zhou", "Ting Liu", "Yicheng Xiao", "Mingwen Ou", "Bowen Ji", "Xiu Li", "Kehong Yuan"], "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging multimodal large models for image segmentation has become a\nprominent research direction. However, existing approaches typically rely\nheavily on manually annotated datasets that include explicit reasoning\nprocesses, which are costly and time-consuming to produce. Recent advances\nsuggest that reinforcement learning (RL) can endow large models with reasoning\ncapabilities without requiring such reasoning-annotated data. In this paper, we\npropose SAM-R1, a novel framework that enables multimodal large models to\nperform fine-grained reasoning in image understanding tasks. Our approach is\nthe first to incorporate fine-grained segmentation settings during the training\nof multimodal reasoning models. By integrating task-specific, fine-grained\nrewards with a tailored optimization objective, we further enhance the model's\nreasoning and segmentation alignment. We also leverage the Segment Anything\nModel (SAM) as a strong and flexible reward provider to guide the learning\nprocess. With only 3k training samples, SAM-R1 achieves strong performance\nacross multiple benchmarks, demonstrating the effectiveness of reinforcement\nlearning in equipping multimodal models with segmentation-oriented reasoning\ncapabilities."}
{"id": "2505.22604", "pdf": "https://arxiv.org/pdf/2505.22604", "abs": "https://arxiv.org/abs/2505.22604", "authors": ["Ruixuan Zhang", "He Wang", "Zhengyu Zhao", "Zhiqing Guo", "Xun Yang", "Yunfeng Diao", "Meng Wang"], "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22616", "pdf": "https://arxiv.org/pdf/2505.22616", "abs": "https://arxiv.org/abs/2505.22616", "authors": ["Yezhi Shen", "Qiuchen Zhai", "Fengqing Zhu"], "title": "PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to the CVPR 2025 Workshop on Autonomous Driving (WAD)", "summary": "Neural rendering methods have gained significant attention for their ability\nto reconstruct 3D scenes from 2D images. The core idea is to take multiple\nviews as input and optimize the reconstructed scene by minimizing the\nuncertainty in geometry and appearance across the views. However, the\nreconstruction quality is limited by the number of input views. This limitation\nis further pronounced in complex and dynamic scenes, where certain angles of\nobjects are never seen. In this paper, we propose to use video frame\ninterpolation as the data augmentation method for neural rendering.\nFurthermore, we design a lightweight yet high-quality video frame interpolation\nmodel, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and\nOptimization). PS4PRO is trained on diverse video datasets, implicitly modeling\ncamera movement as well as real-world 3D geometry. Our model performs as an\nimplicit world prior, enriching the photo supervision for 3D reconstruction. By\nleveraging the proposed method, we effectively augment existing datasets for\nneural rendering methods. Our experimental results indicate that our method\nimproves the reconstruction performance on both static and dynamic scenes."}
{"id": "2505.22636", "pdf": "https://arxiv.org/pdf/2505.22636", "abs": "https://arxiv.org/abs/2505.22636", "authors": ["Jixin Zhao", "Shangchen Zhou", "Zhouxia Wang", "Peiqing Yang", "Chen Change Loy"], "title": "ObjectClear: Complete Object Removal via Object-Effect Attention", "categories": ["cs.CV"], "comment": "Project page: https://zjx0101.github.io/projects/ObjectClear/", "summary": "Object removal requires eliminating not only the target object but also its\neffects, such as shadows and reflections. However, diffusion-based inpainting\nmethods often produce artifacts, hallucinate content, alter background, and\nstruggle to remove object effects accurately. To address this challenge, we\nintroduce a new dataset for OBject-Effect Removal, named OBER, which provides\npaired images with and without object effects, along with precise masks for\nboth objects and their associated visual artifacts. The dataset comprises\nhigh-quality captured and simulated data, covering diverse object categories\nand complex multi-object scenes. Building on OBER, we propose a novel\nframework, ObjectClear, which incorporates an object-effect attention mechanism\nto guide the model toward the foreground removal regions by learning attention\nmasks, effectively decoupling foreground removal from background\nreconstruction. Furthermore, the predicted attention map enables an\nattention-guided fusion strategy during inference, greatly preserving\nbackground details. Extensive experiments demonstrate that ObjectClear\noutperforms existing methods, achieving improved object-effect removal quality\nand background fidelity, especially in complex scenarios."}
{"id": "2505.22643", "pdf": "https://arxiv.org/pdf/2505.22643", "abs": "https://arxiv.org/abs/2505.22643", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data."}
{"id": "2505.22647", "pdf": "https://arxiv.org/pdf/2505.22647", "abs": "https://arxiv.org/abs/2505.22647", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "categories": ["cs.CV"], "comment": "Homepage: https://meigen-ai.github.io/multi-talk Github:\n  https://github.com/MeiGen-AI/MultiTalk", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22654", "pdf": "https://arxiv.org/pdf/2505.22654", "abs": "https://arxiv.org/abs/2505.22654", "authors": ["Ce Zhang", "Kaixin Ma", "Tianqing Fang", "Wenhao Yu", "Hongming Zhang", "Zhisong Zhang", "Yaqi Xie", "Katia Sycara", "Haitao Mi", "Dong Yu"], "title": "VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
{"id": "2505.22663", "pdf": "https://arxiv.org/pdf/2505.22663", "abs": "https://arxiv.org/abs/2505.22663", "authors": ["Aimon Rahman", "Kartik Narayan", "Vishal M. Patel"], "title": "Training Free Stylized Abstraction", "categories": ["cs.CV"], "comment": "Project Page: https://kartik-3004.github.io/TF-SA/", "summary": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup."}
{"id": "2505.22664", "pdf": "https://arxiv.org/pdf/2505.22664", "abs": "https://arxiv.org/abs/2505.22664", "authors": ["Kaiyu Yue", "Vasu Singla", "Menglin Jia", "John Kirchenbauer", "Rifaa Qadri", "Zikui Cai", "Abhinav Bhatele", "Furong Huang", "Tom Goldstein"], "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach."}
{"id": "2505.21530", "pdf": "https://arxiv.org/pdf/2505.21530", "abs": "https://arxiv.org/abs/2505.21530", "authors": ["Xuhang Chen", "Zhuo Li", "Yanyan Shen", "Mufti Mahmud", "Hieu Pham", "Chi-Man Pun", "Shuqiang Wang"], "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models."}
{"id": "2505.21546", "pdf": "https://arxiv.org/pdf/2505.21546", "abs": "https://arxiv.org/abs/2505.21546", "authors": ["Sajal Chakroborty", "Suddhasattwa Das"], "title": "Image denoising as a conditional expectation", "categories": ["eess.IV", "cs.CV", "math.OC"], "comment": null, "summary": "All techniques for denoising involve a notion of a true (noise-free) image,\nand a hypothesis space. The hypothesis space may reconstruct the image directly\nas a grayscale valued function, or indirectly by its Fourier or wavelet\nspectrum. Most common techniques estimate the true image as a projection to\nsome subspace. We propose an interpretation of a noisy image as a collection of\nsamples drawn from a certain probability space. Within this interpretation,\nprojection based approaches are not guaranteed to be unbiased and convergent.\nWe present a data-driven denoising method in which the true image is recovered\nas a conditional expectation. Although the probability space is unknown\napriori, integrals on this space can be estimated by kernel integral operators.\nThe true image is reformulated as the least squares solution to a linear\nequation in a reproducing kernel Hilbert space (RKHS), and involving various\nkernel integral operators as linear transforms. Assuming the true image to be a\ncontinuous function on a compact planar domain, the technique is shown to be\nconvergent as the number of pixels goes to infinity. We also show that for a\npicture with finite number of pixels, the convergence result can be used to\nchoose the various parameters for an optimum denoising result."}
{"id": "2505.21581", "pdf": "https://arxiv.org/pdf/2505.21581", "abs": "https://arxiv.org/abs/2505.21581", "authors": ["Zhennan Wang", "Jianing Teng", "Canqun Xiang", "Kangliang Chen", "Xing Pan", "Lu Deng", "Weihao Gu"], "title": "CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While end-to-end autonomous driving has advanced significantly, prevailing\nmethods remain fundamentally misaligned with human cognitive principles in both\nperception and planning. In this paper, we propose CogAD, a novel end-to-end\nautonomous driving model that emulates the hierarchical cognition mechanisms of\nhuman drivers. CogAD implements dual hierarchical mechanisms: global-to-local\ncontext processing for human-like perception and intent-conditioned multi-mode\ntrajectory generation for cognitively-inspired planning. The proposed method\ndemonstrates three principal advantages: comprehensive environmental\nunderstanding through hierarchical perception, robust planning exploration\nenabled by multi-level planning, and diverse yet reasonable multi-modal\ntrajectory generation facilitated by dual-level uncertainty modeling. Extensive\nexperiments on nuScenes and Bench2Drive demonstrate that CogAD achieves\nstate-of-the-art performance in end-to-end planning, exhibiting particular\nsuperiority in long-tail scenarios and robust generalization to complex\nreal-world driving conditions."}
{"id": "2505.21592", "pdf": "https://arxiv.org/pdf/2505.21592", "abs": "https://arxiv.org/abs/2505.21592", "authors": ["Ze Chen", "Shaode Yu"], "title": "Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong\nfunction approximation capability. In our previous work, KAN and its variants\nwere explored in score regression for blind image quality assessment (BIQA).\nHowever, these models encounter challenges when processing high-dimensional\nfeatures, leading to limited performance gains and increased computational\ncost. To address these issues, we propose TaylorKAN that leverages the Taylor\nexpansions as learnable activation functions to enhance local approximation\ncapability. To improve the computational efficiency, network depth reduction\nand feature dimensionality compression are integrated into the TaylorKAN-based\nscore regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and\nFLIVE) with authentic distortions, extensive experiments demonstrate that\nTaylorKAN consistently outperforms the other KAN-related models, indicating\nthat the local approximation via Taylor expansions is more effective than\nglobal approximation using orthogonal functions. Its generalization capacity is\nvalidated through inter-database experiments. The findings highlight the\npotential of TaylorKAN as an efficient and robust model for high-dimensional\nscore regression."}
{"id": "2505.21597", "pdf": "https://arxiv.org/pdf/2505.21597", "abs": "https://arxiv.org/abs/2505.21597", "authors": ["Abdullah Al Mamun", "Pollob Chandra Ray", "Md Rahat Ul Nasib", "Akash Das", "Jia Uddin", "Md Nurul Absur"], "title": "Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "6 pages, & 7 Images", "summary": "The rapid advancement of deep learning in medical image analysis has greatly\nenhanced the accuracy of skin cancer classification. However, current\nstate-of-the-art models, especially those based on transfer learning like\nResNet50, come with significant computational overhead, rendering them\nimpractical for deployment in resource-constrained environments. This study\nproposes a custom CNN model that achieves a 96.7\\% reduction in parameters\n(from 23.9 million in ResNet50 to 692,000) while maintaining a classification\naccuracy deviation of less than 0.022\\%. Our empirical analysis of the HAM10000\ndataset reveals that although transfer learning models provide a marginal\naccuracy improvement of approximately 0.022\\%, they result in a staggering\n13,216.76\\% increase in FLOPs, considerably raising computational costs and\ninference latency. In contrast, our lightweight CNN architecture, which\nencompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,\nsignificantly reduces energy consumption, memory footprint, and inference time.\nThese findings underscore the trade-off between the complexity of deep models\nand their real-world feasibility, positioning our optimized CNN as a practical\nsolution for mobile and edge-based skin cancer diagnostics."}
{"id": "2505.21620", "pdf": "https://arxiv.org/pdf/2505.21620", "abs": "https://arxiv.org/abs/2505.21620", "authors": ["Zhengyuan Jiang", "Moyang Guo", "Kecen Li", "Yuepeng Hu", "Yupu Wang", "Zhicong Huang", "Cheng Hong", "Neil Zhenqiang Gong"], "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."}
{"id": "2505.21634", "pdf": "https://arxiv.org/pdf/2505.21634", "abs": "https://arxiv.org/abs/2505.21634", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Laparoscopic surgeries often suffer from reduced visual clarity due to the\npresence of surgical smoke originated by surgical instruments, which poses\nsignificant challenges for both surgeons and vision based computer-assisted\ntechnologies. In order to remove the surgical smoke, a novel U-Net deep\nlearning with new loss function and integrated differentiable Wiener filter\n(ULW) method is presented. Specifically, the new loss function integrates the\npixel, structural, and perceptual properties. Thus, the new loss function,\nwhich combines the structural similarity index measure loss, the perceptual\nloss, as well as the mean squared error loss, is able to enhance the quality\nand realism of the reconstructed images. Furthermore, the learnable Wiener\nfilter is capable of effectively modelling the degradation process caused by\nthe surgical smoke. The effectiveness of the proposed ULW method is evaluated\nusing the publicly available paired laparoscopic smoke and smoke-free image\ndataset, which provides reliable benchmarking and quantitative comparisons.\nExperimental results show that the proposed ULW method excels in both visual\nclarity and metric-based evaluation. As a result, the proposed ULW method\noffers a promising solution for real-time enhancement of laparoscopic imagery.\nThe code is available at https://github.com/chengyuyang-njit/ImageDesmoke."}
{"id": "2505.21686", "pdf": "https://arxiv.org/pdf/2505.21686", "abs": "https://arxiv.org/abs/2505.21686", "authors": ["Michele Gallo"], "title": "tenSVD algorithm for compression", "categories": ["stat.CO", "cs.CV", "cs.LG"], "comment": null, "summary": "Tensors provide a robust framework for managing high-dimensional data.\nConsequently, tensor analysis has emerged as an active research area in various\ndomains, including machine learning, signal processing, computer vision, graph\nanalysis, and data mining. This study introduces an efficient image storage\napproach utilizing tensors, aiming to minimize memory to store, bandwidth to\ntransmit and energy to processing. The proposed method organizes original data\ninto a higher-order tensor and applies the Tucker model for compression.\nImplemented in R, this method is compared to a baseline algorithm. The\nevaluation focuses on efficient of algorithm measured in term of computational\ntime and the quality of information preserved, using both simulated and real\ndatasets. A detailed analysis of the results is conducted, employing\nestablished quantitative metrics, with significant attention paid to\nsustainability in terms of energy consumption across algorithms."}
{"id": "2505.21699", "pdf": "https://arxiv.org/pdf/2505.21699", "abs": "https://arxiv.org/abs/2505.21699", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Predicting the risk of developing breast cancer is an important clinical tool\nto guide early intervention and tailoring personalized screening strategies.\nEarly risk models have limited performance and recently machine learning-based\nanalysis of mammogram images showed encouraging risk prediction effects. These\nmodels however are limited to the use of a single exam or tend to overlook\nnuanced breast tissue evolvement in spatial and temporal details of\nlongitudinal imaging exams that are indicative of breast cancer risk. In this\npaper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk\nPrediction), a novel Transformer-based model that captures fine-grained\nmammographic imaging evolution simultaneously from bilateral and longitudinal\nasymmetries for breast cancer risk prediction. STA-Risk is innovative by the\nside encoding and temporal encoding to learn spatial-temporal asymmetries,\nregulated by a customized asymmetry loss. We performed extensive experiments\nwith two independent mammogram datasets and achieved superior performance than\nfour representative SOTA models for 1- to 5-year future risk prediction. Source\ncodes will be released upon publishing of the paper."}
{"id": "2505.21715", "pdf": "https://arxiv.org/pdf/2505.21715", "abs": "https://arxiv.org/abs/2505.21715", "authors": ["Md. Zahid Hossain", "Mustofa Ahmed", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam"], "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint, manuscript under-review", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality."}
{"id": "2505.21874", "pdf": "https://arxiv.org/pdf/2505.21874", "abs": "https://arxiv.org/abs/2505.21874", "authors": ["Ruiguo Yu", "Yiyang Zhang", "Yuan Tian", "Yujie Diao", "Di Jin", "Witold Pedrycz"], "title": "MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation methods generally assume that the process from\nmedical image to segmentation is unbiased, and use neural networks to establish\nconditional probability models to complete the segmentation task. This\nassumption does not consider confusion factors, which can affect medical\nimages, such as complex anatomical variations and imaging modality limitations.\nConfusion factors obfuscate the relevance and causality of medical image\nsegmentation, leading to unsatisfactory segmentation results. To address this\nissue, we propose a multi-causal aware modeling backdoor-intervention\noptimization (MAMBO-NET) network for medical image segmentation. Drawing\ninsights from causal inference, MAMBO-NET utilizes self-modeling with\nmulti-Gaussian distributions to fit the confusion factors and introduce causal\nintervention into the segmentation process. Moreover, we design appropriate\nposterior probability constraints to effectively train the distributions of\nconfusion factors. For the distributions to effectively guide the segmentation\nand mitigate and eliminate the Impact of confusion factors on the segmentation,\nwe introduce classical backdoor intervention techniques and analyze their\nfeasibility in the segmentation task. To evaluate the effectiveness of our\napproach, we conducted extensive experiments on five medical image datasets.\nThe results demonstrate that our method significantly reduces the influence of\nconfusion factors, leading to enhanced segmentation accuracy."}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities."}
{"id": "2505.21910", "pdf": "https://arxiv.org/pdf/2505.21910", "abs": "https://arxiv.org/abs/2505.21910", "authors": ["Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Chun-Guang Li", "Bojia Zi", "Xili Dai", "Qin Zou", "Rong Xiao"], "title": "Taming Transformer Without Using Learning Rate Warmup", "categories": ["cs.LG", "cs.CV"], "comment": "This paper is published as a conference paper at ICLR 2025", "summary": "Scaling Transformer to a large scale without using some technical tricks such\nas learning rate warump and using an obviously lower learning rate is an\nextremely challenging task, and is increasingly gaining more attention. In this\npaper, we provide a theoretical analysis for the process of training\nTransformer and reveal the rationale behind the model crash phenomenon in the\ntraining process, termed \\textit{spectral energy concentration} of\n${\\bW_q}^{\\top} \\bW_k$, which is the reason for a malignant entropy collapse,\nwhere ${\\bW_q}$ and $\\bW_k$ are the projection matrices for the query and the\nkey in Transformer, respectively. To remedy this problem, motivated by\n\\textit{Weyl's Inequality}, we present a novel optimization strategy, \\ie,\nmaking the weight updating in successive steps smooth -- if the ratio\n$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$ is larger than a\nthreshold, we will automatically bound the learning rate to a weighted multiple\nof $\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$, where $\\nabla\n\\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can\nprevent spectral energy concentration to only a few directions, and thus can\navoid malignant entropy collapse which will trigger the model crash. We conduct\nextensive experiments using ViT, Swin-Transformer and GPT, showing that our\noptimization strategy can effectively and stably train these Transformers\nwithout using learning rate warmup."}
{"id": "2505.21912", "pdf": "https://arxiv.org/pdf/2505.21912", "abs": "https://arxiv.org/abs/2505.21912", "authors": ["Marvin Limpijankit", "John Kender"], "title": "Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics", "categories": ["cs.CY", "cs.CV"], "comment": null, "summary": "We propose a two-step approach for detecting differences in the style of\nimages across sources of differing cultural affinity, where images are first\nclustered into finer visual themes based on content before their aesthetic\nfeatures are compared. We test this approach on 2,400 YouTube video thumbnails\ntaken equally from two U.S. and two Chinese YouTube channels, and relating\nequally to COVID-19 and the Ukraine conflict. Our results suggest that while\nChinese thumbnails are less formal and more candid, U.S. channels tend to use\nmore deliberate, proper photographs as thumbnails. In particular, U.S.\nthumbnails are less colorful, more saturated, darker, more finely detailed,\nless symmetric, sparser, less varied, and more up close and personal than\nChinese thumbnails. We suggest that most of these differences reflect cultural\npreferences, and that our methods and observations can serve as a baseline\nagainst which suspected visual propaganda can be computed and compared."}
{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport."}
{"id": "2505.21928", "pdf": "https://arxiv.org/pdf/2505.21928", "abs": "https://arxiv.org/abs/2505.21928", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties."}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.22024", "pdf": "https://arxiv.org/pdf/2505.22024", "abs": "https://arxiv.org/abs/2505.22024", "authors": ["Long-Khanh Pham", "Thanh V. T. Tran", "Minh-Tan Pham", "Van Nguyen"], "title": "RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "accepted in Interspeech 2025", "summary": "Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,\nfaces challenges in accuracy and naturalness due to limited supervision in\ncapturing linguistic content, accents, and prosody. In this paper, we propose\nRESOUND, a novel L2S system that generates intelligible and expressive speech\nfrom silent talking face videos. Leveraging source-filter theory, our method\ninvolves two components: an acoustic path to predict prosody and a semantic\npath to extract linguistic features. This separation simplifies learning,\nallowing independent optimization of each representation. Additionally, we\nenhance performance by integrating speech units, a proven unsupervised speech\nrepresentation technique, into waveform generation alongside mel-spectrograms.\nThis allows RESOUND to synthesize prosodic speech while preserving content and\nspeaker identity. Experiments conducted on two standard L2S benchmarks confirm\nthe effectiveness of the proposed method across various metrics."}
{"id": "2505.22045", "pdf": "https://arxiv.org/pdf/2505.22045", "abs": "https://arxiv.org/abs/2505.22045", "authors": ["Le Xu", "Chenxing Li", "Yong Ren", "Yujie Chen", "Yu Gu", "Ruibo Fu", "Shan Yang", "Dong Yu"], "title": "Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Current vision-guided audio captioning systems frequently fail to address\naudiovisual misalignment in real-world scenarios, such as dubbed content or\noff-screen sounds. To bridge this critical gap, we present an entropy-aware\ngated fusion framework that dynamically modulates visual information flow\nthrough cross-modal uncertainty quantification. Our novel approach employs\nattention entropy analysis in cross-attention layers to automatically identify\nand suppress misleading visual cues during modal fusion. Complementing this\narchitecture, we develop a batch-wise audiovisual shuffling technique that\ngenerates synthetic mismatched training pairs, greatly enhancing model\nresilience against alignment noise. Evaluations on the AudioCaps benchmark\ndemonstrate our system's superior performance over existing baselines,\nespecially in mismatched modality scenarios. Furthermore, our solution\ndemonstrates an approximately 6x improvement in inference speed compared to the\nbaseline."}
{"id": "2505.22159", "pdf": "https://arxiv.org/pdf/2505.22159", "abs": "https://arxiv.org/abs/2505.22159", "authors": ["Jiawen Yu", "Hairuo Liu", "Qiaojun Yu", "Jieji Ren", "Ce Hao", "Haitong Ding", "Guangyu Huang", "Guofan Huang", "Yan Song", "Panpan Cai", "Cewu Lu", "Wenqiang Zhang"], "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have advanced general-purpose robotic\nmanipulation by leveraging pretrained visual and linguistic representations.\nHowever, they struggle with contact-rich tasks that require fine-grained\ncontrol involving force, especially under visual occlusion or dynamic\nuncertainty. To address these limitations, we propose \\textbf{ForceVLA}, a\nnovel end-to-end manipulation framework that treats external force sensing as a\nfirst-class modality within VLA systems. ForceVLA introduces \\textbf{FVLMoE}, a\nforce-aware Mixture-of-Experts fusion module that dynamically integrates\npretrained visual-language embeddings with real-time 6-axis force feedback\nduring action decoding. This enables context-aware routing across\nmodality-specific experts, enhancing the robot's ability to adapt to subtle\ncontact dynamics. We also introduce \\textbf{ForceVLA-Data}, a new dataset\ncomprising synchronized vision, proprioception, and force-torque signals across\nfive contact-rich manipulation tasks. ForceVLA improves average task success by\n23.2\\% over strong $\\pi_0$-based baselines, achieving up to 80\\% success in\ntasks such as plug insertion. Our approach highlights the importance of\nmultimodal integration for dexterous manipulation and sets a new benchmark for\nphysically intelligent robotic control. Code and data will be released at\nhttps://sites.google.com/view/forcevla2025."}
{"id": "2505.22193", "pdf": "https://arxiv.org/pdf/2505.22193", "abs": "https://arxiv.org/abs/2505.22193", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P40, 81P47, 68Q12, 68T07,", "I.2.6; I.3.3; J.2"], "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource."}
{"id": "2505.22258", "pdf": "https://arxiv.org/pdf/2505.22258", "abs": "https://arxiv.org/abs/2505.22258", "authors": ["Benjamin Serfling", "Hannes Reichert", "Lorenzo Bayerlein", "Konrad Doll", "Kati Radkhah-Lens"], "title": "LiDAR Based Semantic Perception for Forklifts in Outdoor Environments", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "In this study, we present a novel LiDAR-based semantic segmentation framework\ntailored for autonomous forklifts operating in complex outdoor environments.\nCentral to our approach is the integration of a dual LiDAR system, which\ncombines forward-facing and downward-angled LiDAR sensors to enable\ncomprehensive scene understanding, specifically tailored for industrial\nmaterial handling tasks. The dual configuration improves the detection and\nsegmentation of dynamic and static obstacles with high spatial precision. Using\nhigh-resolution 3D point clouds captured from two sensors, our method employs a\nlightweight yet robust approach that segments the point clouds into\nsafety-critical instance classes such as pedestrians, vehicles, and forklifts,\nas well as environmental classes such as driveable ground, lanes, and\nbuildings. Experimental validation demonstrates that our approach achieves high\nsegmentation accuracy while satisfying strict runtime requirements,\nestablishing its viability for safety-aware, fully autonomous forklift\nnavigation in dynamic warehouse and yard environments."}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks."}
{"id": "2505.22313", "pdf": "https://arxiv.org/pdf/2505.22313", "abs": "https://arxiv.org/abs/2505.22313", "authors": ["Kaixuan Wei", "Hector A. Jimenez-Romero", "Hadi Amata", "Jipeng Sun", "Qiang Fu", "Felix Heide", "Wolfgang Heidrich"], "title": "Large-Area Fabrication-aware Computational Diffractive Optics", "categories": ["physics.optics", "cs.CV", "cs.ET", "cs.GR"], "comment": null, "summary": "Differentiable optics, as an emerging paradigm that jointly optimizes optics\nand (optional) image processing algorithms, has made innovative optical designs\npossible across a broad range of applications. Many of these systems utilize\ndiffractive optical components (DOEs) for holography, PSF engineering, or\nwavefront shaping. Existing approaches have, however, mostly remained limited\nto laboratory prototypes, owing to a large quality gap between simulation and\nmanufactured devices. We aim at lifting the fundamental technical barriers to\nthe practical use of learned diffractive optical systems. To this end, we\npropose a fabrication-aware design pipeline for diffractive optics fabricated\nby direct-write grayscale lithography followed by nano-imprinting replication,\nwhich is directly suited for inexpensive mass production of large area designs.\nWe propose a super-resolved neural lithography model that can accurately\npredict the 3D geometry generated by the fabrication process. This model can be\nseamlessly integrated into existing differentiable optics frameworks, enabling\nfabrication-aware, end-to-end optimization of computational optical systems. To\ntackle the computational challenges, we also devise tensor-parallel compute\nframework centered on distributing large-scale FFT computation across many\nGPUs. As such, we demonstrate large scale diffractive optics designs up to\n32.16 mm $\\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760\nfeature points. We find adequate agreement between simulation and fabricated\nprototypes for applications such as holography and PSF engineering. We also\nachieve high image quality from an imaging system comprised only of a single\nDOE, with images processed only by a Wiener filter utilizing the simulation\nPSF. We believe our findings lift the fabrication limitations for real-world\napplications of diffractive optics and differentiable optical design."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22335", "pdf": "https://arxiv.org/pdf/2505.22335", "abs": "https://arxiv.org/abs/2505.22335", "authors": ["Wancai Zheng", "Linlin Ou", "Jiajie He", "Libo Zhou", "Xinyi Yu", "Yan Wei"], "title": "UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous\nLocalization and Mapping (SLAM) have significantly progressed in tracking and\nhigh-fidelity mapping. However, their sequential optimization framework and\nsensitivity to dynamic objects limit real-time performance and robustness in\nreal-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for\ndynamic environments that decouples tracking and mapping through a parallelized\nframework. A probabilistic octree is employed to manage Gaussian primitives\nadaptively, enabling efficient initialization and pruning without hand-crafted\nthresholds. To robustly filter dynamic regions during tracking, we propose a\ntraining-free uncertainty estimator that fuses multi-modal residuals to\nestimate per-pixel motion uncertainty, achieving open-set dynamic object\nhandling without reliance on semantic labels. Furthermore, a temporal encoder\nis designed to enhance rendering quality. Concurrently, low-dimensional\nfeatures are efficiently transformed via a shallow multilayer perceptron to\nconstruct DINO features, which are then employed to enrich the Gaussian field\nand improve the robustness of uncertainty prediction. Extensive experiments on\nmultiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art\nmethods in both localization accuracy (by 59.8%) and rendering quality (by 4.57\ndB PSNR), while maintaining real-time performance and producing reusable,\nartifact-free static maps in dynamic environments.The project:\nhttps://aczheng-cai.github.io/up_slam.github.io/"}
{"id": "2505.22400", "pdf": "https://arxiv.org/pdf/2505.22400", "abs": "https://arxiv.org/abs/2505.22400", "authors": ["Zehao Li", "Hao Jiang", "Yujun Cai", "Jianing Chen", "Baolong Bi", "Shuqin Gao", "Honglong Zhao", "Yiwei Wang", "Tianlu Mao", "Zhaoqi Wang"], "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although dynamic scene reconstruction has long been a fundamental challenge\nin 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a\npromising direction by enabling high-quality, real-time rendering through\nexplicit Gaussian primitives. However, existing 3DGS-based methods for dynamic\nreconstruction often suffer from \\textit{spatio-temporal incoherence} during\ninitialization, where canonical Gaussians are constructed by aggregating\nobservations from multiple frames without temporal distinction. This results in\nspatio-temporally entangled representations, making it difficult to model\ndynamic motion accurately. To overcome this limitation, we propose\n\\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a\nplug-and-play module that learns spatio-temporal probability distributions for\neach Gaussian. STDR introduces a spatio-temporal mask, a separated deformation\nfield, and a consistency regularization to jointly disentangle spatial and\ntemporal patterns. Extensive experiments demonstrate that incorporating our\nmodule into existing 3DGS-based dynamic scene reconstruction frameworks leads\nto notable improvements in both reconstruction quality and spatio-temporal\nconsistency across synthetic and real-world benchmarks."}
{"id": "2505.22416", "pdf": "https://arxiv.org/pdf/2505.22416", "abs": "https://arxiv.org/abs/2505.22416", "authors": ["Sihun Cha", "Serin Yoon", "Kwanggyoon Seo", "Junyong Noh"], "title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurately retargeting facial expressions to a face mesh while enabling\nmanipulation is a key challenge in facial animation retargeting. Recent\ndeep-learning methods address this by encoding facial expressions into a global\nlatent code, but they often fail to capture fine-grained details in local\nregions. While some methods improve local accuracy by transferring deformations\nlocally, this often complicates overall control of the facial expression. To\naddress this, we propose a method that combines the strengths of both global\nand local deformation models. Our approach enables intuitive control and\ndetailed expression cloning across diverse face meshes, regardless of their\nunderlying structures. The core idea is to localize the influence of the global\nlatent code on the target mesh. Our model learns to predict skinning weights\nfor each vertex of the target face mesh through indirect supervision from\npredefined segmentation labels. These predicted weights localize the global\nlatent code, enabling precise and region-specific deformations even for meshes\nwith unseen shapes. We supervise the latent code using Facial Action Coding\nSystem (FACS)-based blendshapes to ensure interpretability and allow\nstraightforward editing of the generated animation. Through extensive\nexperiments, we demonstrate improved performance over state-of-the-art methods\nin terms of expression fidelity, deformation transfer accuracy, and\nadaptability across diverse mesh structures."}
{"id": "2505.22438", "pdf": "https://arxiv.org/pdf/2505.22438", "abs": "https://arxiv.org/abs/2505.22438", "authors": ["Zijian Liang", "Kai Niu", "Changshuo Wang", "Jin Xu", "Ping Zhang"], "title": "Synonymous Variational Inference for Perceptual Image Compression", "categories": ["cs.IT", "cs.AI", "cs.CV", "cs.LG", "eess.IV", "math.IT"], "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster", "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/."}
{"id": "2505.22486", "pdf": "https://arxiv.org/pdf/2505.22486", "abs": "https://arxiv.org/abs/2505.22486", "authors": ["Mujtaba Hussain Mirza", "Maria Rosaria Briglia", "Filippo Bartolucci", "Senad Beadini", "Giuseppe Lisanti", "Iacopo Masi"], "title": "Understanding Adversarial Training with Energy-based Models", "categories": ["cs.LG", "cs.CV"], "comment": "Under review for TPAMI", "summary": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."}
{"id": "2505.22489", "pdf": "https://arxiv.org/pdf/2505.22489", "abs": "https://arxiv.org/abs/2505.22489", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "MICCAI2025 Submitted version", "summary": "We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications."}
{"id": "2505.22496", "pdf": "https://arxiv.org/pdf/2505.22496", "abs": "https://arxiv.org/abs/2505.22496", "authors": ["Long Hui"], "title": "Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays", "categories": ["eess.IV", "cs.CV", "stat.AP"], "comment": null, "summary": "This paper presents a novel approach to catheter and line position detection\nin chest X-rays, combining multi-task learning with risk-sensitive conformal\nprediction to address critical clinical requirements. Our model simultaneously\nperforms classification, segmentation, and landmark detection, leveraging the\nsynergistic relationship between these tasks to improve overall performance. We\nfurther enhance clinical reliability through risk-sensitive conformal\nprediction, which provides statistically guaranteed prediction sets with higher\nreliability for clinically critical findings. Experimental results demonstrate\nexcellent performance with 90.68\\% overall empirical coverage and 99.29\\%\ncoverage for critical conditions, while maintaining remarkable precision in\nprediction sets. Most importantly, our risk-sensitive approach achieves zero\nhigh-risk mispredictions (cases where the system dangerously declares\nproblematic tubes as confidently normal), making the system particularly\nsuitable for clinical deployment. This work offers both accurate predictions\nand reliably quantified uncertainty -- essential features for life-critical\nmedical applications."}
{"id": "2505.22511", "pdf": "https://arxiv.org/pdf/2505.22511", "abs": "https://arxiv.org/abs/2505.22511", "authors": ["Siyeop Yoon", "Yujin Oh", "Pengfei Jin", "Sifan Song", "Matthew Tivnan", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface", "categories": ["eess.IV", "cs.CV"], "comment": "Neurips 2025 submitted", "summary": "We present Surf2CT, a novel cascaded flow matching framework that synthesizes\nfull 3D computed tomography (CT) volumes of the human torso from external\nsurface scans and simple demographic data (age, sex, height, weight). This is\nthe first approach capable of generating realistic volumetric internal anatomy\nimages solely based on external body shape and demographics, without any\ninternal imaging. Surf2CT proceeds through three sequential stages: (1) Surface\nCompletion, reconstructing a complete signed distance function (SDF) from\npartial torso scans using conditional 3D flow matching; (2) Coarse CT\nSynthesis, generating a low-resolution CT volume from the completed SDF and\ndemographic information; and (3) CT Super-Resolution, refining the coarse\nvolume into a high-resolution CT via a patch-wise conditional flow model. Each\nstage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained\nour model on a combined dataset of 3,198 torso CT scans (approximately 1.13\nmillion axial slices) sourced from Massachusetts General Hospital (MGH) and the\nAutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated\nstrong anatomical fidelity: organ volumes exhibited small mean percentage\ndifferences (range from -11.1% to 4.4%), and muscle/fat body composition\nmetrics matched ground truth with strong correlation (range from 0.67 to 0.96).\nLung localization had minimal bias (mean difference -2.5 mm), and surface\ncompletion significantly improved metrics (Chamfer distance: from 521.8 mm to\n2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new\nparadigm for non-invasive internal anatomical imaging using only external data,\nopening opportunities for home-based healthcare, preventive medicine, and\npersonalized clinical assessments without the risks associated with\nconventional imaging techniques."}
{"id": "2505.22568", "pdf": "https://arxiv.org/pdf/2505.22568", "abs": "https://arxiv.org/abs/2505.22568", "authors": ["Aravind R. Krishnan", "Thomas Z. Li", "Lucas W. Remedios", "Michael E. Kim", "Chenyu Gao", "Gaurav Rudravaram", "Elyssa M. McMaster", "Adam M. Saunders", "Shunxing Bao", "Kaiwen Xu", "Lianrui Zuo", "Kim L. Sandler", "Fabien Maldonado", "Yuankai Huo", "Bennett A. Landman"], "title": "Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Reconstruction kernels in computed tomography (CT) affect spatial resolution\nand noise characteristics, introducing systematic variability in quantitative\nimaging measurements such as emphysema quantification. Choosing an appropriate\nkernel is therefore essential for consistent quantitative analysis. We propose\na multipath cycleGAN model for CT kernel harmonization, trained on a mixture of\npaired and unpaired data from a low-dose lung cancer screening cohort. The\nmodel features domain-specific encoders and decoders with a shared latent space\nand uses discriminators tailored for each domain.We train the model on 42\nkernel combinations using 100 scans each from seven representative kernels in\nthe National Lung Screening Trial (NLST) dataset. To evaluate performance, 240\nscans from each kernel are harmonized to a reference soft kernel, and emphysema\nis quantified before and after harmonization. A general linear model assesses\nthe impact of age, sex, smoking status, and kernel on emphysema. We also\nevaluate harmonization from soft kernels to a reference hard kernel. To assess\nanatomical consistency, we compare segmentations of lung vessels, muscle, and\nsubcutaneous adipose tissue generated by TotalSegmentator between harmonized\nand original images. Our model is benchmarked against traditional and\nswitchable cycleGANs. For paired kernels, our approach reduces bias in\nemphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,\nharmonization eliminates confounding differences in emphysema (p>0.05). High\nDice scores confirm preservation of muscle and fat anatomy, while lung vessel\noverlap remains reasonable. Overall, our shared latent space multipath cycleGAN\nenables robust harmonization across paired and unpaired CT kernels, improving\nemphysema quantification and preserving anatomical fidelity."}
{"id": "2505.22592", "pdf": "https://arxiv.org/pdf/2505.22592", "abs": "https://arxiv.org/abs/2505.22592", "authors": ["Yiheng Li", "Francisco Carrillo-Perez", "Mohammed Alawad", "Olivier Gevaert"], "title": "Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Lung cancer is the leading cause of cancer mortality worldwide, and\nnon-invasive methods for detecting key mutations and staging are essential for\nimproving patient outcomes. Here, we compare the performance of two machine\nlearning models - FMCIB+XGBoost, a supervised model with domain-specific\npretraining, and Dinov2+ABMIL, a self-supervised model with attention-based\nmultiple-instance learning - on 3D lung nodule data from the Stanford\nRadiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation\ndetection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving\naccuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In\ncancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving\nan accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,\nsuggesting SSL's adaptability across diverse datasets. Our results emphasize\nthe clinical utility of supervised models in mutation detection and highlight\nthe potential of SSL to improve staging generalization, while identifying areas\nfor enhancement in mutation sensitivity."}
{"id": "2505.22609", "pdf": "https://arxiv.org/pdf/2505.22609", "abs": "https://arxiv.org/abs/2505.22609", "authors": ["Alanna Hazlett", "Naomi Ohashi", "Timothy Rodriguez", "Sodiq Adewole"], "title": "Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we investigate the performance across multiple classification\nmodels to classify chest X-ray images into four categories of COVID-19,\npneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning\ntechniques with state-of-the-art pre-trained Convolutional Neural Networks\n(CNNs) models. We fine-tuned these pre-trained architectures on a labeled\nmedical x-ray images. The initial results are promising with high accuracy and\nstrong performance in key classification metrics such as precision, recall, and\nF1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for\nmodel interpretability to provide visual explanations for classification\ndecisions, improving trust and transparency in clinical applications."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
