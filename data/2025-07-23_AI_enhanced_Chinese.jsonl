{"id": "2507.16952", "title": "Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset", "authors": ["Md Min-Ha-Zul Abedin", "Tazqia Mehrub"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16952v1", "summary": "This study investigates the effectiveness of several machine learning\nalgorithms for static malware detection using the EMBER dataset, which contains\nfeature representations of Portable Executable (PE) files. We evaluate eight\nclassification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\nHistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three\npreprocessing settings: original feature space, Principal Component Analysis\n(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on\naccuracy, precision, recall, F1 score, and AUC to examine both predictive\nperformance and robustness. Ensemble methods, especially LightGBM and XGBoost,\nshow the best overall performance across all configurations, with minimal\nsensitivity to PCA and consistent generalization. LDA improves KNN performance\nbut significantly reduces accuracy for boosting models. TabNet, while promising\nin theory, underperformed under feature reduction, likely due to architectural\nsensitivity to input structure. The analysis is supported by detailed\nexploratory data analysis (EDA), including mutual information ranking, PCA or\nt-SNE visualizations, and outlier detection using Isolation Forest and Local\nOutlier Factor (LOF), which confirm the discriminatory capacity of key features\nin the EMBER dataset. The results suggest that boosting models remain the most\nreliable choice for high-dimensional static malware detection, and that\ndimensionality reduction should be applied selectively based on model type.\nThis work provides a benchmark for comparing classification models and\npreprocessing strategies in malware detection tasks and contributes insights\nthat can guide future system development and real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16952v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "使用EMBER数据集评估集成和深度学习模型在静态恶意软件检测中的降维应用", "tldr": "本研究使用EMBER数据集评估了多种机器学习模型（集成和深度学习）在静态恶意软件检测中的表现，并分析了降维技术（PCA和LDA）的影响，发现集成模型（特别是LightGBM和XGBoost）表现最佳。", "motivation": "本研究旨在调查多种机器学习算法在静态恶意软件检测中的有效性，并评估不同预处理设置（包括降维）对模型性能和鲁棒性的影响。", "method": "研究使用了EMBER数据集，该数据集包含可执行文件（PE）的特征表示。评估了八种分类模型：LightGBM、XGBoost、CatBoost、Random Forest、Extra Trees、HistGradientBoosting、k-Nearest Neighbors (KNN) 和 TabNet。在三种预处理设置下进行评估：原始特征空间、主成分分析（PCA）和线性判别分析（LDA）。模型通过准确率、精确率、召回率、F1分数和AUC进行评估。同时进行了探索性数据分析（EDA），包括互信息排序、PCA或t-SNE可视化以及使用Isolation Forest和Local Outlier Factor (LOF)进行异常值检测。", "result": "集成方法，特别是LightGBM和XGBoost，在所有配置下都表现出最佳的整体性能，对PCA的敏感性最小，并具有一致的泛化能力。LDA改善了KNN的性能，但显著降低了Boosting模型的准确率。TabNet在特征降维下表现不佳，可能由于其架构对输入结构的敏感性。探索性数据分析证实了EMBER数据集中关键特征的判别能力。", "conclusion": "Boosting模型仍然是高维静态恶意软件检测中最可靠的选择，并且降维应根据模型类型选择性应用。这项工作为恶意软件检测任务中的分类模型和预处理策略提供了基准，并提供了可指导未来系统开发和实际部署的见解。", "translation": "本研究调查了使用EMBER数据集进行静态恶意软件检测的几种机器学习算法的有效性，该数据集包含可执行文件（PE）的特征表示。我们评估了八种分类模型：LightGBM、XGBoost、CatBoost、Random Forest、Extra Trees、HistGradientBoosting、k-Nearest Neighbors (KNN) 和 TabNet，在三种预处理设置下：原始特征空间、主成分分析（PCA）和线性判别分析（LDA）。模型通过准确率、精确率、召回率、F1分数和AUC进行评估，以检查预测性能和鲁棒性。集成方法，特别是LightGBM和XGBoost，在所有配置下都显示出最佳的整体性能，对PCA的敏感性最小，并具有一致的泛化能力。LDA改善了KNN的性能，但显著降低了Boosting模型的准确率。TabNet，虽然理论上有前景，但在特征降维下表现不佳，这可能归因于其架构对输入结构的敏感性。该分析得到了详细的探索性数据分析（EDA）的支持，包括互信息排序、PCA或t-SNE可视化以及使用Isolation Forest和Local Outlier Factor (LOF)进行异常值检测，这些都证实了EMBER数据集中关键特征的判别能力。结果表明，Boosting模型仍然是高维静态恶意软件检测最可靠的选择，并且降维应根据模型类型选择性应用。这项工作为恶意软件检测任务中的分类模型和预处理策略提供了基准，并提供了可指导未来系统开发和实际部署的见解。", "summary": "本研究评估了多种机器学习模型（包括集成和深度学习）在基于EMBER数据集的静态恶意软件检测中的性能。实验比较了八种分类器在原始特征空间、PCA和LDA三种预处理设置下的表现。结果显示，集成模型，特别是LightGBM和XGBoost，在所有配置下均表现最佳且泛化能力强，而降维技术需根据模型类型选择性应用。该工作为恶意软件检测提供了模型和预处理策略的基准，并为未来系统开发提供指导。", "keywords": "恶意软件检测, 机器学习, 集成学习, 深度学习, 降维", "comments": "该研究全面评估了多种主流机器学习模型在恶意软件检测任务中的表现，并深入探讨了降维技术的影响，为实际部署提供了宝贵的指导。其创新点在于对不同模型与降维策略结合的系统性分析，尤其指出了Boosting模型在高维数据下的优越性以及TabNet在降维下的局限性。为领域内研究和应用提供了重要的基准和见解。"}}
{"id": "2501.14644", "title": "Optimizing Privacy-Utility Trade-off in Decentralized Learning with Generalized Correlated Noise", "authors": ["Angelo Rodio", "Zheng Chen", "Erik G. Larsson"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, accepted at IEEE ITW 2025", "url": "http://arxiv.org/abs/2501.14644v2", "summary": "Decentralized learning enables distributed agents to collaboratively train a\nshared machine learning model without a central server, through local\ncomputation and peer-to-peer communication. Although each agent retains its\ndataset locally, sharing local models can still expose private information\nabout the local training datasets to adversaries. To mitigate privacy attacks,\na common strategy is to inject random artificial noise at each agent before\nexchanging local models between neighbors. However, this often leads to utility\ndegradation due to the negative effects of cumulated artificial noise on the\nlearning algorithm. In this work, we introduce CorN-DSGD, a novel\ncovariance-based framework for generating correlated privacy noise across\nagents, which unifies several state-of-the-art methods as special cases. By\nleveraging network topology and mixing weights, CorN-DSGD optimizes the noise\ncovariance to achieve network-wide noise cancellation. Experimental results\nshow that CorN-DSGD cancels more noise than existing pairwise correlation\nschemes, improving model performance under formal privacy guarantees.", "comment": "6 pages, 5 figures, accepted at IEEE ITW 2025", "pdf_url": "http://arxiv.org/pdf/2501.14644v2", "cate": "cs.LG", "date": "2025-01-24", "updated": "2025-07-23", "AI": {"title_translation": "在去中心化学习中利用广义相关噪声优化隐私-效用权衡", "tldr": "提出CorN-DSGD，一种新的去中心化学习隐私保护方法，通过生成相关噪声来抵消累积噪声，从而在保证隐私的同时提高模型性能。", "motivation": "去中心化学习中，尽管数据本地存储，但共享本地模型仍可能泄露隐私。现有通过注入随机噪声来保护隐私的方法会导致累积噪声，从而降低模型性能。", "method": "提出CorN-DSGD，一个新颖的基于协方差的框架，用于在代理之间生成相关的隐私噪声。该方法利用网络拓扑和混合权重来优化噪声协方差，实现全网络范围的噪声抵消。", "result": "实验结果表明，CorN-DSGD比现有的成对相关方案能抵消更多的噪声，并在形式化隐私保证下提高了模型性能。", "conclusion": "CorN-DSGD成功解决了去中心化学习中隐私保护与模型效用之间的权衡问题，通过引入广义相关噪声实现了更好的噪声抵消和性能提升。", "translation": "去中心化学习使分布式代理能够在没有中央服务器的情况下，通过本地计算和点对点通信协作训练共享机器学习模型。尽管每个代理都将数据集保留在本地，但共享本地模型仍然可能向攻击者暴露有关本地训练数据集的私人信息。为了减轻隐私攻击，一种常见的策略是在每个代理在邻居之间交换本地模型之前注入随机人工噪声。然而，这通常会导致由于累积人工噪声对学习算法的负面影响而导致效用下降。在这项工作中，我们引入了CorN-DSGD，这是一种新颖的基于协方差的框架，用于在代理之间生成相关的隐私噪声，它将几种最先进的方法统一为特例。通过利用网络拓扑和混合权重，CorN-DSGD优化了噪声协方差以实现全网络范围的噪声抵消。实验结果表明，CorN-DSGD比现有成对相关方案抵消了更多的噪声，在形式化隐私保证下提高了模型性能。", "summary": "这篇论文提出了一种名为CorN-DSGD的新型协方差框架，用于去中心化学习中的隐私保护。针对现有随机噪声注入方法导致的隐私-效用权衡问题，CorN-DSGD通过利用网络拓扑和混合权重生成相关的隐私噪声，并优化噪声协方差以实现网络范围的噪声抵消。实验证明，CorN-DSGD在提供形式化隐私保证的同时，能比现有方法更有效地抵消噪声，从而提高模型性能。", "keywords": "去中心化学习, 隐私保护, 相关噪声, 效用权衡, CorN-DSGD", "comments": "这项工作通过引入广义相关噪声和利用网络拓扑来优化噪声抵消，为去中心化学习中的隐私保护提供了一种创新方法。它统一了现有技术，并有效地解决了隐私与模型效用之间的核心权衡问题，具有重要的实践意义。"}}
{"id": "2411.01579", "title": "Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs", "authors": ["Shuo Tan", "Rui Liu", "Xuesong Han", "XianLei Long", "Kai Wan", "Linqi Song", "Yong Li"], "categories": ["cs.DC", "cs.AI", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      16 pages, 6 figures", "url": "http://arxiv.org/abs/2411.01579v2", "summary": "Deploying Convolutional Neural Networks (CNNs) on resource-constrained\ndevices necessitates efficient management of computational resources, often via\ndistributed environments susceptible to latency from straggler nodes. This\npaper introduces the Flexible Coded Distributed Convolution Computing (FCDCC)\nframework to enhance straggler resilience and numerical stability in\ndistributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant\nand Rotation Matrix Embedding (CRME) which was originally proposed for matrix\nmultiplication to high-dimensional tensor convolution. For the proposed scheme,\nreferred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme,\nwe also propose two new coded partitioning schemes: Adaptive-Padding Coded\nPartitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning\n(KCCP) for the filter tensor. These strategies enable linear decomposition of\ntensor convolutions and encoding them into CDC subtasks, combining model\nparallelism with coded redundancy for robust and efficient execution.\nTheoretical analysis identifies an optimal trade-off between communication and\nstorage costs. Empirical results validate the framework's effectiveness in\ncomputational efficiency, straggler resilience, and scalability across various\nCNN architectures.", "comment": "16 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2411.01579v2", "cate": "cs.DC", "date": "2024-11-03", "updated": "2025-07-23", "AI": {"title_translation": "用于分布式卷积神经网络中增强拖延者弹性与数值稳定性的灵活编码分布式卷积计算", "tldr": "本文提出FCDCC框架，通过扩展编码分布式计算，并引入新的编码分区方案，提升分布式CNNs的拖延者弹性、数值稳定性和计算效率。", "motivation": "在资源受限设备上部署卷积神经网络（CNNs）需要高效管理计算资源，而分布式环境容易受到拖延者节点引起的延迟影响。", "method": "本文提出了灵活编码分布式卷积计算（FCDCC）框架，通过将编码分布式计算（CDC）与循环矩阵和旋转矩阵嵌入（CRME）扩展到高维张量卷积，形成数值稳定编码张量卷积（NSCTC方案）。此外，还提出了两种新的编码分区方案：针对输入张量的自适应填充编码分区（APCP）和针对滤波器张量的核通道编码分区（KCCP）。这些策略实现了张量卷积的线性分解和编码，结合了模型并行和编码冗余。", "result": "理论分析确定了通信和存储成本之间的最佳权衡。实证结果验证了该框架在计算效率、拖延者弹性以及各种CNN架构下的可扩展性方面的有效性。", "conclusion": "FCDCC框架通过引入创新的编码分区方案和扩展CDC，有效提升了分布式CNNs在资源受限环境下的性能，解决了拖延者和数值稳定性问题。", "translation": "在资源受限设备上部署卷积神经网络（CNNs）需要高效管理计算资源，这通常通过容易受到拖延者节点延迟影响的分布式环境进行。本文引入了灵活编码分布式卷积计算（FCDCC）框架，以增强分布式CNNs中的拖延者弹性和数值稳定性。我们将最初为矩阵乘法提出的编码分布式计算（CDC）与循环矩阵和旋转矩阵嵌入（CRME）扩展到高维张量卷积。对于所提出的方案，即数值稳定编码张量卷积（NSCTC）方案，我们还提出了两种新的编码分区方案：用于输入张量的自适应填充编码分区（APCP）和用于滤波器张量的核通道编码分区（KCCP）。这些策略实现了张量卷积的线性分解，并将其编码为CDC子任务，结合了模型并行性与编码冗余，以实现鲁棒和高效的执行。理论分析确定了通信和存储成本之间的最佳权衡。实证结果验证了该框架在计算效率、拖延者弹性以及各种CNN架构下的可扩展性方面的有效性。", "summary": "本文提出了灵活编码分布式卷积计算（FCDCC）框架，旨在解决分布式CNN中因拖延者节点导致的延迟问题，并提高数值稳定性。该框架通过将编码分布式计算（CDC）与循环矩阵和旋转矩阵嵌入（CRME）扩展到高维张量卷积，并引入了自适应填充编码分区（APCP）和核通道编码分区（KCCP）两种新的编码分区方案。这些方案实现了张量卷积的线性分解和编码，结合了模型并行和编码冗余，从而提升了计算效率、拖延者弹性和可扩展性。", "keywords": "分布式CNNs, 编码分布式计算, 拖延者弹性, 数值稳定性, 张量卷积", "comments": "该论文通过将编码分布式计算（CDC）扩展到高维张量卷积，并引入创新的编码分区策略，有效地解决了分布式CNN中拖延者和数值稳定性两大挑战。其贡献在于提供了一个理论和实践上都得到验证的鲁棒高效的分布式计算框架。"}}
{"id": "2507.17534", "title": "Federated Majorize-Minimization: Beyond Parameter Aggregation", "authors": ["Aymeric Dieuleveut", "Gersende Fort", "Mahmoud Hegazy", "Hoi-To Wai"], "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17534v1", "summary": "This paper proposes a unified approach for designing stochastic optimization\nalgorithms that robustly scale to the federated learning setting. Our work\nstudies a class of Majorize-Minimization (MM) problems, which possesses a\nlinearly parameterized family of majorizing surrogate functions. This framework\nencompasses (proximal) gradient-based algorithms for (regularized) smooth\nobjectives, the Expectation Maximization algorithm, and many problems seen as\nvariational surrogate MM. We show that our framework motivates a unifying\nalgorithm called Stochastic Approximation Stochastic Surrogate MM (\\SSMM),\nwhich includes previous stochastic MM procedures as special instances. We then\nextend \\SSMM\\ to the federated setting, while taking into consideration common\nbottlenecks such as data heterogeneity, partial participation, and\ncommunication constraints; this yields \\QSMM. The originality of \\QSMM\\ is to\nlearn locally and then aggregate information characterizing the\n\\textit{surrogate majorizing function}, contrary to classical algorithms which\nlearn and aggregate the \\textit{original parameter}. Finally, to showcase the\nflexibility of this methodology beyond our theoretical setting, we use it to\ndesign an algorithm for computing optimal transport maps in the federated\nsetting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17534v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "联邦化Majorize-Minimization：超越参数聚合", "tldr": "本文提出了一种统一的联邦学习随机优化算法设计方法，通过聚合代理增广函数而非原始参数来解决联邦学习中的瓶颈问题。", "motivation": "为了设计能够鲁棒地扩展到联邦学习环境的随机优化算法，并解决数据异质性、部分参与和通信限制等常见瓶颈。", "method": "本文提出了一种Majorize-Minimization（MM）问题的统一方法，该方法拥有线性参数化的增广代理函数族。在此框架下，作者提出了统一的算法Stochastic Approximation Stochastic Surrogate MM (SSMM)，并将其扩展到联邦设置，考虑到常见瓶颈，得到了QSMM。QSMM的独创性在于在本地学习后聚合表征“代理增广函数”的信息，而非经典算法聚合“原始参数”。", "result": "该框架激发了一种名为SSMM的统一算法，SSMM包含了以前的随机MM过程作为特例。将SSMM扩展到联邦设置后得到了QSMM，其独创性在于聚合代理增广函数而非原始参数。此外，该方法被用于设计联邦设置下计算最优传输映射的算法，展示了其灵活性。", "conclusion": "QSMM的独创性在于学习本地信息并聚合表征代理增广函数的信息，这与聚合原始参数的经典算法不同。该方法具有超越理论设置的灵活性，例如可用于设计联邦设置下的最优传输映射算法。", "translation": "本文提出了一种设计随机优化算法的统一方法，该方法能够鲁棒地扩展到联邦学习设置。我们的工作研究了一类Majorize-Minimization（MM）问题，该问题拥有一系列线性参数化的增广代理函数。这个框架涵盖了（正则化）平滑目标的（近端）基于梯度的算法、期望最大化算法以及许多被视为变分代理MM的问题。我们表明，我们的框架激发了一种名为随机近似随机代理MM（SSMM）的统一算法，该算法包括以前的随机MM过程作为特殊实例。然后，我们将SSMM扩展到联邦设置，同时考虑到数据异质性、部分参与和通信约束等常见瓶颈；这产生了QSMM。QSMM的独创性在于在本地学习后聚合表征代理增广函数的信息，这与学习和聚合原始参数的经典算法相反。最后，为了展示这种方法论超越我们理论设置的灵活性，我们使用它来设计一种在联邦设置下计算最优传输映射的算法。", "summary": "本文提出了一种针对联邦学习环境的统一随机优化算法设计方法。该方法基于Majorize-Minimization (MM) 问题，并引入了Stochastic Approximation Stochastic Surrogate MM (SSMM) 算法。为了适应联邦学习的挑战，如数据异质性和通信限制，作者进一步开发了QSMM算法。QSMM的核心创新在于聚合代理增广函数的信息，而非传统的原始参数，这使其能够更有效地应对联邦学习的瓶颈。研究还通过设计联邦最优传输映射算法展示了该方法的灵活性。", "keywords": "联邦学习, Majorize-Minimization, 随机优化, 代理函数, 参数聚合", "comments": "本文的创新点在于提出了联邦化Majorize-Minimization (QSMM) 算法，其核心思想是聚合代理增广函数而非原始参数。这为解决联邦学习中数据异质性、部分参与和通信约束等挑战提供了一种新颖且可能更有效的方法。这种“超越参数聚合”的范式转变可能对联邦学习的优化算法设计产生重要影响。"}}
{"id": "2507.16964", "title": "DDFEM: A Python Package for Diffuse Domain Methods", "authors": ["Luke Benfield", "Andreas Dedner"], "categories": ["math.NA", "cs.NA", "65-04, 65M60, 65M50"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16964v1", "summary": "Solving partial differential equations (PDEs) on complex domains can present\nsignificant computational challenges. The Diffuse Domain Method (DDM) is an\nalternative that reformulates the partial differential equations on a larger,\nsimpler domain. The original geometry is embedded into the problem by\nrepresenting it with a phase-field function. This paper introduces ddfem, an\nextensible Python package to provide a framework for transforming PDEs into a\nDiffuse Domain formulation. We aim to make the application of a variety of\ndifferent Diffuse Domain approaches more accessible and straightforward to use.\nThe ddfem package includes features to intuitively define complex domains by\ncombining signed distance functions and provides a number of DDM transformers\nfor general second evolution equations. In addition, we present a new approach\nfor combining multiple boundary conditions of different types on distinct\nboundary segments. This is achieved by applying a normalised weighting, derived\nfrom multiple phase fields, to combine the additional boundary terms in the\nDiffuse Domain formulations. The domain definition and Diffuse Domain\ntransformation provided by our package are designed to be compatible with a\nwide range of existing finite element solvers without requiring code\nalterations. Both the original (non-linear) PDEs provided by the user and the\nresulting transformed PDEs on the extended domain are defined using the Unified\nForm Language UFL which is a domain specific language used by a number of\nsoftware packages. Our experiments were carried out using the Dune-Fem\nframework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16964v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "DDFEM：一个用于扩散域方法的Python包", "tldr": "本文介绍了DDFEM，一个Python包，用于将偏微分方程（PDE）在复杂域上的求解问题转化为扩散域方法（DDM）的公式，旨在提高其可访问性和易用性，并兼容现有有限元求解器。", "motivation": "在复杂域上求解偏微分方程（PDE）会带来显著的计算挑战。扩散域方法（DDM）通过在更大、更简单的域上重新表述PDE来解决这一问题。本文旨在提供一个可扩展的Python包，使各种不同的扩散域方法的应用更易于访问和使用。", "method": "本文引入了ddfem，一个Python包，提供将PDE转换为扩散域公式的框架。该包支持通过组合符号距离函数来定义复杂域，并提供用于一般二阶演化方程的DDM转换器。此外，它提出了一种通过应用从多个相场导出的归一化加权来组合不同类型边界条件的新方法。ddfem设计的域定义和扩散域转换与现有有限元求解器兼容，无需代码修改，并使用统一形式语言UFL定义PDE。实验在Dune-Fem框架下进行。", "result": "开发了ddfem Python包，该包提供了将PDE转换为扩散域公式的工具，并包含直观定义复杂域的功能以及用于一般二阶演化方程的DDM转换器。提出了一种结合多种边界条件的新方法。该包与现有有限元求解器兼容，并使用UFL定义PDE。实验在Dune-Fem框架下进行。", "conclusion": "本文介绍了DDFEM，一个Python包，旨在简化和提高在复杂域上应用扩散域方法求解偏微分方程的可访问性。该包提供了一种处理复杂边界条件的新方法，并确保与现有有限元求解器的兼容性。", "translation": "在复杂域上求解偏微分方程（PDE）可能会带来显著的计算挑战。扩散域方法（DDM）是一种替代方案，它将偏微分方程重新表述在一个更大、更简单的域上。原始几何形状通过使用相场函数表示被嵌入到问题中。本文介绍了ddfem，一个可扩展的Python包，旨在提供一个将PDE转换为扩散域公式的框架。我们的目标是使各种不同的扩散域方法的应用更易于访问和使用。ddfem包包括通过组合符号距离函数来直观定义复杂域的功能，并提供了许多用于一般二阶演化方程的DDM转换器。此外，我们提出了一种新的方法，用于在不同的边界段上组合多种不同类型的边界条件。这是通过应用从多个相场导出的归一化加权来实现的，以在扩散域公式中组合额外的边界项。我们包提供的域定义和扩散域转换旨在与各种现有有限元求解器兼容，而无需更改代码。用户提供的原始（非线性）PDE以及扩展域上产生的转换后的PDE都使用统一形式语言UFL定义，UFL是一种被许多软件包使用的领域特定语言。我们的实验是使用Dune-Fem框架进行的。", "summary": "本文介绍了DDFEM，一个可扩展的Python包，旨在简化扩散域方法（DDM）在复杂域上求解偏微分方程（PDE）的应用。DDFEM通过相场函数在更大、更简单的域上重新表述PDE，并提供了直观的域定义和DDM转换功能。该包还提出了一种新颖的方法，通过多相场导出的归一化加权来组合不同边界段上的多种边界条件。DDFEM旨在与现有有限元求解器兼容，并使用统一形式语言（UFL）定义PDE，其功能在Dune-Fem框架下进行了验证。", "keywords": "扩散域方法, 偏微分方程, Python包, 有限元方法, 复杂域, 边界条件", "comments": "该论文提出了一个有价值的软件包，解决了在复杂域上求解偏微分方程的计算挑战。其创新之处在于为扩散域方法（DDM）提供了一个易于访问的框架，并提出了一种处理复杂边界条件的新方法，从而增强了DDM的实用性。与现有有限元求解器的兼容性是其一个显著优势。"}}
{"id": "2505.02586", "title": "RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet", "authors": ["Eliraz Orfaig", "Inna Stainvas", "Igal Bilik"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.02586v3", "summary": "This work introduces RGBX-DiffusionDet, an object detection framework\nextending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB\nimagery via an adaptive multimodal encoder. To enable cross-modal interaction,\nwe design the dynamic channel reduction within a convolutional block attention\nmodule (DCR-CBAM), which facilitates cross-talk between subnetworks by\ndynamically highlighting salient channel features. Furthermore, the dynamic\nmulti-level aggregation block (DMLAB) is proposed to refine spatial feature\nrepresentations through adaptive multiscale fusion. Finally, novel\nregularization losses that enforce channel saliency and spatial selectivity are\nintroduced, leading to compact and discriminative feature embeddings. Extensive\nexperiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric\ndataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We\ndemonstrate consistent superiority of the proposed approach over the baseline\nRGB-only DiffusionDet. The modular architecture maintains the original decoding\ncomplexity, ensuring efficiency. These results establish the proposed\nRGBX-DiffusionDet as a flexible multimodal object detection approach, providing\nnew insights into integrating diverse 2D sensing modalities into\ndiffusion-based detection pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.02586v3", "cate": "cs.CV", "date": "2025-05-05", "updated": "2025-07-23", "AI": {"title_translation": "RGBX-DiffusionDet：一种使用DiffusionDet的多模态RGB-X目标检测框架", "tldr": "本文提出了RGBX-DiffusionDet，一个扩展DiffusionDet以融合RGB与异构2D数据（X）的多模态目标检测框架，通过引入动态通道缩减卷积块注意力模块（DCR-CBAM）、动态多级聚合块（DMLAB）和新颖的正则化损失，实现了更优的性能和效率。", "motivation": "现有DiffusionDet模型主要处理RGB数据，而本工作旨在扩展其能力，使其能够有效地融合异构2D数据（X）与RGB图像，以实现更强大的多模态目标检测。", "method": "本文提出了RGBX-DiffusionDet框架，通过以下方式实现多模态融合：1. 设计了一个自适应多模态编码器来融合RGB与异构2D数据（X）。2. 引入了动态通道缩减卷积块注意力模块（DCR-CBAM），通过动态突出显著通道特征来促进跨模态交互。3. 提出了动态多级聚合块（DMLAB），通过自适应多尺度融合来优化空间特征表示。4. 引入了强制通道显著性和空间选择性的新型正则化损失，以获得紧凑且具有判别力的特征嵌入。", "result": "在RGB-Depth (KITTI)、新型RGB-Polarimetric数据集和RGB-Infrared (M$^3$FD) 基准数据集上的大量实验表明，所提出的方法始终优于基线仅RGB的DiffusionDet。模块化架构保持了原始解码复杂度，确保了效率。", "conclusion": "RGBX-DiffusionDet被确立为一种灵活的多模态目标检测方法，为将多样化的2D传感模态集成到基于扩散的检测管道中提供了新见解。", "translation": "这项工作引入了RGBX-DiffusionDet，一个目标检测框架，它扩展了DiffusionDet模型，通过自适应多模态编码器将异构2D数据（X）与RGB图像融合。为了实现跨模态交互，我们设计了卷积块注意力模块（DCR-CBAM）内的动态通道缩减，通过动态突出显著通道特征来促进子网络之间的交叉对话。此外，提出了动态多级聚合块（DMLAB），通过自适应多尺度融合来优化空间特征表示。最后，引入了强制通道显著性和空间选择性的新型正则化损失，从而产生紧凑且具有判别力的特征嵌入。使用RGB-Depth (KITTI)、一个新颖的RGB-偏振数据集以及RGB-Infrared (M$^3$FD) 基准数据集进行了大量实验。我们证明了所提出的方法相对于基线仅RGB的DiffusionDet具有持续的优越性。模块化架构保持了原始解码复杂度，确保了效率。这些结果确立了所提出的RGBX-DiffusionDet作为一种灵活的多模态目标检测方法，为将多样化的2D传感模态集成到基于扩散的检测管道中提供了新见解。", "summary": "本文提出了RGBX-DiffusionDet，一个将DiffusionDet扩展到多模态RGB-X目标检测的框架。该框架通过自适应多模态编码器融合RGB与异构2D数据。为增强跨模态交互，引入了动态通道缩减卷积块注意力模块（DCR-CBAM）和动态多级聚合块（DMLAB）以优化特征表示。此外，还提出了新的正则化损失以生成更具判别力的特征。实验证明，RGBX-DiffusionDet在多个数据集上均优于基线模型，并保持了高效的解码复杂度，展示了其在集成多样化2D传感模态方面的灵活性和潜力。", "keywords": "多模态目标检测, DiffusionDet, RGB-X, 特征融合, 注意力机制", "comments": "本文的创新点在于将扩散模型（DiffusionDet）扩展到多模态目标检测领域，并针对多模态数据融合设计了特定的模块，如DCR-CBAM和DMLAB，以及创新的正则化损失。这为处理异构传感数据提供了有效途径，并展示了扩散模型在更广泛应用中的潜力。其模块化设计在保持效率的同时提升了性能，具有重要的实践意义。"}}
{"id": "2507.16996", "title": "From Cracks to Crooks: YouTube as a Vector for Malware Distribution", "authors": ["Iman Vakilinia"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16996v1", "summary": "With billions of users and an immense volume of daily uploads, YouTube has\nbecome an attractive target for cybercriminals aiming to leverage its vast\naudience. The platform's openness and trustworthiness provide an ideal\nenvironment for deceptive campaigns that can operate under the radar of\nconventional security tools. This paper explores how cybercriminals exploit\nYouTube to disseminate malware, focusing on campaigns that promote free\nsoftware or game cheats. It discusses deceptive video demonstrations and the\ntechniques behind malware delivery. Additionally, the paper presents a new\nevasion technique that abuses YouTube's multilingual metadata capabilities to\ncircumvent automated detection systems. Findings indicate that this method is\nincreasingly being used in recent malicious videos to avoid detection and\nremoval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16996v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "从破解到骗局：YouTube作为恶意软件分发渠道", "tldr": "论文研究了网络犯罪分子如何利用YouTube分发恶意软件，特别是通过免费软件和游戏作弊，并揭示了一种滥用YouTube多语言元数据的新型规避技术，该技术正被越来越多地用于逃避检测。", "motivation": "YouTube因其庞大的用户群、每日海量上传量以及平台的开放性和可信度，成为网络犯罪分子利用其庞大受众进行恶意软件分发和欺骗性活动的目标。传统的安全工具难以检测到这些活动。", "method": "本文探讨了网络犯罪分子如何利用YouTube传播恶意软件，重点关注推广免费软件或游戏作弊的活动。它讨论了欺骗性视频演示和恶意软件交付技术。此外，论文还提出了一种滥用YouTube多语言元数据功能来规避自动检测系统的新型规避技术。", "result": "研究发现，滥用YouTube多语言元数据的新型规避技术正越来越多地被用于最近的恶意视频中，以逃避检测和删除。", "conclusion": "YouTube已成为网络犯罪分子分发恶意软件的重要载体，并且他们正在开发和利用新的规避技术（如滥用多语言元数据）来逃避检测。", "translation": "拥有数十亿用户和海量日常上传内容的YouTube已成为网络犯罪分子利用其庞大受众的诱人目标。该平台的开放性和可信度为欺骗性活动提供了理想的环境，这些活动可以在传统安全工具的雷达下运行。本文探讨了网络犯罪分子如何利用YouTube传播恶意软件，重点关注推广免费软件或游戏作弊的活动。它讨论了欺骗性视频演示和恶意软件交付技术。此外，论文还提出了一种滥用YouTube多语言元数据功能来规避自动检测系统的新型规避技术。研究结果表明，这种方法正越来越多地被用于最近的恶意视频中，以避免检测和删除。", "summary": "本文分析了YouTube如何被网络犯罪分子用作恶意软件分发渠道，特别是在推广免费软件和游戏作弊的活动中。研究揭示了欺骗性视频演示和恶意软件交付技术，并重点介绍了一种新发现的规避技术，该技术通过滥用YouTube的多语言元数据来绕过自动化检测系统。研究发现，这种规避方法在近期恶意视频中呈上升趋势，使其能够逃避检测和清除。", "keywords": "YouTube, 恶意软件分发, 网络犯罪, 规避技术, 多语言元数据", "comments": "这篇论文揭示了YouTube作为一个广受欢迎的平台，其开放性和信任度如何被恶意利用。其创新之处在于识别并详细阐述了一种新的规避技术——滥用多语言元数据，这对于平台安全和用户保护具有重要意义。论文强调了传统安全工具的局限性，并为平台方和安全研究人员提供了新的视角来对抗日益复杂的网络威胁。"}}
{"id": "2507.17210", "title": "FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second", "authors": ["Chunran Zheng", "Fu Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17210v1", "summary": "This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera\nextrinsic calibration tool based on a custom-made 3D target. FAST-Calib\nsupports both mechanical and solid-state LiDARs by leveraging an efficient and\nreliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It\nalso compensates for edge dilation artifacts caused by LiDAR spot spread\nthrough ellipse fitting, and supports joint optimization across multiple\nscenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and\nMid360), each paired with a wide-angle camera. Experimental results demonstrate\nsuperior accuracy and robustness compared to existing methods. With\npoint-to-point registration errors consistently below 6.5mm and total\nprocessing time under 0.7s, FAST-Calib provides an efficient, accurate, and\ntarget-based automatic calibration pipeline. We have open-sourced our code and\ndataset on GitHub to benefit the robotics community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17210v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "FAST-Calib：一秒内完成激光雷达-相机外参标定", "tldr": "FAST-Calib是一种基于定制3D靶标的激光雷达-相机快速外参标定工具，支持多种激光雷达类型，具有高精度、高效率和鲁棒性。", "motivation": "现有激光雷达-相机外参标定方法可能不够快速、用户友好或对不同类型激光雷达的适应性不强。本文旨在提供一种高效、准确且通用的标定工具。", "method": "FAST-Calib提出了一种基于定制3D靶标的激光雷达-相机外参标定方法。它采用高效可靠的边缘提取算法，不受激光雷达扫描模式限制，支持机械式和固态激光雷达。通过椭圆拟合补偿激光雷达光斑扩散引起的边缘膨胀伪影，并支持多场景联合优化。", "result": "FAST-Calib在三种激光雷达模型（Ouster、Avia、Mid360）和广角相机上进行了验证。实验结果表明，与现有方法相比，其精度和鲁棒性更优。点对点配准误差始终低于6.5mm，总处理时间低于0.7s。", "conclusion": "FAST-Calib提供了一个高效、准确且基于靶标的自动化激光雷达-相机外参标定流程，并通过开源代码和数据集造福机器人社区。", "translation": "本文提出了一种名为FAST-Calib的快速用户友好的激光雷达-相机外参标定工具，该工具基于定制的3D靶标。FAST-Calib通过利用一种高效可靠的边缘提取算法来支持机械式和固态激光雷达，该算法与激光雷达扫描模式无关。它还通过椭圆拟合补偿激光雷达光斑扩散引起的边缘膨胀伪影，并支持跨多个场景的联合优化。我们在三种激光雷达模型（Ouster、Avia和Mid360）上验证了FAST-Calib，每种都与一个广角相机配对。实验结果表明，与现有方法相比，其精度和鲁棒性更优。点对点配准误差始终低于6.5毫米，总处理时间低于0.7秒，FAST-Calib提供了一个高效、准确且基于靶标的自动化标定流程。我们已在GitHub上开源了代码和数据集，以造福机器人社区。", "summary": "FAST-Calib是一种创新的激光雷达-相机外参标定工具，它利用定制的3D靶标和高效的边缘提取算法，实现了在不到一秒的时间内完成高精度标定。该方法对激光雷达类型和扫描模式具有普适性，并能有效处理边缘伪影，支持多场景优化。实验证明其在精度和速度上均优于现有方法，为机器人社区提供了重要的开源资源。", "keywords": "激光雷达-相机标定, 外参标定, FAST-Calib, 3D靶标, 边缘提取", "comments": "FAST-Calib的创新点在于其定制的3D靶标和对激光雷达类型普适的边缘提取算法，以及对边缘膨胀伪影的补偿。其最大的亮点是极快的处理速度（不到0.7秒）和高精度，这对于实时机器人应用非常重要。开源代码和数据集也体现了其对社区的贡献。"}}
{"id": "2507.04074", "title": "Efficiency through Evolution, A Darwinian Approach to Agent-Based Economic Forecast Modeling", "authors": ["Martin Jaraiz"], "categories": ["econ.GN", "cs.CE", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      18 pages, 9 figures, presented at the IIOA Conference, Male 2025", "url": "http://arxiv.org/abs/2507.04074v2", "summary": "This paper presents a novel Darwinian Agent-Based Modeling (ABM) methodology\nformacroeconomic forecasting that leverages evolutionary principles to achieve\nremarkablecomputational efficiency and emergent realism. Unlike conventional\nDSGE and ABM approachesthat rely on complex behavioral rules derived from large\nfirm analysis, our framework employssimple \"common sense\" rules representative\nof small firms directly serving final consumers. Themethodology treats\nhouseholds as the primary drivers of economic dynamics, with firms\nadaptingthrough market-based natural selection within limited interaction\nneighborhoods. We demonstrate that this approach, when constrained by\nInput-Output table structures,generates realistic economic patterns including\nwealth distributions, firm size distributions, andsectoral employment patterns\nwithout extensive parameter calibration. Using FIGARO Input-Output tables for\n46 countries and focusing on Austria as a case study, we show that the\nmodelreproduces empirical regularities while maintaining computational\nefficiency on standard laptopsrather than requiring supercomputing clusters.\nKey findings include: (1) emergence of realistic firm and employment\ndistributions fromminimal behavioral assumptions, (2) accurate reproduction of\nthe initial Social Accounting Matrixvalues through evolutionary dynamics, (3)\nsuccessful calibration using only 5-6 country-specificparameters to complement\nthe FIGARO data, and (4) computational performance enabling fullsimulations on\nconsumer hardware. These results suggest that evolutionary ABM approaches\ncanprovide robust policy insights by capturing decentralized market adaptations\nwhile avoiding thecomputational complexity of traditional DSGE and\ncomprehensive ABM models.", "comment": "18 pages, 9 figures, presented at the IIOA Conference, Male 2025", "pdf_url": "http://arxiv.org/pdf/2507.04074v2", "cate": "econ.GN", "date": "2025-07-05", "updated": "2025-07-23", "AI": {"title_translation": "通过演化实现效率：一种基于达尔文方法的经济预测代理模型", "tldr": "本文提出了一种新颖的达尔文代理基建模（ABM）方法，用于宏观经济预测，该方法利用进化原理实现了卓越的计算效率和涌现的现实主义，并在标准笔记本电脑上展示了其生成真实经济模式的能力。", "motivation": "传统的DSGE和ABM方法依赖于复杂行为规则且计算成本高昂，本文旨在开发一种计算效率更高、能生成真实经济模式且参数校准需求较少的宏观经济预测方法。", "method": "本文提出了一种新颖的达尔文代理基建模（ABM）方法。该方法采用代表服务最终消费者的小型企业的“常识”规则，并将家庭视为经济动态的主要驱动力。企业通过有限交互邻域内的市场自然选择进行适应。该方法受到投入产出表结构的约束，并使用FIGARO投入产出表（针对46个国家，以奥地利为例）进行验证。", "result": "1. 从最少的行为假设中涌现出真实的厂商和就业分布。2. 通过演化动力学准确再现了初始社会核算矩阵值。3. 仅使用5-6个特定国家参数即可成功校准模型，以补充FIGARO数据。4. 计算性能使其能够在消费级硬件上进行完整模拟，无需超级计算集群。", "conclusion": "演化ABM方法通过捕捉去中心化市场适应性，同时避免传统DSGE和综合ABM模型的计算复杂性，可以提供稳健的政策洞察。", "translation": "本文提出了一种新颖的达尔文代理基建模（ABM）方法，用于宏观经济预测，该方法利用进化原理实现了卓越的计算效率和涌现的现实主义。与依赖于大型企业分析得出的复杂行为规则的传统DSGE和ABM方法不同，我们的框架采用代表直接服务于最终消费者的小型企业的简单“常识”规则。该方法将家庭视为经济动态的主要驱动力，企业在有限的交互邻域内通过基于市场的自然选择进行适应。我们证明，当这种方法受到投入产出表结构约束时，无需广泛的参数校准即可生成真实的经济模式，包括财富分布、企业规模分布和部门就业模式。我们使用46个国家的FIGARO投入产出表，并以奥地利作为案例研究，表明该模型在标准笔记本电脑上而非需要超级计算集群的情况下，能够重现经验规律，同时保持计算效率。主要发现包括：(1) 从最少的行为假设中涌现出真实的厂商和就业分布；(2) 通过演化动力学准确再现了初始社会核算矩阵值；(3) 仅使用5-6个特定国家参数即可成功校准模型，以补充FIGARO数据；(4) 计算性能使其能够在消费级硬件上进行完整模拟。这些结果表明，演化ABM方法通过捕捉去中心化市场适应性，同时避免传统DSGE和综合ABM模型的计算复杂性，可以提供稳健的政策洞察。", "summary": "本文提出了一种新颖的达尔文代理基建模（ABM）方法，用于宏观经济预测。该方法利用进化原理，通过简单的“常识”规则模拟小型企业和作为主要驱动力的家庭，并在市场自然选择下实现企业适应。与传统方法相比，该模型在投入产出表约束下，无需大量参数校准即可生成真实的经济模式（如财富和企业规模分布），并表现出卓越的计算效率。以奥地利为例，该模型在消费级硬件上成功重现经验规律，证明了其在提供政策洞察方面的鲁棒性和可行性。", "keywords": "代理基建模, 达尔文方法, 宏观经济预测, 计算效率, 演化动力学", "comments": "本文的创新点在于将达尔文进化原理应用于代理基宏观经济预测模型，显著提高了计算效率并降低了对复杂行为规则和大量参数校准的需求。其在标准硬件上运行的能力，使得这种复杂的经济模型更具可访问性和实用性。通过关注小型企业和家庭行为，该模型提供了一个自下而上的视角，可能更贴近真实经济的去中心化适应过程，这对于政策制定具有重要意义。"}}
{"id": "2507.17377", "title": "A Conditional Probability Framework for Compositional Zero-shot Learning", "authors": ["Peng Wu", "Qiuxia Lai", "Hao Fang", "Guo-Sen Xie", "Yilong Yin", "Xiankai Lu", "Wenguan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17377v1", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations\nof known objects and attributes by leveraging knowledge from previously seen\ncompositions. Traditional approaches primarily focus on disentangling\nattributes and objects, treating them as independent entities during learning.\nHowever, this assumption overlooks the semantic constraints and contextual\ndependencies inside a composition. For example, certain attributes naturally\npair with specific objects (e.g., \"striped\" applies to \"zebra\" or \"shirts\" but\nnot \"sky\" or \"water\"), while the same attribute can manifest differently\ndepending on context (e.g., \"young\" in \"young tree\" vs. \"young dog\"). Thus,\ncapturing attribute-object interdependence remains a fundamental yet\nlong-ignored challenge in CZSL. In this paper, we adopt a Conditional\nProbability Framework (CPF) to explicitly model attribute-object dependencies.\nWe decompose the probability of a composition into two components: the\nlikelihood of an object and the conditional likelihood of its attribute. To\nenhance object feature learning, we incorporate textual descriptors to\nhighlight semantically relevant image regions. These enhanced object features\nthen guide attribute learning through a cross-attention mechanism, ensuring\nbetter contextual alignment. By jointly optimizing object likelihood and\nconditional attribute likelihood, our method effectively captures compositional\ndependencies and generalizes well to unseen compositions. Extensive experiments\non multiple CZSL benchmarks demonstrate the superiority of our approach. Code\nis available at here.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17377v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一个用于组合式零样本学习的条件概率框架", "tldr": "该论文提出了一个条件概率框架（CPF），用于组合式零样本学习（CZSL），以显式建模属性-对象依赖关系，克服了传统方法将属性和对象视为独立实体的局限性，并在多个基准测试中表现出优越性。", "motivation": "传统的组合式零样本学习（CZSL）方法主要关注解耦属性和对象，将它们在学习过程中视为独立的实体，但这种假设忽视了组合内部的语义约束和上下文依赖性。例如，某些属性自然与特定对象配对，同一属性在不同上下文中表现不同。因此，捕获属性-对象相互依赖性是CZSL中一个基本但长期被忽视的挑战。", "method": "本文采用条件概率框架（CPF）来显式建模属性-对象依赖关系。该方法将组合的概率分解为两个部分：对象的似然和其属性的条件似然。为了增强对象特征学习，论文引入了文本描述符来突出语义相关的图像区域。这些增强的对象特征随后通过交叉注意力机制指导属性学习，确保更好的上下文对齐。通过联合优化对象似然和条件属性似然，该方法有效地捕获了组合依赖性。", "result": "在多个CZSL基准测试上进行的广泛实验表明，该方法表现出优越性。", "conclusion": "通过联合优化对象似然和条件属性似然，本文提出的条件概率框架（CPF）能够有效捕获组合依赖性，并很好地泛化到未见的组合。", "translation": "组合式零样本学习（CZSL）旨在通过利用先前已见组合的知识来识别已知对象和属性的未见组合。传统方法主要侧重于解耦属性和对象，在学习过程中将它们视为独立的实体。然而，这种假设忽略了组合内部的语义约束和上下文依赖性。例如，某些属性自然地与特定对象配对（例如，“条纹”适用于“斑马”或“衬衫”，但不适用于“天空”或“水”），而相同的属性可能根据上下文表现不同（例如，“幼小的”在“幼树”中与“幼犬”中）。因此，捕获属性-对象相互依赖性仍然是CZSL中一个基本但长期被忽视的挑战。在本文中，我们采用条件概率框架（CPF）来显式建模属性-对象依赖关系。我们将组合的概率分解为两个组成部分：对象的似然和其属性的条件似然。为了增强对象特征学习，我们结合文本描述符以突出语义相关的图像区域。这些增强的对象特征随后通过交叉注意力机制指导属性学习，确保更好的上下文对齐。通过联合优化对象似然和条件属性似然，我们的方法有效地捕获了组合依赖性并很好地泛化到未见的组合。在多个CZSL基准测试上的广泛实验表明了我们方法的优越性。代码可在此处获取。", "summary": "本文针对组合式零样本学习（CZSL）中长期存在的属性-对象依赖性建模挑战，提出了一种条件概率框架（CPF）。与传统方法将属性和对象视为独立实体不同，CPF通过将组合概率分解为对象似然和属性条件似然来显式建模这些依赖。为增强学习效果，该框架利用文本描述符强化对象特征，并通过交叉注意力机制指导属性学习。实验证明，该方法能有效捕捉组合依赖并良好泛化到未见组合，在多个CZSL基准测试中表现出优越性能。", "keywords": "组合式零样本学习, 条件概率框架, 属性-对象依赖, 交叉注意力, 未见组合", "comments": "该论文的创新之处在于提出了一个条件概率框架来显式建模组合式零样本学习中属性与对象之间的依赖关系，解决了传统方法忽视语义约束和上下文依赖的局限性。通过引入文本描述符增强对象特征学习，并利用交叉注意力机制指导属性学习，该方法在捕获复杂组合关系方面具有重要意义。其在多个基准测试上的优越性表明了其有效性。"}}
{"id": "2506.16596", "title": "A Community-driven vision for a new Knowledge Resource for AI", "authors": ["Vinay K Chaudhri", "Chaitan Baru", "Brandon Bennett", "Mehul Bhatt", "Darion Cassel", "Anthony G Cohn", "Rina Dechter", "Esra Erdem", "Dave Ferrucci", "Ken Forbus", "Gregory Gelfond", "Michael Genesereth", "Andrew S. Gordon", "Benjamin Grosof", "Gopal Gupta", "Jim Hendler", "Sharat Israni", "Tyler R. Josephson", "Patrick Kyllonen", "Yuliya Lierler", "Vladimir Lifschitz", "Clifton McFate", "Hande K. McGinty", "Leora Morgenstern", "Alessandro Oltramari", "Praveen Paritosh", "Dan Roth", "Blake Shepard", "Cogan Shimzu", "Denny Vrandečić", "Mark Whiting", "Michael Witbrock"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2506.16596v2", "summary": "The long-standing goal of creating a comprehensive, multi-purpose knowledge\nresource, reminiscent of the 1984 Cyc project, still persists in AI. Despite\nthe success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and\nother commercial knowledge graphs, verifiable, general-purpose widely available\nsources of knowledge remain a critical deficiency in AI infrastructure. Large\nlanguage models struggle due to knowledge gaps; robotic planning lacks\nnecessary world knowledge; and the detection of factually false information\nrelies heavily on human expertise. What kind of knowledge resource is most\nneeded in AI today? How can modern technology shape its development and\nevaluation? A recent AAAI workshop gathered over 50 researchers to explore\nthese questions. This paper synthesizes our findings and outlines a\ncommunity-driven vision for a new knowledge infrastructure. In addition to\nleveraging contemporary advances in knowledge representation and reasoning, one\npromising idea is to build an open engineering framework to exploit knowledge\nmodules effectively within the context of practical applications. Such a\nframework should include sets of conventions and social structures that are\nadopted by contributors.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.16596v2", "cate": "cs.AI", "date": "2025-06-19", "updated": "2025-07-22", "AI": {"title_translation": "人工智能新知识资源的社区驱动愿景", "tldr": "尽管现有知识资源有成功案例，AI仍缺乏全面的通用知识库。本文基于AAAI研讨会成果，提出构建一个社区驱动的开放工程框架，以解决AI知识缺失问题。", "motivation": "现有的AI知识资源（如WordNet, ConceptNet）未能提供可验证、通用且广泛可用的知识，导致大型语言模型存在知识鸿沟、机器人规划缺乏世界知识、事实错误信息检测依赖人工。因此，AI急需一个新的综合性、多用途知识资源。", "method": "本文综合了最近一次AAAI研讨会（汇集了50多位研究人员）的发现，并提出了一个社区驱动的新知识基础设施愿景。该愿景建议构建一个开放的工程框架，利用知识表示和推理的最新进展，并在实际应用中有效利用知识模块，同时包含贡献者应遵循的约定和社会结构。", "result": "研讨会和论文的成果是提出了一个“社区驱动”的愿景，旨在构建一个开放的工程框架，以解决AI知识资源的不足，并强调利用现代知识表示和推理技术以及建立贡献者规范的重要性。", "conclusion": "为了克服AI在知识方面存在的关键缺陷，需要一个社区驱动的、开放的工程框架，该框架应整合知识表示和推理的最新进展，并建立一套贡献者规范，以有效利用知识模块服务于实际应用。", "translation": "创建一个全面、多用途知识资源的长期目标，让人联想到1984年的Cyc项目，在人工智能领域依然存在。尽管WordNet、ConceptNet、Wolfram|Alpha以及其他商业知识图谱等知识资源取得了成功，但可验证、通用且广泛可用的知识来源仍然是人工智能基础设施中的一个关键缺陷。大型语言模型因知识空白而举步维艰；机器人规划缺乏必要的常识；事实性虚假信息的检测严重依赖人类专业知识。当今人工智能最需要什么样的知识资源？现代技术如何塑造其开发和评估？最近的一次AAAI研讨会汇集了50多位研究人员来探讨这些问题。本文综合了我们的发现，并概述了一个社区驱动的新的知识基础设施愿景。除了利用知识表示和推理的当代进展外，一个有前景的想法是构建一个开放的工程框架，以便在实际应用中有效利用知识模块。这样的框架应包括贡献者所采纳的一系列约定和社会结构。", "summary": "本文探讨了人工智能领域对综合性、多用途知识资源的持续需求，指出现有知识资源在可验证性、通用性和可用性上的不足导致了AI应用的局限。文章基于AAAI研讨会的成果，提出了一个社区驱动的新知识基础设施愿景，强调构建一个开放的工程框架，该框架将整合知识表示与推理的最新进展，并建立一套规范和社群结构，以期在实际应用中有效利用知识模块，从而弥补AI的知识鸿沟。", "keywords": "知识资源, 人工智能, 社区驱动, 知识图谱, 知识表示与推理", "comments": "这篇论文的创新点在于提出了“社区驱动”和“开放工程框架”的概念，以应对AI领域长期存在的知识资源不足问题。它不仅指出了现有知识资源的局限性，还提出了一个具体的实施路径，强调了协作和结构化利用知识的重要性。其重要性在于为未来AI知识库的建设提供了新的思路和方向，特别是其对开放性、模块化和社区参与的强调。"}}
{"id": "2507.03038", "title": "Cautious Next Token Prediction", "authors": ["Yizhou Wang", "Lingzhi Zhang", "Yue Bai", "Mang Tik Chiu", "Zhengmian Hu", "Mingyuan Zhang", "Qihua Dong", "Yu Yin", "Sohrab Amirghodsi", "Yun Fu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025", "url": "http://arxiv.org/abs/2507.03038v2", "summary": "Next token prediction paradigm has been prevailing for autoregressive models\nin the era of LLMs. The current default sampling choice for popular LLMs is\ntemperature scaling together with nucleus sampling to balance diversity and\ncoherence. Nevertheless, such approach leads to inferior performance in various\nNLP tasks when the model is not certain about testing questions. To this end,\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\nToken Prediction (CNTP). In the decoding process, if the model has\ncomparatively high prediction entropy at a certain step, we sample multiple\ntrials starting from the step independently and stop when encountering any\npunctuation. Then we select the trial with the lowest perplexity score viewed\nas the most probable and reliable trial path given the model's capacity. The\ntrial number is negatively correlated with the prediction confidence, i.e., the\nless confident the model is, the more trials it should sample. This is\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\none tends to think more creatively, exploring multiple thinking paths, to\ncautiously select the path one feels most confident about. Extensive\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\noutperforms existing standard decoding strategies consistently by a clear\nmargin. Moreover, the integration of CNTP with self consistency can further\nimprove over vanilla self consistency. We believe our proposed CNTP has the\npotential to become one of the default choices for LLM decoding. Code is\navailable at https://github.com/wyzjack/CNTP.", "comment": "ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.03038v2", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-23", "AI": {"title_translation": "谨慎的下一词元预测", "tldr": "本文提出了一种名为“谨慎的下一词元预测（CNTP）”的无训练解码策略，通过在模型不确定时探索多个路径并选择困惑度最低的路径，显著提升了LLM和MLLM的性能。", "motivation": "当前大型语言模型（LLMs）普遍采用的温度缩放结合核采样（nucleus sampling）的解码策略，在模型对测试问题不确定时，会导致各种自然语言处理（NLP）任务的性能下降。", "method": "本文提出了一种名为“谨慎的下一词元预测（CNTP）”的无训练解码策略。在解码过程中，如果模型在某个步骤的预测熵相对较高，则从该步骤独立采样多个试验，并在遇到任何标点符号时停止。然后，选择困惑度最低的试验作为最可能和最可靠的路径。试验次数与预测置信度负相关，即模型越不自信，采样的试验次数越多。", "result": "在大型语言模型（LLMs）和多模态大型语言模型（MLLMs）上的大量实验表明，所提出的CNTP方法始终以显著优势优于现有的标准解码策略。此外，CNTP与自洽性（self consistency）的结合可以进一步提高香草自洽性的性能。", "conclusion": "本文提出的CNTP方法具有潜力成为LLM解码的默认选择之一。", "translation": "下一词元预测范式在LLMs时代已成为自回归模型的主流。目前流行LLMs的默认采样选择是温度缩放结合核采样，以平衡多样性和连贯性。然而，当模型对测试问题不确定时，这种方法在各种NLP任务中会导致性能下降。为此，我们提出了一种全新的无训练解码策略， dubbed 为谨慎的下一词元预测（CNTP）。在解码过程中，如果模型在某个步骤的预测熵相对较高，我们从该步骤独立采样多个试验，并在遇到任何标点符号时停止。然后，我们选择困惑度分数最低的试验，将其视为在模型能力下最可能和最可靠的试验路径。试验次数与预测置信度负相关，即模型越不自信，采样的试验次数越多。这与人类行为一致：当感到不确定或不自信时，人们倾向于更具创造性地思考，探索多种思维路径，以谨慎地选择他们最自信的路径。在LLMs和MLLMs上的大量实验表明，我们提出的CNTP方法始终以显著优势优于现有的标准解码策略。此外，CNTP与自洽性的结合可以进一步提高香草自洽性。我们相信我们提出的CNTP有潜力成为LLM解码的默认选择之一。代码可在 https://github.com/wyzjack/CNTP 获取。", "summary": "该论文提出了一种名为“谨慎的下一词元预测（CNTP）”的创新型无训练解码策略，旨在解决现有LLM解码方法在模型不确定性高时性能不佳的问题。CNTP通过在模型预测熵较高时探索多条备选路径并选择困惑度最低的路径，来模拟人类在不确定时多方思考的行为。实验结果表明，CNTP在LLM和MLLM上均显著优于现有标准解码策略，并且与自洽性结合后性能可进一步提升，有望成为LLM解码的新标准。", "keywords": "下一词元预测, 解码策略, 大型语言模型, 困惑度, 自洽性", "comments": "CNTP的创新之处在于其模仿人类在不确定性下探索多种可能性并谨慎选择最优解的行为，将其融入到LLM的解码过程中，且无需额外训练。这种方法提供了一种简单而有效的方式来提高模型在不确定场景下的鲁棒性和性能，对提升LLM在实际应用中的可靠性具有重要意义。其无训练的特性也使其易于部署和集成。"}}
{"id": "2507.17007", "title": "The Postman: A Journey of Ethical Hacking in PosteID/SPID Borderland", "authors": ["Gabriele Costa"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17007v1", "summary": "This paper presents a vulnerability assessment activity that we carried out\non PosteID, the implementation of the Italian Public Digital Identity System\n(SPID) by Poste Italiane. The activity led to the discovery of a critical\nprivilege escalation vulnerability, which was eventually patched. The overall\nanalysis and disclosure process represents a valuable case study for the\ncommunity of ethical hackers. In this work, we present both the technical steps\nand the details of the disclosure process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17007v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "邮差：在PosteID/SPID边界地带进行道德黑客攻击之旅", "tldr": "本文介绍了对意大利公共数字身份系统PosteID进行的漏洞评估，发现并促使修复了一个关键的权限提升漏洞，并将其作为道德黑客的案例研究。", "motivation": "本文旨在分享对意大利公共数字身份系统PosteID进行漏洞评估的经验，并为道德黑客社区提供一个有价值的案例研究，详细说明技术步骤和漏洞披露过程。", "method": "进行了针对PosteID（意大利公共数字身份系统SPID的实现）的漏洞评估活动，并详细介绍了技术发现和漏洞披露流程。", "result": "发现了一个关键的权限提升漏洞，该漏洞最终得到了修补。", "conclusion": "对PosteID的整体漏洞分析和披露过程对道德黑客社区来说是一个宝贵的案例研究。", "translation": "本文介绍了一项我们针对意大利邮政（Poste Italiane）实现的意大利公共数字身份系统（SPID）PosteID进行的漏洞评估活动。该活动导致发现了一个关键的权限提升漏洞，该漏洞最终得到了修补。整体的分析和披露过程为道德黑客社区提供了一个宝贵的案例研究。在这项工作中，我们介绍了技术步骤和披露过程的细节。", "summary": "本文详细记录了对意大利数字身份系统PosteID的漏洞评估活动，成功发现并促使修复了一个关键的权限提升漏洞。研究强调了整个分析和披露过程作为道德黑客实践的宝贵案例研究的意义，并分享了技术细节和披露流程。", "keywords": "道德黑客, 漏洞评估, PosteID, SPID, 权限提升", "comments": "这项工作通过揭示关键漏洞并详细说明其发现和披露过程，为网络安全社区，特别是道德黑客，提供了实际操作的指导和案例。其重要性在于提升了关键公共基础设施的安全性，并促进了负责任的漏洞披露实践。"}}
{"id": "2507.17680", "title": "Simulating multiple human perspectives in socio-ecological systems using large language models", "authors": ["Yongchao Zeng", "Calum Brown", "Ioannis Kyriakou", "Ronja Hotz", "Mark Rounsevell"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17680v1", "summary": "Understanding socio-ecological systems requires insights from diverse\nstakeholder perspectives, which are often hard to access. To enable\nalternative, simulation-based exploration of different stakeholder\nperspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)\nmodelling framework. HoPeS employs agents powered by large language models\n(LLMs) to represent various stakeholders; users can step into the agent roles\nto experience perspectival differences. A simulation protocol serves as a\n\"scaffold\" to streamline multiple perspective-taking simulations, supporting\nusers in reflecting on, transitioning between, and integrating across\nperspectives. A prototype system is developed to demonstrate HoPeS in the\ncontext of institutional dynamics and land use change, enabling both\nnarrative-driven and numerical experiments. In an illustrative experiment, a\nuser successively adopts the perspectives of a system observer and a researcher\n- a role that analyses data from the embedded land use model to inform\nevidence-based decision-making for other LLM agents representing various\ninstitutions. Despite the user's effort to recommend technically sound\npolicies, discrepancies persist between the policy recommendation and\nimplementation due to stakeholders' competing advocacies, mirroring real-world\nmisalignment between researcher and policymaker perspectives. The user's\nreflection highlights the subjective feelings of frustration and disappointment\nas a researcher, especially due to the challenge of maintaining political\nneutrality while attempting to gain political influence. Despite this, the user\nexhibits high motivation to experiment with alternative narrative framing\nstrategies, suggesting the system's potential in exploring different\nperspectives. Further system and protocol refinement are likely to enable new\nforms of interdisciplinary collaboration in socio-ecological simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17680v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用大型语言模型模拟社会生态系统中的多重人类视角", "tldr": "该研究开发了HoPeS框架，利用大型语言模型（LLMs）模拟社会生态系统中不同利益相关者的视角，并通过用户沉浸式体验揭示了政策推荐与实施之间的现实世界分歧。", "motivation": "理解社会生态系统需要来自不同利益相关者的视角，但这些视角往往难以获取。本研究旨在通过模拟方式探索不同的利益相关者视角。", "method": "开发了名为HoPeS（Human-Oriented Perspective Shifting）的建模框架。HoPeS利用由大型语言模型（LLMs）驱动的智能体来代表不同的利益相关者，用户可以扮演智能体角色以体验视角差异。设计了一个模拟协议作为“支架”来简化多视角采纳模拟。开发了一个原型系统，在制度动态和土地利用变化的背景下演示HoPeS，支持叙事驱动和数值实验。", "result": "在一个示例实验中，用户扮演系统观察者和研究员的角色，发现尽管努力提出技术上合理的政策，但由于利益相关者相互竞争的主张，政策推荐与实施之间存在差异，这反映了研究人员和决策者视角之间的现实世界不一致。用户体验到作为研究员的沮丧和失望，尤其是在试图获得政治影响力的同时保持政治中立的挑战。尽管如此，用户仍表现出尝试替代叙事框架策略的高度积极性，表明该系统在探索不同视角方面的潜力。", "conclusion": "HoPeS系统和协议的进一步完善有望在社会生态模拟中实现新形式的跨学科协作。", "translation": "理解社会生态系统需要来自不同利益相关者的见解，而这些见解通常难以获取。为了实现对不同利益相关者视角的替代性、基于模拟的探索，我们开发了HoPeS（Human-Oriented Perspective Shifting）建模框架。HoPeS采用由大型语言模型（LLMs）驱动的智能体来代表各种利益相关者；用户可以进入智能体角色来体验视角差异。一个模拟协议作为一个“支架”，简化了多视角采纳模拟，支持用户反思、转换和整合不同视角。开发了一个原型系统，以在制度动态和土地利用变化的背景下演示HoPeS，从而实现叙事驱动和数值实验。在一个说明性实验中，用户依次扮演系统观察者和研究员的角色——研究员角色分析来自嵌入式土地利用模型的数据，为代表各种机构的其他LLM智能体提供基于证据的决策信息。尽管用户努力推荐技术上合理的政策，但由于利益相关者相互竞争的主张，政策推荐与实施之间仍然存在差异，这反映了研究人员和决策者视角之间现实世界中的不一致。用户的反思突出了作为研究员主观的沮丧和失望感，特别是由于在试图获得政治影响力的同时保持政治中立的挑战。尽管如此，用户表现出尝试替代叙事框架策略的高度积极性，这表明该系统在探索不同视角方面的潜力。进一步的系统和协议完善可能会在社会生态模拟中实现新形式的跨学科协作。", "summary": "本研究提出了HoPeS（Human-Oriented Perspective Shifting）建模框架，利用大型语言模型（LLMs）驱动的智能体来模拟社会生态系统中不同利益相关者的视角。该框架允许用户通过扮演智能体角色来体验和探索视角差异，并利用模拟协议指导多视角采纳过程。通过一个原型系统在制度动态和土地利用变化背景下的演示实验表明，即使在有技术支持的政策推荐下，由于利益相关者立场的竞争，政策制定与实施之间仍存在现实世界中的不一致。该研究强调了系统在探索复杂社会生态系统中的多重人类视角方面的潜力，并预示了未来跨学科协作的可能性。", "keywords": "社会生态系统, 大型语言模型, 利益相关者视角, 模拟, HoPeS", "comments": "HoPeS框架的创新之处在于其将大型语言模型应用于模拟复杂的社会生态系统中的多重人类视角，并允许用户沉浸式体验这些视角差异。这为理解和解决现实世界中政策制定与实施之间的分歧提供了一种新颖的、基于模拟的探索工具。其重要性在于能够促进跨学科合作，并帮助用户更好地理解不同利益相关者的立场和挑战。未来的研究可以进一步探索LLM智能体的行为复杂性，并优化用户体验和交互机制。"}}
{"id": "2507.17220", "title": "PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models", "authors": ["Jiansong Wan", "Chengming Zhou", "Jinkua Liu", "Xiangge Huang", "Xiaoyu Chen", "Xiaohan Yi", "Qisen Yang", "Baiting Zhu", "Xin-Qiang Cai", "Lixing Liu", "Rushuai Yang", "Chuheng Zhang", "Sherif Abdelfattah", "Hayong Shin", "Pushi Zhang", "Li Zhao", "Jiang Bian"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17220v1", "summary": "Recent studies have explored pretrained (foundation) models for vision-based\nrobotic navigation, aiming to achieve generalizable navigation and positive\ntransfer across diverse environments while enhancing zero-shot performance in\nunseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal\nNavigation), a new approach that further investigates pretraining strategies\nfor vision-based navigation models and contributes in two key areas.\nModel-wise, we identify two critical design choices that consistently improve\nthe performance of pretrained navigation models: (1) integrating an\nearly-fusion network structure to combine visual observations and goal images\nvia appropriately pretrained Vision Transformer (ViT) image encoder, and (2)\nintroducing suitable auxiliary tasks to enhance global navigation\nrepresentation learning, thus further improving navigation performance.\nDataset-wise, we propose a novel data preprocessing pipeline for efficiently\nlabeling large-scale game video datasets for navigation model training. We\ndemonstrate that augmenting existing open navigation datasets with diverse\ngameplay videos improves model performance. Our model achieves an average\nimprovement of 22.6% in zero-shot settings and a 37.5% improvement in\nfine-tuning settings over existing visual navigation foundation models in two\ncomplex simulated environments and one real-world environment. These results\nadvance the state-of-the-art in pretrained image-goal navigation models.\nNotably, our model maintains competitive performance while requiring\nsignificantly less fine-tuning data, highlighting its potential for real-world\ndeployment with minimal labeled supervision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17220v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PIG-Nav：预训练图像目标导航模型的关键见解", "tldr": "PIG-Nav提出了一种新的预训练策略，通过改进模型结构（早期融合ViT和辅助任务）和高效的数据预处理方法，显著提升了视觉导航模型在零样本和微调设置下的性能。", "motivation": "为了解决视觉机器人导航中泛化能力和零样本性能的挑战，并探索预训练模型在多样化环境中的积极迁移，研究者们进一步深入研究了基于视觉的导航模型的预训练策略。", "method": "本文提出了PIG-Nav（预训练图像目标导航）方法。在模型方面，PIG-Nav整合了早期融合网络结构，通过适当预训练的Vision Transformer（ViT）图像编码器结合视觉观测和目标图像，并引入了合适的辅助任务来增强全局导航表征学习。在数据集方面，PIG-Nav提出了一种新颖的数据预处理流程，用于高效标注大规模游戏视频数据集，以增强现有开放导航数据集的多样性。", "result": "PIG-Nav模型在零样本设置下平均性能提升了22.6%，在微调设置下平均性能提升了37.5%，超越了现有视觉导航基础模型。这些改进在两个复杂的模拟环境和一个真实世界环境中得到验证。此外，该模型在保持竞争性性能的同时，显著减少了微调所需的数据量。", "conclusion": "PIG-Nav通过创新的模型设计和数据处理策略，显著提升了预训练图像目标导航模型的性能，并减少了对标注数据的依赖，这对于实际部署具有重要意义，并推动了该领域的最新进展。", "translation": "最近的研究探索了基于视觉的机器人导航的预训练（基础）模型，旨在实现可泛化的导航和跨多样环境的积极迁移，同时增强在未见设置中的零样本性能。在这项工作中，我们引入了PIG-Nav（预训练图像目标导航），这是一种进一步研究基于视觉的导航模型预训练策略的新方法，并在两个关键领域做出了贡献。在模型方面，我们确定了两个持续改进预训练导航模型性能的关键设计选择：（1）集成一个早期融合网络结构，通过适当预训练的Vision Transformer（ViT）图像编码器结合视觉观测和目标图像，以及（2）引入合适的辅助任务以增强全局导航表征学习，从而进一步提高导航性能。在数据集方面，我们提出了一种新颖的数据预处理流程，用于高效标注大规模游戏视频数据集以进行导航模型训练。我们证明，用多样化的游戏视频增强现有开放导航数据集可以提高模型性能。我们的模型在两个复杂的模拟环境和一个真实世界环境中，在零样本设置下比现有视觉导航基础模型平均提高了22.6%，在微调设置下提高了37.5%。这些结果推动了预训练图像目标导航模型的最新技术水平。值得注意的是，我们的模型在保持竞争性性能的同时，所需微调数据显著减少，这突显了其在实际部署中只需最少标注监督的潜力。", "summary": "本文介绍了PIG-Nav，一种用于预训练图像目标导航模型的新方法。该方法在模型层面通过早期融合ViT结构和辅助任务优化了导航模型性能，并在数据层面提出了高效的游戏视频数据预处理流程。实验结果表明，PIG-Nav在零样本和微调设置下均显著优于现有视觉导航基础模型，平均性能提升分别为22.6%和37.5%，同时大幅减少了对微调数据的需求，展现了其在实际应用中的巨大潜力。", "keywords": "图像目标导航, 预训练模型, 视觉导航, 零样本学习, 数据增强", "comments": "PIG-Nav的创新点在于其结合了模型结构优化（早期融合ViT和辅助任务）与高效的数据处理方法（利用游戏视频进行数据增强），有效提升了预训练视觉导航模型的性能。其显著减少对微调数据的需求，对于实际部署具有重要意义，降低了标注成本，是该研究的重要优势。"}}
{"id": "2507.17703", "title": "Piecewise Control Barrier Functions for Stochastic Systems", "authors": ["Rayan Mazouz", "Luca Laurenti", "Morteza Lahijanian"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17703v1", "summary": "This paper presents a method for the simultaneous synthesis of a barrier\ncertificate and a safe controller for discrete-time nonlinear stochastic\nsystems. Our approach, based on piecewise stochastic control barrier functions,\nreduces the synthesis problem to a minimax optimization, which we solve exactly\nusing a dual linear program with zero gap. This enables the joint optimization\nof the barrier certificate and safe controller within a single formulation. The\nmethod accommodates stochastic dynamics with additive noise and a bounded\ncontinuous control set. The synthesized controllers and barrier certificates\nprovide a formally guaranteed lower bound on probabilistic safety. Case studies\non linear and nonlinear stochastic systems validate the effectiveness of our\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17703v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "分段控制障碍函数用于随机系统", "tldr": "本文提出了一种基于分段随机控制障碍函数的方法，用于同步合成离散时间非线性随机系统的障碍证书和安全控制器，从而保证概率安全性。", "motivation": "本文旨在为离散时间非线性随机系统同步合成障碍证书和安全控制器。", "method": "该方法基于分段随机控制障碍函数，将合成问题简化为最小最大优化问题，并通过零间隙对偶线性规划精确求解。它适用于具有加性噪声和有界连续控制集的随机动力学系统。", "result": "该方法实现了障碍证书和安全控制器的联合优化。合成的控制器和障碍证书为概率安全性提供了形式化保证的下限。在线性和非线性随机系统上的案例研究验证了该方法的有效性。", "conclusion": "本文成功提出了一种用于随机系统合成安全控制器和障碍证书的方法，为概率安全性提供了形式化保证。", "translation": "本文提出了一种用于离散时间非线性随机系统的障碍证书和安全控制器同步合成的方法。我们的方法基于分段随机控制障碍函数，将合成问题简化为最小最大优化问题，并通过零间隙对偶线性规划精确求解。这使得在单一公式中可以联合优化障碍证书和安全控制器。该方法适用于具有加性噪声和有界连续控制集的随机动力学系统。合成的控制器和障碍证书为概率安全性提供了形式化保证的下限。在线性和非线性随机系统上的案例研究验证了我们方法的有效性。", "summary": "本文提出了一种新颖的方法，用于同时合成离散时间非线性随机系统的障碍证书和安全控制器。通过采用分段随机控制障碍函数，该问题被表述为最小最大优化，并通过对偶线性规划精确求解。这种方法促进了两个组件的联合优化，保证了概率安全性，并通过对各种随机系统的案例研究进行了验证。", "keywords": "分段控制障碍函数, 随机系统, 安全性, 最小最大优化, 对偶线性规划", "comments": "该方法的创新之处在于利用分段随机控制障碍函数，并将合成问题表述为可通过对偶线性规划求解的最小最大优化问题，从而实现了联合优化并为随机系统提供了形式化的概率安全保证。"}}
{"id": "2507.17544", "title": "Optimal differentially private kernel learning with random projection", "authors": ["Bonwoo Lee", "Cheolwoo Park", "Jeongyoun Ahn"], "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      110 page, 12 figures", "url": "http://arxiv.org/abs/2507.17544v1", "summary": "Differential privacy has become a cornerstone in the development of\nprivacy-preserving learning algorithms. This work addresses optimizing\ndifferentially private kernel learning within the empirical risk minimization\n(ERM) framework. We propose a novel differentially private kernel ERM algorithm\nbased on random projection in the reproducing kernel Hilbert space using\nGaussian processes. Our method achieves minimax-optimal excess risk for both\nthe squared loss and Lipschitz-smooth convex loss functions under a local\nstrong convexity condition. We further show that existing approaches based on\nalternative dimension reduction techniques, such as random Fourier feature\nmappings or $\\ell_2$ regularization, yield suboptimal generalization\nperformance. Our key theoretical contribution also includes the derivation of\ndimension-free generalization bounds for objective perturbation-based private\nlinear ERM -- marking the first such result that does not rely on noisy\ngradient-based mechanisms. Additionally, we obtain sharper generalization\nbounds for existing differentially private kernel ERM algorithms. Empirical\nevaluations support our theoretical claims, demonstrating that random\nprojection enables statistically efficient and optimally private kernel\nlearning. These findings provide new insights into the design of differentially\nprivate algorithms and highlight the central role of dimension reduction in\nbalancing privacy and utility.", "comment": "110 page, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.17544v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "随机投影下的最优差分隐私核学习", "tldr": "一项新的基于随机投影的差分隐私核学习方法实现了最优性能，并提供了新的理论界限。", "motivation": "为了优化经验风险最小化（ERM）框架下的差分隐私核学习，以开发隐私保护学习算法。", "method": "提出了一种新颖的差分隐私核ERM算法，该算法基于高斯过程在再生核希尔伯特空间中使用随机投影。此外，还推导了基于目标扰动的私有线性ERM的无维度泛化界限。", "result": "实现了平方损失和Lipschitz-光滑凸损失函数下的极小极大最优超额风险；表明现有方法（如随机傅里叶特征映射或$\\ell_2$正则化）表现次优；首次推导出不依赖于噪声梯度机制的基于目标扰动的私有线性ERM的无维度泛化界限；获得了现有差分隐私核ERM算法的更清晰泛化界限；实证评估支持随机投影能够实现统计高效且最优隐私的核学习。", "conclusion": "随机投影在差分隐私算法中平衡隐私和效用方面起着核心作用，为差分隐私算法的设计提供了新见解。", "translation": "差分隐私已成为保护隐私学习算法发展的基石。这项工作旨在优化经验风险最小化（ERM）框架内的差分隐私核学习。我们提出了一种新颖的差分隐私核ERM算法，该算法基于高斯过程在再生核希尔伯特空间中使用随机投影。我们的方法在局部强凸条件下，对于平方损失函数和Lipschitz-光滑凸损失函数，都能实现极小极大最优的超额风险。我们进一步表明，基于替代降维技术（如随机傅里叶特征映射或$\\ell_2$正则化）的现有方法会产生次优的泛化性能。我们的关键理论贡献还包括推导出基于目标扰动的私有线性ERM的无维度泛化界限——这是第一个不依赖于噪声梯度机制的此类结果。此外，我们还获得了现有差分隐私核ERM算法的更清晰的泛化界限。实证评估支持了我们的理论主张，证明了随机投影能够实现统计高效且最优隐私的核学习。这些发现为差分隐私算法的设计提供了新见解，并强调了降维在平衡隐私和效用中的核心作用。", "summary": "这篇论文提出了一种新颖的差分隐私核经验风险最小化（ERM）算法，该算法利用高斯过程在再生核希尔伯特空间中进行随机投影。所提出的方法对于各种损失函数实现了极小极大最优的超额风险，并展示了优于现有降维技术的泛化性能。关键理论贡献包括推导了第一个不依赖于噪声梯度机制的私有线性ERM的无维度泛化界限，并为现有算法获得了更清晰的泛化界限。实证结果验证了该方法在隐私保护核学习中的效率和最优性，强调了随机投影在平衡隐私和效用方面的关键作用。", "keywords": "差分隐私, 核学习, 随机投影, 经验风险最小化, 泛化界限", "comments": "这篇论文通过实现极小极大最优性能和推导新颖的无维度泛化界限，做出了重要的理论贡献，这是一个显著的进步，因为它不依赖于基于噪声梯度的机制。强调随机投影作为平衡隐私和效用的卓越降维技术是富有洞察力的。"}}
{"id": "2408.04044", "title": "Asymptotically optimal $t$-design curves on $S^3$", "authors": ["Ayodeji Lindblad"], "categories": ["math.MG", "cs.NA", "math.NA", "41A55 (primary), 05B30, 41A63, 65D30, 65D32 (secondary)"], "primary_category": "Subjects:       Metric Geometry (math.MG)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures. Typos fixed, figures added, content streamlined", "url": "http://arxiv.org/abs/2408.04044v2", "summary": "A $\\textit{spherical $t$-design curve}$ was defined by Ehler and\nGr\\\"{o}chenig to be a continuous, piecewise smooth, closed curve on the sphere\nwith finitely many self-intersections whose associated line integral applied to\nany polynomial of degree at most $t$ evaluates to the average of this\npolynomial on the sphere. These authors posed the problem of proving that there\nexist sequences $(\\gamma_t)_{t=0}^\\infty$ of $t$-design curves on $S^d$ of\nasymptotically optimal length $\\ell(\\gamma_t)\\asymp t^{d-1}$ as $t\\to\\infty$\nand solved this problem for $d=2$. This work solves the problem for $d=3$ by\nproving that there exists a constant $\\mathscr C>0$ such that for any\n$C\\geq\\mathscr C$ and $t\\in\\Bbb N_+$, there exists a simple $t$-design curve on\n$S^3$ of length $Ct^2$.", "comment": "13 pages, 2 figures. Typos fixed, figures added, content streamlined", "pdf_url": "http://arxiv.org/pdf/2408.04044v2", "cate": "math.MG", "date": "2024-08-07", "updated": "2025-07-22", "AI": {"title_translation": "渐近最优的 $S^3$ 上的 $t$-设计曲线", "tldr": "本文解决了在 $S^3$ 上构建渐近最优长度 $Ct^2$ 的 $t$-设计曲线的问题，此前该问题已在 $S^2$ 上解决。", "motivation": "Ehler 和 Gr\"{o}chenig 定义了球形 $t$-设计曲线，并提出了证明在 $S^d$ 上存在渐近最优长度 $\\ell(\\gamma_t)\\asymp t^{d-1}$ 的 $t$-设计曲线序列的问题。他们解决了 $d=2$ 的情况，而 $d=3$ 的情况仍未解决，这是本文的动机。", "method": "本文通过证明存在一个常数 $\\mathscr C>0$，使得对于任何 $C\\geq\\mathscr C$ 和 $t\\in\\Bbb N_+$，在 $S^3$ 上存在一条长度为 $Ct^2$ 的简单 $t$-设计曲线来解决问题。摘要中未提及具体的证明方法。", "result": "证明了在 $S^3$ 上存在一条简单 $t$-设计曲线，其长度为 $Ct^2$，其中 $C$ 是一个大于或等于某个常数 $\\mathscr C$ 的值，且 $t\\in\\Bbb N_+$。这证实了对于 $d=3$ 的情况，存在渐近最优长度的 $t$-设计曲线。", "conclusion": "本文成功解决了 Ehler 和 Gr\"{o}chenig 提出的在 $S^3$ 上存在渐近最优长度 $t$-设计曲线的问题，从而将先前在 $S^2$ 上的结果推广到了 $S^3$。", "translation": "一个$\\textit{球形 $t$-设计曲线}$由 Ehler 和 Gr\"{o}chenig 定义为球面上连续、分段光滑、闭合的曲线，具有有限多个自交点，其相关线积分应用于任何次数不超过 $t$ 的多项式时，评估结果等于该多项式在球面上的平均值。这些作者提出了证明在 $S^d$ 上存在渐近最优长度 $\\ell(\\gamma_t)\\asymp t^{d-1}$（当 $t\\to\\infty$ 时）的 $t$-设计曲线序列 $(\\gamma_t)_{t=0}^\\infty$ 的问题，并解决了 $d=2$ 的情况。这项工作通过证明存在一个常数 $\\mathscr C>0$，使得对于任何 $C\\geq\\mathscr C$ 和 $t\\in\\Bbb N_+$，在 $S^3$ 上存在一条长度为 $Ct^2$ 的简单 $t$-设计曲线，从而解决了 $d=3$ 的问题。", "summary": "本文解决了 Ehler 和 Gr\"{o}chenig 提出的关于在 $S^d$ 上构建渐近最优长度 $t$-设计曲线的问题，特别是针对 $d=3$ 的情况。研究证明，在 $S^3$ 上存在一条长度为 $Ct^2$ 的简单 $t$-设计曲线，其中 $C$ 为常数，从而将先前在 $S^2$ 上的结果推广到三维球面。", "keywords": "$t$-设计曲线, 球面, 渐近最优, $S^3$, 曲线长度", "comments": "本文的创新之处在于其解决了在 $S^3$ 上的 $t$-设计曲线的渐近最优长度问题，这是对先前在 $S^2$ 上工作的推广。这对于理解和构建高维空间中的数值积分和采样方法具有重要意义。"}}
{"id": "2507.17010", "title": "Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs", "authors": ["H M Mohaimanul Islam", "Huynh Q. N. Vo", "Aditya Rane"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Submitted for peer-review in TrustXR - 2025", "url": "http://arxiv.org/abs/2507.17010v1", "summary": "In the era of synthetic media, deepfake manipulations pose a significant\nthreat to information integrity. To address this challenge, we propose\nTrustDefender, a two-stage framework comprising (i) a lightweight convolutional\nneural network (CNN) that detects deepfake imagery in real-time extended\nreality (XR) streams, and (ii) an integrated succinct zero-knowledge proof\n(ZKP) protocol that validates detection results without disclosing raw user\ndata. Our design addresses both the computational constraints of XR platforms\nwhile adhering to the stringent privacy requirements in sensitive settings.\nExperimental evaluations on multiple benchmark deepfake datasets demonstrate\nthat TrustDefender achieves 95.3% detection accuracy, coupled with efficient\nproof generation underpinned by rigorous cryptography, ensuring seamless\nintegration with high-performance artificial intelligence (AI) systems. By\nfusing advanced computer vision models with provable security mechanisms, our\nwork establishes a foundation for reliable AI in immersive and\nprivacy-sensitive applications.", "comment": "Submitted for peer-review in TrustXR - 2025", "pdf_url": "http://arxiv.org/pdf/2507.17010v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "迈向可信人工智能：使用CNN和零知识证明的安全深度伪造检测", "tldr": "提出TrustDefender框架，利用CNN实时检测深度伪造，并通过零知识证明验证结果，同时保护用户隐私，实现高精度和高效。", "motivation": "深度伪造操纵对信息完整性构成重大威胁，需要在计算受限的XR平台和隐私敏感环境中解决深度伪造检测问题。", "method": "提出TrustDefender，一个两阶段框架：(i) 轻量级卷积神经网络(CNN)用于实时扩展现实(XR)流中的深度伪造图像检测；(ii) 集成简洁零知识证明(ZKP)协议，在不泄露原始用户数据的情况下验证检测结果。该设计旨在解决XR平台的计算约束和严格的隐私要求。", "result": "在多个基准深度伪造数据集上，TrustDefender实现了95.3%的检测准确率，并结合严谨的密码学支持高效的证明生成，确保与高性能AI系统的无缝集成。", "conclusion": "通过将先进的计算机视觉模型与可证明的安全机制相结合，该工作为沉浸式和隐私敏感应用中的可靠人工智能奠定了基础。", "translation": "在合成媒体时代，深度伪造操纵对信息完整性构成重大威胁。为了应对这一挑战，我们提出了TrustDefender，一个两阶段框架，包括 (i) 一个轻量级卷积神经网络 (CNN)，用于实时扩展现实 (XR) 流中的深度伪造图像检测，以及 (ii) 一个集成的简洁零知识证明 (ZKP) 协议，可在不泄露原始用户数据的情况下验证检测结果。我们的设计既解决了 XR 平台的计算限制，又遵循了敏感设置中严格的隐私要求。在多个基准深度伪造数据集上的实验评估表明，TrustDefender 实现了 95.3% 的检测准确率，并结合严谨的密码学支持高效的证明生成，确保与高性能人工智能 (AI) 系统的无缝集成。通过将先进的计算机视觉模型与可证明的安全机制相结合，我们的工作为沉浸式和隐私敏感应用中的可靠人工智能奠定了基础。", "summary": "本文提出了TrustDefender，一个用于安全深度伪造检测的两阶段框架。它结合了轻量级CNN进行实时检测和零知识证明协议来验证结果，同时保护用户隐私。该框架在XR平台和隐私敏感环境中表现出色，实现了95.3%的检测准确率和高效的证明生成，为可信赖的人工智能应用奠定了基础。", "keywords": "深度伪造检测, 零知识证明, 卷积神经网络, 可信人工智能, 隐私保护", "comments": "本文的创新之处在于将深度伪造检测的CNN模型与零知识证明技术相结合，解决了隐私保护下的检测问题。这对于在XR等计算受限和隐私敏感场景下部署可信赖AI系统具有重要意义。"}}
{"id": "2507.17019", "title": "BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation", "authors": ["Ray Zirui Zhang", "Christopher E. Miles", "Xiaohui Xie", "John S. Lowengrub"], "categories": ["cs.LG", "65M32 65M32 65M32", "I.2.6; G.1.8"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17019v1", "summary": "Uncertainty quantification and inverse problems governed by partial\ndifferential equations (PDEs) are central to a wide range of scientific and\nengineering applications. In this second part of a two part series, we extend\nBilevel Local Operator Learning (BiLO) for PDE-constrained optimization\nproblems developed in Part 1 to the Bayesian inference framework. At the lower\nlevel, we train a network to approximate the local solution operator by\nminimizing the local operator loss with respect to the weights of the neural\nnetwork. At the upper level, we sample the PDE parameters from the posterior\ndistribution. We achieve efficient sampling through gradient-based Markov Chain\nMonte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with\nexisting methods based on Bayesian neural networks, our approach bypasses the\nchallenge of sampling in the high-dimensional space of neural network weights\nand does not require specifying a prior distribution on the neural network\nsolution. Instead, uncertainty propagates naturally from the data through the\nPDE constraints. By enforcing strong PDE constraints, the proposed method\nimproves the accuracy of both parameter inference and uncertainty\nquantification. We analyze the dynamic error of the gradient in the MCMC\nsampler and the static error in the posterior distribution due to inexact\nminimization of the lower level problem and demonstrate a direct link between\nthe tolerance for solving the lower level problem and the accuracy of the\nresulting uncertainty quantification. Through numerical experiments across a\nvariety of PDE models, we demonstrate that our method delivers accurate\ninference and quantification of uncertainties while maintaining high\ncomputational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17019v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "BiLO：偏微分方程逆问题的双层局部算子学习。第二部分：低秩适应的高效不确定性量化", "tldr": "本文将BiLO（双层局部算子学习）扩展到贝叶斯推理框架，用于偏微分方程逆问题中的高效不确定性量化，通过梯度MCMC和LoRA避免了高维神经网络权重采样，提高了参数推断和不确定性量化精度。", "motivation": "偏微分方程（PDEs）控制下的不确定性量化和逆问题在广泛的科学和工程应用中至关重要。", "method": "本文将第一部分开发的BiLO（Bilevel Local Operator Learning）用于PDE约束优化问题扩展到贝叶斯推理框架。在下层，训练一个网络通过最小化局部算子损失来近似局部解算子。在上层，从后验分布中采样PDE参数，通过基于梯度的马尔可夫链蒙特卡洛（MCMC）方法和低秩适应（LoRA）实现高效采样。该方法通过PDE约束使不确定性自然传播，并分析了MCMC采样器中梯度的动态误差和由于下层问题不精确最小化导致的后验分布中的静态误差。", "result": "与现有基于贝叶斯神经网络的方法相比，本文方法避免了神经网络权重高维空间采样的挑战，且无需指定神经网络解的先验分布。通过强制执行强PDE约束，提高了参数推断和不确定性量化的精度。数值实验表明，该方法在保持高计算效率的同时，实现了准确的推断和不确定性量化。", "conclusion": "本文提出的BiLO方法通过结合贝叶斯推理框架、MCMC和LoRA，为偏微分方程逆问题提供了一种高效且准确的不确定性量化方案，克服了传统贝叶斯神经网络在高维空间采样上的挑战，并有效利用PDE约束提高精度。", "translation": "偏微分方程（PDEs）控制下的不确定性量化和逆问题在广泛的科学和工程应用中至关重要。作为两部分系列文章的第二部分，我们将第一部分中为PDE约束优化问题开发的双层局部算子学习（BiLO）扩展到贝叶斯推理框架。在下层，我们训练一个网络，通过最小化局部算子损失来近似局部解算子。在上层，我们从后验分布中采样PDE参数。我们通过基于梯度的马尔可Kov链蒙特卡洛（MCMC）方法和低秩适应（LoRA）实现了高效采样。与现有基于贝叶斯神经网络的方法相比，我们的方法绕过了神经网络权重高维空间采样的挑战，并且不需要指定神经网络解的先验分布。相反，不确定性通过PDE约束自然地从数据传播。通过强制执行强PDE约束，所提出的方法提高了参数推断和不确定性量化的精度。我们分析了MCMC采样器中梯度的动态误差以及由于下层问题不精确最小化导致的后验分布中的静态误差，并证明了求解下层问题的容差与所得不确定性量化精度之间的直接联系。通过对各种PDE模型的数值实验，我们证明了我们的方法在保持高计算效率的同时，能够提供准确的推断和不确定性量化。", "summary": "本文是BiLO系列研究的第二部分，将双层局部算子学习（BiLO）应用于偏微分方程（PDE）逆问题的贝叶斯推理框架，以实现高效的不确定性量化。该方法通过在下层学习局部解算子，在上层利用梯度MCMC和低秩适应（LoRA）对PDE参数进行高效采样。与传统贝叶斯神经网络相比，BiLO避免了高维神经网络权重空间的采样难题，并利用PDE约束自然传播不确定性，从而提高了参数推断和不确定性量化的精度。研究还分析了MCMC采样误差与不确定性量化精度之间的关系，并通过数值实验验证了其高效性和准确性。", "keywords": "不确定性量化, 偏微分方程逆问题, 双层局部算子学习, 贝叶斯推理, 低秩适应", "comments": "该论文的创新点在于将双层优化与贝叶斯推理结合，并引入低秩适应（LoRA）来解决PDE逆问题中的不确定性量化。其重要性体现在有效避免了传统贝叶斯神经网络在高维权重空间采样的挑战，并通过利用PDE约束提高了精度和计算效率。这种方法为科学和工程领域中PDE逆问题的不确定性量化提供了新的思路和工具。"}}
{"id": "2507.17043", "title": "Computational Performance Bounds Prediction in Quantum Computing with Unstable Noise", "authors": ["Jinyang Li", "Samudra Dasgupta", "Yuhong Song", "Lei Yang", "Travis Humble", "Weiwen Jiang"], "categories": ["quant-ph", "cs.AI"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17043v1", "summary": "Quantum computing has significantly advanced in recent years, boasting\ndevices with hundreds of quantum bits (qubits), hinting at its potential\nquantum advantage over classical computing. Yet, noise in quantum devices poses\nsignificant barriers to realizing this supremacy. Understanding noise's impact\nis crucial for reproducibility and application reuse; moreover, the\nnext-generation quantum-centric supercomputing essentially requires efficient\nand accurate noise characterization to support system management (e.g., job\nscheduling), where ensuring correct functional performance (i.e., fidelity) of\njobs on available quantum devices can even be higher-priority than traditional\nobjectives. However, noise fluctuates over time, even on the same quantum\ndevice, which makes predicting the computational bounds for on-the-fly noise is\nvital. Noisy quantum simulation can offer insights but faces efficiency and\nscalability issues. In this work, we propose a data-driven workflow, namely\nQuBound, to predict computational performance bounds. It decomposes historical\nperformance traces to isolate noise sources and devises a novel encoder to\nembed circuit and noise information processed by a Long Short-Term Memory\n(LSTM) network. For evaluation, we compare QuBound with a state-of-the-art\nlearning-based predictor, which only generates a single performance value\ninstead of a bound. Experimental results show that the result of the existing\napproach falls outside of performance bounds, while all predictions from our\nQuBound with the assistance of performance decomposition better fit the bounds.\nMoreover, QuBound can efficiently produce practical bounds for various circuits\nwith over 106 speedup over simulation; in addition, the range from QuBound is\nover 10x narrower than the state-of-the-art analytical approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17043v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "量子计算中不稳定噪声下的计算性能边界预测", "tldr": "QuBound是一种数据驱动的工作流，用于预测量子计算中不稳定噪声下的计算性能边界，相比现有方法具有更高的准确性和效率。", "motivation": "量子设备中的噪声是实现量子优势的重大障碍，且噪声随时间波动，使得预测计算性能边界变得至关重要。传统的噪声量子模拟存在效率和可伸缩性问题。", "method": "本研究提出了一种名为QuBound的数据驱动工作流来预测计算性能边界。它分解历史性能轨迹以隔离噪声源，并设计了一种新颖的编码器来嵌入由长短期记忆（LSTM）网络处理的电路和噪声信息。", "result": "实验结果表明，现有方法的预测结果超出性能边界，而QuBound在性能分解的帮助下，所有预测都更好地拟合边界。此外，QuBound可以高效地为各种电路生成实际边界，比模拟加速超过106倍；并且，QuBound的预测范围比最先进的分析方法窄10倍以上。", "conclusion": "QuBound是一种有效且高效的方法，能够准确预测量子计算中存在不稳定噪声时的计算性能边界，解决了现有方法在准确性和效率上的局限性。", "translation": "量子计算近年来取得了显著进展，拥有数百个量子比特（qubits）的设备，预示着其超越经典计算的潜在量子优势。然而，量子设备中的噪声是实现这种优势的重大障碍。理解噪声的影响对于可重复性和应用重用至关重要；此外，下一代以量子为中心的超级计算本质上需要高效准确的噪声表征来支持系统管理（例如，作业调度），其中确保作业在可用量子设备上的正确功能性能（即保真度）甚至可能比传统目标具有更高的优先级。然而，噪声随时间波动，即使在同一量子设备上也是如此，这使得预测实时噪声的计算边界变得至关重要。噪声量子模拟可以提供洞察力，但面临效率和可伸缩性问题。在这项工作中，我们提出了一种数据驱动的工作流，即QuBound，来预测计算性能边界。它分解历史性能轨迹以隔离噪声源，并设计了一种新颖的编码器来嵌入由长短期记忆（LSTM）网络处理的电路和噪声信息。为了评估，我们将QuBound与一种最先进的基于学习的预测器进行比较，该预测器仅生成单个性能值而不是边界。实验结果表明，现有方法的结果落在性能边界之外，而我们QuBound在性能分解的帮助下，所有预测都更好地拟合边界。此外，QuBound可以高效地为各种电路生成实际边界，比模拟加速超过106倍；此外，QuBound的范围比最先进的分析方法窄10倍以上。", "summary": "本文提出了一种名为QuBound的数据驱动工作流，旨在预测量子计算中不稳定噪声下的计算性能边界。QuBound通过分解历史性能轨迹来隔离噪声源，并利用LSTM网络处理编码后的电路和噪声信息。实验证明，QuBound的预测结果比现有方法更准确地拟合实际性能边界，并且在效率上远超量子模拟（加速超过106倍），同时预测范围比最先进的分析方法窄10倍以上。", "keywords": "量子计算, 噪声, 性能边界预测, LSTM, QuBound", "comments": "本文的创新点在于提出了一个数据驱动的框架QuBound，能够预测量子计算中受不稳定噪声影响的性能“边界”，而非单一性能值，这对于实际的量子系统管理和作业调度具有重要意义。其通过分解噪声源并结合LSTM网络的方法是新颖的，且在准确性、效率和预测范围的紧凑性方面均显示出显著优势。"}}
{"id": "2507.17623", "title": "SA-WiSense: A Blind-Spot-Free Respiration Sensing Framework for Single-Antenna Wi-Fi Devices", "authors": ["Guangteng Liu", "Xiayue Liu", "Zhixiang Xu", "Yufeng Yuan", "Hui Zhao", "Yuxuan Liu", "Yufei Jiang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12pages, 10figures", "url": "http://arxiv.org/abs/2507.17623v1", "summary": "Wi-Fi sensing offers a promising technique for contactless human respiration\nmonitoring. A key challenge, however, is the blind spot problem caused by\nrandom phase offsets that corrupt the complementarity of respiratory signals.\nTo address the challenge, we propose a single-antenna-Wi-Fi-sensing\n(SA-WiSense) framework to improve accuracy of human respiration monitoring,\nrobust against random phase offsets. The proposed SA-WiSense framework is\ncost-efficient, as only a single antenna is used rather than multiple antennas\nas in the previous works. Therefore, the proposed framework is applicable to\nInternet of Thing (IoT), where most of sensors are equipped with a single\nantenna. On one hand, we propose a cross-subcarrier channel state information\n(CSI) ratio (CSCR) based blind spot mitigation approach for IoT, where the\nratios of two values of CSI between subcarriers are leveraged to mitigate\nrandom phase offsets. We prove that the random phase offsets can be cancelled\nby the proposed CSCR approach, thereby restoring the inherent complementarity\nof signals for blind-spot-free sensing. On the other hand, we propose a genetic\nalgorithm (GA) based subcarrier selection (GASS) approach by formulating an\noptimization problem in terms of the sensing-signal-to-noise ratio (SSNR) of\nCSCR between subcarriers. GA is utilized to solve the formulated optimization\nproblem. We use commodity ESP32 microcontrollers to build an experiment test.\nThe proposed works are validated to achieve an detection rate of 91.2% for\nrespiration monitoring at distances up to 8.0 meters, substantially more\naccurate than the state-of-the-art methods with a single antenna.", "comment": "12pages, 10figures", "pdf_url": "http://arxiv.org/pdf/2507.17623v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SA-WiSense：一种适用于单天线Wi-Fi设备的无盲点呼吸感知框架", "tldr": "SA-WiSense提出了一种基于单天线Wi-Fi的呼吸监测框架，通过跨子载波CSI比率和遗传算法解决了盲点问题，实现了高精度监测。", "motivation": "Wi-Fi感知在非接触式人体呼吸监测方面具有前景，但关键挑战是随机相位偏移导致的盲点问题，这会破坏呼吸信号的互补性。此外，现有方法多使用多天线，成本较高，不适用于大部分传感器为单天线的物联网设备。", "method": "本文提出了SA-WiSense框架，用于提高人体呼吸监测的准确性，并能抵抗随机相位偏移。1. 提出了基于跨子载波信道状态信息(CSI)比率(CSCR)的盲点缓解方法，利用子载波之间两个CSI值的比率来消除随机相位偏移，恢复信号的互补性。2. 提出了基于遗传算法(GA)的子载波选择(GASS)方法，通过公式化子载波之间CSCR的感知信噪比(SSNR)优化问题，并利用GA求解。该框架成本效益高，仅使用单天线，适用于物联网设备。", "result": "所提出的方法在呼吸监测方面实现了91.2%的检测率，监测距离可达8.0米，并且比现有单天线方法更准确。", "conclusion": "SA-WiSense框架通过创新的CSCR和GASS方法，有效解决了单天线Wi-Fi设备在呼吸监测中的盲点问题，实现了高精度、远距离的呼吸监测，并具有成本效益和广泛的物联网适用性。", "translation": "Wi-Fi感知为非接触式人体呼吸监测提供了一种有前景的技术。然而，一个关键挑战是随机相位偏移引起的盲点问题，它会破坏呼吸信号的互补性。为了解决这一挑战，我们提出了一种单天线Wi-Fi感知（SA-WiSense）框架，以提高人体呼吸监测的准确性，并能抵抗随机相位偏移。所提出的SA-WiSense框架具有成本效益，因为它只使用单个天线，而不是像以前的工作那样使用多个天线。因此，所提出的框架适用于物联网（IoT），因为大多数传感器都配备了单天线。一方面，我们提出了一种基于跨子载波信道状态信息（CSI）比率（CSCR）的物联网盲点缓解方法，其中利用子载波之间两个CSI值的比率来缓解随机相位偏移。我们证明，通过所提出的CSCR方法可以消除随机相位偏移，从而恢复信号固有的互补性，实现无盲点感知。另一方面，我们提出了一种基于遗传算法（GA）的子载波选择（GASS）方法，通过根据子载波之间CSCR的感知信噪比（SSNR）来制定优化问题。GA被用于解决所制定的优化问题。我们使用商用ESP32微控制器构建了一个实验测试。经验证，所提出的工作在最远8.0米的距离上，实现了91.2%的呼吸监测检测率，比现有的单天线方法准确得多。", "summary": "本文提出了SA-WiSense，一个针对单天线Wi-Fi设备的无盲点呼吸感知框架。为解决随机相位偏移导致的盲点问题，SA-WiSense引入了基于跨子载波CSI比率（CSCR）的盲点缓解方法，通过CSI比率消除相位偏移，恢复信号互补性。同时，结合遗传算法（GA）的子载波选择（GASS）优化了感知信噪比。该框架成本效益高，适用于物联网设备，并在实验中验证了其在8米距离内91.2%的呼吸监测检测率，显著优于现有单天线方法。", "keywords": "Wi-Fi sensing, respiration monitoring, blind spot, single antenna, CSI ratio, genetic algorithm, IoT", "comments": "该论文的创新点在于提出了SA-WiSense框架，通过结合CSCR和GASS两种方法，有效地解决了单天线Wi-Fi设备在呼吸监测中长期存在的盲点问题，克服了随机相位偏移的影响。其重要性在于，通过仅使用单天线，显著降低了设备成本和复杂度，使得Wi-Fi呼吸监测技术能够更广泛地应用于成本敏感的物联网设备中。这对于智能家居、智慧医疗等领域的发展具有重要意义。"}}
{"id": "2507.16036", "title": "Entanglement-Efficient Distribution of Quantum Circuits over Large-Scale Quantum Networks", "authors": ["Felix Burt", "Kuan-Cheng Chen", "Kin K. Leung"], "categories": ["quant-ph", "cs.DC"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 10 figures, to be published in proceedings of IEEE QCE2025", "url": "http://arxiv.org/abs/2507.16036v2", "summary": "Quantum computers face inherent scaling challenges, a fact that necessitates\ninvestigation of distributed quantum computing systems, whereby scaling is\nachieved through interconnection of smaller quantum processing units. However,\nconnecting large numbers of QPUs will eventually result in connectivity\nconstraints at the network level, where the difficulty of entanglement sharing\nincreases with network path lengths. This increases the complexity of the\nquantum circuit partitioning problem, since the cost of generating entanglement\nbetween end nodes varies with network topologies and existing links. We address\nthis challenge using a simple modification to existing partitioning schemes\ndesigned for all-to-all connected networks, that efficiently accounts for both\nof these factors. We investigate the performance in terms of entanglement\nrequirements and optimisation time of various quantum circuits over different\nnetwork topologies, achieving lower entanglement costs in the majority of cases\nthan state-of-the-art methods. We provide techniques for scaling to large-scale\nquantum networks employing both network and problem coarsening. We show that\ncoarsened methods can achieve improved solution quality in most cases with\nsignificantly lower run-times than direct partitioning methods.", "comment": "12 pages, 10 figures, to be published in proceedings of IEEE QCE2025", "pdf_url": "http://arxiv.org/pdf/2507.16036v2", "cate": "quant-ph", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "大规模量子网络中量子电路的纠缠高效分发", "tldr": "本文提出了一种改进的量子电路划分方案，通过考虑网络拓扑和现有链路，在大规模量子网络中实现更低的纠缠成本和更快的优化时间。", "motivation": "量子计算机面临固有的扩展挑战，需要分布式量子计算系统。然而，连接大量量子处理单元（QPU）会导致网络层面的连接性限制，纠缠共享的难度随网络路径长度增加。这使得量子电路划分问题更加复杂，因为端节点之间生成纠缠的成本因网络拓扑和现有链路而异。", "method": "我们通过对现有为全连接网络设计的划分方案进行简单修改来解决这一挑战，该修改能有效考虑网络拓扑和现有链路。我们还提供了利用网络和问题粗化技术来扩展到大规模量子网络的方法。", "result": "我们在不同网络拓扑下，针对各种量子电路，在纠缠需求和优化时间方面进行了性能研究，结果显示在大多数情况下，其纠缠成本低于现有最先进方法。我们还表明，粗化方法在大多数情况下能获得更好的解决方案质量，且运行时间显著低于直接划分方法。", "conclusion": "通过对现有划分方案的修改和引入粗化技术，本文提出的方法能有效降低大规模量子网络中量子电路分发的纠缠成本，并缩短优化时间，从而为分布式量子计算的扩展提供了有效途径。", "translation": "量子计算机面临固有的扩展挑战，这使得分布式量子计算系统的研究成为必要，其中通过互连较小的量子处理单元来实现扩展。然而，连接大量量子处理单元最终会导致网络层面的连接性限制，其中纠缠共享的难度随网络路径长度增加。这增加了量子电路划分问题的复杂性，因为端节点之间生成纠缠的成本因网络拓扑和现有链路而异。我们通过对现有为全连接网络设计的划分方案进行简单修改来解决这一挑战，该修改能有效考虑这两个因素。我们研究了不同网络拓扑下各种量子电路在纠缠需求和优化时间方面的性能，在大多数情况下实现了比现有最先进方法更低的纠缠成本。我们提供了利用网络和问题粗化技术来扩展到大规模量子网络的方法。我们表明，粗化方法在大多数情况下能获得更好的解决方案质量，且运行时间显著低于直接划分方法。", "summary": "本文针对大规模量子网络中量子电路分发面临的纠缠共享和划分复杂性问题，提出了一种改进的电路划分方案。该方案通过修改现有方法并引入网络和问题粗化技术，有效地考虑了网络拓扑和链路成本。实验结果表明，与现有技术相比，该方法在大多数情况下能显著降低纠缠成本并缩短优化时间，为分布式量子计算的扩展提供了高效的解决方案。", "keywords": "量子电路, 分布式量子计算, 纠缠, 网络拓扑, 电路划分", "comments": "本文针对分布式量子计算中大规模量子网络互连的实际挑战，提出了一种创新性的解决方案。其主要创新点在于对现有电路划分方案的有效修改，以及引入网络和问题粗化技术，以适应非全连接网络的复杂性。该研究对于推动分布式量子计算的实用化和可扩展性具有重要意义，通过降低纠缠成本和优化时间，为构建更大型的量子系统奠定了基础。抽象中未提及具体局限性，但实际应用中可能需要考虑粗化粒度对结果精度的影响。"}}
{"id": "2507.17640", "title": "The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)", "authors": ["Thomas M. Metz", "Matthew Q. Hill", "Alice J. O'Toole"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17640v1", "summary": "Person identification in unconstrained viewing environments presents\nsignificant challenges due to variations in distance, viewpoint, imaging\nconditions, and clothing. We introduce $\\textbf{E}$va $\\textbf{C}$lothes-Change\nfrom $\\textbf{H}$idden $\\textbf{O}$bjects - $\\textbf{B}$ody\n$\\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built\non object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other\nmodels that vary systematically in backbone architecture, model size, scale of\nobject classification pretraining, and transfer learning protocol. Models were\nevaluated on benchmark datasets across constrained, unconstrained, and occluded\nsettings. ECHO-BID, with transfer learning on the most challenging\nclothes-change data, achieved state-of-the-art results on long-term re-id --\nsubstantially outperforming other methods. ECHO-BID also surpassed other\nmethods by a wide margin in occluded viewing scenarios. A combination of\nincreased model size and Masked Image Modeling during pretraining underlie\nECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more\nchallenging transfer learning dataset, generalized better across datasets than\na larger, less challenging one. However, the larger dataset with an additional\nfine-tuning step proved best on the most difficult data. Selecting the correct\npretrained backbone architecture and transfer learning protocols can drive\nsubstantial gains in long-term re-id performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17640v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "早起的鸟儿识别蠕虫：在长期人体再识别中先发制人（ECHO-BID）", "tldr": "ECHO-BID模型通过利用对象预训练的EVA-02大型骨干和挑战性迁移学习，在长期人体再识别和遮挡场景下取得了最先进的性能。", "motivation": "在非受限视角环境下识别人体存在显著挑战，这主要是由于距离、视角、成像条件和衣着的变化。", "method": "本文介绍了ECHO-BID，一类基于对象预训练的EVA-02大型骨干构建的长期再识别模型。研究将ECHO-BID与9个在骨干架构、模型大小、对象分类预训练规模和迁移学习协议上系统性变化的模型进行了比较，并在受限、非受限和遮挡设置下的基准数据集上进行了评估。", "result": "ECHO-BID在最具挑战性的换装数据上进行迁移学习后，在长期再识别方面取得了最先进的结果，显著优于其他方法。它在遮挡视角场景中也大幅超越了其他方法。模型尺寸的增加和预训练期间的掩码图像建模是ECHO-BID在长期再识别上表现强劲的基础。值得注意的是，一个更小但更具挑战性的迁移学习数据集比一个更大、挑战性较小的数据集在跨数据集泛化方面表现更好，但更大的数据集加上额外的微调步骤在最困难的数据上表现最佳。", "conclusion": "选择正确的预训练骨干架构和迁移学习协议可以显著提高长期人体再识别的性能。", "translation": "在非受限视角环境下识别人体存在显著挑战，这主要是由于距离、视角、成像条件和衣着的变化。我们引入了Eva Clothes-Change from Hidden Objects - Body IDentification (ECHO-BID)，这是一类基于对象预训练的EVA-02大型骨干构建的长期再识别模型。我们将ECHO-BID与9个在骨干架构、模型大小、对象分类预训练规模和迁移学习协议上系统性变化的模型进行了比较。模型在受限、非受限和遮挡设置下的基准数据集上进行了评估。ECHO-BID在最具挑战性的换装数据上进行迁移学习后，在长期再识别方面取得了最先进的结果——显著优于其他方法。ECHO-BID在遮挡视角场景中也大幅超越了其他方法。模型尺寸的增加和预训练期间的掩码图像建模是ECHO-BID在长期再识别上表现强劲的基础。值得注意的是，一个更小但更具挑战性的迁移学习数据集比一个更大、挑战性较小的数据集在跨数据集泛化方面表现更好。然而，更大的数据集加上额外的微调步骤在最困难的数据上表现最佳。选择正确的预训练骨干架构和迁移学习协议可以显著提高长期再识别的性能。", "summary": "本文提出了ECHO-BID，一种基于对象预训练的EVA-02大型骨干构建的长期人体再识别模型。研究表明，ECHO-BID在最具挑战性的换装数据上进行迁移学习后，在长期人体再识别和遮挡场景下均达到了最先进的性能，显著优于现有方法。其卓越表现得益于增大的模型尺寸和预训练中的掩码图像建模。研究还发现，恰当选择预训练骨干和迁移学习协议对提升长期再识别性能至关重要。", "keywords": "长期人体再识别, 人体识别, ECHO-BID, EVA-02, 迁移学习", "comments": "该论文通过引入ECHO-BID模型，在长期人体再识别领域取得了显著突破，尤其是在处理换装和遮挡场景方面。其创新点在于利用对象预训练的EVA-02大型骨干，并通过系统性比较揭示了模型尺寸、预训练策略和迁移学习协议对性能的关键影响。这为未来的人体再识别研究提供了重要的方向，强调了预训练和迁移学习的重要性。"}}
{"id": "2507.17033", "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI", "authors": ["Joshua Kalyanapu", "Farshad Dizani", "Darsh Asher", "Azam Ghanbari", "Rosario Cammarota", "Aydin Aysu", "Samira Mirbagher Ajorpaz"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted at MICRO 2025", "url": "http://arxiv.org/abs/2507.17033v1", "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.", "comment": "Accepted at MICRO 2025", "pdf_url": "http://arxiv.org/pdf/2507.17033v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "GATEBLEED：利用片上加速器功耗门控对AI进行高性能隐秘攻击", "tldr": "研究发现并利用Intel AMX加速器上的功耗门控导致的GATEBLEED时序侧信道，对AI模型进行隐私攻击，可绕过现有防御。", "motivation": "随着AI训练和推理的功耗持续增加，AI加速器（如Intel AMX）被直接集成到CPU中。研究动机是发现这种集成可能导致的新的安全漏洞，特别是针对AI模型隐私的侧信道攻击。", "method": "研究人员发现了一种名为GATEBLEED的时序侧信道和隐蔽通道，其产生原因是CPU为控制功耗而采用的激进功耗门控。他们通过识别流行ML库中（如HuggingFace、PyTorch、TensorFlow等）的十多个潜在“小工具”，展示了该侧信道对AI隐私的威胁，证明了敏感信息的泄露。此外，他们还实现了一个端到端微架构推理攻击，针对使用Intel AMX优化的Transformer模型和基于CNN/Transformer的专家混合模型。", "result": "GATEBLEED侧信道可以泄露AI模型的私有值，例如置信度阈值和路由逻辑。在针对Intel AMX优化的Transformer模型上，实现了81%的成员推断准确率和0.89的精确度。在基于CNN或Transformer的专家混合模型上，实现了100%的专家选择泄露准确率。GATEBLEED可作为高性能、隐秘的远程隐蔽通道，能绕过传统缓存防御，泄露任意内存地址，并逃避最先进的微架构攻击检测器。", "conclusion": "GATEBLEED是一种新的、高风险的威胁，它利用片上加速器功耗门控导致的侧信道，对AI隐私构成威胁，并且能够绕过现有的防护措施，实现高性能、隐秘的攻击。", "translation": "随着AI训练和推理功耗的持续增长，AI加速器正被直接集成到CPU中。英特尔的先进矩阵扩展（AMX）就是其中一个例子，首次亮相于第四代英特尔至强可扩展CPU。我们发现了一个由为保持CPU在运行限制内而采用的激进功耗门控引起的时序侧信道和隐蔽通道，命名为GATEBLEED。我们表明GATEBLEED侧信道对AI隐私构成威胁，因为许多ML模型（如Transformer和CNN）基于置信度阈值和路由逻辑等私有值做出关键的计算密集型决策。AMX组件选择性断电导致的时序延迟意味着，在AMX加速器上执行的每次矩阵乘法都可能是一个潜在的泄漏点。我们的研究在流行的ML库（HuggingFace、PyTorch、TensorFlow等）中识别出十多个潜在的“小工具”，揭示它们可以泄漏敏感和私有信息。即使在先前的保护措施下，GATEBLEED也对本地和远程时序推断构成风险。GATEBLEED可以作为一种高性能、隐秘的远程隐蔽通道和时序传输通道的通用放大器，能够绕过传统的缓存防御来泄漏任意内存地址，并在以前的攻击失败的现实网络条件和系统配置下，逃避最先进的微架构攻击检测器。我们对一个使用Intel AMX优化的Transformer模型实施了端到端微架构推断攻击，实现了81%的成员推断准确率和0.89的精确度。在一个使用Intel AMX优化的基于CNN或Transformer的专家混合模型中，我们以100%的准确率泄漏了专家选择。", "summary": "这项研究揭示了名为GATEBLEED的新型时序侧信道和隐蔽通道，它源于AI加速器（如Intel AMX）中为控制功耗而采用的激进功耗门控机制。研究表明，该漏洞可用于攻击AI模型的隐私，通过利用每个矩阵乘法中AMX组件选择性断电产生的时序延迟，泄露敏感信息。研究人员在多个流行ML库中发现了可利用的“小工具”，并演示了端到端的微架构推理攻击，成功实现了成员推断和专家选择泄露，且该攻击具有高性能和隐秘性，能够绕过传统防御和先进的攻击检测。", "keywords": "侧信道攻击, AI隐私, 功耗门控, Intel AMX, 微架构攻击", "comments": "这篇论文揭示了AI加速器在功耗优化过程中引入的严重安全隐患，特别是针对Intel AMX这类片上集成加速器。GATEBLEED的发现具有创新性，因为它利用了以往未被充分关注的功耗门控机制。其重要性在于证明了该攻击能够绕过现有防御，并且在实际场景中表现出高效率和隐秘性，对AI模型的隐私构成直接威胁，为未来的硬件设计和软件安全提供了重要的警示。"}}
{"id": "2507.17688", "title": "Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills", "authors": ["Mohammad Nur Hossain Khan", "David creswell", "Jordan Albert", "Patrick O'Connell", "Shawn Fallon", "Mathew Polowitz", "Xuhai \"orson\" Xu", "Bashima islam"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology (IMWUT)", "url": "http://arxiv.org/abs/2507.17688v1", "summary": "Mindfulness training is widely recognized for its benefits in reducing\ndepression, anxiety, and loneliness. With the rise of smartphone-based\nmindfulness apps, digital meditation has become more accessible, but sustaining\nlong-term user engagement remains a challenge. This paper explores whether\nrespiration biosignal feedback and mindfulness skill estimation enhance system\nusability and skill development. We develop a smartphone's accelerometer-based\nrespiration tracking algorithm, eliminating the need for additional wearables.\nUnlike existing methods, our approach accurately captures slow breathing\npatterns typical of mindfulness meditation. Additionally, we introduce the\nfirst quantitative framework to estimate mindfulness skills-concentration,\nsensory clarity, and equanimity-based on accelerometer-derived respiration\ndata. We develop and test our algorithms on 261 mindfulness sessions in both\ncontrolled and real-world settings. A user study comparing an experimental\ngroup receiving biosignal feedback with a control group using a standard app\nshows that respiration feedback enhances system usability. Our respiration\ntracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,\nclosely aligning with ground truth data, while our mindfulness skill estimation\nattains F1 scores of 80-84% in tracking skill progression. By integrating\nrespiration tracking and mindfulness estimation into a commercial app, we\ndemonstrate the potential of smartphone sensors to enhance digital mindfulness\ntraining.", "comment": "Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology\n  (IMWUT)", "pdf_url": "http://arxiv.org/pdf/2507.17688v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "正念冥想与呼吸：基于加速度计的呼吸速率和正念进展估计以增强应用程序参与度和正念技能", "tldr": "本文利用智能手机加速度计跟踪呼吸并估计正念技能，旨在提高正念应用程序的参与度和正念训练效果。", "motivation": "为了解决智能手机正念应用程序中用户长期参与度难以维持的挑战，本文旨在探索呼吸生物信号反馈和正念技能估计是否能增强系统可用性和技能发展。", "method": "开发了一种基于智能手机加速度计的呼吸追踪算法，无需额外可穿戴设备，并能准确捕捉正念冥想的缓慢呼吸模式。首次引入了基于加速度计呼吸数据估计正念技能（专注、感官清晰、平等心）的定量框架。算法在261个正念会话中进行开发和测试。通过用户研究，比较了接收生物信号反馈的实验组与使用标准应用程序的对照组。", "result": "呼吸反馈增强了系统可用性。呼吸追踪模型的平均绝对误差（MAE）为每分钟1.6次呼吸，与真实数据高度一致。正念技能估计在追踪技能进展方面达到了80-84%的F1分数。", "conclusion": "将基于智能手机传感器的呼吸追踪和正念估计集成到商业应用程序中，具有增强数字正念训练的潜力。", "translation": "正念训练因其在减轻抑郁、焦虑和孤独方面的益处而广受认可。随着智能手机正念应用程序的兴起，数字冥想变得更加普及，但维持长期用户参与度仍然是一个挑战。本文探讨了呼吸生物信号反馈和正念技能估计是否能增强系统可用性和技能发展。我们开发了一种基于智能手机加速度计的呼吸追踪算法，无需额外可穿戴设备。与现有方法不同，我们的方法能准确捕捉正念冥想典型的缓慢呼吸模式。此外，我们引入了首个基于加速度计衍生的呼吸数据来估计正念技能（专注、感官清晰、平等心）的定量框架。我们在受控和真实世界环境中，对261个正念会话开发并测试了我们的算法。一项用户研究比较了接收生物信号反馈的实验组与使用标准应用程序的对照组，结果表明呼吸反馈增强了系统可用性。我们的呼吸追踪模型实现了每分钟1.6次呼吸的平均绝对误差（MAE），与地面真实数据非常吻合，而我们的正念技能估计在追踪技能进展方面达到了80-84%的F1分数。通过将呼吸追踪和正念估计集成到商业应用程序中，我们展示了智能手机传感器增强数字正念训练的潜力。", "summary": "本文旨在解决正念应用程序用户参与度低的挑战，提出了一种利用智能手机加速度计进行创新性测量的方法。该研究开发了一种无需额外可穿戴设备即可追踪呼吸速率的算法，并能捕捉冥想中的慢呼吸模式。此外，它首次提出了一个定量框架，利用加速度计衍生的呼吸数据来估计正念技能（专注、感官清晰、平等心）。该系统在261个冥想会话中进行了测试，呼吸追踪的平均绝对误差（MAE）为每分钟1.6次呼吸，正念技能估计的F1分数达到80-84%。用户研究表明，呼吸反馈能提高应用程序的可用性，证明了智能手机传感器在增强数字正念训练方面的巨大潜力。", "keywords": "正念, 呼吸追踪, 加速度计, 数字健康, 用户参与度", "comments": "该论文的创新之处在于，它利用了智能手机中普遍可用的加速度计，实现了呼吸追踪以及独创的定量正念技能估计，从而消除了对额外可穿戴设备的需求。这显著降低了用户的入门门槛，并有望极大地提高数字正念训练的参与度和有效性，解决了该领域的一个关键挑战。"}}
{"id": "2507.17241", "title": "Eco-Friendly AI: Unleashing Data Power for Green Federated Learning", "authors": ["Mattia Sabella", "Monica Vitali"], "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17241v1", "summary": "The widespread adoption of Artificial Intelligence (AI) and Machine Learning\n(ML) comes with a significant environmental impact, particularly in terms of\nenergy consumption and carbon emissions. This pressing issue highlights the\nneed for innovative solutions to mitigate AI's ecological footprint. One of the\nkey factors influencing the energy consumption of ML model training is the size\nof the training dataset. ML models are often trained on vast amounts of data\ncontinuously generated by sensors and devices distributed across multiple\nlocations. To reduce data transmission costs and enhance privacy, Federated\nLearning (FL) enables model training without the need to move or share raw\ndata. While FL offers these advantages, it also introduces challenges due to\nthe heterogeneity of data sources (related to volume and quality),\ncomputational node capabilities, and environmental impact.\n  This paper contributes to the advancement of Green AI by proposing a\ndata-centric approach to Green Federated Learning. Specifically, we focus on\nreducing FL's environmental impact by minimizing the volume of training data.\nOur methodology involves the analysis of the characteristics of federated\ndatasets, the selecting of an optimal subset of data based on quality metrics,\nand the choice of the federated nodes with the lowest environmental impact. We\ndevelop a comprehensive methodology that examines the influence of data-centric\nfactors, such as data quality and volume, on FL training performance and carbon\nemissions. Building on these insights, we introduce an interactive\nrecommendation system that optimizes FL configurations through data reduction,\nminimizing environmental impact during training. Applying this methodology to\ntime series classification has demonstrated promising results in reducing the\nenvironmental impact of FL tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17241v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "环保人工智能：释放数据力量实现绿色联邦学习", "tldr": "本文提出一种以数据为中心的绿色联邦学习方法，通过优化数据量和质量、选择低环境影响节点来减少AI的环境足迹，并在时间序列分类中取得积极效果。", "motivation": "人工智能和机器学习的广泛应用导致巨大的能源消耗和碳排放，对环境造成显著影响。联邦学习虽能减少数据传输，但仍面临数据异构性和环境影响的挑战，因此迫切需要创新方案来减轻AI的生态足迹，特别是通过减少训练数据量。", "method": "本文提出一种数据中心化的绿色联邦学习方法，旨在通过最小化训练数据量来降低联邦学习的环境影响。具体方法包括：分析联邦数据集的特征，基于质量指标选择最优数据子集，以及选择环境影响最低的联邦节点。此外，还开发了一个交互式推荐系统，通过数据缩减来优化联邦学习配置，从而最小化训练过程中的环境影响。", "result": "将该方法应用于时间序列分类任务，结果表明在降低联邦学习任务的环境影响方面取得了显著成效。", "conclusion": "通过以数据为中心的方法，优化数据量和质量，并引入交互式推荐系统，可以有效降低联邦学习的环境影响，推动绿色AI的发展。", "translation": "人工智能（AI）和机器学习（ML）的广泛应用带来了显著的环境影响，尤其是在能源消耗和碳排放方面。这个紧迫的问题凸显了减轻AI生态足迹的创新解决方案的需求。影响ML模型训练能耗的关键因素之一是训练数据集的大小。ML模型通常在由分布在多个位置的传感器和设备持续生成的大量数据上进行训练。为了降低数据传输成本并增强隐私，联邦学习（FL）允许在无需移动或共享原始数据的情况下进行模型训练。尽管FL提供了这些优势，但由于数据源（与数量和质量相关）、计算节点能力和环境影响的异构性，它也带来了挑战。\n本文通过提出一种以数据为中心的绿色联邦学习方法，为绿色AI的进步做出了贡献。具体而言，我们通过最小化训练数据量来减少FL的环境影响。我们的方法包括分析联邦数据集的特征，根据质量指标选择最优数据子集，以及选择环境影响最低的联邦节点。我们开发了一种全面的方法，检查数据中心因素（如数据质量和数量）对FL训练性能和碳排放的影响。基于这些见解，我们引入了一个交互式推荐系统，通过数据缩减优化FL配置，从而最小化训练期间的环境影响。将此方法应用于时间序列分类已在减少FL任务的环境影响方面取得了有希望的结果。", "summary": "本文针对人工智能和机器学习训练带来的环境影响问题，提出了一种以数据为中心的绿色联邦学习方法。该方法通过分析联邦数据集特征、基于质量选择最优数据子集、以及选择低环境影响的节点来最小化训练数据量，从而降低联邦学习的碳排放。研究还开发了一个交互式推荐系统以优化配置，并在时间序列分类任务中验证了其在减少环境影响方面的有效性。", "keywords": "绿色AI, 联邦学习, 数据优化, 环境影响, 碳排放", "comments": "这篇论文的创新点在于提出了一个数据驱动的框架来解决联邦学习的环境影响问题，这与传统的侧重于模型或算法优化的绿色AI研究有所不同。通过关注数据量和质量的优化，以及节点选择，为实现更可持续的AI训练提供了新颖且实用的途径。其提出的交互式推荐系统具有潜在的应用价值。"}}
{"id": "2507.17522", "title": "STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic Point Clouds", "authors": ["Tian Guo", "Hui Yuan", "Xiaolong Mao", "Shiqi Jiang", "Raouf Hamzaoui", "Sam Kwong"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17522v1", "summary": "Very few studies have addressed quality enhancement for compressed dynamic\npoint clouds. In particular, the effective exploitation of spatial-temporal\ncorrelations between point cloud frames remains largely unexplored. Addressing\nthis gap, we propose a spatial-temporal attribute quality enhancement (STQE)\nnetwork that exploits both spatial and temporal correlations to improve the\nvisual quality of G-PCC compressed dynamic point clouds. Our contributions\ninclude a recoloring-based motion compensation module that remaps reference\nattribute information to the current frame geometry to achieve precise\ninter-frame geometric alignment, a channel-aware temporal attention module that\ndynamically highlights relevant regions across bidirectional reference frames,\na Gaussian-guided neighborhood feature aggregation module that efficiently\ncaptures spatial dependencies between geometry and color attributes, and a\njoint loss function based on the Pearson correlation coefficient, designed to\nalleviate over-smoothing effects typical of point-wise mean squared error\noptimization. When applied to the latest G-PCC test model, STQE achieved\nimprovements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with\nBj{\\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%\nfor the Luma, Cb, and Cr components, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17522v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "STQE：G-PCC压缩动态点云的时空质量增强", "tldr": "本文提出了一种名为STQE的时空属性质量增强网络，通过利用时空相关性来提升G-PCC压缩动态点云的视觉质量，并在PSNR和BD-rate上取得了显著改进。", "motivation": "现有研究很少关注压缩动态点云的质量增强，特别是对点云帧之间时空相关性的有效利用仍未得到充分探索。", "method": "本文提出了一种时空属性质量增强（STQE）网络，利用时空相关性来提升G-PCC压缩动态点云的视觉质量。该网络包含：1) 基于重着色的运动补偿模块，用于精确帧间几何对齐；2) 通道感知时间注意力模块，动态突出双向参考帧中的相关区域；3) 高斯引导邻域特征聚合模块，有效捕获几何和颜色属性之间的空间依赖性；4) 基于皮尔逊相关系数的联合损失函数，以减轻点式均方误差优化中常见的过平滑效应。", "result": "STQE应用于最新的G-PCC测试模型时，在Delta PSNR方面，Luma、Cb和Cr分量分别取得了0.855 dB、0.682 dB和0.828 dB的改进；在Bj{\\o}ntegaard Delta rate（BD-rate）方面，Luma、Cb和Cr分量分别降低了-25.2%、-31.6%和-32.5%。", "conclusion": "该研究提出的STQE网络通过有效利用时空相关性，显著提升了G-PCC压缩动态点云的视觉质量，克服了现有方法的局限性，并取得了量化性能的显著改善。", "translation": "很少有研究关注压缩动态点云的质量增强。特别是，点云帧之间时空相关性的有效利用仍未得到充分探索。为了弥补这一空白，我们提出了一种时空属性质量增强（STQE）网络，该网络利用空间和时间相关性来提高G-PCC压缩动态点云的视觉质量。我们的贡献包括：一个基于重着色的运动补偿模块，它将参考属性信息重新映射到当前帧几何以实现精确的帧间几何对齐；一个通道感知时间注意力模块，它动态地突出双向参考帧中的相关区域；一个高斯引导邻域特征聚合模块，它有效地捕获几何和颜色属性之间的空间依赖性；以及一个基于皮尔逊相关系数的联合损失函数，旨在减轻点式均方误差优化中常见的过平滑效应。当应用于最新的G-PCC测试模型时，STQE在Delta PSNR方面，Luma、Cb和Cr分量分别取得了0.855 dB、0.682 dB和0.828 dB的改进，同时Bj{\\o}ntegaard Delta rate（BD-rate）分别降低了-25.2%、-31.6%和-32.5%。", "summary": "本文针对G-PCC压缩动态点云的质量增强问题，提出了一种名为STQE的时空属性质量增强网络。该网络通过创新的模块（如基于重着色的运动补偿、通道感知时间注意力、高斯引导邻域特征聚合）和基于皮尔逊相关系数的联合损失函数，有效利用了点云数据的空间和时间相关性。实验结果表明，STQE在PSNR和BD-rate方面均取得了显著的性能提升，有效改善了压缩动态点云的视觉质量。", "keywords": "点云质量增强, 时空相关性, G-PCC, 动态点云, 深度学习", "comments": "该论文的创新点在于提出了一个端到端的STQE网络，特别关注了对压缩动态点云中未充分利用的时空相关性的挖掘。其模块设计，如基于重着色的运动补偿和通道感知时间注意力，体现了对点云数据特性（几何与属性）的深刻理解。联合损失函数的设计也有效地解决了传统MSE优化带来的过平滑问题，提升了视觉质量。这些创新使其在点云质量增强领域具有重要意义。"}}
{"id": "2507.17709", "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17709v1", "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17709v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "TyDi QA-WANA：西亚和北非语言信息检索问答基准", "tldr": "TyDi QA-WANA是一个包含2.8万个实例的问答数据集，覆盖西亚和北非的10种语言，旨在评估模型处理大型文本上下文和信息检索式问题的能力，且数据直接收集未翻译，并提供了基线模型性能。", "motivation": "现有的问答数据集可能缺乏对西亚和北非多种语言的支持，并且可能没有充分关注信息检索式问题或评估模型处理大型文本上下文的能力，或存在翻译带来的文化相关性问题。", "method": "作者构建了一个名为TyDi QA-WANA的问答数据集，包含2.8万个实例，涵盖西亚和北非的10种语言。数据收集旨在获取真实的信息检索式问题，并为每个问题匹配一篇完整的文章。数据直接以各种语言收集，避免了翻译。论文还提供了两个基线模型的性能。", "result": "论文成功构建了TyDi QA-WANA数据集，包含2.8万个实例，覆盖10种西亚和北非语言。该数据集适合评估模型处理大型文本上下文和信息检索式问答的能力。作者还展示了两个基线模型的性能。", "conclusion": "TyDi QA-WANA数据集提供了一个重要的资源，用于推动西亚和北非语言的信息检索问答研究，通过其独特的数据收集方法和对大型文本上下文的关注，有助于开发更强大的多语言问答模型。", "translation": "我们提出了TyDi QA-WANA，一个问答数据集，包含2.8万个实例，分布在西亚和北非的10种语言变体中。数据收集过程旨在引出信息检索式问题，即提问者真心想知道答案。每个问题都配有一整篇文章，文章可能包含也可能不包含答案；文章相对较大的篇幅使得这项任务适合评估模型利用大型文本上下文回答问题的能力。此外，数据直接以每种语言变体收集，没有使用翻译，以避免文化相关性问题。我们展示了两个基线模型的性能，并发布了我们的代码和数据，以促进研究社区的进一步改进。", "summary": "TyDi QA-WANA是一个新颖的问答数据集，包含2.8万个实例，涵盖西亚和北非的10种语言。它专注于信息检索式问题，并为每个问题提供完整的文章，旨在评估模型处理大型文本上下文的能力。数据集直接以原生语言收集，避免了翻译引入的文化偏差。研究者提供了两个基线模型的性能，并开源了数据和代码以促进后续研究。", "keywords": "问答, 数据集, 多语言, 西亚和北非, 信息检索", "comments": "TyDi QA-WANA的创新在于其对多语言（特别是西亚和北非语言）问答的关注，以及采用直接收集而非翻译数据的方式，有效避免了文化相关性问题。其设计的信息检索式问题和配对完整文章的特点，使其成为评估模型在真实场景下处理长文本理解和信息查找能力的重要基准。该数据集的发布及其开源的代码和数据，对推动低资源语言和多语言问答领域的研究具有重要意义。"}}
{"id": "2505.10027", "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "authors": ["Shijie Lyu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This submission included authors who did not consent to the submission. The paper is being withdrawn until authorship issues are resolved", "url": "http://arxiv.org/abs/2505.10027v2", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "comment": "This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved", "pdf_url": "http://arxiv.org/pdf/2505.10027v2", "cate": "cs.CV", "date": "2025-05-15", "updated": "2025-07-23", "AI": {"title_translation": "ORL-LDM：离线强化学习引导的潜在扩散模型超分辨率重建", "tldr": "本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率重建，在RESISC45数据集上取得了显著的性能提升。", "motivation": "遥感技术快速发展，超分辨率图像重建具有重要的研究和实践意义。现有深度学习方法在处理复杂场景和保留图像细节方面仍有限制。", "method": "本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率。该方法构建了一个包含状态、动作和奖励的强化学习环境，并通过近端策略优化（PPO）在LDM模型的逆向去噪过程中优化决策目标。", "result": "在RESISC45数据集上的实验表明，该方法在PSNR、SSIM和LPIPS方面均显著优于基线模型，其中PSNR提高了3-4dB，SSIM提高了0.08-0.11，LPIPS降低了0.06-0.10，尤其在结构化和复杂自然场景中表现突出。", "conclusion": "实验结果证明了该方法在提高超分辨率质量和场景适应性方面的有效性。", "translation": "随着遥感技术的快速发展，超分辨率图像重建具有重要的研究和实践意义。现有的深度学习方法取得了进展，但在处理复杂场景和保留图像细节方面仍然面临限制。本文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于遥感图像超分辨率。该方法构建了一个包含状态、动作和奖励的强化学习环境，通过近端策略优化（PPO）在LDM模型的逆向去噪过程中优化决策目标。在RESISC45数据集上的实验表明，与基线模型相比，该方法在PSNR、SSIM和LPIPS方面均有显著改进，其中PSNR增加了3-4dB，SSIM提高了0.08-0.11，LPIPS降低了0.06-0.10，特别是在结构化和复杂自然场景中。结果表明该方法在提高超分辨率质量和场景适应性方面是有效的。", "summary": "本文提出了一种名为ORL-LDM的新型遥感图像超分辨率重建方法。该方法通过构建强化学习环境，并利用近端策略优化（PPO）在潜在扩散模型（LDM）的逆向去噪过程中对模型进行微调。实验结果表明，ORL-LDM在RESISC45数据集上显著提升了PSNR、SSIM等指标，尤其在复杂场景下表现出色，证明了其在提高超分辨率质量和场景适应性方面的有效性。", "keywords": "超分辨率重建, 强化学习, 潜在扩散模型, 遥感图像, PPO", "comments": "该论文的创新点在于将强化学习引入到潜在扩散模型的微调过程中，以优化遥感图像的超分辨率重建。通过构建强化学习环境和使用PPO算法，有效解决了现有深度学习方法在处理复杂场景和细节保留方面的局限性，为遥感图像处理提供了一个新的视角和有效的解决方案。"}}
{"id": "2507.17064", "title": "SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure", "authors": ["Nafisa Anjum", "Tasnuva Farheen"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17064v1", "summary": "With the advent of modern technology, critical infrastructure,\ncommunications, and national security depend increasingly on space-based\nassets. These assets, along with associated assets like data relay systems and\nground stations, are, therefore, in serious danger of cyberattacks. Strong\nsecurity defenses are essential to ensure data integrity, maintain secure\noperations, and protect assets in space and on the ground against various\nthreats. Previous research has found discrete vulnerabilities in space systems\nand suggested specific solutions to address them. Such research has yielded\nvaluable insights, but lacks a thorough examination of space cyberattack\nvectors and a rigorous assessment of the efficacy of mitigation techniques.\nThis study tackles this issue by taking a comprehensive approach to analyze the\nrange of possible space cyber-attack vectors, which include ground, space,\nsatellite, and satellite constellations. In order to address the particular\nthreats, the study also assesses the efficacy of mitigation measures that are\nlinked with space infrastructures and proposes a Risk Scoring Framework. Based\non the analysis, this paper identifies potential research challenges for\ndeveloping and testing cutting-edge technology solutions, encouraging robust\ncybersecurity measures needed in space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17064v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "SoK：保障天基基础设施网络安全的最后防线", "tldr": "鉴于天基基础设施日益增长的重要性及其面临的网络攻击风险，本文全面分析了潜在的网络攻击向量和缓解措施的有效性，并提出了一个风险评分框架，同时指出了未来的研究挑战。", "motivation": "现代技术的发展使得关键基础设施、通信和国家安全日益依赖天基资产。这些资产及其相关系统（如数据中继系统和地面站）面临着严重的网络攻击危险。现有研究在太空系统漏洞方面缺乏对网络攻击向量的彻底检查和对缓解技术有效性的严格评估。", "method": "本研究采取综合方法，分析了地面、太空、卫星和卫星星座等一系列可能的太空网络攻击向量，评估了与天基基础设施相关的缓解措施的有效性，并提出了一个风险评分框架。", "result": "基于分析，本文识别了开发和测试尖端技术解决方案的潜在研究挑战，以促进太空领域所需的强大网络安全措施。", "conclusion": "为了确保太空领域的强大网络安全，需要开发和测试尖端技术解决方案，本文指出了实现这一目标的关键研究挑战。", "translation": "随着现代科技的进步，关键基础设施、通信和国家安全越来越依赖天基资产。因此，这些资产以及数据中继系统和地面站等相关资产，正面临着网络攻击的严重威胁。强大的安全防御对于确保数据完整性、维护安全操作以及保护太空和地面资产免受各种威胁至关重要。先前的研究已经发现了太空系统中的离散漏洞，并提出了具体的解决方案来解决这些问题。尽管这些研究提供了宝贵的见解，但它们缺乏对太空网络攻击向量的彻底检查以及对缓解技术有效性的严格评估。本研究通过采取综合方法来解决这个问题，分析了可能的太空网络攻击向量的范围，包括地面、太空、卫星和卫星星座。为了应对特定的威胁，本研究还评估了与天基基础设施相关的缓解措施的有效性，并提出了一个风险评分框架。基于此分析，本文识别了开发和测试尖端技术解决方案的潜在研究挑战，以鼓励太空领域所需的强大网络安全措施。", "summary": "本研究针对天基基础设施面临日益增长的网络安全威胁，进行了全面分析。文章详细探讨了包括地面、太空、卫星和卫星星座在内的各类潜在网络攻击向量，并评估了现有缓解措施的有效性。此外，本文提出了一种风险评分框架，并基于此分析，识别了未来在开发和测试尖端技术解决方案方面存在的潜在研究挑战，旨在推动太空网络安全能力的提升。", "keywords": "网络安全, 天基基础设施, 网络攻击, 缓解措施, 风险评分框架", "comments": "本文的创新之处在于其对太空网络攻击向量的全面分析以及对缓解措施有效性的评估，填补了现有研究的空白。提出的风险评分框架是一个有价值的贡献，为评估和管理太空网络风险提供了系统方法。文章强调了太空网络安全的关键性，并为未来研究指明了方向。"}}
{"id": "2507.16886", "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning", "authors": ["Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figure, under review", "url": "http://arxiv.org/abs/2507.16886v1", "summary": "Spatial transcriptomics (ST) has revolutionized biomedical research by\nenabling high resolution gene expression profiling within tissues. However, the\nhigh cost and scarcity of high resolution ST data remain significant\nchallenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel\nframework for accurate ST imputation that requires only a single and low-cost\nsparsely sampled ST dataset alongside widely available natural images for\nco-training. Our approach integrates three key innovations: (1) a\nsparser-to-sparse self-supervised learning strategy that leverages intrinsic\nspatial patterns in ST data, (2) cross-domain co-learning with natural images\nto enhance feature representation, and (3) a Cascaded Data Consistent\nImputation Network (CDCIN) that iteratively refines predictions while\npreserving sampled gene data fidelity. Extensive experiments on diverse tissue\ntypes, including breast cancer, liver, and lymphoid tissue, demonstrate that\nour method outperforms state-of-the-art approaches in imputation accuracy. By\nenabling robust ST reconstruction from sparse inputs, our framework\nsignificantly reduces reliance on costly high resolution data, facilitating\npotential broader adoption in biomedical research and clinical applications.", "comment": "16 pages, 5 figure, under review", "pdf_url": "http://arxiv.org/pdf/2507.16886v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "Sparser2Sparse: 单次稀疏到稀疏学习，用于空间转录组学插补与自然图像协同学习", "tldr": "S2S-ST是一种新颖的框架，通过单次稀疏采样ST数据和自然图像协同训练，实现高精度空间转录组学数据插补，显著降低成本。", "motivation": "高分辨率空间转录组学数据成本高昂且稀缺，这阻碍了其广泛应用。", "method": "提出了Single-shot Sparser-to-Sparse (S2S-ST) 框架，仅需单次低成本稀疏采样ST数据集和广泛可用的自然图像进行协同训练。该方法整合了三项创新：1) 利用ST数据内在空间模式的稀疏到稀疏自监督学习策略；2) 与自然图像的跨域协同学习以增强特征表示；3) 级联数据一致性插补网络 (CDCIN) 迭代优化预测并保留采样基因数据保真度。", "result": "在乳腺癌、肝脏和淋巴组织等多种组织类型上的广泛实验表明，该方法在插补精度方面优于现有最先进的方法。", "conclusion": "S2S-ST通过从稀疏输入中实现稳健的ST重建，显著降低了对昂贵高分辨率数据的依赖，促进了其在生物医学研究和临床应用中的潜在更广泛采用。", "translation": "空间转录组学 (ST) 通过在组织内实现高分辨率基因表达谱分析，彻底改变了生物医学研究。然而，高成本和高分辨率ST数据的稀缺性仍然是重大挑战。我们提出了单次稀疏到稀疏 (S2S-ST)，这是一种用于精确ST插补的新颖框架，仅需要单个低成本稀疏采样ST数据集以及广泛可用的自然图像进行协同训练。我们的方法整合了三项关键创新：(1) 一种利用ST数据内在空间模式的稀疏到稀疏自监督学习策略，(2) 与自然图像的跨域协同学习以增强特征表示，以及 (3) 一个级联数据一致性插补网络 (CDCIN)，该网络迭代地细化预测，同时保留采样基因数据的保真度。在包括乳腺癌、肝脏和淋巴组织在内的多种组织类型上的广泛实验表明，我们的方法在插补精度方面优于现有最先进的方法。通过从稀疏输入中实现稳健的ST重建，我们的框架显著降低了对昂贵高分辨率数据的依赖，促进了其在生物医学研究和临床应用中的潜在更广泛采用。", "summary": "本文提出了S2S-ST框架，旨在解决高分辨率空间转录组学数据成本高昂和稀缺的问题。该框架通过单次低成本稀疏采样ST数据和自然图像协同训练，利用稀疏到稀疏自监督学习、跨域协同学习和级联数据一致性插补网络实现高精度基因表达谱插补。实验证明，S2S-ST在多种组织类型上优于现有方法，显著降低了对高分辨率数据的依赖，有望推动空间转录组学在生物医学领域的应用。", "keywords": "空间转录组学, 数据插补, 稀疏学习, 自然图像协同学习, 自监督学习", "comments": "S2S-ST的创新之处在于其“单次稀疏到稀疏”的学习策略，以及巧妙地将“自然图像”引入协同学习，以弥补空间转录组数据稀缺的挑战。这不仅降低了数据获取成本，也为数据增强和表示学习提供了新的视角，对于推动空间转录组学在临床和研究中的普及具有重要意义。"}}
{"id": "2507.17192", "title": "Vec2Face+ for Face Dataset Generation", "authors": ["Haiyu Wu", "Jaskirat Singh", "Sicong Tian", "Liang Zheng", "Kevin W. Bowyer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17192v1", "summary": "When synthesizing identities as face recognition training data, it is\ngenerally believed that large inter-class separability and intra-class\nattribute variation are essential for synthesizing a quality dataset. % This\nbelief is generally correct, and this is what we aim for. However, when\nincreasing intra-class variation, existing methods overlook the necessity of\nmaintaining intra-class identity consistency. % To address this and generate\nhigh-quality face training data, we propose Vec2Face+, a generative model that\ncreates images directly from image features and allows for continuous and easy\ncontrol of face identities and attributes. Using Vec2Face+, we obtain datasets\nwith proper inter-class separability and intra-class variation and identity\nconsistency using three strategies: 1) we sample vectors sufficiently different\nfrom others to generate well-separated identities; 2) we propose an AttrOP\nalgorithm for increasing general attribute variations; 3) we propose LoRA-based\npose control for generating images with profile head poses, which is more\nefficient and identity-preserving than AttrOP. % Our system generates VFace10K,\na synthetic face dataset with 10K identities, which allows an FR model to\nachieve state-of-the-art accuracy on seven real-world test sets. Scaling the\nsize to 4M and 12M images, the corresponding VFace100K and VFace300K datasets\nyield higher accuracy than the real-world training dataset, CASIA-WebFace, on\nfive real-world test sets. This is the first time a synthetic dataset beats the\nCASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11\nsynthetic datasets outperforms random guessing (\\emph{i.e., 50\\%}) in twin\nverification and that models trained with synthetic identities are more biased\nthan those trained with real identities. Both are important aspects for future\ninvestigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17192v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Vec2Face+ 用于人脸数据集生成", "tldr": "本文提出了Vec2Face+模型，用于生成高质量的合成人脸数据集，解决了现有方法在增加类内变化时忽视身份一致性的问题，并首次实现了合成数据集在平均准确率上超越真实世界数据集CASIA-WebFace。", "motivation": "现有的人脸识别训练数据合成方法在增加类内变化时，忽略了保持类内身份一致性的必要性，导致合成数据集质量不足。", "method": "本文提出了Vec2Face+，一个直接从图像特征生成图像的生成模型，并允许对人脸身份和属性进行连续且简便的控制。为实现高类间可分离性、类内变化和身份一致性，采用了三种策略：1）采样足够不同的向量以生成分离良好的身份；2）提出AttrOP算法以增加通用属性变化；3）提出基于LoRA的姿态控制，用于生成侧面头部姿态图像，该方法比AttrOP更高效且能保持身份。", "result": "利用Vec2Face+生成了VFace10K（1万身份）合成人脸数据集，使人脸识别模型在七个真实世界测试集上达到了最先进的准确率。将数据集规模扩展到4M和12M图像时，相应的VFace100K和VFace300K数据集在五个真实世界测试集上比真实世界训练数据集CASIA-WebFace产生了更高的准确率，这是合成数据集首次在平均准确率上超越CASIA-WebFace。此外，研究发现11个合成数据集中只有1个在双胞胎验证中表现优于随机猜测（即50%），并且用合成身份训练的模型比用真实身份训练的模型更具偏见。", "conclusion": "Vec2Face+成功生成了高质量的合成人脸数据集，这些数据集在人脸识别任务中能够超越真实世界数据集。同时，研究也揭示了合成数据在双胞胎验证和模型偏见方面的重要发现，为未来的研究指明了方向。", "translation": "在合成身份作为人脸识别训练数据时，通常认为大的类间可分离性和类内属性变化对于合成高质量数据集至关重要。这种观点通常是正确的，也是我们追求的目标。然而，在增加类内变化时，现有方法忽视了保持类内身份一致性的必要性。为了解决这个问题并生成高质量的人脸训练数据，我们提出了Vec2Face+，一个直接从图像特征创建图像的生成模型，并允许对人脸身份和属性进行连续且简便的控制。使用Vec2Face+，我们通过三种策略获得了具有适当类间可分离性、类内变化和身份一致性的数据集：1）我们采样足够不同的向量以生成分离良好的身份；2）我们提出了AttrOP算法以增加通用属性变化；3）我们提出了基于LoRA的姿态控制，用于生成侧面头部姿态的图像，这比AttrOP更高效且能保持身份。我们的系统生成了VFace10K，一个包含1万身份的合成人脸数据集，它使人脸识别模型在七个真实世界测试集上达到了最先进的准确率。将规模扩展到4M和12M图像时，相应的VFace100K和VFace300K数据集在五个真实世界测试集上比真实世界训练数据集CASIA-WebFace产生了更高的准确率。这是合成数据集首次在平均准确率上超越CASIA-WebFace。此外，我们发现11个合成数据集中只有1个在双胞胎验证中表现优于随机猜测（即50%），并且用合成身份训练的模型比用真实身份训练的模型更具偏见。这两个方面都是未来研究的重要方向。", "summary": "本文提出了Vec2Face+，一种新型生成模型，用于创建高质量的合成人脸数据集。该模型通过直接从图像特征生成图像，并利用三种策略（差异化向量采样、AttrOP算法和基于LoRA的姿态控制）在增加类内变化的同时保持身份一致性。实验证明，Vec2Face+生成的VFace系列数据集（VFace10K、VFace100K、VFace300K）在人脸识别任务上实现了最先进的性能，甚至首次在平均准确率上超越了真实世界数据集CASIA-WebFace。研究还指出，合成数据集在双胞胎验证方面表现不佳，且用合成数据训练的模型可能更具偏见，为未来研究提供了重要启示。", "keywords": "人脸数据集生成, 合成数据, Vec2Face+, 身份一致性, 人脸识别", "comments": "本文的创新点在于提出了Vec2Face+模型，它解决了合成人脸数据集中类内变化与身份一致性之间的矛盾，并首次证明了合成数据集在人脸识别任务中能够超越传统真实数据集的性能。其重要性体现在为高质量合成数据生成提供了有效方案，有望降低对大规模真实标注数据的依赖。同时，论文也指出了合成数据在特定挑战（如双胞胎验证）和模型偏见方面存在的局限性，体现了研究的全面性和深刻性。"}}
{"id": "2507.16875", "title": "Technical report: Impact of Duration Prediction on Speaker-specific TTS for Indian Languages", "authors": ["Isha Pandey", "Pranav Gaikwad", "Amruta Parulekar", "Ganesh Ramakrishnan"], "categories": ["eess.AS", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16875v1", "summary": "High-quality speech generation for low-resource languages, such as many\nIndian languages, remains a significant challenge due to limited data and\ndiverse linguistic structures. Duration prediction is a critical component in\nmany speech generation pipelines, playing a key role in modeling prosody and\nspeech rhythm. While some recent generative approaches choose to omit explicit\nduration modeling, often at the cost of longer training times. We retain and\nexplore this module to better understand its impact in the linguistically rich\nand data-scarce landscape of India. We train a non-autoregressive Continuous\nNormalizing Flow (CNF) based speech model using publicly available Indian\nlanguage data and evaluate multiple duration prediction strategies for\nzero-shot, speaker-specific generation. Our comparative analysis on\nspeech-infilling tasks reveals nuanced trade-offs: infilling based predictors\nimprove intelligibility in some languages, while speaker-prompted predictors\nbetter preserve speaker characteristics in others. These findings inform the\ndesign and selection of duration strategies tailored to specific languages and\ntasks, underscoring the continued value of interpretable components like\nduration prediction in adapting advanced generative architectures to\nlow-resource, multilingual settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16875v1", "cate": "eess.AS", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "技术报告：持续时间预测对印度语言特定说话人文本到语音转换的影响", "tldr": "本研究探讨了持续时间预测在低资源印度语言特定说话人文本到语音转换（TTS）中的作用。研究发现，不同的持续时间预测策略在语音清晰度和说话人特性保留方面存在权衡，强调了在多语言低资源环境下可解释组件的持续价值。", "motivation": "由于数据有限和语言结构多样，为许多印度语言等低资源语言生成高质量语音仍然是一个重大挑战。持续时间预测是语音生成管道中的关键组件，在建模韵律和语音节奏方面发挥着关键作用。尽管一些最新的生成方法选择省略显式持续时间建模，但通常以更长的训练时间为代价。本研究保留并探索了该模块，以更好地理解其在语言丰富且数据稀缺的印度环境中的影响。", "method": "研究训练了一个基于非自回归连续归一化流（CNF）的语音模型，使用公开可用的印度语言数据，并评估了多种持续时间预测策略，用于零样本、特定说话人的生成。通过语音填充任务进行了比较分析。", "result": "比较分析揭示了细微的权衡：基于填充的预测器在某些语言中提高了清晰度，而说话人提示的预测器在其他语言中更好地保留了说话人特性。", "conclusion": "这些发现为针对特定语言和任务的持续时间策略的设计和选择提供了信息，强调了持续时间预测等可解释组件在使先进生成架构适应低资源、多语言环境中的持续价值。", "translation": "技术报告：持续时间预测对印度语言特定说话人文本到语音转换的影响\n\n摘要：由于数据有限和语言结构多样，为许多印度语言等低资源语言生成高质量语音仍然是一个重大挑战。持续时间预测是许多语音生成管道中的关键组件，在建模韵律和语音节奏方面发挥着关键作用。尽管一些最新的生成方法选择省略显式持续时间建模，但通常以更长的训练时间为代价。我们保留并探索了该模块，以更好地理解其在语言丰富且数据稀缺的印度环境中的影响。我们使用公开可用的印度语言数据训练了一个基于非自回归连续归一化流（CNF）的语音模型，并评估了多种持续时间预测策略，用于零样本、特定说话人生成。我们对语音填充任务的比较分析揭示了细微的权衡：基于填充的预测器在某些语言中提高了清晰度，而说话人提示的预测器在其他语言中更好地保留了说话人特性。这些发现为针对特定语言和任务的持续时间策略的设计和选择提供了信息，强调了持续时间预测等可解释组件在使先进生成架构适应低资源、多语言环境中的持续价值。", "summary": "本研究探讨了持续时间预测在低资源印度语言特定说话人文本到语音转换（TTS）中的关键作用。通过使用非自回归连续归一化流（CNF）模型和评估多种持续时间预测策略，研究发现不同策略在语音清晰度和说话人特性保留之间存在权衡。结果表明，基于填充的预测器可提高某些语言的清晰度，而说话人提示的预测器则能更好地保留说话人特征。这强调了在低资源、多语言TTS系统中，持续时间预测等可解释组件对于优化性能的重要性。", "keywords": "持续时间预测, 印度语言, 文本到语音转换, 低资源语言, 连续归一化流", "comments": "这篇技术报告深入探讨了持续时间预测在低资源印度语言TTS中的重要性，这在一个日益关注端到端生成模型的领域中显得尤为创新。它强调了在特定语言和数据稀缺背景下，保留和优化可解释组件（如持续时间预测）的价值，而非盲目采用更复杂的黑盒模型。研究通过详细的比较分析揭示了不同预测策略的实际权衡，为未来在多语言低资源环境下的TTS系统设计提供了宝贵的指导。"}}
{"id": "2403.13574", "title": "Enhancing Sequential Recommender with Large Language Models for Joint Video and Comment Recommendation", "authors": ["Bowen Zheng", "Zihan Lin", "Enze Liu", "Chen Yang", "Enyang Bai", "Cheng Ling", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by RecSys2025", "url": "http://arxiv.org/abs/2403.13574v2", "summary": "Nowadays, reading or writing comments on captivating videos has emerged as a\ncritical part of the viewing experience on online video platforms. However,\nexisting recommender systems primarily focus on users' interaction behaviors\nwith videos, neglecting comment content and interaction in user preference\nmodeling. In this paper, we propose a novel recommendation approach called\nLSVCR that utilizes user interaction histories with both videos and comments to\njointly perform personalized video and comment recommendation. Specifically,\nour approach comprises two key components: sequential recommendation (SR) model\nand supplemental large language model (LLM) recommender. The SR model functions\nas the primary recommendation backbone (retained in deployment) of our method\nfor efficient user preference modeling. Concurrently, we employ a LLM as the\nsupplemental recommender (discarded in deployment) to better capture underlying\nuser preferences derived from heterogeneous interaction behaviors. In order to\nintegrate the strengths of the SR model and the supplemental LLM recommender,\nwe introduce a two-stage training paradigm. The first stage, personalized\npreference alignment, aims to align the preference representations from both\ncomponents, thereby enhancing the semantics of the SR model. The second stage,\nrecommendation-oriented fine-tuning, involves fine-tuning the\nalignment-enhanced SR model according to specific objectives. Extensive\nexperiments in both video and comment recommendation tasks demonstrate the\neffectiveness of LSVCR. Moreover, online A/B testing on KuaiShou platform\nverifies the practical benefits of our approach. In particular, we attain a\ncumulative gain of 4.13% in comment watch time.", "comment": "Accepted by RecSys2025", "pdf_url": "http://arxiv.org/pdf/2403.13574v2", "cate": "cs.IR", "date": "2024-03-20", "updated": "2025-07-23", "AI": {"title_translation": "利用大型语言模型增强序列推荐器实现视频与评论的联合推荐", "tldr": "LSVCR是一种新颖的推荐方法，它利用用户与视频和评论的互动历史，通过结合序列推荐模型和辅助大型语言模型来共同执行个性化视频和评论推荐。", "motivation": "现有的推荐系统主要关注用户与视频的互动行为，忽略了评论内容和互动在用户偏好建模中的作用，而评论已成为在线视频观看体验的关键部分。", "method": "本文提出了一种名为LSVCR的新型推荐方法，它利用用户与视频和评论的互动历史来联合执行个性化视频和评论推荐。LSVCR包含两个关键组件：序列推荐（SR）模型作为主要推荐骨干，以及辅助大型语言模型（LLM）推荐器以更好地捕捉异构互动行为中潜在的用户偏好。为了整合两者的优势，引入了两阶段训练范式：第一阶段是个性化偏好对齐，旨在对齐两个组件的偏好表示以增强SR模型的语义；第二阶段是面向推荐的微调，根据特定目标对对齐增强的SR模型进行微调。", "result": "在视频和评论推荐任务中进行的广泛实验证明了LSVCR的有效性。在快手平台上的在线A/B测试验证了该方法的实际效益，特别是评论观看时间累计增长了4.13%。", "conclusion": "LSVCR方法通过整合视频和评论互动数据以及利用大型语言模型，能够有效提升序列推荐系统的性能，并在实际平台中展现出显著的效益。", "translation": "如今，在引人入胜的视频上阅读或撰写评论已成为在线视频平台上观看体验的关键部分。然而，现有的推荐系统主要关注用户与视频的互动行为，忽略了评论内容和互动在用户偏好建模中的作用。在本文中，我们提出了一种名为LSVCR的新型推荐方法，它利用用户与视频和评论的互动历史来共同执行个性化视频和评论推荐。具体而言，我们的方法包含两个关键组件：序列推荐（SR）模型和辅助大型语言模型（LLM）推荐器。SR模型作为我们方法的主要推荐骨干（在部署中保留），用于高效的用户偏好建模。同时，我们使用LLM作为辅助推荐器（在部署中丢弃），以更好地捕捉来自异构互动行为的潜在用户偏好。为了整合SR模型和辅助LLM推荐器的优势，我们引入了两阶段训练范式。第一阶段，个性化偏好对齐，旨在对齐两个组件的偏好表示，从而增强SR模型的语义。第二阶段，面向推荐的微调，涉及根据特定目标对对齐增强的SR模型进行微调。在视频和评论推荐任务中进行的广泛实验证明了LSVCR的有效性。此外，在快手平台上的在线A/B测试验证了我们方法的实际效益。特别是，我们的评论观看时间累计增长了4.13%。", "summary": "本文提出LSVCR，一种结合序列推荐（SR）模型和大型语言模型（LLM）的新型方法，用于联合视频和评论推荐。针对现有系统忽略评论互动的不足，LSVCR通过两阶段训练（偏好对齐和面向推荐微调）整合SR模型与LLM的优势。实验证明LSVCR在视频和评论推荐任务中有效，并在快手平台的在线A/B测试中实现了评论观看时间4.13%的累积增长，验证了其在实际应用中的效益。", "keywords": "序列推荐, 大型语言模型, 视频推荐, 评论推荐, 联合推荐", "comments": "该论文的创新点在于将大型语言模型引入序列推荐系统，并首次尝试联合推荐视频和评论，充分利用了用户在评论上的互动数据。其两阶段训练范式，特别是通过LLM辅助SR模型进行偏好对齐，提供了一种新颖的增强传统推荐模型语义的方法。在线A/B测试结果，尤其是评论观看时间的显著提升，验证了其在真实世界场景中的实用价值和重要性。"}}
{"id": "2506.15606", "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "authors": ["Gabriel J. Perin", "Runjin Chen", "Xuxi Chen", "Nina S. T. Hirata", "Zhangyang Wang", "Junyuan Hong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15606v2", "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15606v2", "cate": "cs.LG", "date": "2025-06-18", "updated": "2025-07-23", "AI": {"title_translation": "LoX: 低秩外推增强大型语言模型对抗微调的安全鲁棒性", "tldr": "LoX是一种无训练方法，通过低秩外推增强LLM安全，使其在微调后不易受攻击。", "motivation": "大型语言模型（LLMs）在实际应用中广泛采用，但其安全性令人担忧，尤其是在响应有害问题时。尽管已通过对齐提高安全性，但对齐模型仍可能因后续微调而导致安全防护被破坏，即使训练数据看似无害。", "method": "本文提出了一种名为低秩外推（LoX）的新型无训练方法，通过外推对齐LLM的安全子空间来增强安全鲁棒性。研究发现，这种脆弱性源于LLM参数中安全关键低秩子空间对微调的敏感性。", "result": "LoX有效性得到证实，在对抗良性微调和恶意微调攻击时，鲁棒性显著提高，同时保留了模型对新任务的适应性。例如，LoX使良性或恶意微调攻击的攻击成功率（ASR）绝对降低了11%到54%。通过研究参数的ASR图景，我们将LoX的成功归因于外推将LLM参数移动到更平坦的区域，从而对扰动不那么敏感。", "conclusion": "LoX通过将LLM参数移动到更平坦的区域，从而降低其对扰动的敏感性，有效提高了LLM在微调后的安全鲁棒性。", "translation": "大型语言模型（LLMs）已成为现实世界应用中不可或缺的一部分。然而，它们的广泛采用引发了重大的安全问题，尤其是在响应社会有害问题方面。尽管通过对齐在提高模型安全性方面付出了巨大努力，但对齐模型仍可能因后续微调而导致安全防护被破坏——即使额外的训练数据看似无害。在本文中，我们通过实证证明，这种脆弱性源于LLM参数中安全关键低秩子空间对微调的敏感性。基于这一见解，我们提出了一种新颖的无训练方法，称为低秩外推（LoX），通过外推对齐LLM的安全子空间来增强安全鲁棒性。我们的实验结果证实了LoX的有效性，证明在对抗良性微调和恶意微调攻击时，鲁棒性显著提高，同时保留了模型对新任务的适应性。例如，LoX使良性或恶意微调攻击的攻击成功率（ASR）绝对降低了11%到54%。通过研究参数的ASR图景，我们将LoX的成功归因于外推将LLM参数移动到更平坦的区域，从而对扰动不那么敏感。代码可在github.com/VITA-Group/LoX获取。", "summary": "本文关注大型语言模型（LLMs）在微调后安全防护易受损的问题。研究实证表明，此脆弱性源于安全关键低秩子空间对微调的敏感性。为此，提出了一种名为低秩外推（LoX）的无训练方法，通过外推LLM的安全子空间来增强其安全鲁棒性。实验证明LoX能显著降低良性或恶意微调攻击的成功率（11%至54%），同时保持模型适应性。LoX的成功在于它将LLM参数移动到更平坦的区域，使其对扰动不敏感。", "keywords": "大型语言模型, 安全鲁棒性, 微调, 低秩外推, 攻击成功率", "comments": "LoX的创新之处在于提出了一种无训练的低秩外推方法来解决LLM微调后的安全鲁棒性问题。它通过改变参数空间，将模型推向更平坦的区域，从而降低对扰动的敏感性，这提供了一个新的视角来增强模型安全性，且无需额外的训练成本。该方法对于提升LLM在实际部署中的可靠性具有重要意义。"}}
{"id": "2507.17074", "title": "Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond", "authors": ["Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan", "Madhusanka Liyanage"], "categories": ["cs.CR", "cs.NI", "cs.PF"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Table 5, Figures 7, This paper has been accepted as a regular paper at LCN 2025 and will appear in the conference proceedings. The final version will be published by IEEE and the copyright will belong to IEEE", "url": "http://arxiv.org/abs/2507.17074v1", "summary": "The advent of quantum computing threatens the security of classical\npublic-key cryptographic systems, prompting the transition to post-quantum\ncryptography (PQC). While PQC has been analyzed in theory, its performance in\npractical wireless communication environments remains underexplored. This paper\npresents a detailed implementation and performance evaluation of NIST-selected\nPQC algorithms in user equipment (UE) to UE communications over 5G networks.\nUsing a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3\nvia BoringSSL and liboqs, we examine key encapsulation mechanisms and digital\nsignature schemes across realistic network conditions. We evaluate performance\nbased on handshake latency, CPU and memory usage, bandwidth, and retransmission\nrates, under varying cryptographic configurations and client loads. Our\nfindings show that ML-KEM with ML-DSA offers the best efficiency for\nlatency-sensitive applications, while SPHINCS+ and HQC combinations incur\nhigher computational and transmission overheads, making them unsuitable for\nsecurity-critical but time-sensitive 5G scenarios.", "comment": "Table 5, Figures 7, This paper has been accepted as a regular paper\n  at LCN 2025 and will appear in the conference proceedings. The final version\n  will be published by IEEE and the copyright will belong to IEEE", "pdf_url": "http://arxiv.org/pdf/2507.17074v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "5G及未来用户设备中后量子密码学的分析", "tldr": "本文在5G网络中对用户设备间的NIST PQC算法进行了实际实现和性能评估，发现ML-KEM与ML-DSA组合在低延迟应用中表现最佳。", "motivation": "量子计算的出现威胁到传统公钥密码系统的安全性，促使向后量子密码学（PQC）过渡。虽然PQC在理论上已被分析，但其在实际无线通信环境中的性能仍未被充分探索。", "method": "本研究在用户设备（UE）到用户设备（UE）的5G网络通信中，详细实现并评估了NIST选定的PQC算法。使用完整的5G仿真堆栈（Open5GS和UERANSIM）以及通过BoringSSL和liboqs启用PQC的TLS 1.3，研究人员在实际网络条件下检查了密钥封装机制和数字签名方案。性能评估基于握手延迟、CPU和内存使用、带宽以及重传率，在不同的密码配置和客户端负载下进行。", "result": "研究结果表明，ML-KEM与ML-DSA组合为延迟敏感型应用提供了最佳效率，而SPHINCS+和HQC组合则导致更高的计算和传输开销，使其不适用于安全关键但时间敏感的5G场景。", "conclusion": "ML-KEM与ML-DSA是5G网络中延迟敏感型后量子密码应用的最佳选择，而SPHINCS+和HQC组合由于其高开销，不适合时间敏感的5G场景。", "translation": "量子计算的出现威胁着经典公钥密码系统的安全，促使向后量子密码学（PQC）过渡。虽然PQC在理论上已被分析，但其在实际无线通信环境中的性能仍未被充分探索。本文详细实现了NIST选定的PQC算法，并在用户设备（UE）到用户设备（UE）的5G网络通信中进行了性能评估。通过使用完整的5G仿真堆栈（Open5GS和UERANSIM）以及通过BoringSSL和liboqs启用PQC的TLS 1.3，我们检查了在实际网络条件下的密钥封装机制和数字签名方案。我们根据握手延迟、CPU和内存使用、带宽以及重传率，在不同的密码配置和客户端负载下评估了性能。我们的研究结果表明，ML-KEM与ML-DSA组合为延迟敏感型应用提供了最佳效率，而SPHINCS+和HQC组合则导致更高的计算和传输开销，使其不适用于安全关键但时间敏感的5G场景。", "summary": "本文在5G网络环境下，对用户设备间部署的NIST后量子密码（PQC）算法进行了首次实际性能评估。研究团队利用5G仿真堆栈和PQC支持的TLS 1.3，测试了不同PQC算法在握手延迟、资源消耗和带宽等方面的表现。结果表明，ML-KEM与ML-DSA的组合最适合对延迟敏感的应用，而SPHINCS+和HQC则因高开销不适用于时间敏感的5G场景。", "keywords": "后量子密码学, 5G网络, 用户设备, 性能评估, ML-KEM", "comments": "本文的创新之处在于首次将NIST选定的PQC算法在实际5G用户设备环境中进行性能评估，填补了理论分析与实际应用之间的空白。其重要性在于为未来5G及B5G网络中PQC的部署提供了关键的性能基准和选型依据，特别是指出了哪些算法组合更适合特定场景，如延迟敏感型应用。这对于保障未来通信网络的安全性和效率具有指导意义。"}}
{"id": "2507.17314", "title": "How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?", "authors": ["Ricardo Hidalgo Aragón", "Jesús M. González-Barahona", "Gregorio Robles"], "categories": ["cs.SE", "K.3.2, D.2.m, D.1.7"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Registered Report accepted at ICSME 2025", "url": "http://arxiv.org/abs/2507.17314v1", "summary": "Context. Code smells, which are recurring anomalies in design or style, have\nbeen extensively researched in professional code. However, their significance\nin block-based projects created by novices is still largely unknown.\nBlock-based environments such as Scratch offer a unique, data-rich setting to\nexamine how emergent design problems intersect with the cultivation of\ncomputational-thinking (CT) skills. Objective. This research explores the\nconnection between CT proficiency and design-level code smells--issues that may\nhinder software maintenance and evolution--in programs created by Scratch\ndevelopers. We seek to identify which CT dimensions align most strongly with\nwhich code smells and whether task context moderates those associations.\nMethod. A random sample of aprox. 2 million public Scratch projects is mined.\nUsing open-source linters, we extract nine CT scores and 40 code smell\nindicators from these projects. After rigorous pre-processing, we apply\ndescriptive analytics, robust correlation tests, stratified cross-validation,\nand exploratory machine-learning models; qualitative spot-checks contextualize\nquantitative patterns. Impact. The study will deliver the first large-scale,\nfine-grained map linking specific CT competencies to concrete design flaws and\nantipatterns. Results are poised to (i) inform evidence-based curricula and\nautomated feedback systems, (ii) provide effect-size benchmarks for future\neducational interventions, and (iii) supply an open, pseudonymized dataset and\nreproducible analysis pipeline for the research community. By clarifying how\nprogramming habits influence early skill acquisition, the work advances both\ncomputing-education theory and practical tooling for sustainable software\nmaintenance and evolution.", "comment": "Registered Report accepted at ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2507.17314v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "代码异味如何影响Scratch新手程序员的技能成长？", "tldr": "本研究探讨了Scratch新手程序员在编程过程中，设计层面的代码异味与计算思维能力之间的关联。", "motivation": "尽管代码异味在专业代码中已被广泛研究，但它们在新手创建的基于块的项目中的重要性仍不清楚。Scratch等基于块的环境为研究新兴设计问题如何与计算思维（CT）技能培养相结合提供了独特的数据丰富环境。本研究旨在探索CT熟练程度与Scratch开发者创建的程序中设计层面代码异味（可能阻碍软件维护和演进的问题）之间的联系。", "method": "研究挖掘了大约200万个公开的Scratch项目。使用开源代码检查工具，从这些项目中提取了9个计算思维分数和40个代码异味指标。经过严格的预处理后，应用了描述性分析、鲁棒相关性测试、分层交叉验证和探索性机器学习模型；定性抽查用于情境化定量模式。", "result": "研究将首次提供大规模、细粒度的图谱，将特定的计算思维能力与具体的设计缺陷和反模式联系起来。研究结果有望（i）为循证课程和自动化反馈系统提供信息，（ii）为未来的教育干预提供效应量基准，以及（iii）为研究社区提供开放、假名化数据集和可复现的分析流程。", "conclusion": "通过阐明编程习惯如何影响早期技能习得，这项工作推进了计算教育理论以及可持续软件维护和演进的实用工具。", "translation": "背景。代码异味是设计或风格中反复出现的异常，在专业代码中已得到广泛研究。然而，它们在新手创建的基于块的项目中的重要性仍 largely 未知。Scratch 等基于块的环境提供了一个独特、数据丰富的环境，以检查新兴设计问题如何与计算思维（CT）技能的培养相结合。目标。本研究探讨了Scratch开发者创建的程序中，计算思维熟练程度与设计层面的代码异味（可能阻碍软件维护和演进的问题）之间的联系。我们旨在识别哪些计算思维维度与哪些代码异味最强相关，以及任务上下文是否调节这些关联。方法。对约200万个公共Scratch项目进行随机抽样挖掘。使用开源代码检查工具，我们从这些项目中提取了9个计算思维分数和40个代码异味指标。经过严格的预处理后，我们应用了描述性分析、鲁棒相关性测试、分层交叉验证和探索性机器学习模型；定性抽查用于情境化定量模式。影响。这项研究将提供第一个大规模、细粒度的图谱，将特定的计算思维能力与具体的设计缺陷和反模式联系起来。研究结果有望（i）为循证课程和自动化反馈系统提供信息，（ii）为未来的教育干预提供效应量基准，以及（iii）为研究社区提供开放、假名化数据集和可复现的分析流程。通过阐明编程习惯如何影响早期技能习得，这项工作推进了计算教育理论以及可持续软件维护和演进的实用工具。", "summary": "本研究调查了代码异味如何影响Scratch新手程序员的技能成长。通过对约200万个Scratch项目进行大规模数据挖掘和分析，研究旨在识别计算思维能力与设计层面代码异味之间的关联，并探讨任务上下文的作用。研究结果将首次提供计算思维能力与设计缺陷之间的细粒度映射，为教育课程、自动化反馈系统以及未来的教育干预提供实证基础，并为社区提供开放数据集和分析流程，从而推进计算教育理论和软件维护实践。", "keywords": "代码异味, Scratch, 新手程序员, 计算思维, 技能成长", "comments": "该研究通过对海量Scratch项目进行大规模数据分析，首次将计算思维能力与新手编程中的代码异味关联起来，具有重要的创新性和实践意义。其提供的开放数据集和分析流程将极大地促进后续研究，并有望直接影响编程教育课程和工具的开发。"}}
{"id": "2412.09839", "title": "AI and Deep Learning for Terahertz Ultra-Massive MIMO: From Model-Driven Approaches to Foundation Models", "authors": ["Wentao Yu", "Hengtao He", "Shenghui Song", "Jun Zhang", "Linglong Dai", "Lizhong Zheng", "Khaled B. Letaief"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      30 pages, 8 figures, 1 table, accepted by Engineering. Model-driven deep learning, CSI foundation models, and applications of LLMs are presented as three systematic research roadmaps for AI-enabled THz ultra-massive MIMO systems", "url": "http://arxiv.org/abs/2412.09839v3", "summary": "This study explored the transformative potential of artificial intelligence\n(AI) in addressing the challenges posed by terahertz ultra-massive\nmultiple-input multiple-output (UM-MIMO) systems. It begins by outlining the\ncharacteristics of terahertz UM-MIMO systems and identifies three primary\nchallenges for transceiver design: computational complexity, modeling\ndifficulty, and measurement limitations. The study posits that AI provides a\npromising solution to these challenges. Three systematic research roadmaps are\nproposed for developing AI algorithms tailored to terahertz UM-MIMO systems.\nThe first roadmap, model-driven deep learning (DL), emphasizes the importance\nof leveraging available domain knowledge and advocates the adoption of AI only\nto enhance bottleneck modules within an established signal processing or\noptimization framework. Four essential steps are discussed: algorithmic\nframeworks, basis algorithms, loss-function design, and neural architecture\ndesign. The second roadmap presents channel state information (CSI) foundation\nmodels, aimed at unifying the design of different transceiver modules by\nfocusing on their shared foundation, that is, the wireless channel. The\ntraining of a single compact foundation model is proposed to estimate the score\nfunction of wireless channels, which serve as a versatile prior for designing a\nwide variety of transceiver modules. Four essential steps are outlined: general\nframeworks, conditioning, site-specific adaptation, joint design of CSI\nfoundation models, and model-driven DL. The third roadmap aims to explore\npotential directions for applying pretrained large language models (LLMs) to\nterahertz UM-MIMO systems. Several application scenarios are envisioned,\nincluding LLM-based estimation, optimization, search, network management, and\nprotocol understanding. Finally, the study highlights open problems and future\nresearch directions.", "comment": "30 pages, 8 figures, 1 table, accepted by Engineering. Model-driven\n  deep learning, CSI foundation models, and applications of LLMs are presented\n  as three systematic research roadmaps for AI-enabled THz ultra-massive MIMO\n  systems", "pdf_url": "http://arxiv.org/pdf/2412.09839v3", "cate": "eess.SP", "date": "2024-12-13", "updated": "2025-07-22", "AI": {"title_translation": "人工智能与深度学习在太赫兹超大规模MIMO中的应用：从模型驱动方法到基础模型", "tldr": "本研究探讨了人工智能在解决太赫兹超大规模MIMO系统挑战方面的潜力，并提出了模型驱动深度学习、CSI基础模型和大型语言模型应用三个研究路线图。", "motivation": "太赫兹超大规模MIMO系统在收发机设计上面临计算复杂度高、建模困难和测量受限等挑战，而人工智能被认为能提供有前景的解决方案。", "method": "本研究提出了三个系统的研究路线图：1. 模型驱动深度学习，强调利用领域知识增强现有框架中的瓶颈模块。2. 信道状态信息（CSI）基础模型，旨在通过关注无线信道来统一不同收发机模块的设计，并提出训练一个紧凑的基础模型来估计无线信道的得分函数。3. 探索预训练大型语言模型（LLMs）在太赫兹超大规模MIMO系统中的潜在应用方向，包括基于LLM的估计、优化、搜索、网络管理和协议理解。", "result": "Not mentioned in abstract", "conclusion": "人工智能为解决太赫兹超大规模MIMO系统面临的挑战提供了有前景的解决方案，并提出了未来的研究方向和开放性问题。", "translation": "本研究探讨了人工智能（AI）在解决太赫兹超大规模多输入多输出（UM-MIMO）系统所带来的挑战方面的变革潜力。研究首先概述了太赫兹UM-MIMO系统的特性，并指出了收发机设计面临的三个主要挑战：计算复杂度、建模困难和测量受限。研究提出AI为解决这些挑战提供了有前景的方案。为此，提出了三个系统性的研究路线图，用于开发适用于太赫兹UM-MIMO系统的AI算法。第一个路线图是模型驱动的深度学习（DL），强调利用可用的领域知识的重要性，并提倡仅在既定信号处理或优化框架内，采用AI来增强瓶颈模块。讨论了四个基本步骤：算法框架、基础算法、损失函数设计和神经网络架构设计。第二个路线图提出了信道状态信息（CSI）基础模型，旨在通过关注其共享基础——无线信道，来统一不同收发机模块的设计。提出训练一个单一的紧凑型基础模型来估计无线信道的得分函数，该函数可作为设计各种收发机模块的通用先验。概述了四个基本步骤：通用框架、条件化、特定站点适应、CSI基础模型与模型驱动深度学习的联合设计。第三个路线图旨在探索将预训练大型语言模型（LLMs）应用于太赫兹UM-MIMO系统的潜在方向。设想了多种应用场景，包括基于LLM的估计、优化、搜索、网络管理和协议理解。最后，本研究强调了开放性问题和未来的研究方向。", "summary": "本研究探讨了人工智能在解决太赫兹超大规模MIMO系统挑战方面的变革潜力。针对计算复杂度、建模困难和测量受限等收发机设计挑战，提出了三个系统的AI算法研究路线图：一是模型驱动深度学习，利用领域知识增强现有框架；二是信道状态信息（CSI）基础模型，旨在通过统一无线信道设计来整合收发机模块；三是探索预训练大型语言模型（LLMs）在太赫兹UM-MIMO系统中的应用。最后，指出了开放性问题和未来的研究方向。", "keywords": "太赫兹, 超大规模MIMO, 人工智能, 深度学习, 基础模型", "comments": "本文创新性地提出了将人工智能，特别是基础模型和大型语言模型应用于太赫兹超大规模MIMO系统的全面研究路线图，为解决该领域面临的复杂挑战提供了前瞻性的视角和潜在方向。"}}
{"id": "2505.22404", "title": "Efficient Precision-Scalable Hardware for Microscaling (MX) Processing in Robotics Learning", "authors": ["Stef Cuyckens", "Xiaoling Yi", "Nitish Satya Murthy", "Chao Fang", "Marian Verhelst"], "categories": ["cs.AR", "cs.RO"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      To appear in 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED 2025)", "url": "http://arxiv.org/abs/2505.22404v2", "summary": "Autonomous robots require efficient on-device learning to adapt to new\nenvironments without cloud dependency. For this edge training, Microscaling\n(MX) data types offer a promising solution by combining integer and\nfloating-point representations with shared exponents, reducing energy\nconsumption while maintaining accuracy. However, the state-of-the-art\ncontinuous learning processor, namely Dacapo, faces limitations with its\nMXINT-only support and inefficient vector-based grouping during\nbackpropagation. In this paper, we present, to the best of our knowledge, the\nfirst work that addresses these limitations with two key innovations: (1) a\nprecision-scalable arithmetic unit that supports all six MX data types by\nexploiting sub-word parallelism and unified integer and floating-point\nprocessing; and (2) support for square shared exponent groups to enable\nefficient weight handling during backpropagation, removing storage redundancy\nand quantization overhead. We evaluate our design against Dacapo under\niso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology\nat 400MHz, reaching a 51% lower memory footprint, and 4x higher effective\ntraining throughput, while achieving comparable energy efficiency, enabling\nefficient robotics continual learning at the edge.", "comment": "To appear in 2025 IEEE/ACM International Symposium on Low Power\n  Electronics and Design (ISLPED 2025)", "pdf_url": "http://arxiv.org/pdf/2505.22404v2", "cate": "cs.AR", "date": "2025-05-28", "updated": "2025-07-23", "AI": {"title_translation": "机器人学习中微缩放 (MX) 处理的高效精度可伸缩硬件", "tldr": "本文提出了一种高效的精度可伸缩硬件，旨在解决机器人边缘学习中微缩放（MX）数据类型处理的局限性，通过支持所有MX数据类型并优化反向传播，显著提升了训练效率和降低了内存占用。", "motivation": "自主机器人需要在设备端进行高效学习以适应新环境，减少对云的依赖。微缩放（MX）数据类型是边缘训练的有前景解决方案，但现有最先进的连续学习处理器（如Dacapo）仅支持MXINT且反向传播效率低下。", "method": "本文提出了两项关键创新：1) 一个精度可伸缩的算术单元，通过利用子字并行和统一的整数与浮点处理，支持所有六种MX数据类型；2) 支持方形共享指数组，以在反向传播期间实现高效的权重处理，消除存储冗余和量化开销。", "result": "在TSMC 16nm FinFET技术下，400MHz频率，针对四种机器人工作负载，与Dacapo在等峰值吞吐量下进行评估，结果显示内存占用降低51%，有效训练吞吐量提高4倍，同时能效相当。", "conclusion": "本文提出的设计能够实现在边缘设备上高效的机器人持续学习。", "translation": "自主机器人需要高效的设备端学习以适应新环境，而无需依赖云。对于这种边缘训练，微缩放（MX）数据类型通过结合整数和浮点表示与共享指数，在保持精度的同时降低能耗，提供了一种有前景的解决方案。然而，现有最先进的连续学习处理器（即Dacapo）面临其仅支持MXINT以及反向传播期间低效的基于向量的分组的限制。在本文中，据我们所知，我们首次通过两项关键创新解决了这些限制：(1) 一个精度可伸缩的算术单元，通过利用子字并行和统一的整数和浮点处理，支持所有六种MX数据类型；以及(2) 支持方形共享指数组，以在反向传播期间实现高效的权重处理，消除存储冗余和量化开销。我们在TSMC 16nm FinFET技术下，以400MHz频率，在四种机器人工作负载上，在等峰值吞吐量下，将我们的设计与Dacapo进行评估，结果显示内存占用降低51%，有效训练吞吐量提高4倍，同时能效相当，从而实现了在边缘设备上高效的机器人持续学习。", "summary": "本文针对机器人边缘学习中MX数据类型处理的效率问题，提出了一种新型的精度可伸缩硬件设计。该设计通过支持所有六种MX数据类型的算术单元和高效的方形共享指数组处理，克服了现有处理器（如Dacapo）的局限性。实验结果表明，该设计在内存占用、训练吞吐量和能效方面均表现出色，为边缘机器人持续学习提供了高效的硬件支持。", "keywords": "微缩放, 机器人学习, 边缘计算, 硬件加速, 精度可伸缩", "comments": "这项工作创新性地解决了MX数据类型在硬件实现上的局限性，特别是对所有MX数据类型的全面支持以及针对反向传播的优化。其显著的内存占用减少和训练吞吐量提升对于资源受限的边缘机器人学习至关重要，具有重要的实际应用价值。"}}
{"id": "2507.17051", "title": "Exact closure for discrete large-eddy simulation", "authors": ["Syver Døving Agdestein", "Roel Verstappen", "Benjamin Sanderse"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      26 pages, 12 figures", "url": "http://arxiv.org/abs/2507.17051v1", "summary": "In this article we propose new discretization-consistent expressions for the\nsub-filter stress (SFS) tensor in discrete LES, where the filter is induced by\nthe discretization. We introduce a new two-grid filter that allows us to\nexactly compute the SFS tensor when DNS data is available. This new filter\nsatisfies a \"filter-swap\" property, such that filtering and finite differencing\ncan be interchanged and the resulting commutator expressions are of structural\nform (they can be written as the discrete divergence of an SFS tensor). For 1D\nconservation laws such as Burgers' equation, the resulting\ndiscretization-consistent SFS expression is markedly different from the\ncommonly used (discretization-inconsistent) expression $\\overline{u u} -\n\\bar{u} \\bar{u}$. For the 3D incompressible Navier-Stokes equations, we propose\nthree new two-grid filters, based on either volume- or surface-averaging, each\ninducing new discretization-consistent commutator expressions. We show that\nvolume-averaging is required to obtain a commutator expression of structural\nform. However, the resulting SFS tensor is shown to be non-symmetric. Based on\nDNS results, we show that the non-symmetric part of the SFS tensor plays an\nimportant role in the discrete LES equation. When the non-symmetric part is\nincluded, our SFS expressions give zero a-posteriori error in LES, while\nexisting SFS expressions give errors that increase over time. We propose to use\na class of non-symmetric tensor-basis closure models to approximate the new\nexact SFS expressions.", "comment": "26 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.17051v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "离散大涡模拟的精确闭合", "tldr": "本文提出了一种新的离散大涡模拟（LES）中离散化一致的子滤波器应力（SFS）张量表达式，通过引入一种新型双网格滤波器，实现了SFS张量的精确计算，并发现其非对称部分在LES中扮演重要角色，显著降低了误差。", "motivation": "为了解决离散大涡模拟（LES）中子滤波器应力（SFS）张量表达式的离散化一致性问题，并克服现有常用表达式的局限性，本文旨在提出新的离散化一致的SFS表达式。", "method": "本文提出了新的离散化一致的子滤波器应力（SFS）张量表达式。引入了一种新型双网格滤波器，该滤波器在DNS数据可用时能够精确计算SFS张量，并满足“滤波器交换”特性。针对一维守恒律和三维不可压缩纳维-斯托克斯方程，分别提出了相应的滤波器和表达式。特别是对于三维情况，提出了基于体积平均或表面平均的三种新型双网格滤波器，并研究了体积平均对结构形式的必要性以及SFS张量的非对称性。最后，提出使用非对称张量基闭合模型来近似新的精确SFS表达式。", "result": "本文提出的新型双网格滤波器能够精确计算SFS张量，并满足“滤波器交换”特性，使得交换子表达式呈结构形式。对于一维守恒律，所得的离散化一致SFS表达式与常用表达式显著不同。对于三维不可压缩纳维-斯托克斯方程，体积平均是获得结构形式交换子表达式所必需的，但导致SFS张量非对称。研究表明，SFS张量的非对称部分在离散LES方程中起着重要作用。当包含非对称部分时，本文的SFS表达式在LES中产生零后验误差，而现有SFS表达式的误差随时间增加。", "conclusion": "本文成功提出了离散大涡模拟中精确的离散化一致子滤波器应力（SFS）表达式，并通过DNS结果证明了SFS张量非对称部分在LES中的重要性，实现了零后验误差。这为使用非对称张量基闭合模型近似新的精确SFS表达式奠定了基础。", "translation": "在本文中，我们提出了离散大涡模拟（LES）中子滤波器应力（SFS）张量的新型离散化一致表达式，其中滤波器由离散化诱导。我们引入了一种新的双网格滤波器，当DNS数据可用时，该滤波器允许我们精确计算SFS张量。这种新滤波器满足“滤波器交换”特性，使得滤波和有限差分可以互换，并且所得的交换子表达式具有结构形式（它们可以写成SFS张量的离散散度）。对于诸如Burgers方程的一维守恒律，所得的离散化一致SFS表达式与常用的（离散化不一致的）表达式$\\overline{u u} - \\bar{u} \\bar{u}$显着不同。对于三维不可压缩纳维-斯托克斯方程，我们提出了三种新的双网格滤波器，基于体积平均或表面平均，每种都诱导新的离散化一致交换子表达式。我们表明，体积平均是获得结构形式交换子表达式所必需的。然而，结果表明SFS张量是非对称的。基于DNS结果，我们表明SFS张量的非对称部分在离散LES方程中扮演重要角色。当包含非对称部分时，我们的SFS表达式在LES中给出零后验误差，而现有SFS表达式的误差随时间增加。我们建议使用一类非对称张量基闭合模型来近似新的精确SFS表达式。", "summary": "本文提出了一种在离散大涡模拟（LES）中离散化一致的子滤波器应力（SFS）张量新表达式。通过引入一种新型双网格滤波器，该滤波器允许在DNS数据下精确计算SFS张量，并满足“滤波器交换”特性。研究表明，对于1D守恒律和3D纳维-斯托克斯方程，新表达式与传统方法显著不同，特别是3D情况下，体积平均是获得结构形式所必需的，且SFS张量是非对称的。重要的是，SFS张量的非对称部分在离散LES中至关重要，包含该部分后，LES的后验误差为零，显著优于现有方法的误差随时间增加。文章提出使用非对称张量基闭合模型来近似这些新的精确SFS表达式。", "keywords": "离散大涡模拟, 子滤波器应力, 双网格滤波器, 非对称张量, 精确闭合", "comments": "本文在离散大涡模拟领域取得了重要的理论突破，通过推导出精确的、离散化一致的子滤波器应力（SFS）张量表达式，解决了该领域长期存在的挑战。其创新之处在于提出了“滤波器交换”特性，并明确指出了SFS张量非对称部分的关键作用，这一发现对提高LES的准确性具有深远影响。实现“零后验误差”的声明尤其引人注目，预示着模拟精度可能达到一个新的水平，有望推动更可靠的流体动力学模拟。"}}
{"id": "2505.10764", "title": "SurgXBench: Explainable Vision-Language Model Benchmark for Surgery", "authors": ["Jiajun Cheng", "Xianwu Zhao", "Sainan Liu", "Xiaofan Yu", "Ravi Prakash", "Patrick J. Codd", "Jonathan Elliott Katz", "Shan Lin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10764v3", "summary": "Innovations in digital intelligence are transforming robotic surgery with\nmore informed decision-making. Real-time awareness of surgical instrument\npresence and actions (e.g., cutting tissue) is essential for such systems. Yet,\ndespite decades of research, most machine learning models for this task are\ntrained on small datasets and still struggle to generalize. Recently,\nvision-Language Models (VLMs) have brought transformative advances in reasoning\nacross visual and textual modalities. Their unprecedented generalization\ncapabilities suggest great potential for advancing intelligent robotic surgery.\nHowever, surgical VLMs remain under-explored, and existing models show limited\nperformance, highlighting the need for benchmark studies to assess their\ncapabilities and limitations and to inform future development. To this end, we\nbenchmark the zero-shot performance of several advanced VLMs on two public\nrobotic-assisted laparoscopic datasets for instrument and action\nclassification. Beyond standard evaluation, we integrate explainable AI to\nvisualize VLM attention and uncover causal explanations behind their\npredictions. This provides a previously underexplored perspective in this field\nfor evaluating the reliability of model predictions. We also propose several\nexplainability analysis-based metrics to complement standard evaluations. Our\nanalysis reveals that surgical VLMs, despite domain-specific training, often\nrely on weak contextual cues rather than clinically relevant visual evidence,\nhighlighting the need for stronger visual and reasoning supervision in surgical\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10764v3", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-07-23", "AI": {"title_translation": "SurgXBench：手术可解释视觉-语言模型基准", "tldr": "本文提出了SurgXBench，一个用于评估手术领域视觉-语言模型（VLMs）零样本性能的基准，并结合可解释AI揭示VLM在手术应用中依赖弱上下文线索而非临床相关视觉证据的问题，强调需要更强的视觉和推理监督。", "motivation": "机器人手术需要实时的手术器械存在和动作识别，但现有机器学习模型泛化能力差。视觉-语言模型（VLMs）具有强大的泛化潜力，但在手术领域探索不足且性能有限，因此需要基准研究来评估其能力和局限性。", "method": "1. 在两个公共机器人辅助腹腔镜数据集上，对多个先进VLM的零样本性能进行基准测试，用于器械和动作分类。2. 整合可解释AI（XAI）来可视化VLM的注意力并揭示预测背后的因果解释。3. 提出基于可解释性分析的新指标来补充标准评估。", "result": "分析表明，尽管经过领域特定训练，手术VLM通常依赖弱上下文线索而非临床相关的视觉证据。", "conclusion": "手术VLM需要更强的视觉和推理监督，以避免依赖弱上下文线索，从而提高其在临床应用中的可靠性。", "translation": "数字智能的创新正在通过更明智的决策改变机器人手术。实时感知手术器械的存在和动作（例如切割组织）对于此类系统至关重要。然而，尽管经过数十年的研究，大多数用于此任务的机器学习模型都是在小型数据集上训练的，并且仍然难以泛化。最近，视觉-语言模型（VLMs）在跨视觉和文本模态的推理方面带来了变革性的进展。它们前所未有的泛化能力预示着在推进智能机器人手术方面具有巨大潜力。然而，手术VLM仍未得到充分探索，现有模型表现出有限的性能，这突出表明需要基准研究来评估其能力和局限性，并为未来的发展提供信息。为此，我们对多个先进VLM在两个公共机器人辅助腹腔镜数据集上进行器械和动作分类的零样本性能进行了基准测试。除了标准评估之外，我们还整合了可解释AI来可视化VLM的注意力并揭示其预测背后的因果解释。这为该领域评估模型预测可靠性提供了一个以前未充分探索的视角。我们还提出了几种基于可解释性分析的指标来补充标准评估。我们的分析表明，手术VLM尽管经过领域特定训练，通常依赖弱上下文线索而非临床相关的视觉证据，这突出表明手术应用中需要更强的视觉和推理监督。", "summary": "本文提出了SurgXBench，一个用于评估手术领域视觉-语言模型（VLMs）零样本性能的基准。研究通过在两个公共数据集上测试先进VLM的器械和动作分类能力，并结合可解释AI来可视化VLM的注意力并揭示其预测的因果解释。分析发现，手术VLM在预测时常依赖弱上下文线索而非实际的临床视觉证据，这强调了在手术应用中需要更强的视觉和推理监督。", "keywords": "手术机器人, 视觉-语言模型, 基准测试, 可解释AI, 零样本学习", "comments": "本文的创新点在于首次提出了针对手术领域的视觉-语言模型基准SurgXBench，并创造性地将可解释AI引入对VLM在手术任务中表现的评估，这不仅提供了模型性能的量化数据，更深入揭示了模型决策的内在机制，对于理解和改进手术VLM的可靠性具有重要意义。其发现模型依赖弱上下文而非临床证据的局限性，为未来研究指明了方向。"}}
{"id": "2507.17180", "title": "A Privacy-Preserving Data Collection Method for Diversified Statistical Analysis", "authors": ["Hao Jiang", "Quan Zhou", "Dongdong Zhao", "Shangshang Yang", "Wenjian Luo", "Xingyi Zhang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17180v1", "summary": "Data perturbation-based privacy-preserving methods have been widely adopted\nin various scenarios due to their efficiency and the elimination of the need\nfor a trusted third party. However, these methods primarily focus on individual\nstatistical indicators, neglecting the overall quality of the collected data\nfrom a distributional perspective. Consequently, they often fall short of\nmeeting the diverse statistical analysis requirements encountered in practical\ndata analysis. As a promising sensitive data perturbation method, negative\nsurvey methods is able to complete the task of collecting sensitive information\ndistribution while protecting personal privacy. Yet, existing negative survey\nmethods are primarily designed for discrete sensitive information and are\ninadequate for real-valued data distributions. To bridge this gap, this paper\nproposes a novel real-value negative survey model, termed RVNS, for the first\ntime in the field of real-value sensitive information collection. The RVNS\nmodel exempts users from the necessity of discretizing their data and only\nrequires them to sample a set of data from a range that deviates from their\nactual sensitive details, thereby preserving the privacy of their genuine\ninformation. Moreover, to accurately capture the distribution of sensitive\ninformation, an optimization problem is formulated, and a novel approach is\nemployed to solve it. Rigorous theoretical analysis demonstrates that the RVNS\nmodel conforms to the differential privacy model, ensuring robust privacy\npreservation. Comprehensive experiments conducted on both synthetic and\nreal-world datasets further validate the efficacy of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17180v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种用于多样化统计分析的隐私保护数据收集方法", "tldr": "本文提出了一种名为RVNS的新型实值负面调查模型，首次解决了现有隐私保护数据扰动方法和负面调查方法在处理实值敏感数据分布时的不足，并通过理论分析和实验验证了其在保护隐私的同时准确捕获数据分布的有效性。", "motivation": "现有基于数据扰动（Data perturbation-based）的隐私保护方法主要关注个体统计指标，忽视数据分布的整体质量，难以满足多样化的统计分析需求。同时，现有的负面调查（negative survey）方法主要针对离散敏感信息，不适用于实值数据分布。", "method": "本文首次提出了一种新颖的实值负面调查模型（real-value negative survey model），命名为RVNS。RVNS模型允许用户从偏离其实际敏感信息的范围内采样一组数据来保护隐私，而无需对数据进行离散化。为了准确捕获敏感信息的分布，该方法还提出了一个优化问题并采用了一种新颖的方法来解决它。", "result": "严格的理论分析表明RVNS模型符合差分隐私（differential privacy）模型，确保了强大的隐私保护。在合成数据集和真实世界数据集上进行的全面实验进一步验证了所提出方法的有效性。", "conclusion": "RVNS模型首次为实值敏感信息收集领域提供了一种新颖的隐私保护数据收集方法，能够准确捕获敏感信息分布，并符合差分隐私模型，有效解决了现有方法的局限性。", "translation": "基于数据扰动（Data perturbation-based）的隐私保护方法因其高效性以及无需可信第三方而被广泛应用于各种场景。然而，这些方法主要关注个体统计指标，从分布角度忽视了所收集数据的整体质量。因此，它们往往无法满足实际数据分析中遇到的多样化统计分析需求。作为一种有前景的敏感数据扰动方法，负面调查（negative survey）方法能够在保护个人隐私的同时完成敏感信息分布的收集任务。然而，现有的负面调查方法主要针对离散敏感信息，不适用于实值数据分布。为了弥补这一空白，本文首次在实值敏感信息收集领域提出了一种新颖的实值负面调查模型，命名为RVNS。RVNS模型免除了用户离散化数据的必要性，仅要求他们从偏离其实际敏感细节的范围内采样一组数据，从而保护了其真实信息的隐私。此外，为了准确捕获敏感信息的分布，本文提出了一个优化问题，并采用了一种新颖的方法来解决它。严格的理论分析表明RVNS模型符合差分隐私（differential privacy）模型，确保了强大的隐私保护。在合成数据集和真实世界数据集上进行的全面实验进一步验证了所提出方法的有效性。", "summary": "本文提出了一种名为RVNS的新型实值负面调查模型，旨在解决现有隐私保护数据扰动方法在处理多样化统计分析和实值敏感信息收集方面的不足。RVNS允许用户通过从偏离实际敏感信息的范围中采样数据来保护隐私，并提出优化问题以准确捕获数据分布。理论分析证明了RVNS符合差分隐私，并在合成和真实数据集上的实验验证了其有效性。", "keywords": "隐私保护, 数据收集, 负面调查, 实值数据, 差分隐私", "comments": "该论文的创新点在于首次提出了用于实值敏感信息收集的负面调查模型RVNS，填补了现有负面调查方法主要针对离散数据的空白。其重要性在于提供了一种在保护用户隐私的同时，能够更准确地捕获敏感数据整体分布的方法，这对于需要进行多样化统计分析的实际应用至关重要。RVNS模型符合差分隐私，增强了其隐私保护的鲁棒性。"}}
{"id": "2507.17626", "title": "Quotegraph: A Social Network Extracted from Millions of News Quotations", "authors": ["Marko Čuljak", "Robert West", "Andreas Spitz", "Akhil Arora"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17626v1", "summary": "We introduce Quotegraph, a novel large-scale social network derived from\nspeaker-attributed quotations in English news articles published between 2008\nand 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million\ndirected edges, pointing from speakers to persons they mention. The nodes are\nlinked to their corresponding items in Wikidata, thereby endowing the dataset\nwith detailed biographic entity information, including nationality, gender, and\npolitical affiliation. Being derived from Quotebank, a massive corpus of\nquotations, relations in Quotegraph are additionally enriched with the\ninformation about the context in which they are featured. Each part of the\nnetwork construction pipeline is language agnostic, enabling the construction\nof similar datasets based on non-English news corpora. We believe Quotegraph is\na compelling resource for computational social scientists, complementary to\nonline social networks, with the potential to yield novel insights into the\nbehavior of public figures and how it is captured in the news.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17626v1", "cate": "cs.SI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Quotegraph：一个从数百万新闻引用中提取的社交网络", "tldr": "论文介绍了Quotegraph，一个从2008-2020年英文新闻引语中提取的大规模社交网络，包含52.8万个节点和863万条边，并与Wikidata关联，为计算社会科学家提供新资源。", "motivation": "提供一个从新闻引语中提取的、与在线社交网络互补的大规模社交网络，以期对公众人物的行为及其在新闻中的体现产生新的见解。", "method": "Quotegraph通过分析2008年至2020年间英文新闻文章中带有说话者属性的引语构建。它源自Quotebank，并将节点链接到Wikidata以获取详细的传记实体信息，同时包含引语的上下文信息。网络构建流程具有语言无关性。", "result": "Quotegraph包含52.8万个独特节点和863万条有向边（从说话者指向他们提及的人）。节点与Wikidata项关联，提供国籍、性别、政治立场等详细传记信息。关系还通过引语上下文信息得到丰富。", "conclusion": "Quotegraph被认为是一个对计算社会科学家极具吸引力的资源，与在线社交网络互补，有望为公众人物行为及其在新闻中的体现提供新的见解。", "translation": "我们介绍了Quotegraph，这是一个从2008年至2020年间发布的英文新闻文章中带有说话者属性的引语中提取的新型大规模社交网络。Quotegraph由52.8万个独特节点和863万条有向边组成，边从说话者指向他们提及的人。这些节点与Wikidata中的对应项链接，从而为数据集提供了详细的传记实体信息，包括国籍、性别和政治立场。由于Quotegraph源自一个庞大的引语语料库Quotebank，因此其中的关系还额外丰富了它们出现的上下文信息。网络构建管道的每个部分都与语言无关，这使得基于非英文新闻语料库构建类似数据集成为可能。我们相信Quotegraph是计算社会科学家一个引人注目的资源，与在线社交网络互补，有潜力对公众人物的行为以及新闻如何捕捉这些行为产生新的见解。", "summary": "本文介绍了Quotegraph，一个从2008年至2020年英文新闻引语中构建的大规模社交网络。该网络包含52.8万个节点和863万条有向边，节点与Wikidata关联以提供丰富的实体信息，关系则通过上下文信息得到增强。Quotegraph的构建流程是语言无关的，被视为计算社会科学领域的重要新资源，有望对公众人物行为及其新闻表现提供独特见解。", "keywords": "Quotegraph, 社交网络, 新闻引语, Wikidata", "comments": "Quotegraph的创新之处在于其从新闻引语中而非传统的在线互动数据中构建社交网络，并与Wikidata的结合极大地丰富了节点信息。其大规模和语言无关的特性也使其具有广泛的应用潜力，为计算社会科学研究提供了一个独特且互补的数据集。"}}
{"id": "2507.17354", "title": "Realisability and Complementability of Multiparty Session Types", "authors": ["Cinzia Di Giusto", "Etienne Lozes", "Pascal Urso"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17354v1", "summary": "Multiparty session types (MPST) are a type-based approach for specifying\nmessage-passing distributed systems. They rely on the notion of global type\nspecifying the global behaviour and local types, which are the projections of\nthe global behaviour onto each local participant. An essential property of\nglobal types is realisability, i.e., whether the composition of the local\nbehaviours conforms to those specified by the global type. We explore how\nrealisability of MPST relates to their complementability, i.e., whether there\nexists a global type that describes the complementary behaviour of the original\nglobal type. First, we show that if a global type is realisable with p2p\ncommunications, then it is realisable with synchronous communications. Second,\nwe show that if a global type is realisable in the synchronous model, then it\nis complementable, in the sense that there exists a global type that describes\nthe complementary behaviour of the original global type. Third, we give an\nalgorithm to decide whether a complementable global type, given with an\nexplicit complement, is realisable in p2p. The algorithm is PSPACE in the size\nof the global type and its complement. As a side contribution, we propose a\ncomplementation construction for global types with sender-driven choice with a\nlinear blowup in the size of the global type.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17354v1", "cate": "cs.FL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多方会话类型中的可实现性与互补性", "tldr": "该论文探讨了多方会话类型中可实现性与互补性之间的关系，证明了点对点（p2p）通信下的可实现性蕴含同步通信下的可实现性，同步通信下的可实现性蕴含互补性，并提出了一个用于判断可互补全局类型在点对点通信中是否可实现性的算法。", "motivation": "多方会话类型（MPST）是指定消息传递分布式系统的一种基于类型的方法。全局类型的可实现性是一个基本属性，即局部行为的组合是否符合全局类型所指定的行为。本研究旨在探索MPST的可实现性与它们的互补性之间的关系，即是否存在描述原始全局类型互补行为的全局类型。", "method": "本文首先理论上分析了全局类型在点对点（p2p）通信和同步通信模型下的可实现性与互补性之间的关系。其次，提出了一种算法，用于判断给定明确互补的、可互补的全局类型在点对点（p2p）通信中是否可实现。此外，还提出了一种针对带有发送方驱动选择的全局类型的互补构造方法。", "result": "1. 如果一个全局类型在点对点（p2p）通信下是可实现的，那么它在同步通信下也是可实现的。2. 如果一个全局类型在同步模型下是可实现的，那么它是可互补的，即存在一个描述原始全局类型互补行为的全局类型。3. 提出了一个算法来判断一个具有明确互补的、可互补的全局类型在点对点（p2p）通信中是否可实现，该算法在全局类型及其互补的大小上是P空间复杂度。4. 提出了一个带有发送方驱动选择的全局类型的互补构造方法，其大小与全局类型成线性增长。", "conclusion": "本论文建立了多方会话类型中可实现性与互补性之间的理论联系，揭示了它们在不同通信模型下的相互关系，并提供了判断可实现性的实用算法和互补构造方法，对分布式系统的设计和分析具有重要意义。", "translation": "多方会话类型（MPST）是一种基于类型的方法，用于指定消息传递分布式系统。它们依赖于全局类型（指定全局行为）和局部类型（全局行为在每个局部参与者上的投影）的概念。全局类型的一个基本属性是可实现性，即局部行为的组合是否符合全局类型所指定的行为。我们探讨了MPST的可实现性与其互补性之间的关系，即是否存在一个描述原始全局类型互补行为的全局类型。首先，我们展示了如果一个全局类型在点对点（p2p）通信下是可实现的，那么它在同步通信下也是可实现的。其次，我们展示了如果一个全局类型在同步模型下是可实现的，那么它是可互补的，即存在一个描述原始全局类型互补行为的全局类型。第三，我们给出了一个算法，用于判断一个具有明确互补的、可互补的全局类型在点对点（p2p）通信中是否可实现。该算法在全局类型及其互补的大小上是P空间复杂度。作为一项额外贡献，我们提出了一种针对带有发送方驱动选择的全局类型的互补构造方法，其大小与全局类型成线性增长。", "summary": "本论文深入探讨了多方会话类型（MPST）中可实现性与互补性这两个核心属性之间的关系。可实现性确保了局部行为与全局规范的一致性，而互补性则指是否存在描述互补行为的全局类型。研究证明，点对点（p2p）通信下的可实现性蕴含同步通信下的可实现性；同步模型下的可实现性则蕴含互补性。此外，文章还提出了一个P空间复杂度的算法，用于判断给定明确互补的、可互补的全局类型在点对点通信中是否可实现，并提供了一种线性膨胀的、带有发送方驱动选择的全局类型互补构造方法。", "keywords": "多方会话类型, 可实现性, 互补性, 分布式系统, 类型理论", "comments": "该论文在多方会话类型（MPST）领域做出了重要的理论贡献，通过形式化地连接了可实现性与互补性这两个关键属性，加深了对其基本性质的理解。关于不同通信模型（点对点与同步）之间可实现性蕴含关系的发现富有洞察力。此外，所提出的判断可实现性的实用算法和高效的互补构造方法，对于使用MPST进行分布式系统设计和分析具有实际价值。"}}
{"id": "2507.17476", "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17476v1", "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17476v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MultiNRC：一个针对大型语言模型的挑战性原生多语言推理评估基准", "tldr": "引入了MultiNRC，一个用于评估LLM在法语、西班牙语和中文中原生多语言推理能力的基准，发现LLM在此方面表现不佳。", "motivation": "尽管大型语言模型在英语推理方面取得了显著进展，但现有的大多数多语言推理基准都是通过翻译英语基准构建的，这导致它们偏向英语语境，限制了对LLM在多样语言和文化背景下多语言推理能力的全面评估。", "method": "本文引入了MultiNRC，这是一个包含1000多个由法语、西班牙语和中文母语者编写的原生、语言和文化相关推理问题的基准。MultiNRC涵盖语言特定语言推理、文字游戏与谜语、文化/传统推理以及与文化相关的数学推理。对于部分问题，还提供了由母语者手动翻译的英语等效版本。研究者系统地评估了14个主流LLM在MultiNRC及其英语等效集上的表现。", "result": "1. 当前LLM在原生多语言推理方面表现不佳，在MultiNRC上的得分均未超过50%。2. LLM在处理语言、文化和逻辑推理任务时表现出不同的优缺点。3. 大多数模型在英语数学推理方面的表现明显优于原始语言（+10%），表明在文化相关知识方面仍然存在持续的挑战。", "conclusion": "当前的大型语言模型在原生多语言推理方面，特别是在处理文化相关知识时，仍存在显著不足。", "translation": "尽管最近的大型语言模型（LLM）在英语推理基准上显示出快速改进，但对这些LLM在不同语言和文化背景下的多语言推理能力的评估仍然有限。现有的多语言推理基准通常通过翻译现有英语推理基准来构建，这使得这些基准偏向于英语语言/文化语境下的推理问题。在这项工作中，我们引入了多语言原生推理挑战（MultiNRC），这是一个旨在评估LLM在法语、西班牙语和中文中由母语者编写的1000多个原生、语言和文化相关推理问题上的基准。MultiNRC涵盖四个核心推理类别：语言特定语言推理、文字游戏与谜语、文化/传统推理以及与文化相关的数学推理。对于文化/传统推理和与文化相关的数学推理，我们还提供了由精通英语的母语者手动翻译的多语言问题的英语等效翻译。这组英语等效问题可以直接比较LLM在其他语言与英语在相同推理问题上的推理能力。我们系统地评估了覆盖大多数LLM家族的当前14个领先LLM在MultiNRC及其英语等效集上的表现。结果表明：（1）当前LLM在原生多语言推理方面仍然不擅长，在MultiNRC上的得分均未超过50%；（2）LLM在处理语言、文化和逻辑推理任务时表现出明显的优点和缺点；（3）大多数模型在英语数学推理方面的表现明显优于原始语言（+10%），这表明在文化相关知识方面仍然存在持续的挑战。", "summary": "本文介绍了MultiNRC，一个用于评估大型语言模型在法语、西班牙语和中文中原生多语言推理能力的基准。该基准包含由母语者编写的语言、文化和数学推理问题，并提供部分英语翻译用于比较。研究结果表明，当前LLM在原生多语言推理方面表现普遍不佳，尤其在处理文化相关知识时，且在英语数学推理上表现优于其他语言。", "keywords": "多语言推理, 大型语言模型, 评估基准, 文化相关推理, MultiNRC", "comments": "MultiNRC创新性地提供了原生多语言推理问题，而非简单的翻译，这对于全面评估LLM的跨文化和跨语言推理能力至关重要。研究结果揭示了LLM在非英语、文化相关推理方面的显著局限性，特别是在数学推理中，英语表现优于其他语言，这指出了未来LLM在真正实现多语言智能方面需要克服的关键挑战。"}}
{"id": "2507.17326", "title": "Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task", "authors": ["Milena Davudova", "Ziyuan Cai", "Valentina Giunchiglia", "Dragos C. Gruia", "Giulia Sanguedolce", "Adam Hampshire", "Fatemeh Geranmayeh"], "categories": ["cs.SD"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17326v1", "summary": "Detailed assessment of language impairment following stroke remains a\ncognitively complex and clinician-intensive task, limiting timely and scalable\ndiagnosis. Automatic Speech Recognition (ASR) foundation models offer a\npromising pathway to augment human evaluation through intelligent systems, but\ntheir effectiveness in the context of speech and language impairment remains\nuncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR\nfoundation model, can be applied to transcribe and analyze speech from patients\nwith stroke during a commonly used picture-naming task. We assess both verbatim\ntranscription accuracy and the model's ability to support downstream prediction\nof language function, which has major implications for outcomes after stroke.\nOur results show that the baseline Whisper model performs poorly on single-word\nspeech utterances. Nevertheless, fine-tuning Whisper significantly improves\ntranscription accuracy (reducing Word Error Rate by 87.72% in healthy speech\nand 71.22% in speech from patients). Further, learned representations from the\nmodel enable accurate prediction of speech quality (average F1 Macro of 0.74\nfor healthy, 0.75 for patients). However, evaluations on an unseen (TORGO)\ndataset reveal limited generalizability, highlighting the inability of Whisper\nto perform zero-shot transcription of single-word utterances on out-of-domain\nclinical speech and emphasizing the need to adapt models to specific clinical\npopulations. While challenges remain in cross-domain generalization, these\nfindings highlight the potential of foundation models, when appropriately\nfine-tuned, to advance automated speech and language assessment and\nrehabilitation for stroke-related impairments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17326v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Whisper在临床实践中的应用：命名任务中卒中后言语评估", "tldr": "本研究评估了先进的ASR模型Whisper在卒中患者言语评估中的应用，发现微调后在转录准确性和语言功能预测方面表现良好，但跨域泛化能力有限。", "motivation": "卒中后语言障碍的详细评估复杂且耗时，限制了及时和可扩展的诊断。ASR基础模型有望通过智能系统增强人工评估，但其在言语和语言障碍背景下的有效性尚不确定。", "method": "本研究评估了Whisper模型在转录和分析卒中患者在常用图片命名任务中的言语表现。评估内容包括逐字转录准确性以及模型支持下游语言功能预测的能力。", "result": "基线Whisper模型在单词言语中表现不佳。微调后的Whisper显著提高了转录准确性（健康言语的词错率降低87.72%，患者言语降低71.22%）。模型学习到的表示能够准确预测言语质量（健康言语平均F1宏为0.74，患者言语为0.75）。然而，在未见过的数据集（TORGO）上的评估显示泛化能力有限，Whisper无法对域外临床言语进行零样本单词转录。", "conclusion": "尽管跨域泛化仍面临挑战，但这些发现强调了基础模型在经过适当微调后，在推进卒中相关障碍的自动化言语和语言评估及康复方面的潜力。", "translation": "卒中后语言障碍的详细评估仍然是一项认知复杂且临床医生密集型任务，限制了及时和可扩展的诊断。自动语音识别（ASR）基础模型为通过智能系统增强人工评估提供了一条有前景的途径，但它们在言语和语言障碍背景下的有效性仍不确定。在本研究中，我们评估了最先进的ASR基础模型Whisper是否可以应用于转录和分析卒中患者在常用图片命名任务中的言语。我们评估了逐字转录准确性以及模型支持下游语言功能预测的能力，这对于卒中后的结果具有重要意义。我们的结果显示，基线Whisper模型在单词言语中表现不佳。然而，微调Whisper显著提高了转录准确性（健康言语的词错率降低87.72%，患者言语降低71.22%）。此外，从模型中学习到的表示能够准确预测言语质量（健康人平均F1宏为0.74，患者为0.75）。然而，对未见过的数据集（TORGO）的评估显示泛化能力有限，突出了Whisper无法对域外临床言语进行零样本单词转录，并强调了模型需要适应特定临床人群。尽管跨域泛化仍面临挑战，但这些发现强调了基础模型在经过适当微调后，在推进卒中相关障碍的自动化言语和语言评估及康复方面的潜力。", "summary": "本研究探讨了先进的ASR模型Whisper在卒中患者言语评估中的应用。研究发现，尽管基线Whisper模型在单词转录上表现不佳，但经过微调后，其在健康言语和患者言语的转录准确性上均显著提高，并能有效预测言语质量。然而，模型在未见过的数据集上表现出有限的跨域泛化能力，提示未来需要针对特定临床人群进行模型适应性研究。研究结果表明，经过适当微调的基础模型在自动化卒中相关言语和语言评估及康复方面具有巨大潜力。", "keywords": "Whisper, 卒中, 言语评估, 自动语音识别, 语言障碍", "comments": "该研究创新性地将先进的ASR基础模型Whisper应用于临床卒中患者的言语评估，展示了AI在辅助医疗诊断和康复方面的潜力。研究不仅关注转录准确性，还深入探讨了模型支持下游语言功能预测的能力，这对于临床实践具有重要意义。然而，论文也坦诚指出了模型在跨域泛化方面的局限性，强调了未来需要针对特定临床人群进行模型适应性训练，这为后续研究指明了方向。"}}
{"id": "2507.04224", "title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services", "authors": ["Haining Wang", "Jason Clark", "Yueru Yan", "Star Bradley", "Ruiyang Chen", "Yiqiong Zhang", "Hengyi Fu", "Zuoyu Tian"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04224v2", "summary": "As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04224v2", "cate": "cs.CL", "date": "2025-07-06", "updated": "2025-07-23", "AI": {"title_translation": "学术图书馆参考服务中大型语言模型的公平性评估", "tldr": "研究评估了大型语言模型在学术图书馆参考服务中对不同用户身份的响应公平性，发现种族/民族差异不明显，对女性存在轻微刻板印象偏见，并能根据机构角色进行适当调整。", "motivation": "图书馆在探索将大型语言模型用于虚拟参考服务时，关注LLMs是否能公平服务所有用户，因为LLMs可能重现训练数据中的社会偏见，从而损害图书馆对公平服务的承诺。", "method": "通过提示六个最先进的LLMs，模拟不同性别、种族/民族和机构角色的用户，评估LLMs是否会区别对待这些用户身份。", "result": "未发现LLMs在种族或民族方面有差异化响应的证据；只有一个模型对女性表现出轻微的刻板印象偏见；LLMs通过使用与正式程度、礼貌和领域特定词汇相关的语言选择，对机构角色表现出细致的适应，这反映了专业规范而非歧视性待遇。", "conclusion": "当前的LLMs在学术图书馆参考服务中，展现出支持公平且符合语境的交流的良好准备程度。", "translation": "随着图书馆探索将大型语言模型（LLMs）用于虚拟参考服务，一个关键问题随之产生：LLMs能否公平地服务所有用户，无论其人口统计学特征或社会地位如何？尽管它们为可扩展的支持提供了巨大潜力，但LLMs也可能复制其训练数据中嵌入的社会偏见，从而危及图书馆对公平服务承诺的完整性。为了解决这一担忧，我们通过提示六个最先进的LLMs来评估它们是否会根据用户身份（性别、种族/民族和机构角色）区分响应。我们发现没有证据表明LLMs在种族或民族方面存在差异化，只有一个模型对女性表现出轻微的刻板印象偏见。LLMs通过使用与正式程度、礼貌和领域特定词汇相关的语言选择，对机构角色表现出细致的适应，这反映了专业规范而非歧视性待遇。这些发现表明，当前的LLMs在支持学术图书馆参考服务中的公平和符合语境的交流方面，展现出令人鼓舞的准备程度。", "summary": "本研究评估了大型语言模型（LLMs）在学术图书馆虚拟参考服务中对用户公平性的表现。通过测试六个LLMs对不同性别、种族/民族和机构角色的用户查询的响应，研究发现LLMs在种族/民族方面未显示出差异，仅在一个模型中对女性存在轻微的刻板印象偏见。同时，LLMs能根据用户的机构角色进行适当的语言调整，以符合专业规范。研究结果表明，LLMs在提供公平和语境适宜的图书馆参考服务方面具有良好潜力。", "keywords": "大型语言模型, 公平性, 图书馆服务, 偏见, 用户身份", "comments": "这项研究通过实证评估，直接回应了LLMs在公平性方面的重要担忧，特别是在公共服务领域如图书馆的应用。其创新之处在于针对具体的用户身份维度进行测试，并区分了专业规范性调整与歧视性偏见。研究结果为LLMs在学术图书馆服务中的部署提供了积极的初步证据，但其局限性在于仅在一个特定场景下进行了评估，且仅发现“轻微”偏见，未来需要更广泛、更深入的研究来全面理解LLMs的公平性。"}}
{"id": "2507.17199", "title": "Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG", "authors": ["Ruoyang Rykie Guo"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17199v1", "summary": "LLM-powered search services have driven data integration as a significant\ntrend. However, this trend's progress is fundamentally hindered, despite the\nfact that combining individual knowledge can significantly improve the\nrelevance and quality of responses in specialized queries and make AI more\nprofessional at providing services. Two key bottlenecks are private data\nrepositories' locality constraints and the need to maintain compatibility with\nmainstream search techniques, particularly Hierarchical Navigable Small World\n(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a\nsecure and privacy-preserving aggregated approximate nearest neighbor search\n(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing\nprimitive. A sharable bitgraph structure is constructed and extended to support\nsearches and dynamical insertions over shared data without compromising the\nunderlying graph topology. The approach reduces the complexity of a search from\n$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when\norganizing graphs in the identical HNSW manner.\n  On the theoretical front, we explore a novel security analytical framework\nthat incorporates privacy analysis via reductions. The proposed\nleakage-guessing proof system is built upon an entirely different interactive\ngame that is independent of existing coin-toss game design. Rather than being\npurely theoretical, this system is rooted in existing proof systems but goes\nbeyond them to specifically address leakage concerns and standardize leakage\nanalysis -- one of the most critical security challenges with AI's rapid\ndevelopment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17199v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "阈值保护可搜索共享：协作RAG中隐私保护聚合ANN搜索", "tldr": "提出一种阈值保护的可搜索共享方案SP-A$^2$NN，用于协作RAG中的隐私保护聚合ANN搜索，兼容HNSW，并降低了搜索复杂度，同时引入了新的泄漏分析框架。", "motivation": "LLM驱动的搜索服务面临私有数据存储库的局部性限制以及与主流搜索技术（特别是HNSW索引）兼容性的挑战，这阻碍了数据集成趋势的发展，尽管结合个体知识能显著提高专业查询的响应质量和相关性。", "method": "开发了一种安全且隐私保护的聚合近似最近邻搜索（SP-A$^2$NN），在阈值可搜索共享原语下兼容HNSW。构建并扩展了一种可共享的位图结构，以支持在共享数据上的搜索和动态插入，而不损害底层图拓扑。同时，探索了一种新颖的、基于归约的隐私分析安全分析框架，并构建了一个独立于现有抛硬币博弈设计的泄漏猜测证明系统。", "result": "与以相同HNSW方式组织图的朴素（无向）图共享方法相比，将搜索复杂度从O(n^2)降低到O(n)。", "conclusion": "该工作通过提出解决泄漏问题的证明系统，解决了AI快速发展中最关键的安全挑战之一，即标准化泄漏分析。", "translation": "LLM驱动的搜索服务已推动数据集成成为一个重要趋势。然而，尽管结合个体知识可以显著提高专业查询中响应的相关性和质量，并使人工智能在提供服务方面更加专业，但这一趋势的进展却受到了根本性阻碍。两个关键瓶颈是私有数据存储库的局部性限制以及需要保持与主流搜索技术（特别是用于高维向量空间的层次可导航小世界（HNSW）索引）的兼容性。在这项工作中，我们开发了一种安全且隐私保护的聚合近似最近邻搜索（SP-A$^2$NN），在基于阈值的可搜索共享原语下兼容HNSW。构建并扩展了一种可共享的位图结构，以支持在共享数据上的搜索和动态插入，而不损害底层图拓扑。与以相同HNSW方式组织图的朴素（无向）图共享方法相比，该方法将搜索复杂度从O(n^2)降低到O(n)。在理论方面，我们探索了一种新颖的安全分析框架，该框架通过归约整合了隐私分析。所提出的泄漏猜测证明系统建立在一个完全不同于现有抛硬币博弈设计的交互式博弈之上。该系统并非纯粹的理论，而是植根于现有证明系统，但超越它们以专门解决泄漏问题并标准化泄漏分析——这是AI快速发展中最关键的安全挑战之一。", "summary": "该论文提出了一种名为SP-A$^2$NN的安全隐私保护聚合近似最近邻搜索方案，旨在解决LLM驱动搜索服务中私有数据局部性限制和HNSW兼容性问题。通过构建可共享的位图结构并在阈值可搜索共享原语下运行，该方案实现了在共享数据上的高效搜索和动态插入，并将搜索复杂度从O(n^2)降低到O(n)。此外，论文还引入了一个新颖的泄漏猜测证明系统，为AI安全领域的泄漏分析提供了一个独立于现有方法的标准化框架。", "keywords": "阈值保护共享, 隐私保护ANN, 协作RAG, HNSW, 泄漏分析", "comments": "该论文创新性地将阈值保护的可搜索共享与HNSW兼容的聚合ANN搜索相结合，显著降低了搜索复杂度。其提出的泄漏猜测证明系统为AI安全领域的泄漏分析提供了一个新颖且实用的框架，具有重要的理论和实践意义。"}}
{"id": "2502.02170", "title": "Graph Neural Networks for O-RAN Mobility Management: A Link Prediction Approach", "authors": ["Ana Gonzalez Bermudez", "Miquel Farreras", "Milan Groshev", "José Antonio Trujillo", "Isabel de la Bandera", "Raquel Barco"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, 1 table. Submitted to IEEE Vehicular Technology Magazine, Special Issue on \"AI for 6G O-RAN Intelligent, Cost-Efficient and Secure Automation\". Version after Major Revision", "url": "http://arxiv.org/abs/2502.02170v2", "summary": "Mobility performance has been a key focus in cellular networks up to 5G. To\nenhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO)\nand Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these\nreactive HO strategies address the trade-off between HO failures (HOF) and\nping-pong effects, they often result in inefficient radio resource utilization\ndue to additional HO preparations. To overcome these challenges, this article\nproposes a proactive HO framework for mobility management in O-RAN, leveraging\nuser-cell link predictions to identify the optimal target cell for HO. We\nexplore various categories of Graph Neural Networks (GNNs) for link prediction\nand analyze the complexity of applying them to the mobility management domain.\nTwo GNN models are compared using a real-world dataset, with experimental\nresults demonstrating their ability to capture the dynamic and graph-structured\nnature of cellular networks. Finally, we present key insights from our study\nand outline future steps to enable the integration of GNN-based link prediction\nfor mobility management in O-RAN networks.", "comment": "7 pages, 5 figures, 1 table. Submitted to IEEE Vehicular Technology\n  Magazine, Special Issue on \"AI for 6G O-RAN Intelligent, Cost-Efficient and\n  Secure Automation\". Version after Major Revision", "pdf_url": "http://arxiv.org/pdf/2502.02170v2", "cate": "cs.NI", "date": "2025-02-04", "updated": "2025-07-23", "AI": {"title_translation": "用于O-RAN移动性管理的图神经网络：一种链路预测方法", "tldr": "本文提出了一种利用图神经网络进行链路预测的O-RAN主动切换框架，以提高移动性管理效率。", "motivation": "5G蜂窝网络中的移动性性能是关键焦点。3GPP引入的条件切换（CHO）和物理层/数据链路层触发移动性（LTM）等反应式切换策略虽然解决了切换失败和乒乓效应之间的权衡，但由于额外的切换准备，常常导致无线资源利用效率低下。", "method": "本文提出了一种用于O-RAN移动性管理的主动切换框架，通过用户-小区链路预测来识别最佳目标小区进行切换。文中探讨了各种类别的图神经网络（GNNs）用于链路预测，并分析了将其应用于移动性管理领域的复杂性。", "result": "使用真实世界数据集对两种GNN模型进行了比较，实验结果表明它们能够捕捉蜂窝网络动态的图结构特性。", "conclusion": "研究结果提供了关键见解，并概述了未来将基于GNN的链路预测集成到O-RAN网络移动性管理中的步骤。", "translation": "移动性性能一直是5G蜂窝网络中的关键焦点。为了增强切换（HO）性能，3GPP在5G中引入了条件切换（CHO）和物理层/数据链路层触发移动性（LTM）机制。虽然这些反应式切换策略解决了切换失败（HOF）和乒乓效应之间的权衡，但它们常常由于额外的切换准备而导致无线资源利用效率低下。为了克服这些挑战，本文提出了一种用于O-RAN移动性管理的主动切换框架，利用用户-小区链路预测来识别最佳目标小区进行切换。我们探索了各种类别的图神经网络（GNNs）用于链路预测，并分析了将它们应用于移动性管理领域的复杂性。使用真实世界数据集对两种GNN模型进行了比较，实验结果表明它们能够捕捉蜂窝网络动态的图结构特性。最后，我们提出了研究中的关键见解，并概述了未来将基于GNN的链路预测集成到O-RAN网络移动性管理中的步骤。", "summary": "本文针对现有蜂窝网络中反应式切换策略导致的资源利用效率低下问题，提出了一种基于图神经网络（GNN）的主动切换框架，用于O-RAN的移动性管理。该框架利用GNN进行用户-小区链路预测，以确定最佳切换目标小区。研究比较了两种GNN模型在真实数据集上的表现，证明了GNN能够有效捕捉蜂窝网络的动态图结构特性，为O-RAN网络中基于GNN的移动性管理提供了可行性。", "keywords": "图神经网络, O-RAN, 移动性管理, 链路预测, 切换", "comments": "本文的创新点在于将图神经网络应用于O-RAN的移动性管理，特别是通过链路预测实现主动切换，这有望显著提升网络效率和用户体验，减少不必要的资源消耗。其重要性在于为未来O-RAN网络的高效运行提供了新的技术路径。"}}
{"id": "2507.17455", "title": "VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization", "authors": ["Sania Waheed", "Na Min An", "Michael Milford", "Sarvapali D. Ramchurn", "Shoaib Ehsan"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17455v1", "summary": "Geo-localization from a single image at planet scale (essentially an advanced\nor extreme version of the kidnapped robot problem) is a fundamental and\nchallenging task in applications such as navigation, autonomous driving and\ndisaster response due to the vast diversity of locations, environmental\nconditions, and scene variations. Traditional retrieval-based methods for\ngeo-localization struggle with scalability and perceptual aliasing, while\nclassification-based approaches lack generalization and require extensive\ntraining data. Recent advances in vision-language models (VLMs) offer a\npromising alternative by leveraging contextual understanding and reasoning.\nHowever, while VLMs achieve high accuracy, they are often prone to\nhallucinations and lack interpretability, making them unreliable as standalone\nsolutions. In this work, we propose a novel hybrid geo-localization framework\nthat combines the strengths of VLMs with retrieval-based visual place\nrecognition (VPR) methods. Our approach first leverages a VLM to generate a\nprior, effectively guiding and constraining the retrieval search space. We then\nemploy a retrieval step, followed by a re-ranking mechanism that selects the\nmost geographically plausible matches based on feature similarity and proximity\nto the initially estimated coordinates. We evaluate our approach on multiple\ngeo-localization benchmarks and show that it consistently outperforms prior\nstate-of-the-art methods, particularly at street (up to 4.51%) and city level\n(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in\ncombination with VPR lead to scalable, robust, and accurate geo-localization\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17455v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VLM引导的视觉地点识别用于行星级地理定位", "tldr": "该论文提出了一种VLM引导的混合视觉地点识别（VPR）框架，用于行星级地理定位，通过VLM生成先验信息来引导检索，并结合重排序机制，在多个基准测试中显著优于现有最先进方法。", "motivation": "行星级单图像地理定位是一项基础且具有挑战性的任务，因为位置、环境条件和场景变化巨大。传统的检索和分类方法存在可扩展性、感知混叠、泛化能力差和需要大量训练数据等问题。视觉-语言模型（VLMs）虽然有前景，但易受幻觉影响且缺乏可解释性，使其无法作为独立解决方案。", "method": "我们提出了一种新颖的混合地理定位框架，结合了VLM的优势和基于检索的视觉地点识别（VPR）方法。该方法首先利用VLM生成一个先验信息，有效引导和约束检索搜索空间。然后，我们采用检索步骤，接着是重排序机制，根据特征相似性和与初始估计坐标的接近程度选择地理上最合理的匹配。", "result": "我们的方法在多个地理定位基准测试中进行了评估，结果表明它始终优于先前的最先进方法，尤其是在街道级别（高达4.51%）和城市级别（高达13.52%）。", "conclusion": "我们的结果表明，VLM生成的地理先验与VPR相结合，可以产生可扩展、鲁棒和准确的地理定位系统。", "translation": "从单幅图像进行行星级地理定位（本质上是绑架机器人问题的高级或极端版本）是导航、自动驾驶和灾害响应等应用中一项基本且具有挑战性的任务，因为位置、环境条件和场景变化巨大。传统的基于检索的地理定位方法在可扩展性和感知混叠方面存在困难，而基于分类的方法缺乏泛化能力且需要大量的训练数据。视觉-语言模型（VLMs）的最新进展通过利用上下文理解和推理提供了一种有前景的替代方案。然而，尽管VLM实现了高精度，但它们通常容易产生幻觉且缺乏可解释性，使其作为独立解决方案不可靠。在这项工作中，我们提出了一种新颖的混合地理定位框架，结合了VLM的优势与基于检索的视觉地点识别（VPR）方法。我们的方法首先利用VLM生成一个先验信息，有效引导和约束检索搜索空间。然后，我们采用检索步骤，接着是重排序机制，根据特征相似性和与初始估计坐标的接近程度选择地理上最合理的匹配。我们在多个地理定位基准测试中评估了我们的方法，结果表明它始终优于先前的最先进方法，尤其是在街道级别（高达4.51%）和城市级别（高达13.52%）。我们的结果表明，VLM生成的地理先验与VPR相结合，可以产生可扩展、鲁棒和准确的地理定位系统。", "summary": "本论文提出了一种新颖的混合框架，用于解决行星级地理定位的挑战。该方法结合了视觉-语言模型（VLM）的上下文理解能力和视觉地点识别（VPR）的检索机制。通过利用VLM生成地理先验信息来引导和限制VPR的搜索空间，并结合重排序机制，该框架有效克服了传统方法的可扩展性、泛化性以及VLM自身幻觉和可解释性差的问题。实验结果表明，该混合方法在多个地理定位基准测试中，尤其是在街道和城市级别，显著优于现有最先进技术，验证了VLM先验与VPR结合的有效性，从而实现了可扩展、鲁棒且准确的地理定位。", "keywords": "地理定位, 视觉地点识别, 视觉-语言模型, 混合框架, 行星级", "comments": "该论文的创新之处在于巧妙地结合了VLM的语义理解能力和VPR的精确检索能力。它利用VLM生成高层地理先验来解决VPR在大规模场景下的搜索空间过大问题，同时通过VPR的精细匹配和重排序机制弥补了VLM易产生幻觉和缺乏可解释性的缺点。这种混合方法为行星级地理定位提供了一个强大且实用的解决方案，对导航、自动驾驶等领域具有重要意义。"}}
{"id": "2507.17650", "title": "XStacking: Explanation-Guided Stacked Ensemble Learning", "authors": ["Moncef Garouani", "Ayah Barhrhouj", "Olivier Teste"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17650v1", "summary": "Ensemble Machine Learning (EML) techniques, especially stacking, have been\nshown to improve predictive performance by combining multiple base models.\nHowever, they are often criticized for their lack of interpretability. In this\npaper, we introduce XStacking, an effective and inherently explainable\nframework that addresses this limitation by integrating dynamic feature\ntransformation with model-agnostic Shapley additive explanations. This enables\nstacked models to retain their predictive accuracy while becoming inherently\nexplainable. We demonstrate the effectiveness of the framework on 29 datasets,\nachieving improvements in both the predictive effectiveness of the learning\nspace and the interpretability of the resulting models. XStacking offers a\npractical and scalable solution for responsible ML.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17650v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "XStacking: 解释引导的堆叠集成学习", "tldr": "XStacking是一个结合了动态特征变换和Shapley解释的集成学习框架，旨在提高堆叠模型的预测性能和可解释性。", "motivation": "集成机器学习（EML）技术，特别是堆叠，虽然能提高预测性能，但常因缺乏可解释性而受到批评。本研究旨在解决堆叠模型的这一局限性。", "method": "本文引入了XStacking，一个通过将动态特征变换与模型无关的Shapley加性解释相结合，来解决堆叠模型可解释性问题的框架。", "result": "XStacking在29个数据集上展示了其有效性，在学习空间的预测有效性和结果模型的可解释性方面都取得了改进。", "conclusion": "XStacking为负责任的机器学习提供了一个实用且可扩展的解决方案，使堆叠模型在保持预测准确性的同时，具有固有的可解释性。", "translation": "集成机器学习（EML）技术，特别是堆叠，已被证明通过结合多个基础模型来提高预测性能。然而，它们常因缺乏可解释性而受到批评。在本文中，我们引入了XStacking，一个有效且内在可解释的框架，它通过将动态特征变换与模型无关的Shapley加性解释相结合来解决这一局限性。这使得堆叠模型在保持其预测准确性的同时，变得内在可解释。我们在29个数据集上展示了该框架的有效性，在学习空间的预测有效性和结果模型的可解释性方面都取得了改进。XStacking为负责任的机器学习提供了一个实用且可扩展的解决方案。", "summary": "XStacking是一种新型的集成学习框架，旨在解决传统堆叠模型缺乏可解释性的问题。它通过整合动态特征变换和Shapley加性解释，使得堆叠模型在保持高预测性能的同时，实现了固有的可解释性。该框架在29个数据集上验证了其在预测效果和模型可解释性方面的提升，为负责任的机器学习提供了一个实用且可扩展的解决方案。", "keywords": "XStacking, 集成学习, 可解释性AI, 堆叠, Shapley解释", "comments": "XStacking的创新之处在于其将可解释性直接融入了堆叠集成学习过程，而非事后解释，这对于推动负责任的机器学习具有重要意义。通过结合动态特征变换和Shapley值，它有效地解决了集成模型在透明度方面的长期挑战。其在多个数据集上的验证也表明了其实用性和泛化能力。"}}
{"id": "2507.05515", "title": "LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Chuang Yu", "Zhaochun Ren", "Pablo Cesar", "Junxiao Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This version has been anonymized for double-blind review", "url": "http://arxiv.org/abs/2507.05515v2", "summary": "Vision-language models (VLMs) are facing the challenges of understanding and\nfollowing multimodal assembly instructions, particularly when fine-grained\nspatial reasoning and precise object state detection are required. In this\nwork, we explore LEGO Co-builder, a hybrid benchmark combining real-world LEGO\nassembly logic with programmatically generated multimodal scenes. The dataset\ncaptures stepwise visual states and procedural instructions, allowing\ncontrolled evaluation of instruction-following, object detection, and state\ndetection. We introduce a unified framework and assess leading VLMs such as\nGPT-4o, Gemini, and Qwen-VL, under zero-shot and fine-tuned settings. Our\nresults reveal that even advanced models like GPT-4o struggle with fine-grained\nassembly tasks, with a maximum F1 score of just 40.54\\% on state detection,\nhighlighting gaps in fine-grained visual understanding. We release the\nbenchmark, codebase, and generation pipeline to support future research on\nmultimodal assembly assistants grounded in real-world workflows.", "comment": "This version has been anonymized for double-blind review", "pdf_url": "http://arxiv.org/pdf/2507.05515v2", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-23", "AI": {"title_translation": "LEGO Co-builder：探索多模态乐高组装助手的细粒度视觉-语言建模", "tldr": "视觉-语言模型（VLMs）在细粒度多模态组装任务中表现不佳。本文介绍了LEGO Co-builder，一个混合基准和统一框架，用于评估VLMs，结果显示即使是先进模型，如GPT-4o，在状态检测上的F1分数也仅为40.54%。", "motivation": "视觉-语言模型（VLMs）在理解和遵循多模态组装指令时面临挑战，尤其是在需要细粒度空间推理和精确对象状态检测的情况下。", "method": "本研究引入了LEGO Co-builder，一个结合真实世界乐高组装逻辑与程序生成多模态场景的混合基准。该数据集捕获了逐步的视觉状态和程序指令，并引入了一个统一框架，评估了GPT-4o、Gemini和Qwen-VL等领先VLM在零样本和微调设置下的表现。", "result": "研究结果显示，即使是像GPT-4o这样的高级模型在细粒度组装任务上也表现不佳，在状态检测上的F1分数最高仅为40.54%，这突显了细粒度视觉理解方面的不足。", "conclusion": "当前的先进视觉-语言模型在细粒度视觉理解和多模态组装任务方面存在显著差距。本研究发布的基准、代码库和生成管道旨在支持未来对基于真实世界工作流的多模态组装助手的研究。", "translation": "视觉-语言模型（VLMs）在理解和遵循多模态组装指令方面面临挑战，特别是在需要细粒度空间推理和精确对象状态检测时。在这项工作中，我们探索了LEGO Co-builder，这是一个结合了真实世界乐高组装逻辑与程序生成的多模态场景的混合基准。该数据集捕获了逐步的视觉状态和程序指令，允许对指令遵循、对象检测和状态检测进行受控评估。我们引入了一个统一的框架，并在零样本和微调设置下评估了领先的VLM，如GPT-4o、Gemini和Qwen-VL。我们的结果显示，即使是像GPT-4o这样的高级模型在细粒度组装任务上也表现不佳，在状态检测上的F1分数最高仅为40.54%，这突显了细粒度视觉理解方面的不足。我们发布了该基准、代码库和生成管道，以支持未来基于真实世界工作流的多模态组装助手的研究。", "summary": "该论文介绍了LEGO Co-builder，一个用于评估视觉-语言模型（VLMs）在细粒度多模态乐高组装任务中表现的混合基准。该基准结合了真实乐高组装逻辑和程序生成场景，并用于测试GPT-4o、Gemini等主流VLM。研究结果表明，即使是先进的VLM在细粒度组装任务，尤其是在状态检测方面，表现仍不理想，最高F1分数仅为40.54%，揭示了当前模型在细粒度视觉理解方面的不足。论文发布了相关资源以促进未来研究。", "keywords": "视觉-语言模型, 乐高组装, 细粒度理解, 多模态助手, 基准测试", "comments": "这篇论文通过引入一个新颖的混合基准LEGO Co-builder，有效地揭示了当前先进视觉-语言模型在细粒度视觉理解和多模态组装任务上的显著局限性。其创新之处在于结合了真实世界逻辑与程序生成场景，提供了受控的评估环境。结果表明，即使是顶级模型也远未达到理想水平，这为未来研究指明了清晰的方向，即需要更强大的细粒度视觉推理能力。"}}
{"id": "2507.16874", "title": "Budget Allocation Policies for Real-Time Multi-Agent Path Finding", "authors": ["Raz Beck", "Roni Stern"], "categories": ["cs.MA", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 3 tables", "url": "http://arxiv.org/abs/2507.16874v1", "summary": "Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of\nagents such that each agent reaches its desired destination while avoiding\ncollisions with the other agents. Many MAPF solvers are designed to run\noffline, that is, first generate paths for all agents and then execute them.\nReal-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot\nwait until a complete path for each agent has been found before they start to\nmove. Instead, planning and execution are interleaved, where the agents must\ncommit to a fixed number of steps in a constant amount of computation time,\nreferred to as the planning budget. Existing solutions to RT-MAPF iteratively\ncall windowed versions of MAPF algorithms in every planning period, without\nexplicitly considering the size of the planning budget. We address this gap and\nexplore different policies for allocating the planning budget in windowed\nversions of standard MAPF algorithms, namely Prioritized Planning (PrP) and\nMAPF-LNS2. Our exploration shows that the baseline approach in which all agents\ndraw from a shared planning budget pool is ineffective in over-constrained\nsituations. Instead, policies that distribute the planning budget over the\nagents are able to solve more problems with a smaller makespan.", "comment": "8 pages, 2 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.16874v1", "cate": "cs.MA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "实时多智能体路径规划的预算分配策略", "tldr": "本文探讨了实时多智能体路径规划（RT-MAPF）中规划预算的分配策略，发现将预算分配给各个智能体的策略比共享预算池的策略更有效。", "motivation": "现有的实时多智能体路径规划（RT-MAPF）解决方案在每个规划周期中迭代调用MAPF算法的窗口版本，但没有明确考虑规划预算的大小。本文旨在弥补这一空白。", "method": "研究人员探索了在标准MAPF算法（如优先级规划PrP和MAPF-LNS2）的窗口版本中，不同的规划预算分配策略。他们比较了所有智能体从共享预算池中提取预算的方法与将预算分配给各个智能体的策略。", "result": "研究表明，在过约束情况下，所有智能体从共享规划预算池中提取预算的基线方法是无效的。相反，将规划预算分配给各个智能体的策略能够解决更多问题，并具有更小的完工时间。", "conclusion": "将规划预算分配给各个智能体的策略在实时多智能体路径规划中比共享预算池的策略更有效，尤其是在资源受限或过约束的环境下。", "translation": "多智能体路径规划（MAPF）问题是为一组智能体寻找路径，使每个智能体都能到达其期望的目的地，同时避免与其他智能体发生碰撞。许多MAPF求解器设计为离线运行，即首先为所有智能体生成路径，然后执行它们。实时MAPF（RT-MAPF）体现了一种真实的MAPF设置，在这种设置中，不能等到为每个智能体找到完整路径后才开始移动。相反，规划和执行是交错进行的，智能体必须在恒定的计算时间内提交固定数量的步骤，这被称为规划预算。现有的RT-MAPF解决方案在每个规划周期中迭代调用MAPF算法的窗口版本，没有明确考虑规划预算的大小。我们解决了这一空白，并探索了在标准MAPF算法（即优先级规划（PrP）和MAPF-LNS2）的窗口版本中分配规划预算的不同策略。我们的探索表明，所有智能体从共享规划预算池中提取预算的基线方法在过约束情况下是无效的。相反，将规划预算分配给各个智能体的策略能够解决更多问题，并具有更小的完工时间。", "summary": "本文研究了实时多智能体路径规划（RT-MAPF）中的预算分配策略，旨在解决现有方法未明确考虑规划预算大小的问题。研究人员在优先级规划（PrP）和MAPF-LNS2等标准MAPF算法的窗口版本中，对比了共享预算池与向各个智能体分配预算的策略。结果表明，在过约束场景下，共享预算的方法效率低下，而将预算分配给智能体的策略能解决更多问题并缩短完工时间。", "keywords": "多智能体路径规划, 实时规划, 预算分配, 规划策略, 碰撞避免", "comments": "本文的创新点在于首次明确探讨了实时多智能体路径规划中规划预算的分配策略，并证明了在过约束环境下，细粒度的预算分配（而非简单的共享预算池）能显著提升求解效率和性能，为RT-MAPF的实际应用提供了新的优化方向。"}}
{"id": "2505.18079", "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding", "authors": ["Xiaoyi Zhang", "Zhaoyang Jia", "Zongyu Guo", "Jiahao Li", "Bin Li", "Houqiang Li", "Yan Lu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      V3 draft. Under review", "url": "http://arxiv.org/abs/2505.18079v3", "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code has been released in\nhttps://github.com/microsoft/DeepVideoDiscovery.", "comment": "V3 draft. Under review", "pdf_url": "http://arxiv.org/pdf/2505.18079v3", "cate": "cs.CV", "date": "2025-05-23", "updated": "2025-07-23", "AI": {"title_translation": "深度视频发现：面向长视频理解的工具使用型智能体搜索", "tldr": "提出了一种名为“深度视频发现”（DVD）的智能体，通过利用基于工具的智能体搜索策略，显著提升了长视频理解和问答的性能，并在LVBench数据集上达到了SOTA。", "motivation": "现有的大语言模型在处理信息密集的长视频（如小时级视频）时，在视频分析和长上下文处理方面仍存在局限性，特别是在处理长视频的复杂时空特性和问答挑战时。", "method": "提出了一种深度视频发现（DVD）智能体，通过对分段视频片段采用智能体搜索策略来克服这些限制。该方法强调智能体的自主性，通过在多粒度视频数据库上提供一套以搜索为中心的工具，利用LLM的高级推理能力来规划当前观察状态、战略性地选择工具、为行动制定合适的参数，并根据收集到的信息迭代地完善其内部推理。", "result": "在多个长视频理解基准测试中进行了全面评估，结果表明该系统设计具有优势。DVD智能体取得了SOTA性能，在具有挑战性的LVBench数据集上显著超越了现有工作。还提供了全面的消融研究和深入的工具分析。", "conclusion": "Deep Video Discovery（DVD）智能体通过其自主的、基于工具的智能体搜索策略，有效解决了长视频理解的挑战，并在相关基准测试中取得了领先性能，为进一步发展长视频理解智能体提供了有益的见解。", "translation": "长视频理解由于其广泛的时空复杂性和在扩展上下文下进行问答的困难而带来了重大挑战。尽管大型语言模型（LLM）在视频分析能力和长上下文处理方面取得了显著进展，但在处理信息密集的小时级视频时，它们仍然表现出局限性。为了克服这些局限性，我们提出了深度视频发现（Deep Video Discovery）智能体，以利用分段视频片段上的智能体搜索策略。与之前手动设计僵化工作流程的视频智能体不同，我们的方法强调智能体的自主性。通过在多粒度视频数据库上提供一套以搜索为中心的工具，我们的DVD智能体利用LLM的高级推理能力来规划其当前观察状态，战略性地选择工具，为行动制定适当的参数，并根据收集到的信息迭代地完善其内部推理。我们在多个长视频理解基准测试上进行了全面评估，证明了整个系统设计的优势。我们的DVD智能体取得了SOTA性能，在具有挑战性的LVBench数据集上显著超越了现有工作。还提供了全面的消融研究和深入的工具分析，为进一步推进针对长视频理解任务的智能智能体提供了见解。代码已在https://github.com/microsoft/DeepVideoDiscovery 发布。", "summary": "本文提出了“深度视频发现”（DVD）智能体，旨在解决长视频理解中LLM处理复杂和信息密集视频的局限性。DVD智能体采用自主的、基于工具的智能体搜索策略，通过LLM的推理能力在多粒度视频数据库上规划、选择工具并迭代优化。该方法在长视频理解基准测试中表现出色，尤其是在LVBench数据集上达到了SOTA性能，并提供了深入的分析以促进未来研究。", "keywords": "长视频理解, 智能体, 工具使用, 大语言模型, 视频搜索", "comments": "这篇论文通过引入一个自主的、基于工具的智能体搜索策略，为长视频理解提供了一个创新的解决方案。其核心创新在于智能体能够利用LLM的推理能力，动态地选择和使用工具，而非依赖于固定的工作流，这大大增强了处理复杂长视频的能力。在LVBench数据集上取得SOTA性能证明了其有效性和重要性。"}}
{"id": "2507.17036", "title": "Fast One-Pass Sparse Approximation of the Top Eigenvectors of Huge Low-Rank Matrices? Yes, $MAM^*$!", "authors": ["Edem Boahen", "Simone Brugiapaglia", "Hung-Hsu Chou", "Mark Iwen", "Felix Krahmer"], "categories": ["cs.IT", "cs.DS", "cs.NA", "math.IT", "math.NA"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17036v1", "summary": "Motivated by applications such as sparse PCA, in this paper we present\nprovably-accurate one-pass algorithms for the sparse approximation of the top\neigenvectors of extremely massive matrices based on a single compact linear\nsketch. The resulting compressive-sensing-based approaches can approximate the\nleading eigenvectors of huge approximately low-rank matrices that are too large\nto store in memory based on a single pass over its entries while utilizing a\ntotal memory footprint on the order of the much smaller desired sparse\neigenvector approximations. Finally, the compressive sensing recovery algorithm\nitself (which takes the gathered compressive matrix measurements as input, and\nthen outputs sparse approximations of its top eigenvectors) can also be\nformulated to run in a time which principally depends on the size of the sought\nsparse approximations, making its runtime sublinear in the size of the large\nmatrix whose eigenvectors one aims to approximate. Preliminary experiments on\nhuge matrices having $\\sim 10^{16}$ entries illustrate the developed theory and\ndemonstrate the practical potential of the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17036v1", "cate": "cs.IT", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "快速单通道稀疏近似巨型低秩矩阵的主特征向量？是的，$MAM^*$！", "tldr": "本文提出了一种名为$MAM^*$的快速单通道算法，利用压缩感知技术，为无法完全存储在内存中的巨型低秩矩阵提供主特征向量的稀疏近似，具有高效的内存和亚线性时间复杂度。", "motivation": "受稀疏主成分分析（PCA）等应用的启发，需要对无法完全存储在内存中的超大规模矩阵的主特征向量进行稀疏近似。", "method": "提出了一种基于单个紧凑线性草图的单通道算法，利用压缩感知方法来近似巨型近似低秩矩阵的领先特征向量。所提出的恢复算法的运行时间主要取决于所需稀疏近似的大小，从而实现了相对于原始大矩阵大小的亚线性时间复杂度。", "result": "该方法能够对无法存储在内存中的巨型近似低秩矩阵的主特征向量进行可证明准确的稀疏近似，仅需单次遍历其条目。内存占用量与所需的稀疏特征向量近似值大小相当。恢复算法的运行时间相对于大矩阵的大小是亚线性的。初步实验（针对约$10^{16}$个条目的巨型矩阵）验证了理论并展示了该方法的实用潜力。", "conclusion": "所提出的$MAM^*$算法为处理超大规模低秩矩阵的主特征向量稀疏近似提供了一个高效且实用的解决方案，即使矩阵无法完全存储在内存中也能有效工作。", "translation": "受稀疏PCA等应用的启发，本文提出了一种可证明准确的单通道算法，用于基于单个紧凑线性草图对极大规模矩阵的主特征向量进行稀疏近似。由此产生的基于压缩感知的方法可以近似无法存储在内存中的巨型近似低秩矩阵的主特征向量，仅需单次遍历其条目，同时总内存占用量与所需的小得多稀疏特征向量近似值相当。最后，压缩感知恢复算法本身（以收集到的压缩矩阵测量值为输入，然后输出其主特征向量的稀疏近似）也可以被构建为在主要取决于所需稀疏近似大小的时间内运行，使其运行时间相对于目标近似特征向量的大矩阵大小呈亚线性。针对具有约$10^{16}$个条目的巨型矩阵进行的初步实验说明了所开发的理论并展示了所提出方法的实用潜力。", "summary": "本文提出了一种名为$MAM^*$的创新性单通道算法，旨在解决稀疏PCA等应用中对无法完全存储在内存中的巨型低秩矩阵进行主特征向量稀疏近似的挑战。该方法利用单个紧凑线性草图和压缩感知技术，实现了对领先特征向量的准确近似，同时保持了极低的内存占用（与所需稀疏近似大小相当）和相对于原始大矩阵大小的亚线性运行时间。初步实验验证了其理论基础和在处理万亿级别矩阵时的实用潜力。", "keywords": "稀疏近似, 特征向量, 低秩矩阵, 压缩感知, 单通道", "comments": "该论文的创新点在于提出了一个名为$MAM^*$的算法，通过结合单通道处理、紧凑线性草图和压缩感知，解决了处理超大规模（无法完全存储在内存中）低秩矩阵主特征向量稀疏近似的重大挑战。其关键优势在于实现了可证明的准确性、极低的内存占用和亚线性时间复杂度，这对于大数据分析至关重要。该方法在处理例如$10^{16}$量级条目矩阵时的能力，预示着其在实际应用中具有巨大潜力。"}}
{"id": "2507.17063", "title": "Compatibility of Max and Sum Objectives for Committee Selection and $k$-Facility Location", "authors": ["Yue Han", "Elliot Anshelevich"], "categories": ["cs.DS", "cs.AI"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17063v1", "summary": "We study a version of the metric facility location problem (or, equivalently,\nvariants of the committee selection problem) in which we must choose $k$\nfacilities in an arbitrary metric space to serve some set of clients $C$. We\nconsider four different objectives, where each client $i\\in C$ attempts to\nminimize either the sum or the maximum of its distance to the chosen\nfacilities, and where the overall objective either considers the sum or the\nmaximum of the individual client costs. Rather than optimizing a single\nobjective at a time, we study how compatible these objectives are with each\nother, and show the existence of solutions which are simultaneously\nclose-to-optimum for any pair of the above objectives. Our results show that\nwhen choosing a set of facilities or a representative committee, it is often\npossible to form a solution which is good for several objectives at the same\ntime, instead of sacrificing one desideratum to achieve another.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17063v1", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "最大和总和目标在委员会选择和k设施选址中的兼容性", "tldr": "本文研究了在设施选址和委员会选择问题中，最大化和最小化目标之间的兼容性，并发现存在能同时接近最优解的多目标方案。", "motivation": "传统上在设施选址和委员会选择问题中通常一次只优化一个目标，但实际应用中往往需要平衡多个潜在目标。本文旨在探索不同目标之间的兼容性，以找到能同时满足多个目标的解决方案，避免在不同目标之间进行取舍。", "method": "研究了度量设施选址问题（或委员会选择问题）的四种不同目标组合：每个客户到所选设施距离的总和或最大值，以及整体客户成本的总和或最大值。通过理论分析，证明了存在同时接近任意一对目标最优解的方案。", "result": "结果表明，在所考虑的设施选址和委员会选择问题中，存在能够同时接近任意一对目标最优解的解决方案。", "conclusion": "在选择设施集合或代表性委员会时，通常可以找到一个同时对多个目标都表现良好的解决方案，而无需为了实现一个目标而牺牲另一个目标。", "translation": "我们研究了度量设施选址问题（或等效地，委员会选择问题的变体），其中我们必须在一个任意度量空间中选择 k 个设施来服务一组客户 C。我们考虑了四种不同的目标，其中每个客户 i∈C 试图最小化其到所选设施距离的总和或最大值，并且整体目标考虑个体客户成本的总和或最大值。我们不是一次优化一个单一目标，而是研究这些目标彼此之间有多兼容，并证明存在同时接近上述任意一对目标最优解的方案。我们的结果表明，在选择一组设施或一个代表性委员会时，通常可以形成一个同时对多个目标都很好的解决方案，而不是为了实现一个期望而牺牲另一个。", "summary": "本文探讨了在度量设施选址和委员会选择问题中，四种不同目标（基于客户到设施距离的总和/最大值，以及整体客户成本的总和或最大值）之间的兼容性。研究发现，存在能够同时接近任意一对目标最优解的解决方案，这表明在实际应用中，通常可以找到一个同时满足多个目标的良好方案，从而避免在不同目标之间进行取舍。", "keywords": "设施选址, 委员会选择, 多目标优化, 兼容性, 度量空间", "comments": "这项研究的创新之处在于其多目标优化视角，特别是探讨了不同目标之间的“兼容性”而非仅仅是帕累托最优。它为设施选址和委员会选择等决策问题提供了新的理论基础，表明在某些情况下，决策者可能不需要在看似冲突的目标之间做出艰难的权衡。这对于实际应用中需要平衡多方面需求的场景具有重要意义。"}}
{"id": "2507.17402", "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning", "authors": ["Li Jun", "Wang Jinpeng", "Tan Chaolei", "Lian Niu", "Chen Long", "Zhang Min", "Wang Yaowei", "Xia Shu-Tao", "Chen Bin"], "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV'25. 13 pages, 6 figures, 4 tables", "url": "http://arxiv.org/abs/2507.17402v1", "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.", "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.17402v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HLFormer：通过双曲学习增强部分相关视频检索", "tldr": "HLFormer提出首个用于部分相关视频检索的双曲建模框架，通过双曲空间学习和混合注意力机制，解决了欧几里得空间中的几何失真和次优层次建模问题，显著优于现有方法。", "motivation": "现有部分相关视频检索方法在欧几里得空间中存在几何失真，这有时会错误地表示视频的内在层次结构，并忽略某些层次语义，导致次优的时间建模能力。", "method": "提出首个用于部分相关视频检索的双曲建模框架HLFormer。该框架利用双曲空间学习来弥补欧几里得空间次优的层次建模能力。具体来说，HLFormer集成了洛伦兹注意力块和欧几里得注意力块以在混合空间中编码视频嵌入，并使用均值引导自适应交互模块动态融合特征。此外，引入部分顺序保持损失，通过洛伦兹锥约束来强制执行“文本 < 视频”的层次结构。", "result": "广泛的实验表明，HLFormer优于最先进的方法。", "conclusion": "HLFormer通过引入双曲学习和创新的模块设计，有效解决了部分相关视频检索中的层次结构建模问题，并显著提升了检索性能。", "translation": "部分相关视频检索（PRVR）解决了匹配未剪辑视频与仅描述部分内容的文本查询的关键挑战。现有方法存在欧几里得空间中的几何失真问题，这有时会错误地表示视频的内在层次结构并忽略某些层次语义，最终导致次优的时间建模。为了解决这个问题，我们提出了首个用于PRVR的双曲建模框架，即HLFormer，它利用双曲空间学习来弥补欧几里得空间次优的层次建模能力。具体来说，HLFormer集成了洛伦兹注意力块和欧几里得注意力块，以在混合空间中编码视频嵌入，使用均值引导自适应交互模块动态融合特征。此外，我们引入了部分顺序保持损失，通过洛伦兹锥约束来强制执行“文本 < 视频”的层次结构。这种方法通过加强视频内容和文本查询之间的部分相关性，进一步增强了跨模态匹配。广泛的实验表明，HLFormer优于最先进的方法。代码已发布在https://github.com/lijun2005/ICCV25-HLFormer。", "summary": "本文针对部分相关视频检索（PRVR）中的欧几里得空间几何失真和层次结构建模不足问题，提出了首个双曲建模框架HLFormer。HLFormer通过结合洛伦兹注意力块和欧几里得注意力块在混合空间中编码视频嵌入，并利用均值引导自适应交互模块进行特征融合。此外，引入部分顺序保持损失，通过洛伦兹锥约束强化“文本 < 视频”的层次关系，从而增强跨模态匹配。实验证明HLFormer性能超越现有SOTA方法。", "keywords": "部分相关视频检索, 双曲学习, 混合空间, 视频检索, 跨模态匹配", "comments": "HLFormer的创新点在于首次将双曲学习引入部分相关视频检索领域，有效解决了欧几里得空间中难以捕捉复杂层次结构的问题。通过混合空间编码和新颖的损失函数，该方法在理论和实践上都取得了显著进展，为视频检索提供了新的视角和强大的解决方案。"}}
{"id": "2507.17545", "title": "Scalable DC Optimization via Adaptive Frank-Wolfe Algorithms", "authors": ["Sebastian Pokutta"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17545v1", "summary": "We consider the problem of minimizing a difference of (smooth) convex\nfunctions over a compact convex feasible region $P$, i.e., $\\min_{x \\in P} f(x)\n- g(x)$, with smooth $f$ and Lipschitz continuous $g$. This computational study\nbuilds upon and complements the framework of Maskan et al. [2025] by\nintegrating advanced Frank-Wolfe variants to reduce computational overhead. We\nempirically show that constrained DC problems can be efficiently solved using a\ncombination of the Blended Pairwise Conditional Gradients (BPCG) algorithm\n[Tsuji et al., 2022] with warm-starting and the adaptive error bound from\nMaskan et al. [2025]. The result is a highly efficient and scalable\nprojection-free algorithm for constrained DC optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17545v1", "cate": "math.OC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过自适应Frank-Wolfe算法实现可扩展的DC优化", "tldr": "本文通过结合先进的Frank-Wolfe变体（特别是BPCG算法与热启动）和自适应误差界，提出了一种高效、可扩展且无需投影的约束差分凸（DC）优化算法。", "motivation": "该研究旨在通过集成先进的Frank-Wolfe变体来减少计算开销，从而高效解决约束差分凸（DC）优化问题，并在此基础上补充Maskan等人[2025]的框架。", "method": "本文通过结合Blended Pairwise Conditional Gradients (BPCG) 算法[Tsuji et al., 2022]与热启动技术，以及Maskan等人[2025]的自适应误差界，来解决约束差分凸（DC）优化问题。", "result": "结果是一个高效且可扩展的无需投影的约束差分凸（DC）优化算法，能够有效解决约束DC问题。", "conclusion": "实验结果表明，通过结合BPCG算法和自适应误差界，可以高效地解决约束差分凸（DC）问题。", "translation": "我们考虑在紧凸可行域P上最小化（光滑）凸函数之差的问题，即 $\\min_{x \\in P} f(x) - g(x)$，其中$f$是光滑的，$g$是Lipschitz连续的。这项计算研究建立在Maskan等人[2025]的框架之上并对其进行了补充，通过集成先进的Frank-Wolfe变体来减少计算开销。我们通过实证表明，使用混合成对条件梯度（BPCG）算法[Tsuji et al., 2022]与热启动以及Maskan等人[2025]的自适应误差界相结合，可以高效地解决约束DC问题。其结果是一种用于约束DC优化的高效且可扩展的无需投影算法。", "summary": "本文提出了一种解决紧凸可行域上光滑凸函数之差（DC）最小化问题的高效且可扩展的无需投影算法。该方法在Maskan等人[2025]的框架基础上，创新性地结合了Blended Pairwise Conditional Gradients (BPCG) 算法、热启动技术以及自适应误差界，旨在显著降低计算开销。实证研究表明，这种集成方法能够高效地处理约束DC优化问题。", "keywords": "DC优化, Frank-Wolfe算法, BPCG, 可扩展性, 无需投影", "comments": "本文的创新之处在于将Frank-Wolfe算法的先进变体（特别是BPCG与热启动）与自适应误差界相结合，从而为DC优化提供了一种高效且可扩展的无需投影解决方案。这对于处理大规模或复杂约束优化问题具有重要意义，因为它避免了昂贵的投影操作，提高了计算效率。"}}
{"id": "2507.10382", "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis", "authors": ["Yue Ding", "Conor McCarthy", "Kevin O'Shea", "Mingming Liu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10382v2", "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10382v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-23", "AI": {"title_translation": "利用RAG-LLM进行城市出行模拟与分析", "tldr": "本文提出了一个基于云和LLM的共享电动出行平台，集成了个性化路线推荐，并评估了其优化模块和RAG框架的查询准确性。", "motivation": "随着智能出行和共享电动出行服务的兴起，以及用户对电动出行需求的增长，提供全面的端到端解决方案变得至关重要。LLM在此领域展现出潜力，可以支持智能决策、用户交互和实时交通分析。", "method": "本文提出了一个基于云的、LLM驱动的共享电动出行平台，该平台集成了用于个性化路线推荐的移动应用程序。优化模块根据不同交通场景下的旅行时间和成本进行评估。此外，LLM驱动的RAG框架在模式级别对不同用户使用各种评估方法进行了评估。", "result": "模式级RAG与XiYanSQL在系统操作员查询上的平均执行准确率为0.81，在用户查询上的平均执行准确率为0.98。", "conclusion": "本文成功构建了一个云端LLM驱动的共享电动出行平台，并在查询准确性方面展示了其RAG框架的有效性，尤其在用户查询方面表现出色，验证了RAG-LLMs在城市出行模拟和分析中的潜力。", "translation": "随着智能出行和共享电动出行服务的兴起，众多先进技术已应用于该领域。基于云的交通模拟解决方案蓬勃发展，提供了日益真实的不断演变的出行场景表示。大型语言模型（LLM）已成为开创性工具，为包括智能决策、用户交互和实时交通分析在内的各种应用提供强有力的支持。随着用户对电动出行需求的持续增长，提供全面的端到端解决方案变得至关重要。在本文中，我们提出了一个基于云的、由LLM驱动的共享电动出行平台，该平台集成了用于个性化路线推荐的移动应用程序。优化模块根据不同交通场景下的旅行时间和成本进行评估。此外，LLM驱动的RAG框架在模式级别对不同用户使用各种评估方法进行了评估。使用XiYanSQL的模式级RAG在系统操作员查询上的平均执行准确率为0.81，在用户查询上的平均执行准确率为0.98。", "summary": "本文针对智能出行和共享电动出行领域日益增长的需求，提出了一个基于云和大型语言模型（LLM）的共享电动出行平台。该平台集成了移动应用程序以提供个性化路线推荐，并通过评估优化模块的旅行时间与成本效益。特别地，研究评估了LLM驱动的检索增强生成（RAG）框架在模式级别的查询准确性，结果显示其在用户查询上表现出高准确性（0.98），在系统操作员查询上也有良好表现（0.81），验证了RAG-LLMs在城市出行模拟和分析中的潜力。", "keywords": "城市出行, RAG-LLM, 共享电动出行, 交通模拟, 查询准确性", "comments": "本文创新性地将RAG-LLM技术应用于城市共享电动出行领域，构建了一个端到端的平台，并关注了实际应用中的查询准确性，尤其是在用户交互层面的高准确性是其亮点。这为智能交通系统提供了新的解决方案，但抽象中未详细说明其模拟和分析的具体方法和更广泛的影响。"}}
{"id": "2507.17259", "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs", "authors": ["Eyal German", "Sagiv Antebi", "Daniel Samira", "Asaf Shabtai", "Yuval Elovici"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17259v1", "summary": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17259v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Tab-MIA：一个用于LLM表格数据成员推断攻击的基准数据集", "tldr": "Tab-MIA是一个新基准数据集，用于评估大型语言模型（LLMs）在表格数据上的成员推断攻击（MIAs）风险。研究发现LLMs会记忆表格数据，即使经过少量微调也极易受到MIAs攻击。", "motivation": "大型语言模型（LLMs）越来越多地在表格数据上进行训练，而表格数据通常包含高度结构化的个人身份信息（PII），这带来了隐私风险，因为敏感记录可能被模型无意中保留并通过数据提取或成员推断攻击（MIAs）暴露。现有的MIA方法主要针对文本内容，但它们应用于结构化数据时的有效性和威胁影响可能不同，因为结构化数据内容有限、数据类型多样、值分布独特以及具有列级语义。", "method": "本文提出了Tab-MIA，一个用于评估LLM表格数据上MIAs的基准数据集。Tab-MIA包含五个数据集合，每个集合以六种不同的编码格式表示。利用Tab-MIA基准，研究人员首次评估了在多种编码格式下，使用表格数据微调的LLMs上最先进的MIA方法。在评估中，分析了预训练LLMs对源自维基百科表格的结构化数据的记忆行为。", "result": "研究结果表明，LLMs以不同编码格式记忆表格数据，使其容易受到MIAs的提取攻击。即使仅微调了三个epochs，模型也表现出高度的脆弱性，在大多数情况下，AUROC分数接近90%。", "conclusion": "Tab-MIA能够系统地评估这些风险，并为开发LLM中表格数据的隐私保护方法提供了基础。", "translation": "大型语言模型（LLMs）越来越多地在表格数据上进行训练，与非结构化文本不同，表格数据通常以高度结构化和明确的格式包含个人身份信息（PII）。因此，隐私风险随之产生，因为敏感记录可能被模型无意中保留并通过数据提取或成员推断攻击（MIAs）暴露。虽然现有的MIA方法主要针对文本内容，但由于其内容有限、数据类型多样、独特的值分布和列级语义，当应用于结构化数据时，其有效性和威胁影响可能有所不同。在本文中，我们提出了Tab-MIA，一个用于评估LLM中表格数据上MIAs的基准数据集，并展示了如何使用它。Tab-MIA包含五个数据集合，每个集合以六种不同的编码格式表示。利用我们的Tab-MIA基准，我们首次评估了在多种编码格式下，使用表格数据微调的LLMs上最先进的MIA方法。在评估中，我们分析了预训练LLMs对源自维基百科表格的结构化数据的记忆行为。我们的研究结果表明，LLMs以不同编码格式记忆表格数据，使其容易受到MIAs的提取攻击。即使仅微调了三个epochs，模型也表现出高度的脆弱性，在大多数情况下，AUROC分数接近90%。Tab-MIA能够系统地评估这些风险，并为开发LLM中表格数据的隐私保护方法提供了基础。", "summary": "本文介绍了Tab-MIA，一个专门用于评估大型语言模型（LLMs）中表格数据上成员推断攻击（MIAs）的基准数据集。鉴于LLMs越来越多地使用包含敏感PII的表格数据进行训练，研究旨在量化隐私泄露风险。Tab-MIA包含五个数据集合和六种编码格式，用于系统评估。通过实验，研究发现LLMs会记忆表格数据，并且即使经过少量微调，也极易受到MIAs攻击，AUROC得分在多数情况下接近90%。Tab-MIA为评估和开发LLMs表格数据隐私保护方法提供了基础。", "keywords": "成员推断攻击, 表格数据, 大型语言模型, 隐私风险, 基准数据集", "comments": "本文通过引入Tab-MIA基准数据集，填补了LLMs在表格数据上成员推断攻击评估的空白。其创新之处在于首次系统地评估了不同编码格式下LLMs对表格数据的记忆行为和MIAs的脆弱性。研究结果强调了LLMs处理结构化数据时存在的严重隐私风险，即使是轻度微调也可能导致显著泄露。Tab-MIA的发布对于推动LLMs隐私保护技术的发展具有重要意义，因为它提供了一个标准化的评估工具。"}}
{"id": "2502.20650", "title": "Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models", "authors": ["Yu Pan", "Jiahao Chen", "Bingrong Dai", "Lin Wang", "Yi Du", "Jiao Liu"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20650v4", "summary": "In recent years, Diffusion Models (DMs) have demonstrated significant\nadvances in the field of image generation. However, according to current\nresearch, DMs are vulnerable to backdoor attacks, which allow attackers to\ncontrol the model's output by inputting data containing covert triggers, such\nas a specific visual patch or phrase. Existing defense strategies are well\nequipped to thwart such attacks through backdoor detection and trigger\ninversion because previous attack methods are constrained by limited input\nspaces and low-dimensional triggers. For example, visual triggers are easily\nobserved by defenders, text-based or attention-based triggers are more\nsusceptible to neural network detection. To explore more possibilities of\nbackdoor attack in DMs, we propose Gungnir, a novel method that enables\nattackers to activate the backdoor in DMs through style triggers within input\nimages. Our approach proposes using stylistic features as triggers for the\nfirst time and implements backdoor attacks successfully in image-to-image tasks\nby introducing Reconstructing-Adversarial Noise (RAN) and Short-Term\nTimesteps-Retention (STTR). Our technique generates trigger-embedded images\nthat are perceptually indistinguishable from clean images, thus bypassing both\nmanual inspection and automated detection neural networks. Experiments\ndemonstrate that Gungnir can easily bypass existing defense methods. Among\nexisting DM defense frameworks, our approach achieves a 0 backdoor detection\nrate (BDR). Our codes are available at https://github.com/paoche11/Gungnir.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20650v4", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-23", "AI": {"title_translation": "Gungnir：利用图像中的风格特征对扩散模型进行后门攻击", "tldr": "Gungnir提出了一种利用图像风格特征作为触发器的新型后门攻击方法，可以绕过现有扩散模型防御，因为其生成的带触发器图像与干净图像在感知上无法区分。", "motivation": "现有针对扩散模型的后门攻击方法受限于输入空间和低维触发器，容易被检测和防御。为了探索扩散模型中后门攻击的更多可能性，并开发更隐蔽、更难检测的攻击，本文提出了Gungnir。", "method": "本文提出了Gungnir，一种利用输入图像中的风格特征作为触发器来激活扩散模型中后门的新方法。它首次将风格特征用作触发器，并通过引入重构对抗噪声（RAN）和短期时间步保留（STTR）在图像到图像的任务中成功实现后门攻击。", "result": "Gungnir生成的嵌入触发器的图像与干净图像在感知上无法区分，因此可以绕过人工检查和自动化检测神经网络。实验表明，Gungnir能够轻松绕过现有防御方法，在现有扩散模型防御框架中实现了0%的后门检测率（BDR）。", "conclusion": "Gungnir证明了利用图像风格特征作为后门触发器对扩散模型进行攻击的可行性和隐蔽性，对现有防御构成了严重挑战，并揭示了扩散模型安全性的新漏洞。", "translation": "近年来，扩散模型（DMs）在图像生成领域取得了显著进展。然而，根据当前研究，扩散模型容易受到后门攻击，攻击者可以通过输入包含隐蔽触发器的数据来控制模型的输出，例如特定的视觉补丁或短语。现有的防御策略能够通过后门检测和触发器反演来有效阻止此类攻击，因为以往的攻击方法受限于有限的输入空间和低维触发器。例如，视觉触发器容易被防御者观察到，基于文本或基于注意力的触发器更容易被神经网络检测到。为了探索扩散模型中后门攻击的更多可能性，我们提出了Gungnir，一种新颖的方法，使攻击者能够通过输入图像中的风格触发器来激活扩散模型中的后门。我们的方法首次提出使用风格特征作为触发器，并通过引入重构对抗噪声（RAN）和短期时间步保留（STTR），在图像到图像的任务中成功实现了后门攻击。我们的技术生成的嵌入触发器的图像在感知上与干净图像无法区分，从而绕过了人工检查和自动化检测神经网络。实验表明，Gungnir可以轻松绕过现有防御方法。在现有扩散模型防御框架中，我们的方法实现了0%的后门检测率（BDR）。我们的代码可在https://github.com/paoche11/Gungnir 获取。", "summary": "本研究提出了一种名为Gungnir的新型后门攻击方法，旨在针对扩散模型。与现有易于检测的后门触发器不同，Gungnir利用输入图像中的风格特征作为隐蔽触发器，并通过引入重构对抗噪声（RAN）和短期时间步保留（STTR）来实现攻击。该方法生成的带触发器图像在感知上与原始图像无异，从而有效规避了人工审查和自动化检测系统。实验结果表明，Gungnir能够成功绕过当前的扩散模型防御机制，实现了0%的后门检测率。", "keywords": "扩散模型, 后门攻击, 风格特征, 图像生成, Gungnir", "comments": "Gungnir的创新之处在于首次将图像的风格特征作为后门攻击的触发器，这显著提高了攻击的隐蔽性。通过生成与干净图像在感知上无法区分的触发器嵌入图像，它成功绕过了现有的人工检测和自动化神经网络防御，对扩散模型的安全性构成了新的且更严峻的挑战。这项工作突出了在开发更鲁棒的防御机制以应对这种新型隐蔽攻击方面的紧迫性。"}}
{"id": "2507.17245", "title": "DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs", "authors": ["Haolin Jin", "Mengbai Xiao", "Yuan Yuan", "Xiao Zhang", "Dongxiao Yu", "Guanghui Zhang", "Haoliang Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17245v1", "summary": "The Transformer architecture has revolutionized deep learning, delivering the\nstate-of-the-art performance in areas such as natural language processing,\ncomputer vision, and time series prediction. However, its core component,\nself-attention, has the quadratic time complexity relative to input sequence\nlength, which hinders the scalability of Transformers. The exsiting approaches\non optimizing self-attention either discard full-contextual information or lack\nof flexibility. In this work, we design DistrAttention, an effcient and\nflexible self-attention mechanism with the full context. DistrAttention\nachieves this by grouping data on the embedding dimensionality, usually\nreferred to as $d$. We realize DistrAttention with a lightweight sampling and\nfusion method that exploits locality-sensitive hashing to group similar data. A\nblock-wise grouping framework is further designed to limit the errors\nintroduced by locality sensitive hashing. By optimizing the selection of block\nsizes, DistrAttention could be easily integrated with FlashAttention-2, gaining\nhigh-performance on modern GPUs. We evaluate DistrAttention with extensive\nexperiments. The results show that our method is 37% faster than\nFlashAttention-2 on calculating self-attention. In ViT inference,\nDistrAttention is the fastest and the most accurate among approximate\nself-attention mechanisms. In Llama3-1B, DistrAttention still achieves the\nlowest inference time with only 1% accuray loss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17245v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DistrAttention：一种现代GPU上高效灵活的自注意力机制", "tldr": "DistrAttention是一种高效灵活的自注意力机制，通过对嵌入维度数据进行分组，实现全上下文处理，并在现代GPU上比FlashAttention-2更快，且在ViT和Llama3-1B中表现出优异的性能和精度。", "motivation": "Transformer架构的核心组件自注意力具有二次时间复杂度，这阻碍了其可扩展性。现有优化方法要么丢弃全上下文信息，要么缺乏灵活性。", "method": "DistrAttention通过对嵌入维度（d）上的数据进行分组来实现高效灵活的全上下文自注意力。它采用轻量级的采样和融合方法，利用局部敏感哈希（LSH）对相似数据进行分组，并设计了块状分组框架以限制LSH引入的误差。通过优化块大小，DistrAttention可以与FlashAttention-2集成。", "result": "在计算自注意力时比FlashAttention-2快37%。在ViT推理中，DistrAttention在近似自注意力机制中速度最快且精度最高。在Llama3-1B中，DistrAttention仍以仅1%的精度损失实现了最低的推理时间。", "conclusion": "DistrAttention提供了一种高效、灵活且准确的自注意力机制，解决了Transformer的可扩展性问题，同时保留了完整的上下文信息。", "translation": "Transformer架构彻底改变了深度学习，在自然语言处理、计算机视觉和时间序列预测等领域取得了最先进的性能。然而，其核心组件自注意力，相对于输入序列长度具有二次时间复杂度，这阻碍了Transformer的可扩展性。现有的优化自注意力的方法要么丢弃全上下文信息，要么缺乏灵活性。在这项工作中，我们设计了DistrAttention，一种高效灵活的自注意力机制，具有全上下文。DistrAttention通过对嵌入维度（通常称为$d$）上的数据进行分组来实现这一点。我们通过一种轻量级的采样和融合方法实现了DistrAttention，该方法利用局部敏感哈希来对相似数据进行分组。进一步设计了一个块状分组框架，以限制局部敏感哈希引入的误差。通过优化块大小的选择，DistrAttention可以很容易地与FlashAttention-2集成，在现代GPU上获得高性能。我们通过大量的实验评估了DistrAttention。结果表明，我们的方法在计算自注意力时比FlashAttention-2快37%。在ViT推理中，DistrAttention在近似自注意力机制中是速度最快、精度最高的。在Llama3-1B中，DistrAttention仍然以仅1%的精度损失实现了最低的推理时间。", "summary": "本文介绍了DistrAttention，一种新颖的自注意力机制，旨在克服传统自注意力的二次时间复杂度，同时保留完整的上下文信息。它通过利用局部敏感哈希和块状分组框架，沿嵌入维度对数据进行分组，从而实现高效和灵活性。DistrAttention可以与FlashAttention-2集成，在现代GPU上实现高性能。实验结果表明，DistrAttention在计算自注意力时显著快于FlashAttention-2，并在各种深度学习任务（包括ViT推理和Llama3-1B）中保持高精度。", "keywords": "自注意力, Transformer, GPU, 局部敏感哈希, 效率", "comments": "DistrAttention的创新之处在于其通过嵌入维度上的数据分组和局部敏感哈希实现全上下文的自注意力机制，同时解决了现有方法在效率和灵活性上的不足。其与FlashAttention-2的兼容性以及在实际应用（如ViT和Llama3-1B）中表现出的显著性能提升（速度更快，精度损失小）表明其具有重要的实用价值和广阔的应用前景。"}}
{"id": "2507.17115", "title": "Stochastically Structured Reservoir Computers for Financial and Economic System Identification", "authors": ["Lendy Banegas", "Fredy Vides"], "categories": ["math.OC", "cs.SY", "econ.TH", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17115v1", "summary": "This paper introduces a methodology for identifying and simulating financial\nand economic systems using stochastically structured reservoir computers\n(SSRCs). The proposed framework leverages structure-preserving embeddings and\ngraph-informed coupling matrices to model inter-agent dynamics with enhanced\ninterpretability. A constrained optimization scheme ensures that the learned\nmodels satisfy both stochastic and structural constraints. Two empirical case\nstudies, a dynamic behavioral model of resource competition among agents, and\nregional inflation network dynamics, illustrate the effectiveness of the\napproach in capturing and anticipating complex nonlinear patterns and enabling\ninterpretable predictive analysis under uncertainty.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17115v1", "cate": "math.OC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于金融和经济系统识别的随机结构储层计算器", "tldr": "本文提出了一种使用随机结构储层计算器（SSRCs）识别和模拟金融经济系统的方法，该方法通过结构保持嵌入和图知耦合矩阵增强可解释性，并通过约束优化确保模型满足随机和结构约束，并在两个案例研究中展示了其在捕获和预测复杂非线性模式以及在不确定性下进行可解释预测分析的有效性。", "motivation": "现有方法在识别和模拟金融和经济系统时可能面临复杂非线性模式、可解释性和不确定性下的预测分析挑战。本文旨在提出一种新方法来解决这些问题，实现对复杂系统更有效的识别和模拟。", "method": "本文引入了一种使用随机结构储层计算器（SSRCs）的方法。该框架利用结构保持嵌入和图知耦合矩阵来建模代理间动力学，以增强可解释性。此外，还采用了一种约束优化方案，以确保学习到的模型满足随机和结构约束。", "result": "该方法在两个经验案例研究中（代理间资源竞争的动态行为模型和区域通胀网络动力学）得到了验证。结果表明，该方法在捕获和预测复杂非线性模式以及在不确定性下实现可解释的预测分析方面是有效的。", "conclusion": "本文提出的随机结构储层计算器（SSRCs）方法在识别和模拟金融经济系统方面表现出显著的有效性，尤其是在捕获和预测复杂非线性模式以及在不确定性下进行可解释的预测分析方面。", "translation": "本文介绍了一种使用随机结构储层计算器（SSRCs）识别和模拟金融和经济系统的方法。所提出的框架利用结构保持嵌入和图知耦合矩阵来建模代理间动力学，并增强了可解释性。一种约束优化方案确保学习到的模型同时满足随机和结构约束。两个经验案例研究，一个关于代理间资源竞争的动态行为模型，以及区域通胀网络动力学，说明了该方法在捕获和预测复杂非线性模式以及在不确定性下实现可解释的预测分析方面的有效性。", "summary": "本文提出了一种基于随机结构储层计算器（SSRCs）的新方法，用于识别和模拟金融与经济系统。该方法结合了结构保持嵌入和图知耦合矩阵以提高模型可解释性，并通过约束优化确保模型符合随机和结构限制。通过两个实证案例研究，包括资源竞争模型和区域通胀网络，验证了该方法在捕获复杂非线性模式、实现可解释预测分析方面的有效性。", "keywords": "储层计算器, 金融系统, 经济系统, 随机结构, 系统识别", "comments": "本文的创新之处在于将随机结构储层计算器应用于金融和经济系统识别，并结合了结构保持嵌入和图知耦合矩阵以增强模型的解释性。通过引入约束优化，确保了模型的鲁棒性和符合系统特性。这对于理解和预测复杂金融经济动态具有重要意义，尤其是在需要可解释性分析的领域。"}}
{"id": "2507.17651", "title": "CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts", "authors": ["Olaf Dünkel", "Artur Jesslen", "Jiahao Xie", "Christian Theobalt", "Christian Rupprecht", "Adam Kortylewski"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.17651v1", "summary": "An important challenge when using computer vision models in the real world is\nto evaluate their performance in potential out-of-distribution (OOD) scenarios.\nWhile simple synthetic corruptions are commonly applied to test OOD robustness,\nthey often fail to capture nuisance shifts that occur in the real world.\nRecently, diffusion models have been applied to generate realistic images for\nbenchmarking, but they are restricted to binary nuisance shifts. In this work,\nwe introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD\nrobustness of image classifiers for continuous and realistic generative\nnuisance shifts. CNS-Bench allows generating a wide range of individual\nnuisance shifts in continuous severities by applying LoRA adapters to diffusion\nmodels. To address failure cases, we propose a filtering mechanism that\noutperforms previous methods, thereby enabling reliable benchmarking with\ngenerative models. With the proposed benchmark, we perform a large-scale study\nto evaluate the robustness of more than 40 classifiers under various nuisance\nshifts. Through carefully designed comparisons and analyses, we find that model\nrankings can change for varying shifts and shift scales, which cannot be\ncaptured when applying common binary shifts. Additionally, we show that\nevaluating the model performance on a continuous scale allows the\nidentification of model failure points, providing a more nuanced understanding\nof model robustness. Project page including code and data:\nhttps://genintel.github.io/CNS.", "comment": "ICCV 2025. Project page: https://genintel.github.io/CNS", "pdf_url": "http://arxiv.org/pdf/2507.17651v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CNS-Bench：连续干扰偏移下图像分类器鲁棒性基准测试", "tldr": "CNS-Bench引入了一个基准测试，用于评估图像分类器在连续和真实分布外（OOD）干扰偏移下的鲁棒性，通过结合扩散模型和LoRA适配器实现。", "motivation": "在现实世界中使用计算机视觉模型时，评估其在潜在的分布外（OOD）场景中的性能是一个重要挑战。现有方法（如简单的合成损坏或扩散模型）在测试OOD鲁棒性时，往往无法捕捉到真实世界中发生的连续干扰偏移，且受限于二元干扰偏移。", "method": "本文介绍了CNS-Bench，一个连续干扰偏移基准测试，用于量化图像分类器在连续且真实的生成干扰偏移下的OOD鲁棒性。CNS-Bench通过将LoRA适配器应用于扩散模型，能够以连续的严重程度生成各种独立的干扰偏移。为了解决失败案例，我们提出了一种优于以往方法的过滤机制，从而实现了生成模型的可靠基准测试。利用该基准，我们进行了一项大规模研究，评估了40多个分类器在各种干扰偏移下的鲁棒性。", "result": "通过精心设计的比较和分析，我们发现模型排名会随着不同的偏移和偏移尺度而变化，这是使用常见二元偏移时无法捕捉到的。此外，我们表明在连续尺度上评估模型性能可以识别模型的失效点，从而提供对模型鲁棒性更细致的理解。", "conclusion": "CNS-Bench提供了一种更全面、更实际的方法来评估图像分类器的鲁棒性，通过考虑连续的干扰偏移，揭示了传统二元偏移评估无法获得的洞察。", "translation": "在现实世界中使用计算机视觉模型时，评估其在潜在的分布外（OOD）场景中的性能是一个重要挑战。虽然通常应用简单的合成损坏来测试OOD鲁棒性，但它们往往未能捕捉到真实世界中发生的干扰偏移。最近，扩散模型已被应用于生成逼真图像进行基准测试，但它们仅限于二元干扰偏移。在这项工作中，我们引入了CNS-Bench，一个连续干扰偏移基准测试，用于量化图像分类器在连续且真实的生成干扰偏移下的OOD鲁棒性。CNS-Bench通过将LoRA适配器应用于扩散模型，能够以连续的严重程度生成各种独立的干扰偏移。为了解决失败案例，我们提出了一种优于以往方法的过滤机制，从而实现了生成模型的可靠基准测试。利用该基准，我们进行了一项大规模研究，评估了40多个分类器在各种干扰偏移下的鲁棒性。通过精心设计的比较和分析，我们发现模型排名会随着不同的偏移和偏移尺度而变化，这是使用常见二元偏移时无法捕捉到的。此外，我们表明在连续尺度上评估模型性能可以识别模型的失效点，从而提供对模型鲁棒性更细致的理解。项目页面包括代码和数据：https://genintel.github.io/CNS。", "summary": "CNS-Bench旨在解决当前分布外鲁棒性基准测试的局限性，通过引入一种利用扩散模型和LoRA适配器生成连续且逼真干扰偏移的方法。该基准包含一种新颖的过滤机制，以确保评估的可靠性。一项基于CNS-Bench的大规模研究揭示，模型性能排名在连续偏移下会显著变化，并且连续评估有助于识别特定的模型失效点，从而提供了对模型鲁棒性更深入的理解。", "keywords": "图像分类器鲁棒性, 连续干扰偏移, 扩散模型, 基准测试, 分布外检测", "comments": "这篇论文通过引入连续且真实的干扰偏移，显著改进了图像分类器鲁棒性的基准测试方法，超越了传统的二元偏移限制。利用LoRA适配器与扩散模型结合来生成多样化偏移的创新方法，对于开发更适应现实世界场景的视觉模型至关重要。研究结果表明模型排名会因偏移类型和尺度而异，并且连续评估能够揭示模型失效点，这些发现对于理解和提升模型鲁棒性具有重要意义。"}}
{"id": "2505.19166", "title": "JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19166v2", "summary": "We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required. JEDI is model-agnostic and\napplicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently\nimproving prompt alignment and disentanglement in complex scenes. Additionally,\nJEDI provides a lightweight, CLIP-free disentanglement score derived from\ninternal attention distributions, offering a principled benchmark for\ncompositional alignment under test-time conditions. Code and results are\navailable at https://ericbill21.github.io/JEDI/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19166v2", "cate": "cs.CV", "date": "2025-05-25", "updated": "2025-07-23", "AI": {"title_translation": "JEDI：詹森-香农散度在解耦扩散模型中的力量", "tldr": "JEDI是一种无需再训练或外部监督的测试时自适应方法，通过最小化注意力图中的语义纠缠来解耦扩散模型，提高主体分离和组合对齐。", "motivation": "扩散模型在复杂场景中存在主体语义纠缠和组合对齐不足的问题，需要一种无需再训练或外部监督的方法来解决。", "method": "JEDI通过最小化注意力图中的语义纠缠来实现，其核心是使用一种基于詹森-香农散度（Jensen-Shannon divergence）的新颖目标函数。为提高效率，该方法利用对抗性优化减少了所需的更新步骤。", "result": "JEDI增强了扩散模型中的主体分离和组合对齐，持续改进了复杂场景中的提示对齐和解耦。它具有模型无关性，适用于Stable Diffusion 1.5和3.5等架构。此外，JEDI提供了一个轻量级、无需CLIP的解耦分数，该分数源自内部注意力分布，为测试条件下的组合对齐提供了原则性的基准。", "conclusion": "JEDI是一种有效的测试时自适应方法，通过利用詹森-香农散度解决扩散模型的语义纠缠问题，显著提高了生成内容的质量和对齐性，并提供了一种新的、基于内部注意力分布的轻量级评估指标。", "translation": "我们引入了JEDI，这是一种测试时自适应方法，无需再训练或外部监督即可增强扩散模型中的主体分离和组合对齐。JEDI通过使用一种新颖的基于詹森-香农散度的目标函数来最小化注意力图中的语义纠缠。为了提高效率，我们利用对抗性优化，减少了所需的更新步骤。JEDI是模型无关的，适用于Stable Diffusion 1.5和3.5等架构，持续改进了复杂场景中的提示对齐和解耦。此外，JEDI提供了一个轻量级、无需CLIP的解耦分数，该分数源自内部注意力分布，为测试条件下的组合对齐提供了原则性的基准。代码和结果可在https://ericbill21.github.io/JEDI/获取。", "summary": "JEDI是一种创新的测试时自适应方法，旨在无需再训练或外部监督的情况下，通过基于詹森-香农散度的目标函数最小化注意力图中的语义纠缠，从而提高扩散模型的主体分离和组合对齐能力。该方法利用对抗性优化提高效率，具有模型无关性，并为复杂场景中的提示对齐和解耦带来了显著改进。此外，JEDI还提供了一种轻量级的、基于内部注意力分布的解耦评估分数，作为组合对齐的基准。", "keywords": "扩散模型, 詹森-香农散度, 测试时自适应, 解耦, 注意力图", "comments": "该论文的创新点在于提出了基于詹森-香农散度来最小化注意力图中的语义纠缠，从而实现扩散模型的解耦。其在测试时自适应的特性，以及无需外部监督和模型无关性，使其具有很高的实用价值。此外，提供一个轻量级的、无需CLIP的解耦分数也为评估提供了新的视角和工具。"}}
{"id": "2507.17061", "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems", "authors": ["Chengxuan Xia", "Qianye Wu", "Sixuan Tian", "Yilun Hao"], "categories": ["cs.MA", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures", "url": "http://arxiv.org/abs/2507.17061v1", "summary": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.", "comment": "8 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.17061v1", "cate": "cs.MA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "并行性与适应性：多智能体大型语言模型系统中可扩展的文档理解", "tldr": "本文提出了一种新的多智能体大型语言模型（LLM）协调框架，通过动态任务路由、双向反馈和并行评估实现适应性，显著提高了文档理解的覆盖率、连贯性和效率。", "motivation": "现有的大型语言模型（LLM）多智能体框架依赖于静态工作流、固定角色和有限的智能体间通信，这降低了它们在开放式、高复杂性领域的有效性。", "method": "本文提出了一种协调框架，通过动态任务路由、双向反馈和并行智能体评估这三个核心机制实现适应性。该框架允许智能体根据置信度和工作负载重新分配任务，交换结构化批判以迭代改进输出，并在高模糊性子任务上进行竞争，由评估器驱动选择最合适的结果。这些原则被实例化在一个模块化架构中。", "result": "与静态和部分自适应的基线相比，该框架在事实覆盖率、连贯性和效率方面显示出显著改进。", "conclusion": "研究结果强调了在多智能体LLM系统中结合适应性和结构化竞争的益处。", "translation": "大型语言模型（LLM）智能体在协作完成任务方面显示出越来越大的前景。然而，现有的多智能体框架通常依赖于静态工作流、固定角色和有限的智能体间通信，这降低了它们在开放式、高复杂性领域的有效性。本文提出了一种协调框架，通过三个核心机制实现适应性：动态任务路由、双向反馈和并行智能体评估。该框架允许智能体根据置信度和工作负载重新分配任务，交换结构化批判以迭代改进输出，并且最重要的是，在高模糊性子任务上进行竞争，由评估器驱动选择最合适的结果。我们将这些原则实例化在一个模块化架构中，并证明在事实覆盖率、连贯性和效率方面，相对于静态和部分自适应的基线有实质性改进。我们的研究结果强调了在多智能体LLM系统中结合适应性和结构化竞争的益处。", "summary": "本文针对现有大型语言模型（LLM）多智能体框架在开放式、高复杂性任务中表现不佳的问题，提出了一种新的协调框架。该框架通过引入动态任务路由、双向反馈和并行智能体评估等核心机制，实现了系统的适应性。实验结果表明，与传统方法相比，该自适应框架在文档理解任务中显著提升了事实覆盖率、连贯性和效率，强调了适应性和结构化竞争在多智能体LLM系统中的重要性。", "keywords": "多智能体系统, 大型语言模型, 适应性, 任务路由, 并行评估", "comments": "这篇论文的创新点在于提出了一个结合了适应性和结构化竞争的多智能体LLM协调框架。通过动态任务路由、双向反馈和并行评估，解决了现有系统静态、缺乏灵活性的问题。其重要性在于为构建更鲁棒、高效的LLM协作系统提供了新的范式，尤其是在处理复杂、开放式任务时具有显著优势。"}}
{"id": "2507.17253", "title": "Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology", "authors": ["Maharshi Shastri", "Ujjval Shrivastav"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17253v1", "summary": "The increasing demand for fast and cost effective last mile delivery\nsolutions has catalyzed significant advancements in drone based logistics. This\nresearch describes the development of an AI integrated drone delivery system,\nfocusing on route optimization, object detection, secure package handling, and\nreal time tracking. The proposed system leverages YOLOv4 Tiny for object\ndetection, the NEO 6M GPS module for navigation, and the A7670 SIM module for\nreal time communication. A comparative analysis of lightweight AI models and\nhardware components is conducted to determine the optimal configuration for\nreal time UAV based delivery. Key challenges including battery efficiency,\nregulatory compliance, and security considerations are addressed through the\nintegration of machine learning techniques, IoT devices, and encryption\nprotocols. Preliminary studies demonstrate improvement in delivery time\ncompared to conventional ground based logistics, along with high accuracy\nrecipient authentication through facial recognition. The study also discusses\nethical implications and societal acceptance of drone deliveries, ensuring\ncompliance with FAA, EASA and DGCA regulatory standards. Note: This paper\npresents the architecture, design, and preliminary simulation results of the\nproposed system. Experimental results, simulation benchmarks, and deployment\nstatistics are currently being acquired. A comprehensive analysis will be\nincluded in the extended version of this work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17253v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "优化配送物流：利用无人机技术提升速度与安全性", "tldr": "本研究开发了一种集成AI的无人机配送系统，通过优化路线、物体检测、安全包裹处理和实时追踪，旨在提升最后一英里配送的速度和安全性。初步研究显示配送时间有所改善，并讨论了监管和伦理问题。", "motivation": "对快速且经济高效的最后一英里配送解决方案日益增长的需求，推动了无人机物流的显著发展。本研究旨在解决这一需求，通过开发一个AI集成无人机配送系统来优化配送物流，提升速度和安全性。", "method": "本研究开发了一个AI集成无人机配送系统，专注于路线优化、物体检测、安全包裹处理和实时追踪。该系统利用YOLOv4 Tiny进行物体检测，NEO 6M GPS模块进行导航，A7670 SIM模块进行实时通信。研究还对轻量级AI模型和硬件组件进行了比较分析，以确定实时无人机配送的最佳配置。通过集成机器学习技术、物联网设备和加密协议，解决了电池效率、监管合规性和安全等关键挑战。", "result": "初步研究表明，与传统地面物流相比，配送时间有所改善，并通过面部识别实现了高精度的收件人认证。该系统能够提升配送速度和安全性。", "conclusion": "本研究提出了一个AI集成无人机配送系统的架构、设计和初步模拟结果，展示了其在提升配送速度和安全性方面的潜力。全面的分析（包括实验结果、模拟基准和部署统计数据）将在后续的扩展版本中提供。", "translation": "对快速且经济高效的最后一英里配送解决方案日益增长的需求，推动了无人机物流的显著发展。本研究描述了一种AI集成无人机配送系统的开发，重点关注路线优化、物体检测、安全包裹处理和实时追踪。所提出的系统利用YOLOv4 Tiny进行物体检测，NEO 6M GPS模块进行导航，A7670 SIM模块进行实时通信。研究对轻量级AI模型和硬件组件进行了比较分析，以确定实时无人机配送的最佳配置。通过集成机器学习技术、物联网设备和加密协议，解决了包括电池效率、监管合规性和安全考虑在内的关键挑战。初步研究表明，与传统地面物流相比，配送时间有所改善，并通过面部识别实现了高精度的收件人认证。本研究还讨论了无人机配送的伦理影响和社会接受度，确保符合FAA、EASA和DGCA的监管标准。注：本文介绍了所提出系统的架构、设计和初步模拟结果。实验结果、模拟基准和部署统计数据目前正在获取中。全面的分析将包含在本文的扩展版本中。", "summary": "本研究旨在通过开发一个AI集成无人机配送系统来优化最后一英里物流，提升配送速度和安全性。该系统集成了YOLOv4 Tiny进行物体检测，NEO 6M GPS和A7670 SIM模块进行导航和通信，并解决了电池效率、监管和安全挑战。初步模拟结果显示，与传统方法相比，配送时间有所改善，并实现了高精度收件人认证。论文还讨论了伦理和监管合规性，并指出实验结果将在后续工作中提供。", "keywords": "无人机配送, AI物流, 最后一英里, 路线优化, 物体检测", "comments": "该论文提出了一种结合AI和无人机技术优化配送物流的创新方法，特别关注了实时性、安全性和合规性。其亮点在于集成了多种先进技术（如YOLOv4 Tiny、机器学习、IoT和加密协议）来解决复杂配送挑战。然而，一个重要的局限性是目前仅提供了初步模拟结果，缺乏实际的实验数据和部署统计，这限制了对其系统性能和实际应用效果的全面评估。未来的扩展工作将弥补这一不足。"}}
{"id": "2507.17202", "title": "DesignLab: Designing Slides Through Iterative Detection and Correction", "authors": ["Jooyeol Yun", "Heng Wang", "Yotaro Shimose", "Jaegul Choo", "Shingo Takamatsu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2507.17202v1", "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.", "comment": "https://yeolj00.github.io/personal-projects/designlab", "pdf_url": "http://arxiv.org/pdf/2507.17202v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DesignLab：通过迭代检测和纠正设计幻灯片", "tldr": "DesignLab是一个通过将设计过程分解为设计审查员和设计贡献者两个角色，并利用迭代检测和纠正循环，帮助非专家设计高质量演示幻灯片的系统。它通过微调大型语言模型并模拟中间草稿来学习检测和修复设计问题，实验证明其性能优于现有方法。", "motivation": "对于非专业人士来说，设计高质量的演示幻灯片具有挑战性，因为涉及复杂的多种设计选择。现有的自动化工具通常缺乏完善自身输出的能力，而这在实际工作流程中至关重要。", "method": "提出DesignLab系统，将设计过程分为设计审查员（识别问题）和设计贡献者（纠正问题）两个角色。这种分解实现了迭代循环，审查员持续检测问题，贡献者进行纠正。通过引入受控扰动来模拟中间草稿，并微调大型语言模型以扮演这些角色，使审查员学习设计错误，贡献者学习如何修复。", "result": "实验表明，DesignLab在性能上优于现有的设计生成方法，包括一种商业工具。", "conclusion": "DesignLab通过采纳设计的迭代特性，能够生成精美、专业的幻灯片，从而超越了现有设计生成方法的局限性。", "translation": "设计高质量的演示幻灯片对于非专业人士来说可能具有挑战性，因为涉及到各种设计选择的复杂性。许多自动化工具可以提供布局和配色方案建议，但往往缺乏完善自身输出的能力，而这在实际工作流程中是一个关键方面。我们提出了 DesignLab，它将设计过程分为两个角色：设计审查员（负责识别设计相关问题）和设计贡献者（负责纠正这些问题）。这种分解实现了一个迭代循环，审查员持续检测问题，贡献者纠正问题，从而使草稿在每次迭代中得到进一步完善，达到以前无法实现的高质量水平。我们对大型语言模型进行了微调，以适应这些角色，并通过引入受控扰动来模拟中间草稿，从而使设计审查员学习设计错误，并使贡献者学习如何修复它们。我们的实验表明，DesignLab 通过采纳设计的迭代特性，其性能优于现有的设计生成方法，包括一种商业工具，从而可以生成精美、专业的幻灯片。", "summary": "DesignLab是一个旨在帮助非专家设计高质量演示幻灯片的系统。它通过将设计流程分解为识别问题的“设计审查员”和纠正问题的“设计贡献者”两个角色，实现了迭代检测和纠正的循环。该系统利用微调过的大型语言模型，并通过模拟中间草稿来训练这些角色。实验证明，DesignLab在生成专业级幻灯片方面优于现有设计工具。", "keywords": "幻灯片设计,迭代设计,大型语言模型,设计自动化,人机交互", "comments": "DesignLab的创新之处在于其将设计过程解耦为迭代的检测与纠正循环，并利用大型语言模型赋能这两个角色。这种方法模拟了人类设计师的迭代工作流，有效解决了现有自动化工具缺乏自我完善能力的问题。其重要性在于为非专业用户提供了生成高质量演示文稿的有效途径，提升了自动化设计工具的实用性。"}}
{"id": "2507.17645", "title": "Quaternion-Domain Super MDS for Robust 3D Localization", "authors": ["Alessio Lukaj", "Keigo Masuoka", "Takumi Takahashi", "Giuseppe Thadeu Freitas de Abreu", "Hideki Ochiai"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures", "url": "http://arxiv.org/abs/2507.17645v1", "summary": "This paper proposes a novel low-complexity three-dimensional (3D)\nlocalization algorithm for wireless sensor networks, termed quanternion-domain\nsuper multi-dimensional scaling (QD-SMDS). The algorithm is based on a\nreformulation of the SMDS, originally developed in the real domain, using\nquaternion algebra. By representing 3D coordinates as quaternions, the method\nconstructs a rank-1 Gram edge kernel (GEK) matrix that integrates both relative\ndistance and angular information between nodes, which enhances the noise\nreduction effect achieved through low-rank truncation employing singular value\ndecomposition (SVD), thereby improving robustness against information loss. To\nfurther reduce computational complexity, we also propose a variant of QD-SMDS\nthat eliminates the need for the computationally expensive SVD by leveraging\nthe inherent structure of the quaternion-domain GEK matrix. This alternative\ndirectly estimates node coordinates using only matrix multiplications within\nthe quaternion domain. Simulation results demonstrate that the proposed method\nsignificantly improves localization accuracy compared to the original SMDS\nalgorithm, especially in scenarios with substantial measurement errors. The\nproposed method also achieves comparable localization accuracy without\nrequiring SVD.", "comment": "12 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.17645v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "四元数域超多维尺度变换用于鲁棒三维定位", "tldr": "本文提出一种新的低复杂度三维定位算法QD-SMDS，通过将SMDS重构到四元数域，并构建集成距离和角度信息的GEK矩阵，提高了定位精度和对测量误差的鲁棒性。同时，还提出了一种无需SVD的变体，进一步降低了计算复杂度。", "motivation": "本文旨在为无线传感器网络提出一种鲁棒且低复杂度的三维定位算法，以解决现有方法在信息丢失和计算复杂性方面的挑战，并提高在存在显著测量误差情况下的定位精度。", "method": "本文提出四元数域超多维尺度变换（QD-SMDS）算法。该算法基于将实数域的SMDS利用四元数代数进行重构，将三维坐标表示为四元数。它构建了一个集成了节点间相对距离和角度信息的秩1格拉姆边缘核（GEK）矩阵，通过奇异值分解（SVD）的低秩截断增强了降噪效果，从而提高了对信息丢失的鲁棒性。为进一步降低计算复杂度，还提出了一种QD-SMDS的变体，通过利用四元数域GEK矩阵的固有结构，无需计算成本高昂的SVD，仅利用四元数域内的矩阵乘法直接估计节点坐标。", "result": "仿真结果表明，与原始SMDS算法相比，所提出的方法显著提高了定位精度，尤其是在存在大量测量误差的情况下。所提出的方法在无需SVD的情况下也达到了可比的定位精度。", "conclusion": "所提出的四元数域超多维尺度变换（QD-SMDS）算法及其无需SVD的变体，为无线传感器网络提供了在复杂噪声环境下具有更高鲁棒性和精度的三维定位解决方案，并且在计算效率上有所提升。", "translation": "本文提出了一种用于无线传感器网络的低复杂度三维（3D）定位新算法，称为四元数域超多维尺度变换（QD-SMDS）。该算法基于对最初在实数域开发的SMDS的重构，利用四元数代数。通过将3D坐标表示为四元数，该方法构建了一个秩1格拉姆边缘核（GEK）矩阵，该矩阵集成了节点间的相对距离和角度信息，通过采用奇异值分解（SVD）的低秩截断增强了降噪效果，从而提高了对信息丢失的鲁棒性。为了进一步降低计算复杂度，我们还提出了一种QD-SMDS的变体，通过利用四元数域GEK矩阵的固有结构，消除了对计算成本高昂的SVD的需求。这种替代方法仅利用四元数域内的矩阵乘法直接估计节点坐标。仿真结果表明，与原始SMDS算法相比，所提出的方法显著提高了定位精度，尤其是在存在大量测量误差的情况下。所提出的方法在无需SVD的情况下也达到了可比的定位精度。", "summary": "本文提出了一种名为四元数域超多维尺度变换（QD-SMDS）的新型低复杂度三维定位算法，用于无线传感器网络。该算法通过使用四元数代数重新构建了多维尺度变换（SMDS），并将三维坐标表示为四元数，以构建一个集成距离和角度信息的格拉姆边缘核（GEK）矩阵，从而增强了降噪效果并提高了对信息丢失的鲁棒性。此外，为降低计算成本，提出了一种无需奇异值分解（SVD）的QD-SMDS变体。仿真结果显示，该方法在定位精度上显著优于原始SMDS，特别是在高测量误差环境下，且无需SVD也能保持相当的精度。", "keywords": "四元数, 三维定位, 无线传感器网络, 多维尺度变换, 鲁棒性", "comments": "本文的创新之处在于将传统的超多维尺度变换（SMDS）扩展到四元数域，有效地利用四元数代数来表示三维坐标并整合角度信息，从而提高了定位的鲁棒性。此外，提出了一种无需SVD的计算优化方案，显著降低了算法的复杂度，使其更适用于资源受限的无线传感器网络。这对于提高定位精度和效率具有重要意义。"}}
{"id": "2507.17728", "title": "Megrez2 Technical Report", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17728v1", "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17728v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Megrez2 技术报告", "tldr": "Megrez2是一种新型轻量级高性能语言模型架构，通过跨层专家共享和预门控路由实现参数减少和高效推理，在设备端部署方面表现出色，即使参数量较小也能达到或超越大型模型性能。", "motivation": "现有语言模型在设备端部署时面临参数量大、效率低下的挑战。该研究旨在开发一种轻量级、高性能且易于在设备本地部署的语言模型架构。", "method": "Megrez2引入了跨层专家共享机制，通过在相邻Transformer层间重用专家模块来显著减少总参数量；同时，它还结合了预门控路由，以实现内存高效的专家加载和更快的推理。作为该架构的首个实例，Megrez2-Preview模型在5万亿token语料库上进行了预训练，并通过监督微调和可验证奖励的强化学习进一步增强。", "result": "Megrez2-Preview模型仅需30亿激活参数和75亿存储参数，但在语言理解、指令遵循、数学推理和代码生成等多种任务上表现出与更大模型相当或更优的性能。这些结果证明了Megrez2架构在平衡准确性、效率和可部署性方面的有效性。", "conclusion": "Megrez2架构通过其创新的设计，成功地实现了高性能和轻量化的结合，使其成为资源受限的真实世界应用场景的有力选择。", "translation": "我们介绍了Megrez2，一种新颖的轻量级高性能语言模型架构，专为设备本地部署优化。Megrez2引入了一种新颖的跨层专家共享机制，通过在相邻Transformer层之间重用专家模块，显著减少了总参数数量，同时保持了模型的大部分容量。它还整合了预门控路由，实现了内存高效的专家加载和更快的推理。作为Megrez2架构的首次实例化，我们推出了Megrez2-Preview模型，该模型在一个5万亿token语料库上进行了预训练，并通过监督微调和基于可验证奖励的强化学习进一步增强。Megrez2-Preview仅需30亿激活参数和75亿存储参数，但在语言理解、指令遵循、数学推理和代码生成等广泛任务上展现出与更大模型相当或更优的性能。这些结果突显了Megrez2架构在实现准确性、效率和可部署性之间平衡的有效性，使其成为真实世界、资源受限应用的有力候选。", "summary": "Megrez2是一种为设备本地部署而优化的新型轻量级高性能语言模型架构。它通过独特的跨层专家共享机制有效减少参数量，并通过预门控路由提升推理效率。Megrez2-Preview作为其首个实例，在大量数据上预训练并经过微调，仅以30亿激活参数即在多项任务上展现出超越或媲美大型模型的性能，验证了其在准确性、效率和可部署性上的优秀平衡，适用于资源受限环境。", "keywords": "语言模型, 轻量级, 设备部署, 专家共享, 预门控路由", "comments": "Megrez2的创新之处在于其跨层专家共享机制和预门控路由，这有效地解决了大型模型在设备端部署的挑战。它在参数效率和性能之间找到了极佳的平衡点，对于推动AI模型在边缘设备上的应用具有重要意义。该架构有望为未来的轻量级模型设计提供新的思路。"}}
{"id": "2507.06565", "title": "A Mathematical Theory of Discursive Networks", "authors": ["Juan B. Gutiérrez"], "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      42 pages, 4 figures, 4 tables, 3 algorithm, 61 references", "url": "http://arxiv.org/abs/2507.06565v5", "summary": "Large language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any\nset of agents critique one another while a harmonizer merges their verdicts. We\nidentify an ethical transgression, epithesis, that occurs when humans fail to\nengage in the discursive network. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nconnecting imperfect ones into networks that enforce mutual accountability.", "comment": "42 pages, 4 figures, 4 tables, 3 algorithm, 61 references", "pdf_url": "http://arxiv.org/pdf/2507.06565v5", "cate": "cs.CL", "date": "2025-07-09", "updated": "2025-07-23", "AI": {"title_translation": "论述网络的数学理论", "tldr": "本文提出了一种论述网络的数学理论，将LLM和人类视为平等节点，并证明通过同行评审可以显著降低错误信息率，实现信息可靠性。", "motivation": "描述LLMs生成错误信息（invalidation）的机制，并探索如何提高这种新型媒介（人与LLM交互的论述网络）的可靠性。", "method": "将LLM与人类的写作互动定义为“论述网络”，其中人类和LLM是平等节点，跟踪言论的传播。将错误信息定义为“invalidation”，并识别其四种危害：偏离真相、自我修复、新造谣和外部检测。开发了一个通用的论述网络数学模型来分析错误率。提出并操作化了“同行评审”机制，通过开源的Flaws-of-Others (FOO) 算法实现，该算法是一个可配置的循环，其中代理相互批评，协调器合并裁决。", "result": "数学模型显示，仅受偏离真相和自我修复支配的网络会稳定在一个适度的错误率。即使每个虚假声明只有很小的同行评审机会，也能使系统转向以真相为主导的状态。提出了Flaws-of-Others (FOO) 算法作为同行评审的实现方式。识别出一种伦理越轨行为“epithesis”，即人类未能参与论述网络。", "conclusion": "在人与LLM交互的新型媒介中，可靠性并非来自完善单一模型，而是通过将不完善的模型连接成强制相互问责的网络来实现。", "translation": "大型语言模型（LLMs）将写作转变为人与软件之间的实时交流。我们将这种新媒介描述为一个论述网络，它将人与LLMs视为平等的节点，并追踪它们的言论如何流通。我们将错误信息的产生定义为失效（任何事实、逻辑或结构上的违背），并表明它遵循四种危害：偏离真相、自我修复、新造谣和外部检测。我们开发了一个通用的论述网络数学模型，该模型表明，一个仅受偏离真相和自我修复支配的网络会稳定在一个适度的错误率。即使每个虚假声明只有很小的同行评审机会，也能使系统转向以真相为主导的状态。我们通过开源的Flaws-of-Others（FOO）算法将同行评审付诸实践：这是一个可配置的循环，其中任何一组代理相互批评，同时协调器合并它们的裁决。我们识别出一种伦理越轨行为，即当人类未能参与论述网络时发生的“epithesis”。其启示是实践性和文化性的：在这种新媒介中的可靠性并非来自完善单一模型，而是通过将不完善的模型连接成强制相互问责的网络来实现。", "summary": "本文提出了一个关于人与大型语言模型（LLM）之间交互的“论述网络”的数学理论。该理论将人类和LLM视为网络中的平等节点，并定义了错误信息（invalidation）及其四种产生机制。通过数学模型，研究发现，在没有外部干预的情况下，网络会维持适度的错误率，但引入小概率的同行评审机制（例如通过FOO算法实现）可以显著提高信息的真实性。文章强调，未来LLM应用的可靠性应建立在模型间的相互问责网络而非单一模型的完善上。", "keywords": "论述网络, 大型语言模型, 错误信息, 同行评审, 数学模型", "comments": "本文创新性地将LLM与人类的交互视为一个“论述网络”，并从数学角度分析了信息传播中的错误产生和纠正机制。其提出的“同行评审”和FOO算法为提高LLM生成内容的可靠性提供了一个可行的框架，强调了去中心化、协作和相互问责的重要性，而非仅仅依赖于模型本身的改进。这对于理解和构建更可靠的AI辅助交流系统具有重要意义。"}}
{"id": "2507.17324", "title": "An Empirical Study on Virtual Reality Software Security Weaknesses", "authors": ["Yifan Xu", "Jinfu Chen", "Zhenyu Qi", "Huashan Chen", "Junyi Wang", "Pengfei Hu", "Feng Liu", "Sen He"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17324v1", "summary": "Virtual Reality (VR) has emerged as a transformative technology across\nindustries, yet its security weaknesses, including vulnerabilities, are\nunderinvestigated. This study investigates 334 VR projects hosted on GitHub,\nexamining 1,681 software security weaknesses to understand: what types of\nweaknesses are prevalent in VR software; {\\em when} and {\\em how} weaknesses\nare introduced; how long they have survived; and how they have been removed.\nDue to the limited availability of VR software security weaknesses in public\ndatabases (e.g., the National Vulnerability Database or NVD), we prepare the\n{first systematic} dataset of VR software security weaknesses by introducing a\nnovel framework to collect such weaknesses from GitHub commit data. Our\nempirical study on the dataset leads to useful insights, including: (i) VR\nweaknesses are heavily skewed toward user interface weaknesses, followed by\nresource-related weaknesses; (ii) VR development tools pose higher security\nrisks than VR applications; (iii) VR security weaknesses are often introduced\nat the VR software birth time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17324v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "虚拟现实软件安全弱点的一项实证研究", "tldr": "对GitHub上的334个VR项目进行了实证研究，发现了VR软件安全弱点的普遍类型、引入时机、生存时间及移除方式，并构建了首个VR软件安全弱点数据集。", "motivation": "虚拟现实（VR）作为一种变革性技术正在兴起，但其安全弱点（包括漏洞）尚未得到充分研究。由于公共数据库中VR软件安全弱点数据有限，因此需要进行深入调查。", "method": "本研究调查了GitHub上托管的334个VR项目，检查了1,681个软件安全弱点。研究引入了一个新颖的框架，通过收集GitHub提交数据来构建首个系统的VR软件安全弱点数据集。", "result": "1. VR弱点严重偏向用户界面弱点，其次是资源相关弱点。 2. VR开发工具比VR应用程序带来更高的安全风险。 3. VR安全弱点通常在VR软件诞生时引入。", "conclusion": "VR软件的安全弱点主要集中在用户界面和资源方面，且VR开发工具比应用更具风险，多数弱点在软件开发初期就已存在。", "translation": "虚拟现实（VR）作为一项变革性技术已在各行业兴起，但其安全弱点（包括漏洞）尚未得到充分研究。本研究调查了GitHub上托管的334个VR项目，检查了1,681个软件安全弱点，以了解：VR软件中普遍存在哪些类型的弱点；弱点何时以及如何引入；它们存在了多久；以及它们是如何被移除的。由于公共数据库（例如国家漏洞数据库或NVD）中VR软件安全弱点的可用性有限，我们通过引入一种新颖的框架从GitHub提交数据中收集此类弱点，从而准备了第一个系统的VR软件安全弱点数据集。我们对数据集的实证研究得出了有用的见解，包括：(i) VR弱点严重偏向用户界面弱点，其次是资源相关弱点；(ii) VR开发工具比VR应用程序带来更高的安全风险；(iii) VR安全弱点通常在VR软件诞生时引入。", "summary": "本研究对GitHub上的334个VR项目进行了实证分析，调查了1,681个软件安全弱点。由于现有公共数据库数据不足，研究构建了首个系统化的VR软件安全弱点数据集。主要发现包括：VR弱点多集中在用户界面和资源方面；VR开发工具的安全风险高于VR应用程序；以及VR安全弱点常在软件创建之初便已存在。", "keywords": "虚拟现实, 软件安全, 安全弱点, 实证研究, GitHub", "comments": "这项研究通过构建首个VR软件安全弱点数据集，填补了VR安全领域的数据空白，具有重要的开创性。其对弱点类型、引入时机和高风险区域（开发工具）的发现，为VR软件开发者和安全研究人员提供了宝贵的实践指导，有助于提升VR生态系统的整体安全性。"}}
{"id": "2507.17699", "title": "Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations", "authors": ["Zhao Song", "Song Yue", "Jiahao Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17699v1", "summary": "Large Reasoning Models (LRMs) have become a central focus in today's large\nlanguage model (LLM) research, where models are designed to output a\nstep-by-step thinking process before arriving at a final answer to handle\ncomplex reasoning tasks. Despite their promise, recent empirical studies (e.g.,\n[Shojaee et al., 2025] from Apple) suggest that this thinking process may not\nactually enhance reasoning ability, where LLMs without explicit reasoning\nactually outperform LRMs on tasks with low or high complexity. In this work, we\nrevisit these findings and investigate whether the limitations of LRMs persist\nwhen tool augmentations are introduced. We incorporate two types of tools,\nPython interpreters and scratchpads, and evaluate three representative LLMs and\ntheir LRM counterparts on Apple's benchmark reasoning puzzles. Our results show\nthat, with proper tool use, LRMs consistently outperform their non-reasoning\ncounterparts across all levels of task complexity. These findings challenge the\nrecent narrative that reasoning is an illusion and highlight the potential of\ntool-augmented LRMs for solving complex problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17699v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "思维并非幻象：通过工具增强克服推理模型的局限性", "tldr": "本文通过引入工具增强，反驳了大型推理模型（LRMs）的思维过程无效的观点，并证明了工具增强的LRMs在解决复杂问题上的潜力。", "motivation": "当前研究表明大型推理模型（LRMs）的逐步思考过程可能无法增强推理能力，甚至在某些任务上表现不如没有显式推理的LLMs。本文旨在重新审视这些发现，并探究引入工具增强后LRMs的局限性是否依然存在。", "method": "研究引入了Python解释器和草稿本两种工具，并评估了三种代表性LLMs及其LRM版本在苹果的基准推理谜题上的表现。", "result": "结果显示，通过适当的工具使用，LRMs在所有任务复杂性级别上都持续优于其非推理的对应模型。", "conclusion": "本文挑战了近期关于推理是幻象的说法，并强调了工具增强型LRMs在解决复杂问题方面的潜力。", "translation": "大型推理模型（LRMs）已成为当前大型语言模型（LLM）研究的焦点，它们被设计为在得出最终答案之前输出分步思考过程，以处理复杂的推理任务。尽管前景广阔，但最近的实证研究（例如，苹果公司[Shojaee et al., 2025]）表明，这种思考过程可能实际上并未增强推理能力，在低或高复杂度的任务上，没有显式推理的LLMs甚至表现优于LRMs。在这项工作中，我们重新审视了这些发现，并研究了当引入工具增强时，LRMs的局限性是否依然存在。我们结合了两种类型的工具：Python解释器和草稿本，并在苹果的基准推理谜题上评估了三种代表性LLMs及其LRM对应模型。我们的结果表明，通过适当的工具使用，LRMs在所有任务复杂性级别上都持续优于其非推理的对应模型。这些发现挑战了近期关于推理是幻象的说法，并强调了工具增强型LRMs在解决复杂问题方面的潜力。", "summary": "本文旨在反驳大型推理模型（LRMs）的思维过程无效的观点。通过引入Python解释器和草稿本等工具增强，研究发现LRMs在所有复杂性级别的任务中均持续优于其非推理的对应模型。这表明工具增强能够有效提升LRMs的推理能力，挑战了当前关于推理是幻象的看法，并突出了工具增强型LRMs在解决复杂问题上的潜力。", "keywords": "大型推理模型, 工具增强, 推理能力, LLM, Python解释器", "comments": "这项研究通过引入工具增强，为大型推理模型（LRMs）的有效性提供了新的视角，挑战了当前关于“推理是幻象”的流行观点。其创新之处在于将外部工具（如Python解释器和草稿本）与LRMs结合，有效提升了模型的推理表现。这对于未来LLM在复杂任务上的应用具有重要指导意义，尤其是在需要精确计算或逻辑推理的场景。"}}
{"id": "2507.17711", "title": "Reasoning about Rare-Event Reachability in Stochastic Vector Addition Systems via Affine Vector Spaces", "authors": ["Joshua Jeppson", "Landon Taylor", "Bingqing Hu", "Zhen Zhang"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17711v1", "summary": "Rare events in Stochastic Vector Addition System (VAS) are of significant\ninterest because, while extremely unlikely, they may represent undesirable\nbehavior that can have adverse effects. Their low probabilities and potentially\nextremely large state spaces challenge existing probabilistic model checking\nand stochastic rare-event simulation techniques. In particular, in Chemical\nReaction Networks (CRNs), a chemical kinetic language often represented as VAS,\nrare event effects may be pathological. We present two novel heuristics for\npriority-first partial state space expansion and trace generation tuned to the\ntransient analysis of rare-event probability in VAS: Iterative Subspace\nReduction (ISR) and Single Distance Priority (SDP). Both methods construct a\nclosed vector space containing all solution states. SDP then simply prioritizes\nshorter distances to this ``solution space'', while ISR constructs a set of\nnested subspaces, where short and highly-probable satisfying traces are likely\nto pass through in sequence. The resulting partial state graph from each method\ncontains likely traces to rare-event states, allowing efficient probabilistic\nmodel checking to compute a lower-bound probability of a rare event of\ninterest. These methods are deterministic, fast, and demonstrate marked\nperformance on challenging CRN models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17711v1", "cate": "cs.FL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过仿射向量空间推理随机向量加法系统中稀有事件的可达性", "tldr": "本文提出了两种新颖的启发式方法（ISR和SDP），用于在随机向量加法系统（VAS）中高效分析稀有事件的可达性，解决了现有方法在处理低概率和大规模状态空间时的挑战，并在化学反应网络（CRN）模型上表现出色。", "motivation": "随机向量加法系统（VAS）中的稀有事件虽然极不可能发生，但可能代表具有不利影响的不良行为，因此具有重要意义。然而，它们的低概率和潜在的极大状态空间对现有的概率模型检测和随机稀有事件模拟技术构成了挑战，尤其是在化学反应网络（CRN）中，稀有事件的影响可能是病态的。", "method": "本文提出了两种新颖的启发式方法：迭代子空间约简（ISR）和单距离优先级（SDP），用于优先进行部分状态空间扩展和轨迹生成，以适应VAS中稀有事件概率的瞬态分析。这两种方法都构建了一个包含所有解状态的闭合向量空间。SDP简单地优先考虑到达此“解空间”的较短距离，而ISR则构建了一组嵌套子空间，其中短且高概率的满足条件轨迹可能按顺序通过。", "result": "通过ISR和SDP方法生成的局部状态图包含通往稀有事件状态的可能轨迹，从而允许高效的概率模型检测来计算感兴趣的稀有事件的概率下限。这些方法是确定性的、快速的，并在具有挑战性的化学反应网络（CRN）模型上表现出显著的性能。", "conclusion": "本文提出的ISR和SDP两种新颖的启发式方法能够有效地分析随机向量加法系统（VAS）中稀有事件的可达性，克服了现有技术在处理低概率和大规模状态空间时的困难。它们通过构建包含解状态的向量空间并优先生成可能轨迹，实现了高效的概率下限计算，并在实际模型中展现出卓越的性能。", "translation": "随机向量加法系统（VAS）中的稀有事件具有重要意义，因为它们虽然极不可能发生，但可能代表具有不利影响的不良行为。它们的低概率和潜在的极大状态空间对现有的概率模型检测和随机稀有事件模拟技术构成了挑战。特别是，在化学反应网络（CRN）中，一种通常表示为VAS的化学动力学语言，稀有事件的影响可能是病态的。我们提出了两种新颖的启发式方法，用于优先进行部分状态空间扩展和轨迹生成，以适应VAS中稀有事件概率的瞬态分析：迭代子空间约简（ISR）和单距离优先级（SDP）。这两种方法都构建了一个包含所有解状态的闭合向量空间。SDP简单地优先考虑到达此“解空间”的较短距离，而ISR则构建了一组嵌套子空间，其中短且高概率的满足条件轨迹可能按顺序通过。每种方法产生的局部状态图都包含通往稀有事件状态的可能轨迹，从而允许高效的概率模型检测来计算感兴趣的稀有事件的概率下限。这些方法是确定性的、快速的，并在具有挑战性的CRN模型上表现出显著的性能。", "summary": "本文针对随机向量加法系统（VAS）中稀有事件的可达性分析提出了两种新颖的启发式方法：迭代子空间约简（ISR）和单距离优先级（SDP）。鉴于稀有事件的低概率和巨大状态空间对现有技术造成的挑战，特别是对于化学反应网络（CRN），这两种方法通过构建包含所有解状态的闭合向量空间，并优化轨迹生成过程，从而实现高效的概率下限计算。实验证明，这些方法是确定性、快速的，并在复杂的CRN模型上表现出显著的性能。", "keywords": "稀有事件, 随机向量加法系统, 可达性, 迭代子空间约简, 单距离优先级", "comments": "该论文创新性地将仿射向量空间的概念引入到稀有事件可达性分析中，提出了ISR和SDP两种针对性强的启发式方法。其重要性在于有效解决了传统概率模型检测和模拟技术在处理低概率和大规模状态空间问题上的局限性，特别是在生物和化学系统建模（如CRN）中具有实际应用价值。方法的确定性和速度是其显著优势。"}}
{"id": "2409.10876", "title": "Coordinate-based Speed of Sound Recovery for Aberration-Corrected Photoacoustic Computed Tomography", "authors": ["Tianao Li", "Manxiu Cui", "Cheng Ma", "Emma Alexander"], "categories": ["eess.IV", "cs.CV", "eess.SP"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2409.10876v4", "summary": "Photoacoustic computed tomography (PACT) is a non-invasive imaging modality,\nsimilar to ultrasound, with wide-ranging medical applications. Conventional\nPACT images are degraded by wavefront distortion caused by the heterogeneous\nspeed of sound (SOS) in tissue. Accounting for these effects can improve image\nquality and provide medically useful information, but measuring the SOS\ndirectly is burdensome and the existing joint reconstruction method is\ncomputationally expensive. Traditional supervised learning techniques are\ncurrently inaccessible in this data-starved domain. In this work, we introduce\nan efficient, self-supervised joint reconstruction method that recovers SOS and\nhigh-quality images for ring array PACT systems. To solve this semi-blind\ninverse problem, we parametrize the SOS using either a pixel grid or a neural\nfield (NF) and update it directly by backpropagating the gradients through a\ndifferentiable imaging forward model. Our method removes SOS aberrations more\naccurately and 35x faster than the current SOTA. We demonstrate the success of\nour method quantitatively in simulation and qualitatively on\nexperimentally-collected and in vivo data. Our code and synthetic numerical\nphantoms are available on our project page:\nhttps://lukeli0425.github.io/Coord-SoS-PACT/.", "comment": "Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2409.10876v4", "cate": "eess.IV", "date": "2024-09-17", "updated": "2025-07-22", "AI": {"title_translation": "基于坐标的声速恢复用于像差校正光声计算断层扫描", "tldr": "该论文提出了一种高效的自监督方法，用于校正光声计算断层扫描（PACT）图像中的声速像差，比现有技术更准确、速度快35倍。", "motivation": "传统光声计算断层扫描（PACT）图像因组织中不均匀的声速（SOS）而降质。虽然校正这些影响可以改善图像质量，但直接测量声速很繁琐，并且现有的联合重建方法计算成本高昂。此外，在该数据稀缺领域，传统的监督学习技术目前无法应用。", "method": "本文提出了一种高效的自监督联合重建方法，用于环形阵列PACT系统，以恢复声速和高质量图像。为了解决这个半盲逆问题，该方法使用像素网格或神经场（NF）来参数化声速，并通过可微分成像正向模型反向传播梯度来直接更新它。", "result": "所提出的方法比现有最先进技术更准确地消除了声速像差，并且速度快35倍。该方法在模拟中进行了定量验证，并在实验收集和体内数据上进行了定性验证，均取得了成功。", "conclusion": "该论文成功开发了一种高效、准确的自监督PACT声速恢复方法，显著提高了图像质量和计算效率，解决了该领域中的关键挑战。", "translation": "光声计算断层扫描（PACT）是一种非侵入性成像模式，类似于超声，具有广泛的医学应用。传统的PACT图像会因组织中不均匀的声速（SOS）引起的波前畸变而降质。考虑这些影响可以改善图像质量并提供医学上有用的信息，但直接测量声速很繁琐，并且现有的联合重建方法计算成本高昂。传统的监督学习技术目前在该数据稀缺领域无法应用。在这项工作中，我们介绍了一种高效的自监督联合重建方法，可以为环形阵列PACT系统恢复声速和高质量图像。为了解决这个半盲逆问题，我们使用像素网格或神经场（NF）来参数化声速，并通过可微分成像正向模型反向传播梯度来直接更新它。我们的方法比目前的SOTA更准确地消除了声速像差，并且速度快35倍。我们在模拟中定量地证明了我们方法的成功，并在实验收集和体内数据上进行了定性证明。我们的代码和合成数值体模可在我们的项目页面上获取：https://lukeli0425.github.io/Coord-SoS-PACT/。", "summary": "本研究提出了一种高效的自监督联合重建方法，用于光声计算断层扫描（PACT）系统，以解决图像因组织中不均匀声速（SOS）导致的像差问题。该方法通过使用像素网格或神经场参数化SOS，并通过可微分成像正向模型进行梯度反向传播来直接更新SOS。实验结果表明，该方法能够比现有技术更准确地消除SOS像差，并且速度提高35倍，在模拟、实验和体内数据上均表现出色。", "keywords": "光声计算断层扫描, 声速, 像差校正, 自监督学习, 神经场", "comments": "该论文在PACT成像领域取得了重要进展，解决了声速像差这一关键问题。其提出的自监督方法在数据稀缺的领域具有创新性，并且计算速度比现有技术快35倍，对实际应用具有重大影响。利用可微分成像和神经场进行声速参数化是解决半盲逆问题的巧妙方案。"}}
{"id": "2412.10595", "title": "Recommendation and Temptation", "authors": ["Md Sanzeed Anwar", "Paramveer S. Dhillon", "Grant Schoenebeck"], "categories": ["cs.IR", "cs.CY", "cs.GT"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Published in Proceedings of the 19th ACM Conference on Recommender Systems (RecSys 2025)", "url": "http://arxiv.org/abs/2412.10595v2", "summary": "Traditional recommender systems based on revealed preferences often fail to\ncapture the fundamental duality in user behavior, where consumption choices are\ndriven by both inherent value (enrichment) and instant appeal (temptation).\nConsequently, these systems may generate recommendations that prioritize\nshort-term engagement over long-lasting user satisfaction. We propose a novel\nrecommender design that explicitly models the tension between enrichment and\ntemptation. We introduce a behavioral model that accounts for how both\nenrichment and temptation influence user choices, while incorporating the\nreality of off-platform alternatives. Building on this model, we formulate a\nnovel recommendation objective aligned with maximizing consumed enrichment and\nprove the optimality of a locally greedy recommendation strategy. Finally, we\npresent an estimation framework that leverages the distinction between explicit\nuser feedback and implicit choice data while making minimal assumptions about\noff-platform options. Through comprehensive evaluation using both synthetic\nsimulations and real-world data from the MovieLens dataset, we demonstrate that\nour approach consistently outperforms competitive baselines that ignore\ntemptation dynamics either by assuming revealed preferences or recommending\nsolely based on enrichment. Our work represents a paradigm shift toward more\nnuanced and user-centric recommender design, with significant implications for\ndeveloping responsible AI systems that genuinely serve users' long-term\ninterests rather than merely maximizing engagement.", "comment": "Published in Proceedings of the 19th ACM Conference on Recommender\n  Systems (RecSys 2025)", "pdf_url": "http://arxiv.org/pdf/2412.10595v2", "cate": "cs.IR", "date": "2024-12-13", "updated": "2025-07-23", "AI": {"title_translation": "推荐与诱惑", "tldr": "传统推荐系统未能捕捉用户行为中内在价值（丰富性）和即时吸引力（诱惑）之间的二元性。本文提出一种新的推荐系统设计，明确建模丰富性和诱惑之间的张力，并证明了局部贪婪推荐策略的最优性。实验证明，该方法优于忽略诱惑动态的基线，代表了更细致、以用户为中心的推荐设计范式。", "motivation": "传统推荐系统基于显式偏好，未能捕捉用户行为中内在价值（丰富性）和即时吸引力（诱惑）之间的根本二元性，导致推荐系统可能优先考虑短期参与度而非长期用户满意度。", "method": "本文提出一种新型推荐系统设计，明确建模丰富性和诱惑之间的张力。引入一个行为模型，解释丰富性和诱惑如何影响用户选择，并考虑平台外替代方案。在此基础上，提出一个旨在最大化已消费丰富度的新推荐目标，并证明了局部贪婪推荐策略的最优性。最后，提出了一个估计框架，利用显式用户反馈和隐式选择数据之间的区别，并对平台外选项做出最小假设。", "result": "通过合成模拟和MovieLens数据集的真实世界数据进行的全面评估表明，本文提出的方法始终优于那些通过假设显式偏好或仅基于丰富性进行推荐而忽略诱惑动态的竞争基线。", "conclusion": "本文的工作代表了向更细致、以用户为中心的推荐设计范式的转变，对于开发真正服务用户长期利益而非仅仅最大化参与度的负责任AI系统具有重要意义。", "translation": "传统推荐系统基于显式偏好，往往未能捕捉用户行为中的根本二元性，即消费选择既受内在价值（丰富性）驱动，也受即时吸引力（诱惑）驱动。因此，这些系统可能会生成优先考虑短期参与度而非长期用户满意度的推荐。我们提出了一种新颖的推荐器设计，明确建模丰富性和诱惑之间的张力。我们引入了一个行为模型，该模型解释了丰富性和诱惑如何影响用户选择，同时纳入了平台外替代方案的现实情况。在此模型的基础上，我们制定了一个新的推荐目标，旨在最大化已消费的丰富性，并证明了局部贪婪推荐策略的最优性。最后，我们提出了一个估计框架，该框架利用了显式用户反馈和隐式选择数据之间的区别，同时对平台外选项做出了最小假设。通过使用合成模拟和来自MovieLens数据集的真实世界数据进行的全面评估，我们证明了我们的方法始终优于那些通过假设显式偏好或仅基于丰富性进行推荐而忽略诱惑动态的竞争基线。我们的工作代表了向更细致、以用户为中心的推荐设计范式的转变，对于开发真正服务用户长期利益而非仅仅最大化参与度的负责任AI系统具有重要意义。", "summary": "本研究提出了一种新型推荐系统设计，旨在解决传统推荐系统未能捕捉用户行为中“丰富性”（内在价值）和“诱惑”（即时吸引力）二元性的问题。文章引入一个行为模型来解释这两种因素如何影响用户选择，并考虑了平台外选项。在此基础上，提出了一种最大化已消费丰富度的推荐目标，并证明了局部贪婪策略的最优性。通过合成数据和真实世界数据集的实验验证，该方法显著优于现有基线，标志着推荐系统向更以用户为中心、更负责任的方向发展。", "keywords": "推荐系统, 诱惑, 丰富性, 用户行为, 负责任AI", "comments": "本文创新性地将“诱惑”这一概念引入推荐系统，解决了传统系统只关注显式偏好而忽略用户行为复杂性的局限。通过建模丰富性和诱惑的张力，并考虑平台外替代方案，提出了一个更符合用户长期利益的推荐框架。这对于开发负责任的AI系统具有重要意义，因为它超越了单纯追求短期参与度的目标。"}}
{"id": "2507.01630", "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.01630v2", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. The sources code are available at\nhttps://github.com/YuxiaoWang-AI/P3HOT.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.01630v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-23", "AI": {"title_translation": "用于HOT预测的提示引导和人体近距离感知与区域联合损失", "tldr": "P3HOT通过结合提示引导、人体近距离感知和区域联合损失，显著提升了人-物体接触（HOT）检测的性能，并在两个基准数据集上取得了最先进的结果。", "motivation": "当前的人-物体接触（HOT）检测模型仅限于单一图像类型，常常导致在交互较少区域的过度分割，并且难以在特定区域内保持类别一致性。", "method": "本文提出了一个名为P3HOT的HOT框架，它融合了提示引导和人体近距离感知。首先，利用语义驱动的提示机制，根据图像和文本之间的关联将网络注意力导向相关区域。其次，采用人体近距离感知机制，通过可学习参数动态感知人体周围的关键深度范围，有效消除预期没有交互的区域，提供准三维视角以解决二维视角下人与物体重叠的不确定性。此外，还创建了一种新的区域联合损失（RJLoss）来抑制同一区域内的异常类别。为解决现有方法在处理负样本方面的不足，引入了一种新的评估指标“AD-Acc.”。", "result": "实验结果表明，所提出的方法在两个基准数据集上的四项指标上均达到了最先进的性能。具体而言，在HOT-Annotated数据集上，模型在SC-Acc.、mIoU、wIoU和AD-Acc.指标上分别取得了0.7、2.0、1.6和11.0的提升。", "conclusion": "P3HOT框架通过结合语义驱动的提示机制、人体近距离感知和区域联合损失，有效解决了现有HOT检测模型在过度分割和类别一致性方面的限制，并在多项关键指标上实现了显著的性能提升，达到了最先进水平。", "translation": "人体-物体接触（HOT）检测任务涉及识别与物体接触的人体特定区域。然而，当前模型仅限于一种图像类型，常常导致在交互较少区域的过度分割，并且难以在特定区域内保持类别一致性。为了解决这个问题，本文提出了一个名为P3HOT的HOT框架，它融合了提示引导和人体近距离感知。首先，我们利用语义驱动的提示机制，根据图像和文本之间的关联将网络的注意力导向相关区域。然后，采用人体近距离感知机制，利用可学习参数动态感知人体周围的关键深度范围，有效消除预期没有交互的区域。计算深度解决了二维视角下人与物体重叠的不确定性，提供了准三维视角。此外，还创建了一种新的区域联合损失（RJLoss）来抑制同一区域内的异常类别。为了解决现有方法在处理负样本方面的不足，引入了一种新的评估指标“AD-Acc.”。全面的实验结果表明，我们的方法在两个基准数据集上的四项指标上均达到了最先进的性能。具体而言，我们的模型在HOT-Annotated数据集上，在SC-Acc.、mIoU、wIoU和AD-Acc.指标上分别取得了0.7↑、2.0↑、1.6↑和11.0↑的提升。源代码可在https://github.com/YuxiaoWang-AI/P3HOT获取。", "summary": "本文针对人-物体接触（HOT）检测中现有模型在单一图像类型限制、过度分割和类别一致性差的问题，提出了P3HOT框架。该框架结合了语义驱动的提示机制以引导网络注意力，以及人体近距离感知机制以动态感知深度范围并消除非交互区域，从而提供准三维视角。此外，引入了区域联合损失（RJLoss）来抑制异常类别，并提出了新的评估指标“AD-Acc.”以更好地处理负样本。实验结果表明，P3HOT在多个基准数据集上实现了最先进的性能，显著提升了HOT检测的准确性。", "keywords": "人-物体接触, HOT检测, 提示引导, 深度感知, 区域联合损失", "comments": "P3HOT的创新之处在于其多方面的综合方法。通过引入语义驱动的提示机制，它能更智能地聚焦于相关区域，避免了传统方法的过度分割问题。人体近距离感知机制利用深度信息，有效地将2D问题提升到准3D视角来解决模糊性，这是一个重要的进步。此外，区域联合损失和新的评估指标AD-Acc.也显示了对HOT检测任务细节的深刻理解和有效解决。这些结合使其在复杂的人-物体交互场景中表现出色，具有较高的实用价值。"}}
{"id": "2507.17134", "title": "Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating LLMs and Blockchain for Transparent Coordination", "authors": ["Mariam ALMutairi", "Hyungmin Kim"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figure", "url": "http://arxiv.org/abs/2507.17134v1", "summary": "Global health emergencies, such as the COVID-19 pandemic, have exposed\ncritical weaknesses in traditional medical supply chains, including\ninefficiencies in resource allocation, lack of transparency, and poor\nadaptability to dynamic disruptions. This paper presents a novel hybrid\nframework that integrates blockchain technology with a decentralized, large\nlanguage model (LLM) powered multi-agent negotiation system to enhance the\nresilience and accountability of medical supply chains during crises. In this\nsystem, autonomous agents-representing manufacturers, distributors, and\nhealthcare institutions-engage in structured, context-aware negotiation and\ndecision-making processes facilitated by LLMs, enabling rapid and ethical\nallocation of scarce medical resources. The off-chain agent layer supports\nadaptive reasoning and local decision-making, while the on-chain blockchain\nlayer ensures immutable, transparent, and auditable enforcement of decisions\nvia smart contracts. The framework also incorporates a formal cross-layer\ncommunication protocol to bridge decentralized negotiation with institutional\nenforcement. A simulation environment emulating pandemic scenarios evaluates\nthe system's performance, demonstrating improvements in negotiation efficiency,\nfairness of allocation, supply chain responsiveness, and auditability. This\nresearch contributes an innovative approach that synergizes blockchain trust\nguarantees with the adaptive intelligence of LLM-driven agents, providing a\nrobust and scalable solution for critical supply chain coordination under\nuncertainty.", "comment": "11 pages, 6 figure", "pdf_url": "http://arxiv.org/pdf/2507.17134v1", "cate": "cs.MA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "医疗供应链的弹性多智能体协商：整合大型语言模型和区块链以实现透明协调", "tldr": "本文提出了一种结合区块链和大型语言模型（LLM）驱动的多智能体协商系统的新型混合框架，旨在增强医疗供应链在危机期间的弹性、透明度和可追溯性，并通过模拟验证了其在资源分配效率、公平性、响应性和可审计性方面的改进。", "motivation": "全球健康紧急情况（如COVID-19大流行）暴露了传统医疗供应链的关键弱点，包括资源分配效率低下、缺乏透明度以及对动态中断的适应性差。本研究旨在解决这些问题，提高医疗供应链在危机期间的弹性。", "method": "本文提出了一种结合区块链技术和去中心化、由大型语言模型（LLM）驱动的多智能体协商系统的新型混合框架。该系统中的自主代理（代表制造商、分销商和医疗机构）通过LLM促进结构化、上下文感知的协商和决策。链下代理层支持自适应推理和本地决策，链上区块链层通过智能合约确保决策的不可变、透明和可审计执行。框架还包含一个正式的跨层通信协议。", "result": "模拟环境评估了该系统的性能，结果表明在协商效率、分配公平性、供应链响应能力和可审计性方面都有所改进。", "conclusion": "这项研究贡献了一种创新方法，它将区块链的信任保证与LLM驱动智能体的自适应智能相结合，为不确定性下的关键供应链协调提供了一个强大且可扩展的解决方案。", "translation": "全球健康紧急情况，例如COVID-19大流行，暴露了传统医疗供应链的关键弱点，包括资源分配效率低下、缺乏透明度以及对动态中断的适应性差。本文提出了一种新颖的混合框架，该框架将区块链技术与去中心化、由大型语言模型（LLM）驱动的多智能体协商系统相结合，以增强医疗供应链在危机期间的弹性和责任性。在该系统中，代表制造商、分销商和医疗机构的自主代理通过LLM促进结构化、上下文感知的协商和决策过程，从而实现稀缺医疗资源的快速和道德分配。链下代理层支持自适应推理和本地决策，而链上区块链层通过智能合约确保决策的不可变、透明和可审计执行。该框架还包含一个正式的跨层通信协议，以弥合去中心化协商与机构执行之间的鸿沟。模拟环境模拟大流行情景，评估了该系统的性能，展示了在协商效率、分配公平性、供应链响应能力和可审计性方面的改进。这项研究贡献了一种创新方法，它将区块链的信任保证与LLM驱动智能体的自适应智能相结合，为不确定性下的关键供应链协调提供了一个强大且可扩展的解决方案。", "summary": "本文针对全球健康紧急情况中传统医疗供应链的弱点，提出了一种创新的混合框架。该框架将区块链技术与基于大型语言模型（LLM）的去中心化多智能体协商系统相结合，旨在提升医疗供应链的弹性、透明度和可追溯性。系统中的自主代理利用LLM进行资源协商和决策，同时区块链确保决策的不可变性与可审计性。通过模拟验证，该系统在协商效率、资源分配公平性、供应链响应能力及审计性方面均有显著提升，为不确定性下的供应链协调提供了强大且可扩展的解决方案。", "keywords": "医疗供应链, 多智能体协商, 大型语言模型, 区块链, 弹性", "comments": "这项研究的创新之处在于其独特地整合了大型语言模型（LLMs）的智能决策能力与区块链的透明和不可篡改特性，以解决医疗供应链在危机管理中的核心挑战。通过多智能体协商，它提供了一个去中心化且适应性强的解决方案，有望显著提高紧急情况下稀缺医疗资源的分配效率和公平性，同时增强问责制。"}}
{"id": "2507.17056", "title": "Pragmatic Policy Development via Interpretable Behavior Cloning", "authors": ["Anton Matsson", "Yaochen Rao", "Heather J. Litman", "Fredrik D. Johansson"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17056v1", "summary": "Offline reinforcement learning (RL) holds great promise for deriving optimal\npolicies from observational data, but challenges related to interpretability\nand evaluation limit its practical use in safety-critical domains.\nInterpretability is hindered by the black-box nature of unconstrained RL\npolicies, while evaluation -- typically performed off-policy -- is sensitive to\nlarge deviations from the data-collecting behavior policy, especially when\nusing methods based on importance sampling. To address these challenges, we\npropose a simple yet practical alternative: deriving treatment policies from\nthe most frequently chosen actions in each patient state, as estimated by an\ninterpretable model of the behavior policy. By using a tree-based model, which\nis specifically designed to exploit patterns in the data, we obtain a natural\ngrouping of states with respect to treatment. The tree structure ensures\ninterpretability by design, while varying the number of actions considered\ncontrols the degree of overlap with the behavior policy, enabling reliable\noff-policy evaluation. This pragmatic approach to policy development\nstandardizes frequent treatment patterns, capturing the collective clinical\njudgment embedded in the data. Using real-world examples in rheumatoid\narthritis and sepsis care, we demonstrate that policies derived under this\nframework can outperform current practice, offering interpretable alternatives\nto those obtained via offline RL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17056v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "通过可解释的行为克隆进行务实的策略开发", "tldr": "该论文提出了一种务实的策略开发方法，通过可解释的树状模型从观察数据中提取最频繁的行动，以解决离线强化学习在可解释性和评估方面的挑战，并在实际医疗案例中展示了其优于现有实践的性能。", "motivation": "离线强化学习在从观察数据中推导最优策略方面潜力巨大，但在安全关键领域，其可解释性和评估方面的挑战限制了其实际应用。具体来说，不受约束的强化学习策略的黑盒性质阻碍了可解释性，而脱策略评估对与数据收集行为策略的偏差敏感。", "method": "我们提出了一种简单而实用的替代方法：通过行为策略的可解释模型，从每个患者状态中最常选择的行动中推导出治疗策略。通过使用专门设计用于利用数据模式的树状模型，我们获得了关于治疗的状态自然分组。树结构确保了固有的可解释性，同时通过改变考虑的行动数量来控制与行为策略的重叠程度，从而实现可靠的脱策略评估。", "result": "在类风湿关节炎和败血症护理的真实世界案例中，我们证明了在此框架下推导出的策略可以优于当前实践，并提供了比离线强化学习获得策略更具可解释性的替代方案。", "conclusion": "这种务实的策略开发方法标准化了频繁的治疗模式，捕获了数据中嵌入的集体临床判断，并提供了比离线强化学习更具可解释性且表现更优的替代方案。", "translation": "离线强化学习（RL）在从观察数据中推导最优策略方面潜力巨大，但可解释性和评估方面的挑战限制了其在安全关键领域的实际应用。可解释性受限于无约束RL策略的黑盒性质，而评估——通常是脱策略执行——对与数据收集行为策略的较大偏差敏感，尤其是在使用基于重要性采样的S方法时。为了解决这些挑战，我们提出了一种简单而实用的替代方案：通过行为策略的可解释模型，从每个患者状态中最常选择的行动中推导出治疗策略。通过使用专门设计用于利用数据模式的树状模型，我们获得了关于治疗的状态自然分组。树结构确保了固有的可解释性，同时改变考虑的行动数量可以控制与行为策略的重叠程度，从而实现可靠的脱策略评估。这种务实的策略开发方法标准化了频繁的治疗模式，捕获了数据中嵌入的集体临床判断。在类风湿关节炎和败血症护理的真实世界案例中，我们证明了在此框架下推导出的策略可以优于当前实践，并提供了比离线RL获得策略更具可解释性的替代方案。", "summary": "本研究提出了一种通过可解释行为克隆进行务实策略开发的方法，旨在解决离线强化学习在可解释性和评估方面的局限性。该方法利用树状模型从观察数据中识别最频繁的行动模式，从而生成易于理解且能可靠评估的治疗策略。在类风湿关节炎和败血症的真实世界应用中，该框架下导出的策略不仅优于现有实践，还提供了比传统离线强化学习更具透明度的替代方案，有效捕获了临床判断。", "keywords": "离线强化学习, 可解释性, 行为克隆, 策略开发, 树状模型", "comments": "该论文的创新之处在于提出了一种简单而实用的方法，通过行为克隆和树状模型来解决离线强化学习在可解释性和评估方面的核心挑战。其重要性体现在为安全关键领域（如医疗保健）提供了更透明、更可靠的决策支持工具，克服了传统黑盒RL的局限性。通过将临床判断融入模型设计，该方法不仅提高了策略的实用性，也增强了其在实际应用中的接受度。"}}
{"id": "2507.17208", "title": "SLASH: Self-Supervised Speech Pitch Estimation Leveraging DSP-derived Absolute Pitch", "authors": ["Ryo Terashima", "Yuma Shirahata", "Masaya Kawamura"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to INTERSPEECH 2025", "url": "http://arxiv.org/abs/2507.17208v1", "summary": "We present SLASH, a pitch estimation method of speech signals based on\nself-supervised learning (SSL). To enhance the performance of conventional\nSSL-based approaches that primarily depend on the relative pitch difference\nderived from pitch shifting, our method incorporates absolute pitch values by\n1) introducing a prior pitch distribution derived from digital signal\nprocessing (DSP), and 2) optimizing absolute pitch through gradient descent\nwith a loss between the target and differentiable DSP-derived spectrograms. To\nstabilize the optimization, a novel spectrogram generation method is used that\nskips complicated waveform generation. In addition, the aperiodic components in\nspeech are accurately predicted through differentiable DSP, enhancing the\nmethod's applicability to speech signal processing. Experimental results showed\nthat the proposed method outperformed both baseline DSP and SSL-based pitch\nestimation methods, attributed to the effective integration of SSL and DSP.", "comment": "Accepted to INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.17208v1", "cate": "eess.AS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SLASH：利用DSP导出的绝对音高进行自监督语音音高估计", "tldr": "SLASH是一种基于自监督学习的语音音高估计方法，通过结合DSP导出的绝对音高来提升性能，优于传统方法。", "motivation": "传统的自监督学习（SSL）音高估计方法主要依赖于音高偏移产生的相对音高差，这限制了其性能。该研究的动机是为了通过引入绝对音高值来增强这些方法的表现。", "method": "SLASH通过以下方式结合绝对音高：1）引入源自数字信号处理（DSP）的先验音高分布；2）通过目标与可微分DSP导出的频谱图之间的损失，利用梯度下降优化绝对音高。为了稳定优化，该方法使用了一种跳过复杂波形生成的新颖频谱图生成方法，并通过可微分DSP准确预测语音中的非周期成分。", "result": "实验结果表明，所提出的SLASH方法优于基线DSP和基于SSL的音高估计方法。", "conclusion": "SLASH方法通过有效整合自监督学习（SSL）和数字信号处理（DSP），在语音音高估计方面取得了优越的性能。", "translation": "我们提出了SLASH，一种基于自监督学习（SSL）的语音信号音高估计方法。为了提高主要依赖于音高偏移产生的相对音高差的传统基于SSL的方法的性能，我们的方法通过以下方式整合了绝对音高值：1）引入了源自数字信号处理（DSP）的先验音高分布，以及2）通过目标与可微分DSP导出的频谱图之间的损失，通过梯度下降优化绝对音高。为了稳定优化，使用了跳过复杂波形生成的新颖频谱图生成方法。此外，通过可微分DSP准确预测语音中的非周期成分，增强了该方法在语音信号处理中的适用性。实验结果表明，由于有效整合了SSL和DSP，所提出的方法优于基线DSP和基于SSL的音高估计方法。", "summary": "本文提出了SLASH，一种结合自监督学习（SSL）和数字信号处理（DSP）的语音音高估计方法。SLASH旨在通过引入DSP导出的先验音高分布和利用可微分DSP优化绝对音高，解决传统SSL方法仅依赖相对音高的问题。该方法还采用新颖的频谱图生成技术和准确的非周期成分预测，实验证明其性能优于现有DSP和SSL基线方法，体现了SSL与DSP有效整合的优势。", "keywords": "语音音高估计, 自监督学习, 数字信号处理, 绝对音高, 可微分DSP", "comments": "该论文的创新点在于将自监督学习与数字信号处理（DSP）相结合，特别引入了DSP导出的绝对音高信息，弥补了传统SSL方法只关注相对音高的不足。通过可微分DSP和新颖的频谱图生成方法，提高了音高估计的准确性和稳定性，并增强了对语音非周期成分的处理能力，使其在语音信号处理领域具有重要意义。"}}
{"id": "2507.17053", "title": "Matrix-Free Evaluation of High-Order Shifted Boundary Finite Element Operators", "authors": ["Michał Wichrowski"], "categories": ["math.NA", "cs.NA", "65N30, 65Y20, 65Y05, 68W10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17053v1", "summary": "This paper presents a matrix-free approach for implementing the shifted\nboundary method (SBM) in finite element analysis. The SBM is a versatile\ntechnique for solving partial differential equations on complex geometries by\nshifting boundary conditions to nearby surrogate boundaries. We focus on the\nefficient evaluation of shifted boundary operators using precomputed data and\ntensor-product structures. The proposed method avoids the explicit assembly of\nglobal matrices, achieving a computational complexity of $O(p^{2d-1})$ per face\nfor the evaluation of shifted boundary contributions on elements of polynomial\ndegree $p$ in $d$ dimensions. Numerical experiments validate the accuracy and\nefficiency of the approach, demonstrating its scalability and applicability to\nhigh-order finite element methods for both continuous and discontinuous\nGalerkin formulations. We compare the performance of the proposed method with a\nmatrix-free CutFEM implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17053v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "高阶移位边界有限元算子的无矩阵评估", "tldr": "该论文提出了一种用于移位边界方法（SBM）的无矩阵方法，通过避免全局矩阵的显式组装，实现了高效的计算复杂度和高阶有限元方法的适用性。", "motivation": "在有限元分析中，为了高效评估移位边界算子，本文提出了一种无矩阵方法，以解决复杂几何上的偏微分方程。", "method": "本文提出了一种无矩阵方法来实现有限元分析中的移位边界方法（SBM）。该方法利用预计算数据和张量积结构来高效评估移位边界算子，并避免了全局矩阵的显式组装。", "result": "该方法在$d$维多项式度为$p$的单元上，每个面评估移位边界贡献的计算复杂度为$O(p^{2d-1})$。数值实验验证了该方法的准确性、效率、可扩展性以及对高阶有限元方法的适用性，包括连续和不连续Galerkin公式。", "conclusion": "所提出的无矩阵移位边界有限元方法在计算效率、准确性和可扩展性方面表现出色，适用于高阶有限元分析，为解决复杂几何问题提供了有效途径。", "translation": "本文提出了一种在有限元分析中实现移位边界方法（SBM）的无矩阵方法。SBM 是一种通过将边界条件转移到附近的替代边界来解决复杂几何上偏微分方程的通用技术。我们专注于使用预计算数据和张量积结构来高效评估移位边界算子。所提出的方法避免了全局矩阵的显式组装，在 $d$ 维多项式度为 $p$ 的单元上，每个面评估移位边界贡献的计算复杂度为 $O(p^{2d-1})$。数值实验验证了该方法的准确性和效率，证明了其可扩展性以及对连续和不连续 Galerkin 公式的高阶有限元方法的适用性。我们将所提出方法的性能与无矩阵 CutFEM 实现进行了比较。", "summary": "本文介绍了一种针对有限元分析中移位边界方法（SBM）的无矩阵实现。通过利用预计算数据和张量积结构，该方法能够高效评估移位边界算子，同时避免了全局矩阵的显式组装。研究表明，该方法在计算复杂性上具有显著优势，并且在数值实验中验证了其准确性、效率和对高阶有限元方法的广泛适用性。", "keywords": "无矩阵, 移位边界方法, 有限元, 高阶, 计算复杂度", "comments": "该论文的创新之处在于将无矩阵方法应用于移位边界有限元算子的评估，显著提高了计算效率，尤其是在高阶有限元方法中。通过避免显式组装全局矩阵，该方法克服了传统有限元方法在处理复杂几何和高阶多项式时的内存和计算瓶颈，具有重要的实际应用价值。"}}
{"id": "2507.17530", "title": "Generalized Advantage Estimation for Distributional Policy Gradients", "authors": ["Shahil Shaik", "Jonathon M. Smereka", "Yue Wang"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, published at ACC 2025 Conference", "url": "http://arxiv.org/abs/2507.17530v1", "summary": "Generalized Advantage Estimation (GAE) has been used to mitigate the\ncomputational complexity of reinforcement learning (RL) by employing an\nexponentially weighted estimation of the advantage function to reduce the\nvariance in policy gradient estimates. Despite its effectiveness, GAE is not\ndesigned to handle value distributions integral to distributional RL, which can\ncapture the inherent stochasticity in systems and is hence more robust to\nsystem noises. To address this gap, we propose a novel approach that utilizes\nthe optimal transport theory to introduce a Wasserstein-like directional\nmetric, which measures both the distance and the directional discrepancies\nbetween probability distributions. Using the exponentially weighted estimation,\nwe leverage this Wasserstein-like directional metric to derive distributional\nGAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a\nlow-variance advantage estimate with controlled bias, making it well-suited for\npolicy gradient algorithms that rely on advantage estimation for policy\nupdates. We integrated DGAE into three different policy gradient methods.\nAlgorithms were evaluated across various OpenAI Gym environments and compared\nwith the baselines with traditional GAE to assess the performance.", "comment": "6 pages, 3 figures, published at ACC 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2507.17530v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "广义优势估计用于分布策略梯度", "tldr": "本文提出了分布广义优势估计（DGAE），将广义优势估计（GAE）扩展到分布强化学习领域，通过引入类Wasserstein方向度量来提供低方差的优势估计。", "motivation": "传统的广义优势估计（GAE）虽有效，但未设计用于处理分布强化学习中不可或缺的价值分布，这限制了其捕捉系统固有随机性并提高对系统噪声鲁棒性的能力。", "method": "提出一种新方法，利用最优传输理论引入一种类Wasserstein方向度量，该度量可衡量概率分布之间的距离和方向差异。然后，利用指数加权估计，结合此度量推导出分布广义优势估计（DGAE）。", "result": "提出的DGAE能够提供具有可控偏差的低方差优势估计，适用于依赖优势估计进行策略更新的策略梯度算法。DGAE被集成到三种不同的策略梯度方法中，并在各种OpenAI Gym环境中进行了评估，与使用传统GAE的基线进行了性能比较。", "conclusion": "本文成功提出并推导了分布广义优势估计（DGAE），有效解决了传统GAE在处理分布强化学习中价值分布的局限性，为策略梯度算法提供了更鲁棒且低方差的优势估计。", "translation": "广义优势估计（GAE）已被用于通过采用优势函数的指数加权估计来减少策略梯度估计中的方差，从而减轻强化学习（RL）的计算复杂性。尽管GAE有效，但它并未设计用于处理分布强化学习中不可或缺的价值分布，而价值分布能够捕捉系统固有的随机性，因此对系统噪声更具鲁棒性。为了解决这一空白，我们提出了一种新方法，该方法利用最优传输理论引入了一种类Wasserstein方向度量，该度量可衡量概率分布之间的距离和方向差异。通过指数加权估计，我们利用这种类Wasserstein方向度量推导出分布广义优势估计（DGAE）。与传统GAE类似，我们提出的DGAE提供了具有可控偏差的低方差优势估计，使其非常适合依赖优势估计进行策略更新的策略梯度算法。我们将DGAE集成到三种不同的策略梯度方法中。算法在各种OpenAI Gym环境中进行了评估，并与使用传统GAE的基线进行了比较，以评估其性能。", "summary": "本文针对传统广义优势估计（GAE）无法处理分布强化学习中价值分布的局限性，提出了一种新的分布广义优势估计（DGAE）方法。DGAE利用最优传输理论和类Wasserstein方向度量来衡量概率分布的距离和方向差异，并结合指数加权估计推导而来。实验表明，DGAE能为策略梯度算法提供低方差、可控偏差的优势估计，并在OpenAI Gym环境中表现良好。", "keywords": "分布广义优势估计, 最优传输理论, 策略梯度, 强化学习, Wasserstein度量", "comments": "本文创新性地将最优传输理论引入广义优势估计，提出了DGAE以解决传统GAE在分布强化学习中的局限性。其引入的类Wasserstein方向度量是关键创新点，有望在处理不确定性和随机性方面提供更鲁棒的策略梯度估计。"}}
{"id": "2507.14446", "title": "Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness", "authors": ["Defeng Liu", "Ying Liu", "Carson Eisenach"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14446v2", "summary": "In this work, we study how to efficiently apply reinforcement learning (RL)\nfor solving large-scale stochastic optimization problems by leveraging\nintervention models. The key of the proposed methodology is to better explore\nthe solution space by simulating and composing the stochastic processes using\npre-trained deep learning (DL) models. We demonstrate our approach on a\nchallenging real-world application, the multi-sourcing multi-period inventory\nmanagement problem in supply chain optimization. In particular, we employ deep\nRL models for learning and forecasting the stochastic supply chain processes\nunder a range of assumptions. Moreover, we also introduce a constraint\ncoordination mechanism, designed to forecast dual costs given the\ncross-products constraints in the inventory network. We highlight that instead\nof directly modeling the complex physical constraints into the RL optimization\nproblem and solving the stochastic problem as a whole, our approach breaks down\nthose supply chain processes into scalable and composable DL modules, leading\nto improved performance on large real-world datasets. We also outline open\nproblems for future research to further investigate the efficacy of such\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14446v2", "cate": "cs.LG", "date": "2025-07-19", "updated": "2025-07-22", "AI": {"title_translation": "深度强化学习双源库存管理与供应和能力风险感知", "tldr": "本文提出一种通过干预模型将深度强化学习应用于大规模随机优化问题的方法，通过将供应链过程分解为可伸伸缩和可组合的深度学习模块，提高了大规模真实世界数据集上的性能。", "motivation": "旨在有效应用强化学习解决大规模随机优化问题，特别是在具有挑战性的多源多周期库存管理问题中。", "method": "利用干预模型高效应用强化学习；通过使用预训练的深度学习模型模拟和组合随机过程以探索解决方案空间；采用深度强化学习模型学习和预测随机供应链过程；引入约束协调机制，预测库存网络中交叉产品约束下的双重成本；将复杂的供应链过程分解为可伸伸缩和可组合的深度学习模块，而非直接建模为单一的RL优化问题。", "result": "在多源多周期库存管理问题中验证了方法的有效性；在大规模真实世界数据集上实现了性能提升。", "conclusion": "提出了一种将复杂随机问题分解为可伸缩DL模块的有效方法，并在实际应用中取得了良好效果，并指出了未来的研究方向。", "translation": "在这项工作中，我们研究了如何通过利用干预模型有效应用强化学习（RL）来解决大规模随机优化问题。所提出方法的关键在于通过使用预训练的深度学习（DL）模型模拟和组合随机过程，更好地探索解决方案空间。我们在一个具有挑战性的真实世界应用——供应链优化中的多源多周期库存管理问题上演示了我们的方法。特别是，我们采用深度RL模型来学习和预测一系列假设下的随机供应链过程。此外，我们还引入了一种约束协调机制，旨在根据库存网络中的交叉产品约束来预测双重成本。我们强调，我们的方法不是将复杂的物理约束直接建模到RL优化问题中并作为一个整体解决随机问题，而是将这些供应链过程分解为可伸伸缩和可组合的DL模块，从而在大规模真实世界数据集上提高了性能。我们还概述了未来研究的开放问题，以进一步调查此类模型的功效。", "summary": "本文提出了一种将深度强化学习应用于大规模随机优化问题的新方法，通过利用干预模型和将复杂的供应链过程分解为可伸缩、可组合的深度学习模块。该方法在解决多源多周期库存管理问题时，通过模拟和组合随机过程，并引入约束协调机制，在大规模真实世界数据集上展现出显著的性能提升。", "keywords": "深度强化学习, 库存管理, 供应链优化, 随机优化, 深度学习模块化", "comments": "本文的创新点在于提出了将复杂随机优化问题分解为可伸缩、可组合的深度学习模块的策略，避免了直接建模复杂约束的困难，并在大规模真实世界应用中展现了有效性。这种模块化方法对于处理复杂的供应链优化问题具有重要意义。"}}
{"id": "2507.17385", "title": "A Zero-overhead Flow for Security Closure", "authors": ["Mohammad Eslami", "Ashira Johara", "Kyungbin Park", "Samuel Pagliarini"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17385v1", "summary": "In the traditional Application-Specific Integrated Circuit (ASIC) design\nflow, the concept of timing closure implies to reach convergence during\nphysical synthesis such that, under a given area and power budget, the design\nworks at the targeted frequency. However, security has been largely neglected\nwhen evaluating the Quality of Results (QoR) from physical synthesis. In\ngeneral, commercial place & route tools do not understand security goals. In\nthis work, we propose a modified ASIC design flow that is security-aware and,\ndifferently from prior research, does not degrade QoR for the sake of security\nimprovement. Therefore, we propose a first-of-its-kind zero-overhead flow for\nsecurity closure. Our flow is concerned with two distinct threat models: (i)\ninsertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.\nImportantly, the flow is entirely executed within a commercial place & route\nengine and is scalable. In several metrics, our security-aware flow achieves\nthe best-known results for the ISPD`22 set of benchmark circuits while\nincurring negligible design overheads due to security-related strategies.\nFinally, we open source the entire methodology (as a set of scripts) and also\nshare the protected circuits (as design databases) for the benefit of the\nhardware security community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17385v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种零开销安全闭合流程", "tldr": "本文提出了一种零开销、安全感知的ASIC设计流程，用于缓解硬件木马和物理攻击，且不影响性能。", "motivation": "在传统的ASIC设计流程中，安全性在评估结果质量（QoR）时被很大程度上忽略，且商业布局布线工具不理解安全目标，导致存在硬件木马和物理探测等安全漏洞。", "method": "本文提出了一种修改后的ASIC设计流程，该流程是安全感知的，并且与先前的研究不同，它不会为了提高安全性而降低QoR。该流程关注两种威胁模型：硬件木马（HTs）的插入和物理探测/故障注入。该流程完全在商业布局布线引擎中执行，并且具有可扩展性。", "result": "在多项指标上，本文提出的安全感知流程在ISPD'22基准电路集上取得了已知最佳结果，同时因安全相关策略而产生的额外设计开销可以忽略不计。", "conclusion": "本文成功提出了一种零开销、可扩展的ASIC设计安全闭合流程，在不牺牲性能的情况下解决了硬件安全威胁，并公开了其方法论。", "translation": "在传统的专用集成电路（ASIC）设计流程中，时序闭合的概念意味着在物理综合过程中达到收敛，使得在给定的面积和功耗预算下，设计能够以目标频率工作。然而，在评估物理综合的结果质量（QoR）时，安全性在很大程度上被忽视。通常，商业布局布线工具不理解安全目标。在这项工作中，我们提出了一种修改后的ASIC设计流程，该流程具有安全意识，并且与先前的研究不同，它不会为了安全改进而降低QoR。因此，我们提出了一种首创的零开销安全闭合流程。我们的流程关注两种不同的威胁模型：（i）硬件木马（HTs）的插入和（ii）物理探测/故障注入。重要的是，该流程完全在商业布局布线引擎中执行并且是可扩展的。在多项指标上，我们的安全感知流程在ISPD'22基准电路集上取得了已知最佳结果，同时由于安全相关策略而产生的额外设计开销可以忽略不计。最后，我们开源了整个方法论（作为一组脚本），并分享了受保护的电路（作为设计数据库），以造福硬件安全社区。", "summary": "本文介绍了一种新颖的零开销、安全感知的专用集成电路（ASIC）设计流程，该流程集成在商业布局布线工具中。与以往方法不同，该流程在不降低结果质量（QoR）的前提下，有效解决了硬件木马和物理攻击等安全威胁。它在ISPD'22基准测试电路上展示了卓越的性能，且开销可忽略不计，并已开源以供硬件安全社区使用。", "keywords": "硬件安全, ASIC设计, 安全闭合, 硬件木马, 物理攻击", "comments": "该论文的创新之处在于提出了“零开销”的安全闭合流程，这显著优于以往为实现安全而牺牲性能的研究。其与商业工具的集成以及开源做法，也大大提升了其实用价值和对硬件安全社区的贡献。"}}
{"id": "2507.17734", "title": "DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models", "authors": ["Liwenhan Xie", "Yanna Lin", "Can Liu", "Huamin Qu", "Xinhuan Shu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to the IEEE Visualization Conference (VIS'25). 11 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17734v1", "summary": "Creating aesthetically pleasing data visualizations remains challenging for\nusers without design expertise or familiarity with visualization tools. To\naddress this gap, we present DataWink, a system that enables users to create\ncustom visualizations by adapting high-quality examples. Our approach combines\nlarge multimodal models (LMMs) to extract data encoding from existing SVG-based\nvisualization examples, featuring an intermediate representation of\nvisualizations that bridges primitive SVG and visualization programs. Users may\nexpress adaptation goals to a conversational agent and control the visual\nappearance through widgets generated on demand. With an interactive interface,\nusers can modify both data mappings and visual design elements while\nmaintaining the original visualization's aesthetic quality. To evaluate\nDataWink, we conduct a user study (N=12) with replication and free-form\nexploration tasks. As a result, DataWink is recognized for its learnability and\neffectiveness in personalized authoring tasks. Our results demonstrate the\npotential of example-driven approaches for democratizing visualization\ncreation.", "comment": "Accepted to the IEEE Visualization Conference (VIS'25). 11 pages, 6\n  figures", "pdf_url": "http://arxiv.org/pdf/2507.17734v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DataWink：使用大型多模态模型重用和改编基于 SVG 的可视化示例", "tldr": "DataWink 帮助非专业用户通过改编高质量的 SVG 示例，使用大型多模态模型创建个性化数据可视化。", "motivation": "对于缺乏设计专业知识或不熟悉可视化工具的用户来说，创建美观的数据可视化仍然具有挑战性。", "method": "DataWink 系统结合大型多模态模型（LMMs）从现有基于 SVG 的可视化示例中提取数据编码，并使用连接原始 SVG 和可视化程序的中间表示。用户可以通过会话代理表达改编目标，并通过按需生成的部件控制视觉外观。该系统提供交互式界面，允许用户修改数据映射和视觉设计元素，同时保持原始可视化的美学质量。", "result": "用户研究（N=12）表明，DataWink 在个性化创作任务中具有可学习性和有效性。", "conclusion": "示例驱动的方法在普及可视化创建方面具有潜力。", "translation": "对于缺乏设计专业知识或不熟悉可视化工具的用户来说，创建美观的数据可视化仍然具有挑战性。为了解决这一差距，我们提出了 DataWink，一个允许用户通过改编高质量示例来创建自定义可视化的系统。我们的方法结合了大型多模态模型（LMMs），从现有基于 SVG 的可视化示例中提取数据编码，其特点是可视化中间表示，连接了原始 SVG 和可视化程序。用户可以向会话代理表达改编目标，并通过按需生成的部件控制视觉外观。通过交互式界面，用户可以修改数据映射和视觉设计元素，同时保持原始可视化的美学质量。为了评估 DataWink，我们进行了一项用户研究（N=12），包括复制和自由探索任务。结果显示，DataWink 在个性化创作任务中具有可学习性和有效性。我们的结果证明了示例驱动方法在普及可视化创建方面的潜力。", "summary": "DataWink 是一个旨在帮助非专业用户创建个性化数据可视化的系统。它通过结合大型多模态模型（LMMs）从现有高质量的 SVG 示例中提取数据编码，并利用中间表示来桥接原始 SVG 和可视化程序。用户可以通过会话代理表达需求，并通过交互式界面调整数据映射和视觉设计，同时保持美学质量。用户研究证实了 DataWink 在个性化创作任务中的可学习性和有效性，表明了示例驱动方法在普及可视化创建方面的潜力。", "keywords": "数据可视化, 大型多模态模型, SVG, 示例驱动, 人机交互", "comments": "DataWink 的创新之处在于将大型多模态模型应用于可视化领域，实现了基于 SVG 示例的智能重用和改编。其通过中间表示连接底层 SVG 和上层可视化程序，为用户提供了灵活且易于操作的界面，有效降低了可视化创作的门槛。该系统对于非专业用户创建高质量可视化具有重要意义，展现了示例驱动和AI辅助设计在民主化可视化创作方面的潜力。"}}
{"id": "2507.17080", "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings", "authors": ["Ramin Giahi", "Kehui Yao", "Sriram Kollipara", "Kai Zhao", "Vahid Mirjalili", "Jianpeng Xu", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "categories": ["cs.IR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at RecSys 2025; DOI: this https URL", "url": "http://arxiv.org/abs/2507.17080v1", "summary": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.", "comment": "Accepted at RecSys 2025; DOI:https://doi.org/10.1145/3705328.3748064", "pdf_url": "http://arxiv.org/pdf/2507.17080v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "VL-CLIP：通过视觉定位和LLM增强的CLIP嵌入增强多模态推荐", "tldr": "VL-CLIP通过结合视觉定位和LLM增强的文本嵌入，解决了电子商务推荐系统中CLIP模型在对象级对齐、文本表示模糊和领域不匹配等问题，显著提升了推荐准确性和效率。", "motivation": "现有视觉-语言模型（如CLIP）在电子商务推荐系统中面临挑战：1）弱对象级对齐，全局图像嵌入无法捕捉细粒度产品属性；2）文本表示模糊，产品描述缺乏上下文清晰度；3）领域不匹配，通用模型难以泛化到电子商务特定数据。", "method": "我们提出了VL-CLIP框架，通过集成视觉定位（Visual Grounding）来精炼图像表示，以及一个基于LLM的代理来生成更丰富的文本嵌入。视觉定位通过定位关键产品来优化图像表示，而LLM代理通过消除产品描述的歧义来增强文本特征。", "result": "在北美最大的电子商务平台之一上，VL-CLIP显著提高了检索准确性、多模态检索效率和推荐质量，用户点击率（CTR）提升18.6%，加入购物车率（ATC）提升15.5%，商品交易总额（GMV）提升4.0%。额外实验结果表明，该框架在精确度和语义对齐方面均优于包括CLIP、FashionCLIP和GCL在内的视觉-语言模型。", "conclusion": "结合对象感知的视觉定位和LLM增强的文本表示，VL-CLIP在多模态推荐方面展现出强大的潜力，有效解决了现有模型的局限性并显著提升了电子商务推荐系统的性能。", "translation": "多模态学习在当今的电子商务推荐平台中扮演着关键角色，实现了准确的推荐和产品理解。然而，现有的视觉-语言模型，如CLIP，在电子商务推荐系统中面临关键挑战：1）弱对象级对齐，全局图像嵌入未能捕捉细粒度的产品属性，导致检索性能不佳；2）文本表示模糊，产品描述通常缺乏上下文清晰度，影响跨模态匹配；3）领域不匹配，通用视觉-语言模型可能无法很好地泛化到电子商务特定数据。为了解决这些局限性，我们提出了一个框架VL-CLIP，通过集成视觉定位（Visual Grounding）进行细粒度视觉理解和基于LLM的代理生成增强的文本嵌入来增强CLIP嵌入。视觉定位通过定位关键产品来精炼图像表示，而LLM代理通过消除产品描述的歧义来增强文本特征。我们的方法在美国最大的电子商务平台之一上，对数千万件商品显著提高了检索准确性、多模态检索效率和推荐质量，用户点击率（CTR）提升18.6%，加入购物车率（ATC）提升15.5%，商品交易总额（GMV）提升4.0%。额外的实验结果表明，我们的框架在精确度和语义对齐方面均优于包括CLIP、FashionCLIP和GCL在内的视觉-语言模型，展示了结合对象感知的视觉定位和LLM增强的文本表示在鲁棒多模态推荐方面的潜力。", "summary": "VL-CLIP是一个旨在增强电子商务多模态推荐的框架，它通过结合视觉定位技术来提高图像的细粒度理解，并利用大型语言模型（LLM）来丰富和消除产品文本描述的歧义。该方法有效解决了现有CLIP模型在对象级对齐不足、文本表示模糊和领域不匹配等问题，并在实际电子商务平台中显著提升了推荐系统的各项关键指标，表现优于其他主流视觉-语言模型。", "keywords": "多模态推荐, 视觉定位, LLM, CLIP, 电子商务", "comments": "VL-CLIP的创新之处在于其将视觉定位（Visual Grounding）与LLM增强的文本嵌入相结合，以解决多模态推荐中的核心挑战。这种双重增强策略有效弥补了通用视觉-语言模型在电子商务特定场景下的不足，尤其是在细粒度特征捕捉和上下文理解方面。其在真实大规模电子商务平台上的显著性能提升（CTR、ATC、GMV）证明了其实用性和重要性。"}}
{"id": "2507.17170", "title": "Advancing Quantum State Preparation using LimTDD", "authors": ["Xin Hong", "Aochu Dai", "Chenjian Li", "Sanjiang Li", "Shenggang Ying", "Mingsheng Ying"], "categories": ["cs.DS", "quant-ph"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.14496", "url": "http://arxiv.org/abs/2507.17170v1", "summary": "Quantum state preparation (QSP) is a fundamental task in quantum computing\nand quantum information processing. It is critical to the execution of many\nquantum algorithms, including those in quantum machine learning. In this paper,\nwe propose a family of efficient QSP algorithms tailored to different numbers\nof available ancilla qubits - ranging from no ancilla qubits, to a single\nancilla qubit, to a sufficiently large number of ancilla qubits. Our algorithms\nare based on a novel decision diagram that is fundamentally different from the\napproaches used in previous QSP algorithms. Specifically, our approach exploits\nthe power of Local Invertible Map Tensor Decision Diagrams (LimTDDs) - a highly\ncompact representation of quantum states that combines tensor networks and\ndecision diagrams to reduce quantum circuit complexity. Extensive experiments\ndemonstrate that our methods significantly outperform existing approaches and\nexhibit better scalability for large-scale quantum states, both in terms of\nruntime and gate complexity. Furthermore, our method shows exponential\nimprovement in best-case scenarios. This paper is an extended version of [1],\nwith three more algorithms proposed.", "comment": "arXiv admin note: text overlap with arXiv:2507.14496", "pdf_url": "http://arxiv.org/pdf/2507.17170v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用LimTDD推进量子态制备", "tldr": "提出基于LimTDD的高效量子态制备算法，在性能和可扩展性上显著优于现有方法。", "motivation": "量子态制备（QSP）是量子计算和量子信息处理中的一项基本任务，对许多量子算法（包括量子机器学习）的执行至关重要。", "method": "提出了一系列高效的QSP算法，可根据可用辅助量子比特的数量进行调整（从无辅助比特到大量辅助比特）。这些算法基于一种新颖的决策图，即局部可逆映射张量决策图（LimTDD），它结合了张量网络和决策图，以实现量子态的紧凑表示并降低量子电路复杂性。", "result": "实验证明，所提出的方法在运行时和门复杂度方面显著优于现有方法，并对大规模量子态表现出更好的可扩展性。在最佳情况下，该方法显示出指数级改进。", "conclusion": "基于LimTDD的量子态制备算法能够显著提升效率和可扩展性，为量子计算中的基本任务提供了优越的解决方案。", "translation": "量子态制备（QSP）是量子计算和量子信息处理中的一项基本任务。它对许多量子算法的执行至关重要，包括量子机器学习中的算法。在本文中，我们提出了一系列高效的QSP算法，这些算法根据可用辅助量子比特的不同数量进行定制——从没有辅助量子比特，到一个辅助量子比特，到足够多的辅助量子比特。我们的算法基于一种新颖的决策图，这与以前QSP算法中使用的方法根本不同。具体来说，我们的方法利用了局部可逆映射张量决策图（LimTDD）的力量——这是一种高度紧凑的量子态表示，它结合了张量网络和决策图以降低量子电路复杂性。广泛的实验表明，我们的方法在运行时和门复杂度方面都显著优于现有方法，并且对大规模量子态表现出更好的可扩展性。此外，我们的方法在最佳情况下显示出指数级改进。本文是[1]的扩展版本，提出了另外三种算法。", "summary": "本文提出了一系列基于新型局部可逆映射张量决策图（LimTDD）的高效量子态制备（QSP）算法。这些算法可根据辅助量子比特的数量进行调整，并利用LimTDD的紧凑表示来降低量子电路复杂度。实验结果表明，该方法在运行时和门复杂度方面显著优于现有方法，并对大规模量子态表现出更好的可扩展性，在最佳情况下甚至实现指数级改进。", "keywords": "量子态制备, LimTDD, 量子算法, 决策图, 量子电路复杂性", "comments": "这篇论文的创新点在于引入了基于LimTDD的新型决策图来解决量子态制备问题，这种方法结合了张量网络和决策图的优势，实现了量子态的紧凑表示并有效降低了电路复杂性。其在性能和可扩展性上的显著提升，尤其是在最佳情况下的指数级改进，表明了该方法在推进量子计算领域，特别是量子算法效率方面的巨大潜力。"}}
{"id": "2507.07935", "title": "Working with AI: Measuring the Occupational Implications of Generative AI", "authors": ["Kiran Tomlinson", "Sonia Jaffe", "Will Wang", "Scott Counts", "Siddharth Suri"], "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      41 pages", "url": "http://arxiv.org/abs/2507.07935v3", "summary": "Given the rapid adoption of generative AI and its potential to impact a wide\nrange of tasks, understanding the effects of AI on the economy is one of\nsociety's most important questions. In this work, we take a step toward that\ngoal by analyzing the work activities people do with AI, how successfully and\nbroadly those activities are done, and combine that with data on what\noccupations do those activities. We analyze a dataset of 200k anonymized and\nprivacy-scrubbed conversations between users and Microsoft Bing Copilot, a\npublicly available generative AI system. We find the most common work\nactivities people seek AI assistance for involve gathering information and\nwriting, while the most common activities that AI itself is performing are\nproviding information and assistance, writing, teaching, and advising.\nCombining these activity classifications with measurements of task success and\nscope of impact, we compute an AI applicability score for each occupation. We\nfind the highest AI applicability scores for knowledge work occupation groups\nsuch as computer and mathematical, and office and administrative support, as\nwell as occupations such as sales whose work activities involve providing and\ncommunicating information. Additionally, we characterize the types of work\nactivities performed most successfully, how wage and education correlate with\nAI applicability, and how real-world usage compares to predictions of\noccupational AI impact.", "comment": "41 pages", "pdf_url": "http://arxiv.org/pdf/2507.07935v3", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-22", "AI": {"title_translation": "与AI协作：衡量生成式AI对职业的影响", "tldr": "本文通过分析用户与生成式AI的真实对话数据，量化了AI对不同职业的工作活动影响，发现知识型和信息交流型职业的AI适用性最高。", "motivation": "鉴于生成式AI的快速普及及其对广泛任务的潜在影响，理解AI对经济的影响是社会最重要的课题之一。", "method": "本研究分析了20万条匿名化且经过隐私处理的用户与Microsoft Bing Copilot（一个公开可用的生成式AI系统）之间的对话数据集。研究识别了人们寻求AI协助和AI本身执行的常见工作活动类型。通过结合活动分类、任务成功率和影响范围的测量，计算了每个职业的AI适用性得分。此外，还分析了最成功的工作活动类型、工资和教育与AI适用性的相关性，以及实际使用情况与职业AI影响预测的比较。", "result": "研究发现，人们寻求AI协助最常见的工作活动是信息收集和写作。AI本身最常执行的活动是提供信息和协助、写作、教学和建议。计算机和数学、办公室和行政支持等知识型职业群体，以及销售等工作活动涉及提供和交流信息的职业，AI适用性得分最高。此外，还描述了最成功完成的工作活动类型，以及工资和教育与AI适用性的相关性，以及实际使用情况与职业AI影响预测的比较。", "conclusion": "生成式AI对知识型和信息交流型职业的影响尤为显著，通过分析实际使用数据，有助于更准确地理解AI对劳动力市场的潜在影响。", "translation": "鉴于生成式AI的快速普及及其影响广泛任务的潜力，理解AI对经济的影响是社会最重要的课题之一。在这项工作中，我们通过分析人们与AI进行的工作活动、这些活动的成功程度和广度，并结合这些活动所涉及的职业数据，向着这一目标迈进了一步。我们分析了一个包含20万条匿名化且经过隐私处理的用户与Microsoft Bing Copilot（一个公开可用的生成式AI系统）之间的对话数据集。我们发现人们寻求AI协助最常见的工作活动涉及信息收集和写作，而AI本身执行的最常见活动是提供信息和帮助、写作、教学和建议。我们将这些活动分类与任务成功率和影响范围的测量相结合，计算出每个职业的AI适用性得分。我们发现计算机和数学、办公室和行政支持等知识型职业群体，以及销售等其工作活动涉及提供和交流信息的职业，AI适用性得分最高。此外，我们还描述了最成功完成的工作活动类型，工资和教育与AI适用性的相关性，以及实际使用情况与职业AI影响预测的比较。", "summary": "本研究旨在量化生成式AI对职业的影响。通过分析20万条用户与Bing Copilot的对话数据，论文识别出人们寻求AI协助和AI实际执行的主要工作活动类型。研究发现，AI适用性得分最高的职业集中在知识型工作（如计算机、行政支持）和涉及信息交流的职业（如销售）。该工作还探讨了任务成功率、工资与教育对AI适用性的影响，并比较了实际使用与预测。", "keywords": "生成式AI, 职业影响, 劳动力市场, AI适用性, Bing Copilot", "comments": "本文通过分析真实的AI使用数据，为理解生成式AI对劳动力市场和职业的实际影响提供了宝贵的实证证据，而非仅停留在理论预测。其方法论结合了用户活动分析和职业分类，具有创新性。"}}
{"id": "2506.05367", "title": "Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with Consistency Rewards", "authors": ["Aakash Garg", "Libing Zeng", "Andrii Tsarov", "Nima Khademi Kalantari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05367v2", "summary": "In this paper, we propose a novel diffusion-based approach to generate stereo\nimages given a text prompt. Since stereo image datasets with large baselines\nare scarce, training a diffusion model from scratch is not feasible. Therefore,\nwe propose leveraging the strong priors learned by Stable Diffusion and\nfine-tuning it on stereo image datasets to adapt it to the task of stereo\ngeneration. To improve stereo consistency and text-to-image alignment, we\nfurther tune the model using prompt alignment and our proposed stereo\nconsistency reward functions. Comprehensive experiments demonstrate the\nsuperiority of our approach in generating high-quality stereo images across\ndiverse scenarios, outperforming existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05367v2", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-07-23", "AI": {"title_translation": "Text2Stereo：利用稳定扩散模型进行立体图像生成，并引入一致性奖励", "tldr": "本文提出了一种新颖的基于扩散模型的方法，利用Stable Diffusion生成文本提示驱动的立体图像，并通过一致性奖励进一步优化。", "motivation": "由于大基线立体图像数据集的稀缺性，从头开始训练扩散模型生成立体图像不可行。", "method": "本文提出利用Stable Diffusion的强大先验知识，并在立体图像数据集上对其进行微调，以适应立体图像生成任务。为提高立体一致性和文本到图像的对齐，进一步使用提示对齐和提出的立体一致性奖励函数对模型进行调整。", "result": "全面的实验证明，该方法在生成高质量立体图像方面优于现有方法，适用于多种场景。", "conclusion": "该方法成功地利用了现有扩散模型的强大能力，并通过引入一致性奖励机制，有效地解决了立体图像生成中的数据稀缺和一致性问题，实现了高质量的立体图像生成。", "translation": "在本文中，我们提出了一种新颖的基于扩散模型的方法，用于根据文本提示生成立体图像。由于具有大基线的立体图像数据集稀缺，从头开始训练扩散模型是不可行的。因此，我们建议利用Stable Diffusion学习到的强大先验知识，并在立体图像数据集上对其进行微调，以使其适应立体图像生成任务。为了提高立体一致性和文本到图像的对齐，我们使用提示对齐和我们提出的立体一致性奖励函数进一步调整模型。全面的实验表明，我们的方法在生成高质量立体图像方面表现优越，适用于各种场景，并且优于现有方法。", "summary": "本文提出Text2Stereo，一种基于扩散模型的新方法，用于从文本提示生成立体图像。鉴于大基线立体图像数据集的稀缺性，该方法通过微调现有的Stable Diffusion模型来适应立体生成任务。为了增强立体一致性和文本-图像对齐，模型进一步通过提示对齐和提出的立体一致性奖励函数进行优化。实验结果表明，该方法在生成高质量立体图像方面表现出色，并超越了现有方法。", "keywords": "立体图像生成, 扩散模型, Stable Diffusion, 一致性奖励, 文本到图像", "comments": "该论文的创新点在于巧妙地利用了预训练的Stable Diffusion模型，避免了从零开始训练扩散模型对大规模立体数据集的依赖。通过引入立体一致性奖励函数，有效解决了立体图像生成中的关键挑战，即保持左右视图的一致性。这种方法为文本到立体图像生成提供了一个有效且高效的解决方案，具有重要的应用潜力。"}}
{"id": "2507.17433", "title": "Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach", "authors": ["Hugh Adams", "Srijoni Majumdar", "Evangelos Pournaras"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17433v1", "summary": "Participatory budgeting is a method of collectively understanding and\naddressing spending priorities where citizens vote on how a budget is spent, it\nis regularly run to improve the fairness of the distribution of public funds.\nParticipatory budgeting requires voters to make decisions on projects which can\nlead to ``choice overload\". A multi-agent reinforcement learning approach to\ndecision support can make decision making easier for voters by identifying\nvoting strategies that increase the winning proportion of their vote. This\nnovel approach can also support policymakers by highlighting aspects of\nelection design that enable fair compromise on projects. This paper presents a\nnovel, ethically aligned approach to decision support using multi-agent deep\nreinforcement learning modelling. This paper introduces a novel use of a\nbranching neural network architecture to overcome scalability challenges of\nmulti-agent reinforcement learning in a decentralized way. Fair compromises are\nfound through optimising voter actions towards greater representation of voter\npreferences in the winning set. Experimental evaluation with real-world\nparticipatory budgeting data reveals a pattern in fair compromise: that it is\nachievable through projects with smaller cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17433v1", "cate": "cs.MA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "公平妥协在参与式预算中：一种多智能体深度强化学习方法", "tldr": "本文提出了一种新颖的多智能体深度强化学习方法，通过优化投票策略来促进参与式预算中的公平妥协，并通过实验证明小成本项目有助于实现公平妥协。", "motivation": "参与式预算会导致选民的“选择过载”问题，并且决策者需要支持来设计选举以实现公平妥协。本文旨在通过识别增加选票获胜比例的投票策略，使选民的决策过程更容易，并支持政策制定者突出能够实现项目公平妥协的选举设计方面。", "method": "本文提出了一种新颖的、符合伦理的多智能体深度强化学习决策支持方法。它引入了一种新颖的分支神经网络架构，以去中心化的方式克服多智能体强化学习的可扩展性挑战。通过优化选民行为，使其在获胜集合中更好地代表选民偏好，从而找到公平妥协。", "result": "对真实世界参与式预算数据的实验评估揭示了公平妥协的一种模式：通过成本较小的项目可以实现公平妥协。", "conclusion": "通过多智能体深度强化学习优化选民行为，可以实现参与式预算中的公平妥协，特别是成本较小的项目有助于达成这种妥协。", "translation": "参与式预算是一种集体理解和解决支出优先事项的方法，公民投票决定预算如何使用，定期运行以提高公共资金分配的公平性。参与式预算要求选民对项目做出决策，这可能导致“选择过载”。一种多智能体强化学习的决策支持方法可以通过识别增加其投票获胜比例的投票策略，使选民的决策更容易。这种新颖的方法还可以通过突出选举设计的方面来支持政策制定者，从而实现项目上的公平妥协。本文提出了一种新颖的、符合伦理的多智能体深度强化学习建模决策支持方法。本文引入了一种新颖的分支神经网络架构的使用，以去中心化的方式克服多智能体强化学习的可扩展性挑战。通过优化选民行为，使其在获胜集合中更好地代表选民偏好，从而找到公平妥协。对真实世界参与式预算数据的实验评估揭示了公平妥协的一种模式：通过成本较小的项目可以实现公平妥协。", "summary": "该论文提出了一种新颖的、符合伦理的多智能体深度强化学习方法，以解决参与式预算中投票者的“选择过载”问题，并通过优化投票策略来促进公平妥协。该方法引入了分支神经网络架构以应对可扩展性挑战，并利用真实世界数据进行实验评估，结果表明小成本项目有助于实现公平妥协。", "keywords": "参与式预算, 多智能体强化学习, 公平妥协, 分支神经网络, 决策支持", "comments": "本文的创新之处在于将多智能体深度强化学习和新颖的分支神经网络应用于参与式预算领域，同时解决了选民决策和公平妥协的问题。关于小成本项目有助于实现公平妥协的发现具有实际应用价值。"}}
{"id": "2507.16946", "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts", "authors": ["Chiao-An Yang", "Kuan-Chuan Peng", "Raymond A. Yeh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper is accepted to ICCV 2025. The supplementary material is included. The long-tailed online anomaly detection dataset is available at this https URL", "url": "http://arxiv.org/abs/2507.16946v1", "summary": "Anomaly detection (AD) identifies the defect regions of a given image. Recent\nworks have studied AD, focusing on learning AD without abnormal images, with\nlong-tailed distributed training data, and using a unified model for all\nclasses. In addition, online AD learning has also been explored. In this work,\nwe expand in both directions to a realistic setting by considering the novel\ntask of long-tailed online AD (LTOAD). We first identified that the offline\nstate-of-the-art LTAD methods cannot be directly applied to the online setting.\nSpecifically, LTAD is class-aware, requiring class labels that are not\navailable in the online setting. To address this challenge, we propose a\nclass-agnostic framework for LTAD and then adapt it to our online learning\nsetting. Our method outperforms the SOTA baselines in most offline LTAD\nsettings, including both the industrial manufacturing and the medical domain.\nIn particular, we observe +4.63% image-AUROC on MVTec even compared to methods\nthat have access to class labels and the number of classes. In the most\nchallenging long-tailed online setting, we achieve +0.53% image-AUROC compared\nto baselines. Our LTOAD benchmark is released here:\nhttps://doi.org/10.5281/zenodo.16283852 .", "comment": "This paper is accepted to ICCV 2025. The supplementary material is\n  included. The long-tailed online anomaly detection dataset is available at\n  https://doi.org/10.5281/zenodo.16283852", "pdf_url": "http://arxiv.org/pdf/2507.16946v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "面向长尾在线异常检测的类别无关概念", "tldr": "本文提出了一种类别无关的框架，用于解决长尾在线异常检测（LTOAD）这一新任务，并在离线和在线设置中均优于现有方法。", "motivation": "现有的长尾异常检测（LTAD）方法是类别感知的，需要在线设置中不可用的类别标签，因此无法直接应用于在线场景。为了应对这一挑战，并扩展到更现实的长尾在线异常检测（LTOAD）任务，作者提出了新的方法。", "method": "提出一个类别无关的框架用于长尾异常检测（LTAD），并将其适应到在线学习设置中。", "result": "在大多数离线长尾异常检测（LTAD）设置中，包括工业制造和医疗领域，该方法均优于SOTA基线。特别是在MVTec数据集上，图像AUROC提高了4.63%，甚至超过了那些可以使用类别标签和类别数量的方法。在最具挑战性的长尾在线设置中，图像AUROC比基线提高了0.53%。", "conclusion": "该研究成功开发了一个类别无关的框架，有效解决了长尾在线异常检测（LTOAD）任务的挑战，并在离线和在线设置中均取得了显著优于现有基线的性能。", "translation": "异常检测（AD）识别给定图像的缺陷区域。最近的研究已经探索了AD，重点在于在没有异常图像的情况下学习AD、使用长尾分布的训练数据以及为所有类别使用统一模型。此外，在线AD学习也得到了探索。在这项工作中，我们通过考虑长尾在线AD（LTOAD）这一新颖任务，在这两个方向上扩展到更现实的设置。我们首先发现，离线最先进的LTAD方法不能直接应用于在线设置。具体来说，LTAD是类别感知的，需要在线设置中不可用的类别标签。为了解决这一挑战，我们提出了一个用于LTAD的类别无关框架，然后将其适应到我们的在线学习设置中。我们的方法在大多数离线LTAD设置中，包括工业制造和医疗领域，都优于SOTA基线。特别是，即使与那些可以访问类别标签和类别数量的方法相比，我们在MVTec上的图像AUROC也观察到+4.63%的提升。在最具挑战性的长尾在线设置中，我们比基线实现了+0.53%的图像AUROC提升。我们的LTOAD基准已发布在此处：https://doi.org/10.5281/zenodo.16283852。", "summary": "本文提出了一种新颖的类别无关框架，旨在解决长尾在线异常检测（LTOAD）这一现实且具有挑战性的任务。研究发现，现有离线长尾异常检测（LTAD）方法由于其类别感知特性无法直接应用于在线环境。为克服此限制，该框架被设计为类别无关，并成功适应在线学习。实验结果表明，该方法在离线LTAD设置（包括工业和医疗领域）中显著优于现有最佳基线，在MVTec数据集上图像AUROC提升4.63%。在最具挑战性的长尾在线设置中，图像AUROC也实现了0.53%的提升。", "keywords": "异常检测, 长尾分布, 在线学习, 类别无关, LTOAD", "comments": "这篇论文的创新点在于提出了一个新颖的“长尾在线异常检测（LTOAD）”任务，并针对该任务开发了一个“类别无关”的框架。其重要性体现在解决了现有离线方法无法适应在线环境的问题，尤其是在类别标签不可用的情况下。该方法在现实世界的工业和医疗领域的性能提升，以及基准的发布，都显示了其潜在的应用价值和对未来研究的贡献。"}}
{"id": "2507.17255", "title": "Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions", "authors": ["Songxuan Shi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17255v1", "summary": "This paper explores the generative capabilities of Autoencoders (AEs) and\nestablishes connections between Variational Autoencoders (VAEs) and Vector\nQuantized-Variational Autoencoders (VQ-VAEs) through a reformulated training\nframework. We demonstrate that AEs exhibit generative potential via latent\nspace interpolation and perturbation, albeit limited by undefined regions in\nthe encoding space. To address this, we propose a new VAE-like training method\nthat introduces clustering centers to enhance data compactness and ensure\nwell-defined latent spaces without relying on traditional KL divergence or\nreparameterization techniques. Experimental results on MNIST, CelebA, and\nFashionMNIST datasets show smooth interpolative transitions, though blurriness\npersists. Extending this approach to multiple learnable vectors, we observe a\nnatural progression toward a VQ-VAE-like model in continuous space. However,\nwhen the encoder outputs multiple vectors, the model degenerates into a\ndiscrete Autoencoder (VQ-AE), which combines image fragments without learning\nsemantic representations. Our findings highlight the critical role of encoding\nspace compactness and dispersion in generative modeling and provide insights\ninto the intrinsic connections between VAEs and VQ-VAEs, offering a new\nperspective on their design and limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17255v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "重新思考VAE：从连续到离散表示，无需概率假设", "tldr": "本文探讨了自编码器（AEs）、变分自编码器（VAEs）和向量量化变分自编码器（VQ-VAEs）之间的联系，提出了一种无需概率假设的新型VAE训练方法，通过引入聚类中心来改善潜在空间，并揭示了模型在特定条件下的退化。", "motivation": "探索自编码器（AEs）的生成能力；建立变分自编码器（VAEs）和向量量化变分自编码器（VQ-VAEs）之间的联系；解决AE编码空间中未定义区域的局限性；为VAEs和VQ-VAEs的设计和局限性提供新的视角。", "method": "提出了一种新的类VAE训练方法，通过引入聚类中心来增强数据紧凑性并确保明确定义的潜在空间，该方法不依赖传统的KL散度或重参数化技术。将此方法扩展到多个可学习向量。", "result": "自编码器（AEs）通过潜在空间插值和扰动展现生成潜力，但受限于未定义区域。新的类VAE方法在MNIST、CelebA和FashionMNIST数据集上显示出平滑的插值过渡，但仍存在模糊性。将方法扩展到多个可学习向量时，在连续空间中自然地向类VQ-VAE模型演进。然而，当编码器输出多个向量时，模型退化为离散自编码器（VQ-AE），仅组合图像片段而未学习语义表示。研究结果强调了编码空间紧凑性和分散性在生成建模中的关键作用。", "conclusion": "编码空间紧凑性和分散性在生成建模中至关重要。本文深入揭示了VAEs和VQ-VAEs之间的内在联系，并为它们的设计和局限性提供了新的见解。", "translation": "这篇论文探讨了自编码器（AEs）的生成能力，并通过一个重新构建的训练框架，建立了变分自编码器（VAEs）与向量量化变分自编码器（VQ-VAEs）之间的联系。我们证明了AEs通过潜在空间插值和扰动展现出生成潜力，尽管受限于编码空间中未定义的区域。为了解决这个问题，我们提出了一种新的类VAE训练方法，该方法引入了聚类中心，以增强数据紧凑性并确保明确定义的潜在空间，而无需依赖传统的KL散度或重参数化技术。在MNIST、CelebA和FashionMNIST数据集上的实验结果显示了平滑的插值过渡，尽管模糊性依然存在。将这种方法扩展到多个可学习向量时，我们观察到在连续空间中自然地向类VQ-VAE模型演进。然而，当编码器输出多个向量时，模型退化为离散自编码器（VQ-AE），它结合图像片段而没有学习语义表示。我们的发现强调了编码空间紧凑性和分散性在生成建模中的关键作用，并提供了对VAEs和VQ-VAEs之间内在联系的见解，为它们的设计和局限性提供了新的视角。", "summary": "本文重新审视了自编码器（AEs）及其生成能力，并通过一个新颖的训练框架建立了变分自编码器（VAEs）与向量量化变分自编码器（VQ-VAEs）之间的联系。论文提出了一种类VAE方法，利用聚类中心创建紧凑且定义明确的潜在空间，从而避免了传统的KL散度或重参数化技术，解决了传统AEs的局限性。实验结果显示了平滑的插值，但也揭示了当扩展到多个向量时，模型可能退化为离散VQ-AE，无法学习语义表示。该研究强调了潜在空间特性对生成建模的重要性，并为VAEs和VQ-VAEs的设计提供了新见解。", "keywords": "变分自编码器,向量量化,潜在空间,生成模型,离散表示", "comments": "本文通过提出一种不依赖传统概率假设（KL散度、重参数化）的训练方法，为VAEs提供了新颖的视角。其引入聚类中心来确保潜在空间明确定义的方法，是改进生成建模的一种创新途径。关于使用多个离散向量时模型退化为VQ-AE的发现，突出了关键局限性，并为这类模型的设计选择提供了宝贵见解。对编码空间紧凑性和分散性的强调是本文的核心贡献。"}}
{"id": "2506.08979", "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "authors": ["Longyu Yang", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu", "Ping Hu"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08979v2", "summary": "LiDAR segmentation has emerged as an important task to enrich scene\nperception and understanding. Range-view-based methods have gained popularity\ndue to their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08979v2", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-23", "AI": {"title_translation": "重新思考恶劣天气下的距离视图激光雷达分割", "tldr": "本文提出了一种模块化、轻量级的框架，通过分别处理几何属性和反射强度来提高距离视图激光雷达分割模型在恶劣天气下的泛化能力。", "motivation": "距离视图激光雷达分割方法因其高计算效率和实时部署兼容性而受到欢迎，但在恶劣天气条件下的泛化性能尚未得到充分探索，这限制了它们在现实世界环境中的可靠性。", "method": "本文提出了一个模块化、轻量级的框架，通过将标准距离视图网络的初始干块重构为两个分支来分别处理几何属性和反射强度。具体来说，引入了几何异常抑制（GAS）模块以减少天气引起的空间噪声影响，以及反射畸变校准（RDC）模块通过记忆引导自适应实例归一化来校正反射畸变。处理后的特征被融合并传递给原始分割管道。", "result": "在不同基准和基线模型上的大量实验表明，该方法显著提高了对恶劣天气的泛化能力，且推理开销极小。", "conclusion": "本文提出的模块化、轻量级框架通过解决恶劣天气下距离视图激光雷达分割的独特挑战，为现实世界中的激光雷达分割提供了一种实用且有效的解决方案。", "translation": "激光雷达分割已成为丰富场景感知和理解的重要任务。基于距离视图的方法因其高计算效率和与实时部署的兼容性而受到欢迎。然而，它们在恶劣天气条件下的泛化性能仍未得到充分探索，这限制了它们在现实世界环境中的可靠性。在这项工作中，我们识别并分析了影响距离视图激光雷达分割在恶劣天气下泛化的独特挑战。为了应对这些挑战，我们提出了一种模块化、轻量级的框架，该框架在不改变现有模型核心架构的情况下增强了鲁棒性。我们的方法将标准距离视图网络的初始干块重构为两个分支，以分别处理几何属性和反射强度。具体来说，几何异常抑制（GAS）模块减少了天气引起的空间噪声的影响，而反射畸变校准（RDC）模块通过记忆引导自适应实例归一化来校正反射畸变。处理后的特征随后被融合并传递给原始分割管道。在不同基准和基线模型上的大量实验表明，我们的方法以极小的推理开销显著提高了对恶劣天气的泛化能力，为现实世界中的激光雷达分割提供了一种实用且有效的解决方案。", "summary": "本文针对距离视图激光雷达分割在恶劣天气下泛化能力不足的问题，深入分析了其面临的挑战。为此，提出了一种模块化、轻量级的框架，该框架通过重构初始干块为两个分支，分别处理几何属性（通过几何异常抑制GAS模块）和反射强度（通过反射畸变校准RDC模块），以增强模型鲁棒性而不改变现有核心架构。实验证明，该方法显著提升了模型在恶劣天气下的泛化性能，且计算开销极小，为实际应用提供了有效方案。", "keywords": "激光雷达分割, 距离视图, 恶劣天气, 泛化能力, 鲁棒性", "comments": "该论文的创新点在于提出了一个模块化且轻量级的框架，该框架能够在不改变现有模型核心架构的情况下，通过分别处理几何属性和反射强度来有效应对恶劣天气对激光雷达分割的影响。特别是GAS和RDC模块的设计，针对性地解决了天气引起的空间噪声和反射畸变问题，使其成为一个实用且高效的解决方案。该工作对于提高自动驾驶和机器人领域在复杂环境下的感知可靠性具有重要意义。"}}
{"id": "2507.17606", "title": "Time Deep Gradient Flow Method for pricing American options", "authors": ["Jasper Rou"], "categories": ["q-fin.CP", "cs.LG", "math.PR", "q-fin.MF", "91G20, 91G60, 68T07"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17606v1", "summary": "In this research, we explore neural network-based methods for pricing\nmultidimensional American put options under the BlackScholes and Heston model,\nextending up to five dimensions. We focus on two approaches: the Time Deep\nGradient Flow (TDGF) method and the Deep Galerkin Method (DGM). We extend the\nTDGF method to handle the free-boundary partial differential equation inherent\nin American options. We carefully design the sampling strategy during training\nto enhance performance. Both TDGF and DGM achieve high accuracy while\noutperforming conventional Monte Carlo methods in terms of computational speed.\nIn particular, TDGF tends to be faster during training than DGM.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17606v1", "cate": "q-fin.CP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于美式期权定价的时间深度梯度流方法", "tldr": "本文提出并扩展了基于神经网络的TDGF和DGM方法，用于高维美式期权定价，它们在精度和速度上均优于传统蒙特卡洛方法，尤其TDGF训练更快。", "motivation": "研究旨在探索和应用基于神经网络的方法（特别是TDGF和DGM）来定价多维美式看跌期权，以克服传统蒙特卡洛方法在计算速度上的局限性，并提高定价精度。", "method": "本文研究了两种基于神经网络的方法：时间深度梯度流（TDGF）方法和深度伽辽金方法（DGM），用于在Black-Scholes和Heston模型下定价多维美式看跌期权。研究扩展了TDGF方法以处理美式期权固有的自由边界偏微分方程，并精心设计了训练期间的采样策略以提高性能。", "result": "TDGF和DGM两种方法都实现了高精度，并在计算速度上优于传统的蒙特卡洛方法。具体而言，TDGF在训练期间比DGM更快。", "conclusion": "基于神经网络的TDGF和DGM方法是有效且高效的工具，用于定价多维美式期权，尤其TDGF在训练速度上表现出色，为期权定价提供了优于传统方法的替代方案。", "translation": "在这项研究中，我们探索了基于神经网络的方法，用于在Black-Scholes和Heston模型下对多维美式看跌期权进行定价，维度最高可达五维。我们重点关注两种方法：时间深度梯度流（TDGF）方法和深度伽辽金方法（DGM）。我们扩展了TDGF方法以处理美式期权固有的自由边界偏微分方程。我们精心设计了训练期间的采样策略以提高性能。TDGF和DGM都实现了高精度，同时在计算速度方面优于传统的蒙特卡洛方法。特别是，TDGF在训练期间往往比DGM更快。", "summary": "本研究探讨了基于神经网络的时间深度梯度流（TDGF）和深度伽辽金方法（DGM）在Black-Scholes和Heston模型下对多维美式看跌期权进行定价的应用。通过扩展TDGF以处理自由边界PDE并优化采样策略，两种方法均表现出高精度，并在计算速度上超越传统蒙特卡洛方法，其中TDGF在训练效率上优于DGM。", "keywords": "美式期权定价, 深度学习, 时间深度梯度流, 深度伽辽金方法, 蒙特卡洛", "comments": "该论文的创新之处在于将时间深度梯度流（TDGF）方法扩展应用于处理美式期权特有的自由边界偏微分方程，并将其与深度伽辽金方法（DGM）进行比较。其重要性在于证明了这些神经网络方法在处理高维美式期权定价问题上，不仅能达到高精度，还能显著提升计算速度，超越了传统的蒙特卡洛方法，为金融工程领域提供了高效的数值解法。"}}
{"id": "2507.09205", "title": "Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Yuqi Ren", "Deyi Xiong"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09205v3", "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model to enhance its generative capabilities in Tibetan. To evaluate the\nTibetan capabilities of the model, we create new high-quality Tibetan\nbenchmarks, and complement them with existing public benchmarks. Experimental\nresults demonstrate that our model consistently and significantly outperforms\nboth open-source models of similar scale and Tibetan-tailored models across a\nwide range of tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09205v3", "cate": "cs.CL", "date": "2025-07-12", "updated": "2025-07-23", "AI": {"title_translation": "策展数据与持续预训练推进藏语大语言模型", "tldr": "通过策展高质量藏语数据并进行持续预训练，显著提升了藏语大语言模型的性能。", "motivation": "藏语作为一种典型的低资源语言，在现有大语言模型中代表性不足，主要原因是高质量训练语料的稀缺。", "method": "研究团队策展了迄今为止最大的藏语预训练语料库，整合了多样化来源的数据，并应用了专门为藏语定制的数据清洗和处理流程。接着，利用这些策展数据对一个多语言基础模型进行持续的预训练/后训练，以增强其在藏语方面的生成能力。为了评估模型，还创建了新的高质量藏语基准测试，并辅以现有的公共基准测试。", "result": "实验结果表明，该模型在广泛的任务中，始终显著优于同等规模的开源模型和专门针对藏语定制的模型。", "conclusion": "通过高质量数据策展和持续预训练的方法，可以有效提升大语言模型在低资源语言（如藏语）上的性能，并为该领域的发展提供了新的基准。", "translation": "大语言模型在多种语言中取得了显著进展。然而，藏语作为一种代表性的低资源语言，由于高质量训练语料的稀缺，在现有模型中代表性尤其不足。为了解决这一差距，我们策展了迄今为止最大的藏语预训练语料库，整合了来自不同来源的数据，并应用了专门为藏语定制的数据清洗和处理流程。利用这些策展数据，我们持续预训练/后训练了一个多语言基础模型，以增强其在藏语方面的生成能力。为了评估模型的藏语能力，我们创建了新的高质量藏语基准测试，并辅以现有的公共基准测试。实验结果表明，我们的模型在广泛的任务中，始终显著优于同等规模的开源模型和专门针对藏语定制的模型。", "summary": "本研究旨在解决藏语作为低资源语言在大语言模型中代表性不足的问题。通过构建迄今最大的藏语预训练语料库并进行精细化处理，研究人员对多语言基础模型进行了持续预训练。为评估模型性能，团队还创建了新的藏语基准测试。实验证明，该模型在多项任务上均显著超越了现有同类模型，有效提升了藏语大语言模型的性能。", "keywords": "藏语大语言模型, 数据策展, 持续预训练, 低资源语言, 性能基准", "comments": "该论文的创新点在于针对低资源语言（藏语）的数据稀缺问题，通过精心策展大规模高质量语料并结合持续预训练策略，显著提升了藏语大语言模型的性能。这对于促进低资源语言的数字包容性和语言技术发展具有重要意义。新创建的藏语基准测试也为未来的研究提供了宝贵的评估工具。"}}
{"id": "2507.17491", "title": "Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement", "authors": ["Nazatul H. Sultan", "Xinlong Guan", "Josef Pieprzyk", "Wei Ni", "Sharif Abuadbba", "Hajime Suzuki"], "categories": ["cs.CR", "cs.NI", "68M25", "C.2.2"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted at RAID 2025", "url": "http://arxiv.org/abs/2507.17491v1", "summary": "As 5G networks expand into critical infrastructure, secure and efficient user\nauthentication is more important than ever. The 5G-AKA protocol, standardized\nby 3GPP in TS 33.501, is central to authentication in current 5G deployments.\nIt provides mutual authentication, user privacy, and key secrecy. However,\ndespite its adoption, 5G-AKA has known limitations in both security and\nperformance. While it focuses on protecting privacy against passive attackers,\nrecent studies show its vulnerabilities to active attacks. It also relies on a\nsequence number mechanism to prevent replay attacks, requiring perfect\nsynchronization between the device and the core network. This stateful design\nadds complexity, causes desynchronization, and incurs extra communication\noverhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing\npast communications if long-term keys are compromised-an increasing concern\namid sophisticated threats. This paper proposes an enhanced authentication\nprotocol that builds on 5G-AKA's design while addressing its shortcomings.\nFirst, we introduce a stateless version that removes sequence number reliance,\nreducing complexity while staying compatible with existing SIM cards and\ninfrastructure. We then extend this design to add PFS with minimal\ncryptographic overhead. Both protocols are rigorously analyzed using ProVerif,\nconfirming their compliance with all major security requirements, including\nresistance to passive and active attacks, as well as those defined by 3GPP and\nacademic studies. We also prototype both protocols and evaluate their\nperformance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the\nproposed protocols offer stronger security with only minor computational\noverhead, making them practical, future-ready solutions for 5G and beyond.", "comment": "Accepted at RAID 2025", "pdf_url": "http://arxiv.org/pdf/2507.17491v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "5G中的主动攻击弹性：认证和密钥协商的新视角", "tldr": "该论文提出了一种增强型5G认证协议，解决了现有5G-AKA协议在主动攻击、无状态设计和完美前向保密性方面的不足，并通过形式化分析和原型评估证明了其安全性和实用性。", "motivation": "当前的5G-AKA协议虽然被广泛采用，但存在安全和性能限制，尤其是在抵御主动攻击方面存在漏洞，并且其有状态的序列号机制导致复杂性、去同步和额外通信开销。更关键的是，5G-AKA缺乏完美前向保密性（PFS），在长期密钥泄露时会暴露过去的通信。", "method": "论文提出了一种增强型认证协议，它在5G-AKA设计的基础上解决了上述缺点。首先，引入了一个无状态版本，消除了对序列号的依赖，降低了复杂性，同时与现有SIM卡和基础设施兼容。其次，在此基础上扩展，以最小的加密开销增加了完美前向保密性（PFS）。这两种协议都使用ProVerif进行了严格分析，并进行了原型实现和性能评估。", "result": "通过ProVerif分析，确认所提出的协议符合所有主要安全要求，包括抵御被动和主动攻击，以及3GPP和学术研究定义的要求。原型评估表明，与5G-AKA和5G-AKA'（USENIX'21）相比，所提出的协议提供了更强的安全性，而计算开销仅微乎其微。", "conclusion": "所提出的协议为5G及未来提供了实用且面向未来的解决方案，它们在仅增加少量计算开销的情况下提供了更强的安全性。", "translation": "随着5G网络扩展到关键基础设施，安全高效的用户认证比以往任何时候都更加重要。3GPP在TS 33.501中标准化的5G-AKA协议是当前5G部署中认证的核心。它提供相互认证、用户隐私和密钥保密性。然而，尽管被采纳，5G-AKA在安全性和性能方面都存在已知限制。虽然它侧重于保护隐私免受被动攻击者侵害，但最近的研究表明其对主动攻击存在漏洞。它还依赖于序列号机制来防止重放攻击，这需要设备和核心网络之间的完美同步。这种有状态的设计增加了复杂性，导致去同步，并产生额外的通信开销。更关键的是，5G-AKA缺乏完美前向保密性（PFS），如果长期密钥受到威胁，就会暴露过去的通信——在复杂的威胁面前，这日益成为一个问题。本文提出了一种增强型认证协议，它建立在5G-AKA的设计之上，同时解决了其缺点。首先，我们引入了一个无状态版本，消除了对序列号的依赖，降低了复杂性，同时与现有SIM卡和基础设施兼容。然后，我们扩展此设计以最小的加密开销添加PFS。两种协议都使用ProVerif进行了严格分析，证实它们符合所有主要安全要求，包括抵御被动和主动攻击，以及3GPP和学术研究定义的要求。我们还对两种协议进行了原型设计，并评估了它们相对于5G-AKA和5G-AKA'（USENIX'21）的性能。我们的结果表明，所提出的协议以微小的计算开销提供了更强的安全性，使其成为5G及未来实用且面向未来的解决方案。", "summary": "本论文针对现有5G-AKA认证协议在主动攻击防御、无状态设计和完美前向保密性方面的不足，提出了一种增强型认证协议。该协议首先引入无状态版本以消除序列号依赖并兼容现有基础设施，随后扩展以增加完美前向保密性。通过ProVerif形式化验证和原型性能评估，证明了所提协议在提供更强安全性的同时，仅引入微小计算开销，是5G及未来网络的实用解决方案。", "keywords": "5G, 认证, 主动攻击, 无状态, 完美前向保密性", "comments": "该论文的创新之处在于其对5G-AKA协议的改进，特别是在引入无状态设计和完美前向保密性方面的努力，这直接解决了现有协议在实际部署中面临的复杂性、同步问题以及长期密钥泄露风险。通过形式化验证和原型评估，增强了其方案的可信度，对于提升5G网络在面对日益复杂的主动攻击时的韧性具有重要意义。"}}
{"id": "2507.17369", "title": "Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java", "authors": ["Corentin Latappy", "Thomas Degueule", "Jean-Rémy Falleri", "Romain Robbes", "Lina Ochoa"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17369v1", "summary": "Understanding API evolution and the introduction of breaking changes (BCs) in\nsoftware libraries is essential for library maintainers to manage backward\ncompatibility and for researchers to conduct empirical studies on software\nlibrary evolution. In Java, tools such as JApiCmp and Revapi are commonly used\nto detect BCs between library releases, but their reliance on binary JARs\nlimits their applicability. This restriction hinders large-scale longitudinal\nstudies of API evolution and fine-grained analyses such as commit-level BC\ndetection. In this paper, we introduce Roseau, a novel static analysis tool\nthat constructs technology-agnostic API models from library code equipped with\nrich semantic analyses. API models can be analyzed to study API evolution and\ncompared to identify BCs between any two versions of a library (releases,\ncommits, branches, etc.). Unlike traditional approaches, Roseau can build API\nmodels from source code or bytecode, and is optimized for large-scale\nlongitudinal analyses of library histories. We assess the accuracy,\nperformance, and suitability of Roseau for longitudinal studies of API\nevolution, using JApiCmp and Revapi as baselines. We extend and refine an\nestablished benchmark of BCs and show that Roseau achieves higher accuracy (F1\n= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular\nlibraries from Maven Central and find that Roseau delivers excellent\nperformance, detecting BCs between versions in under two seconds, including in\nlibraries with hundreds of thousands of lines of code. We further illustrate\nthe limitations of JApiCmp and Revapi for longitudinal studies and the novel\nanalysis capabilities offered by Roseau by tracking the evolution of Google's\nGuava API and the introduction of BCs over 14 years and 6,839 commits, reducing\nanalysis times from a few days to a few minutes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17369v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Roseau：Java中快速、准确、基于源代码的API破坏性变更分析", "tldr": "Roseau是一种新的静态分析工具，用于Java中快速、准确地从源代码或字节码检测API破坏性变更，解决了现有工具的局限性，并支持大规模API演化研究。", "motivation": "理解API演化和破坏性变更的引入对于库维护者管理向后兼容性以及研究人员进行软件库演化实证研究至关重要。现有工具（如JApiCmp和Revapi）依赖二进制JAR，限制了其在大规模纵向研究和细粒度分析（如提交级别检测）中的适用性。", "method": "引入Roseau，一个新颖的静态分析工具。它从库代码构建技术无关的API模型，并配备丰富的语义分析。API模型可以用于研究API演化，并比较以识别库任意两个版本（发布、提交、分支等）之间的破坏性变更。Roseau可以从源代码或字节码构建API模型，并针对库历史的大规模纵向分析进行了优化。", "result": "Roseau在扩展和改进的破坏性变更基准测试中，实现了比JApiCmp (F1=0.86) 和Revapi (F1=0.91) 更高的准确性 (F1=0.99)。分析Maven Central的60个流行库，Roseau在不到两秒内检测出版本间的破坏性变更，包括包含数十万行代码的库，表现出卓越的性能。通过跟踪Google Guava API在14年和6,839次提交中的演化，将分析时间从几天缩短到几分钟，进一步展示了Roseau在纵向研究中的新颖分析能力和对现有工具局限性的克服。", "conclusion": "Roseau是一种高效、准确的API破坏性变更分析工具，尤其适用于Java库的大规模纵向研究，克服了现有二进制依赖工具的局限性，并显著提高了分析效率和准确性。", "translation": "理解API演化和软件库中破坏性变更（BCs）的引入对于库维护者管理向后兼容性以及研究人员进行软件库演化实证研究至关重要。在Java中，JApiCmp和Revapi等工具常用于检测库版本发布之间的BCs，但它们对二进制JAR的依赖限制了其适用性。这种限制阻碍了API演化的大规模纵向研究和细粒度分析，例如提交级别的BC检测。在本文中，我们引入了Roseau，一个新颖的静态分析工具，它从库代码构建技术无关的API模型，并配备了丰富的语义分析。API模型可以被分析以研究API演化，并进行比较以识别库任意两个版本（发布、提交、分支等）之间的BCs。与传统方法不同，Roseau可以从源代码或字节码构建API模型，并针对库历史的大规模纵向分析进行了优化。我们以JApiCmp和Revapi为基线，评估了Roseau在API演化纵向研究中的准确性、性能和适用性。我们扩展并完善了一个既定的BCs基准，结果显示Roseau实现了比JApiCmp (F1 = 0.86) 和Revapi (F1 = 0.91) 更高的准确性 (F1 = 0.99)。我们分析了Maven Central的60个流行库，发现Roseau表现出卓越的性能，在不到两秒内检测出版本间的BCs，包括包含数十万行代码的库。我们通过跟踪Google Guava API在14年和6,839次提交中的演化以及BCs的引入，进一步说明了JApiCmp和Revapi在纵向研究中的局限性以及Roseau提供的新颖分析能力，将分析时间从几天缩短到几分钟。", "summary": "本文介绍了Roseau，一个针对Java库的静态分析工具，用于快速、准确地检测API破坏性变更。Roseau通过从源代码或字节码构建技术无关的API模型，克服了现有工具依赖二进制JAR的局限性，从而支持大规模、细粒度的API演化纵向研究。实验结果表明，Roseau在准确性（F1=0.99）和性能上均优于现有工具（JApiCmp和Revapi），并能显著加速对大型库历史的分析。", "keywords": "API演化, 破坏性变更, 静态分析, Java, 源代码分析", "comments": "Roseau的创新之处在于其能够从源代码或字节码构建API模型，并针对大规模纵向分析进行优化，这显著扩展了API破坏性变更检测的范围和深度。它解决了现有工具在处理大型项目历史时的性能瓶颈和粒度限制，对于软件库维护者和API演化研究人员都具有重要意义。"}}
{"id": "2507.17205", "title": "VBCD: A Voxel-Based Framework for Personalized Dental Crown Design", "authors": ["Linda Wei", "Chang Liu", "Wenran Zhang", "Zengji Zhang", "Shaoting Zhang", "Hongsheng Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17205v1", "summary": "The design of restorative dental crowns from intraoral scans is\nlabor-intensive for dental technicians. To address this challenge, we propose a\nnovel voxel-based framework for automated dental crown design (VBCD). The VBCD\nframework generates an initial coarse dental crown from voxelized intraoral\nscans, followed by a fine-grained refiner incorporating distance-aware\nsupervision to improve accuracy and quality. During the training stage, we\nemploy the Curvature and Margin line Penalty Loss (CMPL) to enhance the\nalignment of the generated crown with the margin line. Additionally, a\npositional prompt based on the FDI tooth numbering system is introduced to\nfurther improve the accuracy of the generated dental crowns. Evaluation on a\nlarge-scale dataset of intraoral scans demonstrated that our approach\noutperforms existing methods, providing a robust solution for personalized\ndental crown design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17205v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VBCD：一种用于个性化牙冠设计的体素化框架", "tldr": "VBCD是一个体素化框架，通过粗略生成和精细优化，结合距离感知监督、曲率和边缘线惩罚损失以及FDI牙齿编号系统，实现了自动化且优于现有方法的个性化牙冠设计。", "motivation": "修复性牙冠的设计对于牙科技师来说是劳动密集型的，因此需要一种自动化解决方案。", "method": "本研究提出了VBCD（体素化牙冠设计）框架，通过以下步骤实现自动化牙冠设计：1. 从体素化的口内扫描生成初始粗略牙冠。2. 使用结合距离感知监督的精细化器提高准确性和质量。3. 在训练阶段采用曲率和边缘线惩罚损失（CMPL）来增强生成牙冠与边缘线的对齐。4. 引入基于FDI牙齿编号系统的位置提示以进一步提高生成牙冠的准确性。", "result": "在大型口内扫描数据集上的评估表明，VBCD方法优于现有方法。", "conclusion": "VBCD为个性化牙冠设计提供了一个强大的解决方案。", "translation": "从口内扫描设计修复性牙冠对牙科技师来说是劳动密集型的。为了解决这一挑战，我们提出了一种新颖的基于体素的自动化牙冠设计框架（VBCD）。VBCD框架从体素化的口内扫描生成初始粗略牙冠，然后通过结合距离感知监督的精细化器来提高准确性和质量。在训练阶段，我们采用曲率和边缘线惩罚损失（CMPL）来增强生成牙冠与边缘线的对齐。此外，引入了基于FDI牙齿编号系统的位置提示，以进一步提高生成牙冠的准确性。在大型口内扫描数据集上的评估表明，我们的方法优于现有方法，为个性化牙冠设计提供了一个强大的解决方案。", "summary": "VBCD是一种新颖的体素化框架，旨在自动化牙冠设计，以解决传统方法劳动密集的问题。该框架首先从体素化的口内扫描生成粗略牙冠，随后通过精细化器进行优化，并结合距离感知监督、曲率和边缘线惩罚损失（CMPL）以及基于FDI牙齿编号系统的位置提示，以提高设计精度和质量。实验结果表明，VBCD在大型数据集上表现优异，为个性化牙冠设计提供了鲁棒的解决方案。", "keywords": "体素化, 牙冠设计, 自动化, 口内扫描, 个性化", "comments": "该论文提出了一种创新的体素化框架VBCD，用于自动化牙冠设计，解决了牙科技师工作量大的问题。其创新点在于结合了粗略生成与精细化、距离感知监督、专门设计的CMPL损失函数以及FDI牙齿编号系统作为位置提示，这些都旨在提高生成牙冠的准确性和与边缘线的对齐。该方法在实际应用中具有重要意义，因为它能显著提高牙冠设计的效率和个性化程度。"}}
{"id": "2507.17486", "title": "Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease", "authors": ["Hugues Roy", "Reuben Dorent", "Ninon Burgos"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17486v1", "summary": "Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for\nidentifying deviations from healthy subject data and thus facilitating the\ndiagnosis of neurological disorders. In this work, we focus on Bayesian flow\nnetworks (BFNs), a novel class of generative models, which have not yet been\napplied to medical imaging or anomaly detection. BFNs combine the strength of\ndiffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension\nof BFNs for UAD, designed to: i) perform conditional image generation under\nhigh levels of spatially correlated noise, and ii) preserve subject specificity\nby incorporating a recursive feedback from the input image throughout the\ngenerative process. We evaluate AnoBFN on the challenging task of Alzheimer's\ndisease-related anomaly detection in FDG PET images. Our approach outperforms\nother state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and\ndiffusion models (AnoDDPM), demonstrating its effectiveness at detecting\nanomalies while reducing false positive rates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17486v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用贝叶斯流网络进行无监督异常检测：在阿尔茨海默病背景下应用于脑部FDG PET", "tldr": "提出AnoBFN，一种基于贝叶斯流网络的新型无监督异常检测方法，在脑部FDG PET图像中有效检测阿尔茨海默病相关异常，优于现有SOTA方法。", "motivation": "无监督异常检测在神经影像学中识别健康受试者数据的偏差，从而促进神经系统疾病的诊断至关重要。本文旨在将贝叶斯流网络（一种新型生成模型）应用于医学影像和异常检测，特别是阿尔茨海默病。", "method": "本文引入了AnoBFN，它是贝叶斯流网络（BFNs）的扩展，用于无监督异常检测。AnoBFN旨在：i) 在高空间相关噪声下执行条件图像生成；ii) 通过在生成过程中整合来自输入图像的递归反馈来保持受试者特异性。它结合了扩散框架和贝叶斯推理的优势。", "result": "AnoBFN在FDG PET图像中对阿尔茨海默病相关异常检测的挑战性任务中进行了评估。结果表明，该方法优于其他基于VAE（beta-VAE）、GAN（f-AnoGAN）和扩散模型（AnoDDPM）的最新方法，证明了其在检测异常方面的有效性，同时降低了假阳性率。", "conclusion": "本文成功地将贝叶斯流网络应用于无监督异常检测，特别是阿尔茨海默病脑部FDG PET图像的异常识别，并取得了优于现有先进方法的性能，证明了其在神经影像诊断中的潜力。", "translation": "无监督异常检测（UAD）在神经影像学中识别健康受试者数据的偏差，从而促进神经系统疾病的诊断方面发挥着关键作用。在这项工作中，我们专注于贝叶斯流网络（BFNs），这是一种新型的生成模型，尚未应用于医学影像或异常检测。BFNs结合了扩散框架和贝叶斯推理的优势。我们引入了AnoBFN，这是BFNs在UAD方面的扩展，旨在：i) 在高空间相关噪声下执行条件图像生成；ii) 通过在生成过程中整合来自输入图像的递归反馈来保持受试者特异性。我们在FDG PET图像中阿尔茨海默病相关异常检测这一具有挑战性的任务上评估了AnoBFN。我们的方法优于其他基于VAE（beta-VAE）、GAN（f-AnoGAN）和扩散模型（AnoDDPM）的最新方法，证明了其在检测异常方面的有效性，同时降低了假阳性率。", "summary": "该论文提出了一种名为AnoBFN的新型无监督异常检测方法，该方法是贝叶斯流网络（BFNs）的扩展，首次应用于医学影像领域。AnoBFN结合了扩散框架和贝叶斯推理的优势，能够在高噪声下生成图像并保持受试者特异性。在阿尔茨海默病脑部FDG PET图像的异常检测任务中，AnoBFN的表现优于现有的VAE、GAN和扩散模型方法，有效降低了假阳性率，显示出其在神经影像诊断中的潜力。", "keywords": "无监督异常检测, 贝叶斯流网络, 阿尔茨海默病, FDG PET, 神经影像", "comments": "该研究的创新之处在于首次将贝叶斯流网络（BFNs）应用于医学影像和无监督异常检测领域。AnoBFN通过结合扩散框架和贝叶斯推理，并引入递归反馈机制，有效地解决了高噪声下图像生成和受试者特异性保持的挑战。其在阿尔茨海默病FDG PET图像上的优异表现，表明了该方法在神经系统疾病早期诊断方面的巨大潜力。"}}
{"id": "2507.17657", "title": "Attention (as Discrete-Time Markov) Chains", "authors": ["Yotam Erel", "Olaf Dünkel", "Rishabh Dabral", "Vladislav Golyanik", "Christian Theobalt", "Amit H. Bermano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.17657v1", "summary": "We introduce a new interpretation of the attention matrix as a discrete-time\nMarkov chain. Our interpretation sheds light on common operations involving\nattention scores such as selection, summation, and averaging in a unified\nframework. It further extends them by considering indirect attention,\npropagated through the Markov chain, as opposed to previous studies that only\nmodel immediate effects. Our main observation is that tokens corresponding to\nsemantically similar regions form a set of metastable states, where the\nattention clusters, while noisy attention scores tend to disperse. Metastable\nstates and their prevalence can be easily computed through simple matrix\nmultiplication and eigenanalysis, respectively. Using these lightweight tools,\nwe demonstrate state-of-the-art zero-shot segmentation. Lastly, we define\nTokenRank -- the steady state vector of the Markov chain, which measures global\ntoken importance. We demonstrate that using it brings improvements in\nunconditional image generation. We believe our framework offers a fresh view of\nhow tokens are being attended in modern visual transformers.", "comment": "Project page: https://yoterel.github.io/attention_chains/", "pdf_url": "http://arxiv.org/pdf/2507.17657v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "注意力（作为离散时间马尔可夫）链", "tldr": "该论文将注意力矩阵解释为离散时间马尔可夫链，以统一注意力操作并考虑间接注意力。它发现语义相似的标记形成亚稳态，并引入了衡量全局标记重要性的TokenRank，在零样本分割和无条件图像生成方面取得了改进。", "motivation": "之前的研究只模拟了直接的注意力效果。本文旨在提供一种将注意力矩阵解释为离散时间马尔可夫链的新方法，以在一个统一的框架中阐明常见的注意力操作，并通过考虑间接注意力来扩展它们，并为现代视觉 Transformer 中标记如何被关注提供一个全新的视角。", "method": "通过将注意力矩阵解释为离散时间马尔可夫链，统一了注意力分数涉及的常见操作（如选择、求和、平均）。通过考虑通过马尔可夫链传播的间接注意力来扩展现有模型。观察到与语义相似区域对应的标记形成亚稳态，并通过简单的矩阵乘法和特征分析计算其普遍性。定义TokenRank作为马尔可夫链的稳态向量，以衡量全局标记的重要性。", "result": "使用亚稳态和其普遍性实现了最先进的零样本分割。使用TokenRank带来了无条件图像生成的改进。", "conclusion": "该框架通过将注意力解释为离散时间马尔可夫链，为现代视觉 Transformer 中标记如何被关注提供了全新的视角，从而能够统一理解操作、模拟间接效应、识别语义聚类以及衡量全局标记重要性。", "translation": "我们引入了一种将注意力矩阵解释为离散时间马尔可夫链的新方法。我们的解释在一个统一的框架中阐明了涉及注意力分数（如选择、求和、平均）的常见操作。它通过考虑通过马尔可夫链传播的间接注意力进一步扩展了这些操作，而之前的研究只模拟了直接效果。我们的主要观察是，与语义相似区域对应的标记形成了一组亚稳态，注意力在此处聚集，而嘈杂的注意力分数则倾向于分散。亚稳态及其普遍性可以通过简单的矩阵乘法和特征分析分别轻松计算。利用这些轻量级工具，我们展示了最先进的零样本分割。最后，我们定义了TokenRank——马尔可夫链的稳态向量，它衡量了全局标记的重要性。我们证明使用它带来了无条件图像生成的改进。我们相信我们的框架为现代视觉 Transformer 中标记如何被关注提供了全新的视角。", "summary": "该论文提出将注意力矩阵视为离散时间马尔可夫链，从而统一了注意力操作并引入了间接注意力的概念。研究发现，语义相似的标记形成亚稳态，注意力在此聚集，可通过矩阵乘法和特征分析计算。基于此，论文在零样本分割任务中取得了最先进的成果。此外，论文还定义了衡量全局标记重要性的TokenRank（马尔可夫链的稳态向量），并展示了其在无条件图像生成方面的改进。该框架为理解视觉Transformer中的注意力机制提供了新视角。", "keywords": "Attention, Markov chain, Transformer, Zero-shot segmentation, TokenRank", "comments": "该论文通过将注意力机制与离散时间马尔可夫链相结合，提供了一个新颖且统一的视角来理解和分析Transformer中的注意力操作。其创新点在于引入了“间接注意力”的概念，并通过马尔可夫链的性质识别“亚稳态”和“TokenRank”，从而揭示了注意力在语义聚类和全局重要性方面的作用。这种理论框架不仅深化了对注意力机制的理解，而且通过在零样本分割和无条件图像生成任务中的实际应用，证明了其有效性和潜在价值。"}}
{"id": "2507.01939", "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars", "authors": ["Xiaosheng Zhao", "Yang Huang", "Guirong Xue", "Xiao Kong", "Jifeng Liu", "Xiaoyu Tang", "Timothy C. Beers", "Yuan-Sen Ting", "A-Li Luo"], "categories": ["astro-ph.IM", "astro-ph.SR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      27 pages, 8 figures, 5 tables. Updated with minor corrections to flux normalization, and to related tables and figures. Submitted to AAS Journals. Comments welcome", "url": "http://arxiv.org/abs/2507.01939v2", "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.", "comment": "27 pages, 8 figures, 5 tables. Updated with minor corrections to flux\n  normalization, and to related tables and figures. Submitted to AAS Journals.\n  Comments welcome", "pdf_url": "http://arxiv.org/pdf/2507.01939v2", "cate": "astro-ph.IM", "date": "2025-07-02", "updated": "2025-07-23", "AI": {"title_translation": "SpecCLIP：对恒星光谱测量进行对齐和转换", "tldr": "SpecCLIP是一个受LLM启发的恒星光谱基础模型框架，它通过对比学习对齐不同仪器的光谱，并利用辅助解码器实现光谱类型间的转换，从而提高恒星参数估计和化学丰度测定的精度，并支持异常检测。", "motivation": "受到大型语言模型（LLMs）在自然语言理解方面成功的启发，该研究旨在将LLM启发的方法应用于恒星光谱分析，以学习鲁棒且信息丰富的嵌入，支持多样的下游应用。", "method": "SpecCLIP是一个基础模型框架，其方法包括：在LAMOST低分辨率和Gaia XP两种光谱类型上进行预训练；使用CLIP（对比语言-图像预训练）框架进行对比对齐，以关联来自不同仪器的光谱；通过辅助解码器保留光谱特有信息并实现光谱类型间的转换（预测），其中信息保留通过最大化嵌入和输入光谱之间的互信息实现。", "result": "构建了一个跨光谱框架，实现了内在校准和跨仪器灵活应用。在适度大小的标记数据集上进行微调后，提高了模型对恒星参数估计和化学丰度测定等任务的适应性。提高了参数估计的准确性和精度，并与外部巡天数据进行了基准测试。其相似性搜索和跨光谱预测能力为异常检测提供了潜力。", "conclusion": "对比训练的基础模型，辅以光谱感知解码器，可以推动高精度恒星光谱学的发展。", "translation": "近年来，大型语言模型（LLMs）通过庞大的数据集和大规模参数化彻底改变了自然语言理解。受此成功启发，我们提出了SpecCLIP，一个基础模型框架，它将LLM启发的方法扩展到恒星光谱分析。恒星光谱，类似于结构化语言，编码了关于恒星丰富的物理和化学信息。通过在大规模光谱数据集上训练基础模型，我们的目标是学习鲁棒且信息丰富的嵌入，以支持多样化的下游应用。作为概念验证，SpecCLIP涉及在两种光谱类型——LAMOST低分辨率和Gaia XP——上进行预训练，随后使用CLIP（对比语言-图像预训练）框架进行对比对齐，该框架经过调整以关联来自不同仪器的光谱。这种对齐辅以辅助解码器，这些解码器保留了光谱特有信息并能够实现光谱类型之间的转换（预测），其中前者通过最大化嵌入和输入光谱之间的互信息来实现。结果是一个跨光谱框架，能够实现内在校准和跨仪器的灵活应用。我们证明，在适度大小的标记数据集上对这些模型进行微调，可以提高其对恒星参数估计和化学丰度测定等任务的适应性。SpecCLIP还提高了参数估计的准确性和精度，并与外部巡天数据进行了基准测试。此外，其相似性搜索和跨光谱预测能力为异常检测提供了潜力。我们的结果表明，通过对比训练的基础模型，并辅以光谱感知解码器，可以推进高精度恒星光谱学。", "summary": "SpecCLIP是一个受大型语言模型（LLM）启发，并借鉴CLIP框架的恒星光谱基础模型。它通过在不同光谱数据集上进行预训练和跨仪器光谱的对比对齐，来学习鲁棒的恒星光谱嵌入。辅助解码器用于保留光谱特有信息并实现不同光谱类型间的转换。该框架实现了内在校准和灵活的跨仪器应用，显著提高了恒星参数估计和化学丰度测定的准确性和适应性，并展现出异常检测的潜力。", "keywords": "恒星光谱, 基础模型, 对比学习, CLIP, 天体物理学", "comments": "SpecCLIP的创新之处在于将LLM和CLIP框架的成功经验引入到天体物理学中的恒星光谱分析领域，通过对比学习和辅助解码器有效地处理多源异构光谱数据。这为恒星参数估计和异常检测提供了新的范式，有望显著提高光谱数据分析的精度和效率。"}}
{"id": "2507.16853", "title": "MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation", "authors": ["Ning Li", "Xiangmou Qu", "Jiamu Zhou", "Jun Wang", "Muning Wen", "Kounianhua Du", "Xingyu Lou", "Qiuying Peng", "Jun Wang", "Weinan Zhang"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      A technical report on a GUI agent based on multi-agent systems", "url": "http://arxiv.org/abs/2507.16853v1", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled the\ndevelopment of mobile agents that can understand visual inputs and follow user\ninstructions, unlocking new possibilities for automating complex tasks on\nmobile devices. However, applying these models to real-world mobile scenarios\nremains a significant challenge due to the long-horizon task execution,\ndifficulty in error recovery, and the cold-start problem in unfamiliar\nenvironments. To address these challenges, we propose MobileUse, a GUI agent\ndesigned for robust and adaptive mobile task execution. To improve resilience\nin long-horizon tasks and dynamic environments, we introduce a hierarchical\nreflection architecture that enables the agent to self-monitor, detect, and\nrecover from errors across multiple temporal scales-ranging from individual\nactions to overall task completion-while maintaining efficiency through a\nreflection-on-demand strategy. To tackle cold-start issues, we further\nintroduce a proactive exploration module, which enriches the agent's\nunderstanding of the environment through self-planned exploration. Evaluations\non AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse\nestablishes new state-of-the-art performance, achieving success rates of 62.9%\nand 44.2%, respectively. To facilitate real-world applications, we release an\nout-of-the-box toolkit for automated task execution on physical mobile devices,\nwhich is available at https://github.com/MadeAgents/mobile-use.", "comment": "A technical report on a GUI agent based on multi-agent systems", "pdf_url": "http://arxiv.org/pdf/2507.16853v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "MobileUse：一种具有分层反射的GUI代理，用于自主移动操作", "tldr": "MobileUse是一个新的GUI代理，通过引入分层反射架构和主动探索模块，有效解决了移动设备上长周期任务执行、错误恢复困难和冷启动等挑战，并在AndroidWorld和AndroidLab基准测试中取得了最先进的性能。", "motivation": "现有基于多模态大型语言模型（MLLMs）的移动代理在真实世界的移动场景中，面临长周期任务执行、错误恢复困难以及在不熟悉环境中的冷启动问题，这些都阻碍了其在移动设备上自动化复杂任务的广泛应用。", "method": "本文提出了MobileUse，一个用于鲁棒和自适应移动任务执行的GUI代理。为提高长周期任务和动态环境中的弹性，MobileUse引入了分层反射架构，该架构支持代理在多时间尺度上（从单个动作到整体任务完成）进行自我监控、错误检测和恢复，并通过按需反射策略保持效率。为解决冷启动问题，MobileUse还引入了一个主动探索模块，通过自我规划的探索来丰富代理对环境的理解。", "result": "在AndroidWorld和AndroidLab基准测试中的评估表明，MobileUse建立了新的最先进性能，成功率分别达到62.9%和44.2%。此外，作者还发布了一个开箱即用的工具包，用于在物理移动设备上执行自动化任务。", "conclusion": "MobileUse通过其创新的分层反射架构和主动探索模块，显著提升了移动设备上自动化任务执行的鲁棒性和适应性，有效解决了长周期任务中的错误恢复和冷启动问题，并在行业基准测试中取得了领先的性能，为真实世界移动应用提供了强大的解决方案。", "translation": "多模态大型语言模型（MLLMs）的最新进展使得移动代理得以开发，这些代理能够理解视觉输入并遵循用户指令，为在移动设备上自动化复杂任务开辟了新的可能性。然而，由于长周期任务执行、错误恢复困难以及在不熟悉环境中的冷启动问题，将这些模型应用于真实世界的移动场景仍然是一个重大挑战。为了解决这些挑战，我们提出了MobileUse，一个旨在实现鲁棒和自适应移动任务执行的GUI代理。为了提高长周期任务和动态环境中的弹性，我们引入了一种分层反射架构，该架构使代理能够跨多个时间尺度（从单个动作到整体任务完成）进行自我监控、检测和从错误中恢复，同时通过按需反射策略保持效率。为了解决冷启动问题，我们进一步引入了一个主动探索模块，该模块通过自我规划的探索来丰富代理对环境的理解。在AndroidWorld和AndroidLab基准测试中的评估表明，MobileUse建立了新的最先进性能，成功率分别达到62.9%和44.2%。为了促进实际应用，我们发布了一个开箱即用的工具包，用于在物理移动设备上执行自动化任务，该工具包可在https://github.com/MadeAgents/mobile-use获取。", "summary": "本文介绍了MobileUse，一个用于自主移动操作的GUI代理。该代理旨在解决当前MLLM驱动的移动代理在真实世界场景中面临的长周期任务执行、错误恢复和冷启动问题。MobileUse通过引入分层反射架构来提高任务弹性及错误恢复能力，并利用主动探索模块来解决冷启动问题。实验结果表明，MobileUse在AndroidWorld和AndroidLab基准测试中取得了最先进的性能，并提供了开源工具包以促进实际应用。", "keywords": "移动代理, GUI代理, 分层反射, 错误恢复, 冷启动", "comments": "MobileUse的创新之处在于其独特的分层反射架构和主动探索模块，这使其能够有效应对移动代理在复杂真实世界环境中面临的核心挑战，如长周期任务的鲁棒性和在陌生环境中的适应性。其在基准测试中取得的SOTA性能，以及发布的开箱即用工具包，都显著提升了该研究的实用价值和潜在影响力。"}}
{"id": "2507.17585", "title": "From Scan to Action: Leveraging Realistic Scans for Embodied Scene Understanding", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Sombit Dey", "Luc Van Gool", "Danda Pani Paudel"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is not included in the official CVPR proceedings", "url": "http://arxiv.org/abs/2507.17585v1", "summary": "Real-world 3D scene-level scans offer realism and can enable better\nreal-world generalizability for downstream applications. However, challenges\nsuch as data volume, diverse annotation formats, and tool compatibility limit\ntheir use. This paper demonstrates a methodology to effectively leverage these\nscans and their annotations. We propose a unified annotation integration using\nUSD, with application-specific USD flavors. We identify challenges in utilizing\nholistic real-world scan datasets and present mitigation strategies. The\nefficacy of our approach is demonstrated through two downstream applications:\nLLM-based scene editing, enabling effective LLM understanding and adaptation of\nthe data (80% success), and robotic simulation, achieving an 87% success rate\nin policy learning.", "comment": "Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is\n  not included in the official CVPR proceedings", "pdf_url": "http://arxiv.org/pdf/2507.17585v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从扫描到行动：利用真实扫描进行具身场景理解", "tldr": "本研究提出了一种有效利用真实世界3D场景扫描及其标注的方法，通过统一的USD标注集成来克服数据量、标注格式和工具兼容性挑战，并在LLM场景编辑和机器人模拟中取得了显著成效。", "motivation": "真实世界3D场景扫描虽然提供了高真实感并能增强下游应用的泛化能力，但其使用受限于数据量大、标注格式多样和工具兼容性差等挑战。", "method": "本研究提出了一种统一的标注集成方法，利用USD（Universal Scene Description），并为特定应用定制USD风味。同时，识别并提出了利用整体真实世界扫描数据集的缓解策略。", "result": "本方法在两个下游应用中展示了其有效性：在基于LLM的场景编辑中，实现了80%的成功率，表明LLM对数据的有效理解和适应；在机器人模拟中，策略学习达到了87%的成功率。", "conclusion": "本研究通过提出一种统一的USD标注集成方法，成功地克服了真实世界3D场景扫描数据利用中的挑战，并显著提升了LLM场景理解和机器人策略学习的效率和成功率。", "translation": "真实世界的3D场景级扫描提供了真实感，并能为下游应用带来更好的现实世界泛化能力。然而，数据量、多样化的标注格式和工具兼容性等挑战限制了它们的使用。本文展示了一种有效利用这些扫描及其标注的方法。我们提出了一种使用USD（Universal Scene Description）的统一标注集成，并为特定应用定制USD风味。我们识别了利用整体真实世界扫描数据集的挑战，并提出了缓解策略。我们的方法通过两个下游应用证明了其有效性：基于LLM的场景编辑，使LLM能够有效理解和适应数据（80%成功率），以及机器人模拟，在策略学习中实现了87%的成功率。", "summary": "本文提出了一种有效利用真实世界3D场景扫描及其标注的创新方法，旨在克服现有数据量大、标注格式多样和工具兼容性差等挑战。核心贡献在于引入了基于USD的统一标注集成方案，并为特定应用定制了USD风味。该方法通过在LLM场景编辑（80%成功率）和机器人模拟（87%策略学习成功率）中的应用，验证了其在具身场景理解方面的有效性。", "keywords": "3D场景扫描, USD, 具身场景理解, LLM场景编辑, 机器人模拟", "comments": "这项研究提出了一种实用的方法来解决真实世界3D扫描数据利用中的关键挑战，即数据集成和兼容性。其创新点在于采用USD作为统一的标注框架，并针对具体应用进行优化，这对于推动具身AI和机器人技术的发展具有重要意义。通过在LLM和机器人模拟中的验证，展示了其在实际应用中的潜力。"}}
{"id": "2507.17215", "title": "Triadic First-Order Logic Queries in Temporal Networks", "authors": ["Omkar Bhalerao", "Yunjie Pan", "C. Seshadhri", "Nishil Talati"], "categories": ["cs.DB", "cs.DS", "cs.IR", "cs.SI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17215v1", "summary": "Motif counting is a fundamental problem in network analysis, and there is a\nrich literature of theoretical and applied algorithms for this problem. Given a\nlarge input network $G$, a motif $H$ is a small \"pattern\" graph indicative of\nspecial local structure. Motif/pattern mining involves finding all matches of\nthis pattern in the input $G$. The simplest, yet challenging, case of motif\ncounting is when $H$ has three vertices, often called a \"triadic\" query. Recent\nwork has focused on \"temporal graph mining\", where the network $G$ has edges\nwith timestamps (and directions) and $H$ has time constraints.\n  Inspired by concepts in logic and database theory, we introduce the study of\n\"thresholded First Order Logic (FOL) Motif Analysis\" for massive temporal\nnetworks. A typical triadic motif query asks for the existence of three\nvertices that form a desired temporal pattern. An \"FOL\" motif query is obtained\nby having both existential and thresholded universal quantifiers. This allows\nfor query semantics that can mine richer information from networks. A typical\ntriadic query would be \"find all triples of vertices $u,v,w$ such that they\nform a triangle within one hour\". A thresholded FOL query can express \"find all\npairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$\nalso formed an edge within an hour\".\n  We design the first algorithm, FOLTY, for mining thresholded FOL triadic\nqueries. The theoretical running time of FOLTY matches the best known running\ntime for temporal triangle counting in sparse graphs. We give an efficient\nimplementation of FOLTY using specialized temporal data structures. FOLTY has\nexcellent empirical behavior, and can answer triadic FOL queries on graphs with\nnearly 70M edges is less than hour on commodity hardware. Our work has the\npotential to start a new research direction in the classic well-studied problem\nof motif analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17215v1", "cate": "cs.DB", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "时间网络中的三元一阶逻辑查询", "tldr": "本文介绍了针对大规模时间网络中三元一阶逻辑（FOL）查询的分析，并设计了首个算法FOLTY，该算法在理论和实践中均表现出色，能够高效处理大规模图上的查询。", "motivation": "图中的模式计数是一个基础问题，现有研究主要集中在时间图挖掘。受逻辑和数据库理论启发，本文旨在引入并研究大规模时间网络中的“阈值一阶逻辑（FOL）模式分析”，以挖掘更丰富的网络信息。", "method": "本文引入了“阈值一阶逻辑（FOL）模式分析”的概念，通过结合存在量词和阈值全称量词来表达比传统三元查询更丰富的语义。为此，作者设计了首个算法FOLTY，并使用专门的时间数据结构进行了高效实现。", "result": "FOLTY算法的理论运行时间与稀疏图中时间三角形计数的最佳已知运行时间相匹配。在实际应用中，FOLTY表现出卓越的性能，能够在商品硬件上在一小时内处理包含近7000万条边的图上的三元FOL查询。", "conclusion": "本文的工作有望在经典且经过充分研究的模式分析问题中开辟新的研究方向。", "translation": "模式计数是网络分析中的一个基本问题，关于该问题存在着丰富的理论和应用算法文献。给定一个大型输入网络G，模式H是一个指示特殊局部结构的小型“模式”图。模式/模式挖掘涉及在输入G中查找该模式的所有匹配项。最简单但也最具挑战性的模式计数情况是当H具有三个顶点时，通常称为“三元”查询。最近的工作集中在“时间图挖掘”上，其中网络G的边带有时间戳（和方向），并且H具有时间约束。\n受逻辑和数据库理论概念的启发，我们引入了对大规模时间网络中“阈值一阶逻辑（FOL）模式分析”的研究。典型的三元模式查询要求存在形成所需时间模式的三个顶点。“FOL”模式查询是通过同时具有存在量词和阈值全称量词获得的。这允许查询语义能够从网络中挖掘更丰富的信息。典型的三元查询是“找到所有顶点u、v、w的三元组，使得它们在一小时内形成一个三角形”。阈值FOL查询可以表达“找到所有对u、v，使得对于一半的w，其中（u，w）形成一条边，（v，w）也在一小时内形成一条边”。\n我们设计了第一个用于挖掘阈值FOL三元查询的算法FOLTY。FOLTY的理论运行时间与稀疏图中时间三角形计数的最佳已知运行时间相匹配。我们使用专门的时间数据结构高效实现了FOLTY。FOLTY具有出色的经验行为，并且能够在商品硬件上在一小时内回答具有近7000万条边的图上的三元FOL查询。我们的工作有可能在经典且经过充分研究的模式分析问题中开启一个新的研究方向。", "summary": "本文针对大规模时间网络中的模式分析问题，引入了结合存在量词和阈值全称量词的“阈值一阶逻辑（FOL）三元查询”概念，旨在挖掘比传统方法更丰富的网络信息。作者设计并实现了首个处理此类查询的算法FOLTY。实验结果表明，FOLTY在理论运行时间上与现有最佳算法相当，并在实际应用中展现出高效性能，能在大规模图上快速执行复杂查询，为模式分析领域开辟了新的研究方向。", "keywords": "时间网络, 一阶逻辑, 模式分析, 三元查询, FOLTY", "comments": "本文的创新点在于将一阶逻辑和阈值概念引入到时间网络的三元模式分析中，极大地丰富了查询的表达能力。设计的FOLTY算法是该领域的首个算法，并在理论和实践上都取得了显著的效率提升，能够处理接近7000万条边的大规模图，这对于实际应用具有重要意义。该工作为时间图挖掘和模式分析提供了新的视角和强大的工具，有望推动相关研究的深入发展。"}}
{"id": "2507.17668", "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?", "authors": ["Alexander David Goldie", "Zilin Wang", "Jakob Nicolaus Foerster", "Shimon Whiteson"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted paper at Reinforcement Learning Conference (RLC) 2025", "url": "http://arxiv.org/abs/2507.17668v1", "summary": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.", "comment": "Accepted paper at Reinforcement Learning Conference (RLC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.17668v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "我们应该如何元学习强化学习算法？", "tldr": "该论文实证比较了不同的强化学习元学习方法，考虑了性能、可解释性、样本成本和训练时间，并提出了指导方针。", "motivation": "元学习在强化学习（RL）中显示出巨大潜力，但现有RL算法通常次优（从监督或无监督学习改编）。此外，目前严重缺乏对不同RL元学习算法的比较。", "method": "本文对应用于针对RL管道不同部分的元学习算法的不同方法进行了实证比较。除了元训练和元测试性能外，还调查了每个元学习算法的可解释性、样本成本和训练时间等因素。", "result": "基于实证比较的发现，本文提出了几项关于元学习新RL算法的指导方针。", "conclusion": "本文提出了几项关于元学习新RL算法的指导方针，以帮助确保未来的学习算法尽可能地高性能。", "translation": "从数据中元学习算法而不是依赖手动设计的过程，作为提高机器学习系统性能的范式，正日益受到欢迎。元学习在强化学习（RL）中表现出特别的潜力，因为RL算法通常是从监督或无监督学习中改编而来，尽管它们对RL来说是次优的。然而，到目前为止，不同的元学习算法之间（例如使用进化来优化黑盒函数或LLM来提出代码）缺乏严格的比较。在本文中，我们对应用于针对RL管道不同部分的元学习算法的不同方法进行了实证比较。除了元训练和元测试性能外，我们还调查了每个元学习算法的可解释性、样本成本和训练时间等因素。基于这些发现，我们提出了几项关于元学习新RL算法的指导方针，这将有助于确保未来的学习算法尽可能地高性能。", "summary": "本文针对强化学习（RL）中不同元学习方法之间缺乏实证比较的问题。它进行了一项全面的研究，评估了针对不同RL管道组件的各种元学习方法，并考量了元训练/测试性能、可解释性、样本成本和训练时间。基于其发现，本文提出了开发高性能元学习RL算法的指导方针。", "keywords": "元学习, 强化学习, 实证比较, 算法设计, 指导方针", "comments": "该论文通过提供急需的实证比较，解决了RL元学习领域的一个关键空白。其对可解释性和训练时间等实用方面的关注，以及对性能的考量，使得其提出的指导方针对于该领域未来的研究和发展具有高度价值。"}}
{"id": "2403.14459", "title": "Multi-Level Explanations for Generative Language Models", "authors": ["Lucas Monteiro Paes", "Dennis Wei", "Hyo Jin Do", "Hendrik Strobelt", "Ronny Luss", "Amit Dhurandhar", "Manish Nagireddy", "Karthikeyan Natesan Ramamurthy", "Prasanna Sattigeri", "Werner Geyer", "Soumya Ghosh"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted as an oral presentation at ACL 2025. Code available at this https URL", "url": "http://arxiv.org/abs/2403.14459v2", "summary": "Despite the increasing use of large language models (LLMs) for\ncontext-grounded tasks like summarization and question-answering, understanding\nwhat makes an LLM produce a certain response is challenging. We propose\nMulti-Level Explanations for Generative Language Models (MExGen), a technique\nto provide explanations for context-grounded text generation. MExGen assigns\nscores to parts of the context to quantify their influence on the model's\noutput. It extends attribution methods like LIME and SHAP to LLMs used in\ncontext-grounded tasks where (1) inference cost is high, (2) input text is\nlong, and (3) the output is text. We conduct a systematic evaluation, both\nautomated and human, of perturbation-based attribution methods for\nsummarization and question answering. The results show that our framework can\nprovide more faithful explanations of generated output than available\nalternatives, including LLM self-explanations. We open-source code for MExGen\nas part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.", "comment": "Accepted as an oral presentation at ACL 2025. Code available at\n  https://github.com/IBM/ICX360", "pdf_url": "http://arxiv.org/pdf/2403.14459v2", "cate": "cs.CL", "date": "2024-03-21", "updated": "2025-07-23", "AI": {"title_translation": "生成式语言模型的多级解释", "tldr": "提出MExGen，一种为生成式语言模型提供上下文解释的技术，它在忠实性方面优于现有方法。", "motivation": "尽管大型语言模型（LLMs）在诸如摘要和问答等上下文相关任务中的使用日益增加，但理解LLM为何产生特定响应仍然具有挑战性。", "method": "本文提出MExGen，一种为上下文相关的文本生成提供解释的技术。MExGen通过对上下文的不同部分进行评分来量化它们对模型输出的影响。它将LIME和SHAP等归因方法扩展到用于上下文相关任务的LLM，这些任务的特点是推理成本高、输入文本长以及输出为文本。", "result": "系统评估（包括自动化和人工评估）表明，MExGen框架可以比现有替代方法（包括LLM自解释）提供更忠实的生成输出解释。", "conclusion": "MExGen能够为生成式语言模型提供更忠实、有效的多级解释，有助于理解LLM在上下文相关任务中的行为。", "translation": "尽管大型语言模型（LLM）在诸如摘要和问答等上下文相关任务中的使用日益增加，但理解LLM为何产生特定响应仍然具有挑战性。我们提出了生成式语言模型的多级解释（MExGen），这是一种为上下文相关文本生成提供解释的技术。MExGen对上下文部分进行评分，以量化它们对模型输出的影响。它将LIME和SHAP等归因方法扩展到用于上下文相关任务的LLM，这些任务的特点是（1）推理成本高，（2）输入文本长，以及（3）输出是文本。我们对用于摘要和问答的基于扰动的归因方法进行了系统评估，包括自动化和人工评估。结果表明，我们的框架可以比现有替代方案（包括LLM自解释）提供更忠实的生成输出解释。我们开源了MExGen的代码，作为ICX360工具包的一部分：https://github.com/IBM/ICX360。", "summary": "本文提出MExGen，一种为上下文相关生成式语言模型提供多级解释的技术。MExGen通过量化上下文部分对模型输出的影响来扩展LIME和SHAP等归因方法，以适应高推理成本、长输入和文本输出的LLM任务。系统评估表明，MExGen比现有方法（包括LLM自解释）能提供更忠实的解释。", "keywords": "生成式语言模型, 解释性AI, 上下文相关任务, 归因方法, MExGen", "comments": "MExGen通过将现有归因方法（如LIME和SHAP）扩展到大型语言模型，特别关注了LLM在上下文相关任务中面临的挑战（高推理成本、长输入、文本输出），具有创新性。其提供更忠实解释的能力对于提高LLM的透明度和可信度至关重要。"}}
{"id": "2507.15917", "title": "HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs", "authors": ["Adrian Kaiser", "Claudiu Leoveanu-Condrei", "Ryan Gold", "Marius-Constantin Dinu", "Markus Hofmarcher"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures", "url": "http://arxiv.org/abs/2507.15917v2", "summary": "The synergy between symbolic knowledge, often represented by Knowledge Graphs\n(KGs), and the generative capabilities of neural networks is central to\nadvancing neurosymbolic AI. A primary bottleneck in realizing this potential is\nthe difficulty of automating KG construction, which faces challenges related to\noutput reliability, consistency, and verifiability. These issues can manifest\nas structural inconsistencies within the generated graphs, such as the\nformation of disconnected $\\textit{isolated islands}$ of data or the inaccurate\nconflation of abstract classes with specific instances. To address these\nchallenges, we propose HyDRA, a $\\textbf{Hy}$brid-$\\textbf{D}$riven\n$\\textbf{R}$easoning $\\textbf{A}$rchitecture designed for verifiable KG\nautomation. Given a domain or an initial set of documents, HyDRA first\nconstructs an ontology via a panel of collaborative neurosymbolic agents. These\nagents collaboratively agree on a set of competency questions (CQs) that define\nthe scope and requirements the ontology must be able to answer. Given these\nCQs, we build an ontology graph that subsequently guides the automated\nextraction of triplets for KG generation from arbitrary documents. Inspired by\ndesign-by-contracts (DbC) principles, our method leverages verifiable contracts\nas the primary control mechanism to steer the generative process of Large\nLanguage Models (LLMs). To verify the output of our approach, we extend beyond\nstandard benchmarks and propose an evaluation framework that assesses the\nfunctional correctness of the resulting KG by leveraging symbolic verifications\nas described by the neurosymbolic AI framework, $\\textit{SymbolicAI}$. This\nwork contributes a hybrid-driven architecture for improving the reliability of\nautomated KG construction and the exploration of evaluation methods for\nmeasuring the functional integrity of its output. The code is publicly\navailable.", "comment": "8 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.15917v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "HyDRA：一种可验证知识图谱的混合驱动推理架构", "tldr": "HyDRA是一种新的混合驱动推理架构，它通过神经符号智能体和可验证契约来自动化可靠的知识图谱构建，并使用新颖的评估框架验证其输出。", "motivation": "自动化知识图谱（KG）构建面临可靠性、一致性和可验证性方面的挑战，例如在生成的图中可能出现断开的“孤立数据岛”或抽象类与特定实例的不准确混淆。", "method": "本文提出了HyDRA（混合驱动推理架构），旨在实现可验证的KG自动化。HyDRA首先通过协同神经符号智能体构建本体，这些智能体共同商定能力问题（CQs）以定义本体范围。基于CQs，构建本体图以指导从文档中自动提取三元组。该方法受契约式设计（DbC）原则启发，利用可验证契约作为主要控制机制来引导大型语言模型（LLMs）的生成过程。", "result": "HyDRA提出了一个混合驱动架构，显著提高了自动化知识图谱构建的可靠性。同时，该研究还探索了衡量其输出功能完整性的评估方法，通过提出一个超越标准基准的评估框架，利用符号验证来评估所得KG的功能正确性。", "conclusion": "HyDRA提供了一种混合驱动架构，显著提高了自动化知识图谱构建的可靠性，并引入了评估其输出功能完整性的新方法。", "translation": "符号知识（常以知识图谱（KGs）表示）与神经网络生成能力之间的协同作用是推动神经符号AI发展的核心。实现这一潜力的主要瓶颈在于自动化知识图谱构建的难度，它面临着输出可靠性、一致性和可验证性方面的挑战。这些问题可能表现为生成图中的结构不一致，例如形成断开的“孤立数据岛”或抽象类与特定实例的不准确混淆。为了解决这些挑战，我们提出了HyDRA，一种旨在实现可验证知识图谱自动化的混合驱动推理架构。给定一个领域或一组初始文档，HyDRA首先通过一个协同神经符号智能体小组构建本体。这些智能体共同商定一组能力问题（CQs），这些问题定义了本体必须能够回答的范围和要求。基于这些CQs，我们构建一个本体图，该图随后指导从任意文档中自动提取三元组以生成KG。受契约式设计（DbC）原则的启发，我们的方法利用可验证契约作为主要的控制机制来引导大型语言模型（LLMs）的生成过程。为了验证我们方法的输出，我们超越了标准基准，并提出了一个评估框架，通过利用神经符号AI框架SymbolicAI所描述的符号验证来评估所得KG的功能正确性。这项工作贡献了一种混合驱动架构，用于提高自动化知识图谱构建的可靠性，并探索了衡量其输出功能完整性的评估方法。代码已公开可用。", "summary": "本文提出了HyDRA，一个混合驱动推理架构，旨在解决自动化知识图谱（KG）构建中面临的可靠性、一致性和可验证性挑战。HyDRA通过协同神经符号智能体构建本体并利用能力问题（CQs）指导三元组提取，并结合契约式设计原则利用可验证契约控制大型语言模型（LLMs）的生成过程。此外，该工作还提出了一个新颖的评估框架，通过符号验证来衡量所生成KG的功能正确性。HyDRA显著提升了自动化KG构建的可靠性，并为评估KG的功能完整性提供了新方法。", "keywords": "知识图谱自动化, 神经符号AI, 大型语言模型, 可验证性, 本体构建", "comments": "本文提出HyDRA架构，创新性地结合了神经符号AI、契约式设计原则和LLMs，以解决自动化知识图谱构建中的核心挑战，特别是其可靠性和可验证性。通过引入协同智能体构建本体和利用可验证契约指导生成过程，HyDRA提供了一个实用的解决方案。此外，其提出的超越标准基准的评估框架，利用符号验证来衡量KG功能正确性，也具有重要意义，有助于推动可信赖的神经符号AI发展。"}}
{"id": "2507.16839", "title": "Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for Vehicle System Development", "authors": ["Gregory Beale", "Gibran Ali"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025)", "url": "http://arxiv.org/abs/2507.16839v1", "summary": "This paper presents a methodology to process large-scale naturalistic driving\nstudies (NDS) to describe the driving behavior for five vehicle metrics,\nincluding speed, speeding, lane keeping, following distance, and headway,\ncontextualized by roadway characteristics, vehicle classes, and driver\ndemographics. Such descriptions of normative driving behaviors can aid in the\ndevelopment of vehicle safety and intelligent transportation systems. The\nmethodology is demonstrated using data from the Second Strategic Highway\nResearch Program (SHRP 2) NDS, which includes over 34 million miles of driving\nacross more than 3,400 drivers. Summaries of each driving metric were generated\nusing vehicle, GPS, and forward radar data. Additionally, interactive online\nanalytics tools were developed to visualize and compare driving behavior across\ngroups through dynamic data selection and grouping. For example, among drivers\non 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit\nby 7.5 to 15 mph slightly more often than their male counterparts, and younger\ndrivers maintained headways under 1.5 seconds more frequently than older\ndrivers. This work supports better vehicle systems and safer infrastructure by\nquantifying normative driving behaviors and offers a methodology for analyzing\nNDS datasets for cross group comparisons.", "comment": "Accepted to the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.16839v1", "cate": "cs.RO", "date": "2025-07-18", "updated": "2025-07-18", "AI": {"title_translation": "从大规模自然驾驶研究数据集中总结规范驾驶行为以用于车辆系统开发", "tldr": "本文提出了一种处理大规模自然驾驶研究数据的方法，以描述规范驾驶行为，并开发了交互式工具进行可视化和比较，旨在支持车辆系统和交通基础设施的开发。", "motivation": "描述规范驾驶行为可以帮助开发车辆安全和智能交通系统。", "method": "提出了一种处理大规模自然驾驶研究（NDS）数据的方法，用于描述五种车辆指标（速度、超速、车道保持、跟车距离、车头时距）的驾驶行为，并结合道路特征、车辆类型和驾驶员人口统计信息进行情境化分析。该方法使用第二期战略公路研究计划（SHRP 2）NDS数据（超过3400名驾驶员的3400万英里驾驶数据）进行演示。通过车辆、GPS和前向雷达数据生成各项驾驶指标的摘要，并开发了交互式在线分析工具以可视化和比较不同群体间的驾驶行为。", "result": "生成了各项驾驶指标的摘要。例如，在SHRP 2 NDS的65英里/小时道路上，16-19岁的女性驾驶员超速的频率略高于同年龄段的男性驾驶员；年轻驾驶员保持车头时距小于1.5秒的频率高于老年驾驶员。", "conclusion": "这项工作通过量化规范驾驶行为来支持更好的车辆系统和更安全的交通基础设施，并提供了一种分析NDS数据集进行跨群体比较的方法。", "translation": "本文提出了一种处理大规模自然驾驶研究（NDS）的方法，旨在描述五种车辆指标的驾驶行为，包括速度、超速、车道保持、跟车距离和车头时距，并结合道路特征、车辆类别和驾驶员人口统计信息进行情境化分析。对规范驾驶行为的此类描述有助于车辆安全和智能交通系统的开发。该方法利用第二期战略公路研究计划（SHRP 2）NDS的数据进行了演示，该数据包含超过3400名驾驶员的3400万英里驾驶里程。各项驾驶指标的摘要是利用车辆、GPS和前向雷达数据生成的。此外，还开发了交互式在线分析工具，通过动态数据选择和分组来可视化和比较不同群体间的驾驶行为。例如，在SHRP 2 NDS中，在65英里/小时的道路上，16-19岁的女性驾驶员超速7.5至15英里/小时的频率略高于同年龄段的男性驾驶员，而年轻驾驶员保持车头时距小于1.5秒的频率高于老年驾驶员。这项工作通过量化规范驾驶行为来支持更好的车辆系统和更安全的交通基础设施，并提供了一种分析NDS数据集进行跨群体比较的方法。", "summary": "本文提出了一种新颖的方法，用于分析大规模自然驾驶研究（NDS）数据集，以量化和描述规范驾驶行为。该方法关注速度、超速、车道保持、跟车距离和车头时距等五项关键驾驶指标，并结合道路类型、车辆类别和驾驶员人口统计学信息进行情境化分析。研究利用SHRP 2 NDS的庞大数据集（3400万英里，3400多名驾驶员）进行了演示，并开发了交互式在线分析工具以支持跨群体的行为可视化和比较。具体发现包括年轻女性驾驶员在特定条件下超速频率略高于男性，以及年轻驾驶员更频繁地保持较短的车头时距。这项工作为车辆系统开发和交通基础设施改进提供了有价值的洞察和分析工具。", "keywords": "自然驾驶研究, 驾驶行为, 数据分析, 车辆系统, 智能交通系统", "comments": "本文的创新之处在于其提出的系统性方法论，能够从大规模复杂数据集中提取并情境化描述规范驾驶行为。通过开发交互式分析工具，极大地提升了数据分析的灵活性和可视化效果，使得跨群体比较成为可能。这对于车辆安全系统和智能交通系统的开发具有重要意义，因为它提供了基于真实世界驾驶行为的量化依据。"}}
{"id": "2504.09751", "title": "Accelerating Ray Tracing-Based Wireless Channels Generation for Real-Time Network Digital Twins", "authors": ["Cláudio Modesto", "Lucas Mozart", "Pedro Batista", "André Cavalcante", "Aldebaro Klautau"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been published at IEEE Open Journal of the Communications Society (DOI: this https URL )", "url": "http://arxiv.org/abs/2504.09751v2", "summary": "Ray tracing (RT) simulation is a widely used approach to enable modeling\nwireless channels in applications such as network digital twins. However, the\ncomputational cost to execute ray tracing (RT) is proportional to factors such\nas the level of detail used in the adopted 3D scenario. This work proposes RT\npre-processing algorithms that aim at simplifying the 3D scene without\ndistorting the channel, by reducing the scenario area and/or simplifying object\nshapes in the scenario. It also proposes a post-processing method that augments\na set of RT results to achieve an improved time resolution. These methods\nenable using RT in applications that use a detailed and photorealistic 3D\nscenario while generating consistent wireless channels over time. Our\nsimulation results with different urban scenarios scales, in terms of area and\nobject details, demonstrate that it is possible to reduce the simulation time\nby more than 50% without compromising the accuracy of the multipath RT\nparameters, such as angles of arrival and departure, delay, phase, and path\ngain.", "comment": "This paper has been published at IEEE Open Journal of the\n  Communications Society (DOI: https://doi.org/10.1109/OJCOMS.2025.3583202)", "pdf_url": "http://arxiv.org/pdf/2504.09751v2", "cate": "cs.NI", "date": "2025-04-13", "updated": "2025-07-23", "AI": {"title_translation": "加速基于光线追踪的无线信道生成用于实时网络数字孪生", "tldr": "本文提出了一种光线追踪（RT）预处理和后处理方法，以降低RT模拟的计算成本，同时保持无线信道建模的准确性，从而实现实时网络数字孪生应用。", "motivation": "光线追踪（RT）模拟在网络数字孪生等应用中广泛用于建模无线信道，但其计算成本高昂，与3D场景的细节水平成正比。", "method": "本文提出RT预处理算法，通过减少场景区域和/或简化场景中的物体形状来简化3D场景而不扭曲信道。同时，还提出了一种后处理方法，用于增强RT结果集以提高时间分辨率。", "result": "模拟结果表明，在不损害多径RT参数（如到达角和离开角、延迟、相位和路径增益）准确性的前提下，可以将模拟时间减少50%以上。", "conclusion": "这些方法使得在需要详细和真实感3D场景的应用中能够使用RT，并生成随时间推移一致的无线信道。", "translation": "光线追踪（RT）模拟是一种广泛使用的方法，用于在网络数字孪生等应用中对无线信道进行建模。然而，执行光线追踪（RT）的计算成本与所采用的3D场景中使用的细节水平等因素成正比。这项工作提出了RT预处理算法，旨在通过减少场景区域和/或简化场景中的物体形状来简化3D场景而不扭曲信道。它还提出了一种后处理方法，用于增强一组RT结果以实现改进的时间分辨率。这些方法使得在需要详细和真实感3D场景的应用中能够使用RT，同时生成随时间推移一致的无线信道。我们在不同城市场景规模（在区域和物体细节方面）的模拟结果表明，在不损害多径RT参数（如到达角和离开角、延迟、相位和路径增益）准确性的前提下，可以将模拟时间减少50%以上。", "summary": "本文针对光线追踪（RT）模拟在无线信道建模中计算成本高的问题，提出了一系列RT预处理和后处理算法。预处理算法通过简化3D场景（减少区域或简化形状）来降低复杂性，而后处理方法则用于提升时间分辨率。实验结果表明，这些方法可以在不牺牲信道参数准确性的情况下，将模拟时间缩短一半以上，从而支持RT在实时网络数字孪生中应用详细和高逼真度的3D场景。", "keywords": "光线追踪, 无线信道, 网络数字孪生, 预处理, 后处理", "comments": "这项工作在加速光线追踪模拟方面具有重要意义，尤其是在实时网络数字孪生等需要高精度和低延迟的应用中。通过引入预处理和后处理步骤，该方法有效地解决了计算开销大的核心挑战，同时保持了信道建模的准确性。其创新点在于对3D场景的智能简化和对时间分辨率的增强，使得RT在实际应用中更具可行性。"}}
{"id": "2507.17099", "title": "Weather-Aware AI Systems versus Route-Optimization AI: A Comprehensive Analysis of AI Applications in Transportation Productivity", "authors": ["Tatsuru Kikuchi"], "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      41 pages, 5 figures", "url": "http://arxiv.org/abs/2507.17099v1", "summary": "While recent research demonstrates that AI route-optimization systems improve\ntaxi driver productivity by 14\\%, this study reveals that such findings capture\nonly a fraction of AI's potential in transportation. We examine comprehensive\nweather-aware AI systems that integrate deep learning meteorological prediction\nwith machine learning positioning optimization, comparing their performance\nagainst traditional operations and route-only AI approaches. Using simulation\ndata from 10,000 taxi operations across varied weather conditions, we find that\nweather-aware AI systems increase driver revenue by 107.3\\%, compared to 14\\%\nimprovements from route-optimization alone. Weather prediction contributes the\nlargest individual productivity gain, with strong correlations between\nmeteorological conditions and demand ($r=0.575$). Economic analysis reveals\nannual earnings increases of 13.8 million yen per driver, with rapid payback\nperiods and superior return on investment. These findings suggest that current\nAI literature significantly underestimates AI's transformative potential by\nfocusing narrowly on routing algorithms, while weather intelligence represents\nan untapped \\$8.9 billion market opportunity. Our results indicate that future\nAI implementations should adopt comprehensive approaches that address multiple\noperational challenges simultaneously rather than optimizing isolated\nfunctions.", "comment": "41 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.17099v1", "cate": "econ.GN", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "天气感知型AI系统与路径优化AI：交通生产力中AI应用的综合分析", "tldr": "本研究发现，整合天气预测的AI系统能显著提高出租车司机收入（107.3%），远超单纯路径优化AI（14%），揭示了天气智能在交通领域巨大的未开发潜力。", "motivation": "现有研究表明AI路径优化系统仅能提升出租车司机生产力14%，本研究旨在揭示AI在交通领域更全面的潜力，特别是通过整合天气感知能力。", "method": "本研究通过模拟10,000次出租车运营数据，比较了整合深度学习气象预测和机器学习定位优化的天气感知型AI系统与传统运营及纯路径优化AI方法的性能。", "result": "天气感知型AI系统使司机收入增加107.3%，而单独路径优化仅增加14%。其中，天气预测贡献了最大的生产力增益，气象条件与需求之间存在强相关性（r=0.575）。经济分析显示，每位司机每年收入可增加1380万日元，投资回报期短，回报率高。", "conclusion": "目前的AI文献通过狭隘地关注路径算法而显著低估了AI的变革潜力，而天气智能代表着一个未开发的89亿美元市场机遇。未来的AI实施应采取综合方法，同时解决多个运营挑战，而不是优化孤立的功能。", "translation": "尽管最近的研究表明AI路径优化系统可使出租车司机生产力提高14%，但本研究揭示了此类发现仅捕捉了AI在交通领域潜力的一小部分。我们研究了综合性的天气感知型AI系统，该系统将深度学习气象预测与机器学习定位优化相结合，并将其性能与传统运营和纯路径优化AI方法进行了比较。利用10,000次出租车在不同天气条件下的模拟运营数据，我们发现天气感知型AI系统使司机收入增加了107.3%，而单独路径优化仅提高了14%。天气预测贡献了最大的个体生产力增益，气象条件与需求之间存在强相关性（r=0.575）。经济分析显示，每位司机每年可增加1380万日元的收入，投资回报期短，投资回报率高。这些发现表明，目前的AI文献通过狭隘地关注路径算法而显著低估了AI的变革潜力，而天气智能代表着一个未开发的89亿美元市场机遇。我们的结果表明，未来的AI实施应采用综合方法，同时解决多个运营挑战，而不是优化孤立的功能。", "summary": "本研究通过模拟分析，对比了天气感知型AI系统与纯路径优化AI在交通生产力中的表现。结果显示，整合深度学习气象预测和机器学习定位优化的天气感知型AI能将出租车司机收入提升107.3%，远高于纯路径优化的14%。研究强调了天气预测在提升生产力中的关键作用，并指出当前AI研究低估了天气智能的巨大市场潜力，呼吁未来AI应用采取更全面的集成方法。", "keywords": "天气感知型AI, 路径优化, 交通生产力, 深度学习, 经济效益", "comments": "这篇论文的创新点在于提出了“天气感知型AI系统”的概念，并量化了其相对于传统路径优化AI的巨大优势。它不仅从技术层面整合了气象预测和定位优化，更从经济层面揭示了天气智能在交通领域的巨大未开发市场（89亿美元）。其重要性在于改变了人们对AI在交通领域应用潜力的认知，指出了未来AI发展应走向综合性而非孤立功能的优化。"}}
{"id": "2507.17406", "title": "Physics-based Human Pose Estimation from a Single Moving RGB Camera", "authors": ["Ayce Idil Aytekin", "Chuqiao Li", "Diogo Luvizon", "Rishabh Dabral", "Martin Oswald", "Marc Habermann", "Christian Theobalt"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17406v1", "summary": "Most monocular and physics-based human pose tracking methods, while achieving\nstate-of-the-art results, suffer from artifacts when the scene does not have a\nstrictly flat ground plane or when the camera is moving. Moreover, these\nmethods are often evaluated on in-the-wild real world videos without\nground-truth data or on synthetic datasets, which fail to model the real world\nlight transport, camera motion, and pose-induced appearance and geometry\nchanges. To tackle these two problems, we introduce MoviCam, the first\nnon-synthetic dataset containing ground-truth camera trajectories of a\ndynamically moving monocular RGB camera, scene geometry, and 3D human motion\nwith human-scene contact labels. Additionally, we propose PhysDynPose, a\nphysics-based method that incorporates scene geometry and physical constraints\nfor more accurate human motion tracking in case of camera motion and non-flat\nscenes. More precisely, we use a state-of-the-art kinematics estimator to\nobtain the human pose and a robust SLAM method to capture the dynamic camera\ntrajectory, enabling the recovery of the human pose in the world frame. We then\nrefine the kinematic pose estimate using our scene-aware physics optimizer.\nFrom our new benchmark, we found that even state-of-the-art methods struggle\nwith this inherently challenging setting, i.e. a moving camera and non-planar\nenvironments, while our method robustly estimates both human and camera poses\nin world coordinates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17406v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于物理的单移动RGB相机人体姿态估计", "tldr": "本文介绍了MoviCam数据集和PhysDynPose方法，用于解决移动相机和非平面场景下基于物理的单目人体姿态估计的挑战。MoviCam是首个包含真实世界地面真值数据的非合成数据集，而PhysDynPose则结合场景几何和物理约束，在复杂环境中实现鲁棒的人体和相机姿态估计。", "motivation": "大多数单目和基于物理的人体姿态追踪方法在场景不平坦或相机移动时会出现伪影。此外，这些方法通常在缺乏真实世界光传输、相机运动和姿态引起的外观与几何变化建模的合成数据集上进行评估，或在没有地面真值数据的真实世界视频上进行评估。", "method": "本文引入了MoviCam，这是第一个非合成数据集，包含动态移动的单目RGB相机的地面真值相机轨迹、场景几何和带有场景接触标签的3D人体运动。此外，本文提出了PhysDynPose，一种基于物理的方法，该方法结合了场景几何和物理约束，用于在相机移动和非平坦场景下进行更准确的人体运动追踪。具体来说，它使用最先进的运动学估计器获取人体姿态，并使用鲁棒的SLAM方法捕获动态相机轨迹，从而恢复世界坐标系中的人体姿态。然后使用场景感知物理优化器来细化运动学姿态估计。", "result": "在新基准测试中，即使是最先进的方法也难以应对移动相机和非平面环境这种固有的挑战性设置。而本文提出的方法能够鲁棒地估计世界坐标系中的人体和相机姿态。", "conclusion": "本文通过引入MoviCam数据集和PhysDynPose方法，有效解决了在移动相机和非平面场景下进行基于物理的人体姿态估计的挑战，并证明了其在复杂环境中的鲁棒性。", "translation": "大多数单目和基于物理的人体姿态追踪方法，尽管取得了最先进的结果，但在场景没有严格平坦的地面或相机移动时会产生伪影。此外，这些方法通常在没有地面真值数据的真实世界视频或合成数据集上进行评估，而这些数据集未能模拟真实世界的光传输、相机运动以及姿态引起的外观和几何变化。为了解决这两个问题，我们引入了MoviCam，这是第一个非合成数据集，包含动态移动的单目RGB相机的地面真值相机轨迹、场景几何以及带有场景接触标签的3D人体运动。此外，我们提出了PhysDynPose，一种基于物理的方法，该方法结合了场景几何和物理约束，用于在相机移动和非平坦场景下进行更准确的人体运动追踪。更具体地说，我们使用最先进的运动学估计器来获取人体姿态，并使用鲁棒的SLAM方法来捕获动态相机轨迹，从而能够在世界坐标系中恢复人体姿态。然后我们使用我们的场景感知物理优化器来细化运动学姿态估计。从我们的新基准测试中，我们发现即使是最先进的方法也难以应对这种固有的挑战性设置，即移动相机和非平面环境，而我们的方法能够鲁棒地估计世界坐标系中的人体和相机姿态。", "summary": "本文针对现有基于物理的单目人体姿态估计方法在相机移动和非平坦场景下的局限性，提出了MoviCam数据集和PhysDynPose方法。MoviCam是首个包含真实世界地面真值相机轨迹、场景几何和3D人体运动的非合成数据集。PhysDynPose是一种基于物理的方法，通过结合场景几何和物理约束，利用运动学估计器和SLAM技术，并进行物理优化，实现了在复杂动态环境下对人体和相机姿态的鲁棒估计。实验证明，该方法在具有挑战性的新基准上优于现有技术。", "keywords": "基于物理, 人体姿态估计, 移动相机, 非平面场景, 数据集", "comments": "本文的创新点在于同时解决了两个关键问题：缺乏真实世界的评估数据和现有方法在复杂动态环境下的性能不足。MoviCam数据集的发布填补了真实世界地面真值数据的空白，为未来的研究提供了宝贵的资源。PhysDynPose方法通过结合场景感知物理优化，有效地提升了在移动相机和非平面场景下的人体姿态估计鲁棒性，具有重要的实际应用价值。"}}
{"id": "2502.06269", "title": "UNGER: Generative Recommendation with A Unified Code via Semantic and Collaborative Integration", "authors": ["Longtao Xiao", "Haozhao Wang", "Cheng Wang", "Linfei Ji", "Yifan Wang", "Jieming Zhu", "Zhenhua Dong", "Rui Zhang", "Ruixuan Li"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.06269v2", "summary": "With the rise of generative paradigms, generative recommendation has garnered\nincreasing attention. The core component is the item code, generally derived by\nquantizing collaborative or semantic representations to serve as candidate\nitems identifiers in the context. However, existing methods typically construct\nseparate codes for each modality, leading to higher computational and storage\ncosts and hindering the integration of their complementary strengths.\nConsidering this limitation, we seek to integrate two different modalities into\na unified code, fully unleashing the potential of complementary nature among\nmodalities. Nevertheless, the integration remains challenging: the integrated\nembedding obtained by the common concatenation method would lead to\nunderutilization of collaborative knowledge, thereby resulting in limited\neffectiveness. To address this, we propose a novel method, named UNGER, which\nintegrates semantic and collaborative knowledge into a unified code for\ngenerative recommendation. Specifically, we propose to adaptively learn an\nintegrated embedding through the joint optimization of cross-modality knowledge\nalignment and next item prediction tasks. Subsequently, to mitigate the\ninformation loss caused by the quantization process, we introduce an\nintra-modality knowledge distillation task, using the integrated embeddings as\nsupervised signals to compensate. Extensive experiments on three widely used\nbenchmarks demonstrate the superiority of our approach compared to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.06269v2", "cate": "cs.IR", "date": "2025-02-10", "updated": "2025-07-23", "AI": {"title_translation": "UNGER：通过语义和协作集成统一代码的生成式推荐", "tldr": "提出UNGER，一种新颖的生成式推荐方法，通过语义和协作知识的统一代码解决了现有方法中模态集成和量化信息损失的问题。", "motivation": "现有生成式推荐方法通常为每种模态构建单独的代码，导致计算和存储成本高，且阻碍了互补优势的整合。直接拼接集成会导致协作知识利用不足，从而限制有效性。", "method": "提出UNGER方法，将语义和协作知识集成到统一代码中。通过跨模态知识对齐和下一项预测任务的联合优化来学习集成嵌入，并引入模态内知识蒸馏任务以减轻量化过程造成的信息损失。", "result": "在三个广泛使用的基准测试上进行的广泛实验表明，该方法优于现有方法。", "conclusion": "UNGER通过统一语义和协作知识并缓解量化信息损失，显著提升了生成式推荐的性能。", "translation": "随着生成范式的兴起，生成式推荐受到了越来越多的关注。其核心组件是物品代码，通常通过量化协作或语义表示来充当上下文中的候选物品标识符。然而，现有方法通常为每种模态构建单独的代码，导致更高的计算和存储成本，并阻碍了其互补优势的整合。考虑到这一限制，我们试图将两种不同的模态集成到一个统一的代码中，充分释放模态之间互补性的潜力。然而，这种集成仍然具有挑战性：通过常见的拼接方法获得的集成嵌入会导致协作知识的利用不足，从而导致有效性有限。为了解决这个问题，我们提出了一种名为UNGER的新颖方法，它将语义和协作知识集成到一个统一的代码中，用于生成式推荐。具体来说，我们提出通过跨模态知识对齐和下一项预测任务的联合优化来自适应地学习集成嵌入。随后，为了减轻量化过程造成的信息损失，我们引入了模态内知识蒸馏任务，使用集成嵌入作为监督信号进行补偿。在三个广泛使用的基准测试上进行的广泛实验表明，我们的方法优于现有方法。", "summary": "本文提出了一种名为UNGER的新型生成式推荐方法，旨在通过将语义和协作知识集成到统一的代码中来克服现有方法的局限性。现有方法常因模态分离而导致高成本和集成障碍。UNGER通过联合优化跨模态知识对齐和下一项预测任务来学习集成嵌入，并引入模态内知识蒸馏以缓解量化引起的信息损失。实验结果表明，UNGER在多个基准测试上表现优越。", "keywords": "生成式推荐, 统一代码, 语义集成, 协作集成, 知识蒸馏", "comments": "UNGER的创新点在于提出了一个统一的代码框架，有效地融合了语义和协作信息，并解决了量化过程中的信息损失问题，这对于提升生成式推荐的效率和效果具有重要意义。"}}
{"id": "2506.07986", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025; Project Page: this https URL", "url": "http://arxiv.org/abs/2506.07986v3", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "comment": "Accepted by ICCV 2025; Project Page: https://vchitect.github.io/TACA/", "pdf_url": "http://arxiv.org/pdf/2506.07986v3", "cate": "cs.CV", "date": "2025-06-09", "updated": "2025-07-23", "AI": {"title_translation": "重新思考多模态扩散Transformer中的跨模态交互", "tldr": "本文提出了TACA方法，通过温度调整和时间步依赖的权重，解决了多模态扩散Transformer中文本与图像对齐不精确的问题，显著提升了生成内容的语义忠实度。", "motivation": "当前的多模态扩散Transformer (MM-DiTs) 在文本驱动的视觉生成中表现出色，但即便如FLUX这样的先进模型，也难以实现文本提示与生成内容之间的精确对齐。主要问题在于：1) 视觉和文本模态之间的token不平衡导致跨模态注意力被抑制；2) 缺乏时间步感知的注意力权重。", "method": "本文提出了温度调整跨模态注意力 (TACA)，这是一种参数高效的方法，通过温度缩放和时间步依赖的调整，动态地重新平衡多模态交互。TACA与LoRA微调结合使用。", "result": "TACA显著增强了T2I-CompBench基准上的文本-图像对齐，且计算开销极小。在FLUX和SD3.5等先进模型上进行了测试，证明了其在改善图像-文本对齐方面的能力，包括物体外观、属性绑定和空间关系。", "conclusion": "研究结果强调了在文本到图像扩散模型中平衡跨模态注意力对于提高语义忠实度的重要性。", "translation": "多模态扩散Transformer (MM-DiTs) 在文本驱动的视觉生成方面取得了显著进展。然而，即使是FLUX等最先进的MM-DiT模型，在文本提示和生成内容之间实现精确对齐方面仍面临困难。我们识别出MM-DiT注意力机制中的两个关键问题，即1) 由于视觉和文本模态之间的token不平衡导致跨模态注意力受到抑制，以及2) 缺乏时间步感知的注意力加权，这些都阻碍了对齐。为了解决这些问题，我们提出了温度调整跨模态注意力 (TACA)，这是一种参数高效的方法，通过温度缩放和时间步依赖的调整动态地重新平衡多模态交互。当与LoRA微调结合时，TACA在T2I-CompBench基准上显著增强了文本-图像对齐，且计算开销极小。我们在FLUX和SD3.5等最先进的模型上测试了TACA，证明了其在物体外观、属性绑定和空间关系方面改善图像-文本对齐的能力。我们的研究结果强调了平衡跨模态注意力在提高文本到图像扩散模型语义忠实度方面的重要性。我们的代码已在GitHub公开。", "summary": "本文针对多模态扩散Transformer (MM-DiTs) 中文本与图像对齐不精确的问题，分析了跨模态注意力受抑制和缺乏时间步感知权重两大原因。为解决这些问题，作者提出了温度调整跨模态注意力 (TACA)，该方法通过动态平衡跨模态交互和结合LoRA微调，显著提升了T2I-CompBench基准上的文本-图像对齐效果，并在FLUX和SD3.5等模型上验证了其在改善物体外观、属性绑定和空间关系方面的有效性。", "keywords": "多模态扩散Transformer, 跨模态交互, 文本-图像对齐, TACA, 注意力机制", "comments": "本文创新性地指出了多模态扩散Transformer中跨模态交互的两个关键问题，并提出了参数高效的TACA方法予以解决。其通过温度缩放和时间步依赖调整动态平衡注意力，有效提升了文本-图像对齐的语义忠实度，对未来文本到图像生成模型的发展具有重要指导意义。方法的参数高效性也增强了其实用性。"}}
{"id": "2507.17516", "title": "Frequency Estimation of Correlated Multi-attribute Data under Local Differential Privacy", "authors": ["Shafizur Rahman Seeam", "Ye Zheng", "Yidan Hu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17516v1", "summary": "Large-scale data collection, from national censuses to IoT-enabled smart\nhomes, routinely gathers dozens of attributes per individual. These\nmulti-attribute datasets are vital for analytics but pose significant privacy\nrisks. Local Differential Privacy (LDP) is a powerful tool to protect user data\nprivacy by allowing users to locally perturb their records before releasing to\nan untrusted data aggregator. However, existing LDP mechanisms either split the\nprivacy budget across all attributes or treat each attribute independently,\nignoring natural inter-attribute correlations. This leads to excessive noise or\nfragmented budgets, resulting in significant utility loss, particularly in\nhigh-dimensional settings.\n  To overcome these limitations, we propose Correlated Randomized Response\n(Corr-RR), a novel LDP mechanism that leverages correlations among attributes\nto substantially improve utility while maintaining rigorous LDP guarantees.\nCorr-RR allocates the full privacy budget to perturb a single, randomly\nselected attribute and reconstructs the remaining attributes using estimated\ninterattribute dependencies, without incurring additional privacy cost. To\nenable this, Corr-RR operates in two phases: (1) a subset of users apply\nstandard LDP mechanisms to estimate correlations, and (2) each remaining user\nperturbs one attribute and infers the others using the learned correlations. We\ntheoretically prove that Corr-RR satisfies $\\epsilon$-LDP, and extensive\nexperiments on synthetic and real-world datasets demonstrate that Corr-RR\nconsistently outperforms state-of-the-art LDP mechanisms, particularly in\nscenarios with many attributes and strong inter-attribute correlations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17516v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "局部差分隐私下相关多属性数据的频率估计", "tldr": "提出Corr-RR，一种新的局部差分隐私机制，通过利用属性间相关性，显著提高了多属性数据频率估计的效用，同时保持严格的隐私保障。", "motivation": "大规模多属性数据收集存在隐私风险。现有局部差分隐私（LDP）机制在处理多属性数据时，要么分散隐私预算，要么独立处理属性，忽略了属性间的自然关联，导致在数据维度较高时效用损失严重。", "method": "提出Correlated Randomized Response (Corr-RR) 机制。该机制将全部隐私预算分配给一个随机选取的属性进行扰动，然后利用估计的属性间依赖关系重建其余属性，且不增加额外隐私成本。Corr-RR分为两个阶段：1) 一部分用户使用标准LDP机制估计属性间的相关性；2) 剩余用户扰动一个属性，并利用学习到的相关性推断其他属性。", "result": "理论证明Corr-RR满足ε-LDP。在合成和真实世界数据集上的大量实验表明，Corr-RR持续优于最先进的LDP机制，特别是在属性多且属性间相关性强的场景下。", "conclusion": "Corr-RR通过有效利用多属性数据中的相关性，显著提升了局部差分隐私下频率估计的效用，克服了现有LDP机制在处理高维相关数据时的局限性。", "translation": "大规模数据收集，从国家人口普查到物联网智能家居，通常会为每个人收集数十个属性。这些多属性数据集对数据分析至关重要，但也带来了显著的隐私风险。局部差分隐私（LDP）是一种强大的工具，通过允许用户在将记录发布给不可信数据聚合器之前对其进行局部扰动，从而保护用户数据隐私。然而，现有的LDP机制要么将隐私预算分配给所有属性，要么独立处理每个属性，忽略了属性间的自然关联。这导致过度噪声或预算碎片化，从而造成显著的效用损失，尤其是在高维设置中。\n为了克服这些局限性，我们提出了Correlated Randomized Response (Corr-RR)，一种新颖的LDP机制，它利用属性间的相关性，在保持严格LDP保证的同时，大幅提高了效用。Corr-RR将全部隐私预算分配给一个单一的、随机选择的属性进行扰动，并利用估计的属性间依赖关系重建其余属性，而无需承担额外的隐私成本。为了实现这一点，Corr-RR分两个阶段运行：(1) 一部分用户应用标准LDP机制来估计相关性，以及 (2) 剩余的每个用户扰动一个属性，并利用学习到的相关性推断其他属性。我们理论证明了Corr-RR满足ε-LDP，并且在合成和真实世界数据集上的大量实验表明，Corr-RR持续优于最先进的LDP机制，特别是在属性多且属性间相关性强的场景中。", "summary": "该论文提出了Correlated Randomized Response (Corr-RR)，一种新颖的局部差分隐私（LDP）机制，旨在解决现有LDP方法在处理高维、相关多属性数据时效用损失严重的问题。Corr-RR通过将全部隐私预算集中用于扰动单个随机选定的属性，并利用估计的属性间相关性来重建其他属性，从而显著提高了数据效用，同时保持了严格的隐私保障。该机制分两阶段运行：首先估计相关性，然后利用这些相关性进行属性推断。理论分析和实验结果均表明，Corr-RR在多属性且属性间相关性强的场景下，性能优于现有LDP机制。", "keywords": "局部差分隐私, 多属性数据, 频率估计, 相关性, 隐私保护", "comments": "该论文的创新点在于提出了一种有效利用多属性数据中固有相关性的LDP机制，解决了传统LDP方法在处理高维数据时效用下降的问题。通过将隐私预算集中分配给单个属性并利用相关性进行推断，Corr-RR在保持隐私的同时显著提升了数据效用，这对于实际应用中大规模多属性数据的隐私保护和分析具有重要意义。"}}
{"id": "2507.17129", "title": "Secure Wireless Communication via Polarforming", "authors": ["Jingze Ding", "Zijian Zhou", "Bingli Jiao", "Rui Zhang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17129v1", "summary": "Polarforming is a promising technique that enables dynamic adjustment of\nantenna polarization to mitigate depolarization effects commonly encountered\nduring electromagnetic (EM) wave propagation. In this letter, we investigate\nthe polarforming design for secure wireless communication systems, where the\nbase station (BS) is equipped with polarization-reconfigurable antennas (PRAs)\nand can flexibly adjust the antenna polarization to transmit confidential\ninformation to a legitimate user in the presence of an eavesdropper. To\nmaximize the achievable secrecy rate, we propose an efficient iterative\nalgorithm to jointly optimize transmit beamforming and polarforming, where\nbeamforming exploits spatial degrees of freedom (DoFs) to steer the transmit\nbeam toward the user, while polarforming leverages polarization DoFs to align\nthe polarization state of the EM wave received by the user with that of its\nantenna. Simulation results demonstrate that, compared to conventional\nfixed-polarization antenna (FPA) systems, polarforming can fully exploit the\nDoFs in antenna polarization optimization to significantly enhance the security\nperformance of wireless communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17129v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过极化成形实现安全无线通信", "tldr": "本文研究了极化成形技术在安全无线通信系统中的应用，通过联合优化波束成形和极化成形，显著提升了系统的安全性能。", "motivation": "在电磁波传播过程中，去极化效应普遍存在，影响无线通信的安全性。为了提高无线通信系统的安全性能，需要一种能够动态调整天线极化的技术来对抗窃听者。", "method": "提出了一种高效的迭代算法，用于联合优化发射波束成形和极化成形。波束成形利用空间自由度将发射波束导向合法用户，而极化成形利用极化自由度使合法用户接收到的电磁波极化状态与其天线对齐。基站配备极化可重构天线（PRAs）。", "result": "仿真结果表明，与传统的固定极化天线（FPA）系统相比，极化成形能充分利用天线极化优化中的自由度，显著增强无线通信系统的安全性能。", "conclusion": "极化成形技术能够有效提升无线通信系统的安全性能，通过联合优化波束成形和极化成形，可以最大化可实现的安全速率。", "translation": "极化成形是一种很有前途的技术，它能够动态调整天线极化，以减轻电磁（EM）波传播过程中常见的去极化效应。在这封信中，我们研究了用于安全无线通信系统的极化成形设计，其中基站（BS）配备了极化可重构天线（PRAs），并且可以灵活调整天线极化，以便在存在窃听者的情况下向合法用户传输机密信息。为了最大化可实现的安全速率，我们提出了一种高效的迭代算法，以联合优化发射波束成形和极化成形，其中波束成形利用空间自由度（DoFs）将发射波束导向用户，而极化成形利用极化自由度使用户接收到的电磁波的极化状态与其天线对齐。仿真结果表明，与传统的固定极化天线（FPA）系统相比，极化成形可以充分利用天线极化优化中的DoFs，显著增强无线通信系统的安全性能。", "summary": "本文研究了在存在窃听者的安全无线通信系统中，基于极化可重构天线的极化成形设计。为最大化可实现的安全速率，文章提出了一种高效的迭代算法，联合优化了发射波束成形和极化成形。其中，波束成形利用空间自由度引导信号，极化成形则利用极化自由度实现接收端极化状态对齐。仿真结果表明，该方法相比传统固定极化天线系统，能显著提升无线通信的安全性能。", "keywords": "极化成形, 安全通信, 波束成形, 极化可重构天线, 安全速率", "comments": "该论文提出了一种新颖的方法，通过结合极化成形和波束成形来增强无线通信的安全性。其创新点在于利用天线极化这一额外的自由度来对抗窃听者，这为物理层安全提供了新的思路。该技术在未来安全通信领域具有重要应用潜力。"}}
{"id": "2507.17273", "title": "Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance", "authors": ["Rishi Parekh", "Saisubramaniam Gopalakrishnan", "Zishan Ahmad", "Anirudh Deodhar"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2507.17273v1", "summary": "Analyzing large, complex output datasets from Discrete Event Simulations\n(DES) of warehouse operations to identify bottlenecks and inefficiencies is a\ncritical yet challenging task, often demanding significant manual effort or\nspecialized analytical tools. Our framework integrates Knowledge Graphs (KGs)\nand Large Language Model (LLM)-based agents to analyze complex Discrete Event\nSimulation (DES) output data from warehouse operations. It transforms raw DES\ndata into a semantically rich KG, capturing relationships between simulation\nevents and entities. An LLM-based agent uses iterative reasoning, generating\ninterdependent sub-questions. For each sub-question, it creates Cypher queries\nfor KG interaction, extracts information, and self-reflects to correct errors.\nThis adaptive, iterative, and self-correcting process identifies operational\nissues mimicking human analysis. Our DES approach for warehouse bottleneck\nidentification, tested with equipment breakdowns and process irregularities,\noutperforms baseline methods. For operational questions, it achieves\nnear-perfect pass rates in pinpointing inefficiencies. For complex\ninvestigative questions, we demonstrate its superior diagnostic ability to\nuncover subtle, interconnected issues. This work bridges simulation modeling\nand AI (KG+LLM), offering a more intuitive method for actionable insights,\nreducing time-to-insight, and enabling automated warehouse inefficiency\nevaluation and diagnosis.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.17273v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "利用知识图谱和大型语言模型推理识别仓库运营瓶颈以辅助规划", "tldr": "该研究提出了一种结合知识图谱和大型语言模型代理的框架，用于分析仓库离散事件仿真数据，以自动识别运营瓶颈和低效率问题，并通过迭代推理和自我纠正，在识别复杂运营问题方面表现出色。", "motivation": "分析复杂的大型仓库离散事件仿真(DES)输出数据以识别瓶颈和低效率是一项关键但具有挑战性的任务，通常需要大量手动工作或专业分析工具。现有方法效率低下，难以处理复杂数据。", "method": "该框架整合了知识图谱(KGs)和基于大型语言模型(LLM)的代理。它将原始DES数据转换为语义丰富的KG，捕获模拟事件和实体之间的关系。LLM代理使用迭代推理，生成相互依赖的子问题，并为每个子问题创建Cypher查询以与KG交互，提取信息并进行自我反思以纠正错误。这是一个自适应、迭代和自我纠正的过程。", "result": "该方法在识别仓库瓶颈方面（针对设备故障和流程异常）优于基线方法。对于运营问题，它在指出低效率方面的通过率接近完美。对于复杂的调查问题，它展现了卓越的诊断能力，能够发现微妙的、相互关联的问题。", "conclusion": "这项工作弥合了仿真建模和AI（KG+LLM）之间的鸿沟，提供了一种更直观的方法来获取可操作的见解，减少了获取见解的时间，并实现了仓库低效率的自动化评估和诊断。", "translation": "分析来自仓库操作的离散事件仿真（DES）的大型复杂输出数据集以识别瓶颈和低效率是一项关键但具有挑战性的任务，通常需要大量的手动工作或专业的分析工具。我们的框架整合了知识图谱（KGs）和基于大型语言模型（LLM）的代理来分析仓库操作中复杂的离散事件仿真（DES）输出数据。它将原始DES数据转换为语义丰富的知识图谱，捕获模拟事件和实体之间的关系。一个基于LLM的代理使用迭代推理，生成相互依赖的子问题。对于每个子问题，它创建Cypher查询以进行知识图谱交互，提取信息，并进行自我反思以纠正错误。这种自适应、迭代和自我纠正的过程模仿人类分析来识别运营问题。我们用于仓库瓶颈识别的DES方法，通过设备故障和流程不规范进行测试，优于基线方法。对于运营问题，它在精确定位低效率方面的通过率接近完美。对于复杂的调查问题，我们展示了其卓越的诊断能力，能够揭示微妙的、相互关联的问题。这项工作弥合了仿真建模和AI（KG+LLM）之间的鸿沟，提供了一种更直观的方法来获取可操作的见解，减少了获取见解的时间，并实现了仓库低效率的自动化评估和诊断。", "summary": "该研究提出了一种创新框架，结合知识图谱（KGs）和大型语言模型（LLM）代理，用于分析仓库离散事件仿真（DES）输出数据，以自动识别运营瓶颈和低效率。该框架将原始DES数据转化为语义丰富的KG，并通过LLM代理的迭代推理、Cypher查询生成和自我纠正机制，模仿人类分析过程来诊断复杂问题。实验结果表明，该方法在识别运营问题和复杂调查问题方面均优于基线方法，显著提高了仓库规划的效率和洞察力。", "keywords": "知识图谱, 大型语言模型, 仓库运营, 瓶颈识别, 离散事件仿真", "comments": "该论文的创新之处在于将知识图谱的结构化表示能力与大型语言模型的强大推理能力相结合，用于解决仓库运营瓶颈识别这一实际且复杂的问题。这种结合使得系统能够处理非结构化和复杂的数据，并通过迭代和自我纠正的过程模仿人类的分析思维，从而提供更准确和可操作的洞察。其重要性在于能够显著减少人工分析工作量，加速问题诊断，并推动仓库规划的自动化和智能化。"}}
{"id": "2412.12749", "title": "Safe Trajectory Sets for Online Operation of Power Systems under Uncertainty", "authors": ["Florian Klein-Helmkamp", "Tina Möllemann", "Irina Zettl", "Steffen Kortmann", "Andreas Ulbig"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12749v2", "summary": "Flexibility provision from active distribution grids requires efficient and\nrobust methods of optimization and control suitable to online operation. In\nthis paper we introduce conditions for the safe operation of feedback\noptimization based controllers. We use the feasible operating region of a\ncontrolled system as bounds for safe system states and evaluate the\ntrajectories of the controller based on the projection of the full system state\nonto the two-dimensional PQ-plane. We demonstrate the defined conditions for an\nexemplary sub-transmission system. We show that the proposed method is suitable\nto evaluate controller performance and robustness for systems subject to\ndisturbances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12749v2", "cate": "eess.SY", "date": "2024-12-17", "updated": "2025-07-23", "AI": {"title_translation": "不确定性下电力系统在线运行的安全轨迹集", "tldr": "本文提出了一种评估反馈优化控制器在电力系统在线运行中安全性的方法，通过定义安全运行条件和利用可行运行区域来评估控制器性能和鲁棒性。", "motivation": "有源配电网的灵活性供应需要高效、鲁棒且适用于在线操作的优化和控制方法。", "method": "本文引入了基于反馈优化的控制器安全运行条件。该方法使用受控系统的可行运行区域作为安全系统状态的边界，并根据完整系统状态在二维PQ平面上的投影来评估控制器的轨迹。", "result": "该方法在示例性输电系统中得到了验证，并表明其适用于评估受扰动系统中的控制器性能和鲁棒性。", "conclusion": "所提出的方法能够有效地评估电力系统在线运行中反馈优化控制器的安全性和鲁棒性。", "translation": "有源配电网的灵活性供应需要高效、鲁棒且适用于在线操作的优化和控制方法。在本文中，我们引入了基于反馈优化的控制器安全运行条件。我们使用受控系统的可行运行区域作为安全系统状态的边界，并根据完整系统状态在二维PQ平面上的投影来评估控制器的轨迹。我们为示例性输电系统演示了所定义的条件。我们表明，所提出的方法适用于评估受扰动系统中的控制器性能和鲁棒性。", "summary": "本文提出了一种针对不确定性下电力系统在线运行的反馈优化控制器的安全评估方法。通过定义安全运行条件，并利用受控系统的可行运行区域作为安全状态边界，结合完整系统状态在PQ平面上的投影，来评估控制器轨迹。研究表明，该方法能够有效评估受扰动系统中的控制器性能和鲁棒性。", "keywords": "安全轨迹集, 在线运行, 电力系统, 反馈优化, 不确定性", "comments": "本文提出了一种评估反馈优化控制器安全性的新颖方法，特别关注了在线操作和不确定性。通过将系统状态投影到PQ平面并利用可行运行区域作为安全边界，提供了一种实用的评估工具。其创新性在于为复杂电力系统中的控制器性能和鲁棒性评估提供了明确的条件和方法。"}}
{"id": "2507.17062", "title": "Explicit Monotone Stable Super-Time-Stepping Methods for Finite Time Singularities", "authors": ["Zheng Tan", "Tariq D. Aslam", "Andrea L. Bertozzi"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17062v1", "summary": "We explore a novel way to numerically resolve the scaling behavior of\nfinite-time singularities in solutions of nonlinear parabolic PDEs. The\nRunge--Kutta--Legendre (RKL) and Runge--Kutta--Gegenbauer (RKG)\nsuper-time-stepping methods were originally developed for nonlinear complex\nphysics problems with diffusion. These are multi-stage single step\nsecond-order, forward-in-time methods with no implicit solves. The advantage is\nthat the timestep size for stability scales with stage number $s$ as\n$\\mathcal{O}(s^2)$. Many interesting nonlinear PDEs have finite-time\nsingularities, and the presence of diffusion often limits one to using implicit\nor semi-implicit timestep methods for stability constraints. Finite-time\nsingularities are particularly challenging due to the large range of scales\nthat one desires to resolve, often with adaptive spatial grids and adaptive\ntimesteps. Here we show two examples of nonlinear PDEs for which the\nself-similar singularity structure has time and space scales that are\nresolvable using the RKL and RKG methods, without forcing even smaller\ntimesteps. Compared to commonly-used implicit numerical methods, we achieve\nsignificantly smaller run time while maintaining comparable accuracy. We also\nprove numerical monotonicity for both the RKL and RKG methods under their\nlinear stability conditions for the constant coefficient heat equation, in the\ncase of infinite domain and periodic boundary condition, leading to a\ntheoretical guarantee of the superiority of the RKL and RKG methods over\ntraditional super-time-stepping methods, such as the Runge-Kutta-Chebyshev\n(RKC) and the orthogonal Runge-Kutta-Chebyshev (ROCK) methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17062v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "有限时间奇点的显式单调稳定超时间步进方法", "tldr": "本文探讨了Runge-Kutta-Legendre（RKL）和Runge-Kutta-Gegenbauer（RKG）超时间步进方法在数值求解非线性抛物型偏微分方程有限时间奇点问题上的应用。这些方法是显式的，能有效处理大范围尺度，与隐式方法相比，运行时显著缩短，同时保持可比精度，并且理论上证明了其数值单调性。", "motivation": "数值求解非线性抛物型偏微分方程中解的有限时间奇点行为极具挑战性，因为这涉及到需要解析的大范围尺度，且扩散的存在常限制必须使用隐式或半隐式时间步进方法来满足稳定性约束。", "method": "本文使用并分析了Runge-Kutta-Legendre (RKL) 和 Runge-Kutta-Gegenbauer (RKG) 超时间步进方法。这些方法是多阶段、单步、二阶、前向时间步进方法，无需隐式求解。研究还在线性稳定性条件下，对常系数热方程的无限域和周期边界条件情况下的RKL和RKG方法证明了数值单调性。", "result": "RKL和RKG方法能够解析非线性偏微分方程中自相似奇点结构的时间和空间尺度，而无需强制使用更小的时间步长。与常用的隐式数值方法相比，这些方法在保持可比精度的同时，显著缩短了运行时间。此外，研究还证明了RKL和RKG方法在线性稳定性条件下的数值单调性，理论上保证了它们优于传统的超时间步进方法（如Runge-Kutta-Chebyshev (RKC) 和 orthogonal Runge-Kutta-Chebyshev (ROCK) 方法）。", "conclusion": "Runge-Kutta-Legendre (RKL) 和 Runge-Kutta-Gegenbauer (RKG) 超时间步进方法在解决非线性抛物型偏微分方程中的有限时间奇点问题上表现出卓越的性能，不仅能有效解析多尺度结构，还能显著提高计算效率，并通过数值单调性证明确立了其优于传统方法的理论优势。", "translation": "我们探索了一种新颖的数值方法，用于解析非线性抛物型偏微分方程解中有限时间奇点（finite-time singularities）的标度行为。Runge-Kutta-Legendre (RKL) 和 Runge-Kutta-Gegenbauer (RKG) 超时间步进方法最初是为具有扩散的非线性复杂物理问题开发的。这些方法是多阶段、单步、二阶、前向时间步进方法，无需隐式求解。其优点在于，稳定性的时间步长与阶段数 $s$ 的关系为 $\\mathcal{O}(s^2)$。许多有趣的非线性偏微分方程具有有限时间奇点，而扩散的存在常常限制人们只能使用隐式或半隐式时间步进方法来满足稳定性约束。有限时间奇点特别具有挑战性，因为需要解析的尺度范围很大，通常需要自适应空间网格和自适应时间步长。本文展示了两个非线性偏微分方程的例子，对于这些方程，自相似奇点结构的时间和空间尺度可以使用RKL和RKG方法解析，而无需强制使用更小的时间步长。与常用的隐式数值方法相比，我们在保持可比精度的同时，显著缩短了运行时间。我们还在常系数热方程的线性稳定性条件下，在无限域和周期边界条件的情况下，证明了RKL和RKG方法的数值单调性，这为RKL和RKG方法优于传统的超时间步进方法（如Runge-Kutta-Chebyshev (RKC) 和 orthogonal Runge-Kutta-Chebyshev (ROCK) 方法）提供了理论保证。", "summary": "本文研究了显式单调稳定的超时间步进方法，特别是Runge-Kutta-Legendre (RKL) 和 Runge-Kutta-Gegenbauer (RKG) 方法，在数值解析非线性抛物型偏微分方程中有限时间奇点行为的应用。这些方法是显式、二阶的，并具有时间步长随阶段数平方缩放的优势。研究表明，RKL和RKG能够有效处理有限时间奇点带来的巨大尺度范围挑战，与传统隐式方法相比，显著缩短了运行时间并保持了相当的精度。此外，本文还在线性稳定性条件下，从理论上证明了RKL和RKG方法的数值单调性，从而确立了其相对于Runge-Kutta-Chebyshev (RKC) 和 orthogonal Runge-Kutta-Chebyshev (ROCK) 等传统超时间步进方法的优越性。", "keywords": "有限时间奇点, 超时间步进, RKL, RKG, 非线性偏微分方程, 单调性", "comments": "本文的创新之处在于成功地将RKL和RKG超时间步进方法应用于处理有限时间奇点这一复杂问题，该问题通常需要计算成本更高的隐式方案。这些显式方法所展现的优越稳定性尺度（时间步长与阶段数平方成正比）和经理论证明的数值单调性，对于提升复杂非线性问题的数值模拟效率和可靠性具有重要意义。"}}
{"id": "2507.14871", "title": "Tiny language models", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      23 pages, 1 figure and 12 tables, The data and code that support the findings of this study are openly available in a GitHub repository at this https URL", "url": "http://arxiv.org/abs/2507.14871v2", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language. The data and\ncode that support the findings of this study are openly available on\nhttps://github.com/Rg32601/Tiny-Language-Models .", "comment": "23 pages, 1 figure and 12 tables, The data and code that support the\n  findings of this study are openly available in a GitHub repository at\n  https://github.com/Rg32601/Tiny-Language-Models", "pdf_url": "http://arxiv.org/pdf/2507.14871v2", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-23", "AI": {"title_translation": "小型语言模型", "tldr": "本研究探讨了小型语言模型（TLMs）是否具有大型语言模型（LLMs）的关键特征，并证明了预训练在TLMs上的有效性，以及通过浅层模型组合实现低延迟TLMs的可能性，为更广泛的NLP研究提供了可行的替代方案。", "motivation": "当前大型语言模型（LLMs）的预训练需要巨大的计算资源，只有少数公司能够承担，这限制了更广泛的研究参与。因此，迫切需要更易于获取的替代方案。", "method": "本研究探索了小型语言模型（TLMs）是否具备大型语言模型（LLMs）的关键定性特征。研究团队在维基百科数据集的子集上预训练了BERT-6和BERT-1的变体，并在FewRel、AGNews和DBPedia分类任务上评估了它们的性能。", "result": "研究表明，在分类任务中，预训练的TLMs与非预训练模型之间存在明显的性能差距，这表明即使在小规模模型中，预训练也是有效的。这种性能差距随着预训练数据集的大小以及预训练和分类数据集之间词元重叠的增加而增大。此外，通过多个独立预训练的浅层架构组成的软委员会，可以复制预训练的深层TLM架构所达到的分类准确率，从而在不影响分类准确率的情况下实现低延迟的TLMs。", "conclusion": "对TLM的未来研究有望进一步阐明自然语言处理（NLP）的潜在机制，特别是考虑到其生物学启发的模型表明TLM可能足以让儿童或青少年发展语言能力。", "translation": "自然语言处理（NLP）的一个突出成就是其理解和生成有意义人类语言的能力。这种能力依赖于在大型语言模型（LLMs）上预训练的复杂前馈Transformer块架构。然而，由于所需的巨大计算资源，LLM预训练目前仅对少数几家主要公司可行，这限制了更广泛的研究参与。这产生了对更易获取替代方案的迫切需求。在本研究中，我们探讨了小型语言模型（TLMs）是否表现出与LLMs相同的关键定性特征。我们证明了TLMs在分类任务中，预训练模型和非预训练模型之间存在明显的性能差距，这表明即使在微小规模下，预训练也是有效的。性能差距随着预训练数据集的大小以及预训练和分类数据集之间词元重叠的增加而增大。此外，预训练的深层TLM架构所达到的分类准确率可以通过多个独立预训练的浅层架构组成的软委员会来复制，从而在不影响分类准确率的情况下实现低延迟的TLMs。我们的结果基于在维基百科数据集子集上预训练BERT-6和BERT-1的变体，并评估它们在FewRel、AGNews和DBPedia分类任务上的性能。对TLM的未来研究有望进一步阐明NLP的潜在机制，特别是考虑到其生物学启发的模型表明TLM可能足以让儿童或青少年发展语言能力。支持本研究发现的数据和代码已在https://github.com/Rg32601/Tiny-Language-Models 上公开。", "summary": "本研究旨在解决大型语言模型（LLMs）预训练所需巨大计算资源导致的研究受限问题。作者探索了小型语言模型（TLMs）是否能展现LLMs的关键特征，并通过在BERT-6和BERT-1变体上进行预训练并在多个分类任务上进行评估，证明了预训练在TLMs上的有效性。研究发现，预训练能显著提升TLMs的性能，且性能增益随数据集规模和数据重叠度增加而提高。此外，通过组合多个浅层TLM架构，可在保持准确率的同时实现低延迟。这些发现为NLP研究提供了更易于获取且计算效率更高的替代方案，并为理解语言发展机制提供了新的视角。", "keywords": "小型语言模型, 预训练, BERT, 分类任务, 低延迟", "comments": "这项研究的创新之处在于它证明了即使是小型语言模型（TLMs）也能通过预训练获得显著的性能提升，并且可以通过组合浅层模型来达到深层模型的准确率，同时降低延迟。这对于解决大型语言模型训练成本高昂、限制研究参与的问题具有重要意义。它为更广泛的NLP研究社区提供了可行的替代方案，并可能促进对语言学习生物学机制的理解。"}}
{"id": "2507.16940", "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation", "authors": ["Nima Fathi", "Amar Kumar", "Tal Arbel"], "categories": ["cs.CV", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, International Conference on Medical Image Computing and Computer-Assisted Intervention", "url": "http://arxiv.org/abs/2507.16940v1", "summary": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.", "comment": "9 pages, 3 figures, International Conference on Medical Image\n  Computing and Computer-Assisted Intervention", "pdf_url": "http://arxiv.org/pdf/2507.16940v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "AURA: 一个用于理解、推理和标注的多模态医疗智能体", "tldr": "AURA是首个用于全面医学图像分析、解释和评估的多模态医疗智能体，它利用基于大型语言模型（LLM）的智能体AI。", "motivation": "大型语言模型（LLM）驱动的智能体系统虽然在许多领域前景广阔，但在医学影像领域的应用仍处于起步阶段。本研究旨在通过开发更透明、适应性更强、更符合临床需求的AI系统，将医学图像分析从静态预测转变为交互式决策支持。", "method": "本文介绍了AURA，首个专为医学图像的全面分析、解释和评估而设计的视觉语言可解释性智能体。AURA利用基于LLM的Qwen-32B架构，集成了一个模块化工具箱，包括：(i)一个分割套件，用于定位临床有意义的区域（包含相位接地、病理分割和解剖分割）；(ii)一个反事实图像生成模块，通过图像级解释支持推理；(iii)一套评估工具，包括像素级差异图分析、分类和先进的最新组件，以评估诊断相关性和视觉可解释性。", "result": "AURA实现了动态交互、上下文解释和假设检验，代表了在开发更透明、适应性更强、更符合临床需求的AI系统方面取得了重大进展。它突出了智能体AI在将医学图像分析从静态预测转变为交互式决策支持方面的潜力。", "conclusion": "AURA通过利用智能体AI，实现了医学图像分析从静态预测到交互式决策支持的转变，代表着在构建更透明、适应性更强、更符合临床需求的医学AI系统方面迈出了重要一步。", "translation": "大型语言模型（LLM）的最新进展促使范式从静态预测系统转向能够推理、与工具交互以及适应复杂任务的智能体AI。尽管基于LLM的智能体系统在许多领域显示出潜力，但它们在医学影像领域的应用仍处于起步阶段。在这项工作中，我们介绍了AURA，这是首个专门为医学图像的全面分析、解释和评估而设计的视觉语言可解释性智能体。通过实现动态交互、上下文解释和假设检验，AURA代表了在开发更透明、适应性更强、更符合临床需求的AI系统方面取得了重大进展。我们强调了智能体AI在将医学图像分析从静态预测转变为交互式决策支持方面的潜力。AURA利用基于LLM的Qwen-32B架构，集成了一个模块化工具箱，包括：(i)一个分割套件，包含相位接地、病理分割和解剖分割，用于定位临床有意义的区域；(ii)一个反事实图像生成模块，通过图像级解释支持推理；(iii)一套评估工具，包括像素级差异图分析、分类和先进的最新组件，以评估诊断相关性和视觉可解释性。", "summary": "本文介绍了AURA，这是首个专为医学图像全面分析、解释和评估而设计的视觉语言可解释性智能体。AURA利用Qwen-32B这一基于LLM的架构，集成了模块化工具箱，包括用于定位临床区域的分割套件、支持图像级解释的反事实图像生成模块以及评估诊断相关性和视觉可解释性的评估工具。该系统实现了动态交互和上下文解释，标志着在开发更透明、适应性更强、更符合临床需求的医学影像AI系统方面取得了重大进展，将分析从静态预测转变为交互式决策支持。", "keywords": "医疗智能体, 多模态AI, 医学影像, 可解释AI, LLM", "comments": "这篇论文通过将智能体大型语言模型（LLMs）应用于医学影像领域，解决了医学AI中的一个关键空白，超越了传统的静态模型。AURA的模块化设计，结合了分割、反事实推理和评估，具有创新性，提供了临床采纳所必需的可解释性和交互性。这项工作有望提高医疗保健领域AI系统的诊断准确性和信任度。"}}
{"id": "2503.12192", "title": "Closing the Chain: How to reduce your risk of being SolarWinds, Log4j, or XZ Utils", "authors": ["Sivana Hamer", "Jacob Bowen", "Md Nazmul Haque", "Robert Hines", "Chris Madden", "Laurie Williams"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.12192v2", "summary": "Software supply chain frameworks, such as the US NIST Secure Software\nDevelopment Framework (SSDF), detail what tasks software development\norganizations are recommended or mandated to adopt to reduce security risk.\nHowever, to further reduce the risk of similar attacks occurring, software\norganizations benefit from knowing what tasks mitigate attack techniques the\nattackers are currently using to address specific threats, prioritize tasks,\nand close mitigation gaps. The goal of this study is to aid software\norganizations in reducing the risk of software supply chain attacks by\nsystematically synthesizing how framework tasks mitigate the attack techniques\nused in the SolarWinds, Log4j, and XZ Utils attacks. We qualitatively analyzed\n106 Cyber Threat Intelligence (CTI) reports of the 3 attacks to gather the\nattack techniques. We then systematically constructed a mapping between attack\ntechniques and the 73 tasks enumerated in 10 software supply chain frameworks.\nAfterward, we established and ranked priority tasks that mitigate attack\ntechniques. The three mitigation tasks with the highest scores are role-based\naccess control, system monitoring, and boundary protection. Additionally, three\nmitigation tasks were missing from all ten frameworks, including sustainable\nopen-source software and environmental scanning tools. Thus, software products\nwould still be vulnerable to software supply chain attacks even if\norganizations adopted all recommended tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.12192v2", "cate": "cs.SE", "date": "2025-03-15", "updated": "2025-07-23", "AI": {"title_translation": "弥合鸿沟：如何降低成为SolarWinds、Log4j或XZ Utils受害者的风险", "tldr": "本研究系统地分析了软件供应链框架任务如何缓解SolarWinds、Log4j和XZ Utils攻击中使用的技术，并发现现有框架存在缓解措施缺失。", "motivation": "为了进一步降低类似攻击发生的风险，软件组织需要了解哪些任务能够有效缓解攻击者当前使用的攻击技术，从而优先处理任务并弥补缓解措施的空白。本研究旨在通过系统性地综合框架任务如何缓解SolarWinds、Log4j和XZ Utils攻击中使用的攻击技术，帮助软件组织降低软件供应链攻击的风险。", "method": "研究人员定性分析了106份关于SolarWinds、Log4j和XZ Utils这三种攻击的网络威胁情报（CTI）报告，以收集攻击技术。然后，系统地构建了攻击技术与10个软件供应链框架中列举的73项任务之间的映射关系。之后，研究人员建立并排定了缓解攻击技术的优先任务。", "result": "得分最高的三项缓解任务是基于角色的访问控制、系统监控和边界保护。此外，所有十个框架中都缺失了三项缓解任务，包括可持续的开源软件和环境扫描工具。", "conclusion": "即使组织采纳了所有推荐的任务，软件产品仍然容易受到软件供应链攻击，因为现有框架中存在缓解措施的缺失。", "translation": "软件供应链框架，例如美国国家标准与技术研究院（NIST）的安全软件开发框架（SSDF），详细说明了软件开发组织为降低安全风险而建议或强制采纳的任务。然而，为了进一步降低类似攻击发生的风险，软件组织通过了解哪些任务能够缓解攻击者当前正在使用的攻击技术，从而解决特定威胁、优先处理任务并弥补缓解措施的空白，将从中受益。本研究的目标是系统地综合框架任务如何缓解SolarWinds、Log4j和XZ Utils攻击中使用的攻击技术，从而帮助软件组织降低软件供应链攻击的风险。我们定性分析了106份关于这三种攻击的网络威胁情报（CTI）报告，以收集攻击技术。然后，我们系统地构建了攻击技术与10个软件供应链框架中列举的73项任务之间的映射关系。之后，我们建立并排定了缓解攻击技术的优先任务。得分最高的三项缓解任务是基于角色的访问控制、系统监控和边界保护。此外，所有十个框架中都缺失了三项缓解任务，包括可持续的开源软件和环境扫描工具。因此，即使组织采纳了所有推荐的任务，软件产品仍然容易受到软件供应链攻击。", "summary": "本研究旨在帮助软件组织降低软件供应链攻击的风险，通过系统地分析现有软件供应链框架任务如何缓解SolarWinds、Log4j和XZ Utils等重大攻击中使用的技术。研究人员分析了106份网络威胁情报报告，建立了攻击技术与10个框架中73项任务的映射关系，并排定了优先任务。结果显示，基于角色的访问控制、系统监控和边界保护是优先级最高的缓解任务。值得注意的是，研究还发现现有框架中缺失了可持续开源软件和环境扫描工具等三项关键缓解措施，这意味着即使完全遵循现有框架，软件仍可能面临供应链攻击的风险。", "keywords": "软件供应链, 安全框架, 攻击缓解, 威胁情报, 漏洞", "comments": "该论文通过对具体历史攻击事件的详细分析，揭示了当前主流软件供应链安全框架的不足之处，具有重要的实践指导意义。其创新之处在于将实际攻击技术与框架任务进行系统性映射，并识别出优先级高的缓解措施和现有框架的空白点，为未来框架的完善提供了数据支持。这对于提升软件供应链的整体安全性具有重要价值。"}}
{"id": "2507.17219", "title": "A Low-Cost Machine Learning Approach for Timber Diameter Estimation", "authors": ["Fatemeh Hasanzadeh Fard", "Sanaz Hasanzadeh Fard", "Mehdi Jonoobi"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17219v1", "summary": "The wood processing industry, particularly in facilities such as sawmills and\nMDF production lines, requires accurate and efficient identification of species\nand thickness of the wood. Although traditional methods rely heavily on expert\nhuman labor, they are slow, inconsistent, and prone to error, especially when\nprocessing large volumes. This study focuses on practical and cost-effective\nmachine learning frameworks that automate the estimation of timber log diameter\nusing standard RGB images captured under real-world working conditions. We\nemploy the YOLOv5 object detection algorithm, fine-tuned on a public dataset\n(TimberSeg 1.0), to detect individual timber logs and estimate thickness\nthrough bounding-box dimensions. Unlike previous methods that require expensive\nsensors or controlled environments, this model is trained on images taken in\ntypical industrial sheds during timber delivery. Experimental results show that\nthe model achieves a mean Average Precision (mAP@0.5) of 0.64, demonstrating\nreliable log detection even with modest computing resources. This lightweight,\nscalable solution holds promise for practical integration into existing\nworkflows, including on-site inventory management and preliminary sorting,\nparticularly in small and medium-sized operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17219v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种低成本的木材直径估算机器学习方法", "tldr": "开发了一种基于YOLOv5的低成本机器学习方法，利用标准RGB图像估算木材原木直径，适用于工业环境。", "motivation": "木材加工行业传统识别木材种类和厚度的方法速度慢、不一致且容易出错，尤其是在处理大量木材时。现有自动化方法常需要昂贵的传感器或受控环境，成本较高。", "method": "本研究采用YOLOv5目标检测算法，在公共数据集TimberSeg 1.0上进行微调，利用在真实工业环境下（典型工业厂房）捕获的标准RGB图像，通过边界框尺寸检测单个木材原木并估算其厚度。", "result": "该模型实现了0.64的平均精度（mAP@0.5），即使在适度的计算资源下也能可靠地检测原木。", "conclusion": "该轻量级、可扩展的解决方案有望实际集成到现有工作流程中，包括现场库存管理和初步分拣，特别适用于中小型企业。", "translation": "木材加工行业，特别是锯木厂和中密度纤维板生产线等设施，需要准确高效地识别木材种类和厚度。尽管传统方法严重依赖专业人工，但它们速度慢、不一致且容易出错，尤其是在处理大量木材时。本研究专注于实用且经济高效的机器学习框架，该框架使用在真实工作条件下捕获的标准RGB图像自动化估算木材原木直径。我们采用在公共数据集（TimberSeg 1.0）上进行微调的YOLOv5目标检测算法，通过边界框尺寸检测单个木材原木并估算厚度。与需要昂贵传感器或受控环境的先前方法不同，该模型在木材交付期间在典型工业厂房中拍摄的图像上进行训练。实验结果表明，该模型在适度的计算资源下，平均精度（mAP@0.5）达到0.64，即使在资源有限的情况下也能可靠地检测原木。这种轻量级、可扩展的解决方案有望实际集成到现有工作流程中，包括现场库存管理和初步分拣，特别适用于中小型企业。", "summary": "本研究提出了一种低成本的机器学习方法，利用标准RGB图像和YOLOv5算法在真实工业条件下自动估算木材原木直径。该模型在TimberSeg 1.0数据集上训练，实现了0.64的mAP@0.5，证明了其在资源有限环境下的可靠性，为木材加工业的库存管理和分拣提供了实用的解决方案。", "keywords": "木材直径, 机器学习, YOLOv5, 目标检测, 低成本", "comments": "该论文通过利用现成的RGB图像和成熟的目标检测模型（YOLOv5），提出了一种创新且经济高效的木材直径估算方法，避免了以往方法所需昂贵专用传感器或受控环境。其在真实工业环境中的实际适用性，尤其对中小型企业而言，是一个显著的优势。报告的mAP@0.5为0.64，对于低成本、实际应用而言是可接受的，表明在性能和实用性之间取得了良好的平衡。"}}
{"id": "2507.17494", "title": "To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks", "authors": ["Rashika Raina", "Nidhi Simmons", "David E. Simmons", "Michel Daoud Yacoub", "Trung Q. Duong"], "categories": ["stat.ML", "cs.AI", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17494v1", "summary": "In next-generation communications and networks, machine learning (ML) models\nare expected to deliver not only accurate predictions but also well-calibrated\nconfidence scores that reflect the true likelihood of correct decisions. This\npaper studies the calibration performance of an ML-based outage predictor\nwithin a single-user, multi-resource allocation framework. We first establish\nkey theoretical properties of this system's outage probability (OP) under\nperfect calibration. Importantly, we show that as the number of resources\ngrows, the OP of a perfectly calibrated predictor approaches the expected\noutput conditioned on it being below the classification threshold. In contrast,\nwhen only one resource is available, the system's OP equals the model's overall\nexpected output. We then derive the OP conditions for a perfectly calibrated\npredictor. These findings guide the choice of the classification threshold to\nachieve a desired OP, helping system designers meet specific reliability\nrequirements. We also demonstrate that post-processing calibration cannot\nimprove the system's minimum achievable OP, as it does not introduce new\ninformation about future channel states. Additionally, we show that\nwell-calibrated models are part of a broader class of predictors that\nnecessarily improve OP. In particular, we establish a monotonicity condition\nthat the accuracy-confidence function must satisfy for such improvement to\noccur. To demonstrate these theoretical properties, we conduct a rigorous\nsimulation-based analysis using post-processing calibration techniques: Platt\nscaling and isotonic regression. As part of this framework, the predictor is\ntrained using an outage loss function specifically designed for this system.\nFurthermore, this analysis is performed on Rayleigh fading channels with\ntemporal correlation captured by Clarke's 2D model, which accounts for receiver\nmobility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17494v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "信任与否：无线网络中基于机器学习的资源分配校准研究", "tldr": "本文研究了无线网络中基于机器学习的资源分配中，ML模型校准性能对系统中断概率（OP）的影响。结果表明，随着资源数量增加，完美校准的预测器OP趋近于特定条件下的期望输出，并建立了OP条件，指导系统设计以满足可靠性要求。研究还指出后处理校准不能提高最小可达OP，并提出了改善OP的单调性条件。", "motivation": "在下一代通信和网络中，机器学习（ML）模型不仅需要提供准确的预测，还需要提供能够反映正确决策真实可能性的良好校准置信度分数。本文研究了ML模型校准性能在无线网络资源分配中的重要性。", "method": "本文首先建立了在完美校准下系统中断概率（OP）的关键理论性质，并推导了完美校准预测器的OP条件。随后，通过使用Platt缩放和等渗回归等后处理校准技术，在瑞利衰落信道（通过Clarke的2D模型捕获时间相关性）上进行了严格的仿真分析，其中预测器使用专门为此系统设计的停机损失函数进行训练。", "result": "研究表明，当资源数量增加时，完美校准预测器的OP接近于在分类阈值以下时的预期输出；而当只有一个资源可用时，系统OP等于模型的整体预期输出。推导出了完美校准预测器的OP条件。此外，研究还发现后处理校准不能改善系统的最小可实现OP，因为它不会引入关于未来信道状态的新信息。最后，证明了良好校准的模型是更广泛的预测器类别的一部分，这些预测器必然会改善OP，并建立了精度-置信度函数必须满足的单调性条件。", "conclusion": "本文的研究结果指导系统设计者选择分类阈值以实现所需的中断概率，从而帮助满足特定的可靠性要求。同时，揭示了模型校准对系统性能提升的关键作用，并指出了改善中断概率的必要条件。", "translation": "在下一代通信和网络中，机器学习（ML）模型不仅需要提供准确的预测，还需要提供能够反映正确决策真实可能性的良好校准置信度分数。本文研究了单用户、多资源分配框架下，基于ML的中断预测器的校准性能。我们首先建立了该系统在完美校准下中断概率（OP）的关键理论性质。重要的是，我们表明随着资源数量的增加，完美校准预测器的OP趋近于以其低于分类阈值为条件的期望输出。相比之下，当只有一个资源可用时，系统的OP等于模型的整体期望输出。然后，我们推导了完美校准预测器的OP条件。这些发现指导了分类阈值的选择以实现所需的OP，帮助系统设计者满足特定的可靠性要求。我们还证明了后处理校准不能改善系统的最小可实现OP，因为它没有引入关于未来信道状态的新信息。此外，我们表明良好校准的模型是更广泛的预测器类别的一部分，这些预测器必然会改善OP。特别是，我们建立了精度-置信度函数必须满足的单调性条件，才能发生这种改善。为了证明这些理论性质，我们使用后处理校准技术：Platt缩放和等渗回归，进行了严格的基于仿真的分析。作为该框架的一部分，预测器使用专门为此系统设计的中断损失函数进行训练。此外，该分析在瑞利衰落信道上进行，其时间相关性由Clarke的2D模型捕获，该模型考虑了接收器移动性。", "summary": "本文探讨了下一代无线网络中，机器学习模型校准性能对资源分配中中断概率（OP）的影响。研究建立了完美校准下OP的关键理论性质，揭示了OP随资源数量变化的规律，并推导了实现期望OP的条件。研究还指出，后处理校准无法改善系统最小可实现OP，并提出了精度-置信度函数需满足的单调性条件，以确保OP的改善。通过仿真验证了这些理论发现，为系统设计者提供了满足可靠性要求的指导。", "keywords": "机器学习, 校准, 无线网络, 资源分配, 中断概率", "comments": "本文深入探讨了机器学习模型在无线网络资源分配中校准的重要性，不仅提出了理论见解，还通过严格的仿真验证了这些理论。其创新点在于揭示了校准与系统可靠性（中断概率）之间的深层联系，特别是提出了资源数量对OP的影响、后处理校准的局限性以及改善OP的单调性条件。这对于未来无线网络中ML应用的可靠性设计具有重要的指导意义。"}}
{"id": "2507.15120", "title": "Automated planning with ontologies under coherence update semantics (Extended Version)", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele Röger"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Extended version of a paper accepted at 22nd International Conference on Principles of Knowledge Representation and Reasoning (KR 2025)", "url": "http://arxiv.org/abs/2507.15120v2", "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "comment": "Extended version of a paper accepted at 22nd International Conference\n  on Principles of Knowledge Representation and Reasoning (KR 2025)", "pdf_url": "http://arxiv.org/pdf/2507.15120v2", "cate": "cs.AI", "date": "2025-07-20", "updated": "2025-07-23", "AI": {"title_translation": "基于相干更新语义的本体自动化规划（扩展版）", "tldr": "本文提出了一种结合本体论和相干更新语义的DL-Lite本体自动化规划新方法，其复杂性不高于现有方法，并通过编译为经典规划进行了实现和评估。", "motivation": "将背景知识（例如本体论）整合到自动化规划问题中，以克服标准自动化规划中封闭世界语义的限制。", "method": "提出了一种结合显式输入知识和动作库（eKABs）提供的基于本体的动作条件，以及在相干更新语义下的本体感知动作效果的DL-Lite本体规划新方法。通过多项式编译将其实现为经典规划。", "result": "结果形式的复杂性不高于现有方法。对现有和新基准的评估考察了规划系统在编译不同变体上的性能。", "conclusion": "该研究成功地提出了一种将本体论整合到自动化规划中的新方法，并在复杂性和性能上展现出竞争力。", "translation": "标准自动化规划采用封闭世界语义下的一阶公式，以给定的动作集从初始状态实现目标。我们遵循一项研究路线，旨在将背景知识整合到自动化规划问题中，例如通过本体论，本体论通常在开放世界语义下解释。我们提出了一种DL-Lite本体规划的新方法，该方法结合了显式输入知识和动作库（eKABs）提供的基于本体的动作条件的优势，以及在相干更新语义下的本体感知动作效果。我们表明，所得形式的复杂性不高于以前的方法，并通过多项式编译将其实现为经典规划。对现有和新基准的评估考察了规划系统在编译不同变体上的性能。", "summary": "本文提出了一种在相干更新语义下，结合DL-Lite本体论进行自动化规划的新方法。该方法将基于本体的动作条件与本体感知的动作效果相结合，其复杂性未增加，并可通过多项式编译转换为经典规划。实验评估证明了其在不同基准上的性能。", "keywords": "自动化规划, 本体论, 相干更新语义, DL-Lite, 知识整合", "comments": "该论文的创新之处在于将本体论（DL-Lite）与相干更新语义结合应用于自动化规划，并证明其复杂性未高于现有方法。通过将其编译为经典规划，提供了一种实用的实现途径。这对于将背景知识融入规划领域具有重要意义。"}}
{"id": "2507.01955", "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "Oğuzhan Fatih Kar", "Amir Zamir"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page at this https URL", "url": "http://arxiv.org/abs/2507.01955v2", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.", "comment": "Project page at https://fm-vision-evals.epfl.ch/", "pdf_url": "http://arxiv.org/pdf/2507.01955v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-23", "AI": {"title_translation": "GPT-4o对视觉的理解程度如何？评估多模态基础模型在标准计算机视觉任务上的表现", "tldr": "本文评估了GPT-4o、Gemini 1.5 Pro等主流多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不是专业模型的SOTA，但作为通用模型表现出色，且在语义任务上优于几何任务，GPT-4o在非推理模型中表现最佳。", "motivation": "尽管GPT-4o等多模态基础模型取得了显著进展，但它们在视觉理解方面的具体能力尚不明确。", "method": "研究人员通过将标准视觉任务转化为文本可提示和API兼容的任务，并利用提示链（prompt chaining）创建了标准化基准测试框架，在COCO、ImageNet等数据集上对GPT-4o、Gemini 1.5 Pro等模型进行了语义分割、目标检测、图像分类、深度和表面法线预测等标准计算机视觉任务的性能评估。", "result": "1) 这些模型在任何任务上都未接近最先进的专业模型；2) 它们是可敬的通用模型，考虑到它们主要基于图像-文本任务进行训练，这一点值得关注；3) 它们在语义任务上的表现明显优于几何任务；4) 提示链技术会影响性能，但更好的模型对提示变化的敏感度较低；5) GPT-4o在非推理模型中表现最佳，在6项任务中有4项排名第一；6) 推理模型（如o3）在几何任务上显示出改进；7) 对具有原生图像生成能力的模型（如最新的GPT-4o）的初步分析显示，它们存在幻觉和空间错位等问题。", "conclusion": "多模态基础模型在视觉理解方面表现出强大的通用能力，尤其是在语义任务上，但尚未达到专业模型的SOTA水平。GPT-4o在现有模型中表现突出，但具备原生图像生成能力的模型仍存在局限性。", "translation": "多模态基础模型，如GPT-4o，最近取得了显著进展，但这些模型在视觉理解方面的具体水平尚不清楚。在本文中，我们使用既定数据集（例如COCO、ImageNet及其变体等）在标准计算机视觉任务（语义分割、目标检测、图像分类、深度和表面法线预测）上对流行的多模态基础模型（GPT-4o、o4-mini、Gemini 1.5 Pro和Gemini 2.0 Flash、Claude 3.5 Sonnet、Qwen2-VL、Llama 3.2）的性能进行了基准测试。\n执行此操作的主要挑战是：1) 大多数模型被训练为输出文本，无法原生表达多功能领域，例如分割或3D几何；2) 许多领先模型是专有的，只能通过API访问，即无法访问权重来适应它们。我们通过提示链（prompt chaining）将标准视觉任务转换为等效的文本可提示和API兼容的任务，从而创建了一个标准化的基准测试框架，以应对这些挑战。\n我们观察到：1) 这些模型在任何任务上都未接近最先进的专业模型。然而，2) 它们是可敬的通用模型；这值得关注，因为它们可能主要基于图像-文本任务进行训练。3) 它们在语义任务上的表现明显优于几何任务。4) 尽管提示链技术会影响性能，但更好的模型对提示变化的敏感度较低。5) GPT-4o在非推理模型中表现最佳，在6项任务中有4项排名第一，6) 推理模型（例如o3）在几何任务上显示出改进，7) 对具有原生图像生成能力的模型（如最新的GPT-4o）的初步分析显示，它们存在幻觉和空间错位等问题。", "summary": "本文评估了GPT-4o、Gemini 1.5 Pro等主流多模态基础模型在标准计算机视觉任务上的表现。研究人员通过将视觉任务转化为文本提示和API兼容的形式，建立了一个标准化基准框架。结果显示，这些模型虽未达到专业模型的顶尖水平，但作为通用模型表现出色，尤其在语义任务上优于几何任务。GPT-4o在非推理模型中性能最佳，而具有原生图像生成能力的模型仍存在一些视觉缺陷。", "keywords": "多模态基础模型, GPT-4o, 计算机视觉, 基准测试, 提示链", "comments": "本文的创新之处在于其针对API可访问的专有多模态模型，通过“提示链”策略将其转换为可进行标准计算机视觉任务基准测试的形式，有效解决了模型权重不可访问的挑战。其重要性在于首次系统性地评估了GPT-4o等前沿多模态模型在传统视觉任务上的真实能力，揭示了它们作为通用视觉模型的潜力与局限性。局限性在于，尽管通用性强，但与SOTA专业模型仍有显著差距，且部分模型在原生图像生成方面存在视觉问题。"}}
{"id": "2507.16841", "title": "AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens", "authors": ["Waseem Akram", "Muhayy Ud Din", "Abdelhaleem Saad", "Irfan Hussain"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16841v1", "summary": "Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16841v1", "cate": "cs.RO", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "AquaChat：一个LLM引导的ROV框架，用于水产养殖网箱的自适应检查", "tldr": "AquaChat是一个结合LLM的ROV框架，通过自然语言命令实现水产养殖网箱的智能自适应检查，提高灵活性、准确性和效率。", "motivation": "水产养殖网箱的检查对于维护结构完整性、生物安全性和运营效率至关重要。传统检查方法（预编程任务或手动控制）在动态水下条件和用户特定需求方面适应性有限。", "method": "提出了AquaChat，一个结合大型语言模型（LLM）的遥控潜水器（ROV）框架，用于智能自适应网箱检查。该系统采用多层架构：(1) 高级规划层：使用LLM解释自然语言用户命令生成符号任务计划；(2) 中级任务管理器：将计划转换为ROV控制序列；(3) 低级运动控制层：精确执行导航和检查任务。通过实时反馈和事件触发的重新规划增强鲁棒性。", "result": "在模拟和受控水生环境中进行的实验验证了该框架。结果表明，任务灵活性、检查准确性和操作效率均得到改善。", "conclusion": "AquaChat展示了将基于语言的AI与海洋机器人技术相结合的潜力，以实现用于可持续水产养殖的智能、用户交互式检查系统。", "translation": "水产养殖网箱的检查对于维持鱼类养殖系统的结构完整性、生物安全性和运营效率至关重要。传统的检查方法依赖于预编程任务或手动控制，对动态水下条件和用户特定需求的适应性有限。在本研究中，我们提出了AquaChat，这是一种新型的遥控潜水器（ROV）框架，它集成了大型语言模型（LLM），用于智能和自适应的网箱检查。该系统具有多层架构：（1）一个高级规划层，使用LLM解释自然语言用户命令以生成符号任务计划；（2）一个中级任务管理器，将计划转换为ROV控制序列；（3）一个低级运动控制层，精确执行导航和检查任务。实时反馈和事件触发的重新规划增强了在挑战性水产养殖环境中的鲁棒性。该框架通过在模拟和代表水产养殖网箱的受控水生环境中进行的实验得到验证。结果表明，任务灵活性、检查准确性和操作效率均得到改善。AquaChat展示了将基于语言的AI与海洋机器人技术相结合的潜力，以实现用于可持续水产养殖的智能、用户交互式检查系统。", "summary": "本研究提出了AquaChat，一个创新的ROV框架，通过集成大型语言模型（LLM）来解决传统水产养殖网箱检查方法适应性差的问题。AquaChat采用多层架构，能够将自然语言命令转化为ROV操作，并通过实时反馈和重规划提高在复杂环境中的鲁棒性。实验结果验证了其在任务灵活性、检查准确性和操作效率方面的显著提升，展示了将AI与海洋机器人结合以实现智能、交互式检查系统的巨大潜力。", "keywords": "水产养殖网箱, ROV, 大型语言模型, 自适应检查, 海洋机器人", "comments": "该论文的创新点在于将大型语言模型（LLM）引入到海洋机器人（ROV）控制中，实现了通过自然语言进行任务规划和自适应检查，极大地提升了传统水下检查的灵活性和智能化水平。这种结合有望为水产养殖等领域提供更高效、更用户友好的自动化解决方案。"}}
{"id": "2507.17275", "title": "Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning", "authors": ["Po-Yen Wu", "Cheng-Yu Kuo", "Yuki Kadokawa", "Takamitsu Matsubara"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.17275v1", "summary": "In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.17275v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "延长工具寿命：通过寿命引导的强化学习学习通用工具的熟练使用", "tldr": "机器人通过一个结合工具寿命估计的强化学习框架，学习延长通用工具寿命的策略，并在仿真和现实任务中验证了其有效性。", "motivation": "在难以进入且任务需求不确定的环境中，机器人依赖通用工具，但这些工具的寿命对其使用方式高度敏感。因此，需要学习一种既能完成任务又能延长工具寿命的策略。", "method": "本文引入了一个强化学习（RL）框架，将工具寿命作为策略优化的因素。该框架利用有限元分析（FEA）和Miner规则估计剩余使用寿命（RUL），并将RUL整合到RL奖励中。为处理RUL只能在任务执行后估计的问题，引入了自适应奖励归一化（ARN）机制。", "result": "学习到的策略显著延长了工具寿命（在模拟中最高达8.01倍），并有效地迁移到现实世界设置中，在物体移动和开门等任务中得到了验证。", "conclusion": "通过寿命引导的强化学习，机器人能够学习到既能完成任务又能显著延长通用工具寿命的策略，具有实际应用价值。", "translation": "在任务需求不确定的难以进入的环境中，机器人通常依赖于缺乏预定义使用策略的通用工具。这些工具并非为特定操作量身定制，因此其寿命对其使用方式高度敏感。这带来了一个根本性挑战：机器人如何学习一种既能完成任务又能延长工具寿命的工具使用策略？在这项工作中，我们通过引入一个强化学习（RL）框架来解决这一挑战，该框架将工具寿命作为策略优化过程中的一个因素。我们的框架利用有限元分析（FEA）和Miner规则来根据累积应力估计剩余使用寿命（RUL），并将RUL整合到RL奖励中，以指导策略学习朝着寿命引导的行为发展。为了处理RUL只能在任务执行后估计的事实，我们引入了一种自适应奖励归一化（ARN）机制，该机制根据估计的RUL动态调整奖励缩放，确保稳定的学习信号。我们通过模拟和现实世界的工具使用任务验证了我们的方法，包括使用多种通用工具进行物体移动和开门。学习到的策略持续延长了工具寿命（在模拟中高达8.01倍），并有效地迁移到现实世界设置中，展示了学习寿命引导的工具使用策略的实际价值。", "summary": "本文提出一种新颖的强化学习框架，旨在解决机器人使用通用工具时，如何在完成任务的同时延长工具寿命的挑战。该框架通过结合有限元分析和Miner规则估计工具的剩余使用寿命（RUL），并将其融入强化学习的奖励机制中。为应对RUL后验估计的难题，引入了自适应奖励归一化（ARN）机制以稳定学习信号。实验结果表明，该方法在模拟和真实世界任务中均能显著延长工具寿命，证明了其在实际应用中的有效性。", "keywords": "强化学习, 工具寿命, 剩余使用寿命, 通用工具, 有限元分析", "comments": "本文的创新点在于将工具寿命（通过FEA和Miner's Rule估计RUL）明确地整合到强化学习的奖励函数中，并提出了ARN机制来解决RUL后验估计带来的学习稳定性问题。这对于在实际部署中提高机器人工具的耐久性和降低维护成本具有重要意义。该研究为机器人长期自主操作提供了一个新的视角。"}}
{"id": "2507.16923", "title": "Stable and Fair Benefit Allocation in Mixed-Energy Truck Platooning: A Coalitional Game Approach", "authors": ["Ting Bai", "Karl Henrik Johansson", "Jonas Mårtensson", "Andreas A. Malikopoulos"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      16 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16923v1", "summary": "This paper addresses the benefit allocation in a mixed-energy truck platoon\ncomposed of fuel-powered and electric trucks. The interactions among trucks\nduring platoon formation are modeled as a coalitional game with transferable\nutility. We first design a stable payoff allocation scheme that accounts for\ntruck heterogeneity in energy savings and platoon roles (leader or follower),\nestablishing core-stability conditions to ensure that no subset of trucks has\nan incentive to deviate for greater benefit. To enhance payoff fairness, we\nthen propose a closed-form, Shapley value-based allocation approach that is\ncomputationally efficient and independent of the platoon size. Sufficient\nconditions under which the allocation is both fair and core-stable are\nprovided. In scenarios where the Shapley value falls outside the core, we\ndevelop an alternative allocation based on the stable payoff that minimizes the\nmean relative deviation from the Shapley value while preserving core stability.\nThis deviation is further proved to be upper-bounded by $1$, showing a\nfavorable trade-off between stability and fairness. Finally, extensive\nnumerical studies validate the theoretical results and demonstrate the\neffectiveness of the proposed framework in facilitating stable, equitable, and\nsustainable cooperation in mixed-energy truck platooning.", "comment": "16 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16923v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "混合能源卡车队列中稳定公平的利益分配：一种联盟博弈方法", "tldr": "本论文研究了混合能源卡车队列中的利益分配问题，采用联盟博弈方法设计了核心稳定的分配方案和基于Shapley值的公平分配方案，并提出了一种在Shapley值不在核心时兼顾稳定性和公平性的替代方案，通过数值研究验证了其有效性。", "motivation": "本研究旨在解决由燃油卡车和电动卡车组成的混合能源卡车队列中的利益分配问题，以促进稳定、公平和可持续的合作。", "method": "本研究将卡车在队列形成过程中的互动建模为可转移效用的联盟博弈。首先设计了一个考虑卡车异质性和队列角色（领导者或跟随者）的稳定收益分配方案，并建立了核心稳定性条件。为了提高收益公平性，提出了一个基于Shapley值的封闭式分配方法。当Shapley值不在核心时，开发了一种基于稳定收益的替代分配方案，该方案在保持核心稳定性的同时，最小化了与Shapley值的平均相对偏差。", "result": "研究设计了一个考虑卡车异质性的稳定收益分配方案，并建立了核心稳定性条件。提出了计算高效且与队列规模无关的基于Shapley值的公平分配方法。提供了分配既公平又核心稳定的充分条件。证明了替代分配方案与Shapley值的偏差上限为1，显示了稳定性和公平性之间的良好权衡。广泛的数值研究验证了理论结果，并证明了所提出框架在促进混合能源卡车队列中稳定、公平和可持续合作方面的有效性。", "conclusion": "本研究提出的框架能够有效促进混合能源卡车队列中的稳定、公平和可持续合作，为异构卡车队列的利益分配提供了理论和实践指导。", "translation": "本论文解决了由燃油卡车和电动卡车组成的混合能源卡车队列中的利益分配问题。将卡车在队列形成过程中的互动建模为可转移效用的联盟博弈。我们首先设计了一个稳定的收益分配方案，该方案考虑了卡车在节能和队列角色（领导者或跟随者）方面的异质性，建立了核心稳定性条件，以确保没有卡车子集有偏离以获取更大收益的动机。为了提高收益公平性，我们提出了一个封闭式、基于Shapley值的分配方法，该方法计算高效且与队列规模无关。提供了分配既公平又核心稳定的充分条件。在Shapley值超出核心的情况下，我们开发了一种基于稳定收益的替代分配方案，该方案在保持核心稳定性的同时，最小化了与Shapley值的平均相对偏差。这种偏差进一步被证明上限为1，显示了稳定性和公平性之间的有利权衡。最后，广泛的数值研究验证了理论结果，并证明了所提出框架在促进混合能源卡车队列中稳定、公平和可持续合作方面的有效性。", "summary": "本研究利用联盟博弈方法，针对混合能源卡车队列中的利益分配问题，提出了两种关键分配方案。首先，设计了一个考虑卡车异质性和队列角色的核心稳定收益分配方案。其次，为了增强公平性，引入了一个基于Shapley值的计算高效分配方法。对于Shapley值不在核心的情况，论文还开发了一种替代方案，该方案在保持核心稳定性的同时，最小化了与Shapley值的偏差，并证明了其上限为1，体现了稳定性和公平性之间的权衡。数值研究证实了该框架在促进混合能源卡车队列中稳定、公平和可持续合作方面的有效性。", "keywords": "混合能源卡车队列, 利益分配, 联盟博弈, Shapley值, 核心稳定性", "comments": "该论文的创新之处在于将联盟博弈理论应用于混合能源卡车队列的利益分配，并同时考虑了稳定性与公平性。特别是在Shapley值不在核心时提出的替代分配方案，展示了在实际应用中平衡理论最优解与实践可行性的思路，具有重要的理论和实际意义。"}}
{"id": "2410.22365", "title": "Vascular Segmentation of Functional Ultrasound Images using Deep Learning", "authors": ["Hana Sebia", "Thomas Guyet", "Mickaël Pereira", "Marco Valdebenito", "Hugues Berry", "Benjamin Vidal"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.22365v2", "summary": "Segmentation of medical images is a fundamental task with numerous\napplications. While MRI, CT, and PET modalities have significantly benefited\nfrom deep learning segmentation techniques, more recent modalities, like\nfunctional ultrasound (fUS), have seen limited progress. fUS is a non invasive\nimaging method that measures changes in cerebral blood volume (CBV) with high\nspatio-temporal resolution. However, distinguishing arterioles from venules in\nfUS is challenging due to opposing blood flow directions within the same pixel.\nUltrasound localization microscopy (ULM) can enhance resolution by tracking\nmicrobubble contrast agents but is invasive, and lacks dynamic CBV\nquantification. In this paper, we introduce the first deep learning-based\nsegmentation tool for fUS images, capable of differentiating signals from\ndifferent vascular compartments, based on ULM automatic annotation and enabling\ndynamic CBV quantification. We evaluate various UNet architectures on fUS\nimages of rat brains, achieving competitive segmentation performance, with 90%\naccuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames\nfrom a fUS stack. These results are comparable to those from tubular structure\nsegmentation in other imaging modalities. Additionally, models trained on\nresting-state data generalize well to images captured during visual\nstimulation, highlighting robustness. This work offers a non-invasive,\ncost-effective alternative to ULM, enhancing fUS data interpretation and\nimproving understanding of vessel function. Our pipeline shows high linear\ncorrelation coefficients between signals from predicted and actual compartments\nin both cortical and deeper regions, showcasing its ability to accurately\ncapture blood flow dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.22365v2", "cate": "eess.IV", "date": "2024-10-28", "updated": "2025-07-23", "AI": {"title_translation": "基于深度学习的功能超声图像血管分割", "tldr": "本文首次提出并开发了一种基于深度学习的功能超声（fUS）图像血管分割工具，能够区分不同血管区室的信号，实现动态脑血容量（CBV）量化。该方法是非侵入性且经济高效的，在鼠脑fUS图像上达到了90%的准确率，并能很好地推广到不同状态下的数据，为血管功能研究提供了新途径。", "motivation": "医学图像分割是一个基础性任务，但在功能超声（fUS）等新型模态中的进展有限。fUS虽然是非侵入性且具有高时空分辨率，但难以区分动脉和静脉。超声定位显微镜（ULM）可提高分辨率但具侵入性且缺乏动态脑血容量（CBV）量化能力。本研究旨在开发一种非侵入性、经济高效的fUS血管分割方法，以克服现有技术的局限性。", "method": "本文引入了首个基于深度学习的功能超声图像分割工具，该工具能够区分不同血管区室的信号。该方法基于ULM自动标注，并支持动态CBV量化。研究评估了多种UNet架构在鼠脑fUS图像上的性能，仅使用100帧时间数据进行训练和测试。", "result": "在鼠脑fUS图像上实现了有竞争力的分割性能，准确率达到90%，F1分数为71%，IoU为0.59。这些结果与在其他成像模态中进行的管状结构分割性能相当。此外，在静息状态数据上训练的模型能够很好地泛化到视觉刺激期间捕获的图像，显示出良好的鲁棒性。该管道在皮层和深层区域的预测和实际区室信号之间显示出高线性相关系数。", "conclusion": "这项工作为ULM提供了一种非侵入性、经济高效的替代方案，增强了fUS数据解释并改善了对血管功能的理解。所开发的管道能够准确捕获血流动力学。", "translation": "医学图像分割是具有众多应用的基础任务。虽然MRI、CT和PET等模态已从深度学习分割技术中显著受益，但功能超声（fUS）等更近期的模态进展有限。fUS是一种非侵入性成像方法，能以高时空分辨率测量脑血容量（CBV）的变化。然而，由于同一像素内血流方向相反，在fUS中区分小动脉和小静脉具有挑战性。超声定位显微镜（ULM）可以通过跟踪微泡造影剂来提高分辨率，但它是侵入性的，并且缺乏动态CBV量化能力。在本文中，我们引入了第一个基于深度学习的fUS图像分割工具，该工具能够根据ULM自动标注区分来自不同血管区室的信号，并实现动态CBV量化。我们在鼠脑的fUS图像上评估了各种UNet架构，仅使用fUS堆栈中的100个时间帧就达到了有竞争力的分割性能，准确率达到90%，F1分数为71%，IoU为0.59。这些结果与在其他成像模态中进行的管状结构分割结果相当。此外，在静息状态数据上训练的模型能够很好地泛化到视觉刺激期间捕获的图像，突出了其鲁棒性。这项工作为ULM提供了一种非侵入性、经济高效的替代方案，增强了fUS数据解释并改善了对血管功能的理解。我们的管道在皮层和深层区域的预测和实际区室信号之间显示出高线性相关系数，展示了其准确捕获血流动力学的能力。", "summary": "本研究首次提出了一种基于深度学习的血管分割工具，用于功能超声（fUS）图像，旨在克服fUS在区分血管区室方面的挑战以及超声定位显微镜（ULM）的侵入性限制。该工具利用UNet架构，并基于ULM自动标注，实现了对不同血管区室信号的区分和动态脑血容量（CBV）的量化。在鼠脑fUS图像上的评估显示，该方法仅用100帧数据就达到了90%的准确率、71%的F1分数和0.59的IoU，性能与传统管状结构分割相当。模型在不同生理状态下表现出良好的泛化能力。这项工作为fUS数据解释提供了一种非侵入性、经济高效的替代方案，并有助于深入理解血管功能和血流动力学。", "keywords": "深度学习, 功能超声, 血管分割, UNet, 脑血容量", "comments": "该论文的创新之处在于首次将深度学习应用于功能超声（fUS）图像的血管分割，特别是在区分不同血管区室方面。它提供了一种非侵入性、经济高效的替代方案，以解决现有超声定位显微镜（ULM）的侵入性问题和fUS在血管区分上的挑战。该方法能够实现动态脑血容量（CBV）量化，并展示了在有限数据量下的高精度和鲁棒性，对于神经血管耦合和脑功能研究具有重要意义。"}}
{"id": "2507.17686", "title": "Debiased maximum-likelihood estimators for hazard ratios under machine-learning adjustment", "authors": ["Takashi Hayakawa", "Satoshi Asai"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17686v1", "summary": "Previous studies have shown that hazard ratios between treatment groups\nestimated with the Cox model are uninterpretable because the indefinite\nbaseline hazard of the model fails to identify temporal change in the risk set\ncomposition due to treatment assignment and unobserved factors among multiple,\ncontradictory scenarios. To alleviate this problem, especially in studies based\non observational data with uncontrolled dynamic treatment and real-time\nmeasurement of many covariates, we propose abandoning the baseline hazard and\nusing machine learning to explicitly model the change in the risk set with or\nwithout latent variables. For this framework, we clarify the context in which\nhazard ratios can be causally interpreted, and then develop a method based on\nNeyman orthogonality to compute debiased maximum-likelihood estimators of\nhazard ratios. Computing the constructed estimators is more efficient than\ncomputing those based on weighted regression with marginal structural Cox\nmodels. Numerical simulations confirm that the proposed method identifies the\nground truth with minimal bias. These results lay the foundation for developing\na useful, alternative method for causal inference with uncontrolled,\nobservational data in modern epidemiology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17686v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "机器学习调整下风险比的去偏最大似然估计", "tldr": "本文提出了一种基于机器学习和Neyman正交性的方法，用于计算去偏的风险比最大似然估计，解决了Cox模型在观测数据中风险比难以解释的问题，并显示出更高的效率和更小的偏差。", "motivation": "以往研究表明，Cox模型估计的处理组间风险比难以解释，因为其不确定的基线风险未能识别风险集中组成的时间变化。为了解决这个问题，尤其是在具有不受控动态治疗和多协变量实时测量的观测数据研究中，本文提出了新的方法。", "method": "本文提出放弃基线风险，并使用机器学习明确建模风险集的变化（有或没有潜在变量）。在此框架下，阐明了风险比可以进行因果解释的情境，然后开发了一种基于Neyman正交性的方法来计算风险比的去偏最大似然估计。", "result": "计算所构建的估计量比基于边际结构Cox模型的加权回归计算的估计量更高效。数值模拟证实，所提出的方法以最小的偏差识别了真实值。", "conclusion": "这些结果为现代流行病学中利用不受控观测数据进行因果推断提供了一种有用且替代的方法奠定了基础。", "translation": "以前的研究表明，用Cox模型估计的处理组之间的风险比是不可解释的，因为模型不确定的基线风险未能识别由于治疗分配和多重矛盾情景中未观察到的因素导致的风险集组成的时间变化。为了缓解这个问题，特别是在基于具有不受控动态治疗和许多协变量实时测量的观测数据的研究中，我们建议放弃基线风险，并使用机器学习明确地建模风险集的变化，无论是否有潜在变量。对于这个框架，我们阐明了风险比可以进行因果解释的上下文，然后开发了一种基于Neyman正交性的方法来计算风险比的去偏最大似然估计量。计算所构建的估计量比计算基于边际结构Cox模型的加权回归的估计量更高效。数值模拟证实，所提出的方法以最小的偏差识别了真实值。这些结果为在现代流行病学中利用不受控的观测数据开发一种有用、替代的因果推断方法奠定了基础。", "summary": "本研究旨在解决Cox模型在观测数据中估计风险比时存在的可解释性问题，该问题源于不确定的基线风险未能捕捉风险集组成的时间变化。作者提出了一种新颖的方法，放弃传统的基线风险，转而利用机器学习技术显式地建模风险集的变化。在此基础上，他们开发了一种基于Neyman正交性的方法，用于计算风险比的去偏最大似然估计量。数值模拟结果表明，该方法比传统的加权回归方法更高效，且能以最小的偏差识别真实值。这项工作为在现代流行病学中处理不受控观测数据时的因果推断提供了一个有前景的替代方案。", "keywords": "风险比, 机器学习, 去偏估计, 因果推断, 观测数据", "comments": "本文创新性地将机器学习与因果推断相结合，通过放弃Cox模型的基线风险假设并引入Neyman正交性，有效解决了传统风险比估计在复杂观测数据下可解释性差和偏差大的问题。其提出的去偏最大似然估计量在效率和准确性上均优于现有方法，对于现代流行病学中利用真实世界数据进行因果分析具有重要意义。该研究为处理动态治疗和高维协变量的观测数据提供了新的视角和工具。"}}
{"id": "2108.02283", "title": "Machine Learning Classification and Portfolio Allocation: with Implications from Machine Uncertainty", "authors": ["Yang Bai", "Kuntara Pukthuanthong"], "categories": ["q-fin.GN", "cs.LG", "econ.GN", "q-fin.CP", "q-fin.EC", "q-fin.PM"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2108.02283v2", "summary": "We use multi-class machine learning classifiers to identify the stocks that\noutperform or underperform other stocks. The resulting long-short portfolios\nachieve annual Sharpe ratios of 1.67 (value-weighted) and 3.35\n(equal-weighted), with annual alphas ranging from 29\\% to 48\\%. These results\npersist after controlling for machine learning regressions and remain robust\namong large-cap stocks. Machine uncertainty, as measured by predicted\nprobabilities, impairs the prediction performance. Stocks with higher machine\nuncertainty experience lower returns, particularly when human proxies of\ninformation uncertainty align with machine uncertainty. Consistent with the\nliterature, such an effect is driven by the past underperformers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2108.02283v2", "cate": "q-fin.GN", "date": "2021-08-04", "updated": "2025-07-23", "AI": {"title_translation": "机器学习分类与投资组合配置：机器学习不确定性的启示", "tldr": "本文利用多类别机器学习分类器识别股票表现，构建的多空投资组合表现出色，且发现机器学习不确定性会损害预测性能并导致股票回报率降低。", "motivation": "本文旨在利用机器学习分类器识别表现优异或不差的股票，并探讨机器学习不确定性对预测性能和股票回报的影响。", "method": "研究使用多类别机器学习分类器来识别跑赢或跑输其他股票的股票。然后，根据分类结果构建多空投资组合。此外，研究还通过预测概率来衡量机器学习不确定性，并分析其对预测性能和股票回报的影响。", "result": "构建的多空投资组合实现了1.67（市值加权）和3.35（等权重）的年夏普比率，年化阿尔法介于29%至48%之间。这些结果在控制了机器学习回归后依然存在，并且在大盘股中保持稳健。机器学习不确定性（通过预测概率衡量）损害了预测性能。具有更高机器学习不确定性的股票回报率较低，尤其当人类信息不确定性代理与机器学习不确定性一致时。这种效应是由过去的表现不佳者驱动的。", "conclusion": "机器学习分类器能够有效识别股票表现并构建高回报的投资组合。然而，机器学习不确定性是一个重要的因素，它会负面影响预测性能和股票回报，尤其与人类信息不确定性一致时。", "translation": "我们使用多类别机器学习分类器来识别跑赢或跑输其他股票的股票。由此产生的多空投资组合实现了1.67（市值加权）和3.35（等权重）的年夏普比率，年化阿尔法介于29%至48%之间。这些结果在控制了机器学习回归后依然存在，并且在大盘股中保持稳健。机器学习不确定性（通过预测概率衡量）损害了预测性能。具有更高机器学习不确定性的股票回报率较低，尤其当人类信息不确定性代理与机器学习不确定性一致时。与文献一致，这种效应是由过去的表现不佳者驱动的。", "summary": "本文利用多类别机器学习分类器识别股票表现，并构建了高收益的多空投资组合，年夏普比率高达1.67至3.35，年化阿尔法在29%到48%之间，且在大盘股中保持稳健。研究还发现，机器学习不确定性会降低预测性能，并导致具有高不确定性的股票回报率下降，尤其当这种不确定性与人类信息不确定性一致时，这一现象主要由过去的表现不佳者引起。", "keywords": "机器学习分类, 投资组合配置, 股票预测, 机器学习不确定性, 阿尔法", "comments": "该研究创新性地将多类别机器学习分类器应用于股票选择和投资组合构建，并取得了显著的超额收益。其重要性在于揭示了“机器学习不确定性”这一新概念对预测性能和投资回报的负面影响，为未来机器学习在金融领域的应用提供了新的考量维度。论文还验证了这种效应与传统信息不确定性的一致性，并指出其驱动因素，具有较强的理论和实践意义。"}}
{"id": "2507.17518", "title": "Enabling Cyber Security Education through Digital Twins and Generative AI", "authors": ["Vita Santa Barletta", "Vito Bavaro", "Miriana Calvano", "Antonio Curci", "Antonio Piccinno", "Davide Pio Posa"], "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.HC", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17518v1", "summary": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability\nto replicate complex IT (Information Technology), OT (Operational Technology),\nand IoT (Internet of Things) infrastructures, allowing for real time\nmonitoring, threat analysis, and system simulation. This study investigates how\nintegrating DTs with penetration testing tools and Large Language Models (LLMs)\ncan enhance cybersecurity education and operational readiness. By simulating\nrealistic cyber environments, this approach offers a practical, interactive\nframework for exploring vulnerabilities and defensive strategies. At the core\nof this research is the Red Team Knife (RTK), a custom penetration testing\ntoolkit aligned with the Cyber Kill Chain model. RTK is designed to guide\nlearners through key phases of cyberattacks, including reconnaissance,\nexploitation, and response within a DT powered ecosystem. The incorporation of\nLarge Language Models (LLMs) further enriches the experience by providing\nintelligent, real-time feedback, natural language threat explanations, and\nadaptive learning support during training exercises. This combined DT LLM\nframework is currently being piloted in academic settings to develop hands on\nskills in vulnerability assessment, threat detection, and security operations.\nInitial findings suggest that the integration significantly improves the\neffectiveness and relevance of cybersecurity training, bridging the gap between\ntheoretical knowledge and real-world application. Ultimately, the research\ndemonstrates how DTs and LLMs together can transform cybersecurity education to\nmeet evolving industry demands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17518v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过数字孪生和生成式AI赋能网络安全教育", "tldr": "本研究探讨了如何通过结合数字孪生（DTs）、渗透测试工具和大型语言模型（LLMs）来增强网络安全教育和操作准备，并通过一个名为Red Team Knife (RTK)的定制工具包在学术环境中进行试点，初步结果显示其显著提高了网络安全培训的有效性和相关性。", "motivation": "当前网络安全教育存在理论知识与实际应用之间的差距，本研究旨在通过整合数字孪生和大型语言模型来提升网络安全教育的有效性和操作准备，以满足行业不断变化的需求。", "method": "本研究通过将数字孪生（DTs）与渗透测试工具和大型语言模型（LLMs）相结合，构建了一个用于网络安全教育的实用交互框架。核心是一个名为Red Team Knife (RTK)的定制渗透测试工具包，它与网络杀伤链模型对齐，指导学习者进行网络攻击的关键阶段。LLMs通过提供实时反馈、自然语言威胁解释和自适应学习支持来丰富体验。该DT-LLM框架正在学术环境中进行试点。", "result": "初步研究结果表明，这种集成显著提高了网络安全培训的有效性和相关性，弥合了理论知识与实际应用之间的差距。", "conclusion": "研究最终表明，数字孪生和大型语言模型相结合可以变革网络安全教育，以满足不断发展的行业需求。", "translation": "数字孪生（DTs）因其能够复制复杂的IT（信息技术）、OT（操作技术）和IoT（物联网）基础设施，实现实时监控、威胁分析和系统模拟，而在网络安全领域日益受到重视。本研究调查了如何将DTs与渗透测试工具和大型语言模型（LLMs）相结合，以增强网络安全教育和操作准备。通过模拟真实的网络环境，这种方法为探索漏洞和防御策略提供了一个实用、交互式的框架。本研究的核心是Red Team Knife (RTK)，一个与网络杀伤链模型对齐的定制渗透测试工具包。RTK旨在引导学习者通过网络攻击的关键阶段，包括侦察、利用和在DT驱动生态系统中的响应。大型语言模型（LLMs）的融入通过在培训练习中提供智能、实时反馈、自然语言威胁解释和自适应学习支持，进一步丰富了体验。这种结合DT和LLM的框架目前正在学术环境中进行试点，以培养漏洞评估、威胁检测和安全操作方面的动手技能。初步研究结果表明，这种集成显著提高了网络安全培训的有效性和相关性，弥合了理论知识与实际应用之间的差距。最终，该研究展示了DTs和LLMs如何共同变革网络安全教育，以满足不断发展的行业需求。", "summary": "本研究提出了一种利用数字孪生（DTs）、渗透测试工具和大型语言模型（LLMs）增强网络安全教育的新方法。通过构建名为Red Team Knife (RTK) 的定制渗透测试工具包，并在DT驱动的模拟环境中结合LLMs的智能反馈，该框架旨在提供一个实践性、互动性强的学习平台，以弥合理论与实践的差距。初步试点结果显示，该集成显著提升了网络安全培训的效果和实用性，有望变革未来的网络安全教育。", "keywords": "数字孪生, 生成式AI, 网络安全教育, 渗透测试, 大型语言模型", "comments": "该论文的创新之处在于将数字孪生技术与大型语言模型以及定制的渗透测试工具相结合，为网络安全教育提供了一个高度沉真和互动的平台。这种方法有效地弥合了理论知识与实际操作之间的鸿沟，对于培养学生的实战技能具有重要意义。通过模拟真实环境和提供智能反馈，它提升了学习效率和相关性，是网络安全教育领域的一个重要进展。"}}
{"id": "2507.17730", "title": "Online Submission and Evaluation System Design for Competition Operations", "authors": ["Zhe Chen", "Daniel Harabor", "Ryan Hechnenberger", "Nathan R. Sturtevant"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This work was presented at the Workshop on the International Planning Competition (WIPC 2024)", "url": "http://arxiv.org/abs/2507.17730v1", "summary": "Research communities have developed benchmark datasets across domains to\ncompare the performance of algorithms and techniques However, tracking the\nprogress in these research areas is not easy, as publications appear in\ndifferent venues at the same time, and many of them claim to represent the\nstate-of-the-art. To address this, research communities often organise periodic\ncompetitions to evaluate the performance of various algorithms and techniques,\nthereby tracking advancements in the field. However, these competitions pose a\nsignificant operational burden. The organisers must manage and evaluate a large\nvolume of submissions. Furthermore, participants typically develop their\nsolutions in diverse environments, leading to compatibility issues during the\nevaluation of their submissions. This paper presents an online competition\nsystem that automates the submission and evaluation process for a competition.\nThe competition system allows organisers to manage large numbers of submissions\nefficiently, utilising isolated environments to evaluate submissions. This\nsystem has already been used successfully for several competitions, including\nthe Grid-Based Pathfinding Competition and the League of Robot Runners\ncompetition.", "comment": "This work was presented at the Workshop on the International Planning\n  Competition (WIPC 2024)", "pdf_url": "http://arxiv.org/pdf/2507.17730v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "竞赛运营的在线提交与评估系统设计", "tldr": "本文提出了一种在线竞赛系统，能够自动化提交和评估过程，有效解决传统竞赛中组织者面临的运营负担和兼容性问题。", "motivation": "研究社区通过定期组织竞赛来追踪算法和技术的进展，但这些竞赛带来了巨大的运营负担，包括管理大量提交和处理参与者多样化开发环境导致的兼容性问题。", "method": "本文提出了一种在线竞赛系统，该系统自动化了竞赛的提交和评估过程。它允许组织者高效管理大量提交，并利用隔离环境来评估提交。", "result": "该系统已成功应用于多个竞赛，包括基于网格的寻路竞赛和机器人赛跑联盟竞赛。", "conclusion": "该在线竞赛系统成功解决了研究竞赛中的运营负担和兼容性问题，提高了效率和评估的准确性。", "translation": "研究社区在不同领域开发了基准数据集，用于比较算法和技术的性能。然而，追踪这些研究领域的进展并不容易，因为出版物同时出现在不同的场合，其中许多声称代表了最先进水平。为了解决这个问题，研究社区经常组织定期竞赛来评估各种算法和技术的性能，从而追踪该领域的进展。然而，这些竞赛带来了巨大的运营负担。组织者必须管理和评估大量的提交。此外，参与者通常在不同的环境中开发他们的解决方案，导致在评估提交时出现兼容性问题。本文提出了一种在线竞赛系统，该系统自动化了竞赛的提交和评估过程。该竞赛系统允许组织者高效管理大量提交，并利用隔离环境来评估提交。该系统已成功用于多个竞赛，包括基于网格的寻路竞赛和机器人赛跑联盟竞赛。", "summary": "本文介绍了一种在线竞赛系统，旨在解决研究竞赛中提交和评估过程的运营挑战。该系统通过自动化提交和评估流程，并利用隔离环境来处理多样化的解决方案，从而帮助组织者高效管理大量提交并解决兼容性问题。该系统已成功应用于多个实际竞赛。", "keywords": "在线竞赛系统, 自动化, 提交, 评估, 隔离环境", "comments": "这项工作提出了一种实用的在线竞赛系统，其创新之处在于自动化了提交和评估流程，并引入了隔离环境来解决兼容性问题，这对于大规模研究竞赛的组织具有重要意义，能够显著减轻组织者的负担并提高评估效率和准确性。"}}
{"id": "2507.17596", "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "authors": ["Maciej K. Wozniak", "Lianhang Liu", "Yixi Cai", "Patric Jensfelt"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2507.17596v1", "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.17596v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PRIX: 从原始像素学习规划用于端到端自动驾驶", "tldr": "PRIX是一种高效的端到端自动驾驶模型，仅使用摄像头数据，直接从原始像素规划安全轨迹，在性能上达到SOTA，同时模型更小、推理更快。", "motivation": "现有端到端自动驾驶模型存在模型尺寸大、依赖昂贵激光雷达传感器和计算密集型BEV特征表示的问题，这限制了其在大众市场车辆中的可扩展性。", "method": "提出PRIX（Plan from Raw Pixels），一种新颖高效的端到端驾驶架构。PRIX仅使用摄像头数据，无需显式BEV表示和激光雷达。它利用视觉特征提取器与生成式规划头相结合，直接从原始像素输入预测安全轨迹。核心组件是上下文感知重校准Transformer（CaRT），用于有效增强多级视觉特征以实现更鲁棒的规划。", "result": "PRIX在NavSim和nuScenes基准测试上实现了最先进的性能，与更大、多模态的扩散规划器能力相当，同时在推理速度和模型尺寸方面显著更高效。", "conclusion": "PRIX为真实世界部署提供了一个实用且高效的端到端自动驾驶解决方案。", "translation": "标题：PRIX：从原始像素学习规划用于端到端自动驾驶\n摘要：尽管端到端自动驾驶模型显示出有希望的结果，但其实际部署常常受到大模型尺寸、对昂贵激光雷达传感器和计算密集型BEV特征表示的依赖的阻碍。这限制了它们的可扩展性，特别是对于仅配备摄像头的量产车辆。为了解决这些挑战，我们提出了PRIX（Plan from Raw Pixels）。我们新颖高效的端到端驾驶架构仅使用摄像头数据运行，无需显式BEV表示，并且放弃了对激光雷达的需求。PRIX利用视觉特征提取器与生成式规划头相结合，直接从原始像素输入预测安全轨迹。我们架构的核心组件是上下文感知重校准Transformer（CaRT），这是一个旨在有效增强多级视觉特征以实现更鲁棒规划的新颖模块。我们通过全面的实验证明，PRIX在NavSim和nuScenes基准测试上实现了最先进的性能，与更大、多模态的扩散规划器能力相当，同时在推理速度和模型尺寸方面显著更高效，使其成为真实世界部署的实用解决方案。我们的工作是开源的，代码将在https://maxiuw.github.io/prix。", "summary": "PRIX是一种创新的端到端自动驾驶架构，专门为仅配备摄像头的车辆设计。它通过直接从原始像素输入预测安全轨迹，避免了对激光雷达和计算密集型BEV表示的依赖。该模型的核心是上下文感知重校准Transformer（CaRT），用于增强视觉特征。PRIX在主流基准测试中表现出色，同时在模型大小和推理速度上显著优于现有模型，是实现大规模自动驾驶的实用方案。", "keywords": "自动驾驶, 端到端, 原始像素, 纯视觉, PRIX", "comments": "PRIX的创新之处在于其端到端、纯视觉的解决方案，直接从原始像素进行规划，显著降低了对昂贵传感器的依赖和计算复杂度。这对于自动驾驶技术的大规模商业化部署具有重要意义。CaRT模块的引入也提升了视觉特征处理的鲁棒性。"}}
{"id": "2507.17540", "title": "Clustering-based hard negative sampling for supervised contrastive speaker verification", "authors": ["Piotr Masztalski", "Michał Romaniuk", "Jakub Żak", "Mateusz Matuszewski", "Konrad Kowalczyk"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to INTERSPEECH 2025", "url": "http://arxiv.org/abs/2507.17540v1", "summary": "In speaker verification, contrastive learning is gaining popularity as an\nalternative to the traditionally used classification-based approaches.\nContrastive methods can benefit from an effective use of hard negative pairs,\nwhich are different-class samples particularly challenging for a verification\nmodel due to their similarity. In this paper, we propose CHNS - a\nclustering-based hard negative sampling method, dedicated for supervised\ncontrastive speaker representation learning. Our approach clusters embeddings\nof similar speakers, and adjusts batch composition to obtain an optimal ratio\nof hard and easy negatives during contrastive loss calculation. Experimental\nevaluation shows that CHNS outperforms a baseline supervised contrastive\napproach with and without loss-based hard negative sampling, as well as a\nstate-of-the-art classification-based approach to speaker verification by as\nmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using two\nlightweight model architectures.", "comment": "Accepted to INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.17540v1", "cate": "eess.AS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于聚类的硬负样本采样用于有监督对比说话人确认", "tldr": "本文提出了一种名为CHNS的基于聚类的硬负样本采样方法，用于有监督对比说话人表示学习，该方法通过调整批次组成来优化硬负样本和易负样本的比例，并在说话人确认任务中取得了显著优于基线和最先进方法的性能。", "motivation": "在说话人确认领域，对比学习作为传统分类方法的替代方案越来越受欢迎。对比方法可以通过有效利用硬负样本（即对验证模型而言特别具有挑战性的不同类别但相似的样本）来提升性能。", "method": "本文提出了一种名为CHNS的基于聚类的硬负样本采样方法，专门用于有监督对比说话人表示学习。该方法将相似说话人的嵌入进行聚类，并调整批次组成，以在对比损失计算期间获得最佳的硬负样本和易负样本比例。", "result": "实验评估表明，CHNS在VoxCeleb数据集上，使用两种轻量级模型架构，在相对EER和minDCF方面，比有无基于损失的硬负样本采样的基线有监督对比方法，以及最先进的基于分类的说话人确认方法，性能提升高达18%。", "conclusion": "CHNS方法通过优化硬负样本采样，显著提升了有监督对比说话人确认的性能，超越了现有基线和最先进的分类方法。", "translation": "在说话人确认领域，对比学习作为传统分类方法的替代方案越来越受欢迎。对比方法可以从有效利用硬负样本中受益，这些硬负样本是不同类别但对验证模型而言由于其相似性而特别具有挑战性的样本。在本文中，我们提出了CHNS——一种基于聚类的硬负样本采样方法，专用于有监督对比说话人表示学习。我们的方法将相似说话人的嵌入进行聚类，并调整批次组成，以在对比损失计算期间获得最佳的硬负样本和易负样本比例。实验评估表明，CHNS在VoxCeleb数据集上，使用两种轻量级模型架构，在相对EER和minDCF方面，比有无基于损失的硬负样本采样的基线有监督对比方法，以及最先进的基于分类的说话人确认方法，性能提升高达18%。", "summary": "本文提出了一种名为CHNS的创新性基于聚类的硬负样本采样方法，专为有监督对比说话人表示学习设计。该方法通过对相似说话人嵌入进行聚类，并智能调整批次内硬负样本和易负样本的比例，以优化对比损失计算。实验结果表明，CHNS在VoxCeleb数据集上，显著优于基线有监督对比方法以及最先进的分类方法，在EER和minDCF上实现了高达18%的相对性能提升。", "keywords": "说话人确认, 对比学习, 硬负样本采样, 聚类, CHNS", "comments": "该论文的创新点在于提出了一个基于聚类的硬负样本采样策略，有效解决了对比学习中硬负样本选择的难题。通过优化批次组成来平衡硬负样本和易负样本的比例，提升了模型学习判别性表示的能力。其在轻量级模型上取得的显著性能提升，也突显了该方法在实际应用中的潜力。"}}
{"id": "2507.17659", "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering", "authors": ["Junjie Wang", "Yunhan Tang", "Yijie Wang", "Zhihao Yuan", "Huan Wang", "Yangfan He", "Bin Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17659v1", "summary": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17659v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "见林见木：一种基于知识的视觉问答协同推理框架", "tldr": "多模态大型语言模型（MLLMs）在基于知识的视觉问答（KBVQA）中存在单维证据依赖的瓶颈。Synergos-VQA提出了一种协同推理框架，融合整体、结构和因果证据，实现了最先进的性能，并能有效提升现有MLLMs。", "motivation": "多模态大型语言模型（MLLMs）在基于知识的视觉问答（KBVQA）中存在推理瓶颈，即过度依赖单维证据，导致无法实现鲁棒、多方面的理解，如同“只见树木，不见森林”。", "method": "本文提出了Synergos-VQA，一个新颖的协同推理框架。该框架在推理时同时生成并融合三个互补的证据流：(1) 整体证据，用于感知整个场景（“森林”）；(2) 来自原型驱动模块的结构证据，用于识别关键对象（“树木”）；(3) 来自反事实探针的因果证据，以确保推理的鲁棒性。通过协同融合这些多方面证据，实现更全面可靠的推理。", "result": "Synergos-VQA在包括OK-VQA和A-OKVQA在内的三个具有挑战性的基准测试中取得了新的最先进（SOTA）性能。此外，该方法展示了强大的即插即用能力，显著提升了各种开源MLLMs的性能，并证明了卓越的方法设计可以超越单纯的模型规模。", "conclusion": "Synergos-VQA通过协同融合多方面证据，实现了更全面、更可靠的推理过程，成功解决了现有MLLMs在KBVQA中单维证据依赖的局限性。其卓越的性能证明了方法设计的重要性，甚至可以超越模型规模的单纯扩大。", "translation": "多模态大型语言模型（MLLMs）推动了基于知识的视觉问答（KBVQA）的前沿发展，但它们的推理根本上受限于对单维证据的依赖。这种“只见树木，不见森林”的方法阻碍了鲁棒、多方面的理解。受“见林见木”原则的启发，我们提出了Synergos-VQA，一种新颖的协同推理框架。Synergos-VQA的核心是在推理时同时生成并融合三个互补的证据流：（1）整体证据，用于感知整个场景（“森林”）；（2）来自原型驱动模块的结构证据，用于识别关键对象（“树木”）；以及（3）来自反事实探针的因果证据，以确保推理的鲁棒性。通过协同融合这种多方面证据，我们的框架实现了更全面、更可靠的推理过程。广泛的实验表明，Synergos-VQA在包括OK-VQA和A-OKVQA在内的三个具有挑战性的基准测试中果断地建立了新的最先进水平。此外，我们的方法展示了强大的即插即用能力，显著提升了各种开源MLLM的性能，并证明了卓越的方法设计可以超越单纯的模型规模。", "summary": "Synergos-VQA是一个新颖的协同推理框架，旨在解决多模态大型语言模型在基于知识的视觉问答中因依赖单维证据而导致的推理瓶颈。该框架通过同时生成并融合整体、结构和因果三种互补证据流，实现了更全面、更可靠的推理。实验证明，Synergos-VQA在多个基准测试中取得了最先进的性能，并能有效提升现有MLLM的表现，强调了方法设计的重要性，而非单纯的模型规模。", "keywords": "基于知识的视觉问答, 协同推理, 多模态大型语言模型, 证据融合, 最先进性能", "comments": "本文创新性地通过“见林见木”的理念，提出了一种协同推理框架Synergos-VQA，解决了多模态大型语言模型在KBVQA中单维证据依赖的局限性。其将整体、结构和因果三种不同视角的证据进行融合，为多模态AI中更鲁棒、可解释的推理提供了重要方向。尤其值得关注的是，该研究证明了优越的方法设计可以超越单纯的模型规模，这对于资源有限的研究者和实际应用具有重要启示意义。"}}
{"id": "2506.08735", "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba", "authors": ["Yuhang Wang", "Jun Li", "Zhijian Wu", "Jifeng Shen", "Jianhua Xu", "Wankou Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08735v3", "summary": "Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08735v3", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-23", "AI": {"title_translation": "InceptionMamba：一种高效的混合网络，结合大波段卷积和瓶颈Mamba", "tldr": "提出InceptionMamba，通过正交带卷积和瓶颈Mamba模块，解决了InceptionNeXt在空间依赖和全局上下文建模上的不足，实现了SOTA性能和高效率。", "motivation": "InceptionNeXt在图像分类和下游任务中表现出竞争力，但存在局限性：其基于并行一维条带卷积，导致捕获不同维度空间依赖的能力受限，且未能充分探索局部邻域的空间建模；此外，卷积操作固有的局部性限制不利于有效的全局上下文建模。", "method": "提出InceptionMamba网络架构，用正交带卷积取代传统的一维条带卷积以实现更紧密的空间建模；引入瓶颈Mamba模块实现全局上下文建模，促进增强的跨通道信息融合和扩大的感受野。", "result": "在分类和各种下游任务上的广泛评估表明，所提出的InceptionMamba在参数和计算效率方面表现出色，并实现了最先进的性能。", "conclusion": "InceptionMamba通过结合正交带卷积和瓶颈Mamba模块，有效克服了InceptionNeXt的局限性，并在多项任务上取得了最先进的性能和高效率。", "translation": "在卷积神经网络家族中，InceptionNeXt在图像分类和许多下游任务中表现出卓越的竞争力。然而，它基于并行一维条带卷积，因此在捕获不同维度空间依赖的能力上受限，未能充分探索局部邻域的空间建模。此外，卷积操作固有的局部性限制不利于有效的全局上下文建模。为了克服这些局限性，本研究提出了一种新颖的骨干网络架构，命名为InceptionMamba。更具体地说，InceptionMamba中传统的二维条带卷积被正交带卷积取代，以实现更紧密的空间建模。此外，通过瓶颈Mamba模块可以实现全局上下文建模，促进增强的跨通道信息融合和扩大的感受野。在分类和各种下游任务上的广泛评估表明，所提出的InceptionMamba以卓越的参数和计算效率实现了最先进的性能。源代码将在https://github.com/Wake1021/InceptionMamba 提供。", "summary": "本文提出了InceptionMamba，一种高效的混合网络，旨在克服InceptionNeXt在空间依赖和全局上下文建模方面的局限性。它用正交带卷积取代了一维条带卷积以增强空间建模，并引入瓶颈Mamba模块实现全局上下文建模和跨通道信息融合。实验证明，InceptionMamba在多项任务上实现了最先进的性能，并具有优越的参数和计算效率。", "keywords": "InceptionMamba, 混合网络, 卷积神经网络, Mamba, 图像分类", "comments": "本文的创新点在于结合了卷积和Mamba架构的优势，通过正交带卷积改进局部空间建模，并通过瓶颈Mamba模块实现高效的全局上下文建模，有效克服了传统卷积网络的局部性限制，为高效且高性能的视觉骨干网络提供了新思路。"}}
{"id": "2507.17183", "title": "Regret Minimization in Population Network Games: Vanishing Heterogeneity and Convergence to Equilibria", "authors": ["Die Hu", "Shuyue Hu", "Chunjiang Mu", "Shiqi Fan", "Chen Chu", "Jinzhuo Liu", "Zhen Wang"], "categories": ["cs.GT", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17183v1", "summary": "Understanding and predicting the behavior of large-scale multi-agents in\ngames remains a fundamental challenge in multi-agent systems. This paper\nexamines the role of heterogeneity in equilibrium formation by analyzing how\nsmooth regret-matching drives a large number of heterogeneous agents with\ndiverse initial policies toward unified behavior. By modeling the system state\nas a probability distribution of regrets and analyzing its evolution through\nthe continuity equation, we uncover a key phenomenon in diverse multi-agent\nsettings: the variance of the regret distribution diminishes over time, leading\nto the disappearance of heterogeneity and the emergence of consensus among\nagents. This universal result enables us to prove convergence to quantal\nresponse equilibria in both competitive and cooperative multi-agent settings.\nOur work advances the theoretical understanding of multi-agent learning and\noffers a novel perspective on equilibrium selection in diverse game-theoretic\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17183v1", "cate": "cs.GT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "种群网络博弈中的遗憾最小化：异质性消失与均衡收敛", "tldr": "本文研究了在种群网络博弈中，平滑遗憾匹配如何导致异质代理的行为趋于一致，并证明了遗憾分布方差的减小会消除异质性，使代理达成共识并收敛到量化响应均衡。", "motivation": "理解和预测大规模多智能体在博弈中的行为仍然是多智能体系统中的一个基本挑战。", "method": "通过分析平滑遗憾匹配如何驱动大量异质代理趋向统一行为，并通过将系统状态建模为遗憾的概率分布并利用连续性方程分析其演变。", "result": "遗憾分布的方差随时间减小，导致异质性消失，代理之间出现共识。这一普遍结果使得在竞争和合作的多智能体环境中都能证明收敛到量化响应均衡。", "conclusion": "该工作推进了多智能体学习的理论理解，并为多样化博弈论场景中的均衡选择提供了新颖视角。", "translation": "理解和预测大规模多智能体在博弈中的行为仍然是多智能体系统中的一个基本挑战。本文通过分析平滑遗憾匹配如何驱动大量具有不同初始策略的异质代理趋向统一行为，来考察异质性在均衡形成中的作用。通过将系统状态建模为遗憾的概率分布并通过连续性方程分析其演变，我们揭示了在多样化多智能体设置中一个关键现象：遗憾分布的方差随时间减小，导致异质性消失并促使代理之间出现共识。这一普遍结果使我们能够在竞争和合作的多智能体环境中证明收敛到量化响应均衡。我们的工作推进了多智能体学习的理论理解，并为多样化博弈论场景中的均衡选择提供了新颖视角。", "summary": "本文探讨了大规模多智能体系统中异质性在均衡形成中的作用。研究发现，通过平滑遗憾匹配机制，异质代理的遗憾分布方差会随时间减少，从而消除异质性并促成共识。这一发现进一步证明了在竞争和合作环境中，系统能够收敛到量化响应均衡。该研究深化了对多智能体学习的理论认识，并为博弈论中的均衡选择提供了新颖的视角。", "keywords": "遗憾最小化, 多智能体系统, 异质性, 均衡, 共识", "comments": "该论文的创新之处在于揭示了在多智能体系统中，通过遗憾最小化机制，异质性会自然消失并最终形成共识，进而收敛到均衡。这种对遗憾分布演变的数学建模（通过连续性方程）提供了一个新颖且普遍的理论框架，对于理解复杂多智能体系统的动态行为及其均衡选择具有重要意义。"}}
{"id": "2507.17066", "title": "Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation", "authors": ["Jessup Byun", "Xiaofeng Lin", "Joshua Ward", "Guang Cheng"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by Agentic & GenAI Evaluation KDD2025, poster presentation", "url": "http://arxiv.org/abs/2507.17066v1", "summary": "Synthetic tabular data is essential for machine learning workflows,\nespecially for expanding small or imbalanced datasets and enabling\nprivacy-preserving data sharing. However, state-of-the-art generative models\n(GANs, VAEs, diffusion models) rely on large datasets with thousands of\nexamples. In low-data settings, often the primary motivation for synthetic\ndata, these models can overfit, leak sensitive records, and require frequent\nretraining. Recent work uses large pre-trained transformers to generate rows\nvia in-context learning (ICL), which needs only a few seed examples and no\nparameter updates, avoiding retraining. But ICL repeats seed rows verbatim,\nintroducing a new privacy risk that has only been studied in text. The severity\nof this risk in tabular synthesis-where a single row may identify a\nperson-remains unclear. We address this gap with the first benchmark of three\nfoundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four\nbaselines on 35 real-world tables from health, finance, and policy. We evaluate\nstatistical fidelity, downstream utility, and membership inference leakage.\nResults show foundation models consistently have the highest privacy risk.\nLLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at\n1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly\nvulnerable. We plot the privacy-utility frontier and show that CTGAN and\nGPT-4o-mini offer better tradeoffs. A factorial study finds that three\nzero-cost prompt tweaks-small batch size, low temperature, and using summary\nstatistics-can reduce worst-case AUC by 14 points and rare-class leakage by up\nto 39 points while maintaining over 90% fidelity. Our benchmark offers a\npractical guide for safer low-data synthesis with foundation models.", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025, poster presentation", "pdf_url": "http://arxiv.org/pdf/2507.17066v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "情境中的风险：基座模型在合成表格数据生成中隐私泄露的基准测试", "tldr": "本研究首次对基座模型在合成表格数据生成中的隐私泄露进行了基准测试。结果显示，基座模型（如LLaMA 3.3 70B、GPT-4o-mini、TabPFN v2）比传统基线模型具有更高的隐私风险。然而，通过零成本的提示词调整（小批量大小、低温度、使用统计摘要），可以显著降低隐私泄露，同时保持数据保真度。", "motivation": "合成表格数据对于机器学习工作流至关重要，尤其是在数据量小或不平衡时，以及在需要保护隐私的数据共享场景中。然而，现有的SOTA生成模型在低数据量环境下容易过拟合和泄露敏感记录。虽然上下文学习（ICL）的预训练Transformer模型避免了频繁再训练，但它们可能原样重复种子行，引入了新的隐私风险，这在文本领域有所研究，但在表格数据合成中（单行可能识别个人）的严重性尚不清楚，本研究旨在填补这一空白。", "method": "研究通过对三个基座模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与四个基线模型在来自健康、金融和政策领域的35个真实表格数据上进行基准测试来解决这一问题。评估指标包括统计保真度、下游效用和成员推理泄露。此外，还进行了一项因子研究，探索了三种零成本提示词调整（小批量大小、低温度、使用统计摘要）对隐私泄露的影响。", "result": "结果显示，基座模型始终具有最高的隐私风险。LLaMA 3.3 70B在1% FPR下的真阳性率比最安全的基线模型高出54个百分点。GPT-4o-mini和TabPFN也高度脆弱。研究绘制了隐私-效用边界，表明CTGAN和GPT-4o-mini提供了更好的权衡。因子研究发现，三种零成本的提示词调整可以将最差情况下的AUC降低14点，将稀有类别泄露降低39点，同时保持超过90%的保真度。", "conclusion": "本基准测试为使用基座模型进行更安全的低数据量合成提供了实用指南。", "translation": "合成表格数据对于机器学习工作流至关重要，尤其是在扩展小型或不平衡数据集以及实现隐私保护数据共享方面。然而，最先进的生成模型（GAN、VAE、扩散模型）依赖于包含数千个示例的大型数据集。在低数据设置下（通常是合成数据的主要动机），这些模型可能过拟合，泄露敏感记录，并需要频繁重新训练。最近的工作使用大型预训练Transformer通过上下文学习（ICL）生成行，这只需要少量种子示例且无需参数更新，从而避免了重新训练。但ICL会原样重复种子行，引入了新的隐私风险，这仅在文本领域进行了研究。这种风险在表格合成中（单行可能识别一个人）的严重性仍不清楚。我们通过首次对三个基座模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与四个基线模型在来自健康、金融和政策领域的35个真实表格上进行基准测试，解决了这一空白。我们评估了统计保真度、下游效用和成员推理泄露。结果显示，基座模型始终具有最高的隐私风险。LLaMA 3.3 70B在1% FPR下的真阳性率比最安全的基线模型高出54个百分点。GPT-4o-mini和TabPFN也高度脆弱。我们绘制了隐私-效用边界，并显示CTGAN和GPT-4o-mini提供了更好的权衡。一项因子研究发现，三种零成本的提示词调整——小批量大小、低温度和使用统计摘要——可以将最差情况下的AUC降低14点，将稀有类别泄露降低多达39点，同时保持超过90%的保真度。我们的基准测试为使用基座模型进行更安全的低数据量合成提供了实用指南。", "summary": "本研究首次对基座模型在合成表格数据生成中的隐私泄露进行了基准测试。尽管合成表格数据对机器学习和隐私保护至关重要，但现有模型在低数据量下容易泄露，而基于上下文学习的Transformer模型引入了新的、未在表格数据中充分研究的隐私风险。研究对比了GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2等基座模型与传统基线模型在35个真实数据集上的表现，评估了数据保真度、下游效用和成员推理泄露。结果表明，基座模型普遍存在更高的隐私风险，特别是LLaMA 3.3 70B。然而，研究发现通过调整提示词（如小批量、低温度、使用统计摘要）可以显著降低隐私泄露，同时保持数据质量。本工作为使用基座模型进行低数据量下的安全合成提供了实用指导。", "keywords": "基座模型, 隐私泄露, 合成数据, 表格数据, 上下文学习", "comments": "这项研究具有重要的创新性和实用价值。它是首次系统性地对基座模型在表格数据合成中隐私泄露进行的基准测试，填补了该领域的一个关键空白。研究不仅揭示了基座模型在低数据量场景下存在的显著隐私风险，还提出了通过简单的零成本提示词调整来有效缓解这些风险的实用方法，这对于实际应用中平衡隐私和数据效用具有指导意义。研究结果为在隐私敏感的表格数据合成中使用基座模型提供了宝贵的实践指南。"}}
{"id": "2507.17120", "title": "BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving", "authors": ["Wanyi Zheng", "Minxian Xu", "Shengye Song", "Kejiang Ye"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.17120v1", "summary": "Large language models (LLMs) have become increasingly popular in various\nareas, traditional business gradually shifting from rule-based systems to\nLLM-based solutions. However, the inference of LLMs is resource-intensive or\nlatency-sensitive, posing significant challenges for serving systems. Existing\nLLM serving systems often use static or continuous batching strategies, which\ncan lead to inefficient GPU memory utilization and increased latency,\nespecially under heterogeneous workloads. These methods may also struggle to\nadapt to dynamic workload fluctuations, resulting in suboptimal throughput and\npotential service level objective (SLO) violations. In this paper, we introduce\nBucketServe, a bucket-based dynamic batching framework designed to optimize LLM\ninference performance. By grouping requests into size-homogeneous buckets based\non sequence length, BucketServe minimizes padding overhead and optimizes GPU\nmemory usage through real-time batch size adjustments preventing out-of-memory\n(OOM) errors. It introduces adaptive bucket splitting/merging and\npriority-aware scheduling to mitigate resource fragmentation and ensure SLO\ncompliance. Experiment shows that BucketServe significantly outperforms UELLM\nin throughput, achieving up to 3.58x improvement. It can also handle 1.93x more\nrequest load under the SLO attainment of 80% compared with DistServe and\ndemonstrates 1.975x higher system load capacity compared to the UELLM.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.17120v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "BucketServe：基于桶的动态批处理，用于智能高效的LLM推理服务", "tldr": "BucketServe是一个基于桶的动态批处理框架，通过最小化填充开销和优化GPU内存使用来提高LLM推理性能，显著优于现有系统。", "motivation": "大型语言模型（LLMs）的推理资源密集且对延迟敏感，现有LLM服务系统（如静态或连续批处理）存在GPU内存利用率低下、延迟增加、难以适应动态工作负载波动等问题，导致吞吐量不佳和潜在的服务水平目标（SLO）违规。", "method": "引入BucketServe，一个基于桶的动态批处理框架。通过根据序列长度将请求分组到大小同质的桶中，最小化填充开销并优化GPU内存使用。它通过实时调整批处理大小防止OOM错误，并引入自适应桶分裂/合并和优先级感知调度来缓解资源碎片化并确保SLO合规性。", "result": "BucketServe在吞吐量方面显著优于UELLM，性能提升高达3.58倍。在80%的SLO达成率下，它可以处理比DistServe多1.93倍的请求负载，并且系统负载能力比UELLM高1.975倍。", "conclusion": "BucketServe通过其独特的桶式动态批处理、自适应桶管理和优先级感知调度机制，显著提升了LLM推理服务的效率和性能，有效解决了现有系统在异构和动态工作负载下的挑战，实现了更高的吞吐量和更强的负载处理能力，同时确保了服务质量。", "translation": "大型语言模型（LLMs）在各个领域越来越受欢迎，传统业务逐渐从基于规则的系统转向基于LLM的解决方案。然而，LLM的推理是资源密集型或延迟敏感的，这给服务系统带来了重大挑战。现有的LLM服务系统通常使用静态或连续批处理策略，这可能导致GPU内存利用率低下和延迟增加，尤其是在异构工作负载下。这些方法也可能难以适应动态工作负载波动，从而导致次优的吞吐量和潜在的服务水平目标（SLO）违规。在本文中，我们介绍了BucketServe，一个基于桶的动态批处理框架，旨在优化LLM推理性能。通过根据序列长度将请求分组到大小同质的桶中，BucketServe通过实时批处理大小调整最大限度地减少填充开销并优化GPU内存使用，从而防止内存不足（OOM）错误。它引入了自适应桶分裂/合并和优先级感知调度，以缓解资源碎片并确保SLO合规性。实验表明，BucketServe在吞吐量方面显著优于UELLM，实现了高达3.58倍的改进。与DistServe相比，在80%的SLO达成率下，它还可以处理多1.93倍的请求负载，并且与UELLM相比，系统负载能力高出1.975倍。", "summary": "本文介绍了BucketServe，一个为优化大型语言模型（LLM）推理性能而设计的基于桶的动态批处理框架。针对现有LLM服务系统在资源利用率、延迟和动态负载适应性方面的不足，BucketServe通过将请求按序列长度分组到同质桶中，最小化填充开销并优化GPU内存使用。它还引入了实时批处理大小调整、自适应桶分裂/合并以及优先级感知调度，以防止内存溢出、缓解资源碎片并确保服务水平目标。实验证明，BucketServe在吞吐量和请求负载处理能力上显著优于现有方案。", "keywords": "LLM推理, 动态批处理, GPU内存优化, 服务系统, 吞吐量", "comments": "BucketServe的创新之处在于其桶式动态批处理方法，它有效地解决了LLM推理中常见的填充开销和内存利用率问题。通过自适应桶管理和优先级调度，该系统在处理异构和动态工作负载方面表现出色，对提高LLM服务效率具有重要意义。"}}
{"id": "2507.17300", "title": "RLZ-r and LZ-End-r: Enhancing Move-r", "authors": ["Patrick Dinklage", "Johannes Fischer", "Lukas Nalbach", "Jan Zumbrink"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      full version of SPIRE 2025 conference paper", "url": "http://arxiv.org/abs/2507.17300v1", "summary": "In pattern matching on strings, a locate query asks for an enumeration of all\nthe occurrences of a given pattern in a given text. The r-index [Gagie et al.,\n2018] is a recently presented compressed self index that stores the text and\nauxiliary information in compressed space. With some modifications, locate\nqueries can be answered in optimal time [Nishimoto & Tabei, 2021], which has\nrecently been proven relevant in practice in the form of Move-r [Bertram et\nal., 2024]. However, there remains the practical bottleneck of evaluating\nfunction $\\Phi$ for every occurrence to report. This motivates enhancing the\nindex by a compressed representation of the suffix array featuring efficient\nrandom access, trading off space for faster answering of locate queries\n[Puglisi & Zhukova, 2021]. In this work, we build upon this idea considering\ntwo suitable compression schemes: Relative Lempel-Ziv [Kuruppu et al., 2010],\nimproving the work by Puglisi and Zhukova, and LZ-End [Kreft & Navarro, 2010],\nintroducing a different trade-off where compression is better than for Relative\nLempel-Ziv at the cost of slower access times. We enhance both the r-index and\nMove-r by the compressed suffix arrays and evaluate locate query performance in\nan experiment. We show that locate queries can be sped up considerably in both\nthe r-index and Move-r, especially if the queried pattern has many occurrences.\nThe choice between two different compression schemes offers new trade-offs\nregarding index size versus query performance.", "comment": "full version of SPIRE 2025 conference paper", "pdf_url": "http://arxiv.org/pdf/2507.17300v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "RLZ-r 和 LZ-End-r：增强 Move-r", "tldr": "本研究通过使用两种压缩方案（相对Lempel-Ziv和LZ-End）的压缩后缀数组来增强r-index和Move-r，以加速模式匹配中的定位查询。", "motivation": "Move-r在实践中存在一个瓶颈，即对每个匹配项都需要评估函数Φ，这导致定位查询速度较慢。因此，需要通过引入高效随机访问的压缩后缀数组来增强索引，以空间换取更快的查询速度。", "method": "本研究基于使用压缩后缀数组的思想，考虑了两种压缩方案：相对Lempel-Ziv（RLZ）和LZ-End。通过这两种方案，作者分别增强了r-index和Move-r，并进行了实验评估定位查询的性能。", "result": "实验表明，在r-index和Move-r中，定位查询的速度都可以显著提高，特别是当查询模式有大量匹配项时。两种不同的压缩方案选择提供了索引大小与查询性能之间的新权衡。", "conclusion": "通过将压缩后缀数组集成到r-index和Move-r中，可以显著加速定位查询，并且两种不同的压缩方案提供了在索引大小和查询性能之间进行权衡的新选择。", "translation": "在字符串的模式匹配中，定位查询要求列举给定模式在给定文本中的所有出现。r-index [Gagie et al., 2018] 是最近提出的一种压缩自索引，它以压缩空间存储文本和辅助信息。经过一些修改，定位查询可以以最优时间回答 [Nishimoto & Tabei, 2021]，这最近以 Move-r 的形式在实践中被证明是相关的 [Bertram et al., 2024]。然而，仍然存在一个实际瓶颈，即需要对报告的每个出现评估函数 Φ。这促使通过一种具有高效随机访问的后缀数组的压缩表示来增强索引，以空间换取更快的定位查询回答速度 [Puglisi & Zhukova, 2021]。在这项工作中，我们基于这一思想，考虑了两种合适的压缩方案：相对 Lempel-Ziv [Kuruppu et al., 2010]，改进了 Puglisi 和 Zhukova 的工作；以及 LZ-End [Kreft & Navarro, 2010]，引入了一种不同的权衡，其压缩效果优于相对 Lempel-Ziv，但访问时间较慢。我们通过压缩后缀数组增强了 r-index 和 Move-r，并在实验中评估了定位查询性能。我们表明，在 r-index 和 Move-r 中，定位查询都可以显著加速，特别是当查询模式有许多出现时。两种不同压缩方案的选择在索引大小与查询性能方面提供了新的权衡。", "summary": "本论文旨在解决 Move-r 中模式匹配定位查询的性能瓶颈。作者通过引入两种压缩方案（相对Lempel-Ziv和LZ-End）的压缩后缀数组来增强 r-index 和 Move-r。实验结果表明，这种方法显著加快了定位查询的速度，尤其对于多匹配模式。研究还揭示了不同压缩方案在索引大小和查询性能之间的新权衡，为实际应用提供了更多选择。", "keywords": "模式匹配, 压缩自索引, r-index, Move-r, 压缩后缀数组", "comments": "本文的创新之处在于将两种不同的压缩方案（相对Lempel-Ziv和LZ-End）应用于后缀数组的压缩表示，并将其集成到现有的 r-index 和 Move-r 框架中，以解决定位查询的性能瓶颈。通过实验验证了其有效性，并提供了关于空间与时间权衡的新视角，对于压缩自索引和模式匹配领域具有重要的实践意义。"}}
{"id": "2405.08427", "title": "Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline", "authors": ["Yuanchen Shi", "Biao Ma", "Longyin Zhang", "Fang Kong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2405.08427v2", "summary": "Stickers are increasingly used in social media to express sentiment and\nintent. Despite their significant impact on sentiment analysis and intent\nrecognition, little research has been conducted in this area. To address this\ngap, we propose a new task: \\textbf{M}ultimodal chat \\textbf{S}entiment\n\\textbf{A}nalysis and \\textbf{I}ntent \\textbf{R}ecognition involving\n\\textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal\ndataset containing Chinese chat records and stickers excerpted from several\nmainstream social media platforms. Our dataset includes paired data with the\nsame text but different stickers, the same sticker but different contexts, and\nvarious stickers consisting of the same images with different texts, allowing\nus to better understand the impact of stickers on chat sentiment and intent. We\nalso propose an effective multimodal joint model, MMSAIR, featuring\ndifferential vector construction and cascaded attention mechanisms for enhanced\nmultimodal fusion. Our experiments demonstrate the necessity and effectiveness\nof jointly modeling sentiment and intent, as they mutually reinforce each\nother's recognition accuracy. MMSAIR significantly outperforms traditional\nmodels and advanced MLLMs, demonstrating the challenge and uniqueness of\nsticker interpretation in social media. Our dataset and code are available on\nhttps://github.com/FakerBoom/MSAIRS-Dataset.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2405.08427v2", "cate": "cs.CL", "date": "2024-05-14", "updated": "2025-07-23", "AI": {"title_translation": "社交媒体中贴图对多模态情感和意图的影响：一项新任务、数据集和基线", "tldr": "鉴于社交媒体中贴图对情感和意图表达的显著影响但研究不足，本文提出了一项新任务MSAIRS、一个包含中文聊天记录和贴图的新颖多模态数据集，以及一个名为MMSAIR的有效多模态联合模型。实验证明联合建模情感和意图的必要性和有效性，MMSAIR显著优于现有模型。", "motivation": "尽管贴图在社交媒体中表达情感和意图方面具有显著影响，但该领域的研究较少，存在研究空白，因此需要深入探讨贴图对情感分析和意图识别的影响。", "method": "本文提出了一项名为MSAIRS（涉及贴图的多模态聊天情感分析和意图识别）的新任务。此外，构建了一个包含中文聊天记录和贴图的新颖多模态数据集，该数据集包含具有相同文本但不同贴图、相同贴图但不同上下文、以及相同图片但不同文本的各种贴图等配对数据。同时，提出了一种名为MMSAIR的有效多模态联合模型，该模型采用差分向量构建和级联注意力机制以增强多模态融合。", "result": "实验证明，联合建模情感和意图是必要且有效的，因为它们相互促进识别准确性。MMSAIR模型显著优于传统模型和先进的多模态大型语言模型（MLLMs），这表明了社交媒体中贴图解释的挑战性和独特性。", "conclusion": "联合建模情感和意图对于提高识别准确性至关重要。研究结果强调了社交媒体中贴图解释的复杂性和独特性，并证实了所提出模型在处理此类多模态数据方面的优越性。", "translation": "贴图在社交媒体中表达情感和意图方面被越来越多地使用。尽管它们对情感分析和意图识别有显著影响，但该领域的研究却很少。为了弥补这一空白，我们提出了一项新任务：涉及贴图的多模态聊天情感分析和意图识别（MSAIRS）。此外，我们引入了一个新颖的多模态数据集，其中包含从几个主流社交媒体平台摘录的中文聊天记录和贴图。我们的数据集包括具有相同文本但不同贴图、相同贴图但不同上下文、以及由相同图像但不同文本组成的各种贴图的配对数据，这使我们能够更好地理解贴图对聊天情感和意图的影响。我们还提出了一种有效的多模态联合模型MMSAIR，该模型具有差分向量构建和级联注意力机制，以增强多模态融合。我们的实验证明了联合建模情感和意图的必要性和有效性，因为它们相互促进识别准确性。MMSAIR显著优于传统模型和先进的多模态大型语言模型（MLLMs），这表明了社交媒体中贴图解释的挑战性和独特性。我们的数据集和代码可在https://github.com/FakerBoom/MSAIRS-Dataset上获取。", "summary": "本研究旨在解决社交媒体中贴图对多模态情感分析和意图识别影响的研究空白。为此，论文提出了一项新任务MSAIRS，并构建了一个包含中文聊天记录和贴图的新颖多模态数据集，该数据集特别设计用于捕捉贴图使用的细微差别。此外，本文还提出了一种名为MMSAIR的有效多模态联合模型，该模型利用差分向量构建和级联注意力机制来增强多模态融合。实验结果表明，联合建模情感和意图是必要且有效的，MMSAIR在性能上显著优于现有模型，凸显了贴图解释的复杂性。", "keywords": "贴图, 多模态情感分析, 意图识别, 社交媒体, 数据集", "comments": "本文通过关注社交媒体交流中一个未被充分研究但至关重要的方面——贴图的作用，做出了重要贡献。引入新任务（MSAIRS）和一个专门设计的多模态数据集（特别是其详细的数据配对，如相同文本不同贴图等）极具创新性，这使得能够更深入地理解贴图的影响。所提出的MMSAIR模型，侧重于联合建模和增强多模态融合，展示了解决这一复杂问题的强大方法。研究结果强调了在情感和意图分析中考虑非文本元素的重要性，推动了多模态研究的边界。"}}
{"id": "2507.14900", "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL main 2025", "url": "http://arxiv.org/abs/2507.14900v2", "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "comment": "ACL main 2025", "pdf_url": "http://arxiv.org/pdf/2507.14900v2", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-23", "AI": {"title_translation": "从神经元到语义：通过神经元对齐评估大型语言模型的跨语言对齐能力", "tldr": "本文提出了一种新颖的基于神经元状态的跨语言对齐（NeuronXA）方法，用于评估大型语言模型的跨语言对齐能力，即使在小数据集上也能有效评估，并与下游任务表现和可迁移性高度相关。", "motivation": "尽管大型语言模型（LLMs）展现出卓越的多语言能力，但如何有效评估其跨语言对齐能力仍未得到充分探索。现有对齐基准主要关注句子嵌入，但先前的研究表明，神经模型倾向于产生非平滑的表示空间，这会影响低资源语言的语义对齐评估。", "method": "受神经科学发现（相似信息激活重叠神经元区域）的启发，本文提出了一种新颖的基于神经元状态的跨语言对齐（NeuronXA）方法，以评估LLMs的跨语言对齐能力。该方法提供了一种更具语义基础的评估方法。研究在多种主流多语言LLM（LLaMA、Qwen、Mistral、GLM和OLMo）上，通过两项迁移任务和三个多语言基准对NeuronXA进行了评估。", "result": "研究结果表明，仅使用100对并行句子，NeuronXA与下游任务性能的皮尔逊相关系数达到0.9556，与可迁移性的皮尔逊相关系数达到0.8514。这些发现证明了NeuronXA在评估跨语言对齐和可迁移性方面的有效性，即使在小数据集上也能表现良好。", "conclusion": "NeuronXA是一种有效评估大型语言模型跨语言对齐和可迁移性的方法，即使使用少量数据也能实现高相关性。这突显了其在推进跨语言对齐研究和提高多语言LLMs语义理解方面的潜力。", "translation": "大型语言模型（LLMs）已经展示出卓越的多语言能力，然而，如何评估跨语言对齐仍然未被充分探索。现有对齐基准主要关注句子嵌入，但先前的研究表明，神经模型倾向于诱导非平滑的表示空间，这会影响低资源语言的语义对齐评估。受神经科学发现（相似信息激活重叠神经元区域）的启发，我们提出了一种新颖的基于神经元状态的跨语言对齐（NeuronXA）方法，以评估LLMs的跨语言对齐能力，它提供了一种更具语义基础的方法来评估跨语言对齐。我们在几种主流多语言LLM（LLaMA、Qwen、Mistral、GLM和OLMo）上，通过两项迁移任务和三个多语言基准对NeuronXA进行了评估。结果表明，仅使用100对并行句子，NeuronXA与下游任务性能的皮尔逊相关系数达到0.9556，与可迁移性的皮尔逊相关系数达到0.8514。这些发现证明了NeuronXA在评估跨语言对齐和可迁移性方面的有效性，即使在小数据集上也能表现良好。这突显了其在推进跨语言对齐研究和提高多语言LLMs语义理解方面的潜力。", "summary": "本文提出了一种名为NeuronXA的新型基于神经元状态的跨语言对齐方法，旨在解决现有大型语言模型跨语言对齐评估方法不足的问题，特别是针对非平滑表示空间和低资源语言的挑战。受神经科学启发，NeuronXA提供了一种更具语义基础的评估途径。实验证明，该方法仅需100对并行句子，即可与下游任务性能和可迁移性分别达到0.9556和0.8514的皮尔逊相关系数，显示出其在评估多语言LLMs跨语言对齐和可迁移性方面的有效性与高效性。", "keywords": "大型语言模型, 跨语言对齐, 神经元状态, 评估, 可迁移性", "comments": "本文的创新点在于提出了基于神经元状态的跨语言对齐评估方法NeuronXA，这为评估大型语言模型的跨语言能力提供了一个新颖且更具语义基础的视角。其重要性体现在仅需少量数据（100对并行句子）就能高效准确地评估LLMs的跨语言对齐和可迁移性，这对于低资源语言和大规模模型评估尤其有价值，有望推动跨语言对齐研究的进展。"}}
{"id": "2507.16842", "title": "Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning", "authors": ["Yinan Meng", "Kun Qian", "Jiong Yang", "Renbo Su", "Zhenhong Li", "Charlie C. L. Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16842v1", "summary": "The intrinsic compliance and high degree of freedom (DoF) of redundant soft\nmanipulators facilitate safe interaction and flexible task execution. However,\neffective kinematic control remains highly challenging, as it must handle\ndeformations caused by unknown external loads and avoid actuator saturation due\nto improper null-space regulation - particularly in confined environments. In\nthis paper, we propose a Sensor-Space Imitation Learning Kinematic Control\n(SS-ILKC) framework to enable robust kinematic control under actuator\nsaturation and restrictive environmental constraints. We employ a dual-learning\nstrategy: a multi-goal sensor-space control framework based on reinforcement\nlearning principle is trained in simulation to develop robust control policies\nfor open spaces, while a generative adversarial imitation learning approach\nenables effective policy learning from sparse expert demonstrations for\nconfined spaces. To enable zero-shot real-world deployment, a pre-processed\nsim-to-real transfer mechanism is proposed to mitigate the\nsimulation-to-reality gap and accurately characterize actuator saturation\nlimits. Experimental results demonstrate that our method can effectively\ncontrol a pneumatically actuated soft manipulator, achieving precise\npath-following and object manipulation in confined environments under unknown\nloading conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16842v1", "cate": "cs.RO", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "学习型冗余软机械臂传感器空间鲁棒运动学控制", "tldr": "提出SS-ILKC框架，利用双重学习策略和仿真到现实迁移，实现冗余软机械臂在未知载荷和受限环境下的鲁棒运动学控制。", "motivation": "冗余软机械臂的运动学控制面临挑战，需要处理未知外部载荷引起的变形以及不当零空间调节导致的执行器饱和，尤其是在受限环境中。", "method": "提出传感器空间模仿学习运动学控制 (SS-ILKC) 框架。采用双重学习策略：在仿真中基于强化学习训练多目标传感器空间控制策略以适应开放空间；利用生成对抗模仿学习从稀疏专家演示中有效学习受限空间的策略。为实现零样本真实世界部署，提出了预处理的仿真到现实迁移机制，以减小仿真与现实差距并准确表征执行器饱和限制。", "result": "实验结果表明，该方法能有效控制气动软机械臂，在未知载荷条件下受限环境中实现精确的路径跟踪和物体操作。", "conclusion": "该研究成功开发并验证了一种基于学习的传感器空间控制框架，实现了冗余软机械臂在复杂和受限条件下的鲁棒运动学控制。", "translation": "冗余软机械臂固有的柔顺性和高自由度（DoF）有助于实现安全交互和灵活的任务执行。然而，有效的运动学控制仍然极具挑战性，因为它必须处理未知外部载荷引起的变形，并避免由于不当的零空间调节导致的执行器饱和——尤其是在受限环境中。在本文中，我们提出了一种传感器空间模仿学习运动学控制（SS-ILKC）框架，以实现在执行器饱和和限制性环境约束下的鲁棒运动学控制。我们采用双重学习策略：在仿真中训练基于强化学习原理的多目标传感器空间控制框架，以开发开放空间的鲁棒控制策略；同时，生成对抗模仿学习方法能够从稀疏的专家演示中有效学习受限空间的策略。为了实现零样本的真实世界部署，提出了一种预处理的仿真到现实迁移机制，以减轻仿真到现实的差距并准确表征执行器饱和限制。实验结果表明，我们的方法可以有效地控制气动软机械臂，在未知载荷条件下受限环境中实现精确的路径跟踪和物体操作。", "summary": "本文提出一种名为SS-ILKC的传感器空间模仿学习运动学控制框架，旨在解决冗余软机械臂在未知载荷和受限环境下运动学控制的挑战。该框架结合了强化学习（用于开放空间）和生成对抗模仿学习（用于受限空间）的双重学习策略，并通过预处理的仿真到现实迁移机制，实现了零样本真实世界部署。实验证明，该方法能有效控制软机械臂在复杂条件下进行精确的路径跟踪和操作。", "keywords": "软机械臂, 运动学控制, 传感器空间, 模仿学习, 仿真到现实迁移", "comments": "该论文的创新点在于提出了一个集成了强化学习和模仿学习的双重学习框架，以应对软机械臂在开放和受限空间中的运动学控制挑战。特别值得注意的是，其引入的预处理仿真到现实迁移机制，有效弥补了仿真与现实之间的差距，为软机器人领域的零样本部署提供了有价值的解决方案。这对于软机器人在实际复杂环境中的应用具有重要意义。"}}
{"id": "2507.17174", "title": "GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP", "authors": ["Myeongwon Jung", "Takanori Fujiwara", "Jaemin Jo"], "categories": ["cs.GR", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17174v1", "summary": "Despite the widespread use of Uniform Manifold Approximation and Projection\n(UMAP), the impact of its stochastic optimization process on the results\nremains underexplored. We observed that it often produces unstable results\nwhere the projections of data points are determined mostly by chance rather\nthan reflecting neighboring structures. To address this limitation, we\nintroduce (r,d)-stability to UMAP: a framework that analyzes the stochastic\npositioning of data points in the projection space. To assess how stochastic\nelements, specifically initial projection positions and negative sampling,\nimpact UMAP results, we introduce \"ghosts\", or duplicates of data points\nrepresenting potential positional variations due to stochasticity. We define a\ndata point's projection as (r,d)-stable if its ghosts perturbed within a circle\nof radius r in the initial projection remain confined within a circle of radius\nd for their final positions. To efficiently compute the ghost projections, we\ndevelop an adaptive dropping scheme that reduces a runtime up to 60% compared\nto an unoptimized baseline while maintaining approximately 90% of unstable\npoints. We also present a visualization tool that supports the interactive\nexploration of the (r,d)-stability of data points. Finally, we demonstrate the\neffectiveness of our framework by examining the stability of projections of\nreal-world datasets and present usage guidelines for the effective use of our\nframework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17174v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "GhostUMAP2：测量和分析 UMAP 的 (r,d)-稳定性", "tldr": "本文引入了 (r,d)-稳定性框架和“幽灵”概念来分析和量化 UMAP 投影中数据点位置的随机不稳定性，并开发了高效的计算方法和可视化工具。", "motivation": "尽管 UMAP 被广泛使用，但其随机优化过程对结果的影响尚未得到充分探索。研究发现 UMAP 常常产生不稳定的结果，数据点的投影位置很大程度上是随机决定的，未能反映其邻近结构。", "method": "提出了 UMAP 的 (r,d)-稳定性框架，用于分析投影空间中数据点的随机定位。引入“幽灵”（数据点的副本）来表示由于随机性（初始投影位置和负采样）引起的潜在位置变异。定义数据点的投影为 (r,d)-稳定，如果其在初始投影中半径为 r 的圆内扰动的幽灵，在最终位置仍被限制在半径为 d 的圆内。开发了一种自适应丢弃方案来高效计算幽灵投影，并将运行时减少了高达 60%。还提供了一个可视化工具，支持交互式探索数据点的 (r,d)-稳定性。", "result": "自适应丢弃方案将运行时缩短了高达 60%，同时保留了大约 90% 的不稳定点。该框架在检查真实世界数据集的投影稳定性方面表现出有效性。", "conclusion": "本文提出了 (r,d)-稳定性框架和 GhostUMAP2 工具，用于测量和分析 UMAP 投影的随机不稳定性，并提供了使用指南，有助于用户有效利用 UMAP。", "translation": "尽管统一流形逼近与投影（UMAP）被广泛使用，但其随机优化过程对结果的影响仍未得到充分探索。我们观察到它常常产生不稳定的结果，其中数据点的投影位置很大程度上是随机决定的，而不是反映邻近结构。为了解决这一限制，我们向 UMAP 引入了 (r,d)-稳定性：一个分析投影空间中数据点随机定位的框架。为了评估随机元素，特别是初始投影位置和负采样，如何影响 UMAP 结果，我们引入了“幽灵”，即数据点的副本，代表由于随机性引起的潜在位置变异。我们将数据点的投影定义为 (r,d)-稳定，如果其在初始投影中半径为 r 的圆内扰动的幽灵，在最终位置仍被限制在半径为 d 的圆内。为了高效计算幽灵投影，我们开发了一种自适应丢弃方案，与未优化的基线相比，运行时减少了高达 60%，同时保留了大约 90% 的不稳定点。我们还提供了一个可视化工具，支持交互式探索数据点的 (r,d)-稳定性。最后，我们通过检查真实世界数据集的投影稳定性，展示了我们框架的有效性，并提出了有效使用我们框架的指南。", "summary": "本文针对 UMAP 投影结果的不稳定性问题，引入了 (r,d)-稳定性框架和“幽灵”概念来量化和分析数据点在投影空间中的随机定位。通过定义 (r,d)-稳定性的度量标准，并开发了高效的自适应丢弃方案和可视化工具，以评估随机性对 UMAP 结果的影响。研究表明该方法能够有效测量和分析真实世界数据集的投影稳定性，并提供了使用指南，旨在帮助用户更好地理解和应用 UMAP。", "keywords": "UMAP, 稳定性, 降维, 随机性, 流形学习", "comments": "本文创新性地提出了 (r,d)-稳定性框架和“幽灵”概念来量化 UMAP 投影的随机不稳定性，这是对现有流形学习算法稳定性的重要补充。其高效的计算方法和交互式可视化工具，显著提升了 UMAP 在实际应用中的可靠性和可解释性，对于需要高可靠性降维结果的领域具有重要意义。"}}
{"id": "2507.17285", "title": "Decentralized Federated Learning of Probabilistic Generative Classifiers", "authors": ["Aritz Pérez", "Carlos Echegoyen", "Guzmán Santafé"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17285v1", "summary": "Federated learning is a paradigm of increasing relevance in real world\napplications, aimed at building a global model across a network of\nheterogeneous users without requiring the sharing of private data. We focus on\nmodel learning over decentralized architectures, where users collaborate\ndirectly to update the global model without relying on a central server. In\nthis context, the current paper proposes a novel approach to collaboratively\nlearn probabilistic generative classifiers with a parametric form. The\nframework is composed by a communication network over a set of local nodes,\neach of one having its own local data, and a local updating rule. The proposal\ninvolves sharing local statistics with neighboring nodes, where each node\naggregates the neighbors' information and iteratively learns its own local\nclassifier, which progressively converges to a global model. Extensive\nexperiments demonstrate that the algorithm consistently converges to a globally\ncompetitive model across a wide range of network topologies, network sizes,\nlocal dataset sizes, and extreme non-i.i.d. data distributions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17285v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "概率生成分类器的去中心化联邦学习", "tldr": "提出一种去中心化联邦学习方法，用于在不共享私有数据的情况下，通过节点间共享统计信息，协作学习概率生成分类器，并能稳定收敛。", "motivation": "联邦学习是现实世界应用中日益重要的范式，旨在无需共享私有数据的情况下，在异构用户网络中构建全局模型。本文关注去中心化架构，即用户直接协作更新模型而无需中心服务器。", "method": "本文提出一种新颖的方法，用于协作学习具有参数形式的概率生成分类器。该框架由本地节点间的通信网络和本地更新规则组成，节点与邻居共享本地统计信息，聚合邻居信息并迭代学习本地分类器，逐步收敛到全局模型。", "result": "广泛的实验表明，该算法在各种网络拓扑、网络规模、本地数据集大小和极端非独立同分布数据下，都能持续收敛到具有全局竞争力的模型。", "conclusion": "该研究成功开发并验证了一种在去中心化联邦学习环境中有效训练概率生成分类器的新方法，证明了其在多种复杂条件下的鲁棒收敛性。", "translation": "联邦学习是现实世界应用中日益重要的范式，旨在无需共享私有数据的情况下，在异构用户网络中构建全局模型。我们专注于去中心化架构上的模型学习，其中用户直接协作更新全局模型，而不依赖于中心服务器。在此背景下，本文提出了一种新颖的方法，用于协作学习具有参数形式的概率生成分类器。该框架由一组本地节点上的通信网络组成，每个节点都有自己的本地数据和本地更新规则。该提议涉及与邻居节点共享本地统计信息，其中每个节点聚合邻居的信息并迭代学习自己的本地分类器，该分类器逐渐收敛到全局模型。广泛的实验表明，该算法在各种网络拓扑、网络规模、本地数据集大小和极端非独立同分布数据分布下，都能持续收敛到具有全局竞争力的模型。", "summary": "本文提出一种去中心化联邦学习新方法，旨在无需中心服务器的情况下，通过节点间直接协作学习概率生成分类器。该方法允许本地节点与邻居共享统计信息，并迭代更新自身分类器，最终收敛到全局模型。实验证明，该算法在多种网络条件和非独立同分布数据下均能稳定收敛并达到良好性能。", "keywords": "联邦学习, 去中心化, 概率生成分类器, 分布式学习, 非独立同分布", "comments": "该论文的创新点在于将联邦学习扩展到去中心化架构下的概率生成分类器学习，解决了无需中心服务器进行模型聚合的挑战。通过局部统计信息共享和迭代学习机制，实现了全局模型的收敛，并在复杂数据分布下表现出鲁棒性，对于隐私保护和分布式机器学习领域具有重要意义。"}}
{"id": "2506.21427", "title": "Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning", "authors": ["Prajwal Koirala", "Cody Fleming"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21427v2", "summary": "Generative models such as diffusion and flow-matching offer expressive\npolicies for offline reinforcement learning (RL) by capturing rich, multimodal\naction distributions, but their iterative sampling introduces high inference\ncosts and training instability due to gradient propagation across sampling\nsteps. We propose the \\textit{Single-Step Completion Policy} (SSCP), a\ngenerative policy trained with an augmented flow-matching objective to predict\ndirect completion vectors from intermediate flow samples, enabling accurate,\none-shot action generation. In an off-policy actor-critic framework, SSCP\ncombines the expressiveness of generative models with the training and\ninference efficiency of unimodal policies, without requiring long\nbackpropagation chains. Our method scales effectively to offline,\noffline-to-online, and online RL settings, offering substantial gains in speed\nand adaptability over diffusion-based baselines. We further extend SSCP to\ngoal-conditioned RL, enabling flat policies to exploit subgoal structures\nwithout explicit hierarchical inference. SSCP achieves strong results across\nstandard offline RL and behavior cloning benchmarks, positioning it as a\nversatile, expressive, and efficient framework for deep RL and sequential\ndecision-making.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21427v2", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-07-23", "AI": {"title_translation": "基于流的单步完成，实现高效表达的策略学习", "tldr": "SSCP是一种基于流匹配的生成式策略，通过单步完成预测实现高效且富有表达力的动作生成，解决了传统生成模型在强化学习中推理成本高和训练不稳定的问题。", "motivation": "现有的生成模型（如扩散模型和流匹配）虽然能捕获丰富的多模态动作分布，但其迭代采样导致高昂的推理成本和训练不稳定性，因为梯度需要跨采样步骤传播。", "method": "提出“单步完成策略”（SSCP），这是一种生成式策略，通过增强的流匹配目标进行训练，以预测来自中间流样本的直接完成向量，从而实现准确的单次动作生成。SSCP在离线Actor-Critic框架中，结合了生成模型的表达能力和单峰策略的训练与推理效率，无需长反向传播链。该方法可有效扩展到离线、离线到在线以及在线RL设置，并进一步扩展到目标条件RL。", "result": "SSCP在速度和适应性方面比基于扩散的基线有显著提升。在标准离线RL和行为克隆基准测试中取得了良好结果。", "conclusion": "SSCP是一个多功能、富有表现力且高效的深度强化学习和序列决策框架。", "translation": "**论文标题：** 基于流的单步完成，实现高效表达的策略学习\n\n**论文摘要：** 扩散和流匹配等生成模型通过捕获丰富、多模态的动作分布，为离线强化学习（RL）提供了富有表达力的策略，但其迭代采样引入了高昂的推理成本和训练不稳定性，因为梯度需要在采样步骤之间传播。我们提出了**单步完成策略**（SSCP），这是一种使用增强流匹配目标训练的生成式策略，用于预测来自中间流样本的直接完成向量，从而实现准确的单次动作生成。在离策略Actor-Critic框架中，SSCP结合了生成模型的表达能力与单峰策略的训练和推理效率，而无需长的反向传播链。我们的方法可有效扩展到离线、离线到在线以及在线RL设置，在速度和适应性方面比基于扩散的基线有显著提升。我们进一步将SSCP扩展到目标条件RL，使扁平策略能够利用子目标结构而无需显式分层推理。SSCP在标准离线RL和行为克隆基准测试中取得了良好结果，将其定位为深度RL和序列决策的多功能、富有表达力且高效的框架。", "summary": "本论文提出单步完成策略（SSCP），旨在解决扩散和流匹配等生成模型在强化学习中因迭代采样导致的高推理成本和训练不稳定问题。SSCP通过增强的流匹配目标训练，能够从中间流样本直接预测完成向量，实现一次性动作生成。在离线Actor-Critic框架下，SSCP结合了生成模型的表达能力与单峰策略的训练和推理效率，避免了长反向传播链。该方法在离线、离线到在线以及在线强化学习设置中表现出良好的可扩展性，并在速度和适应性上优于基于扩散的基线，同时在目标条件强化学习中也能有效应用。SSCP在多个基准测试中表现出色，被定位为一个通用、表达力强且高效的深度强化学习框架。", "keywords": "强化学习, 生成模型, 流匹配, 单步完成策略, 高效策略学习", "comments": "SSCP的创新之处在于它通过引入“单步完成”的概念，成功地将生成模型（如流匹配）的强大表达能力与传统单模态策略的效率结合起来，解决了生成模型在强化学习中面临的推理速度慢和训练不稳定的核心痛点。它通过预测直接完成向量而非迭代采样，显著提高了推理效率，并简化了梯度传播，使其在多种RL设置中都表现出强大的实用性。"}}
{"id": "2303.05103", "title": "Algorithmic neutrality", "authors": ["Milo Phillips-Brown"], "categories": ["cs.CY", "cs.IR", "K.4.2"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      24 pages", "url": "http://arxiv.org/abs/2303.05103v4", "summary": "Algorithms wield increasing power over our lives. They can and often do wield\nthat power unfairly, and much has been said about algorithmic fairness. In\ncontrast, algorithmic neutrality has been largely neglected. I investigate\nalgorithmic neutrality, asking: What is it? Is it possible? And what is its\nnormative significance?", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2303.05103v4", "cate": "cs.CY", "date": "2023-03-09", "updated": "2025-07-22", "AI": {"title_translation": "算法中立性", "tldr": "本文研究了算法中立性，探讨了它的定义、可能性及其规范意义，指出该概念与算法公平性相比常被忽视。", "motivation": "算法对我们的生活施加着越来越大的影响力，并且可能不公平地行使这种权力。尽管算法公平性已得到广泛讨论，但算法中立性却在很大程度上被忽视。本文旨在调查算法中立性。", "method": "作者通过提出并探究以下问题来调查算法中立性：它是什么？它是否可能实现？以及它具有怎样的规范意义？", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "算法对我们的生活施加着越来越大的影响力。它们能够并且经常不公平地运用这种权力，关于算法公平性已经有很多讨论。然而，算法中立性却在很大程度上被忽视了。我将对算法中立性进行研究，探究：它是什么？它是否可能实现？以及它具有怎样的规范意义？", "summary": "本文旨在调查算法中立性这一概念，指出其与算法公平性相比常被忽视。研究内容包括探讨算法中立性的定义、实现可能性及其规范意义。", "keywords": "算法中立性, 算法公平性, 规范意义, 算法", "comments": "本文探讨了算法治理中一个关键但常被忽视的方面。与普遍讨论的公平性不同，中立性提供了一个不同的视角来评估算法的影响。其创新之处在于将焦点转向“中立性”，并探讨其基本性质和含义。其重要性在于可能为算法的伦理准则或监管框架奠定基础。"}}
{"id": "2507.11588", "title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics", "authors": ["Suyuan Zhao", "Yizhen Luo", "Ganbo Yang", "Yan Zhong", "Hao Zhou", "Zaiqing Nie"], "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "Comments:      Accpeted by ICML 2025", "url": "http://arxiv.org/abs/2507.11588v2", "summary": "Spatial Transcriptomics (ST) technologies provide biologists with rich\ninsights into single-cell biology by preserving spatial context of cells.\nBuilding foundational models for ST can significantly enhance the analysis of\nvast and complex data sources, unlocking new perspectives on the intricacies of\nbiological tissues. However, modeling ST data is inherently challenging due to\nthe need to extract multi-scale information from tissue slices containing vast\nnumbers of cells. This process requires integrating macro-scale tissue\nmorphology, micro-scale cellular microenvironment, and gene-scale gene\nexpression profile. To address this challenge, we propose SToFM, a multi-scale\nSpatial Transcriptomics Foundation Model. SToFM first performs multi-scale\ninformation extraction on each ST slice, to construct a set of ST sub-slices\nthat aggregate macro-, micro- and gene-scale information. Then an SE(2)\nTransformer is used to obtain high-quality cell representations from the\nsub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest\nhigh-resolution spatial transcriptomics corpus for pretraining. SToFM achieves\noutstanding performance on a variety of downstream tasks, such as tissue region\nsemantic segmentation and cell type annotation, demonstrating its comprehensive\nunderstanding of ST data through capturing and integrating multi-scale\ninformation.", "comment": "Accpeted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.11588v2", "cate": "q-bio.GN", "date": "2025-07-15", "updated": "2025-07-23", "AI": {"title_translation": "SToFM：一种用于空间转录组学的多尺度基础模型", "tldr": "SToFM是一个多尺度空间转录组基础模型，通过提取和整合宏观、微观和基因尺度的信息，并在最大的高分辨率空间转录组语料库SToCorpus-88M上进行预训练，在ST数据分析任务（如语义分割和细胞类型注释）上表现出色。", "motivation": "空间转录组（ST）数据建模的挑战在于需要从包含大量细胞的组织切片中提取和整合多尺度信息，包括宏观尺度的组织形态、微观尺度的细胞微环境和基因尺度的基因表达谱。", "method": "提出SToFM，一个多尺度空间转录组基础模型。它首先对每个ST切片进行多尺度信息提取，以构建聚合宏观、微观和基因尺度信息的ST子切片。然后，使用SE(2) Transformer从子切片中获取高质量的细胞表示。此外，还构建了最大的高分辨率空间转录组语料库SToCorpus-88M用于预训练。", "result": "SToFM在多种下游任务（如组织区域语义分割和细胞类型注释）上取得了出色的性能。", "conclusion": "SToFM通过捕获和整合多尺度信息，展示了其对空间转录组数据的全面理解。", "translation": "空间转录组学（ST）技术通过保留细胞的空间背景，为生物学家提供了对单细胞生物学的丰富见解。为ST构建基础模型可以显著增强对海量复杂数据源的分析，从而揭示生物组织复杂性的新视角。然而，由于需要从包含大量细胞的组织切片中提取多尺度信息，ST数据建模本身就具有挑战性。这个过程需要整合宏观尺度的组织形态、微观尺度的细胞微环境以及基因尺度的基因表达谱。为了应对这一挑战，我们提出了SToFM，一个多尺度空间转录组基础模型。SToFM首先对每个ST切片进行多尺度信息提取，以构建一组聚合宏观、微观和基因尺度信息的ST子切片。然后，使用SE(2) Transformer从子切片中获取高质量的细胞表示。此外，我们构建了**SToCorpus-88M**，这是最大的高分辨率空间转录组语料库，用于预训练。SToFM在各种下游任务（如组织区域语义分割和细胞类型注释）上取得了出色的性能，通过捕获和整合多尺度信息，展示了其对ST数据的全面理解。", "summary": "SToFM是一个新颖的多尺度空间转录组基础模型，旨在解决ST数据中多尺度信息提取和整合的挑战。该模型通过构建聚合宏观、微观和基因尺度信息的ST子切片，并结合SE(2) Transformer生成高质量细胞表示。此外，论文还构建了迄今最大的高分辨率空间转录组语料库SToCorpus-88M用于预训练。实验结果表明，SToFM在组织区域语义分割和细胞类型注释等下游任务中表现出色，证明了其对ST数据的深刻理解。", "keywords": "空间转录组学, 基础模型, 多尺度信息, SE(2) Transformer, SToCorpus-88M", "comments": "SToFM的创新之处在于其独特的多尺度信息提取和整合方法，以及引入SE(2) Transformer来处理空间转录组数据。构建大规模预训练语料库SToCorpus-88M是其另一个重要贡献，这对于推动空间转录组领域的基础模型发展具有重要意义。该模型有望为生物学家提供更强大、更全面的工具来分析复杂的空间生物数据。"}}
{"id": "2507.17012", "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents", "authors": ["Zhihan Zhang", "Alexander Metzger", "Yuxuan Mei", "Felix Hähnlein", "Zachary Englhardt", "Tingyu Cheng", "Gregory D. Abowd", "Shwetak Patel", "Adriana Schulz", "Vikram Iyer"], "categories": ["cs.AI", "cs.CE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17012v1", "summary": "Interest in sustainability information has surged in recent years. However,\nthe data required for a life cycle assessment (LCA) that maps the materials and\nprocesses from product manufacturing to disposal into environmental impacts\n(EI) are often unavailable. Here we reimagine conventional LCA by introducing\nmultimodal AI agents that emulate interactions between LCA experts and\nstakeholders like product managers and engineers to calculate the\ncradle-to-gate (production) carbon emissions of electronic devices. The AI\nagents iteratively generate a detailed life-cycle inventory leveraging a custom\ndata abstraction and software tools that extract information from online text\nand images from repair communities and government certifications. This approach\nreduces weeks or months of expert time to under one minute and closes data\navailability gaps while yielding carbon footprint estimates within 19% of\nexpert LCAs with zero proprietary data. Additionally, we develop a method to\ndirectly estimate EI by comparing an input to a cluster of products with\nsimilar descriptions and known carbon footprints. This runs in 3 ms on a laptop\nwith a MAPE of 12.28% on electronic products. Further, we develop a data-driven\nmethod to generate emission factors. We use the properties of an unknown\nmaterial to represent it as a weighted sum of emission factors for similar\nmaterials. Compared to human experts picking the closest LCA database entry,\nthis improves MAPE by 120.26%. We analyze the data and compute scaling of this\napproach and discuss its implications for future LCA workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17012v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "通过多模态AI代理实现自主可持续性评估", "tldr": "本文提出多模态AI代理，通过自动化数据提取和迭代计算，显著加速并提高了电子设备碳排放评估的准确性，解决了传统生命周期评估的数据缺失和耗时问题。", "motivation": "传统生命周期评估（LCA）所需的数据常常难以获取，导致评估耗时数周或数月。此外，现有方法依赖专有数据。", "method": "引入多模态AI代理，模拟LCA专家与利益相关者（如产品经理、工程师）的互动，以计算电子设备的摇篮到大门（生产）碳排放。AI代理利用自定义数据抽象和软件工具，从在线文本和图像中提取信息，迭代生成详细的生命周期清单。此外，开发了一种直接估算环境影响的方法，通过将输入与具有相似描述和已知碳足迹的产品簇进行比较。还开发了一种数据驱动的方法来生成排放因子，将未知材料的属性表示为相似材料排放因子的加权和。", "result": "该方法将专家时间从数周或数月缩短到一分钟以内，并在零专有数据的情况下，使碳足迹估算与专家LCA的差异在19%以内。直接估算环境影响的方法在笔记本电脑上运行时间为3毫秒，电子产品MAPE为12.28%。生成排放因子方法相较于人类专家挑选最接近的LCA数据库条目，将MAPE提高了120.26%。", "conclusion": "Not mentioned in abstract", "translation": "近年来，对可持续性信息的兴趣激增。然而，生命周期评估（LCA）所需的数据——即将产品制造到处置的材料和过程映射到环境影响（EI）——通常是不可用的。本文通过引入多模态AI代理，重新构想了传统的LCA，这些代理模拟LCA专家与产品经理和工程师等利益相关者之间的互动，以计算电子设备的从摇篮到大门（生产）的碳排放。AI代理利用自定义数据抽象和软件工具，从在线文本和维修社区的图像以及政府认证中提取信息，迭代生成详细的生命周期清单。这种方法将数周或数月的专家时间缩短到一分钟以内，弥补了数据可用性差距，同时在零专有数据的情况下，碳足迹估算与专家LCA的差异在19%以内。此外，我们开发了一种直接估算EI的方法，通过将输入与具有相似描述和已知碳足迹的产品簇进行比较。该方法在笔记本电脑上运行时间为3毫秒，在电子产品上的平均绝对百分比误差（MAPE）为12.28%。此外，我们开发了一种数据驱动的方法来生成排放因子。我们使用未知材料的属性，将其表示为相似材料排放因子的加权和。与人类专家选择最接近的LCA数据库条目相比，这使MAPE提高了120.26%。我们分析了该方法的数据和计算扩展性，并讨论了其对未来LCA工作流程的影响。", "summary": "本文介绍了一种通过多模态AI代理实现自主可持续性评估的方法，旨在解决传统生命周期评估（LCA）中数据获取困难和耗时的问题。该方法通过AI代理模拟专家与利益相关者的互动，从在线文本和图像中提取数据，迭代计算电子设备的碳排放，将评估时间从数周缩短至一分钟以内，且无需专有数据即可达到与专家LCA相近的精度。此外，还提出了直接估算环境影响和生成排放因子的数据驱动方法，显著提高了效率和准确性。这项工作为未来的LCA工作流程提供了新的思路和潜力。", "keywords": "可持续性评估, AI代理, 生命周期评估, 碳排放, 多模态", "comments": "该论文提出了一种创新性的方法，利用多模态AI代理大幅提升了可持续性评估的自动化和效率，尤其是在数据获取和处理方面。通过模拟专家交互和利用在线非结构化数据，它有效地解决了传统LCA的数据瓶颈和高成本问题。其能够在无专有数据下达到较高精度，并显著缩短评估时间，具有重要的实际应用价值和潜力。"}}
{"id": "2507.17000", "title": "Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks", "authors": ["Jacob Piland", "Chris Sweet", "Adam Czajka"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17000v1", "summary": "Existing saliency-guided training approaches improve model generalization by\nincorporating a loss term that compares the model's class activation map (CAM)\nfor a sample's true-class ({\\it i.e.}, correct-label class) against a human\nreference saliency map. However, prior work has ignored the false-class CAM(s),\nthat is the model's saliency obtained for incorrect-label class. We hypothesize\nthat in binary tasks the true and false CAMs should diverge on the important\nclassification features identified by humans (and reflected in human saliency\nmaps). We use this hypothesis to motivate three new saliency-guided training\nmethods incorporating both true- and false-class model's CAM into the training\nstrategy and a novel post-hoc tool for identifying important features. We\nevaluate all introduced methods on several diverse binary close-set and\nopen-set classification tasks, including synthetic face detection, biometric\npresentation attack detection, and classification of anomalies in chest X-ray\nscans, and find that the proposed methods improve generalization capabilities\nof deep learning models over traditional (true-class CAM only) saliency-guided\ntraining approaches. We offer source codes and model weights\\footnote{GitHub\nrepository link removed to preserve anonymity} to support reproducible\nresearch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17000v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "分歧的决策：改进基于显著性的训练以提高二元分类任务的泛化能力", "tldr": "现有基于显著性的训练方法忽略了错误类别激活图（CAMs）。本文提出了新的方法，利用真实和错误类别CAM来提高二元分类的泛化能力。", "motivation": "现有的基于显著性的训练方法只考虑真实类别的类别激活图（CAM），而忽略了错误类别的CAM。作者假设在二元任务中，真实和错误类别的CAM应该在人类识别的重要分类特征上存在差异，而先前的研究并未利用这一点。", "method": "本文提出了三种新的基于显著性的训练方法，这些方法将模型针对真实类别和错误类别的CAM都纳入训练策略。此外，还引入了一种新颖的事后工具用于识别重要特征。这些方法在多种二元闭集和开集分类任务上进行了评估。", "result": "所提出的方法相比传统的（仅使用真实类别CAM的）基于显著性的训练方法，显著提高了深度学习模型的泛化能力。", "conclusion": "在基于显著性的训练中，结合真实和错误类别的类别激活图（CAM）可以显著增强模型在二元分类任务中的泛化能力。", "translation": "现有基于显著性的训练方法通过引入一个损失项来提高模型泛化能力，该损失项将样本真实类别（即正确标签类别）的模型类别激活图（CAM）与人类参考显著性图进行比较。然而，先前的工作忽略了错误类别CAM，即模型针对不正确标签类别获得的显著性。我们假设在二元任务中，真实和错误类别CAM应该在人类识别的重要分类特征上存在差异（并反映在人类显著性图中）。我们利用这一假设来提出三种新的基于显著性的训练方法，这些方法将真实和错误类别模型的CAM都纳入训练策略，并引入了一种新颖的事后工具来识别重要特征。我们在几种不同的二元闭集和开集分类任务上评估了所有引入的方法，包括合成人脸检测、生物识别演示攻击检测以及胸部X射线扫描异常分类，发现所提出的方法比传统（仅真实类别CAM）基于显著性的训练方法提高了深度学习模型的泛化能力。我们提供源代码和模型权重以支持可重复研究。", "summary": "本文针对现有基于显著性的训练方法仅使用真实类别激活图（CAM）的局限性。作者假设在二元任务中，真实和错误类别的CAM应在重要特征上有所不同，并基于此提出了三种新的基于显著性的训练方法，这些方法整合了真实和错误类别的CAM。同时，还引入了一种新颖的事后工具用于特征识别。在多种二元分类任务上的实验证明，与传统方法相比，所提出的方法显著提高了深度学习模型的泛化能力。", "keywords": "显著性训练, 二元分类, 泛化能力, 类别激活图, 错误类别CAM", "comments": "本文的创新之处在于明确考虑了“错误类别”的CAM，这为基于显著性的训练，特别是二元分类任务，提供了一个新颖的视角。真实和错误类别CAM之间的差异性假设具有深刻见解，并有效提升了模型的泛化能力。提供源代码和模型权重也有助于促进研究的可复现性。"}}
{"id": "2507.17239", "title": "MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training", "authors": ["Lei Zhu", "Jun Zhou", "Rick Siow Mong Goh", "Yong Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MedAGI 2025 (Oral)", "url": "http://arxiv.org/abs/2507.17239v1", "summary": "Foundation models have recently gained tremendous popularity in medical image\nanalysis. State-of-the-art methods leverage either paired image-text data via\nvision-language pre-training or unpaired image data via self-supervised\npre-training to learn foundation models with generalizable image features to\nboost downstream task performance. However, learning foundation models\nexclusively on either paired or unpaired image data limits their ability to\nlearn richer and more comprehensive image features. In this paper, we\ninvestigate a novel task termed semi-supervised vision-language pre-training,\naiming to fully harness the potential of both paired and unpaired image data\nfor foundation model learning. To this end, we propose MaskedCLIP, a\nsynergistic masked image modeling and contrastive language-image pre-training\nframework for semi-supervised vision-language pre-training. The key challenge\nin combining paired and unpaired image data for learning a foundation model\nlies in the incompatible feature spaces derived from these two types of data.\nTo address this issue, we propose to connect the masked feature space with the\nCLIP feature space with a bridge transformer. In this way, the more semantic\nspecific CLIP features can benefit from the more general masked features for\nsemantic feature extraction. We further propose a masked knowledge distillation\nloss to distill semantic knowledge of original image features in CLIP feature\nspace back to the predicted masked image features in masked feature space. With\nthis mutually interactive design, our framework effectively leverages both\npaired and unpaired image data to learn more generalizable image features for\ndownstream tasks. Extensive experiments on retinal image analysis demonstrate\nthe effectiveness and data efficiency of our method.", "comment": "Accepted to MedAGI 2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2507.17239v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MaskedCLIP: 弥合掩码与CLIP空间，用于半监督医学视觉-语言预训练", "tldr": "MaskedCLIP通过桥接掩码和CLIP特征空间，利用配对和非配对数据进行半监督医学视觉-语言预训练，以学习更通用的医学图像特征。", "motivation": "现有基础模型学习方法仅依赖配对或非配对图像数据，限制了学习更丰富和全面图像特征的能力。", "method": "提出MaskedCLIP框架，结合掩码图像建模和对比语言-图像预训练。通过“桥接变换器”连接掩码特征空间与CLIP特征空间，并引入掩码知识蒸馏损失，将CLIP特征空间的语义知识蒸馏回掩码特征空间，实现两个特征空间的相互作用。", "result": "在视网膜图像分析上的大量实验证明了该方法的有效性和数据效率。", "conclusion": "MaskedCLIP框架通过有效利用配对和非配对图像数据，学习到更通用的图像特征，从而提升下游任务性能。", "translation": "基础模型最近在医学图像分析中获得了巨大的普及。最先进的方法通过视觉-语言预训练利用配对的图像-文本数据，或通过自监督预训练利用非配对的图像数据来学习具有可泛化图像特征的基础模型，以提升下游任务性能。然而，仅在配对或非配对图像数据上学习基础模型限制了它们学习更丰富、更全面图像特征的能力。在本文中，我们研究了一项名为半监督视觉-语言预训练的新任务，旨在充分利用配对和非配对图像数据在基础模型学习中的潜力。为此，我们提出了MaskedCLIP，一个用于半监督视觉-语言预训练的协同掩码图像建模和对比语言-图像预训练框架。将配对和非配对图像数据结合起来学习基础模型的关键挑战在于这两种数据类型产生的特征空间不兼容。为了解决这个问题，我们提出用一个桥接变换器连接掩码特征空间与CLIP特征空间。通过这种方式，语义更具体的CLIP特征可以受益于更通用的掩码特征，从而进行语义特征提取。我们进一步提出了一种掩码知识蒸馏损失，用于将CLIP特征空间中原始图像特征的语义知识蒸馏回掩码特征空间中预测的掩码图像特征。通过这种相互作用的设计，我们的框架有效地利用了配对和非配对图像数据，以学习更通用的图像特征，用于下游任务。在视网膜图像分析上的大量实验证明了我们方法的有效性和数据效率。", "summary": "本文提出了MaskedCLIP，一个用于半监督视觉-语言预训练的新框架，旨在结合配对和非配对图像数据来学习更通用的医学图像基础模型。针对配对和非配对数据特征空间不兼容的挑战，MaskedCLIP引入了桥接变换器来连接掩码特征空间和CLIP特征空间，并通过掩码知识蒸馏损失实现语义知识的相互传递。实验证明，该方法在视网膜图像分析中有效且数据高效。", "keywords": "半监督学习, 视觉-语言预训练, 掩码图像建模, CLIP, 医学图像分析", "comments": "该论文创新性地提出了半监督视觉-语言预训练任务，并设计了MaskedCLIP框架来解决配对和非配对医学图像数据特征空间不兼容的问题。通过桥接变换器和知识蒸馏，实现了两种数据类型优势的互补，有望学习到更鲁棒和全面的医学图像特征，对于资源受限的医学领域具有重要意义。"}}
{"id": "2507.17616", "title": "Vision Transformer attention alignment with human visual perception in aesthetic object evaluation", "authors": ["Miguel Carrasco", "César González-Martín", "José Aranda", "Luis Oliveros"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      25 pages, 15 figures", "url": "http://arxiv.org/abs/2507.17616v1", "summary": "Visual attention mechanisms play a crucial role in human perception and\naesthetic evaluation. Recent advances in Vision Transformers (ViTs) have\ndemonstrated remarkable capabilities in computer vision tasks, yet their\nalignment with human visual attention patterns remains underexplored,\nparticularly in aesthetic contexts. This study investigates the correlation\nbetween human visual attention and ViT attention mechanisms when evaluating\nhandcrafted objects. We conducted an eye-tracking experiment with 30\nparticipants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal\nobjects comprising basketry bags and ginger jars. Using a Pupil Labs\neye-tracker, we recorded gaze patterns and generated heat maps representing\nhuman visual attention. Simultaneously, we analyzed the same objects using a\npre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting\nattention maps from each of the 12 attention heads. We compared human and ViT\nattention distributions using Kullback-Leibler divergence across varying\nGaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal\ncorrelation at sigma=2.4 +-0.03, with attention head #12 showing the strongest\nalignment with human visual patterns. Significant differences were found\nbetween attention heads, with heads #7 and #9 demonstrating the greatest\ndivergence from human attention (p< 0.05, Tukey HSD test). Results indicate\nthat while ViTs exhibit more global attention patterns compared to human focal\nattention, certain attention heads can approximate human visual behavior,\nparticularly for specific object features like buckles in basketry items. These\nfindings suggest potential applications of ViT attention mechanisms in product\ndesign and aesthetic evaluation, while highlighting fundamental differences in\nattention strategies between human perception and current AI models.", "comment": "25 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.17616v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "视觉Transformer注意力与人类视觉感知在美学对象评估中的对齐", "tldr": "研究发现ViT（特别是注意力头#12）在美学评估中与人类视觉注意力存在一定对齐，但整体策略仍有差异。", "motivation": "Vision Transformer (ViT) 在计算机视觉任务中表现出色，但其注意力模式与人类视觉注意力的对齐程度，尤其是在美学语境下，仍未得到充分探索。本研究旨在调查ViT注意力机制与人类视觉注意力在评估手工制品时的相关性。", "method": "本研究进行了一项眼动追踪实验，30名参与者评估20件手工艺术品（篮子包和姜罐），使用Pupil Labs眼动仪记录人类凝视模式并生成热图。同时，使用一个经过DINO预训练的ViT模型分析相同的对象，并从12个注意力头中提取注意力图。通过Kullback-Leibler散度在不同高斯参数下比较人类和ViT的注意力分布，并进行统计分析（Tukey HSD测试）。", "result": "统计分析显示，在sigma=2.4 ±0.03时，人类和ViT注意力分布达到了最佳相关性。注意力头#12与人类视觉模式对齐最强，而头#7和#9与人类注意力偏离最大（p< 0.05）。结果表明，ViT表现出比人类焦点注意力更全局的注意力模式，但某些注意力头可以近似人类视觉行为，特别是对于特定对象特征（如篮子物品中的搭扣）。", "conclusion": "研究结果表明ViT注意力机制在产品设计和美学评估中具有潜在应用，但同时也突出了人类感知与当前AI模型之间注意力策略的根本差异。", "translation": "视觉注意力机制在人类感知和美学评估中发挥着关键作用。Vision Transformer (ViT) 的最新进展在计算机视觉任务中展现出卓越的能力，然而，它们与人类视觉注意力模式的对齐程度仍未得到充分探索，尤其是在美学语境下。本研究调查了在评估手工制品时，人类视觉注意力与ViT注意力机制之间的相关性。我们进行了一项眼动追踪实验，30名参与者（9名女性，21名男性，平均年龄24.6岁）观看了20件手工艺术品，包括篮子包和姜罐。使用Pupil Labs眼动仪，我们记录了凝视模式并生成了代表人类视觉注意力的热图。同时，我们使用一个经过DINO（无标签自蒸馏）预训练的ViT模型分析了相同的对象，从12个注意力头中的每一个提取了注意力图。我们使用Kullback-Leibler散度在不同的高斯参数（sigma=0.1到3.0）下比较了人类和ViT的注意力分布。统计分析显示，在sigma=2.4 ±0.03时达到了最佳相关性，其中注意力头#12与人类视觉模式对齐最强。注意力头之间存在显著差异，其中头#7和#9与人类注意力的偏离最大（p< 0.05，Tukey HSD测试）。结果表明，虽然ViT与人类的焦点注意力相比表现出更全局的注意力模式，但某些注意力头可以近似人类的视觉行为，特别是对于特定对象特征，如篮子物品中的搭扣。这些发现表明ViT注意力机制在产品设计和美学评估中具有潜在应用，同时也突出了人类感知与当前AI模型之间注意力策略的根本差异。", "summary": "本研究通过结合眼动追踪实验和ViT模型分析，探讨了Vision Transformer在美学对象评估中与人类视觉注意力的对齐情况。研究发现，ViT，尤其是其特定注意力头（如#12），能与人类视觉模式达到一定程度的对齐，尽管ViT整体倾向于更全局的注意力模式。研究结果揭示了ViT在产品设计和美学评估中的潜在应用，同时也强调了人类感知与当前AI模型之间在注意力策略上的根本差异。", "keywords": "Vision Transformer, 注意力机制, 人类视觉感知, 美学评估, 眼动追踪", "comments": "这项研究创新性地结合了眼动追踪和Vision Transformer分析，为理解AI模型如何“看”以及其与人类视觉感知的异同提供了宝贵视角。它不仅验证了ViT在模拟人类注意力方面的潜力，也指出了其局限性，即全局性注意力与人类焦点注意力的差异。这对于未来AI模型在美学、设计等需要精细视觉判断领域的应用具有指导意义。"}}
{"id": "2507.17333", "title": "Design and analysis of twisted and BGG Stokes-de Rham polytopal complexes", "authors": ["Daniele A. Di Pietro", "Jérôme Droniou", "Kaibo Hu", "Arax Leroy"], "categories": ["math.NA", "cs.NA", "65N30, 65N12, 74K20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17333v1", "summary": "We design a discrete Bernstein--Gelfand--Gelfand (BGG) diagram on polygonal\nmeshes based on the DDR framework; the diagram is made of a discrete Stokes\npolygonal complex and a tensorised Discrete De Rham complex, and the BGG\nconstruction leads to a novel elasticity complex applicable on generic\npolygonal meshes. Complete homological and analytical properties of the\ndiscrete Stokes complex are established, including primal and adjoint\nconsistency estimates as well as Poincar\\'e inequalities. Homological\nproperties of the complexes built from the BGG diagram are also established.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17333v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "扭曲和BGG Stokes-de Rham多面体复形的设计与分析", "tldr": "该论文设计了一种在多边形网格上用于弹性的离散BGG图，结合了Stokes和De Rham复形，并确立了它们的同调和分析性质。", "motivation": "该论文旨在设计一种适用于通用多边形网格的新型弹性复形。", "method": "作者基于DDR框架在多边形网格上设计了一个离散的Bernstein--Gelfand--Gelfand (BGG) 图，该图由一个离散的Stokes多边形复形和一个张量化的离散de Rham复形组成。论文确立了离散Stokes复形的完整同调和分析性质，包括原始和伴随一致性估计以及Poincaré不等式，并确立了从BGG图构建的复形的同调性质。", "result": "论文开发了一种适用于通用多边形网格的新型弹性复形。离散Stokes复形的完整同调和分析性质，以及从BGG图构建的复形的同调性质均已确立。", "conclusion": "该论文成功设计并分析了离散BGG图，从而得到了一个具有已确立同调和分析性质的新型弹性复形。", "translation": "我们基于DDR框架在多边形网格上设计了一个离散的Bernstein--Gelfand--Gelfand (BGG) 图；该图由一个离散的Stokes多边形复形和一个张量化的离散de Rham复形组成，BGG构造导出了一个适用于通用多边形网格的新型弹性复形。离散Stokes复形的完整同调和分析性质得以确立，包括原始和伴随一致性估计以及Poincaré不等式。从BGG图构建的复形的同调性质也得到了确立。", "summary": "本论文基于DDR框架，在多边形网格上引入了一种离散的Bernstein--Gelfand--Gelfand (BGG) 图。该图结合了离散的Stokes多边形复形和张量化的离散De Rham复形，从而形成了一种适用于通用多边形网格的新型弹性复形。研究详细确立了离散Stokes复形的完整同调和分析性质，包括一致性估计和Poincaré不等式，并验证了从BGG图导出的复形的同调性质。", "keywords": "离散BGG图, 弹性复形, 多边形网格, Stokes复形, De Rham复形", "comments": "该论文通过利用BGG图并结合离散Stokes和De Rham复形，提出了一种在通用多边形网格上构建弹性复形的创新方法。对全面同调和分析性质的建立突显了所提出方法的理论严谨性。"}}
{"id": "2302.11962", "title": "Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods", "authors": ["El Mahdi Chayti", "Nikita Doikov", "Martin Jaggi"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Published in Transactions on Machine Learning Research", "url": "http://arxiv.org/abs/2302.11962v5", "summary": "We study stochastic Cubic Newton methods for solving general possibly\nnon-convex minimization problems. We propose a new framework, which we call the\nhelper framework, that provides a unified view of the stochastic and\nvariance-reduced second-order algorithms equipped with global complexity\nguarantees. It can also be applied to learning with auxiliary information. Our\nhelper framework offers the algorithm designer high flexibility for\nconstructing and analyzing the stochastic Cubic Newton methods, allowing\narbitrary size batches, and the use of noisy and possibly biased estimates of\nthe gradients and Hessians, incorporating both the variance reduction and the\nlazy Hessian updates. We recover the best-known complexities for the stochastic\nand variance-reduced Cubic Newton, under weak assumptions on the noise. A\ndirect consequence of our theory is the new lazy stochastic second-order\nmethod, which significantly improves the arithmetic complexity for large\ndimension problems. We also establish complexity bounds for the classes of\ngradient-dominated objectives, that include convex and strongly convex\nproblems. For Auxiliary Learning, we show that using a helper (auxiliary\nfunction) can outperform training alone if a given similarity measure is small.", "comment": "Published in Transactions on Machine Learning Research", "pdf_url": "http://arxiv.org/pdf/2302.11962v5", "cate": "math.OC", "date": "2023-02-23", "updated": "2025-07-23", "AI": {"title_translation": "随机和方差削减三次牛顿法的统一收敛理论", "tldr": "本文提出了一个“辅助框架”，统一了随机和方差削减的三次牛顿法，并在弱噪声假设下实现了最佳复杂度，还引入了一种新的惰性随机二阶方法，提高了大规模问题的算术复杂度，并证明了辅助学习的优势。", "motivation": "研究随机三次牛顿法以解决一般的非凸最小化问题，并为随机和方差削减的二阶算法提供一个统一的理论框架和更好的性能。", "method": "提出了一个名为“辅助框架”的新框架，该框架允许任意批次大小、使用噪声和可能有偏的梯度和Hessian估计，并结合了方差削减和惰性Hessian更新。", "result": "1. 提供了随机和方差削减二阶算法的统一视图，并具有全局复杂度保证。2. 在弱噪声假设下，恢复了随机和方差削减三次牛顿法的最佳已知复杂度。3. 导出了一种新的惰性随机二阶方法，显著提高了大维度问题的算术复杂度。4. 建立了梯度主导目标（包括凸和强凸问题）的复杂度界限。5. 在辅助学习中，证明了在相似度度量较小时，使用辅助函数可以优于单独训练。", "conclusion": "该研究通过提出的“辅助框架”统一了随机和方差削减三次牛顿法的收敛理论，实现了最佳复杂度，并引入了对大规模问题有益的新算法，同时证明了辅助学习的有效性。", "translation": "我们研究了用于解决一般可能非凸最小化问题的随机三次牛顿法。我们提出了一个名为“辅助框架”的新框架，它为具备全局复杂度保证的随机和方差削减二阶算法提供了一个统一的视角。它也可以应用于辅助信息学习。我们的辅助框架为算法设计者构建和分析随机三次牛顿法提供了高度灵活性，允许任意大小的批次，并使用梯度和Hessian的噪声和可能有偏的估计，同时结合了方差削减和惰性Hessian更新。在弱噪声假设下，我们恢复了随机和方差削减三次牛顿法的最佳已知复杂度。我们理论的一个直接结果是新的惰性随机二阶方法，它显著提高了大维度问题的算术复杂度。我们还为梯度主导目标（包括凸和强凸问题）建立了复杂度界限。对于辅助学习，我们表明如果给定的相似度度量很小，使用辅助函数可以优于单独训练。", "summary": "本文提出了一种名为“辅助框架”的新方法，用于统一分析随机和方差削减的三次牛顿法，以解决非凸最小化问题。该框架提供了高度灵活性，能够处理不同批次大小、噪声和有偏估计，并整合了方差削减和惰性Hessian更新。研究结果表明，该框架在弱噪声假设下能达到最佳复杂度，并催生了一种新的惰性随机二阶方法，显著提高了大维度问题的计算效率。此外，该理论还为梯度主导目标设定了复杂度界限，并证明了在辅助学习中使用辅助函数的优势。", "keywords": "随机三次牛顿法, 方差削减, 辅助框架, 非凸优化, 复杂度分析", "comments": "这篇论文的创新点在于提出了“辅助框架”，它为随机和方差削减的二阶优化算法提供了一个统一且灵活的理论基础。通过整合多种技术（如方差削减、惰性Hessian更新、处理噪声/偏估计），该框架不仅简化了分析，还在理论上取得了最佳复杂度，并导出了对实际大维度问题有显著提升的新算法。其重要性在于为非凸优化领域提供了更强大的工具和更深刻的理解。"}}
{"id": "2507.17628", "title": "Quantifying the ROI of Cyber Threat Intelligence: A Data-Driven Approach", "authors": ["Matteo Strada"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.17628v1", "summary": "The valuation of Cyber Threat Intelligence (CTI) remains a persistent\nchallenge due to the problem of negative evidence: successful threat prevention\nresults in non-events that generate minimal observable financial impact, making\nCTI expenditures difficult to justify within traditional cost-benefit\nframeworks. This study introduces a data-driven methodology for quantifying the\nreturn on investment (ROI) of CTI, thereby reframing it as a measurable\ncontributor to risk mitigation. The proposed framework extends established\nmodels in security economics, including the Gordon-Loeb and FAIR models, to\naccount for CTI's complex influence on both the probability of security\nbreaches and the severity of associated losses. The framework is\noperationalized through empirically grounded performance indicators, such as\nreductions in mean time to detect (MTTD), mean time to respond (MTTR), and\nadversary dwell time, supported by three sector-specific case studies in\nfinance, healthcare, and retail. To address limitations in conventional linear\nassessment methodologies, the Threat Intelligence Effectiveness Index (TIEI) is\nintroduced as a composite metric based on a weighted geometric mean. TIEI\npenalizes underperformance across critical dimensions: quality, enrichment,\nintegration, and operational impact; thereby capturing bottleneck effect where\nthe least effective component limits overall performance. By integrating\nfinancial quantification, adversarial coverage, and qualitative assessments of\nbusiness enablement, the proposed hybrid model converts negative evidence into\na justifiable ROI explanation. This approach offers a replicable means of\nrepositioning CTI from an expense to a strategic investment, enabling informed\ndecision-making and continuous optimization across diverse organizational\ncontexts.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.17628v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "量化网络威胁情报的投资回报率：一种数据驱动的方法", "tldr": "本研究提出了一种数据驱动的方法，用于量化网络威胁情报（CTI）的投资回报率（ROI），解决了传统上难以证明其价值的问题。", "motivation": "由于负面证据问题（成功的威胁预防导致难以观测的非事件，使得CTI支出难以在传统成本效益框架内证明其合理性），网络威胁情报（CTI）的价值评估一直是一个持续的挑战。", "method": "本研究提出了一种数据驱动的方法来量化CTI的投资回报率，将其重新定义为风险缓解的可衡量贡献者。该框架扩展了安全经济学中的既定模型，包括Gordon-Loeb和FAIR模型，以解释CTI对安全漏洞发生概率和相关损失严重性的复杂影响。该框架通过经验性的绩效指标（如检测平均时间（MTTD）、响应平均时间（MTTR）和攻击者驻留时间的减少）进行操作，并辅以金融、医疗保健和零售行业的三个特定案例研究。为解决传统线性评估方法的局限性，引入了威胁情报有效性指数（TIEI）作为基于加权几何平均的复合指标。TIEI会惩罚关键维度（质量、丰富性、集成和运营影响）的欠佳表现，从而捕捉到最无效的组件限制整体性能的瓶颈效应。", "result": "通过整合财务量化、对抗性覆盖和业务赋能的定性评估，所提出的混合模型将负面证据转化为可证明的投资回报率解释。这种方法提供了一种可复制的手段，将CTI从一项开支重新定位为一项战略投资。", "conclusion": "本研究提出的数据驱动方法通过提供可量化的投资回报率解释，使CTI的价值评估成为可能，从而支持组织进行知情的决策和持续优化。", "translation": "网络威胁情报（CTI）的估值仍然是一个持续的挑战，原因在于负面证据问题：成功的威胁预防导致非事件，这些事件产生的可观察财务影响微乎其微，使得CTI支出在传统成本效益框架内难以证明其合理性。本研究引入了一种数据驱动的方法来量化CTI的投资回报率（ROI），从而将其重新定义为风险缓解的可衡量贡献者。所提出的框架扩展了安全经济学中的既定模型，包括Gordon-Loeb和FAIR模型，以解释CTI对安全漏洞发生概率和相关损失严重性的复杂影响。该框架通过经验性的绩效指标（如检测平均时间（MTTD）、响应平均时间（MTTR）和攻击者驻留时间的减少）进行操作，并辅以金融、医疗保健和零售行业的三个特定案例研究。为解决传统线性评估方法的局限性，引入了威胁情报有效性指数（TIEI）作为基于加权几何平均的复合指标。TIEI会惩罚关键维度：质量、丰富性、集成和运营影响的欠佳表现；从而捕捉到最无效的组件限制整体性能的瓶颈效应。通过整合财务量化、对抗性覆盖和业务赋能的定性评估，所提出的混合模型将负面证据转化为可证明的投资回报率解释。这种方法提供了一种可复制的手段，将CTI从一项开支重新定位为一项战略投资，从而在不同的组织环境中实现知情的决策和持续优化。", "summary": "本研究提出了一种数据驱动的方法，旨在解决网络威胁情报（CTI）投资回报率（ROI）难以量化的问题。通过扩展Gordon-Loeb和FAIR等安全经济学模型，并引入威胁情报有效性指数（TIEI），该框架能够量化CTI对安全漏洞概率和损失严重性的影响。该方法将负面证据转化为可证明的ROI，从而将CTI从一项开支转变为一项战略投资，支持组织进行明智决策和持续优化。", "keywords": "网络威胁情报, 投资回报率, 数据驱动, 风险缓解, 威胁情报有效性指数", "comments": "本研究的创新之处在于其提出了一个数据驱动的混合模型来量化网络威胁情报的投资回报率，有效解决了“负面证据”这一长期存在的挑战。通过引入威胁情报有效性指数（TIEI）并结合现有安全经济学模型，该方法为企业提供了衡量CTI价值的实用工具，有助于将其从成本中心转变为战略投资。其重要性在于为企业决策者提供了更清晰的投资依据，促进了网络安全支出的合理化。"}}
{"id": "2507.17389", "title": "Investigating Training Data Detection in AI Coders", "authors": ["Tianlin Li", "Yunxiang Wei", "Zhiming Li", "Aishan Liu", "Qing Guo", "Xianglong Liu", "Dongning Sun", "Yang Liu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17389v1", "summary": "Recent advances in code large language models (CodeLLMs) have made them\nindispensable tools in modern software engineering. However, these models\noccasionally produce outputs that contain proprietary or sensitive code\nsnippets, raising concerns about potential non-compliant use of training data,\nand posing risks to privacy and intellectual property. To ensure responsible\nand compliant deployment of CodeLLMs, training data detection (TDD) has become\na critical task. While recent TDD methods have shown promise in natural\nlanguage settings, their effectiveness on code data remains largely\nunderexplored. This gap is particularly important given code's structured\nsyntax and distinct similarity criteria compared to natural language. To\naddress this, we conduct a comprehensive empirical study of seven\nstate-of-the-art TDD methods on source code data, evaluating their performance\nacross eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a\nfunction-level benchmark dataset comprising 9,000 code samples in three\nprogramming languages, each explicitly labeled as either included or excluded\nfrom CodeLLM training. Beyond evaluation on the original CodeSnitch, we design\ntargeted mutation strategies to test the robustness of TDD methods under three\ndistinct settings. These mutation strategies are grounded in the\nwell-established Type-1 to Type-4 code clone detection taxonomy. Our study\nprovides a systematic assessment of current TDD techniques for code and offers\ninsights to guide the development of more effective and robust detection\nmethods in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17389v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "调查AI编码器中的训练数据检测", "tldr": "本研究评估了七种最先进的训练数据检测（TDD）方法在代码大型语言模型（CodeLLMs）中的表现，引入了CodeSnitch数据集，并使用变异策略测试了TDD方法的鲁棒性，以应对CodeLLM输出中潜在的专有或敏感代码问题。", "motivation": "代码大型语言模型（CodeLLMs）有时会生成包含专有或敏感代码片段的输出，这引发了对训练数据不合规使用、隐私和知识产权的担忧。尽管自然语言领域的训练数据检测（TDD）方法已显示出潜力，但它们在代码数据上的有效性尚未得到充分探索，尤其考虑到代码的结构化语法和独特的相似性标准。", "method": "本研究对七种最先进的训练数据检测（TDD）方法在源代码数据上进行了全面的实证研究，并评估了它们在八个CodeLLMs上的性能。为支持评估，引入了CodeSnitch，一个函数级基准数据集，包含9,000个明确标记为包含或排除在CodeLLM训练之外的代码样本。此外，设计了基于Type-1到Type-4代码克隆检测分类法的目标变异策略，以在三种不同设置下测试TDD方法的鲁棒性。", "result": "本研究对当前代码训练数据检测（TDD）技术进行了系统评估，并为未来开发更有效和鲁棒的检测方法提供了见解。", "conclusion": "本研究为指导未来开发更有效和鲁棒的代码训练数据检测方法提供了基础和见解。", "translation": "近期代码大型语言模型（CodeLLMs）的进展使其成为现代软件工程中不可或缺的工具。然而，这些模型偶尔会产生包含专有或敏感代码片段的输出，引发对训练数据潜在不合规使用的担忧，并对隐私和知识产权构成风险。为确保CodeLLMs的负责任和合规部署，训练数据检测（TDD）已成为一项关键任务。虽然最近的TDD方法在自然语言环境中展现出前景，但它们在代码数据上的有效性仍未得到充分探索。考虑到代码的结构化语法和与自然语言相比独特的相似性标准，这一差距尤为重要。为了解决这个问题，我们对七种最先进的TDD方法在源代码数据上进行了全面的实证研究，评估了它们在八个CodeLLMs上的性能。为了支持这项评估，我们引入了CodeSnitch，一个函数级基准数据集，包含9,000个三种编程语言的代码样本，每个样本都明确标记为包含或排除在CodeLLM训练之外。除了在原始CodeSnitch上进行评估，我们还设计了目标变异策略，以在三种不同设置下测试TDD方法的鲁棒性。这些变异策略基于成熟的Type-1到Type-4代码克隆检测分类法。我们的研究对当前代码TDD技术进行了系统评估，并为未来开发更有效和鲁棒的检测方法提供了见解。", "summary": "本研究探讨了代码大型语言模型（CodeLLMs）中训练数据检测（TDD）的关键问题，旨在解决模型输出中可能出现的专有或敏感代码片段带来的隐私和知识产权风险。鉴于现有TDD方法在代码数据上的有效性尚未充分验证，研究团队对七种前沿TDD方法在源代码上进行了全面的实证评估，并引入了CodeSnitch这一包含9,000个带标签代码样本的基准数据集。此外，通过基于代码克隆检测分类的变异策略，测试了TDD方法的鲁棒性。研究结果为当前代码TDD技术提供了系统性评估，并为未来开发更高效、更稳健的检测方法指明了方向。", "keywords": "训练数据检测, 代码大型语言模型, CodeSnitch, 代码克隆, 知识产权", "comments": "这项研究通过引入专门针对代码数据的基准数据集CodeSnitch和基于代码克隆分类的变异策略，填补了训练数据检测（TDD）在代码领域研究的空白，具有重要的创新性。它系统性地评估了现有TDD方法在CodeLLMs中的表现，为解决CodeLLMs输出中潜在的知识产权和隐私问题提供了关键见解，对CodeLLMs的负责任部署具有重要意义。"}}
{"id": "2507.16924", "title": "Fast Distribution Grid Topology Estimation via Subset Sum", "authors": ["Yueyao Xu", "Yize Chen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted at 2025 IEEE Electrical Power and Energy Conference (EPEC) Canada, code available at this https URL", "url": "http://arxiv.org/abs/2507.16924v1", "summary": "Faced with increasing penetration of distributed energy resources and fast\ndevelopment of distribution grid energy management, topology identification of\ndistribution grid becomes an important and fundamental task. As the underlying\ngrid topology is usually unknown or incomplete to the utilities, it is becoming\na fundamental task to efficiently identify the distribution grid network\ntopology using limited measurements. A fast and accurate topology\nidentification can help achieving the tasks of load monitoring, operation and\ncontrol of power distribution system as well as outage detection. In this\npaper, we propose a novel and ultra-fast topology identification method. By\nadapting the subset sum method with a hierarchical structure, the overall grid\ntopology can be inferred from fewer samples of smart meter power measurements.\nSuch techniques can be applied in real time under the scenarios with fast\ntopology change, and the proposed hierarchical algorithm is also robust against\nmeasurement noises.", "comment": "Accepted at 2025 IEEE Electrical Power and Energy Conference (EPEC)\n  Canada, code available at https://github.com/chennnnnyize/HSSP", "pdf_url": "http://arxiv.org/pdf/2507.16924v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于子集和的快速配电网拓扑估计", "tldr": "提出一种基于分层子集和的快速鲁棒方法，利用少量测量数据识别配电网拓扑。", "motivation": "随着分布式能源渗透率的增加和配电网能源管理的快速发展，配电网拓扑识别成为一项重要而基础的任务。由于底层电网拓扑通常未知或不完整，因此需要高效识别配电网拓扑以实现负荷监测、系统运行控制和故障检测等目标。", "method": "提出了一种新颖的超快速拓扑识别方法。该方法通过将子集和方法与分层结构相结合，能够利用更少的智能电表功率测量样本推断出整体电网拓扑。", "result": "所提出的分层算法可以在拓扑快速变化的场景下实时应用，并且对测量噪声具有鲁棒性。", "conclusion": "该论文得出结论，所提出的分层子集和方法为配电网拓扑识别提供了一种快速、准确且鲁棒的解决方案，即使在测量数据有限和动态环境下也能适用。", "translation": "面对分布式能源渗透率的增加和配电网能源管理的快速发展，配电网拓扑识别成为一项重要且基础的任务。由于底层电网拓扑对于电力公司来说通常是未知或不完整的，因此利用有限的测量数据高效识别配电网网络拓扑正成为一项基本任务。快速准确的拓扑识别有助于实现电力分配系统的负荷监测、运行控制以及故障检测等任务。在本文中，我们提出了一种新颖且超快速的拓扑识别方法。通过采用具有分层结构的子集和方法，可以从更少的智能电表功率测量样本中推断出整体电网拓扑。这种技术可以在拓扑快速变化的场景下实时应用，并且所提出的分层算法对测量噪声也具有鲁棒性。", "summary": "该论文旨在解决由于分布式能源增加和能源管理发展而导致的配电网拓扑识别的迫切需求。它提出了一种新颖的超快速方法，该方法将子集和方法与分层结构相结合。这项技术能够利用更少的智能电表测量数据推断电网拓扑，可实时应用于快速拓扑变化的场景，并且对测量噪声具有鲁棒性，有助于负荷监测和故障检测等任务。", "keywords": "配电网, 拓扑估计, 子集和, 分层结构, 智能电表测量", "comments": "该论文的创新之处在于将子集和方法与分层结构相结合应用于拓扑估计，从而在有限数据下实现了高速和鲁棒性。这对于具有动态特性的现代配电网至关重要。"}}
{"id": "2507.17687", "title": "Towards Effective Open-set Graph Class-incremental Learning", "authors": ["Jiazhen Chen", "Zheng Ma", "Sichao Fu", "Mingbin Feng", "Tony S. Wirjanto", "Weihua Ou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by 33rd ACM International Conference on Multimedia (MM 2025)", "url": "http://arxiv.org/abs/2507.17687v1", "summary": "Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)\nto adapt to evolving graph analytical tasks by incrementally learning new class\nknowledge while retaining knowledge of old classes. Existing GCIL methods\nprimarily focus on a closed-set assumption, where all test samples are presumed\nto belong to previously known classes. Such an assumption restricts their\napplicability in real-world scenarios, where unknown classes naturally emerge\nduring inference, and are absent during training. In this paper, we explore a\nmore challenging open-set graph class-incremental learning scenario with two\nintertwined challenges: catastrophic forgetting of old classes, which impairs\nthe detection of unknown classes, and inadequate open-set recognition, which\ndestabilizes the retention of learned knowledge. To address the above problems,\na novel OGCIL framework is proposed, which utilizes pseudo-sample embedding\ngeneration to effectively mitigate catastrophic forgetting and enable robust\ndetection of unknown classes. To be specific, a prototypical conditional\nvariational autoencoder is designed to synthesize node embeddings for old\nclasses, enabling knowledge replay without storing raw graph data. To handle\nunknown classes, we employ a mixing-based strategy to generate\nout-of-distribution (OOD) samples from pseudo in-distribution and current node\nembeddings. A novel prototypical hypersphere classification loss is further\nproposed, which anchors in-distribution embeddings to their respective class\nprototypes, while repelling OOD embeddings away. Instead of assigning all\nunknown samples into one cluster, our proposed objective function explicitly\nmodels them as outliers through prototype-aware rejection regions, ensuring a\nrobust open-set recognition. Extensive experiments on five benchmarks\ndemonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN\nmethods.", "comment": "Accepted by 33rd ACM International Conference on Multimedia (MM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17687v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "迈向有效的开放集图类增量学习", "tldr": "现有图类增量学习（GCIL）方法在开放集场景下表现不佳，该论文提出OGCIL框架，通过伪样本生成和新颖的损失函数来缓解灾难性遗忘并有效识别未知类别，从而实现鲁棒的开放集图类增量学习。", "motivation": "现有的图类增量学习（GCIL）方法主要基于闭集假设，限制了它们在实际应用中的普适性，因为在推理过程中未知类别会自然出现。本研究旨在探索更具挑战性的开放集图类增量学习场景，解决旧类别的灾难性遗忘和开放集识别不足这两个相互关联的挑战。", "method": "本文提出了一种新颖的OGCIL框架。该框架利用伪样本嵌入生成来有效缓解灾难性遗忘并实现对未知类别的鲁棒检测。具体而言，设计了一个原型条件变分自编码器来合成旧类别的节点嵌入，从而在不存储原始图数据的情况下实现知识回放。为了处理未知类别，采用了一种基于混合的策略，从伪分布内和当前节点嵌入中生成分布外（OOD）样本。此外，还提出了一种新颖的原型超球面分类损失，该损失将分布内嵌入锚定到各自的类别原型，同时排斥OOD嵌入。通过原型感知拒绝区域，将未知样本明确建模为异常值，确保鲁棒的开放集识别。", "result": "在五个基准数据集上的大量实验表明，OGCIL相对于现有GCIL和开放集GNN方法表现出更优的有效性。", "conclusion": "OGCIL框架通过有效缓解灾难性遗忘和提升开放集识别能力，成功应对了开放集图类增量学习中的挑战，并在实验中展现出优越的性能。", "translation": "图类增量学习（GCIL）允许图神经网络（GNNs）通过增量学习新类别知识同时保留旧类别知识来适应不断演变的图分析任务。现有GCIL方法主要关注闭集假设，即所有测试样本都被假定属于先前已知的类别。这种假设限制了它们在现实世界场景中的适用性，因为在推理过程中未知类别会自然出现，并且在训练期间不存在。在本文中，我们探索了一个更具挑战性的开放集图类增量学习场景，其中包含两个相互关联的挑战：旧类别的灾难性遗忘，这会损害未知类别的检测；以及开放集识别不足，这会破坏已学习知识的保留。为了解决上述问题，本文提出了一种新颖的OGCIL框架，该框架利用伪样本嵌入生成来有效缓解灾难性遗忘并实现对未知类别的鲁棒检测。具体而言，设计了一个原型条件变分自编码器来合成旧类别的节点嵌入，从而在不存储原始图数据的情况下实现知识回放。为了处理未知类别，我们采用了一种基于混合的策略，从伪分布内和当前节点嵌入中生成分布外（OOD）样本。此外，还提出了一种新颖的原型超球面分类损失，该损失将分布内嵌入锚定到各自的类别原型，同时排斥OOD嵌入。与将所有未知样本分配到一个簇不同，我们提出的目标函数通过原型感知拒绝区域将它们明确建模为异常值，确保鲁棒的开放集识别。在五个基准数据集上的大量实验表明OGCIL相对于现有GCIL和开放集GNN方法表现出更优的有效性。", "summary": "本文针对现有闭集图类增量学习（GCIL）方法的局限性，提出了一个新颖的OGCIL框架，以应对开放集图类增量学习中的灾难性遗忘和开放集识别不足问题。OGCIL通过原型条件变分自编码器生成旧类别的伪样本嵌入以缓解遗忘，并采用基于混合的策略生成分布外（OOD）样本。同时，引入原型超球面分类损失，通过原型感知拒绝区域实现对未知类别的鲁棒识别。在五个基准数据集上的实验验证了OGCIL的有效性。", "keywords": "图类增量学习, 开放集识别, 灾难性遗忘, 伪样本生成, 图神经网络", "comments": "该论文创新性地解决了图类增量学习中开放集场景的关键空白，这对于实际应用具有高度相关性。它通过伪样本生成来解决旧类别的知识保留问题，并通过复杂的OOD检测机制（特别是原型感知拒绝区域）来处理新/未知类别，这种双重方法是其重要的贡献。"}}
{"id": "2502.11046", "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing", "authors": ["Zhao Wang", "Yiqi Chen", "Cong Li", "Dimin Niu", "Tianchan Guan", "Zhaoyang Du", "Xingda Wei", "Guangyu Sun"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11046v2", "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11046v2", "cate": "cs.AR", "date": "2025-02-16", "updated": "2025-07-23", "AI": {"title_translation": "在CXL内存共享上实现高效事务处理", "tldr": "该论文提出并评估了CtXnL，一个软硬件协同设计的系统，它通过混合一致性原语解决了CXL在事务处理中直接应用导致性能下降的问题，在OLTP工作负载下，性能优于现有网络系统，并比普通CXL内存共享架构的吞吐量高出2.08倍。", "motivation": "当前多节点事务处理系统因网络开销而缓慢。虽然CXL提供了低延迟、缓存一致的共享内存访问，但直接使用标准CXL原语会导致性能下降，因为维护跨节点缓存一致性的成本很高。", "method": "本文引入了CtXnL，一个软硬件协同设计的系统，它实现了一种新颖的混合一致性原语，专为事务数据的松散一致性特性量身定制。CtXnL的核心创新在于赋予事务系统开发者选择性实现数据一致性的能力。", "result": "在OLTP工作负载上的评估表明，CtXnL提高了性能，优于当前基于网络的系统，并且在通用事务处理策略下，比普通的CXL内存共享架构的吞吐量高出高达2.08倍。", "conclusion": "CtXnL通过引入一种混合一致性原语和选择性数据一致性能力，有效解决了CXL在事务处理中遇到的性能挑战，显著提升了系统吞吐量，超越了传统网络方案和直接CXL方案。", "translation": "事务处理系统是现代数据中心应用的核心，然而当前的多节点系统由于网络开销而缓慢。本文倡导将Compute Express Link (CXL) 作为一种网络替代方案，它能够实现低延迟和缓存一致的共享内存访问。然而，直接采用标准CXL原语会导致性能下降，因为维护跨节点缓存一致性的成本很高。为了解决CXL的挑战，本文介绍了CtXnL，一个软硬件协同设计的系统，它实现了一种新颖的混合一致性原语，专为事务数据的松散一致性特性量身定制。CtXnL的核心创新在于赋予事务系统开发者选择性实现数据一致性的能力。我们对OLTP工作负载的评估表明，CtXnL提高了性能，优于当前基于网络的系统，并且在通用事务处理策略下，比普通的CXL内存共享架构的吞吐量高出高达2.08倍。", "summary": "本文针对现有事务处理系统因网络开销导致性能瓶颈的问题，提出利用CXL作为替代方案。鉴于直接使用CXL会导致高昂的缓存一致性维护成本，论文设计了CtXnL，一个软硬件协同系统。CtXnL引入了一种混合一致性原语，允许事务系统开发者选择性地实现数据一致性，以适应事务数据的松散一致性需求。实验结果表明，CtXnL在OLTP工作负载下显著提升了性能，超越了传统网络方案，并比直接CXL内存共享架构的吞吐量提高了2.08倍。", "keywords": "CXL, 事务处理, 内存共享, 缓存一致性, CtXnL", "comments": "该论文解决了数据中心事务处理的关键性能瓶颈，创新性地将CXL技术应用于此领域。其软硬件协同设计以及为事务数据特性定制的混合一致性原语是核心亮点，特别是选择性数据一致性的概念，对于优化性能至关重要。取得的显著性能提升（高达2.08倍吞吐量）证明了该方案的有效性和潜在影响力。"}}
{"id": "2506.15610", "title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion", "authors": ["Yuqing Lan", "Chenyang Zhu", "Zhirui Gao", "Jiazhao Zhang", "Yihan Cao", "Renjiao Yi", "Yijie Wang", "Kai Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2506.15610v2", "summary": "Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.", "comment": "Project page: https://lanlan96.github.io/BoxFusion/", "pdf_url": "http://arxiv.org/pdf/2506.15610v2", "cate": "cs.CV", "date": "2025-06-18", "updated": "2025-07-23", "AI": {"title_translation": "BoxFusion：通过实时多视图包围盒融合实现无重建的开放词汇3D目标检测", "tldr": "BoxFusion提出了一种无重建的在线框架，通过多视图包围盒融合，实现了实时、内存高效的开放词汇3D目标检测。", "motivation": "现有的开放词汇3D目标检测方法通常依赖于密集的点云重建，这导致了高计算开销和内存限制，阻碍了在下游任务中的实时部署。", "method": "论文提出了一种新颖的无重建在线框架BoxFusion。它接收流式RGB-D视频输入，利用预训练的视觉基础模型Cubify Anything进行单视图3D目标检测（通过包围盒），并结合CLIP捕获开放词汇语义。为了融合不同视图中检测到的包围盒，该方法采用一个关联模块（使用3D NMS和包围盒对应匹配）和一个优化模块（使用基于粒子滤波的IoU引导高效随机优化技术）来融合同一实例在多视图中预测的3D包围盒。", "result": "在ScanNetV2和CA-1M数据集上的大量实验表明，该方法在在线方法中达到了最先进的性能。", "conclusion": "BoxFusion的无重建范式使其在各种场景中展现出出色的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "translation": "开放词汇3D目标检测因其在自动驾驶和具身AI中的关键应用而受到广泛关注。现有的检测方法，无论是离线还是在线的，通常依赖于密集的点云重建，这带来了大量的计算开销和内存限制，阻碍了在下游任务中的实时部署。为了解决这个问题，我们提出了一种新颖的无重建在线框架，专门用于内存高效和实时的3D检测。具体来说，给定流式姿态RGB-D视频输入，我们利用Cubify Anything作为预训练的视觉基础模型（VFM）进行单视图3D目标检测（通过包围盒），并结合CLIP捕获检测到对象的开放词汇语义。为了将所有检测到的包围盒在不同视图之间融合为一个统一的包围盒，我们采用了一个关联模块来处理多视图的对应关系，以及一个优化模块来融合在多视图中预测的同一实例的3D包围盒。关联模块利用3D非最大抑制（NMS）和包围盒对应匹配模块，而优化模块则使用基于粒子滤波的IoU引导高效随机优化技术，以强制执行3D包围盒的多视图一致性，同时最大限度地减少计算复杂度。在ScanNetV2和CA-1M数据集上的大量实验表明，我们的方法在在线方法中达到了最先进的性能。受益于这种新颖的无重建3D目标检测范式，我们的方法在各种场景中展现出强大的泛化能力，甚至在超过1000平方米的环境中也能实现实时感知。", "summary": "本文提出了BoxFusion，一个新颖的无重建在线框架，用于实时、内存高效的开放词汇3D目标检测。该方法利用Cubify Anything和CLIP进行单视图检测，并通过一个关联模块（3D NMS和匹配）和一个优化模块（IoU引导的粒子滤波）将多视图包围盒融合。实验证明，BoxFusion在ScanNetV2和CA-1M数据集上实现了SOTA性能，并在大场景中展现出卓越的泛化能力和实时感知能力。", "keywords": "开放词汇3D目标检测, 无重建, 多视图融合, 实时检测, 包围盒融合", "comments": "BoxFusion的创新之处在于其“无重建”的范式，这显著降低了计算和内存需求，使其能够实现实时3D目标检测，尤其适用于自动驾驶和具身AI等对实时性要求高的应用。其结合预训练VFM和CLIP进行开放词汇检测，并采用高效的多视图包围盒融合策略，是其核心贡献。该方法在泛化能力和处理大规模环境方面的表现也值得关注。"}}
{"id": "2507.17257", "title": "Agent Identity Evals: Measuring Agentic Identity", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17257v1", "summary": "Central to agentic capability and trustworthiness of language model agents\n(LMAs) is the extent they maintain stable, reliable, identity over time.\nHowever, LMAs inherit pathologies from large language models (LLMs)\n(statelessness, stochasticity, sensitivity to prompts and\nlinguistically-intermediation) which can undermine their identifiability,\ncontinuity, persistence and consistency. This attrition of identity can erode\ntheir reliability, trustworthiness and utility by interfering with their\nagentic capabilities such as reasoning, planning and action. To address these\nchallenges, we introduce \\textit{agent identity evals} (AIE), a rigorous,\nstatistically-driven, empirical framework for measuring the degree to which an\nLMA system exhibit and maintain their agentic identity over time, including\ntheir capabilities, properties and ability to recover from state perturbations.\nAIE comprises a set of novel metrics which can integrate with other measures of\nperformance, capability and agentic robustness to assist in the design of\noptimal LMA infrastructure and scaffolding such as memory and tools. We set out\nformal definitions and methods that can be applied at each stage of the LMA\nlife-cycle, and worked examples of how to apply them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17257v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "智能体身份评估：衡量智能体身份", "tldr": "语言模型智能体（LMAs）因继承大型语言模型（LLMs）的特性而难以保持稳定身份，这影响了它们的可靠性和可信度。本文引入了“智能体身份评估”（AIE），这是一个严格的框架，用于衡量和维护LMA的智能体身份，从而提高其性能和实用性。", "motivation": "语言模型智能体（LMAs）难以随时间保持稳定、可靠的身份，这是因为它们继承了大型语言模型（LLMs）的病态（如无状态性、随机性、对提示的敏感性），这损害了它们的身份识别性、连续性、持久性和一致性，进而侵蚀了它们的可靠性、可信度、实用性及其推理、规划和行动等智能体能力。", "method": "本文引入了“智能体身份评估”（AIE），这是一个严格、统计驱动的实证框架，包含一套新颖的指标，用于衡量LMA系统随时间展现和保持其智能体身份的程度，包括其能力、属性以及从状态扰动中恢复的能力。AIE旨在与性能、能力和智能体鲁棒性的其他测量相结合，并提供了可在LMA生命周期每个阶段应用的正式定义和方法，以及应用示例。", "result": "Not mentioned in abstract", "conclusion": "AIE框架为衡量LMA的智能体身份提供了一种解决方案，这对于设计优化LMA基础设施至关重要，能够通过解决身份损耗问题来提升它们的可靠性、可信度和实用性。", "translation": "语言模型智能体（LMAs）的智能体能力和可信度的核心是它们能否随时间保持稳定、可靠的身份。然而，LMAs继承了大型语言模型（LLMs）的病态（无状态性、随机性、对提示的敏感性以及语言中介性），这可能损害它们的身份识别性、连续性、持久性和一致性。这种身份的损耗会通过干扰其推理、规划和行动等智能体能力，从而侵蚀其可靠性、可信度和实用性。为了应对这些挑战，我们引入了“智能体身份评估”（AIE），这是一个严格、统计驱动的实证框架，用于衡量LMA系统随时间展现和保持其智能体身份的程度，包括它们的能力、属性以及从状态扰动中恢复的能力。AIE包含一套新颖的指标，可以与其他性能、能力和智能体鲁棒性测量相结合，以协助设计最佳的LMA基础设施和支架，如记忆和工具。我们提出了可以在LMA生命周期每个阶段应用的正式定义和方法，以及如何应用它们的示例。", "summary": "本文关注语言模型智能体（LMAs）维持稳定智能体身份的关键挑战，该问题常因大型语言模型（LLMs）固有的无状态性和随机性等限制而加剧。为解决此问题，论文提出了“智能体身份评估”（AIE），这是一个统计驱动的实证框架，包含新颖的度量标准，旨在严格衡量和跟踪LMA随时间变化的身份，包括其属性、能力以及从扰动中恢复的能力。AIE旨在通过提供优化LMA基础设施设计的工具，从而提高LMA的可靠性、可信度和实用性。", "keywords": "智能体身份, 语言模型智能体, 身份评估, 可信度, 可靠性", "comments": "该论文引入了一个新颖且重要的框架（AIE），以解决LMA开发中的一个基本挑战：保持稳定的智能体身份。这对于构建可靠和值得信赖的AI智能体至关重要。论文强调的“严格、统计驱动的实证框架”和“新颖指标”表明其方法论贡献强大。AIE与其他性能测量相结合以及在LMA生命周期中应用的潜力，突显了其实用价值。"}}
{"id": "2412.20936", "title": "Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors", "authors": ["Aaqib Zahoor", "Iqra Altaf Gillani", "Janib ul Bashir"], "categories": ["cs.SI", "physics.comp-ph"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.20936v2", "summary": "Influence maximization in temporal social networks presents unique challenges\ndue to the dynamic interactions that evolve over time. Traditional diffusion\nmodels often fall short in capturing the real-world complexities of\nactive-inactive transitions among nodes, obscuring the true behavior of\ninfluence spread. In dynamic networks, nodes do not simply transition to an\nactive state once; rather, they can oscillate between active and inactive\nstates, with the potential for reactivation and reinforcement over time. This\nreactivation allows previously influenced nodes to regain influence potency,\nenhancing their ability to spread influence to others and amplifying the\noverall diffusion process. Ignoring these transitions can thus conceal the\ncumulative impact of influence, making it essential to account for them in any\neffective diffusion model. To address these challenges, we introduce the\nContinuous Persistent Susceptible-Infected Model with Reinforcement and\nRe-activation (cpSI-R), which explicitly incorporates active-inactive\ntransitions, capturing the progressive reinforcement that makes nodes more\npotent spreaders upon reactivation. This model naturally leads to a submodular\nand monotone objective function, which supports efficient optimization for seed\nselection in influence maximization tasks. Alongside cpSI-R, we propose an\nefficient temporal snapshot sampling method, simplifying the analysis of\nevolving networks. We then adapt the prior algorithms of seed selection to our\nmodel and sampling strategy, resulting in reduced computational costs and\nenhanced seed selection efficiency. Experimental evaluations on diverse\ndatasets demonstrate substantial improvements in performance over baseline\nmethods, underscoring the effectiveness of cpSI-R for real-world temporal\nnetworks", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.20936v2", "cate": "cs.SI", "date": "2024-12-30", "updated": "2025-07-22", "AI": {"title_translation": "时间网络中具有持续和反应行为的影响力最大化", "tldr": "本文提出了cpSI-R模型和一种高效的时间快照采样方法，以解决时间网络中影响力最大化问题，该模型考虑了节点在活跃和非活跃状态之间的转换以及再激活和强化，并在实验中表现出显著的性能提升。", "motivation": "传统影响力扩散模型难以捕捉时间社交网络中节点活跃-非活跃状态的复杂转换，忽略了节点在活跃和非活跃状态之间振荡以及再激活和强化的能力，这会掩盖影响力的累积效应。", "method": "本文引入了连续持久易感-感染强化再激活模型（cpSI-R），该模型明确包含了活跃-非活跃转换，并捕捉了节点再激活后传播能力增强的累进强化过程。cpSI-R模型具有次模和单调目标函数，支持高效的种子选择优化。此外，本文还提出了一种高效的时间快照采样方法，并调整了现有种子选择算法以降低计算成本并提高效率。", "result": "在不同数据集上的实验评估表明，与基线方法相比，cpSI-R模型在性能上有显著提升，这突显了其在真实世界时间网络中的有效性。", "conclusion": "cpSI-R模型通过明确考虑活跃-非活跃转换、再激活和强化，有效解决了时间网络中影响力最大化的挑战，并在实际应用中表现出优越的性能。", "translation": "时间社交网络中的影响力最大化由于随时间演变的动态交互而带来了独特的挑战。传统的扩散模型往往无法捕捉节点间活跃-非活跃转换的真实世界复杂性，从而掩盖了影响力传播的真实行为。在动态网络中，节点不仅仅是简单地过渡到活跃状态一次；相反，它们可以在活跃和非活跃状态之间振荡，并随着时间的推移具有再激活和强化的潜力。这种再激活使得先前受影响的节点能够重新获得影响力效力，增强它们向他人传播影响力的能力，并放大整体扩散过程。因此，忽略这些转换会掩盖影响力的累积影响，使其在任何有效的扩散模型中都必须加以考虑。为了解决这些挑战，我们引入了连续持久易感-感染强化再激活模型（cpSI-R），该模型明确包含了活跃-非活跃转换，捕捉了使节点在再激活时成为更有效传播者的渐进强化。该模型自然地导致了一个次模和单调的目标函数，支持影响力最大化任务中种子选择的有效优化。除了cpSI-R，我们还提出了一种高效的时间快照采样方法，简化了演化网络的分析。然后，我们将先前的种子选择算法适应我们的模型和采样策略，从而降低了计算成本并提高了种子选择效率。在不同数据集上的实验评估表明，与基线方法相比，性能有显著提升，这突出了cpSI-R在真实世界时间网络中的有效性。", "summary": "本文针对时间社交网络中影响力最大化问题，提出了一种名为cpSI-R的新型扩散模型。该模型特别关注节点在活跃和非活跃状态之间的转换、再激活以及影响力增强，以更准确地反映真实世界的复杂性。为提高效率，研究还引入了一种时间快照采样方法，并优化了种子选择算法。实验结果表明，cpSI-R在多种数据集上均优于现有基线方法，证实了其在处理动态网络影响力传播方面的有效性。", "keywords": "影响力最大化, 时间网络, 扩散模型, 再激活, cpSI-R", "comments": "该论文的创新之处在于提出了cpSI-R模型，它首次明确地将节点在时间网络中活跃-非活跃状态之间的振荡、再激活和强化纳入影响力扩散模型中，这比传统模型更能捕捉真实世界的复杂性。模型的次模和单调特性也确保了优化的可行性。此外，提出的高效采样方法和算法适应性增强了实际应用中的效率。这项工作对于理解和预测动态社交网络中的影响力传播具有重要意义。"}}
{"id": "2507.17661", "title": "Monocular Semantic Scene Completion via Masked Recurrent Networks", "authors": ["Xuzhi Wang", "Xinran Wu", "Song Wang", "Lingdong Kong", "Ziping Zhao"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025; 15 pages, 10 figures, 6 tables; Code at this https URL", "url": "http://arxiv.org/abs/2507.17661v1", "summary": "Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise\noccupancy and semantic category from a single-view RGB image. Existing methods\nadopt a single-stage framework that aims to simultaneously achieve visible\nregion segmentation and occluded region hallucination, while also being\naffected by inaccurate depth estimation. Such methods often achieve suboptimal\nperformance, especially in complex scenes. We propose a novel two-stage\nframework that decomposes MSSC into coarse MSSC followed by the Masked\nRecurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent\nUnit (MS-GRU) which concentrates on the occupied regions by the proposed mask\nupdating mechanism, and a sparse GRU design is proposed to reduce the\ncomputation cost. Additionally, we propose the distance attention projection to\nreduce projection errors by assigning different attention scores according to\nthe distance to the observed surface. Experimental results demonstrate that our\nproposed unified framework, MonoMRN, effectively supports both indoor and\noutdoor scenes and achieves state-of-the-art performance on the NYUv2 and\nSemanticKITTI datasets. Furthermore, we conduct robustness analysis under\nvarious disturbances, highlighting the role of the Masked Recurrent Network in\nenhancing the model's resilience to such challenges. The source code is\npublicly available.", "comment": "ICCV 2025; 15 pages, 10 figures, 6 tables; Code at\n  https://github.com/alanWXZ/MonoMRN", "pdf_url": "http://arxiv.org/pdf/2507.17661v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过掩码循环网络的单目语义场景补全", "tldr": "该论文提出了一种名为MonoMRN的新型两阶段框架，用于单目语义场景补全（MSSC），通过引入掩码循环网络和距离注意力投影来解决现有方法的局限性，并在室内外场景中实现了最先进的性能和鲁棒性。", "motivation": "现有单目语义场景补全（MSSC）方法采用单阶段框架，同时进行可见区域分割和遮挡区域幻觉，并受不准确的深度估计影响，导致在复杂场景中性能不佳。", "method": "本文提出了一种新颖的两阶段框架，将MSSC分解为粗略MSSC，然后是掩码循环网络。具体来说，提出了掩码稀疏门控循环单元（MS-GRU），通过所提出的掩码更新机制集中于占据区域，并设计了稀疏GRU以降低计算成本。此外，还提出了距离注意力投影，通过根据到观察表面的距离分配不同的注意力分数来减少投影误差。", "result": "实验结果表明，所提出的统一框架MonoMRN有效支持室内和室外场景，并在NYUv2和SemanticKITTI数据集上实现了最先进的性能。此外，在各种干扰下进行了鲁棒性分析，突出了掩码循环网络在增强模型对这些挑战的弹性方面的作用。", "conclusion": "本文提出的统一框架MonoMRN在单目语义场景补全任务中表现出色，不仅在室内外场景中实现了最先进的性能，而且通过掩码循环网络提高了模型的鲁棒性。", "translation": "单目语义场景补全（MSSC）旨在从单视图RGB图像中预测体素级的占据和语义类别。现有方法采用单阶段框架，旨在同时实现可见区域分割和遮挡区域幻觉，同时还受到不准确深度估计的影响。此类方法通常性能不佳，尤其是在复杂场景中。我们提出了一种新颖的两阶段框架，将MSSC分解为粗略MSSC，然后是掩码循环网络。具体来说，我们提出了掩码稀疏门控循环单元（MS-GRU），它通过所提出的掩码更新机制集中于占据区域，并提出了稀疏GRU设计以降低计算成本。此外，我们提出了距离注意力投影，通过根据到观察表面的距离分配不同的注意力分数来减少投影误差。实验结果表明，我们提出的统一框架MonoMRN有效支持室内和室外场景，并在NYUv2和SemanticKITTI数据集上实现了最先进的性能。此外，我们还在各种干扰下进行了鲁棒性分析，突出了掩码循环网络在增强模型对这些挑战的弹性方面的作用。源代码已公开。", "summary": "该论文介绍了一种名为MonoMRN的新型两阶段框架，用于单目语义场景补全（MSSC）。为了克服现有单阶段方法在处理可见区域分割、遮挡区域幻觉和不准确深度估计方面的不足，MonoMRN首先进行粗略MSSC，然后应用掩码循环网络。核心创新包括掩码稀疏门控循环单元（MS-GRU），它通过掩码更新机制专注于占据区域并减少计算成本，以及距离注意力投影，通过加权注意力减少投影误差。实验证明，MonoMRN在室内外场景中均表现出色，并在NYUv2和SemanticKITTI数据集上达到了最先进的性能，同时展示了其在各种干扰下的鲁棒性。", "keywords": "单目语义场景补全, 掩码循环网络, 两阶段框架, 稀疏GRU, 距离注意力投影", "comments": "本文提出了一种新颖的两阶段框架，有效地解决了单目语义场景补全中现有方法的局限性。其创新点在于引入了掩码循环网络，特别是掩码稀疏门控循环单元（MS-GRU）和距离注意力投影，这些设计不仅提高了性能，还增强了模型的鲁棒性。稀疏GRU的设计也体现了对计算效率的考量。该工作在复杂场景下的性能提升和对室内外场景的普适性使其具有重要意义。"}}
{"id": "2507.17420", "title": "CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography", "authors": ["Sneha George Gnanakalavathy", "Hairil Abdul Razak", "Robert Meertens", "Jonathan E. Fieldsend", "Xujiong Ye", "Mohammed M. Abdelsamea"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17420v1", "summary": "In computed tomography (CT), achieving high image quality while minimizing\nradiation exposure remains a key clinical challenge. This paper presents\nCAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and\nPredictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT\nintegrates image data with acquisition metadata (such as tube voltage, tube\ncurrent, and contrast agent types) to model the underlying causal relationships\nthat influence image quality. An ensemble of Variational Autoencoders (VAEs) is\nemployed to extract meaningful features and generate causal representations\nfrom observational data, including CT images and associated imaging parameters.\nThese input features are fused to predict the Signal-to-Noise Ratio (SNR) and\nsupport counterfactual inference, enabling what-if simulations, such as changes\nin contrast agents (types and concentrations) or scan parameters. CAPRI-CT is\ntrained and validated using an ensemble learning approach, achieving strong\npredictive performance. By facilitating both prediction and interpretability,\nCAPRI-CT provides actionable insights that could help radiologists and\ntechnicians design more efficient CT protocols without repeated physical scans.\nThe source code and dataset are publicly available at\nhttps://github.com/SnehaGeorge22/capri-ct.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17420v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CAPRI-CT：计算机断层扫描图像质量优化的因果分析与预测推理", "tldr": "CAPRI-CT是一个新的因果感知深度学习框架，用于在CT成像中优化图像质量，通过整合图像数据和采集元数据来建模因果关系，并实现预测和反事实推理。", "motivation": "在计算机断层扫描（CT）中，在最大限度减少辐射暴露的同时实现高质量图像是一个关键的临床挑战。", "method": "本研究提出了CAPRI-CT，一个新颖的因果感知深度学习框架，用于CT图像质量优化的因果分析和预测推理。CAPRI-CT将图像数据与采集元数据（如管电压、管电流和造影剂类型）整合，以建模影响图像质量的潜在因果关系。采用变分自动编码器（VAEs）集成来提取有意义的特征并从观测数据（包括CT图像和相关成像参数）生成因果表示。这些输入特征被融合以预测信噪比（SNR）并支持反事实推理，从而实现“假设”模拟，例如造影剂（类型和浓度）或扫描参数的变化。CAPRI-CT采用集成学习方法进行训练和验证。", "result": "CAPRI-CT在训练和验证中实现了强大的预测性能。通过促进预测和可解释性，CAPRI-CT提供了可操作的见解。", "conclusion": "CAPRI-CT提供了可操作的见解，可以帮助放射科医生和技术人员设计更高效的CT协议，而无需重复的物理扫描。", "translation": "在计算机断层扫描（CT）中，在最大限度减少辐射暴露的同时实现高质量图像仍然是一个关键的临床挑战。本文提出了CAPRI-CT，一个新颖的因果感知深度学习框架，用于CT成像中图像质量优化的因果分析和预测推理。CAPRI-CT将图像数据与采集元数据（如管电压、管电流和造影剂类型）整合，以建模影响图像质量的潜在因果关系。采用变分自动编码器（VAEs）集成来提取有意义的特征并从观测数据（包括CT图像和相关成像参数）生成因果表示。这些输入特征被融合以预测信噪比（SNR）并支持反事实推理，从而实现“假设”模拟，例如造影剂（类型和浓度）或扫描参数的变化。CAPRI-CT采用集成学习方法进行训练和验证，实现了强大的预测性能。通过促进预测和可解释性，CAPRI-CT提供了可操作的见解，可以帮助放射科医生和技术人员设计更高效的CT协议，而无需重复的物理扫描。源代码和数据集已在https://github.com/SnehaGeorge22/capri-ct 公开。", "summary": "CAPRI-CT是一个新颖的因果感知深度学习框架，旨在优化计算机断层扫描（CT）中的图像质量，同时最小化辐射暴露。它通过整合CT图像和采集元数据（如管电压、管电流和造影剂类型）来建模影响图像质量的因果关系。该框架利用变分自动编码器（VAEs）集成来提取特征并生成因果表示，从而预测信噪比（SNR）并支持反事实推理，实现“假设”模拟。CAPRI-CT通过集成学习进行训练和验证，展现出强大的预测性能，并为CT协议设计提供了可操作的见解。", "keywords": "CT图像质量优化, 因果分析, 深度学习, 预测推理, 变分自动编码器", "comments": "CAPRI-CT的创新之处在于其将因果分析与深度学习相结合，以优化CT图像质量，这有助于在减少辐射暴露的同时提高诊断准确性。该框架通过整合图像数据和采集元数据，并利用VAEs进行特征提取和因果表示生成，提供了一种新颖的方法来理解和预测CT参数对图像质量的影响。其支持反事实推理的能力尤其重要，因为它允许医疗专业人员在不进行实际扫描的情况下模拟不同协议的效果，从而提高CT协议设计的效率和安全性。开源代码和数据集的提供也增强了其在研究和临床应用中的潜在影响力。"}}
{"id": "2507.16727", "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints", "authors": ["Zhenyun Yin", "Shujie Wang", "Xuhong Wang", "Xingjun Ma", "Yinchun Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      The inconsistency of base models undermines the fairness of evaluation comparisons and affects the validity of the paper's conclusions", "url": "http://arxiv.org/abs/2507.16727v2", "summary": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.", "comment": "The inconsistency of base models undermines the fairness of\n  evaluation comparisons and affects the validity of the paper's conclusions", "pdf_url": "http://arxiv.org/pdf/2507.16727v2", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "审慎搜索器：通过带约束的强化学习提高LLM的可靠性", "tldr": "提出“审慎搜索器”框架，结合置信度校准和检索式搜索，通过强化学习提高LLM在开放域问答中的可靠性。", "motivation": "提高大型语言模型（LLMs）的可靠性对于它们在现实世界场景中的部署至关重要。", "method": "本文提出了“Deliberative Searcher”框架，这是第一个将确定性校准与基于检索的搜索集成到开放域问答中的框架。该智能体对维基百科数据进行多步反射和验证，并使用在软可靠性约束下优化准确性的强化学习算法进行训练。", "result": "经验结果表明，所提出的方法提高了模型置信度与正确性之间的一致性，从而产生了更值得信赖的输出。", "conclusion": "通过结合置信度校准和检索式搜索，并利用带约束的强化学习，可以有效提高大型语言模型在开放域问答中的可靠性和输出的可信度。", "translation": "提高大型语言模型（LLMs）的可靠性对于它们在现实世界场景中的部署至关重要。本文提出了“Deliberative Searcher”，这是第一个将确定性校准与基于检索的搜索集成到开放域问答中的框架。该智能体对维基百科数据进行多步反射和验证，并使用在软可靠性约束下优化准确性的强化学习算法进行训练。经验结果表明，所提出的方法提高了模型置信度与正确性之间的一致性，从而产生了更值得信赖的输出。本文将持续更新。", "summary": "本文提出了“Deliberative Searcher”框架，旨在通过将确定性校准与检索式搜索相结合，并利用强化学习在软可靠性约束下优化准确性，来提高大型语言模型在开放域问答中的可靠性。该方法通过多步反射和验证，有效提升了模型置信度与正确性的一致性，从而生成更可信的输出。", "keywords": "大型语言模型, 可靠性, 强化学习, 检索式搜索, 置信度校准", "comments": "这篇论文的创新点在于首次将确定性校准与检索式搜索结合起来，并通过强化学习来提高LLM的可靠性。其方法通过多步反射和验证，解决了LLM在开放域问答中可靠性不足的问题，对于LLM在实际场景中的应用具有重要意义。"}}
{"id": "2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 7 Figures, 10 Tables", "url": "http://arxiv.org/abs/2507.15586v2", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "comment": "16 pages, 7 Figures, 10 Tables", "pdf_url": "http://arxiv.org/pdf/2507.15586v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "通过强化学习提取合理证据用于检索增强生成", "tldr": "RAG中检索噪声影响大。本文提出LEAR，通过强化学习进行显式推理和有意识提取，以获取高质量证据，提高RAG性能。", "motivation": "检索增强生成（RAG）虽然能有效提高大型语言模型（LLM）的准确性，但检索噪声严重影响了LLM的生成质量，因此需要开发去噪机制。以往的方法直接提取证据，缺乏明确的思考过程，这有过滤掉关键线索的风险，并且泛化能力较差。", "method": "本文提出了LEAR模型，该模型通过强化学习来学习提取合理证据。其方法包括两个主要步骤：(1) 首先进行显式推理，以识别检索内容中的潜在线索；(2) 接着有意识地进行提取，以避免遗漏任何有助于回答问题的关键线索。为了实现端到端训练，模型将证据推理和证据提取统一为一个响应。此外，它应用知识令牌掩码进行解耦，以区分基于推理和基于提取的答案，并设计了三种可验证的奖励函数（包括答案、长度和格式）通过策略优化算法来更新模型。", "result": "在三个基准数据集上的广泛实验表明，LEAR模型是有效的，它能够提供紧凑且高质量的证据，显著提高了下游任务的准确性，并促进了在在线RAG系统中的有效应用。", "conclusion": "LEAR模型通过结合显式推理和有意识提取的强化学习方法，有效解决了检索增强生成（RAG）中检索噪声的问题，显著提高了证据质量和下游任务的性能。", "translation": "检索增强生成（RAG）有效提高了大型语言模型（LLM）的准确性。然而，检索噪声严重影响LLM的生成质量，因此需要开发去噪机制。以前的方法直接提取证据，没有明确的思考过程，这有过滤掉关键线索的风险，并且难以泛化。为此，我们提出了LEAR，它通过（1）首先显式推理以识别检索内容中的潜在线索，然后（2）有意识地提取以避免遗漏任何有助于回答问题的关键线索，从而学习提取合理的证据。具体来说，我们将证据推理和证据提取框定为一个统一的响应，进行端到端训练；应用知识令牌掩码进行解耦，以得出基于推理和基于提取的答案；并设计了三种类型的可验证奖励函数，包括答案、长度和格式，通过策略优化算法更新模型。在三个基准数据集上的广泛实验表明了LEAR的有效性，它提供了紧凑和高质量的证据，提高了下游任务的准确性，并促进了在在线RAG系统中的有效应用。", "summary": "本文针对检索增强生成（RAG）中检索噪声影响大型语言模型（LLM）生成质量的问题，提出了一种名为LEAR的强化学习模型。LEAR通过显式推理识别潜在线索并有意识地提取关键证据，以克服传统方法直接提取证据的局限性。模型将推理和提取统一进行端到端训练，并利用知识令牌掩码和基于答案、长度、格式的三种奖励函数通过策略优化进行更新。实验证明LEAR能提供高质量证据，提升下游任务准确性，并适用于在线RAG系统。", "keywords": "检索增强生成, 强化学习, 证据提取, 大型语言模型, 去噪", "comments": "本文的创新点在于提出了一个基于强化学习的证据提取框架LEAR，通过显式的“推理”和“提取”两个阶段，有效解决了RAG中检索噪声的问题。这种有意识的证据选择机制比传统直接提取方法更具鲁棒性和泛化性。奖励函数的设计也为模型优化提供了多维度反馈。该方法对于提升RAG系统在真实世界应用中的可靠性具有重要意义。"}}
{"id": "2507.16846", "title": "Analytical Formulation of Autonomous Vehicle Freeway Merging Control with State-Dependent Discharge Rates", "authors": ["Qing Tang", "Xianbiao Hu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE Transactions on Intelligent Transportation Systems (2025) as a regular paper (minor revision approved)", "url": "http://arxiv.org/abs/2507.16846v1", "summary": "The core of the freeway merging control problem lies in dynamic queue\npropagation and dissipation linked to merging vehicle behavior. Traditionally,\nqueuing is modeled through demand-supply interactions with time varying demand\nand fixed capacity. However, field observations show flow rates decrease during\ncongestion at freeway merges due to the impact of intersecting traffic, a\nfactor overlooked in fundamental diagrams. This manuscript introduces an\nanalytical approach to characterize and control the dynamic multi-stage merging\nof autonomous vehicles, prioritizing traffic efficiency and safety. For the\nfirst time, the effective discharge rate at the merging point, reduced by the\nmulti-stage dynamic merging process, is analytically derived using a closed\nform formulation. Leveraging this expression, performance metrics such as queue\nlength and traffic delay are derived as the first objective. Additionally, a\ncrash risk function is established to quantitatively assess potential\ncollisions during the merging process, serving as the second objective.\nFinally, the problem is formulated as a dynamic programming model to jointly\nminimize delay and crash risk, with the merging location and speed as decision\nvariables. Given the terminal state, the ramp vehicle merging task is\nformulated as a recursive optimization problem, employing backward induction to\nfind the minimum cost solution. Numerical experiments using the NGSIM dataset\nvalidate the derived effective discharge rate. The results indicate that the\nproposed model outperforms two benchmark algorithms, leading to a more\nefficient and safer merging process.", "comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems (2025) as a regular paper (minor revision approved)", "pdf_url": "http://arxiv.org/pdf/2507.16846v1", "cate": "cs.RO", "date": "2025-07-20", "updated": "2025-07-20", "AI": {"title_translation": "自动驾驶汽车高速公路合流控制的分析公式，带有状态依赖的排放率", "tldr": "本文提出一种分析方法，通过动态规划模型解决自动驾驶汽车高速公路合流问题，考虑状态依赖的排放率、交通效率和碰撞风险，并优于基准算法。", "motivation": "传统的高速公路合流排队模型忽略了交叉车流在拥堵时对流量下降的影响，本研究旨在通过引入状态依赖的排放率来解决这一问题，以提高自动驾驶车辆合流的效率和安全性。", "method": "本文提出了一种分析方法来描述和控制自动驾驶汽车的多阶段动态合流过程。首次通过封闭形式推导了受多阶段动态合流过程影响的有效排放率。在此基础上，推导了排队长度和交通延误等性能指标。建立了碰撞风险函数。将问题表述为动态规划模型，以合并位置和速度为决策变量，共同最小化延误和碰撞风险。采用逆向归纳法将匝道车辆合流任务表述为递归优化问题。", "result": "数值实验使用NGSIM数据集验证了所推导的有效排放率。结果表明，所提出的模型优于两种基准算法，从而实现更高效、更安全的合流过程。", "conclusion": "提出的自动驾驶汽车高速公路合流控制模型能够实现更高效、更安全的合流过程，优于现有基准方法。", "translation": "高速公路合流控制问题的核心在于与合流车辆行为相关的动态排队传播和消散。传统上，排队通过需求-供给交互作用建模，具有时变需求和固定容量。然而，现场观察表明，由于交叉车流的影响，高速公路合流点在拥堵期间流量会下降，这是基本图中被忽视的一个因素。本文引入了一种分析方法来描述和控制自动驾驶汽车的动态多阶段合流，优先考虑交通效率和安全性。首次使用封闭形式公式分析推导了在多阶段动态合流过程中降低的合流点有效排放率。利用该表达式，将排队长度和交通延误等性能指标作为首要目标进行推导。此外，建立了碰撞风险函数，以定量评估合流过程中的潜在碰撞，作为第二个目标。最后，将问题表述为动态规划模型，以共同最小化延误和碰撞风险，并将合流位置和速度作为决策变量。给定终点状态，匝道车辆合流任务被表述为递归优化问题，采用逆向归纳法寻找最小成本解决方案。使用NGSIM数据集的数值实验验证了所推导的有效排放率。结果表明，所提出的模型优于两种基准算法，从而实现更高效、更安全的合流过程。", "summary": "本文针对自动驾驶汽车高速公路合流控制问题，提出一种分析方法。该方法首次解析推导了考虑多阶段动态合流的有效排放率，并以此为基础，将交通延误和碰撞风险作为优化目标。通过将问题构建为动态规划模型，并利用逆向归纳法求解，实现了合流位置和速度的决策。数值实验验证了模型的有效性，并表明其在效率和安全性方面均优于现有基准算法。", "keywords": "自动驾驶汽车, 高速公路合流, 动态规划, 排放率, 交通效率", "comments": "本文的创新点在于首次分析推导了状态依赖的有效排放率，并将其整合到动态规划框架中，以同时优化自动驾驶汽车合流的效率和安全性，弥补了传统模型忽视交叉车流影响的不足。其贡献在于为未来自动驾驶系统在复杂交通场景下的决策提供了理论基础和优化方法。"}}
{"id": "2503.05200", "title": "ORANSight-2.0: Foundational LLMs for O-RAN", "authors": ["Pranshav Gajjar", "Vijay K. Shah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05200v2", "summary": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\nto develop specialized foundational LLMs tailored for O-RAN. Built on 18 models\nspanning five open-source LLM frameworks -- Mistral, Qwen, Llama, Phi, and\nGemma -- ORANSight-2.0 fine-tunes models ranging from 1B to 70B parameters,\nsignificantly reducing reliance on proprietary, closed-source models while\nenhancing performance in O-RAN-specific tasks. At the core of ORANSight-2.0 is\nRANSTRUCT, a novel Retrieval-Augmented Generation (RAG)-based\ninstruction-tuning framework that employs two LLM agents -- a Mistral-based\nQuestion Generator and a Qwen-based Answer Generator -- to create high-quality\ninstruction-tuning datasets. The generated dataset is then used to fine-tune\nthe 18 pre-trained open-source LLMs via QLoRA. To evaluate ORANSight-2.0, we\nintroduce srsRANBench, a novel benchmark designed for code generation and\ncodebase understanding in the context of srsRAN, a widely used 5G O-RAN stack.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05200v2", "cate": "cs.CL", "date": "2025-03-07", "updated": "2025-07-22", "AI": {"title_translation": "ORANSight-2.0：面向O-RAN的基础大语言模型", "tldr": "ORANSight-2.0引入了专门为O-RAN定制的基础大语言模型，以弥补现有通用LLM在O-RAN领域应用的不足，并提出了一个新颖的指令微调框架RANSTRUCT和评估基准srsRANBench。", "motivation": "尽管大语言模型（LLMs）在医疗、客户服务和商业营销等关键领域产生了变革性影响，但它们在开放无线接入网络（O-RAN）中的集成仍然有限。这种差距主要是由于缺乏领域特定的基础模型，现有解决方案通常依赖通用LLM，未能解决O-RAN独特的挑战和技术复杂性。", "method": "为弥补这一差距，本文提出了ORANSight-2.0，一项旨在开发专门为O-RAN定制的基础LLM的开创性举措。ORANSight-2.0基于18个模型构建，涵盖Mistral、Qwen、Llama、Phi和Gemma等五种开源LLM框架，对参数范围从1B到70B的模型进行微调。其核心是RANSTRUCT，一个新颖的基于检索增强生成（RAG）的指令微调框架，它利用两个LLM代理（基于Mistral的问题生成器和基于Qwen的答案生成器）来创建高质量的指令微调数据集。生成的数据集随后通过QLoRA用于微调这18个预训练的开源LLM。为了评估ORANSight-2.0，本文引入了srsRANBench，这是一个专为srsRAN（广泛使用的5G O-RAN堆栈）中的代码生成和代码库理解而设计的新型基准。", "result": "ORANSight-2.0显著减少了对专有、闭源模型的依赖，同时增强了在O-RAN特定任务中的性能。此外，本文还引入了srsRANBench这一新型基准用于评估。", "conclusion": "Not mentioned in abstract", "translation": "尽管大语言模型（LLMs）在医疗保健、客户服务和商业营销等关键领域产生了变革性影响，但它们在开放无线接入网络（O-RAN）中的集成仍然有限。这种差距主要是由于缺乏领域特定的基础模型，现有解决方案通常依赖通用LLM，未能解决O-RAN独特的挑战和技术复杂性。为了弥补这一差距，我们引入了ORANSight-2.0（O-RAN Insights），这是一项开发专门为O-RAN定制的基础LLM的开创性举措。ORANSight-2.0基于18个模型构建，涵盖Mistral、Qwen、Llama、Phi和Gemma等五种开源LLM框架，对参数范围从1B到70B的模型进行微调，显著减少了对专有、闭源模型的依赖，同时增强了在O-RAN特定任务中的性能。ORANSight-2.0的核心是RANSTRUCT，一个新颖的基于检索增强生成（RAG）的指令微调框架，它利用两个LLM代理——一个基于Mistral的问题生成器和一个基于Qwen的答案生成器——来创建高质量的指令微调数据集。生成的数据集随后通过QLoRA用于微调这18个预训练的开源LLM。为了评估ORANSight-2.0，我们引入了srsRANBench，这是一个专为srsRAN（广泛使用的5G O-RAN堆栈）中的代码生成和代码库理解而设计的新型基准。", "summary": "本文介绍了ORANSight-2.0，一个为开放无线接入网络（O-RAN）定制的基础大语言模型（LLM）项目，旨在解决通用LLM在该领域应用的局限性。ORANSight-2.0基于18个开源LLM模型进行微调，并引入了RANSTRUCT，一个基于RAG的指令微调框架，用于生成高质量数据集。此外，本文还提出了srsRANBench，一个用于评估O-RAN环境中代码生成和代码库理解的新型基准，显著提升了O-RAN特定任务的性能并减少了对闭源模型的依赖。", "keywords": "LLMs, O-RAN, 基础模型, RANSTRUCT, srsRANBench", "comments": "该论文的创新点在于首次提出了专门针对O-RAN领域的领域特定基础LLM，解决了现有通用LLM无法有效处理O-RAN独特挑战的问题。其提出的RANSTRUCT框架通过LLM代理自动生成高质量指令微调数据集，展示了数据生成的新颖方法。此外，引入srsRANBench作为O-RAN代码生成和理解的评估基准，填补了该领域评估工具的空白，具有重要的实践意义和推动作用。"}}
{"id": "2507.17178", "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17178v1", "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17178v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SKA-Bench：一个评估大型语言模型结构化知识理解的细粒度基准", "tldr": "大型语言模型在结构化知识理解方面存在挑战。SKA-Bench是一个新的、全面的基准，涵盖知识图谱、表格、知识图谱+文本和表格+文本四种形式，并进行细粒度能力测试（噪声鲁棒性、顺序不敏感性、信息整合、否定拒绝）。实证评估表明现有大型语言模型表现不佳。", "motivation": "现有的大型语言模型（LLMs）结构化知识（SK）理解评估不够严谨，缺乏对特定能力的评估，且多只关注单一类型的SK。因此，目标是提出一个更全面、更严格的结构化知识理解基准来诊断LLMs的不足。", "method": "本文提出了SKA-Bench，一个结构化知识增强的问答基准，涵盖知识图谱、表格、知识图谱+文本和表格+文本四种广泛使用的结构化知识形式。通过三阶段管道构建实例，包括问题、答案、正向知识单元和噪声知识单元。为了细粒度评估LLMs的SK理解能力，将实例扩展到四个基本能力测试平台：噪声鲁棒性、顺序不敏感性、信息整合和否定拒绝。", "result": "对包括先进的DeepSeek-R1在内的8个代表性LLM进行的实证评估表明，现有LLM在理解结构化知识方面仍面临重大挑战，并且它们的性能受到噪声量、知识单元顺序和幻觉现象等因素的影响。", "conclusion": "现有的大型语言模型在理解结构化知识方面仍面临重大挑战，它们的性能受到噪声量、知识单元顺序和幻觉现象等因素的影响。", "translation": "虽然大型语言模型（LLMs）在理解知识图谱（KG）和表格等结构化知识（SK）方面取得了显著进展，但现有的SK理解评估不够严谨（即缺乏对特定能力的评估），并且只关注单一类型的SK。因此，我们的目标是提出一个更全面、更严格的结构化知识理解基准，以诊断LLMs的缺点。在本文中，我们引入了SKA-Bench，这是一个结构化知识增强的问答基准，涵盖了四种广泛使用的结构化知识形式：知识图谱（KG）、表格（Table）、知识图谱+文本（KG+Text）和表格+文本（Table+Text）。我们采用三阶段管道来构建SKA-Bench实例，其中包括一个问题、一个答案、正向知识单元和噪声知识单元。为了细粒度地评估LLMs的SK理解能力，我们将实例扩展到四个基本能力测试平台：噪声鲁棒性、顺序不敏感性、信息整合和否定拒绝。对包括先进的DeepSeek-R1在内的8个代表性LLM进行的实证评估表明，现有LLM在理解结构化知识方面仍面临重大挑战，并且它们的性能受到噪声量、知识单元顺序和幻觉现象等因素的影响。我们的数据集和代码可在https://github.com/Lza12a/SKA-Bench获取。", "summary": "本文介绍了SKA-Bench，一个用于评估大型语言模型（LLMs）结构化知识理解能力的新型全面基准。针对现有评估的局限性，SKA-Bench整合了四种结构化知识形式（知识图谱、表格、知识图谱+文本、表格+文本），并测试了四种细粒度能力：噪声鲁棒性、顺序不敏感性、信息整合和否定拒绝。对8个LLM的评估结果表明，当前模型在处理结构化知识方面仍面临显著挑战，其性能受噪声、顺序和幻觉现象的影响。", "keywords": "LLMs, 结构化知识, 基准, 知识图谱, 表格", "comments": "该论文通过提供一个细粒度且全面的基准来评估大型语言模型的结构化知识理解能力，解决了LLM评估中的一个关键空白。包含多种SK形式和特定能力测试（如噪声鲁棒性和顺序不敏感性）是创新性的，对于诊断LLM的不足至关重要。研究结果突出了LLM在处理复杂结构化数据方面持续存在的挑战，为未来的研究指明了方向。"}}
{"id": "2504.01677", "title": "System Level Synthesis for Affine Control Policies: Model Based and Data-Driven Settings", "authors": ["Lukas Schüepp", "Giulia De Pasquale", "Florian Dörfler", "Carmen Amo Alonso"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Conference on Decision and Control (CDC), 2025", "url": "http://arxiv.org/abs/2504.01677v2", "summary": "There is an increasing need for effective control of systems with complex\ndynamics, particularly through data-driven approaches. System Level Synthesis\n(SLS) has emerged as a powerful framework that facilitates the control of\nlarge-scale systems while accounting for model uncertainties. SLS approaches\nare currently limited to linear systems and time-varying linear control\npolicies, thus limiting the class of achievable control strategies. We\nintroduce a novel closed-loop parameterization for time-varying affine control\npolicies, extending the SLS framework to a broader class of systems and\npolicies. We show that the closed-loop behavior under affine policies can be\nequivalently characterized using past system trajectories, enabling a fully\ndata-driven formulation. This parameterization seamlessly integrates affine\npolicies into optimal control problems, allowing for a closed-loop formulation\nof general Model Predictive Control (MPC) problems. To the best of our\nknowledge, this is the first work to extend SLS to affine policies in both\nmodel-based and data-driven settings, enabling an equivalent formulation of MPC\nproblems using closed-loop maps. We validate our approach through numerical\nexperiments, demonstrating that our model-based and data-driven affine SLS\nformulations achieve performance on par with traditional model-based MPC.", "comment": "Accepted to IEEE Conference on Decision and Control (CDC), 2025", "pdf_url": "http://arxiv.org/pdf/2504.01677v2", "cate": "eess.SY", "date": "2025-04-02", "updated": "2025-07-23", "AI": {"title_translation": "系统级综合的仿射控制策略：基于模型和数据驱动的设置", "tldr": "本文提出了一种新颖的闭环参数化方法，将系统级综合（SLS）框架扩展到仿射控制策略，并首次在基于模型和数据驱动的设置中实现了SLS对仿射策略的扩展，其性能与传统模型预测控制（MPC）相当。", "motivation": "目前系统级综合（SLS）方法仅限于线性系统和时变线性控制策略，限制了可实现的控制策略类别，而对复杂动态系统的有效控制需求，特别是通过数据驱动方法，日益增长。", "method": "我们引入了一种新颖的针对时变仿射控制策略的闭环参数化方法，从而将SLS框架扩展到更广泛的系统和策略。我们证明了仿射策略下的闭环行为可以通过过去的系统轨迹等效地表征，从而实现完全数据驱动的公式。这种参数化方法将仿射策略无缝集成到最优控制问题中，允许对通用模型预测控制（MPC）问题进行闭环公式化。", "result": "据我们所知，这是首次将SLS扩展到基于模型和数据驱动设置中的仿射策略的工作，从而能够使用闭环映射对MPC问题进行等效表述。数值实验验证了我们的方法，表明我们基于模型和数据驱动的仿射SLS公式实现了与传统基于模型的MPC相当的性能。", "conclusion": "本研究成功地将系统级综合（SLS）框架扩展到了仿射控制策略，并在基于模型和数据驱动的设置中实现了这一扩展，通过数值实验证明了其性能与传统模型预测控制（MPC）相当。", "translation": "对具有复杂动态的系统进行有效控制的需求日益增长，特别是通过数据驱动的方法。系统级综合（SLS）已成为一个强大的框架，有助于控制大规模系统，同时考虑模型不确定性。SLS方法目前仅限于线性系统和时变线性控制策略，从而限制了可实现的控制策略的类别。我们引入了一种新颖的用于时变仿射控制策略的闭环参数化，将SLS框架扩展到更广泛的系统和策略。我们表明，仿射策略下的闭环行为可以使用过去的系统轨迹等效地表征，从而实现完全数据驱动的公式。这种参数化将仿射策略无缝集成到最优控制问题中，允许对通用模型预测控制（MPC）问题进行闭环公式化。据我们所知，这是首次在基于模型和数据驱动的设置中将SLS扩展到仿射策略的工作，从而能够使用闭环映射对MPC问题进行等效表述。我们通过数值实验验证了我们的方法，证明我们的基于模型和数据驱动的仿射SLS公式实现了与传统基于模型的MPC相当的性能。", "summary": "本文针对复杂动态系统的控制需求，特别是数据驱动方法，解决了当前系统级综合（SLS）框架仅限于线性系统和线性控制策略的局限性。作者提出了一种新颖的闭环参数化方法，用于时变仿射控制策略，成功地将SLS扩展到更广泛的系统和策略类别。该方法能够通过过去的系统轨迹实现完全数据驱动的SLS公式，并将其无缝集成到最优控制问题中，实现通用模型预测控制（MPC）问题的闭环表述。这是首次在基于模型和数据驱动的设置中将SLS扩展到仿射策略，并经数值实验证明，其性能与传统基于模型的MPC相当。", "keywords": "系统级综合, 仿射控制策略, 数据驱动, 模型预测控制, 闭环参数化", "comments": "这项工作具有重要的创新性，因为它首次将系统级综合（SLS）框架扩展到仿射控制策略，涵盖了基于模型和数据驱动两种设置。通过引入新颖的闭环参数化方法，该研究不仅拓宽了SLS的应用范围，使其能够处理更广泛的系统和控制策略，而且还实现了完全数据驱动的MPC问题公式化，这对于处理复杂不确定系统具有重要意义。其性能与传统MPC相当，进一步验证了该方法的有效性和实用性。"}}
{"id": "2507.17713", "title": "Sequential Bayesian Design for Efficient Surrogate Construction in the Inversion of Darcy Flows", "authors": ["Hongji Wang", "Hongqiao Wang", "Jinyong Ying", "Qingping Zhou"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      21 pages, 15 figures", "url": "http://arxiv.org/abs/2507.17713v1", "summary": "Inverse problems governed by partial differential equations (PDEs) play a\ncrucial role in various fields, including computational science, image\nprocessing, and engineering. Particularly, Darcy flow equation is a fundamental\nequation in fluid mechanics, which plays a crucial role in understanding fluid\nflow through porous media. Bayesian methods provide an effective approach for\nsolving PDEs inverse problems, while their numerical implementation requires\nnumerous evaluations of computationally expensive forward solvers. Therefore,\nthe adoption of surrogate models with lower computational costs is essential.\nHowever, constructing a globally accurate surrogate model for high-dimensional\ncomplex problems demands high model capacity and large amounts of data. To\naddress this challenge, this study proposes an efficient locally accurate\nsurrogate that focuses on the high-probability regions of the true likelihood\nin inverse problems, with relatively low model complexity and few training data\nrequirements. Additionally, we introduce a sequential Bayesian design strategy\nto acquire the proposed surrogate since the high-probability region of the\nlikelihood is unknown. The strategy treats the posterior evolution process of\nsequential Bayesian design as a Gaussian process, enabling algorithmic\nacceleration through one-step ahead prior. The complete algorithmic framework\nis referred to as Sequential Bayesian design for locally accurate surrogate\n(SBD-LAS). Finally, three experiments based the Darcy flow equation demonstrate\nthe advantages of the proposed method in terms of both inversion accuracy and\ncomputational speed.", "comment": "21 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.17713v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "达西流反演中高效代理模型构建的序贯贝叶斯设计", "tldr": "本文提出SBD-LAS框架，通过序贯贝叶斯设计构建局部精确代理模型，有效提升达西流反演的计算效率和精度。", "motivation": "解决偏微分方程反问题中贝叶斯推断的高计算成本以及全局代理模型构建困难的问题。", "method": "提出一种名为SBD-LAS（用于局部精确代理的序贯贝叶斯设计）的算法框架。该方法构建高效的局部精确代理模型，聚焦于真实似然的高概率区域，并采用序贯贝叶斯设计策略。为加速算法，将序贯贝叶斯设计的后验演化过程视为高斯过程，并通过一步超前先验实现加速。", "result": "在基于达西流方程的三个实验中，所提出的SBD-LAS方法在反演精度和计算速度方面均表现出显著优势。", "conclusion": "SBD-LAS方法能够有效提升达西流反演问题的精度和计算速度，克服了传统方法的计算瓶颈和代理模型构建难题。", "translation": "由偏微分方程（PDEs）控制的反问题在计算科学、图像处理和工程等各个领域中发挥着至关重要的作用。特别是，达西流方程是流体力学中的一个基本方程，在理解流体通过多孔介质的流动方面起着关键作用。贝叶斯方法为解决PDE反问题提供了一种有效方法，但其数值实现需要大量评估计算成本高昂的正向求解器。因此，采用计算成本较低的代理模型至关重要。然而，为高维复杂问题构建全局精确的代理模型需要高模型容量和大量数据。为了应对这一挑战，本研究提出了一种高效的局部精确代理模型，该模型专注于反问题中真实似然的高概率区域，具有相对较低的模型复杂度和较少的训练数据需求。此外，由于似然的高概率区域是未知的，我们引入了一种序贯贝叶斯设计策略来获取所提出的代理模型。该策略将序贯贝叶斯设计的后验演化过程视为高斯过程，通过一步超前先验实现算法加速。完整的算法框架被称为用于局部精确代理的序贯贝叶斯设计（SBD-LAS）。最后，基于达西流方程的三个实验证明了所提出方法在反演精度和计算速度方面的优势。", "summary": "本文提出了一种名为SBD-LAS（用于局部精确代理的序贯贝叶斯设计）的新型框架，旨在解决偏微分方程反问题中贝叶斯推断的高计算成本和全局代理模型构建困难的问题。SBD-LAS通过构建一个高效的局部精确代理模型，专注于真实似然的高概率区域，从而减少模型复杂度和数据需求。该方法采用序贯贝叶斯设计策略来逐步获取代理模型，并利用高斯过程对后验演化进行建模以加速算法。实验证明，SBD-LAS在达西流反演中显著提高了精度和计算速度。", "keywords": "达西流反演, 贝叶斯设计, 代理模型, 局部精确, 序贯贝叶斯设计, SBD-LAS", "comments": "这项研究通过引入局部精确代理模型和序贯贝叶斯设计策略，有效地解决了偏微分方程反演中贝叶斯方法的计算效率问题，特别是在高维复杂问题中。将后验演化过程视为高斯过程以实现加速是一个创新点。该方法对于需要大量昂贵正向模型评估的领域具有重要意义。"}}
{"id": "2507.11799", "title": "Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network", "authors": ["Shin-ichi Ito"], "categories": ["physics.comp-ph", "cs.AI"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11799v2", "summary": "This paper presents a neural network (NN)-based solver for an\nintegro-differential equation that models shrinkage-induced fragmentation. The\nproposed method directly maps input parameters to the corresponding probability\ndensity function without numerically solving the governing equation, thereby\nsignificantly reducing computational costs. Specifically, it enables efficient\nevaluation of the density function in Monte Carlo simulations while maintaining\naccuracy comparable to or even exceeding that of conventional finite difference\nschemes. Validatation on synthetic data demonstrates both the method's\ncomputational efficiency and predictive reliability. This study establishes a\nfoundation for the data-driven inverse analysis of fragmentation and suggests\nthe potential for extending the framework beyond pre-specified model\nstructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11799v2", "cate": "physics.comp-ph", "date": "2025-07-15", "updated": "2025-07-23", "AI": {"title_translation": "基于物理信息神经网络的收缩致裂碎片尺寸密度估计器", "tldr": "论文提出了一种基于神经网络的求解器，用于收缩致裂的积分微分方程，能显著降低计算成本并保持高精度，为数据驱动的碎片分析奠定基础。", "motivation": "解决收缩致裂建模中积分微分方程的数值求解计算成本高的问题，并实现高效、准确的密度函数评估，为数据驱动的碎片分析奠定基础。", "method": "提出了一种基于神经网络的求解器，直接将输入参数映射到概率密度函数，避免了数值求解控制方程。该方法在蒙特卡洛模拟中能高效评估密度函数。", "result": "在合成数据上的验证表明，该方法具有计算效率高和预测可靠性强的特点，其精度与传统有限差分方案相当甚至更高。", "conclusion": "该研究为碎片化的数据驱动逆分析奠定了基础，并表明该框架有潜力扩展到预设模型结构之外。", "translation": "本文提出了一种基于神经网络（NN）的求解器，用于模拟收缩致裂的积分微分方程。所提出的方法直接将输入参数映射到相应的概率密度函数，而无需数值求解控制方程，从而显著降低了计算成本。具体而言，它可以在蒙特卡洛模拟中高效评估密度函数，同时保持与传统有限差分方案相当甚至更高的精度。在合成数据上的验证证明了该方法的计算效率和预测可靠性。这项研究为碎片化的数据驱动逆分析奠定了基础，并暗示了将该框架扩展到预设模型结构之外的潜力。", "summary": "本文提出了一种基于神经网络的求解器，用于解决模拟收缩致裂的积分微分方程。该方法通过直接映射输入参数到概率密度函数，显著降低了计算成本，并在蒙特卡洛模拟中实现了高效且高精度的密度函数评估。实验证明其计算效率和预测可靠性，为数据驱动的碎片化逆分析提供了新途径，并有望应用于更广泛的模型结构。", "keywords": "神经网络, 收缩致裂, 积分微分方程, 密度估计, 计算效率", "comments": "这篇论文的创新点在于将神经网络应用于积分微分方程的求解，特别是针对收缩致裂模型，通过直接映射而非数值求解，显著提升了计算效率。其重要性在于为数据驱动的碎片分析提供了新的工具，并可能突破传统模型的限制。"}}
{"id": "2507.17054", "title": "New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding", "authors": ["Shao-Hung Chan", "Thomy Phan", "Jiaoyang Li", "Sven Koenig"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figures, International Symposium on Combinatorial Search, 2025", "url": "http://arxiv.org/abs/2507.17054v1", "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths, one for each agent in a shared environment. Its objective\nis to minimize the sum of path costs (SOC), where the path cost of each agent\nis defined as the travel time from its start location to its target location.\nExplicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for\nbounded-suboptimal MAPF, with the SOC of the solution being at most a\nuser-specified factor $w$ away from optimal. EECBS maintains sets of paths and\na lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of\npaths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve\ncollisions. For each path in a set, EECBS maintains a lower bound on its\noptimal path that satisfies constraints. By finding an individually\nbounded-suboptimal path with cost at most a threshold of $w$ times its lower\nbound, EECBS guarantees to find a bounded-suboptimal solution. To speed up\nEECBS, previous work uses flex distribution to increase the threshold. Though\nEECBS with flex distribution guarantees to find a bounded-suboptimal solution,\nincreasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS\nto switch among different sets of paths instead of resolving collisions on a\nparticular set of paths, and thus reducing efficiency. To address this issue,\nwe propose Conflict-Based Flex Distribution that distributes flex in proportion\nto the number of collisions. We also estimate the delays needed to satisfy\nconstraints and propose Delay-Based Flex Distribution. On top of that, we\npropose Mixed-Strategy Flex Distribution, combining both in a hierarchical\nframework. We prove that EECBS with our new flex distribution mechanisms is\ncomplete and bounded-suboptimal. Our experiments show that our approaches\noutperform the original (greedy) flex distribution.", "comment": "9 pages, 10 figures, International Symposium on Combinatorial Search,\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.17054v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "有界次优多智能体路径规划中柔性分配的新机制", "tldr": "本文提出了三种新的柔性分配机制（基于冲突、基于延迟和混合策略），以提高EECBS算法在有界次优多智能体路径规划中的效率和性能。", "motivation": "在多智能体路径规划（MAPF）中，显式估计基于冲突搜索（EECBS）算法在使用柔性分配（flex distribution）时，增加阈值可能会导致总路径成本（SOC）超出预设的有界次优范围，从而迫使算法在不同路径集之间频繁切换而非专注于解决特定冲突，进而降低了算法效率。", "method": "本文提出了三种新的柔性分配机制：基于冲突的柔性分配（Conflict-Based Flex Distribution），它根据冲突数量按比例分配柔性；基于延迟的柔性分配（Delay-Based Flex Distribution），它估计满足约束所需的延迟；以及混合策略柔性分配（Mixed-Strategy Flex Distribution），它在一个分层框架中结合了前两种方法。", "result": "实验结果表明，本文提出的新柔性分配机制优于原始（贪婪）的柔性分配方法。", "conclusion": "本文证明了采用新柔性分配机制的EECBS算法是完备的且能够找到有界次优解。实验结果进一步验证了这些新方法在性能上优于原始的柔性分配。", "translation": "多智能体路径规划（MAPF）是在共享环境中为每个智能体找到一组无冲突路径的问题。其目标是最小化路径成本之和（SOC），其中每个智能体的路径成本定义为从其起始位置到目标位置的旅行时间。显式估计基于冲突搜索（EECBS）是领先的有界次优MAPF算法，其解的SOC最多比最优解偏离一个用户指定因子 $w$。EECBS维护路径集和最优SOC的下界 $LB$。然后，它迭代地选择一个SOC至多为 $w \\cdot LB$ 的路径集，并引入约束来解决冲突。对于路径集中的每条路径，EECBS维护其满足约束的最优路径的下界。通过找到成本至多为其下界 $w$ 倍的个体有界次优路径，EECBS保证找到一个有界次优解。为了加速EECBS，先前的工作使用柔性分配来增加阈值。尽管使用柔性分配的EECBS保证找到一个有界次优解，但增加阈值可能会使SOC超出 $w \\cdot LB$，从而迫使EECBS在不同路径集之间切换，而不是解决特定路径集上的冲突，因此降低了效率。为了解决这个问题，我们提出了基于冲突的柔性分配，它根据冲突数量按比例分配柔性。我们还估计了满足约束所需的延迟，并提出了基于延迟的柔性分配。在此基础上，我们提出了混合策略柔性分配，将两者结合在一个分层框架中。我们证明了采用我们新柔性分配机制的EECBS是完备的且有界次优的。我们的实验表明，我们的方法优于原始（贪婪）的柔性分配。", "summary": "本文针对多智能体路径规划（MAPF）中EECBS算法在柔性分配时可能降低效率的问题，提出了三种新的柔性分配机制：基于冲突的柔性分配、基于延迟的柔性分配以及结合两者的混合策略柔性分配。这些新机制旨在更有效地分配“柔性”，以避免总路径成本超出有界次优范围，从而提高算法的效率。研究证明了新机制的完备性和有界次优性，并通过实验验证了其性能优于传统的柔性分配方法。", "keywords": "多智能体路径规划, EECBS, 柔性分配, 有界次优, 冲突解决", "comments": "本文针对EECBS算法在处理柔性分配时可能出现的效率下降问题，提出了创新的解决方案。通过引入基于冲突和基于延迟的柔性分配策略，并结合为混合策略，有效地解决了因不当柔性分配导致的算法效率降低。其创新点在于将柔性分配与冲突数量和预测延迟关联起来，使得柔性分配更加智能和高效。这对于提升大规模多智能体系统路径规划的实际应用性能具有重要意义。"}}
{"id": "2507.17319", "title": "Construction of Self-Orthogonal Quasi-Cyclic Codes and Their Application to Quantum Error-Correcting Codes", "authors": ["Mengying Gao", "Yuhua Sun", "Tongjiang Yan", "Chun'e Zhao"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17319v1", "summary": "In this paper, necessary and sufficient conditions for the self-orthogonality\nof t-generator quasi-cyclic (QC) codes are presented under the Euclidean,\nHermitian, and symplectic inner products, respectively. Particularly, by\nstudying the structure of the dual codes of a class of 2-generator QC codes, we\nderive necessary and sufficient conditions for the QC codes to be\ndual-containing under the above three inner products. This class of 2-generator\nQC codes generalizes many known codes in the literature. Based on the above\nconditions, we construct several quantum stabilizer codes and quantum\nsynchronizable codes with good parameters, some of which share parameters with\ncertain best-known codes listed in Grassl's code table.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17319v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "自正交拟循环码的构造及其在量子纠错码中的应用", "tldr": "本文提出了t-生成器拟循环码在欧几里得、厄米特和辛内积下的自正交充要条件，并将其应用于构造具有良好参数的量子纠错码。", "motivation": "研究t-生成器拟循环码的自正交性条件，特别是2-生成器拟循环码的对偶码结构，以便为构造量子纠错码提供理论基础和方法。", "method": "通过研究t-生成器拟循环码在欧几里得、厄米特和辛内积下的自正交充要条件，并特别深入研究一类2-生成器拟循环码的对偶码结构，推导出其对偶包含的充要条件。在此基础上，构造了量子稳定子码和量子同步码。", "result": "得到了t-生成器拟循环码在欧几里得、厄米特和辛内积下的自正交充要条件。针对一类2-生成器拟循环码，推导了其对偶包含的充要条件。基于这些条件，构造了多个参数良好的量子稳定子码和量子同步码，其中一些码的参数与Grassl码表中已知最佳码的参数相同。", "conclusion": "本文提出的拟循环码自正交和对偶包含条件，为构造具有良好参数的量子稳定子码和量子同步码提供了有效途径。", "translation": "本文提出了t-生成器拟循环码在欧几里得、厄米特和辛内积下的自正交充要条件。特别地，通过研究一类2-生成器拟循环码的对偶码结构，我们推导了该类拟循环码在上述三种内积下是对偶包含码的充要条件。这类2-生成器拟循环码推广了文献中许多已知的码。基于上述条件，我们构造了几个参数良好的量子稳定子码和量子同步码，其中一些码的参数与Grassl码表中列出的某些最佳已知码的参数相同。", "summary": "本文研究了t-生成器拟循环(QC)码在欧几里得、厄米特和辛内积下的自正交充要条件。通过深入分析一类广义的2-生成器QC码的对偶码结构，推导了其对偶包含的充要条件。基于这些理论成果，成功构造了多类参数优良的量子稳定子码和量子同步码，其中部分码的参数达到了已知最佳水平。", "keywords": "拟循环码, 自正交码, 量子纠错码, 对偶码, 内积", "comments": "本文的创新之处在于系统地给出了t-生成器拟循环码在不同内积下的自正交和对偶包含条件，特别是对2-生成器拟循环码的深入分析，为量子纠错码的构造提供了坚实的理论基础。其重要性体现在成功构造出与最佳已知码参数相当的量子码，对量子信息领域具有实际应用价值。"}}
{"id": "2507.17307", "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning", "authors": ["Zhuokun Chen", "Zeren Chen", "Jiahao He", "Mingkui Tan", "Jianfei Cai", "Bohan Zhuang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17307v1", "summary": "Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of\nlarge language models by encouraging step-by-step intermediate reasoning during\ninference. While effective, CoT introduces substantial computational overhead\ndue to its reliance on autoregressive decoding over long token sequences.\nExisting acceleration strategies either reduce sequence length through early\nstopping or compressive reward designs, or improve decoding speed via\nspeculative decoding with smaller models. However, speculative decoding suffers\nfrom limited speedup when the agreement between small and large models is low,\nand fails to exploit the potential advantages of small models in producing\nconcise intermediate reasoning. In this paper, we present R-Stitch, a\ntoken-level, confidence-based hybrid decoding framework that accelerates CoT\ninference by switching between a small language model (SLM) and a large\nlanguage model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to\ngenerate tokens by default and delegates to the LLM only when the SLM's\nconfidence falls below a threshold. This design avoids full-sequence rollback\nand selectively invokes the LLM on uncertain steps, preserving both efficiency\nand answer quality. R-Stitch is model-agnostic, training-free, and compatible\nwith standard decoding pipelines. Experiments on math reasoning benchmarks\ndemonstrate that R-Stitch achieves up to 85\\% reduction in inference latency\nwith negligible accuracy drop, highlighting its practical effectiveness in\naccelerating CoT reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17307v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "R-Stitch: 高效推理的动态轨迹拼接", "tldr": "R-Stitch是一个基于置信度的混合解码框架，通过在推理轨迹上动态切换小型语言模型和大型语言模型来加速思维链推理，显著降低了推理延迟。", "motivation": "思维链（CoT）推理虽然能增强大型语言模型的解决问题能力，但其对长序列自回归解码的依赖引入了大量的计算开销。现有加速策略（如提前停止、压缩奖励设计或使用小型模型进行推测解码）各有局限，特别是推测解码在大小模型一致性低时加速有限，且未能充分利用小型模型生成简洁中间推理的优势。", "method": "R-Stitch是一个基于token级别的、基于置信度的混合解码框架。它默认使用小型语言模型（SLM）生成token，仅当SLM的置信度低于某个阈值时才委托给大型语言模型（LLM）。这种设计避免了全序列回滚，并选择性地在不确定步骤调用LLM，从而在保持效率和答案质量的同时加速CoT推理。R-Stitch是模型无关、无需训练且与标准解码管道兼容的。", "result": "在数学推理基准测试中，R-Stitch将推理延迟降低了高达85%，同时准确率下降可以忽略不计。", "conclusion": "R-Stitch通过动态切换小型和大型语言模型，有效地加速了思维链推理，显著降低了推理延迟，同时保持了高准确率，证明了其在实际应用中的有效性。", "translation": "思维链（CoT）推理通过在推理过程中鼓励逐步的中间推理，增强了大型语言模型的问题解决能力。尽管有效，但CoT由于依赖于长token序列的自回归解码，引入了大量的计算开销。现有的加速策略要么通过提前停止或压缩奖励设计来减少序列长度，要么通过使用较小模型进行推测解码来提高解码速度。然而，当小型模型和大型模型之间的一致性较低时，推测解码的加速效果有限，并且未能利用小型模型在生成简洁中间推理方面的潜在优势。在本文中，我们提出了R-Stitch，一个基于token级别、基于置信度的混合解码框架，它通过在推理轨迹上在小型语言模型（SLM）和大型语言模型（LLM）之间切换来加速CoT推理。R-Stitch默认使用SLM生成token，并且仅当SLM的置信度低于某个阈值时才委托给LLM。这种设计避免了全序列回滚，并选择性地在不确定步骤调用LLM，从而在保持效率和答案质量的同时。R-Stitch是模型无关、无需训练且与标准解码管道兼容的。在数学推理基准测试上的实验表明，R-Stitch将推理延迟降低了高达85%，同时准确率下降可以忽略不计，突出了其在加速CoT推理方面的实际有效性。", "summary": "R-Stitch是一个创新的混合解码框架，旨在加速大型语言模型中的思维链（CoT）推理。它通过在推理路径上动态地在小型语言模型（SLM）和大型语言模型（LLM）之间切换来实现此目的，默认使用SLM，仅在SLM置信度不足时才调用LLM。这种基于token级别的置信度切换机制避免了传统加速方法的局限性，如推测解码的低一致性问题。R-Stitch无需训练、模型无关，并在数学推理任务上实现了高达85%的推理延迟降低，同时保持了可忽略的准确率损失，证明了其在提高CoT推理效率方面的实用性和有效性。", "keywords": "思维链推理, 混合解码, 语言模型加速, R-Stitch, 推理效率", "comments": "R-Stitch的创新之处在于其基于token级别的、置信度驱动的混合解码策略，有效结合了小型模型的高效性和大型模型的准确性。它解决了现有CoT加速方法（如推测解码）的局限性，特别是避免了全序列回滚并实现了显著的延迟降低。这种无需训练、模型无关的设计使其具有很高的实用价值和兼容性，对于优化LLM推理效率具有重要意义。"}}
{"id": "2309.07138", "title": "Blind Source Separation of Single-Channel Mixtures via Multi-Encoder Autoencoders", "authors": ["Matthew B. Webster", "Joonnyong Lee"], "categories": ["eess.SP", "cs.LG", "I.2.6"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      24 pages (with appendix), 12 figures(with appendix), resubmitted for review", "url": "http://arxiv.org/abs/2309.07138v4", "summary": "The task of blind source separation (BSS) involves separating sources from a\nmixture without prior knowledge of the sources or the mixing system.\nSingle-channel mixtures and non-linear mixtures are a particularly challenging\nproblem in BSS. In this paper, we propose a novel method for addressing BSS\nwith single-channel non-linear mixtures by leveraging the natural feature\nsubspace specialization ability of multi-encoder autoencoders. During the\ntraining phase, our method unmixes the input into the separate encoding spaces\nof the multi-encoder network and then remixes these representations within the\ndecoder for a reconstruction of the input. Then to perform source inference, we\nintroduce a novel encoding masking technique whereby masking out all but one of\nthe encodings enables the decoder to estimate a source signal. To this end, we\nalso introduce a sparse mixing loss that encourages sparse remixing of source\nencodings throughout the decoder and a so-called zero reconstruction loss on\nthe decoder for coherent source estimations. To analyze and evaluate our\nmethod, we conduct experiments on a toy dataset, designed to demonstrate this\nproperty of feature subspace specialization, and with real-world biosignal\nrecordings from a polysomnography sleep study for extracting respiration from\nelectrocardiogram and photoplethysmography signals.", "comment": "24 pages (with appendix), 12 figures(with appendix), resubmitted for\n  review", "pdf_url": "http://arxiv.org/pdf/2309.07138v4", "cate": "eess.SP", "date": "2023-08-31", "updated": "2025-07-23", "AI": {"title_translation": "经多编码器自编码器实现单通道混合信号的盲源分离", "tldr": "本文提出了一种基于多编码器自编码器的新方法，通过编码掩蔽技术和新颖的损失函数，解决单通道非线性混合信号的盲源分离问题，并在合成和真实生物信号数据集上进行了验证。", "motivation": "盲源分离（BSS）任务中，单通道非线性混合信号的分离是一个特别具有挑战性的问题。", "method": "本文提出了一种利用多编码器自编码器特征子空间特化能力的新方法来处理单通道非线性混合信号的盲源分离。在训练阶段，该方法将输入解混合到多编码器网络的独立编码空间中，然后在解码器中重新混合这些表示以重建输入。在源推断时，引入了一种新的编码掩蔽技术，通过掩蔽除一个编码之外的所有编码，使解码器能够估计源信号。此外，还引入了稀疏混合损失以鼓励稀疏的源编码重混合，以及零重建损失以实现连贯的源估计。", "result": "该方法在旨在展示特征子空间特化特性的玩具数据集上进行了实验，并在多导睡眠图研究的真实世界生物信号记录中（用于从心电图和光电容积描记信号中提取呼吸）得到了有效验证。", "conclusion": "该研究提出了一种新颖的多编码器自编码器方法，通过特征子空间特化、编码掩蔽技术以及特定的损失函数，成功解决了单通道非线性混合信号的盲源分离挑战，并在合成和真实生物信号数据上得到了验证。", "translation": "盲源分离 (BSS) 任务涉及在不预先了解源或混合系统的情况下，从混合信号中分离出源信号。单通道混合信号和非线性混合信号是盲源分离中一个特别具有挑战性的问题。在本文中，我们提出了一种新颖的方法，通过利用多编码器自编码器的自然特征子空间特化能力，来解决单通道非线性混合信号的盲源分离问题。在训练阶段，我们的方法将输入解混合到多编码器网络的独立编码空间中，然后在解码器中重新混合这些表示以重建输入。然后为了执行源推断，我们引入了一种新颖的编码掩蔽技术，通过掩蔽除一个编码之外的所有编码，使解码器能够估计一个源信号。为此，我们还引入了稀疏混合损失，该损失鼓励在整个解码器中稀疏地重新混合源编码，以及在解码器上引入所谓的零重建损失以实现连贯的源估计。为了分析和评估我们的方法，我们在一个旨在展示特征子空间特化特性的玩具数据集上进行了实验，并使用多导睡眠图研究的真实世界生物信号记录（用于从心电图和光电容积描记信号中提取呼吸）进行了实验。", "summary": "本文提出了一种基于多编码器自编码器的新型盲源分离方法，专门用于处理单通道非线性混合信号。该方法利用多编码器自编码器的特征子空间特化能力，在训练时将输入解混合到独立的编码空间并进行重建，在推断时通过编码掩蔽技术和引入的稀疏混合损失、零重建损失来估计单个源信号。研究在合成玩具数据集和真实世界的生物信号（从ECG和PPG中提取呼吸）上验证了该方法的有效性。", "keywords": "盲源分离, 单通道混合, 多编码器自编码器, 特征子空间特化, 编码掩蔽", "comments": "该论文的创新点在于将多编码器自编码器应用于单通道非线性盲源分离这一极具挑战性的问题，并提出了独特的编码掩蔽技术和专门设计的损失函数（稀疏混合损失和零重建损失）来增强分离效果和源信号的连贯性。其重要性体现在为处理复杂实际信号（如生物医学信号）的盲源分离提供了新的思路和工具。"}}
{"id": "2303.05263", "title": "Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature", "authors": ["Chengkun Li", "Grégoire Clarté", "Martin Jørgensen", "Luigi Acerbi"], "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Statistics and Computing", "url": "http://arxiv.org/abs/2303.05263v4", "summary": "In applied Bayesian inference scenarios, users may have access to a large\nnumber of pre-existing model evaluations, for example from maximum-a-posteriori\n(MAP) optimization runs. However, traditional approximate inference techniques\nmake little to no use of this available information. We propose the framework\nof post-process Bayesian inference as a means to obtain a quick posterior\napproximation from existing target density evaluations, with no further model\ncalls. Within this framework, we introduce Variational Sparse Bayesian\nQuadrature (VSBQ), a method for post-process approximate inference for models\nwith black-box and potentially noisy likelihoods. VSBQ reuses existing target\ndensity evaluations to build a sparse Gaussian process (GP) surrogate model of\nthe log posterior density function. Subsequently, we leverage sparse-GP\nBayesian quadrature combined with variational inference to achieve fast\napproximate posterior inference over the surrogate. We validate our method on\nchallenging synthetic scenarios and real-world applications from computational\nneuroscience. The experiments show that VSBQ builds high-quality posterior\napproximations by post-processing existing optimization traces, with no further\nmodel evaluations.", "comment": "Accepted for publication in Statistics and Computing", "pdf_url": "http://arxiv.org/pdf/2303.05263v4", "cate": "stat.ML", "date": "2023-03-09", "updated": "2025-07-23", "AI": {"title_translation": "变分稀疏贝叶斯积分的快速后处理贝叶斯推断", "tldr": "提出VSBQ方法，利用现有模型评估数据进行快速后处理贝叶斯推断，无需额外模型调用。", "motivation": "在应用贝叶斯推断中，现有的大量模型评估（例如来自MAP优化运行）未被传统近似推断技术有效利用，导致效率低下。", "method": "提出“后处理贝叶斯推断”框架，并引入变分稀疏贝叶斯积分（VSBQ）。VSBQ通过重用现有目标密度评估来构建对数后验密度函数的稀疏高斯过程（GP）替代模型，然后结合稀疏GP贝叶斯积分和变分推断，在替代模型上实现快速近似后验推断。该方法适用于黑盒和潜在噪声似然模型。", "result": "实验证明VSBQ通过后处理现有优化轨迹，无需额外模型评估，即可构建高质量的后验近似。", "conclusion": "VSBQ提供了一种高效、高质量的后处理贝叶斯推断方法，特别适用于利用现有模型评估数据进行快速后验近似。", "translation": "在应用贝叶斯推断场景中，用户可能拥有大量预先存在的模型评估数据，例如来自最大后验（MAP）优化运行的数据。然而，传统的近似推断技术很少或根本不利用这些可用信息。我们提出了后处理贝叶斯推断框架，作为一种从现有目标密度评估中获得快速后验近似的方法，无需进一步的模型调用。在此框架内，我们引入了变分稀疏贝叶斯积分（VSBQ），这是一种针对黑盒和潜在噪声似然模型的后处理近似推断方法。VSBQ重用现有目标密度评估来构建对数后验密度函数的稀疏高斯过程（GP）替代模型。随后，我们利用稀疏GP贝叶斯积分结合变分推断，在替代模型上实现快速近似后验推断。我们在具有挑战性的合成场景和计算神经科学的实际应用中验证了我们的方法。实验表明，VSBQ通过后处理现有优化轨迹，无需进一步的模型评估，即可构建高质量的后验近似。", "summary": "本文提出了一种名为变分稀疏贝叶斯积分（VSBQ）的新方法，用于后处理贝叶斯推断。该方法旨在利用现有的大量模型评估数据（如MAP优化结果）来快速近似后验分布，而无需进行额外的模型调用。VSBQ通过构建对数后验密度的稀疏高斯过程替代模型，并结合变分推断，实现了高效、高质量的后验近似，已在合成和真实世界应用中得到验证。", "keywords": "贝叶斯推断, 后处理, 变分稀疏贝叶斯积分, 高斯过程, 近似推断", "comments": "VSBQ的创新之处在于其“后处理”思想，能够充分利用已有的计算资源，避免重复的模型评估，这对于计算成本高昂的贝叶斯推断场景具有重要意义。通过结合稀疏GP和变分推断，该方法在效率和近似质量之间取得了良好的平衡。"}}
{"id": "2507.17722", "title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems", "authors": ["Malsha Ashani Mahawatta Dona", "Beatriz Cabrero-Daniel", "Yinan Yu", "Christian Berger"], "categories": ["cs.CV", "I.4.m"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in The IEEE International Conference on Intelligent Transportation Systems (ITSC)2025", "url": "http://arxiv.org/abs/2507.17722v1", "summary": "Large language models (LLMs) are growingly extended to process multimodal\ndata such as text and video simultaneously. Their remarkable performance in\nunderstanding what is shown in images is surpassing specialized neural networks\n(NNs) such as Yolo that is supporting only a well-formed but very limited\nvocabulary, ie., objects that they are able to detect. When being\nnon-restricted, LLMs and in particular state-of-the-art vision language models\n(VLMs) show impressive performance to describe even complex traffic situations.\nThis is making them potentially suitable components for automotive perception\nsystems to support the understanding of complex traffic situations or edge case\nsituation. However, LLMs and VLMs are prone to hallucination, which mean to\neither potentially not seeing traffic agents such as vulnerable road users who\nare present in a situation, or to seeing traffic agents who are not there in\nreality. While the latter is unwanted making an ADAS or autonomous driving\nsystems (ADS) to unnecessarily slow down, the former could lead to disastrous\ndecisions from an ADS. In our work, we are systematically assessing the\nperformance of 3 state-of-the-art VLMs on a diverse subset of traffic\nsituations sampled from the Waymo Open Dataset to support safety guardrails for\ncapturing such hallucinations in VLM-supported perception systems. We observe\nthat both, proprietary and open VLMs exhibit remarkable image understanding\ncapabilities even paying thorough attention to fine details sometimes difficult\nto spot for us humans. However, they are also still prone to making up elements\nin their descriptions to date requiring hallucination detection strategies such\nas BetterCheck that we propose in our work.", "comment": "Accepted in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)2025", "pdf_url": "http://arxiv.org/pdf/2507.17722v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "BetterCheck：迈向保障汽车感知系统中的视觉语言模型", "tldr": "视觉语言模型（VLMs）在汽车感知系统中有巨大潜力，但易受幻觉影响。本研究评估了VLMs的性能，并提出了BetterCheck方法来检测这些幻觉，以保障系统安全。", "motivation": "大型语言模型（LLMs）和视觉语言模型（VLMs）在理解复杂交通情况方面表现出色，使其成为汽车感知系统的潜在组成部分。然而，它们容易产生幻觉，即可能遗漏实际存在的交通参与者（如弱势道路使用者）或虚构不存在的交通参与者。前者可能导致自动驾驶系统（ADS）做出灾难性决策，因此需要安全保障来捕获这些幻觉。", "method": "系统性评估了3个最先进的视觉语言模型（VLMs）在从Waymo开放数据集采样的多样化交通情境子集上的性能，以支持捕获VLM支持的感知系统中幻觉的安全防护措施。本研究提出了BetterCheck作为一种幻觉检测策略。", "result": "观察到专有和开源的视觉语言模型（VLMs）都表现出卓越的图像理解能力，甚至能细致地关注人类难以发现的细节。然而，它们在描述中仍然容易虚构元素，这需要像本研究中提出的BetterCheck这样的幻觉检测策略。", "conclusion": "视觉语言模型（VLMs）尽管在图像理解方面表现出色，但仍易于产生幻觉，因此需要有效的幻觉检测策略（如BetterCheck）来确保其在汽车感知系统中的安全部署。", "translation": "大型语言模型（LLMs）正日益扩展到同时处理文本和视频等多模态数据。它们在理解图像内容方面的卓越性能正在超越专门的神经网络（NNs），例如Yolo，后者仅支持有限但结构良好的词汇表，即它们能够检测的对象。在不受限制的情况下，LLMs，特别是最先进的视觉语言模型（VLMs），在描述复杂的交通情况时表现出令人印象深刻的性能。这使得它们可能成为汽车感知系统的合适组件，以支持对复杂交通情况或边缘情况的理解。然而，LLMs和VLMs容易产生幻觉，这意味着它们可能未能看到实际存在的交通参与者（如弱势道路使用者），或者看到了实际不存在的交通参与者。虽然后者是不受欢迎的，会导致ADAS或自动驾驶系统（ADS）不必要地减速，但前者可能导致ADS做出灾难性决策。在我们的工作中，我们系统地评估了3个最先进的VLMs在从Waymo开放数据集采样的多样化交通情境子集上的性能，以支持捕获VLM支持的感知系统中此类幻觉的安全防护措施。我们观察到，专有和开源的VLMs都表现出卓越的图像理解能力，甚至能细致地关注我们人类有时难以发现的细节。然而，它们至今仍然容易在描述中虚构元素，这需要像我们在工作中提出的BetterCheck这样的幻觉检测策略。", "summary": "视觉语言模型（VLMs）在理解复杂交通情境方面表现出卓越能力，使其在汽车感知系统中具有巨大潜力。然而，VLMs易受幻觉影响，即可能遗漏实际存在的交通参与者或虚构不存在的交通参与者，这可能导致自动驾驶系统做出危险决策。本研究系统评估了3个最先进的VLMs在Waymo开放数据集上的性能，发现它们在图像理解方面表现出色，但仍普遍存在幻觉问题。为此，论文提出了“BetterCheck”策略，旨在检测并捕获VLM支持的感知系统中的此类幻觉，从而保障汽车系统的安全。", "keywords": "视觉语言模型, 汽车感知, 幻觉检测, BetterCheck, 安全", "comments": "该论文解决了在自动驾驶等高风险应用中部署先进视觉语言模型（VLMs）的关键安全问题。其关注幻觉检测的创新方法对于确保实际部署的可靠性至关重要。利用Waymo开放数据集进行评估增加了研究结果的可信度，为未来VLMs在汽车领域安全应用奠定了基础。"}}
{"id": "2507.17655", "title": "Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses", "authors": ["Shams Shaikh", "Trima P. Fernandes e Fizardo"], "categories": ["cs.CR", "cs.NI", "cs.SE", "C.2.4; D.4.6; E.3; E.5; K.6.5"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      9 pages, 2 Flowcharts, 2 Tables", "url": "http://arxiv.org/abs/2507.17655v1", "summary": "As organizations rapidly migrate to the cloud, the security of cryptographic\nkey management has become a growing concern. Hardware Security Modules (HSMs)\nand Trusted Platform Modules (TPMs), traditionally seen as the gold standard\nfor securing encryption keys and digital trust, are increasingly challenged by\ncloud-native threats. Real-world breaches have exposed weaknesses in cloud\ndeployments, including misconfigurations, API abuse, and privilege escalations,\nallowing attackers to access sensitive key material and bypass protections.\nThese incidents reveal that while the hardware remains secure, the surrounding\ncloud ecosystem introduces systemic vulnerabilities. This paper analyzes\nnotable security failures involving HSMs and TPMs, identifies common attack\nvectors, and questions longstanding assumptions about their effectiveness in\ndistributed environments. We explore alternative approaches such as\nconfidential computing, post-quantum cryptography, and decentralized key\nmanagement. Our findings highlight that while HSMs and TPMs still play a role,\nmodern cloud security requires more adaptive, layered architectures. By\nevaluating both current weaknesses and emerging models, this research equips\ncloud architects and security engineers with strategies to reinforce\ncryptographic trust in the evolving threat landscape.", "comment": "9 pages, 2 Flowcharts, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.17655v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "重新思考云中的HSM和TPM安全：真实世界攻击与下一代防御", "tldr": "本文分析了云环境中HSM和TPM的现有安全漏洞，并探讨了适应性、分层架构的重要性。", "motivation": "随着组织快速迁移到云端，加密密钥管理的安全问题日益突出。传统上被视为安全黄金标准的硬件安全模块（HSM）和可信平台模块（TPM）正面临云原生威胁的挑战，真实世界的泄露事件暴露了云部署中的弱点。", "method": "本文分析了涉及HSM和TPM的显著安全故障，识别了常见的攻击向量，并质疑了它们在分布式环境中有效性的长期假设。同时，探讨了机密计算、后量子密码学和去中心化密钥管理等替代方法。", "result": "研究发现，虽然硬件本身仍然安全，但周围的云生态系统引入了系统性漏洞。HSM和TPM仍发挥作用，但现代云安全需要更具适应性的分层架构。", "conclusion": "HSM和TPM在云安全中仍有其作用，但为了应对不断演变的安全威胁，现代云安全需要采用更具适应性的分层架构来强化加密信任。", "translation": "随着组织快速迁移到云端，加密密钥管理的安全问题日益突出。硬件安全模块（HSM）和可信平台模块（TPM）传统上被视为保护加密密钥和数字信任的黄金标准，但正日益受到云原生威胁的挑战。真实世界的泄露事件暴露了云部署中的弱点，包括错误配置、API滥用和权限升级，使得攻击者能够访问敏感密钥材料并绕过保护措施。这些事件表明，虽然硬件本身仍然安全，但周围的云生态系统引入了系统性漏洞。本文分析了涉及HSM和TPM的显著安全故障，识别了常见的攻击向量，并质疑了它们在分布式环境中有效性的长期假设。我们探讨了机密计算、后量子密码学和去中心化密钥管理等替代方法。我们的研究结果强调，虽然HSM和TPM仍然发挥作用，但现代云安全需要更具适应性的分层架构。通过评估当前弱点和新兴模型，本研究为云架构师和安全工程师提供了在不断演变威胁环境下强化加密信任的策略。", "summary": "本文探讨了组织向云迁移过程中加密密钥管理面临的安全挑战，特别是硬件安全模块（HSM）和可信平台模块（TPM）在云环境中的局限性。研究分析了真实世界的安全漏洞，指出虽然硬件本身安全，但云生态系统引入了新的系统性漏洞。文章质疑了HSM和TPM在分布式环境中的传统有效性，并提出了包括机密计算、后量子密码学和去中心化密钥管理在内的替代方法。最终强调，现代云安全需要采用更具适应性和分层的架构，以应对不断变化的威胁。", "keywords": "HSM, TPM, 云安全, 密钥管理, 威胁防御", "comments": "本文深入分析了传统硬件安全机制（HSM和TPM）在云环境中面临的挑战，其创新点在于不仅指出了现有问题，还探讨了下一代防御策略。重要性体现在为云架构师和安全工程师提供了实用的安全强化策略，以应对云原生威胁。"}}
{"id": "2507.13508", "title": "Fake or Real: The Impostor Hunt in Texts for Space Operations", "authors": ["Agata Kaczmarek", "Dawid Płudowski", "Piotr Wilczyński", "Krzysztof Kotowski", "Ramez Shendy", "Evridiki Ntagiou", "Jakub Nalepa", "Artur Janicki", "Przemysław Biecek"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13508v3", "summary": "The \"Fake or Real\" competition hosted on Kaggle\n(https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt ) is the\nsecond part of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/ ). The competition idea is\nbased on two real-life AI security threats identified within the project --\ndata poisoning and overreliance in Large Language Models. The task is to\ndistinguish between the proper output from LLM and the output generated under\nmalicious modification of the LLM. As this problem was not extensively\nresearched, participants are required to develop new techniques to address this\nissue or adjust already existing ones to this problem's statement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13508v3", "cate": "cs.LG", "date": "2025-07-17", "updated": "2025-07-23", "AI": {"title_translation": "假冒还是真实：太空操作文本中的冒名顶替者搜寻", "tldr": "Kaggle举办了一场关于识别大型语言模型在恶意修改下产生的输出的比赛，旨在解决数据投毒和过度依赖LLM的AI安全威胁。", "motivation": "该研究旨在解决太空领域AI应用中识别出的两个真实AI安全威胁：数据投毒和大型语言模型中的过度依赖问题。", "method": "本文描述了一项Kaggle竞赛，要求参与者开发新方法或调整现有方法来区分LLM的正常输出与恶意修改后生成的输出。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "Kaggle上举办的“假冒还是真实”竞赛（https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt）是欧洲空间局资助的“空间领域AI应用保障”项目（https://assurance-ai.space-codev.org/）系列后续竞赛和黑客马拉松的第二部分。竞赛理念基于该项目内识别出的两个真实AI安全威胁——数据投毒和大型语言模型中的过度依赖。任务是区分LLM的正确输出与在恶意修改LLM下生成的输出。由于这个问题尚未得到广泛研究，参与者需要开发新的技术来解决这个问题，或者调整已有的技术来适应这个问题。", "summary": "这篇论文介绍了Kaggle上的一项名为“假冒还是真实”的竞赛，该竞赛是欧洲空间局资助的“空间领域AI应用保障”项目的一部分。此竞赛旨在解决大型语言模型面临的数据投毒和过度依赖两大AI安全威胁，其核心任务是识别LLM的正常输出与恶意篡改后的输出。由于该领域研究不足，竞赛鼓励参与者开发或改进相关技术。", "keywords": "大型语言模型, AI安全, 数据投毒, 冒名顶替者检测, 太空操作", "comments": "这项竞赛及其背后的项目具有重要意义，因为它直接关注了大型语言模型在关键应用领域（如太空操作）中的实际AI安全威胁，特别是数据投毒和过度依赖问题。通过竞赛形式鼓励创新解决方案，有助于推动对这一未充分研究领域的探索和技术发展。"}}
{"id": "2507.17240", "title": "Perceptual Classifiers: Detecting Generative Images using Perceptual Features", "authors": ["Krishna Srikar Durbha", "Asvin Kumar Venkataramanan", "Rajesh Sureddi", "Alan C. Bovik"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 3 tables, ICCV VQualA Workshop 2025", "url": "http://arxiv.org/abs/2507.17240v1", "summary": "Image Quality Assessment (IQA) models are employed in many practical image\nand video processing pipelines to reduce storage, minimize transmission costs,\nand improve the Quality of Experience (QoE) of millions of viewers. These\nmodels are sensitive to a diverse range of image distortions and can accurately\npredict image quality as judged by human viewers. Recent advancements in\ngenerative models have resulted in a significant influx of \"GenAI\" content on\nthe internet. Existing methods for detecting GenAI content have progressed\nsignificantly with improved generalization performance on images from unseen\ngenerative models. Here, we leverage the capabilities of existing IQA models,\nwhich effectively capture the manifold of real images within a bandpass\nstatistical space, to distinguish between real and AI-generated images. We\ninvestigate the generalization ability of these perceptual classifiers to the\ntask of GenAI image detection and evaluate their robustness against various\nimage degradations. Our results show that a two-layer network trained on the\nfeature space of IQA models demonstrates state-of-the-art performance in\ndetecting fake images across generative models, while maintaining significant\nrobustness against image degradations.", "comment": "8 pages, 6 figures, 3 tables, ICCV VQualA Workshop 2025", "pdf_url": "http://arxiv.org/pdf/2507.17240v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "感知分类器：利用感知特征检测生成图像", "tldr": "本文利用现有的图像质量评估（IQA）模型来检测AI生成的图像，并通过在IQA特征空间上训练的双层网络实现了最先进的性能和鲁棒性。", "motivation": "互联网上AI生成内容（GenAI）的显著增长，以及现有检测方法对未见生成模型的泛化性能的进步，促使需要更有效的方法来区分真实和AI生成图像。", "method": "利用能够有效捕获真实图像流形的现有图像质量评估（IQA）模型的能力。具体地，在IQA模型的特征空间上训练一个双层网络，并研究其在GenAI图像检测任务中的泛化能力和对图像退化的鲁棒性。", "result": "在检测不同生成模型产生的假图像方面，在IQA模型特征空间上训练的双层网络展示了最先进的性能，同时对图像退化保持了显著的鲁棒性。", "conclusion": "结合IQA模型的感知特征可以有效地、鲁棒地检测AI生成的图像。", "translation": "图像质量评估（IQA）模型被应用于许多实际的图像和视频处理流程中，以减少存储、最小化传输成本并提高数百万观看者的体验质量（QoE）。这些模型对各种图像失真都很敏感，并且能够准确预测人类判断的图像质量。生成模型的最新进展导致互联网上“GenAI”内容的显著涌入。现有的GenAI内容检测方法已经取得了显著进展，对来自未见生成模型的图像具有改进的泛化性能。在此，我们利用现有IQA模型的能力，这些模型能够有效地在带通统计空间中捕获真实图像的流形，从而区分真实图像和AI生成图像。我们研究了这些感知分类器在GenAI图像检测任务中的泛化能力，并评估了它们对各种图像退化的鲁棒性。我们的结果表明，在IQA模型特征空间上训练的双层网络在检测跨生成模型的假图像方面展示了最先进的性能，同时对图像退化保持了显著的鲁棒性。", "summary": "本文提出了一种利用现有图像质量评估（IQA）模型来检测AI生成图像的方法。研究人员发现，IQA模型能够有效捕捉真实图像的特征空间。通过在此特征空间上训练一个双层网络，该方法在区分真实图像和AI生成图像方面取得了最先进的性能，并且对各种图像退化具有显著的鲁棒性。", "keywords": "感知分类器, 生成图像检测, 图像质量评估, GenAI, 鲁棒性", "comments": "这项工作创新性地利用了已有的图像质量评估（IQA）模型的感知特征来解决AI生成内容检测这一新兴且重要的问题。通过将IQA模型的特征空间作为输入，该方法不仅实现了高性能，还展现了对图像失真的鲁棒性，这对于实际应用至关重要。"}}
{"id": "2502.03491", "title": "LanPaint: Training-Free Diffusion Inpainting with Asymptotically Exact and Fast Conditional Sampling", "authors": ["Candi Zheng", "Yuan Lan", "Yang Wang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03491v2", "summary": "Diffusion models excel at joint pixel sampling for image generation but lack\nefficient training-free methods for partial conditional sampling (e.g.,\ninpainting with known pixels). Prior work typically formulates this as an\nintractable inverse problem, relying on coarse variational approximations,\nheuristic losses requiring expensive backpropagation, or slow stochastic\nsampling. These limitations preclude: (1) accurate distributional matching in\ninpainting results, (2) efficient inference modes without gradient, (3)\ncompatibility with fast ODE-based samplers. To address these limitations, we\npropose \\textbf{LanPaint}: a training-free, asymptotically exact partial\nconditional sampling methods for ODE-based and rectified flow diffusion models.\nBy leveraging carefully designed Langevin dynamics, LanPaint enables fast,\nbackpropagation-free Monte Carlo sampling. Experiments demonstrate that our\napproach achieves superior performance with precise partial conditioning and\nvisually coherent inpainting across diverse tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03491v2", "cate": "eess.IV", "date": "2025-02-05", "updated": "2025-07-23", "AI": {"title_translation": "LanPaint：基于渐进精确和快速条件采样的免训练扩散修复", "tldr": "LanPaint提出了一种免训练、渐进精确且快速的扩散模型局部条件采样方法，通过精心设计的Langevin动力学，实现了高效的图像修复。", "motivation": "扩散模型在图像生成方面表现出色，但在局部条件采样（如图像修复）方面缺乏高效的免训练方法。现有方法存在难以处理的逆问题、粗糙的变分近似、需要昂贵反向传播的启发式损失或缓慢的随机采样等局限性，导致修复结果的分布匹配不准确、推理效率低下且与快速ODE采样器不兼容。", "method": "本文提出了LanPaint，一种针对基于ODE和修正流扩散模型的免训练、渐进精确的局部条件采样方法。通过利用精心设计的Langevin动力学，LanPaint实现了快速、无需反向传播的蒙特卡洛采样。", "result": "实验证明，LanPaint在各种任务中都能实现卓越的性能，具有精确的局部条件性和视觉上连贯的图像修复效果。", "conclusion": "LanPaint通过引入一种免训练、渐进精确且快速的局部条件采样方法，有效解决了扩散模型在图像修复中面临的效率和准确性问题，并与快速ODE采样器兼容。", "translation": "扩散模型在图像生成的联合像素采样方面表现出色，但缺乏用于部分条件采样（例如，使用已知像素进行图像修复）的有效免训练方法。现有工作通常将其表述为一个难以处理的逆问题，依赖于粗糙的变分近似、需要昂贵反向传播的启发式损失，或缓慢的随机采样。这些局限性阻碍了：（1）图像修复结果中准确的分布匹配，（2）无需梯度的有效推理模式，（3）与基于ODE的快速采样器的兼容性。为了解决这些局限性，我们提出了 LanPaint：一种针对基于ODE和修正流扩散模型的免训练、渐进精确的部分条件采样方法。通过利用精心设计的Langevin动力学，LanPaint 实现了快速、无需反向传播的蒙特卡洛采样。实验表明，我们的方法在各种任务中都能以精确的部分条件性和视觉上连贯的图像修复实现卓越的性能。", "summary": "LanPaint提出了一种针对扩散模型的免训练图像修复方法，解决了现有局部条件采样方法在准确性、效率和与快速采样器兼容性方面的局限性。该方法利用精心设计的Langevin动力学，实现了渐进精确且快速的、无需反向传播的蒙特卡洛采样，并在实验中展示了卓越的图像修复性能。", "keywords": "扩散模型, 图像修复, 条件采样, Langevin动力学, 免训练", "comments": "LanPaint的创新之处在于其免训练特性和与快速ODE采样器的兼容性，这极大地提高了扩散模型在图像修复任务中的实用性。通过利用Langevin动力学实现渐进精确和快速采样，该方法克服了传统方法对昂贵反向传播或缓慢采样的依赖，为条件图像生成提供了一个高效且精确的解决方案。"}}
{"id": "2408.16446", "title": "Is text normalization relevant for classifying medieval charters?", "authors": ["Florian Atzenhofer-Baumgartner", "Tamás Kovács"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in LNCS volume 15178 and is available online at this https URL", "url": "http://arxiv.org/abs/2408.16446v2", "summary": "This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in LNCS volume 15178 and is available online at\n  https://doi.org/10.1007/978-3-031-72440-4_12", "pdf_url": "http://arxiv.org/pdf/2408.16446v2", "cate": "cs.CL", "date": "2024-08-29", "updated": "2025-07-23", "AI": {"title_translation": "文本标准化与中世纪宪章分类相关吗？", "tldr": "本研究发现，对于中世纪宪章的分类（尤其是年代和地点识别），文本标准化对地点识别有微小改进，但会降低年代识别的准确性。支持向量机和梯度提升模型表现优于Transformer模型。", "motivation": "该研究旨在探究历史文本标准化对中世纪宪章分类的影响，特别是针对文档的年代和地点识别任务。", "method": "研究使用了来自数字档案的中高德语宪章数据集，并评估了包括传统模型和基于Transformer的模型在内的各种分类器，分别在有无标准化的情况下进行测试。", "result": "结果表明，给定的标准化对地点识别任务的改善微乎其微，但降低了年代识别的准确性。这暗示原始文本包含标准化可能掩盖的关键特征。研究还发现支持向量机和梯度提升模型优于其他模型，对Transformer模型在此用例中的效率提出了质疑。", "conclusion": "结论是，对于历史文本标准化应采取选择性方法，强调保留对文档分析中分类任务至关重要的某些文本特征的重要性。", "translation": "本研究探讨了历史文本标准化对中世纪宪章分类的影响，特别是侧重于文档的年代和地点识别。我们使用来自数字档案的中高德语宪章数据集，评估了各种分类器，包括传统模型和基于Transformer的模型，分别在标准化和非标准化的情况下进行测试。结果表明，给定的标准化对地点识别任务的改善微乎其其微，但降低了年代识别的准确性，这意味着原始文本包含标准化可能掩盖的关键特征。我们发现支持向量机和梯度提升模型优于其他模型，对Transformer模型在此用例中的效率提出了质疑。结果表明，应选择性地进行历史文本标准化，强调保留对文档分析中分类任务至关重要的某些文本特征的重要性。", "summary": "本研究评估了历史文本标准化对中世纪宪章分类（包括年代和地点识别）的影响。研究利用中高德语宪章数据集，比较了传统和Transformer模型在有无标准化情况下的表现。结果显示，标准化对地点识别略有帮助，但对年代识别有害，表明原始文本中的关键特征可能被掩盖。此外，支持向量机和梯度提升模型表现优于Transformer模型，提出应选择性地应用文本标准化以保留重要的文本特征。", "keywords": "文本标准化, 中世纪宪章, 文档分类, 支持向量机, 梯度提升", "comments": "这项研究的创新之处在于它直接挑战了在历史文本处理中普遍认为的标准化总是有益的观点。它强调了在处理此类数据时，原始文本中可能包含对特定分类任务至关重要的语言学和历史特征。对Transformer模型在此特定任务中表现不佳的发现也很有趣，这可能意味着在某些特定领域，传统机器学习模型仍然具有优势或更高效。其局限性可能在于其结论的普适性，因为结果是基于特定语言（中高德语）和特定类型文档（中世纪宪章）的数据集。"}}
{"id": "2507.00748", "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning", "authors": ["Bob Zhang", "Haoran Li", "Tao Zhang", "Cilin Yan", "Jiayin Cai", "Yanbin Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.00748v2", "summary": "Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications that involve complex multi-image\ncompositions and multi-modal instructions, revealing limitations in cross-image\nreasoning and generalization. To address these challenges, we adopt a\nReinforcement Learning (RL) based post-training strategy to improve the\nreasoning of MLLMs in multi-image grounding tasks. Our approach begins with\nsynthesizing high-quality chain-of-thought (CoT) data for cold-start\ninitialization, followed by supervised fine-tuning (SFT) using low-rank\nadaptation (LoRA). The cold-start training stage enables the model to identify\ncorrect solutions. Subsequently, we perform rejection sampling using the merged\nSFT model to curate high-quality RL data and leverage rule-based RL to guide\nthe model toward optimal reasoning paths. Extensive experimental results\ndemonstrate the effectiveness of our approach, yielding improvements of +9.04%\non MIG-Bench, +6.37% on MC-Bench, and +4.98% on several out-of-domain reasoning\ngrounding benchmarks compared to the SFT baseline. Furthermore, our method\nexhibits strong generalization in multi-image perception, with gains of +3.1%\nand +2.4% over the base model on BLINK and MMIU benchmarks, respectively.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.00748v2", "cate": "cs.CV", "date": "2025-07-01", "updated": "2025-07-23", "AI": {"title_translation": "通过强化学习改进多模态大语言模型中的多图像定位推理", "tldr": "本文提出了一种基于强化学习的后训练策略，通过合成CoT数据、SFT和规则强化学习，显著提高了多模态大语言模型在多图像定位任务中的推理能力和泛化性。", "motivation": "多模态大语言模型（MLLMs）在单图像场景中的视觉定位表现出色，但在涉及复杂多图像组合和多模态指令的实际应用中性能下降，这暴示出其在跨图像推理和泛化方面的局限性。", "method": "本文采用了一种基于强化学习（RL）的后训练策略来改进MLLMs在多图像定位任务中的推理能力。该方法首先合成高质量的思维链（CoT）数据进行冷启动初始化，然后使用低秩适应（LoRA）进行监督微调（SFT）。冷启动训练阶段使模型能够识别正确的解决方案。随后，使用合并的SFT模型进行拒绝采样以策划高质量的RL数据，并利用基于规则的RL引导模型走向最佳推理路径。", "result": "实验结果表明，与SFT基线相比，该方法在MIG-Bench上提高了+9.04%，在MC-Bench上提高了+6.37%，在几个域外推理定位基准上提高了+4.98%。此外，该方法在多图像感知方面表现出强大的泛化能力，在BLINK和MMIU基准上分别比基础模型提高了+3.1%和+2.4%。", "conclusion": "本文提出的基于强化学习的后训练策略显著提高了多模态大语言模型在多图像定位任务中的推理能力和泛化性，并在多个基准测试中取得了显著的性能提升。", "translation": "最近，多模态大语言模型（MLLMs）在单图像场景中通过文本参考进行视觉定位方面表现出色。然而，当处理涉及复杂多图像组合和多模态指令的实际应用时，它们的性能会下降，这揭示了其在跨图像推理和泛化方面的局限性。为了解决这些挑战，我们采用了一种基于强化学习（RL）的后训练策略来改进MLLMs在多图像定位任务中的推理能力。我们的方法首先合成高质量的思维链（CoT）数据进行冷启动初始化，然后使用低秩适应（LoRA）进行监督微调（SFT）。冷启动训练阶段使模型能够识别正确的解决方案。随后，我们使用合并的SFT模型进行拒绝采样以策划高质量的RL数据，并利用基于规则的RL引导模型走向最佳推理路径。广泛的实验结果表明了我们方法的有效性，与SFT基线相比，在MIG-Bench上取得了+9.04%的改进，在MC-Bench上取得了+6.37%的改进，在几个域外推理定位基准上取得了+4.98%的改进。此外，我们的方法在多图像感知方面表现出强大的泛化能力，在BLINK和MMIU基准上分别比基础模型提高了+3.1%和+2.4%。", "summary": "本文针对多模态大语言模型（MLLMs）在多图像定位任务中推理和泛化能力不足的问题，提出了一种基于强化学习的后训练策略。该方法结合了高质量思维链（CoT）数据合成、LoRA监督微调以及基于规则的强化学习，以引导模型发现最佳推理路径。实验结果表明，该方法在多个多图像定位和泛化基准测试中均取得了显著性能提升。", "keywords": "多模态大语言模型, 强化学习, 多图像定位, 推理, 泛化", "comments": "该论文的创新点在于将强化学习应用于MLLMs的多图像定位推理能力提升，特别是通过合成CoT数据进行冷启动和规则强化学习来优化推理路径。这对于解决MLLMs在复杂多图像场景下的泛化和跨图像推理限制具有重要意义。"}}
{"id": "2501.13686", "title": "Learning in Conjectural Stackelberg Games", "authors": ["Francesco Morri", "Hélène Le Cadre", "Luce Brotcorne"], "categories": ["cs.GT", "cs.MA"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      Presented at Games, Agents and Incentives Workshops at AAMAS 2025", "url": "http://arxiv.org/abs/2501.13686v3", "summary": "We extend the formalism of Conjectural Variations games to Stackelberg games\ninvolving multiple leaders and a single follower. To solve these nonconvex\ngames, a common assumption is that the leaders compute their strategies having\nperfect knowledge of the follower's best response. However, in practice, the\nleaders may have little to no knowledge about the other players' reactions. To\ndeal with this lack of knowledge, we assume that each leader can form\nconjectures about the other players' best responses, and update its strategy\nrelying on these conjectures. Our contributions are twofold: (i) On the\ntheoretical side, we introduce the concept of Conjectural Stackelberg\nEquilibrium -- keeping our formalism conjecture agnostic -- with Stackelberg\nEquilibrium being a refinement of it. (ii) On the algorithmic side, we\nintroduce a two-stage algorithm with guarantees of convergence, which allows\nthe leaders to first learn conjectures on a training data set, and then update\ntheir strategies. Theoretical results are illustrated numerically.", "comment": "Presented at Games, Agents and Incentives Workshops at AAMAS 2025", "pdf_url": "http://arxiv.org/pdf/2501.13686v3", "cate": "cs.GT", "date": "2025-01-23", "updated": "2025-07-23", "AI": {"title_translation": "推测性斯塔克尔伯格博弈中的学习", "tldr": "本文将推测性变异博弈扩展到多领导者单跟随者的斯塔克尔伯格博弈，引入推测性斯塔克尔伯格均衡概念，并提出一种两阶段收敛算法，使领导者能在缺乏完美知识的情况下学习和更新策略。", "motivation": "在多领导者单跟随者的斯塔克尔伯格博弈中，通常假设领导者完美了解跟随者的最优响应，但实践中领导者可能对其他参与者的反应知之甚少。", "method": "假设每个领导者可以形成关于其他参与者最优响应的推测，并依赖这些推测更新其策略。提出了推测性斯塔克尔伯格均衡的概念，并引入了一种具有收敛性保证的两阶段算法，该算法允许领导者首先在训练数据集上学习推测，然后更新其策略。", "result": "理论上引入了推测性斯塔克尔伯格均衡的概念，并证明斯塔克尔伯格均衡是其的一个细化。算法上提出了一种具有收敛性保证的两阶段算法。理论结果通过数值模拟得到验证。", "conclusion": "本文成功地将推测性变异博弈扩展到斯塔克尔伯格博弈，解决了领导者缺乏完美知识的问题，并通过理论和算法上的贡献，为解决这类非凸博弈提供了新的框架和可行的学习方法。", "translation": "我们将推测性变异博弈的形式主义扩展到涉及多个领导者和一个跟随者的斯塔克尔伯格博弈。为了解决这些非凸博弈，一个常见的假设是领导者在完全了解跟随者最优响应的情况下计算其策略。然而，在实践中，领导者可能对其他玩家的反应知之甚少。为了解决这种知识缺乏的问题，我们假设每个领导者可以形成关于其他玩家最优响应的推测，并依赖这些推测更新其策略。我们的贡献是双重的：(i) 在理论方面，我们引入了推测性斯塔克尔伯格均衡的概念——保持我们的形式主义与推测无关——其中斯塔克尔伯格均衡是其的一个细化。(ii) 在算法方面，我们引入了一种具有收敛性保证的两阶段算法，该算法允许领导者首先在训练数据集上学习推测，然后更新其策略。理论结果通过数值模拟得到说明。", "summary": "本文将推测性变异博弈应用于多领导者单跟随者的斯塔克尔伯格博弈，以解决领导者在缺乏其他玩家反应完美知识情况下的策略制定问题。作者提出了推测性斯塔克尔伯格均衡的概念，并设计了一种两阶段学习算法，使领导者能够首先学习推测，然后更新其策略，并保证收敛性。研究的理论结果通过数值实验进行了验证。", "keywords": "推测性斯塔克尔伯格博弈, 推测性变异, 多领导者, 博弈论, 学习算法", "comments": "本文的创新之处在于将推测性变异的概念引入到复杂的斯塔克尔伯格博弈中，从而解决了现实世界中领导者信息不完全的问题。提出的推测性斯塔克尔伯格均衡概念和两阶段学习算法为这类非凸博弈提供了一个实用的框架，具有重要的理论和应用价值。"}}
{"id": "2507.17294", "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback", "authors": ["Jianxin Bi", "Kevin Yuchen Ma", "Ce Hao", "Mike Zheng Shou", "Harold Soh"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      19 pages, 5 figures", "url": "http://arxiv.org/abs/2507.17294v1", "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.17294v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VLA-Touch：通过双层触觉反馈增强视觉-语言-动作模型", "tldr": "VLA-Touch通过引入双层触觉反馈，无需微调即可提升VLA模型在接触密集型任务中的机器人策略，提高任务规划效率和执行精度。", "motivation": "现有视觉-语言-动作（VLA）模型缺乏解释和使用触觉信号的能力，限制了它们在接触密集型任务中的有效性。同时，缺乏大型多模态数据集使得将触觉反馈整合到这些系统中具有挑战性。", "method": "提出VLA-Touch方法，通过引入两项关键创新来增强通用机器人策略的触觉感知，而无需微调基础VLA模型：1) 一个利用预训练触觉-语言模型的管道，为高层任务规划提供语义触觉反馈；2) 一个基于扩散的控制器，利用触觉信号细化VLA生成的动作，以实现接触密集型操作。", "result": "真实世界实验表明，双层触觉反馈的集成提高了任务规划效率，同时增强了执行精度。", "conclusion": "VLA-Touch通过双层触觉反馈的集成，有效提升了机器人策略在接触密集型任务中的表现，解决了现有VLA模型在触觉感知方面的不足。", "translation": "触觉反馈通常被认为是与物理世界有效交互的关键。然而，最先进的视觉-语言-动作（VLA）模型缺乏解释和使用触觉信号的能力，这限制了它们在接触密集型任务中的有效性。由于缺乏大型多模态数据集，将触觉反馈整合到这些系统中具有挑战性。我们提出了VLA-Touch，一种在不微调基础VLA的情况下，通过触觉感知增强通用机器人策略的方法。我们的方法引入了两项关键创新：（1）一个利用预训练触觉-语言模型的管道，为高层任务规划提供语义触觉反馈，以及（2）一个基于扩散的控制器，利用触觉信号细化VLA生成的动作，以实现接触密集型操作。通过真实世界实验，我们证明了双层触觉反馈的集成提高了任务规划效率，同时增强了执行精度。代码已在[此URL](https://github.com/jxbi1010/VLA-Touch)开源。", "summary": "VLA-Touch提出了一种新颖的方法，通过双层触觉反馈集成来增强视觉-语言-动作（VLA）模型在接触密集型任务中的能力，而无需对基础VLA进行微调。该方法包含一个利用预训练触觉-语言模型进行高层规划的管道和一个基于扩散的控制器进行精细操作。实验证明，这种集成显著提升了任务规划效率和执行精度。", "keywords": "触觉反馈, 视觉-语言-动作模型, 机器人策略, 扩散模型, 双层集成", "comments": "VLA-Touch的创新之处在于其双层触觉反馈集成机制，尤其是在不微调现有VLA模型的情况下实现增强，这对于实际应用具有重要意义。通过结合语言和扩散模型处理触觉信息，它为机器人更精细地感知和操作物理世界提供了新的途径。该方法解决了多模态数据缺乏的挑战，并提升了机器人在复杂接触任务中的性能。"}}
{"id": "2507.16963", "title": "Impact of Communication Delay and Sampling on Small-Signal Stability of IBR-rich Power Systems", "authors": ["Saugat Ghimire", "Vaithianathan \"Mani\" Venkatasubramanian", "Gilles Torresan"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16963v1", "summary": "The growing adoption of inverter-based resources (IBRs) has introduced\nunprecedented dynamics in power systems, resulting in oscillations across a\nbroad spectrum of frequencies. Communication delay between the plant-level\ncontrol and the inverter-level control in IBR plants has been recognized as one\nof the causes of such oscillations and a factor that impacts the system's\nstability. The control signals from the plant-level controller also experience\nsampling, with the sampled values held constant by the hold elements for the\nduration of the sampling period. This also has a bearing on the response of IBR\nplants. In this paper, we analyze the impacts of communication delay and\nsampling of control signals between plant-level control and inverter-level\ncontrol of grid-following IBR plants on the small-signal stability of power\nsystems. The underlying fundamentals of communication delay and sampling are\nrevisited to explain the observed responses. Our findings emphasize the unique\neffects of communication delay and sampling period on the stability of IBR-rich\npower systems and suggest strategies to mitigate their detrimental impacts. The\nwork also highlights the need for more accurate approaches for small-signal\nstability analysis of such systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16963v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "通信延迟和采样对富含逆变器并网资源电力系统小信号稳定性的影响", "tldr": "本文分析了逆变器并网资源（IBR）电力系统中，厂级控制与逆变器级控制之间通信延迟和采样对小信号稳定性的影响，并提出了缓解策略和对更精确分析方法的需求。", "motivation": "逆变器并网资源（IBRs）的日益普及给电力系统带来了前所未有的动态，导致了广泛频率范围内的振荡。其中，厂级控制与逆变器级控制之间的通信延迟以及控制信号的采样被认为是影响系统稳定性的原因。", "method": "本文分析了并网型IBR电厂的厂级控制与逆变器级控制之间通信延迟和控制信号采样对电力系统小信号稳定性的影响。研究回顾了通信延迟和采样的基本原理以解释观察到的响应。", "result": "研究发现通信延迟和采样周期对富含IBR的电力系统稳定性具有独特影响，并提出了缓解其有害影响的策略。工作还强调了对此类系统进行更精确小信号稳定性分析的必要性。", "conclusion": "通信延迟和采样对富含IBR的电力系统小信号稳定性有独特影响，需要采取策略来减轻其不利影响，并且需要更精确的分析方法。", "translation": "逆变器并网资源（IBRs）的日益普及给电力系统带来了前所未有的动态，导致了广泛频率范围内的振荡。IBR电厂中厂级控制与逆变器级控制之间的通信延迟已被认为是此类振荡的原因之一，也是影响系统稳定性的一个因素。来自厂级控制器的控制信号也经历采样，采样值在采样周期内由保持元件保持恒定。这也会影响IBR电厂的响应。在本文中，我们分析了并网型IBR电厂的厂级控制与逆变器级控制之间通信延迟和控制信号采样对电力系统小信号稳定性的影响。本文回顾了通信延迟和采样的基本原理以解释观察到的响应。我们的发现强调了通信延迟和采样周期对富含IBR电力系统稳定性的独特影响，并提出了减轻其有害影响的策略。这项工作还强调了对此类系统进行更精确小信号稳定性分析的必要性。", "summary": "本文研究了逆变器并网资源（IBR）日益增多对电力系统稳定性带来的挑战，特别是关注厂级控制与逆变器级控制之间的通信延迟和控制信号采样对小信号稳定性的影响。研究重新审视了这些现象的基本原理，揭示了它们对富含IBR电力系统稳定性的独特影响，并提出了潜在的缓解策略。此外，文章强调了开发更精确的小信号稳定性分析方法的重要性。", "keywords": "逆变器并网资源, 小信号稳定性, 通信延迟, 采样, 电力系统", "comments": "本文深入分析了通信延迟和采样这两个在含IBR电力系统中日益突出的问题对小信号稳定性的影响。其创新之处在于重新审视了这些基本因素，并强调了它们对系统稳定性的独特影响，这对于未来含IBR电力系统的设计和运行具有重要指导意义。此外，提出需要更精确的分析方法也指出了该领域未来研究的方向。"}}
{"id": "2507.17735", "title": "Accent Normalization Using Self-Supervised Discrete Tokens with Non-Parallel Data", "authors": ["Qibing Bai", "Sho Inoue", "Shuai Wang", "Zhongjie Jiang", "Yannan Wang", "Haizhou Li"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to INTERSPEECH 2025", "url": "http://arxiv.org/abs/2507.17735v1", "summary": "Accent normalization converts foreign-accented speech into native-like speech\nwhile preserving speaker identity. We propose a novel pipeline using\nself-supervised discrete tokens and non-parallel training data. The system\nextracts tokens from source speech, converts them through a dedicated model,\nand synthesizes the output using flow matching. Our method demonstrates\nsuperior performance over a frame-to-frame baseline in naturalness,\naccentedness reduction, and timbre preservation across multiple English\naccents. Through token-level phonetic analysis, we validate the effectiveness\nof our token-based approach. We also develop two duration preservation methods,\nsuitable for applications such as dubbing.", "comment": "Accepted to INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.17735v1", "cate": "eess.AS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用自监督离散令牌和非并行数据进行口音标准化", "tldr": "一种使用自监督离散令牌和非并行数据的新型口音标准化方法，在自然度、口音消除和音色保留方面表现优异。", "motivation": "口音标准化旨在将带有外国口音的语音转换为类似母语的语音，同时保留说话者身份。现有的方法可能面临数据或性能上的挑战。", "method": "提出了一种使用自监督离散令牌和非并行训练数据的新型流程。该系统从源语音中提取令牌，通过专用模型进行转换，并使用流匹配合成输出。此外，还开发了两种时长保留方法。", "result": "该方法在自然度、口音消除和音色保留方面表现出优于逐帧基线的性能，适用于多种英语口音。通过令牌级语音分析，验证了其基于令牌方法的有效性。", "conclusion": "本研究提出的基于自监督离散令牌和非并行数据的口音标准化方法是有效的，并在多个关键指标上展现出卓越性能，同时支持时长保留功能，具有实际应用潜力。", "translation": "口音标准化将带有外国口音的语音转换为类似母语的语音，同时保留说话者身份。我们提出了一种使用自监督离散令牌和非并行训练数据的新型流程。该系统从源语音中提取令牌，通过专用模型转换它们，并使用流匹配合成输出。我们的方法在自然度、口音消除和音色保留方面优于逐帧基线，适用于多种英语口音。通过令牌级语音分析，我们验证了我们基于令牌的方法的有效性。我们还开发了两种时长保留方法，适用于配音等应用。", "summary": "这篇论文提出了一种新颖的口音标准化方法，利用自监督离散令牌和非并行数据，旨在将外国口音语音转换为母语般语音并保留说话者身份。该系统通过提取、转换令牌并使用流匹配进行合成。实验结果表明，该方法在自然度、口音消除和音色保留方面优于现有基线，并开发了适用于配音的时长保留方法。", "keywords": "口音标准化, 自监督学习, 离散令牌, 非并行数据, 语音合成", "comments": "这项工作创新性地将自监督离散令牌应用于口音标准化，并有效利用了非并行数据，解决了传统方法对并行数据依赖的问题。其在多口音场景下的优异表现和时长保留功能，使其在语音转换和配音等领域具有潜在的应用价值。"}}
{"id": "2504.14861", "title": "Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum Inner Product Search", "authors": ["Tingyang Chen", "Cong Fu", "Xiangyu Ke", "Yunjun Gao", "Yabo Ni", "Anxiang Zeng"], "categories": ["cs.DB", "cs.IR"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Accepted by SIGIR 2025", "url": "http://arxiv.org/abs/2504.14861v2", "summary": "Maximum Inner Product Search (MIPS) is a fundamental challenge in machine\nlearning and information retrieval, particularly in high-dimensional data\napplications. Existing approaches to MIPS either rely solely on Inner Product\n(IP) similarity, which faces issues with local optima and redundant\ncomputations, or reduce the MIPS problem to the Nearest Neighbor Search under\nthe Euclidean metric via space projection, leading to topology destruction and\ninformation loss. Despite the divergence of the two paradigms, we argue that\nthere is no inherent binary opposition between IP and Euclidean metrics. By\nstitching IP and Euclidean in the design of indexing and search algorithms, we\ncan significantly enhance MIPS performance. Specifically, this paper explores\nthe theoretical and empirical connections between these two metrics from the\nMIPS perspective. Our investigation, grounded in graph-based search, reveals\nthat different indexing and search strategies offer distinct advantages for\nMIPS, depending on the underlying data topology. Building on these insights, we\nintroduce a novel graph-based index called Metric-Amphibious Graph (MAG) and a\ncorresponding search algorithm, Adaptive Navigation with Metric Switch (ANMS).\nTo facilitate parameter tuning for optimal performance, we identify three\nstatistical indicators that capture essential data topology properties and\ncorrelate strongly with parameter tuning. Extensive experiments on 12\nreal-world datasets demonstrate that MAG outperforms existing state-of-the-art\nmethods, achieving up to 4x search speedup while maintaining adaptability and\nscalability.", "comment": "Accepted by SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2504.14861v2", "cate": "cs.DB", "date": "2025-04-21", "updated": "2025-07-23", "AI": {"title_translation": "缝合内积和欧几里得度量以实现拓扑感知最大内积搜索", "tldr": "本文提出了一种新的图基索引MAG和搜索算法ANMS，通过结合内积和欧几里得度量来显著提高最大内积搜索（MIPS）的性能，解决了现有方法在拓扑破坏和信息丢失方面的问题，并在真实世界数据集上实现了显著的性能提升。", "motivation": "现有的最大内积搜索（MIPS）方法存在缺陷：单纯依赖内积（IP）相似性会导致局部最优和冗余计算；而将MIPS问题转换为欧几里得度量下的最近邻搜索会破坏拓扑结构并导致信息丢失。作者认为内积和欧几里得度量并非二元对立，通过结合两者可以显著提升MIPS性能。", "method": "本文从MIPS的角度探讨了内积和欧几里得度量之间的理论和实证联系，发现不同的索引和搜索策略对MIPS有不同的优势，这取决于底层数据拓扑。基于此，提出了一种新颖的图基索引Metric-Amphibious Graph (MAG) 和相应的搜索算法Adaptive Navigation with Metric Switch (ANMS)。为了优化性能，还识别了三个与参数调整强相关的统计指标，用于捕捉数据拓扑属性。", "result": "在12个真实世界数据集上的广泛实验表明，MAG优于现有的最先进方法，实现了高达4倍的搜索速度提升，同时保持了适应性和可扩展性。", "conclusion": "通过在索引和搜索算法设计中结合内积和欧几里得度量，可以显著提高最大内积搜索（MIPS）的性能。本文提出的Metric-Amphibious Graph (MAG) 和 Adaptive Navigation with Metric Switch (ANMS) 在实际应用中表现出卓越的搜索速度、适应性和可扩展性。", "translation": "最大内积搜索（MIPS）是机器学习和信息检索领域的一个基本挑战，尤其是在高维数据应用中。现有的MIPS方法要么完全依赖内积（IP）相似性，这面临局部最优和冗余计算的问题；要么通过空间投影将MIPS问题简化为欧几里得度量下的最近邻搜索，这会导致拓扑破坏和信息丢失。尽管这两种范式存在差异，但我们认为内积和欧几里得度量之间没有固有的二元对立。通过在索引和搜索算法设计中缝合IP和欧几里得，我们可以显著提升MIPS性能。具体而言，本文从MIPS的角度探讨了这两种度量之间的理论和实证联系。我们基于图搜索的调查显示，不同的索引和搜索策略根据底层数据拓扑为MIPS提供了独特的优势。基于这些见解，我们引入了一种新颖的图基索引，称为Metric-Amphibious Graph (MAG)，以及相应的搜索算法Adaptive Navigation with Metric Switch (ANMS)。为了方便参数调整以获得最佳性能，我们确定了三个统计指标，它们捕捉了基本数据拓扑属性并与参数调整密切相关。在12个真实世界数据集上的广泛实验表明，MAG优于现有的最先进方法，实现了高达4倍的搜索速度提升，同时保持了适应性和可扩展性。", "summary": "本文针对高维数据中最大内积搜索（MIPS）的挑战，提出了结合内积（IP）和欧几里得度量的新方法。现有MIPS方法面临局部最优、冗余计算、拓扑破坏和信息丢失等问题。作者指出IP和欧几里得度量并非对立，通过在索引和搜索算法中融合二者可显著提升MIPS性能。论文深入探讨了两种度量在MIPS中的联系，并基于图搜索提出了Metric-Amphibious Graph (MAG) 索引和Adaptive Navigation with Metric Switch (ANMS) 搜索算法。为优化参数，还引入了三个反映数据拓扑的统计指标。实验结果表明，MAG在12个真实数据集上比现有SOTA方法快4倍，且具备良好的适应性和可扩展性。", "keywords": "最大内积搜索, 内积, 欧几里得度量, 图基索引, MAG, ANMS", "comments": "本文的创新点在于打破了内积和欧几里得度量在MIPS问题处理上的传统二元对立，创造性地将两者“缝合”起来。通过引入Metric-Amphibious Graph (MAG)和Adaptive Navigation with Metric Switch (ANMS)，该研究为高维数据下的MIPS提供了一个高效且适应性强的解决方案，尤其在搜索速度上取得了显著提升，对机器学习和信息检索领域具有重要意义。"}}
{"id": "2507.15850", "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15850v2", "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15850v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-22", "AI": {"title_translation": "3LM：通过基准测试连接阿拉伯语、STEM和代码", "tldr": "针对阿拉伯语LLM在STEM和代码领域缺乏基准的问题，本文提出了3LM，一套包含三个新基准，用于填补这一空白并支持相关研究。", "motivation": "尽管阿拉伯语是世界上使用最广泛的语言之一，但针对阿拉伯语大语言模型（LLM）的开发和评估工作相对有限。现有的大多数阿拉伯语基准侧重于语言、文化或宗教内容，在STEM和代码等日益重要的领域存在显著空白。", "method": "本文提出了3LM，一套包含三个专门为阿拉伯语设计的基准。第一个基准是自然源自阿拉伯语教科书和教育工作表的STEM相关问答对。第二个基准是使用相同来源合成生成的STEM问题。第三个基准侧重于代码生成，通过人工循环审查过程精心翻译了两个广泛使用的代码基准。", "result": "本文公开发布了所有三个基准。", "conclusion": "通过公开发布这些基准，旨在支持阿拉伯语LLM在STEM和代码这些重要但代表性不足领域的研究发展。", "translation": "阿拉伯语是世界上使用最广泛的语言之一，然而针对阿拉伯语大语言模型（LLM）的开发和评估工作仍然相对有限。现有的大多数阿拉伯语基准侧重于语言、文化或宗教内容，在STEM和代码等日益重要的领域留下了显著空白，而这些领域对于现实世界的LLM应用越来越重要。为了帮助弥补这一差距，我们提出了3LM，一套专门为阿拉伯语设计的三个基准。第一个基准是一组STEM相关的问答对，自然来源于阿拉伯语教科书和教育工作表。第二个基准包含合成生成的STEM问题，使用相同的来源创建。第三个基准侧重于代码生成，通过精心翻译两个广泛使用的代码基准构建，并结合了多轮人工审查过程，以确保高质量和忠实的翻译。我们公开发布所有这三个基准，以支持阿拉伯语LLM在这些重要但代表性不足领域的研究发展。", "summary": "本文介绍了3LM，一套针对阿拉伯语大语言模型（LLM）的三个新基准，旨在弥补现有阿拉伯语基准在STEM和代码领域中的空白。这些基准包括自然和合成生成的STEM问答数据，以及通过人工审查过程翻译的编程代码生成数据。所有基准均已公开，以促进阿拉伯语LLM在这些关键但未充分研究领域的进步。", "keywords": "阿拉伯语LLM, 基准测试, STEM, 代码生成, 3LM", "comments": "本文的创新之处在于其解决了阿拉伯语LLM在STEM和代码领域基准缺乏的痛点，这是当前LLM应用中日益重要的领域。通过构建多样化且高质量的基准，特别是引入人工审查确保翻译质量，为阿拉伯语LLM在科学技术和编程领域的进一步发展提供了宝贵资源。"}}
{"id": "2507.16859", "title": "Leveraging multi-source and heterogeneous signals for fatigue detection", "authors": ["Luobin Cui", "Yanlai Wu", "Tang Ying", "Weikai Li"], "categories": ["cs.RO", "cs.AI", "62H30", "I.2"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      1figures,32pages", "url": "http://arxiv.org/abs/2507.16859v1", "summary": "Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.", "comment": "1figures,32pages", "pdf_url": "http://arxiv.org/pdf/2507.16859v1", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "利用多源异构信号进行疲劳检测", "tldr": "本文提出了一种利用多源异构信号进行疲劳检测的框架，以解决现有方法在实际应用中受传感器和环境限制的问题。", "motivation": "现有的疲劳检测方法多依赖高端传感器和受控环境，限制了其在航空、采矿和长途运输等安全关键应用中实际场景的适用性。本文旨在解决在传感器受限的实际环境中，如何利用来自不同配置源（包括使用不切实际传感器的受控环境）的知识进行疲劳检测的问题。", "method": "本文提出了一个异构多源疲劳检测框架，该框架能够自适应地利用目标领域中可用的模态，同时受益于源领域中存在的不同配置。", "result": "实验结果表明，该方法在实际部署的传感器设置和两个公开数据集上具有实用性、鲁棒性和改进的泛化能力。", "conclusion": "本文提出的方法为在传感器受限场景中进行有效的疲劳监测提供了实用的途径。", "translation": "疲劳检测在航空、采矿和长途运输等安全关键应用中发挥着关键作用。然而，大多数现有方法依赖于高端传感器和受控环境，限制了它们在实际环境中的适用性。本文正式定义了一个实际但未被充分探索的实际疲劳检测问题设置，其中系统使用与上下文相符的传感器，旨在利用来自不同配置源（包括在受控环境中部署的不切实际传感器）的知识。为了应对这一挑战，我们提出了一种异构多源疲劳检测框架，该框架能够自适应地利用目标领域中可用的模态，同时受益于源领域中存在的不同配置。我们的实验使用实际部署的传感器设置和两个公开数据集进行，证明了我们方法的实用性、鲁棒性和改进的泛化能力，为在传感器受限场景中进行有效的疲劳监测铺平了实用道路。", "summary": "本文针对航空、采矿等安全关键领域中疲劳检测现有方法受限于高端传感器和受控环境的问题，提出了一种利用多源异构信号的疲劳检测框架。该框架能够自适应地整合目标域的可用模态和源域的不同配置知识，从而在传感器受限的实际场景中实现实用、鲁棒且泛化能力更强的疲劳监测。", "keywords": "疲劳检测, 多源, 异构信号, 传感器受限, 泛化", "comments": "本文提出了一种新颖的异构多源疲劳检测框架，解决了现有方法在实际应用中传感器和环境限制的痛点。其创新之处在于能够融合来自不同传感器配置源的数据，并自适应地利用目标域的可用模态，这对于实际部署具有重要意义。该研究为传感器受限环境下的疲劳监测提供了实用的解决方案，具有较强的应用价值。"}}
{"id": "2507.10330", "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "authors": ["Mohammed Bouri", "Adnane Saoud"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL Findings 2025", "url": "http://arxiv.org/abs/2507.10330v1", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "comment": "Accepted to ACL Findings 2025", "pdf_url": "http://arxiv.org/pdf/2507.10330v1", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-14", "AI": {"title_translation": "通过增长边界矩阵方法弥合NLP中对抗词替换攻击的鲁棒性与泛化能力", "tldr": "本文提出了一种基于增长边界矩阵（GBM）的新型正则化技术，以提高NLP模型对抗词替换攻击的鲁棒性，并首次系统分析了状态空间模型（S4）的鲁棒性，实验证明其在对抗鲁棒性方面优于现有基线。", "motivation": "尽管自然语言处理（NLP）取得了进展，但模型仍然容易受到对抗性攻击，例如同义词替换。现有工作主要关注前馈和卷积架构的鲁棒性，而循环网络和现代状态空间模型（SSM，如S4）的鲁棒性仍未得到充分研究。这些架构因其序列处理和复杂的参数动态而带来独特的挑战。", "method": "本文引入了一种基于增长边界矩阵（GBM）的新型正则化技术，通过减少输入扰动对模型输出的影响来提高NLP模型的鲁棒性。该方法侧重于计算LSTM、状态空间模型（S4）和卷积神经网络（CNN）三种架构的GBM。", "result": "实验证明，该方法将对抗性鲁棒性比现有基线提高了高达8.8%，并且在对抗性防御方面优于多项最先进的方法。", "conclusion": "本文提出的基于增长边界矩阵的正则化方法有效提高了NLP模型对抗词替换攻击的鲁棒性，并改善了在干净文本上的泛化能力，同时首次对状态空间模型（S4）的鲁棒性进行了系统分析，显示出优于现有方法的性能。", "translation": "尽管自然语言处理（NLP）取得了进展，但模型仍然容易受到对抗性攻击，例如同义词替换。虽然先前的工作主要集中于改进前馈和卷积架构的鲁棒性，但循环网络和现代状态空间模型（SSM），例如S4的鲁棒性，仍未得到充分研究。这些架构因其序列处理和复杂的参数动态而带来独特的挑战。在本文中，我们引入了一种基于增长边界矩阵（GBM）的新型正则化技术，通过减少输入扰动对模型输出的影响来提高NLP模型的鲁棒性。我们专注于计算三种架构的GBM：长短期记忆（LSTM）、状态空间模型（S4）和卷积神经网络（CNN）。我们的方法旨在（1）增强对词替换攻击的抵抗力，（2）改善在干净文本上的泛化能力，以及（3）提供首次对SSM（S4）鲁棒性的系统分析。在多种架构和基准数据集上进行的大量实验表明，我们的方法将对抗性鲁棒性比现有基线提高了高达8.8%。这些结果突出了我们方法的有效性，其在对抗性防御方面优于多项最先进的方法。代码可在https://github.com/BouriMohammed/GBM获取。", "summary": "本文提出了一种基于增长边界矩阵（GBM）的新型正则化技术，旨在提高NLP模型对抗词替换攻击的鲁棒性并改善泛化能力。该方法计算LSTM、S4和CNN的GBM，并首次对S4模型的鲁棒性进行了系统分析。实验结果表明，该方法在对抗性鲁棒性方面比现有基线提高了高达8.8%，且性能优于多种现有最先进方法。", "keywords": "增长边界矩阵, 对抗性鲁棒性, 词替换攻击, 状态空间模型, 自然语言处理", "comments": "本文的创新点在于引入了增长边界矩阵（GBM）作为一种新型正则化技术，以解决NLP模型在词替换攻击下的鲁棒性问题。特别值得注意的是，该研究首次系统性地分析了状态空间模型（S4）的鲁棒性，填补了该领域的一个空白。其方法不仅提升了对抗鲁棒性，也兼顾了在干净文本上的泛化能力，这在对抗性防御研究中是一个重要且具有挑战性的目标。实验结果显示出显著的性能提升，表明该方法具有较高的实用价值。"}}
{"id": "2507.17664", "title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras", "authors": ["Lingdong Kong", "Dongyue Lu", "Ao Liang", "Rong Li", "Yuhao Dong", "Tianshuai Hu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint; 42 pages, 17 figures, 16 tables; Project Page at this https URL", "url": "http://arxiv.org/abs/2507.17664v1", "summary": "Event cameras offer microsecond-level latency and robustness to motion blur,\nmaking them ideal for understanding dynamic environments. Yet, connecting these\nasynchronous streams to human language remains an open challenge. We introduce\nTalk2Event, the first large-scale benchmark for language-driven object\ngrounding in event-based perception. Built from real-world driving data, we\nprovide over 30,000 validated referring expressions, each enriched with four\ngrounding attributes -- appearance, status, relation to viewer, and relation to\nother objects -- bridging spatial, temporal, and relational reasoning. To fully\nexploit these cues, we propose EventRefer, an attribute-aware grounding\nframework that dynamically fuses multi-attribute representations through a\nMixture of Event-Attribute Experts (MoEE). Our method adapts to different\nmodalities and scene dynamics, achieving consistent gains over state-of-the-art\nbaselines in event-only, frame-only, and event-frame fusion settings. We hope\nour dataset and approach will establish a foundation for advancing multimodal,\ntemporally-aware, and language-driven perception in real-world robotics and\nautonomy.", "comment": "Preprint; 42 pages, 17 figures, 16 tables; Project Page at\n  https://talk2event.github.io", "pdf_url": "http://arxiv.org/pdf/2507.17664v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Talk2Event：基于事件相机的动态场景接地理解", "tldr": "引入了Talk2Event，首个用于事件感知中语言驱动对象接地的基准数据集，并提出了EventRefer框架，通过多属性融合在动态场景理解上实现了显著提升。", "motivation": "事件相机在理解动态环境方面具有优势，但将这些异步数据流与人类语言关联起来仍然是一个开放的挑战。", "method": "提出了Talk2Event，一个包含超过30,000个参照表达的大规模基准数据集，每个表达都富含外观、状态、与观察者的关系以及与其他物体的关系这四种接地属性。为了充分利用这些线索，提出了EventRefer，一个属性感知的接地框架，通过事件-属性专家混合（MoEE）动态融合多属性表示。", "result": "该方法在仅事件、仅帧和事件-帧融合设置中，相对于最先进的基线方法取得了持续的性能提升。", "conclusion": "该数据集和方法有望为推动现实世界机器人和自主系统中的多模态、时间感知和语言驱动感知奠定基础。", "translation": "事件相机提供了微秒级的延迟和对运动模糊的鲁棒性，使其成为理解动态环境的理想选择。然而，将这些异步流与人类语言连接起来仍然是一个开放的挑战。我们引入了Talk2Event，这是首个用于事件感知中语言驱动对象接地的大规模基准。该基准基于真实世界的驾驶数据构建，提供了超过30,000个经过验证的参照表达，每个表达都富含四种接地属性——外观、状态、与观察者的关系以及与其他物体的关系——从而弥合了空间、时间及关系推理之间的鸿沟。为了充分利用这些线索，我们提出了EventRefer，一个属性感知的接地框架，通过事件-属性专家混合（MoEE）动态融合多属性表示。我们的方法适应不同的模态和场景动态，在仅事件、仅帧和事件-帧融合设置中，相对于最先进的基线方法取得了持续的提升。我们希望我们的数据集和方法能为推动现实世界机器人和自主系统中的多模态、时间感知和语言驱动感知奠定基础。", "summary": "本文介绍了Talk2Event，一个用于事件相机感知中语言驱动对象接地的首个大规模基准数据集，该数据集包含来自真实世界驾驶数据的30,000多个参照表达，并附带多维接地属性。同时，提出了EventRefer框架，它通过事件-属性专家混合（MoEE）动态融合多属性表示，以实现属性感知的接地。实验结果表明，该方法在多种感知设置下均优于现有基线，为机器人和自主系统中的多模态、时间感知和语言驱动感知奠定了基础。", "keywords": "事件相机, 语言接地, 动态场景理解, Talk2Event, EventRefer", "comments": "本文的创新点在于首次构建了大规模的语言驱动事件相机感知基准数据集Talk2Event，并提出了一个新颖的属性感知接地框架EventRefer。通过结合事件相机的特性和多属性推理，有效解决了事件数据与人类语言关联的挑战。其在真实世界驾驶数据上的验证以及对多种模态的适应性，使其在机器人和自主驾驶领域具有重要应用潜力。"}}
{"id": "2507.17391", "title": "Residual Prophet Inequalities", "authors": ["Jose Correa", "Sebastian Perez-Salazar", "Dana Pizarro", "Bruno Ziliotto"], "categories": ["cs.DS", "68W27, 60G40"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17391v1", "summary": "We introduce a variant of the classic prophet inequality, called\n\\emph{residual prophet inequality} (RPI). In the RPI problem, we consider a\nfinite sequence of $n$ nonnegative independent random values with known\ndistributions, and a known integer $0\\leq k\\leq n-1$. Before the gambler\nobserves the sequence, the top $k$ values are removed, whereas the remaining\n$n-k$ values are streamed sequentially to the gambler. For example, one can\nassume that the top $k$ values have already been allocated to a higher-priority\nagent. Upon observing a value, the gambler must decide irrevocably whether to\naccept or reject it, without the possibility of revisiting past values.\n  We study two variants of RPI, according to whether the gambler learns online\nof the identity of the variable that he sees (FI model) or not (NI model). Our\nmain result is a randomized algorithm in the FI model with \\emph{competitive\nratio} of at least $1/(k+2)$, which we show is tight. Our algorithm is\ndata-driven and requires access only to the $k+1$ largest values of a single\nsample from the $n$ input distributions. In the NI model, we provide a similar\nalgorithm that guarantees a competitive ratio of $1/(2k+2)$. We further analyze\nindependent and identically distributed instances when $k=1$. We build a\nsingle-threshold algorithm with a competitive ratio of at least 0.4901, and\nshow that no single-threshold strategy can get a competitive ratio greater than\n0.5464.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17391v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "残差先知不等式", "tldr": "本文引入了残差先知不等式（RPI），这是经典先知不等式的一个变体，其中序列中最大的k个值在赌徒观察之前被移除。研究了两种模型（FI和NI），并为它们提供了具有紧密竞争比的随机算法。", "motivation": "研究经典先知不等式的一个变体，即残差先知不等式（RPI），以模拟在序列观察之前，前k个最优值已被更高优先级代理分配的场景。", "method": "引入残差先知不等式（RPI）问题，考虑具有已知分布的n个非负独立随机值，其中前k个值被移除。研究了两种RPI变体：赌徒是否在线学习变量身份（FI模型）或不学习（NI模型）。针对这两种模型，提出了随机算法进行分析。此外，还分析了k=1时的独立同分布（IID）实例，并构建了单阈值算法。", "result": "在FI模型中，提出了一种随机算法，其竞争比至少为1/(k+2)，并证明这是紧密的。该算法是数据驱动的，仅需要访问n个输入分布中单个样本的k+1个最大值。在NI模型中，提供了一个类似算法，保证竞争比为1/(2k+2)。对于k=1的独立同分布实例，构建了一个单阈值算法，竞争比至少为0.4901，并表明任何单阈值策略的竞争比都不能超过0.5464。", "conclusion": "本文引入并分析了残差先知不等式（RPI）问题，该问题考虑了在序列观察前移除前k个值的场景。研究了两种信息模型（FI和NI），并为它们提供了具有紧密竞争比的随机算法。此外，还对k=1的独立同分布情况进行了深入分析，给出了单阈值算法的竞争比界限。", "translation": "我们引入了经典先知不等式的一个变体，称为残差先知不等式（RPI）。在RPI问题中，我们考虑一个包含n个非负独立随机值的有限序列，这些值具有已知分布，并且已知一个整数0≤k≤n-1。在赌徒观察序列之前，前k个值被移除，而剩余的n-k个值按顺序流式传输给赌徒。例如，可以假设前k个值已经被分配给更高优先级的代理。一旦观察到一个值，赌徒必须不可撤销地决定接受或拒绝它，而不能重新访问过去的值。\n我们根据赌徒是否在线学习其所见变量的身份（FI模型）或不学习（NI模型），研究了RPI的两种变体。我们的主要结果是在FI模型中提出了一种随机算法，其竞争比至少为1/(k+2)，我们证明这是紧密的。我们的算法是数据驱动的，仅需要访问n个输入分布中单个样本的k+1个最大值。在NI模型中，我们提供了一个类似的算法，保证竞争比为1/(2k+2)。我们进一步分析了k=1时的独立同分布实例。我们构建了一个单阈值算法，其竞争比至少为0.4901，并表明任何单阈值策略都不能获得大于0.5464的竞争比。", "summary": "本文引入了残差先知不等式（RPI），这是经典先知不等式的一个新变体，其中在赌徒观察前，序列中最大的k个值被移除。研究了两种信息模型：FI（赌徒知道变量身份）和NI（赌徒不知道）。对于FI模型，提出了一种数据驱动的随机算法，实现了1/(k+2)的紧密竞争比。对于NI模型，提供了竞争比为1/(2k+2)的类似算法。此外，论文还分析了k=1的独立同分布情况，提出了一个竞争比至少为0.4901的单阈值算法，并给出了单阈值策略的竞争比上限为0.5464。", "keywords": "先知不等式, 残差先知不等式, 在线算法, 竞争比, 最优停止", "comments": "本文通过引入残差先知不等式，对经典的先知不等式问题进行了有益的扩展，使其能更好地模拟实际场景中资源预分配或高优先级代理优先选择的情况。提出的数据驱动随机算法及其紧密的竞争比结果是重要的理论贡献。对不同信息模型和特定IID情况的细致分析也增加了研究的深度。"}}
{"id": "2507.17378", "title": "An FDM-sFEM scheme on time-space manifolds and its superconvergence analysis", "authors": ["Chengrun Jiang", "Guozhi Dong", "Hailong Guo", "Zuoqiang Shi"], "categories": ["math.NA", "cs.NA", "65M15, 65M60"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17378v1", "summary": "We study superconvergent discretization of the Laplace-Beltrami operator on\ntime-space product manifolds with Neumann temporal boundary values, which arise\nin the context of dynamic optimal transport on general surfaces. We propose a\ncoupled scheme that combines finite difference methods in time with surface\nfinite element methods in space. By establishing a new summation by parts\nformula and proving the supercloseness of the semi-discrete solution, we derive\nsuperconvergence results for the recovered gradient via post-processing\ntechniques. In addition, our geometric error analysis is implemented within a\nnovel framework based on the approximation of the Riemannian metric. Several\nnumerical examples are provided to validate and illustrate the theoretical\nresults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17378v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "时空流形上的FDM-sFEM方案及其超收敛性分析", "tldr": "本文研究了在时空流形上Laplace-Beltrami算子的超收敛离散化，提出了一种结合有限差分和曲面有限元方法的耦合方案，并通过后处理技术得到了恢复梯度的超收敛结果。", "motivation": "研究在具有Neumann时间边界值的时空积流形上Laplace-Beltrami算子的超收敛离散化，该问题出现在一般曲面上的动态最优传输背景中。", "method": "提出了一种耦合方案，将时间上的有限差分方法与空间上的曲面有限元方法相结合。通过建立一个新的分部求和公式并证明半离散解的超逼近性，通过后处理技术得到了恢复梯度场的超收敛结果。此外，几何误差分析是在基于黎曼度量近似的新颖框架内实现的。", "result": "通过后处理技术得到了恢复梯度场的超收敛结果。提供了几个数值例子来验证和说明理论结果。", "conclusion": "所提出的FDM-sFEM耦合方案有效地实现了时空流形上Laplace-Beltrami算子的超收敛离散化，并通过数值例子得到了验证。", "translation": "本文研究了在具有Neumann时间边界值的时空积流形上Laplace-Beltrami算子的超收敛离散化，该问题出现在一般曲面上的动态最优传输中。我们提出了一种耦合方案，该方案将时间上的有限差分方法与空间上的曲面有限元方法相结合。通过建立一个新的分部求和公式并证明半离散解的超逼近性，我们通过后处理技术得到了恢复梯度场的超收敛结果。此外，我们的几何误差分析是在基于黎曼度量近似的新颖框架内实现的。提供了几个数值例子来验证和说明理论结果。", "summary": "本文研究了时空流形上Laplace-Beltrami算子的超收敛离散化，该问题与动态最优传输相关。论文提出了一种结合时间上的有限差分方法和空间上的曲面有限元方法的耦合方案。通过建立新的分部求和公式并证明半离散解的超逼近性，论文通过后处理技术获得了恢复梯度的超收敛结果。此外，还提出了一个基于黎曼度量近似的新颖几何误差分析框架，并通过数值例子验证了理论结果。", "keywords": "Laplace-Beltrami算子, 超收敛, 有限差分方法, 曲面有限元方法, 时空流形", "comments": "该论文为解决时空流形上的Laplace-Beltrami算子问题提出了一种创新的FDM-sFEM耦合方案。其创新点在于建立了新的分部求和公式、证明了半离散解的超逼近性，并开发了一个新颖的几何误差分析框架，这些都是数值分析领域，特别是几何偏微分方程方面的重大贡献。"}}
{"id": "2507.11893", "title": "Spatial Frequency Modulation for Semantic Segmentation", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accept by TPAMI 2025", "url": "http://arxiv.org/abs/2507.11893v2", "summary": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at https://github.com/Linwei-Chen/SFM.", "comment": "Accept by TPAMI 2025", "pdf_url": "http://arxiv.org/pdf/2507.11893v2", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "语义分割的空间频率调制", "tldr": "提出空间频率调制（SFM）方法，通过在下采样前调制高频特征并在上采样时解调来解决语义分割中高频信息丢失和混叠问题，提升多任务性能。", "motivation": "语义分割的准确性高度依赖高空间频率信息（如纹理），但下采样层（如步幅卷积）会导致高频分量出现混叠或失真。", "method": "提出空间频率调制（SFM）方法。SFM在下采样前将高频特征调制到低频，并在上采样时解调回高频。具体通过自适应重采样（ARS）进行调制，该模块通过密集采样高频区域来降低频率；通过多尺度自适应上采样（MSAU）进行解调，该模块通过非均匀上采样恢复高频信息并利用多尺度下采样区域的交互。SFM模块可无缝集成到CNN和Transformer架构中。", "result": "特征可视化和分析证实该方法有效缓解了混叠并成功保留了细节。通过扩展到图像分类、对抗鲁棒性、实例分割和全景分割任务，验证了SFM的广泛适用性和有效性。", "conclusion": "SFM是一种新颖的方法，通过解决下采样过程中的高频信息丢失问题，提高了语义分割及其他视觉任务的性能和细节保留能力。", "translation": "高空间频率信息，包括纹理等精细细节，对语义分割的准确性有显著贡献。然而，根据奈奎斯特-香农采样定理，高频分量在通过诸如步幅卷积等下采样层传播时容易受到混叠或失真。本文提出一种新颖的空间频率调制（SFM）方法，它在下采样前将高频特征调制到较低频率，然后在上采样时将它们解调回来。具体来说，我们通过自适应重采样（ARS）实现调制，并设计了一个轻量级附加模块，该模块可以密集采样高频区域以放大信号，从而根据频率缩放特性降低其频率。我们还提出多尺度自适应上采样（MSAU）来解调调制后的特征，并通过非均匀上采样恢复高频信息。该模块通过明确利用多尺度下采样区域之间的信息交互，进一步提高了分割效果。这两个模块都可以无缝集成到从卷积神经网络到Transformer的各种架构中。特征可视化和分析证实，我们的方法有效缓解了混叠，同时在解调后成功保留了细节。最后，我们通过将其扩展到图像分类、对抗鲁棒性、实例分割和全景分割任务，验证了SFM的广泛适用性和有效性。代码已在https://github.com/Linwei-Chen/SFM提供。", "summary": "本文提出一种新颖的空间频率调制（SFM）方法，旨在解决语义分割中高频信息在下采样过程中易受混叠和失真影响的问题。SFM通过在下采样前将高频特征调制到低频，并在上采样时解调回高频来实现。核心组件包括用于调制的自适应重采样（ARS）和用于解调的多尺度自适应上采样（MSAU）。实验证明SFM能有效缓解混叠并保留细节，且能无缝集成到CNN和Transformer架构中，并在图像分类、实例分割等多个视觉任务中展现出广泛的适用性和有效性。", "keywords": "语义分割, 空间频率调制, 混叠, 下采样, 高频信息", "comments": "该论文提出了一种创新的空间频率调制（SFM）方法，通过在频率域操作来解决深度学习中常见的下采样导致的高频信息丢失和混叠问题。其核心思想是在下采样前对高频信息进行“预处理”（调制）并在上采样时“恢复”（解调），这为提升图像细节保留和分割精度提供了一个新颖的视角。SFM的普适性是其亮点，能应用于多种网络架构和视觉任务。该方法通过引入频率域的思考，为当前依赖空间域操作的深度学习模型提供了新的优化方向，对于需要精细细节处理的视觉任务具有重要意义。"}}
{"id": "2507.17075", "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs", "authors": ["Yihao Xue", "Baharan Mirzasoleiman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17075v1", "summary": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex\nproblems that were previously out of reach. To ensure LLMs do not assist with\nharmful requests, safety alignment fine-tuning is necessary in the\npost-training phase. However, safety alignment fine-tuning has recently been\nshown to significantly degrade reasoning abilities, a phenomenon known as the\n\"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets\neffectively aligns the model for safety without harming its reasoning\ncapabilities. This is because restricting the safety weight updates to a\nlow-rank space minimizes the interference with the reasoning weights. Our\nextensive experiments across four benchmarks covering math, science, and coding\nshow that this approach produces highly safe LLMs -- with safety levels\ncomparable to full-model fine-tuning -- without compromising their reasoning\nabilities. Additionally, we observe that LoRA induces weight updates with\nsmaller overlap with the initial weights compared to full-model fine-tuning. We\nalso explore methods that further reduce such overlap -- via regularization or\nduring weight merging -- and observe some improvement on certain tasks. We hope\nthis result motivates designing approaches that yield more consistent\nimprovements in the reasoning-safety trade-off.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17075v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "LoRA是你推理LLM安全对齐所需的一切", "tldr": "本研究表明，使用LoRA对拒绝数据集进行安全微调，可以在不损害推理能力的情况下，有效实现LLM的安全对齐，从而解决“安全税”问题。", "motivation": "推理大型语言模型（LLMs）在解决复杂问题方面取得了显著突破，但安全对齐微调会显著降低其推理能力，这种现象被称为“安全税”。本研究旨在解决如何在确保LLM安全性的同时，避免其推理能力下降的问题。", "method": "本研究采用LoRA（Low-Rank Adaptation）技术，在拒绝数据集上进行安全微调（SFT）。通过将安全权重更新限制在低秩空间，最大限度地减少对推理权重的干扰。", "result": "实验结果表明，该方法能够生成高度安全的LLM，其安全水平与全模型微调相当，且不损害推理能力。此外，LoRA诱导的权重更新与初始权重的重叠度小于全模型微调。研究还探索了通过正则化或权重合并进一步减少重叠的方法，并在某些任务上观察到了一定的改进。", "conclusion": "本研究的结果表明，LoRA是一种有效的方法，可以在不牺牲推理能力的情况下实现LLM的安全对齐。作者希望这一结果能激励设计出在推理-安全权衡方面产生更一致改进的方法。", "translation": "推理大型语言模型（LLM）在解决以前无法解决的复杂问题方面取得了显著突破。为了确保LLM不会协助有害请求，在训练后阶段进行安全对齐微调是必要的。然而，最近研究表明，安全对齐微调会显著降低推理能力，这种现象被称为“安全税”。在这项工作中，我们表明，使用LoRA在拒绝数据集上进行SFT可以有效地对齐模型以实现安全性，而不会损害其推理能力。这是因为将安全权重更新限制在低秩空间可以最大限度地减少对推理权重的干扰。我们在涵盖数学、科学和编码的四个基准上进行了广泛的实验，结果表明这种方法产生了高度安全的LLM——其安全水平与全模型微调相当——而不会损害其推理能力。此外，我们观察到与全模型微调相比，LoRA诱导的权重更新与初始权重的重叠度更小。我们还探索了通过正则化或在权重合并期间进一步减少这种重叠的方法，并在某些任务上观察到了一些改进。我们希望这一结果能激励设计出在推理-安全权衡方面产生更一致改进的方法。", "summary": "本研究旨在解决大型语言模型（LLM）在进行安全对齐微调时出现的推理能力下降问题，即“安全税”。作者提出并验证了使用LoRA（Low-Rank Adaptation）在拒绝数据集上进行安全微调的方法。实验结果表明，该方法能有效提升LLM的安全性，且其推理能力与全模型微调相当，同时避免了推理能力的损失。研究还发现LoRA导致的权重更新与初始权重重叠较小，并探索了进一步减少重叠的方法，为未来平衡LLM安全性和推理能力的研究提供了新思路。", "keywords": "LoRA, 安全对齐, LLM, 推理能力, 安全税", "comments": "这项工作的重要创新在于提出了一种有效缓解LLM“安全税”问题的方法，即利用LoRA进行安全对齐。通过将安全权重更新限制在低秩空间，巧妙地减少了对模型核心推理能力的干扰，这对于构建既智能又安全的LLM具有重要意义。该研究的发现为LLM的微调策略提供了新的视角，并鼓励了未来在推理-安全权衡方面更深入的探索。"}}
{"id": "2507.17070", "title": "Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach", "authors": ["Adithya Mohan", "Dominik Rößle", "Daniel Cremers", "Torsten Schön"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, 2 tables", "url": "http://arxiv.org/abs/2507.17070v1", "summary": "Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated\nits applicability across various domains, including robotics, healthcare,\nenergy optimization, and autonomous driving. However, a critical question\nremains: How robust are DRL models when exposed to adversarial attacks? While\nexisting defense mechanisms such as adversarial training and distillation\nenhance the resilience of DRL models, there remains a significant research gap\nregarding the integration of multiple defenses in autonomous driving scenarios\nspecifically. This paper addresses this gap by proposing a novel ensemble-based\ndefense architecture to mitigate adversarial attacks in autonomous driving. Our\nevaluation demonstrates that the proposed architecture significantly enhances\nthe robustness of DRL models. Compared to the baseline under FGSM attacks, our\nensemble method improves the mean reward from 5.87 to 18.38 (over 213%\nincrease) and reduces the mean collision rate from 0.50 to 0.09 (an 82%\ndecrease) in the highway scenario and merge scenario, outperforming all\nstandalone defense strategies.", "comment": "6 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.17070v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "深度强化学习中基于集成防御方法的鲁棒性提升", "tldr": "本文提出了一种新颖的集成防御架构，显著提升了深度强化学习模型在自动驾驶场景中抵御对抗性攻击的鲁棒性。", "motivation": "深度强化学习（DRL）模型在对抗性攻击面前的鲁棒性不足是一个关键问题。尽管现有防御机制如对抗训练和蒸馏能增强DRL模型的韧性，但在自动驾驶场景中，集成多种防御机制的研究仍存在显著空白。", "method": "本文提出了一种新颖的基于集成的防御架构，以减轻自动驾驶中的对抗性攻击。", "result": "所提出的集成防御架构显著增强了DRL模型的鲁棒性。在FGSM攻击下，与基线相比，该集成方法在高速公路和合并场景中将平均奖励从5.87提高到18.38（增长超过213%），并将平均碰撞率从0.50降低到0.09（降低82%），优于所有独立的防御策略。", "conclusion": "所提出的集成防御架构能够显著提升深度强化学习模型在自动驾驶场景中抵御对抗性攻击的鲁棒性，并且表现优于单独的防御策略。", "translation": "深度强化学习（DRL）的最新进展已证明其在机器人、医疗保健、能源优化和自动驾驶等各个领域的适用性。然而，一个关键问题仍然存在：DRL模型在暴露于对抗性攻击时有多鲁棒？尽管对抗训练和蒸馏等现有防御机制增强了DRL模型的韧性，但在自动驾驶场景中，集成多种防御机制的研究仍存在显著空白。本文通过提出一种新颖的基于集成的防御架构来弥补这一空白，以减轻自动驾驶中的对抗性攻击。我们的评估表明，所提出的架构显著增强了DRL模型的鲁棒性。与FGSM攻击下的基线相比，我们的集成方法在高速公路场景和合并场景中将平均奖励从5.87提高到18.38（增长超过213%），并将平均碰撞率从0.50降低到0.09（降低82%），优于所有独立的防御策略。", "summary": "本研究提出了一种新颖的集成防御架构，旨在提高深度强化学习（DRL）模型在自动驾驶场景中抵御对抗性攻击的鲁棒性。通过实验验证，该集成方法在FGSM攻击下显著提升了模型的性能，例如在高速公路和合并场景中平均奖励增加了213%以上，碰撞率降低了82%，证明其优于单独的防御策略。", "keywords": "深度强化学习, 对抗性攻击, 鲁棒性, 集成防御, 自动驾驶", "comments": "本文的创新点在于提出了一个针对深度强化学习模型对抗性攻击的集成防御架构，特别关注了自动驾驶场景。其重要性在于通过量化的结果（奖励增加和碰撞率降低）明确展示了集成防御策略在提升DRL模型鲁棒性方面的显著优势，填补了多防御机制集成应用的研究空白。"}}
{"id": "2507.17194", "title": "Dispatch-Aware Deep Neural Network for Optimal Transmission Switching: Toward Real-Time and Feasibility Guaranteed Operation", "authors": ["Minsoo Kim", "Jip Kim"], "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures", "url": "http://arxiv.org/abs/2507.17194v1", "summary": "Optimal transmission switching (OTS) improves optimal power flow (OPF) by\nselectively opening transmission lines, but its mixed-integer formulation\nincreases computational complexity, especially on large grids. To deal with\nthis, we propose a dispatch-aware deep neural network (DA-DNN) that accelerates\nDC-OTS without relying on pre-solved labels. DA-DNN predicts line states and\npasses them through a differentiable DC-OPF layer, using the resulting\ngeneration cost as the loss function so that all physical network constraints\nare enforced throughout training and inference. In addition, we adopt a\ncustomized weight-bias initialization that keeps every forward pass feasible\nfrom the first iteration, which allows stable learning on large grids. Once\ntrained, the proposed DA-DNN produces a provably feasible topology and dispatch\npair in the same time as solving the DCOPF, whereas conventional mixed-integer\nsolvers become intractable. As a result, the proposed method successfully\ncaptures the economic advantages of OTS while maintaining scalability.", "comment": "5 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.17194v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "调度感知深度神经网络用于最优输电切换：迈向实时和可行性保障运行", "tldr": "本文提出了一种调度感知深度神经网络（DA-DNN），用于加速直流最优输电切换（DC-OTS），通过可微分的直流最优潮流（DC-OPF）层和定制的权重-偏差初始化，实现了实时、可行的拓扑和调度，同时保持了经济优势和可扩展性。", "motivation": "最优输电切换（OTS）通过选择性地打开输电线路来改进最优潮流（OPF），但其混合整数公式增加了计算复杂度，尤其是在大型电网中，这限制了其应用。", "method": "本文提出了一种调度感知深度神经网络（DA-DNN），用于加速DC-OTS，无需预先求解的标签。DA-DNN预测线路状态并通过可微分的DC-OPF层，使用由此产生的发电成本作为损失函数，以确保在训练和推理过程中强制执行所有物理网络约束。此外，采用定制的权重-偏差初始化，使每次前向传播从第一次迭代开始就保持可行。", "result": "训练后，所提出的DA-DNN能在与求解DCOPF相同的时间内生成一个可证明可行的拓扑和调度对。它成功地捕捉了OTS的经济优势，同时保持了可扩展性，解决了传统混合整数求解器在大规模问题上变得难以处理的问题。", "conclusion": "所提出的DA-DNN为最优输电切换提供了一个实时、可行且可扩展的解决方案，有效克服了传统混合整数公式带来的计算复杂性问题，使得OTS的经济优势得以在大规模电网中实现。", "translation": "最优输电切换（OTS）通过选择性地打开输电线路来改进最优潮流（OPF），但其混合整数公式增加了计算复杂度，尤其是在大型电网中。为了解决这个问题，我们提出了一种调度感知深度神经网络（DA-DNN），它无需依赖预先求解的标签即可加速直流最优输电切换（DC-OTS）。DA-DNN预测线路状态并通过一个可微分的直流最优潮流（DC-OPF）层，使用由此产生的发电成本作为损失函数，从而在整个训练和推理过程中强制执行所有物理网络约束。此外，我们采用了一种定制的权重-偏差初始化，使每次前向传播从第一次迭代开始就保持可行，这使得在大型电网上的学习变得稳定。一旦训练完成，所提出的DA-DNN能在与求解DCOPF相同的时间内生成一个可证明可行的拓扑和调度对，而传统的混合整数求解器则变得难以处理。因此，所提出的方法成功地捕捉了OTS的经济优势，同时保持了可扩展性。", "summary": "本文提出了一种调度感知深度神经网络（DA-DNN），旨在解决大型电网中最优输电切换（OTS）的计算复杂性问题。DA-DNN通过预测线路状态并将其输入到可微分的直流最优潮流（DC-OPF）层，利用发电成本作为损失函数，确保物理约束的强制执行。结合定制的权重-偏差初始化，该模型在训练和推理过程中始终保持可行性。实验结果表明，DA-DNN能够在与求解DC-OPF相同的时间内生成可行的网络拓扑和调度方案，成功实现OTS的经济效益，同时展现出卓越的实时性能和可扩展性，克服了传统混合整数求解器的局限性。", "keywords": "最优输电切换, 深度神经网络, 实时操作, 可行性保障, 电力系统", "comments": "本文的创新点在于提出了调度感知深度神经网络（DA-DNN），它通过集成可微分的DC-OPF层和定制的权重-偏差初始化，克服了传统OTS混合整数规划计算复杂度高的问题。这种方法不仅加速了DC-OTS，更重要的是，它能在训练和推理过程中保证物理网络约束的强制执行，并从初始迭代就确保可行性，这对于电力系统运行至关重要。该研究对于实现电力系统实时、可靠和经济的运行具有重要意义。"}}
{"id": "2507.16391", "title": "Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing", "authors": ["Chenqi Lin", "Kang Yang", "Tianshi Xu", "Ling Liang", "Yufei Wang", "Zhaohui Chen", "Runsheng Wang", "Mingyu Gao", "Meng Li"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16391v2", "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16391v2", "cate": "cs.AR", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "Ironman：利用近内存处理加速隐私保护AI中的不经意传输扩展", "tldr": "Ironman是一个新的不经意传输（OT）加速器，通过定制硬件和近内存处理显著提高隐私保护机器学习（PPML）的效率，与CPU相比，OT吞吐量提高了39.2-237.4倍，PPML端到端延迟降低了2.1-3.4倍。", "motivation": "随着机器学习的广泛应用，用户数据中的敏感信息引发了隐私担忧。隐私保护机器学习（PPML）作为一种解决方案应运而生，但其严重依赖不经意传输（OT）原语来计算非线性函数。OT主要涉及单点相关OT（SPCOT）和带噪声学习奇偶校验（LPN）操作，目前在通用CPU上计算密集，成为现代PPML框架的延迟瓶颈。", "method": "本文提出了一个名为Ironman的新型OT加速器。针对计算密集型SPCOT，提出了硬件友好的SPCOT算法和定制加速器以提高计算吞吐量。针对内存带宽受限的LPN，利用配备内存侧缓存和索引排序的近内存处理（NMP）架构来提高有效内存带宽。", "result": "Ironman在不同NMP配置下，与全线程CPU实现相比，OT吞吐量提高了39.2-237.4倍。对于不同的PPML框架，Ironman将CNN和Transformer模型的端到端延迟降低了2.1-3.4倍。", "conclusion": "Ironman通过结合定制加速器和近内存处理，显著提高了不经意传输的效率，从而加速了隐私保护机器学习框架，有效解决了OT作为PPML延迟瓶颈的问题。", "translation": "随着机器学习（ML）的广泛应用，用户数据可能包含敏感信息，从而引发了隐私问题。基于密码原语的隐私保护ML（PPML）已成为一种有前景的解决方案，其中ML模型直接在加密数据上进行计算，以提供形式化的隐私保证。然而，PPML框架严重依赖不经意传输（OT）原语来计算非线性函数。OT主要涉及单点相关OT（SPCOT）和带噪声学习奇偶校验（LPN）操作的计算。由于OT仍然在通用CPU上进行大量计算，它成为了现代PPML框架的延迟瓶颈。\n在本文中，我们提出了一个新颖的OT加速器，名为Ironman，以显著提高OT和整个PPML框架的效率。我们观察到SPCOT是计算受限的，因此提出了一种硬件友好的SPCOT算法和定制加速器，以提高SPCOT计算吞吐量。相比之下，LPN由于不规则的内存访问模式而受限于内存带宽。因此，我们进一步利用配备内存侧缓存和索引排序的近内存处理（NMP）架构来提高有效内存带宽。通过大量实验，我们证明了Ironman在不同NMP配置下，与全线程CPU实现相比，OT吞吐量提高了39.2-237.4倍。对于不同的PPML框架，Ironman在CNN和Transformer模型中，端到端延迟降低了2.1-3.4倍。", "summary": "本论文提出了一种名为Ironman的新型不经意传输（OT）加速器，旨在解决隐私保护机器学习（PPML）中OT操作的性能瓶颈。研究发现，OT中的单点相关OT（SPCOT）是计算密集型，而带噪声学习奇偶校验（LPN）是内存带宽受限型。为此，Ironman针对SPCOT设计了硬件友好的算法和定制加速器以提升吞吐量，并针对LPN利用近内存处理（NMP）架构，通过内存侧缓存和索引排序优化内存带宽。实验结果表明，Ironman在OT吞吐量上实现了39.2-237.4倍的提升，并将PPML框架的端到端延迟降低了2.1-3.4倍，显著提升了PPML的效率。", "keywords": "隐私保护机器学习, 不经意传输, 硬件加速器, 近内存处理, 性能优化", "comments": "该论文通过提出Ironman加速器，有效解决了隐私保护机器学习中不经意传输（OT）的性能瓶颈问题。其创新点在于针对OT中不同操作的特性（SPCOT的计算密集型和LPN的内存带宽受限型）采用了差异化的优化策略，即定制硬件加速和近内存处理。这表明了对底层计算特性的深刻理解和硬件协同设计的有效性，对于推动PPML的实际应用具有重要意义。"}}
{"id": "2507.16922", "title": "A Unifying Scheme for Extractive Content Selection Tasks", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16922v1", "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16922v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "提取式内容选择任务的统一方案", "tldr": "本文提出了一种名为指令引导内容选择（IGCS）的统一框架，用于解决各种提取式内容选择任务。为支持该框架，研究者发布了首个统一基准IGCSBench和一个大型合成数据集，并展示了迁移学习的有效性。", "motivation": "尽管许多自然语言处理任务都涉及从源文本中选择相关文本片段（内容选择），但这些任务传统上是孤立研究的，各自拥有独立的建模方法、数据集和评估指标，缺乏统一性。", "method": "本文提出了“指令引导内容选择（IGCS）”作为统一框架，将任务定义和任何实例特定的请求封装为对语言模型的指令。为推广此框架，研究者引入了首个涵盖多种内容选择任务的统一基准IGCSBench，并创建了一个大型通用合成数据集。此外，研究还探讨了基于大型语言模型（LLM）的内容选择建模中出现的通用推理时间问题，并评估了一种通用评估指标。", "result": "研究表明，利用所创建的通用合成数据集进行迁移学习通常能有效提升内容选择任务的性能，无论是否有针对特定目标任务的专用训练数据。", "conclusion": "本文提出的统一框架、基准、合成数据集以及相关方法对于未来内容选择模型的开发和研究具有重要的实用价值。", "translation": "广泛的自然语言处理任务都涉及从给定源文本中选择相关的文本片段。尽管目标相同，但这些内容选择任务传统上是孤立研究的，每个任务都有自己的建模方法、数据集和评估指标。在这项工作中，我们提出了指令引导内容选择（IGCS）作为此类设置的有效统一框架，其中任务定义和任何实例特定的请求都被封装为对语言模型的指令。为了推广此框架，我们引入了IGCSBench，这是第一个涵盖多种内容选择任务的统一基准。此外，我们创建了一个大型通用合成数据集，可用于各种内容选择任务，并表明使用这些数据集进行迁移学习通常能提高性能，无论是否有针对目标任务的专用训练。最后，我们解决了基于LLM的内容选择建模中出现的通用推理时间问题，评估了一种通用评估指标，并总体提出了我们的资源和方法对未来内容选择模型的效用。模型和数据集可在 https://github.com/shmuelamar/igcs 获取。", "summary": "本文针对自然语言处理中广泛存在但传统上孤立研究的内容选择任务，提出了一种名为“指令引导内容选择（IGCS）”的统一框架。该框架将任务定义和请求封装为对语言模型的指令。为支持此框架，研究者发布了首个涵盖多种内容选择任务的统一基准IGCSBench，并创建了一个大型通用合成数据集。实验结果表明，利用这些数据集进行迁移学习能够有效提升内容选择任务的性能。此外，研究还探讨了LLM推理时间问题和通用评估指标，旨在为未来的内容选择模型提供有价值的资源和方法。", "keywords": "内容选择, 统一框架, 指令引导, 迁移学习, 基准测试", "comments": "这项工作通过提出统一的指令引导框架（IGCS）、创建首个统一基准IGCSBench以及一个大型通用合成数据集，解决了内容选择任务碎片化的问题。其创新之处在于将任务定义和请求转化为语言模型指令，并证明了通过通用数据集进行迁移学习的有效性，这对于提高LLM在内容选择任务上的泛化能力和效率具有重要意义。该研究为未来的内容选择模型提供了统一的范式和丰富的资源。"}}
{"id": "2410.02846", "title": "A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk: Default Probabilities and Loan Portfolios", "authors": ["Pascal Kündig", "Fabio Sigrist"], "categories": ["q-fin.RM", "cs.LG", "q-fin.ST"], "primary_category": "Subjects:       Risk Management (q-fin.RM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.02846v2", "summary": "We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of spatio-temporal frailty effects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.02846v2", "cate": "q-fin.RM", "date": "2024-10-03", "updated": "2025-07-23", "AI": {"title_translation": "抵押贷款信用风险的时空机器学习模型：违约概率和贷款组合", "tldr": "本文提出了一种结合树形提升和潜在时空高斯过程的机器学习模型，用于抵押贷款信用风险评估。该模型在预测个体贷款违约概率和贷款组合损失分布方面，比传统模型更准确，主要归因于预测变量中的强交互和非线性效应以及时空脆弱性效应。", "motivation": "开发一种更准确的信用风险模型，以解决传统独立线性风险模型和线性时空模型在预测抵押贷款违约概率和贷款组合损失分布方面的局限性，并更好地捕捉预测变量中的非线性和交互作用以及未被观测变量解释的时空变异。", "method": "本文引入了一种结合树形提升（tree-boosting）和潜在时空高斯过程模型（latent spatio-temporal Gaussian process model）的机器学习模型，该模型考虑了脆弱性相关性（frailty correlation）。这种方法能够灵活地以数据驱动的方式建模预测变量中的非线性和交互作用，并解释未被可观测预测变量解释的时空变异。同时，该模型在计算上是高效的。", "result": "在大型美国抵押贷款信用风险数据集上的应用表明，与传统的独立线性风险模型和线性时空模型相比，使用该新型方法获得的个体贷款预测违约概率和预测贷款组合损失分布都更准确。模型表现优异的原因是预测变量中存在强烈的交互作用和非线性效应，以及时空脆弱性效应。", "conclusion": "结合树形提升和潜在时空高斯过程的机器学习模型能够显著提高抵押贷款信用风险的预测准确性，其优越性在于能够有效捕捉数据中复杂的非线性、交互作用以及时空脆弱性效应。", "translation": "我们引入了一种新颖的信用风险机器学习模型，该模型将树形提升与考虑脆弱性相关性的潜在时空高斯过程模型相结合。这使得能够以灵活的数据驱动方式建模预测变量中的非线性和交互作用，并解释未被可观测预测变量解释的时空变异。我们还展示了如何以计算高效的方式进行估计和预测。在应用于大型美国抵押贷款信用风险数据集时，我们发现与传统的独立线性风险模型和线性时空模型相比，我们新方法获得的个体贷款预测违约概率和预测贷款组合损失分布都更准确。使用机器学习模型的解释性工具，我们发现这种优异表现的可能原因是预测变量中存在强烈的交互作用和非线性效应以及时空脆弱性效应。", "summary": "本文提出了一种新颖的时空机器学习模型，用于抵押贷款信用风险评估。该模型通过结合树形提升和考虑脆弱性相关性的潜在时空高斯过程，能够有效捕捉预测变量中的非线性和交互作用，并解释未被观测变量解释的时空变异。在大规模美国抵押贷款数据集上的应用结果显示，该模型在预测个体违约概率和贷款组合损失分布方面，其准确性优于传统线性模型，这主要归因于模型能够识别数据中复杂的非线性、交互作用和时空脆弱性效应。", "keywords": "信用风险, 机器学习, 时空模型, 违约概率, 贷款组合", "comments": "该论文的创新点在于将树形提升与时空高斯过程相结合，创造了一个能够同时处理非线性、交互作用和时空依赖性的强大模型。这对于信用风险建模来说是一个重要进展，因为它能更全面地捕捉现实世界中复杂的风险驱动因素。模型的计算效率也值得关注，使其在实际应用中具有可行性。"}}
{"id": "2507.16818", "title": "Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design", "authors": ["C. H. E. Jordaan", "M. van der Stelt", "T. J. J. Maal", "V. M. A. Stirler", "R. Leijendekkers", "T. Kachman", "G. A. de Jong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16818v1", "summary": "The quality of a transtibial prosthetic socket depends on the prosthetist's\nskills and expertise, as the fitting is performed manually. This study\ninvestigates multiple artificial intelligence (AI) approaches to help\nstandardize transtibial prosthetic socket design. Data from 118 patients were\ncollected by prosthetists working in the Dutch healthcare system. This data\nconsists of a three-dimensional (3D) scan of the residual limb and a\ncorresponding 3D model of the prosthetist-designed socket. Multiple data\npre-processing steps are performed for alignment, standardization and\noptionally compression using Morphable Models and Principal Component Analysis.\nAfterward, three different algorithms - a 3D neural network, Feedforward neural\nnetwork, and random forest - are developed to either predict 1) the final\nsocket shape or 2) the adaptations performed by a prosthetist to predict the\nsocket shape based on the 3D scan of the residual limb. Each algorithm's\nperformance was evaluated by comparing the prosthetist-designed socket with the\nAI-generated socket, using two metrics in combination with the error location.\nFirst, we measure the surface-to-surface distance to assess the overall surface\nerror between the AI-generated socket and the prosthetist-designed socket.\nSecond, distance maps between the AI-generated and prosthetist sockets are\nutilized to analyze the error's location. For all algorithms, estimating the\nrequired adaptations outperformed direct prediction of the final socket shape.\nThe random forest model applied to adaptation prediction yields the lowest\nerror with a median surface-to-surface distance of 1.24 millimeters, a first\nquartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16818v1", "cate": "cs.LG", "date": "2025-04-30", "updated": "2025-04-30", "AI": {"title_translation": "评估人工智能算法在胫骨假肢承窝形状设计标准化中的应用", "tldr": "本研究评估了多种人工智能算法（3D神经网络、前馈神经网络、随机森林）以标准化胫骨假肢承窝设计，发现预测假肢师调整比直接预测承窝形状更有效，其中随机森林模型的表现最佳，中位误差为1.24毫米。", "motivation": "胫骨假肢承窝的质量依赖于假肢师的手动操作和专业技能，导致质量不一。本研究旨在利用人工智能方法帮助标准化胫骨假肢承窝的设计过程。", "method": "研究收集了118名患者的残肢3D扫描数据及假肢师设计的承窝3D模型。数据经过对齐、标准化和可选压缩（使用可变形模型和主成分分析）等预处理步骤。开发了三种人工智能算法：3D神经网络、前馈神经网络和随机森林。这些算法被用于预测最终承窝形状或预测假肢师所做的调整。通过比较AI生成的承窝和假肢师设计的承窝，使用表面到表面距离和距离图两种指标评估算法性能。", "result": "对于所有算法，估计所需的调整比直接预测最终承窝形状的表现更好。应用于调整预测的随机森林模型产生的误差最低，中位表面到表面距离为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米。", "conclusion": "人工智能算法，特别是用于预测假肢师调整的随机森林模型，能够有效地标准化胫骨假肢承窝的设计，并达到较高的精度。", "translation": "胫骨假肢承窝的质量取决于假肢师的技能和专业知识，因为安装是手动进行的。本研究调查了多种人工智能（AI）方法，以帮助标准化胫骨假肢承窝的设计。荷兰医疗系统中的假肢师收集了118名患者的数据。这些数据包括残肢的三维（3D）扫描和相应的假肢师设计的承窝的3D模型。为了对齐、标准化以及可选地使用可变形模型和主成分分析进行压缩，进行了多个数据预处理步骤。随后，开发了三种不同的算法——3D神经网络、前馈神经网络和随机森林——以预测1）最终承窝形状或2）假肢师为基于残肢3D扫描预测承窝形状所做的调整。通过将AI生成的承窝与假肢师设计的承窝进行比较，并结合误差位置使用两种指标来评估每种算法的性能。首先，我们测量表面到表面的距离，以评估AI生成的承窝与假肢师设计的承窝之间的整体表面误差。其次，利用AI生成承窝和假肢师承窝之间的距离图来分析误差的位置。对于所有算法，估计所需的调整优于直接预测最终承窝形状。应用于调整预测的随机森林模型产生的误差最低，中位表面到表面距离为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米。", "summary": "本研究评估了多种人工智能算法（包括3D神经网络、前馈神经网络和随机森林）在标准化胫骨假肢承窝设计方面的应用。研究利用118名患者的数据，比较了直接预测承窝形状和预测假肢师调整这两种方法。结果显示，预测调整的方法表现更优，其中随机森林模型在预测调整时实现了最低的误差，中位表面到表面距离为1.24毫米，这表明人工智能在提高假肢承窝设计的一致性和质量方面具有巨大潜力。", "keywords": "人工智能, 胫骨假肢承窝, 标准化, 机器学习, 随机森林", "comments": "该论文的创新之处在于将人工智能应用于传统上依赖人工技能的医疗器械设计过程，特别是假肢承窝的定制。研究发现预测假肢师的调整比直接预测最终承窝形状更有效，这是一个重要的洞察。其重要性在于，通过引入数据驱动的标准化方法，有望解决手动适配导致的假肢质量和一致性问题，从而可能改善患者的治疗效果。尽管118名患者的数据量不错，但未来可能需要更大、更多样化的数据集来进一步验证和推广其结果。"}}
{"id": "2507.17265", "title": "Visualization-Driven Illumination for Density Plots", "authors": ["Xin Chen", "Yunhai Wang", "Huaiwei Bao", "Kecheng Lu", "Jaemin Jo", "Chi-Wing Fu", "Jean-Daniel Fekete"], "categories": ["cs.GR", "cs.HC"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17265v1", "summary": "We present a novel visualization-driven illumination model for density plots,\na new technique to enhance density plots by effectively revealing the detailed\nstructures in high- and medium-density regions and outliers in low-density\nregions, while avoiding artifacts in the density field's colors. When\nvisualizing large and dense discrete point samples, scatterplots and dot\ndensity maps often suffer from overplotting, and density plots are commonly\nemployed to provide aggregated views while revealing underlying structures.\nYet, in such density plots, existing illumination models may produce color\ndistortion and hide details in low-density regions, making it challenging to\nlook up density values, compare them, and find outliers. The key novelty in\nthis work includes (i) a visualization-driven illumination model that\ninherently supports density-plot-specific analysis tasks and (ii) a new image\ncomposition technique to reduce the interference between the image shading and\nthe color-encoded density values. To demonstrate the effectiveness of our\ntechnique, we conducted a quantitative study, an empirical evaluation of our\ntechnique in a controlled study, and two case studies, exploring twelve\ndatasets with up to two million data point samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17265v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "密度图的可视化驱动光照", "tldr": "本文提出一种新颖的可视化驱动光照模型，用于增强密度图，有效揭示高、中密度区域的详细结构和低密度区域的异常值，同时避免颜色伪影。", "motivation": "现有的散点图和点密度图存在过度绘制问题，而密度图的现有光照模型可能产生颜色失真，隐藏低密度区域的细节，导致难以查找、比较密度值和发现异常值。", "method": "本文提出一种新颖的、由可视化驱动的密度图光照模型，其固有地支持密度图特定的分析任务；并引入一种新的图像合成技术，以减少图像着色和颜色编码密度值之间的干扰。", "result": "通过定量研究、受控研究中的经验评估和两个案例研究，探索了多达两百万个数据点的十二个数据集，证明了该技术的有效性。", "conclusion": "本文提出的可视化驱动光照模型及其图像合成技术能够有效增强密度图，解决现有方法的局限性，从而更好地揭示密度分布的细节和异常值。", "translation": "我们提出了一种新颖的、由可视化驱动的密度图光照模型，这是一种通过有效揭示高密度和中密度区域的详细结构以及低密度区域的异常值来增强密度图的新技术，同时避免密度场颜色中的伪影。在可视化大型密集离散点样本时，散点图和点密度图经常遇到过度绘制的问题，而密度图通常用于提供聚合视图，同时揭示底层结构。然而，在这样的密度图中，现有的光照模型可能会产生颜色失真，并隐藏低密度区域的细节，使得查找、比较密度值和发现异常值变得具有挑战性。这项工作的关键创新包括（i）一种固有支持密度图特定分析任务的可视化驱动光照模型，以及（ii）一种新的图像合成技术，以减少图像着色和颜色编码密度值之间的干扰。为了证明我们技术的有效性，我们进行了一项定量研究、一项在我们受控研究中对我们技术的经验评估以及两个案例研究，探索了多达两百万个数据点样本的十二个数据集。", "summary": "本文介绍了一种新颖的可视化驱动光照模型，旨在解决现有密度图在显示细节和异常值方面的局限性。该模型通过增强高、中密度区域的结构和低密度区域的异常值，同时避免颜色伪影，提高了密度图的可读性和分析能力。其核心创新在于支持密度图特定分析任务的光照模型和减少着色与颜色编码密度值干扰的图像合成技术。通过广泛的定量和案例研究，证明了该方法的有效性。", "keywords": "密度图, 可视化驱动光照, 图像合成, 数据可视化, 异常值检测", "comments": "本文的创新点在于提出了一个专门为密度图设计的可视化驱动光照模型，并结合了新的图像合成技术，有效解决了密度图在显示细节和异常值时的颜色失真和信息隐藏问题。这对于处理大规模离散点数据，提升数据可视化分析的准确性和效率具有重要意义。"}}
{"id": "2507.17309", "title": "Confounded Causal Imitation Learning with Instrumental Variables", "authors": ["Yan Zeng", "Shenglan Nie", "Feng Xie", "Libo Huang", "Peng Wu", "Zhi Geng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17309v1", "summary": "Imitation learning from demonstrations usually suffers from the confounding\neffects of unmeasured variables (i.e., unmeasured confounders) on the states\nand actions. If ignoring them, a biased estimation of the policy would be\nentailed. To break up this confounding gap, in this paper, we take the best of\nthe strong power of instrumental variables (IV) and propose a Confounded Causal\nImitation Learning (C2L) model. This model accommodates confounders that\ninfluence actions across multiple timesteps, rather than being restricted to\nimmediate temporal dependencies. We develop a two-stage imitation learning\nframework for valid IV identification and policy optimization. In particular,\nin the first stage, we construct a testing criterion based on the defined\npseudo-variable, with which we achieve identifying a valid IV for the C2L\nmodels. Such a criterion entails the sufficient and necessary identifiability\nconditions for IV validity. In the second stage, with the identified IV, we\npropose two candidate policy learning approaches: one is based on a simulator,\nwhile the other is offline. Extensive experiments verified the effectiveness of\nidentifying the valid IV as well as learning the policy.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17309v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于工具变量的混淆因果模仿学习", "tldr": "本文提出了一种名为C2L的模型，利用工具变量解决模仿学习中未测量混淆变量导致的偏差问题，并通过两阶段框架实现有效的工具变量识别和策略学习，实验验证了其有效性。", "motivation": "模仿学习通常受到未测量变量（即混淆因素）对状态和动作的混淆影响，若忽略这些影响会导致策略的偏差估计。为了弥合这一混淆差距，本文旨在利用工具变量的强大能力来解决此问题。", "method": "本文提出了一个混淆因果模仿学习（C2L）模型，该模型能够处理影响多个时间步动作的混淆因素。研究开发了一个两阶段的模仿学习框架：第一阶段，基于定义的伪变量构建一个测试准则，以识别C2L模型的有效工具变量，该准则包含了工具变量有效性的充分必要可识别条件；第二阶段，利用已识别的工具变量，提出了两种候选策略学习方法：一种基于模拟器，另一种是离线方法。", "result": "广泛的实验验证了识别有效工具变量以及学习策略的有效性。", "conclusion": "本文成功提出了基于工具变量的C2L模型及其两阶段学习框架，有效解决了模仿学习中的混淆问题，并通过实验证明了其在识别有效工具变量和学习策略方面的有效性。", "translation": "从演示中进行的模仿学习通常会受到未测量变量（即未测量混淆因素）对状态和动作的混淆影响。如果忽略它们，将导致策略的偏差估计。为了弥合这一混淆差距，本文充分利用工具变量（IV）的强大能力，提出了一种混淆因果模仿学习（C2L）模型。该模型能够处理影响多个时间步动作的混淆因素，而不仅仅局限于即时的时间依赖。我们开发了一个两阶段的模仿学习框架，用于有效的工具变量识别和策略优化。具体而言，在第一阶段，我们基于定义的伪变量构建了一个测试准则，通过该准则我们能够识别C2L模型的有效工具变量。该准则包含了工具变量有效性的充分必要可识别条件。在第二阶段，利用已识别的工具变量，我们提出了两种候选策略学习方法：一种基于模拟器，另一种是离线方法。广泛的实验验证了识别有效工具变量以及学习策略的有效性。", "summary": "本文针对模仿学习中因未测量混淆变量导致策略估计偏差的问题，提出了一种基于工具变量的混淆因果模仿学习（C2L）模型。该模型能够处理跨多时间步的混淆影响。研究设计了一个两阶段框架：首先，通过构建基于伪变量的测试准则来识别有效的工具变量；其次，利用这些已识别的工具变量，通过基于模拟器或离线方式进行策略学习。实验结果验证了所提方法在有效工具变量识别和策略学习方面的有效性。", "keywords": "模仿学习, 混淆因素, 工具变量, 因果推断, 策略学习", "comments": "这项工作创新性地将工具变量引入到模仿学习中，以解决长期存在的未测量混淆偏差问题。其两阶段框架，特别是基于伪变量的工具变量识别准则，是关键的贡献，为在存在未观测混淆因素的情况下进行更准确的因果模仿学习提供了新的视角和有效方法，对于提高模仿学习的鲁棒性和可靠性具有重要意义。"}}
{"id": "2507.14456", "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14456v3", "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14456v3", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-23", "AI": {"title_translation": "GEMINUS：用于端到端自动驾驶的双感知全局和场景自适应专家混合模型", "tldr": "GEMINUS是一个端到端自动驾驶框架，它通过结合全局专家和场景自适应专家，并使用双感知路由器动态选择专家，从而在复杂多样的交通环境中实现自适应和鲁棒的驾驶性能。", "motivation": "现有的端到端自动驾驶单一模式规划方法难以学习多样化的驾驶技能来处理复杂多变的交通环境，导致性能不足。", "method": "本文提出了GEMINUS框架，一个专家混合（Mixture-of-Experts, MoE）的端到端自动驾驶系统。它包含一个在整体数据集上训练的“全局专家”以提供鲁棒性能，一个在对应场景子集上训练的“场景自适应专家组”以实现自适应性能，以及一个同时考虑场景级特征和路由不确定性来动态激活专家模块的“双感知路由器”。通过这三者的有效耦合，GEMINUS能在多样化场景中实现自适应和鲁棒的性能。", "result": "GEMINUS在Bench2Drive闭环基准测试中超越了现有方法，并在驾驶分数（Driving Score）和成功率（Success Rate）上达到了最先进的水平，即使仅使用单目视觉输入。与原始单专家基线相比，消融研究显示驾驶分数提高了7.67%，成功率提高了22.06%，MultiAbility-Mean提高了19.41%。", "conclusion": "GEMINUS通过其双感知全局和场景自适应专家混合架构，成功地在多样化场景中实现了自适应且鲁棒的端到端自动驾驶性能，显著优于现有方法和单专家基线。", "translation": "端到端自动驾驶需要自适应且鲁棒地处理复杂多样的交通环境。然而，主流的单一模式规划方法试图学习一个整体策略，却难以获得多样化的驾驶技能以应对各种场景。因此，本文提出了GEMINUS，一个专家混合的端到端自动驾驶框架，其特点是包含一个全局专家、一个场景自适应专家组，并配备了一个双感知路由器。具体而言，全局专家在整体数据集上进行训练，具有鲁棒性能。场景自适应专家在相应的场景子集上进行训练，实现自适应性能。双感知路由器同时考虑场景级特征和路由不确定性，以动态激活专家模块。通过双感知路由器有效耦合全局专家和场景自适应专家组，GEMINUS在多样化场景中实现了自适应和鲁棒的性能。GEMINUS在Bench2Drive闭环基准测试中超越了现有方法，并在驾驶分数和成功率上取得了最先进的性能，即使仅使用单目视觉输入。此外，消融研究表明，相较于原始的单专家基线，其在驾驶分数上显著提高了7.67%，成功率提高了22.06%，MultiAbility-Mean提高了19.41%。代码将在https://github.com/newbrains1/GEMINUS 提供。", "summary": "本文提出了GEMINUS，一个用于端到端自动驾驶的专家混合框架，旨在解决现有单一模式方法在多样化交通场景中适应性不足的问题。GEMINUS结合了在整体数据集上训练的全局专家和在特定场景子集上训练的场景自适应专家，并通过一个双感知路由器动态选择激活相应的专家模块。实验结果表明，GEMINUS在Bench2Drive基准测试中表现优异，并在驾驶分数和成功率上达到最先进水平，即使仅使用单目视觉输入。消融研究进一步证实了其相较于单专家基线的显著性能提升。", "keywords": "自动驾驶, 专家混合, 端到端, 场景自适应, 双感知", "comments": "GEMINUS的创新之处在于其引入了双感知的专家混合架构，通过全局专家和场景自适应专家的协同作用，有效解决了端到端自动驾驶在复杂多样场景中的适应性和鲁棒性问题。特别是双感知路由器的设计，能够根据场景特征和不确定性动态选择专家，提升了系统的灵活性和决策质量。其在单目视觉输入下仍能达到SOTA性能，显示了其潜在的实用价值和鲁棒性。"}}
{"id": "2206.05437", "title": "ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks", "authors": ["Yuelin Wang", "Kai Yi", "Xinliang Liu", "Yu Guang Wang", "Shi Jin"], "categories": ["cs.LG", "cs.AI", "math.AP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      26 pages, 5 figures. NeurIPS 2022 Workshop on GLFrontiers (Oral). ICLR 2023 (Spotlight)", "url": "http://arxiv.org/abs/2206.05437v4", "summary": "Neural message passing is a basic feature extraction unit for\ngraph-structured data considering neighboring node features in network\npropagation from one layer to the next. We model such process by an interacting\nparticle system with attractive and repulsive forces and the Allen-Cahn force\narising in the modeling of phase transition. The dynamics of the system is a\nreaction-diffusion process which can separate particles without blowing up.\nThis induces an Allen-Cahn message passing (ACMP) for graph neural networks\nwhere the numerical iteration for the particle system solution constitutes the\nmessage passing propagation. ACMP which has a simple implementation with a\nneural ODE solver can propel the network depth up to one hundred of layers with\ntheoretically proven strictly positive lower bound of the Dirichlet energy. It\nthus provides a deep model of GNNs circumventing the common GNN problem of\noversmoothing. GNNs with ACMP achieve state of the art performance for\nreal-world node classification tasks on both homophilic and heterophilic\ndatasets. Codes are available at https://github.com/ykiiiiii/ACMP.", "comment": "26 pages, 5 figures. NeurIPS 2022 Workshop on GLFrontiers (Oral).\n  ICLR 2023 (Spotlight)", "pdf_url": "http://arxiv.org/pdf/2206.05437v4", "cate": "cs.LG", "date": "2022-06-11", "updated": "2025-07-23", "AI": {"title_translation": "ACMP：基于吸引和排斥力的Allen-Cahn消息传递图神经网络", "tldr": "ACMP通过将消息传递建模为Allen-Cahn反应-扩散过程，实现了深度图神经网络（GNNs），克服了过平滑问题，并在节点分类任务上取得了最先进的性能。", "motivation": "为了解决图神经网络（GNNs）中常见的过平滑问题，并使其能够构建更深层的网络。", "method": "本文将神经消息传递建模为一个具有吸引力、排斥力和Allen-Cahn力的相互作用粒子系统，形成一个反应-扩散过程。这构成了图神经网络的Allen-Cahn消息传递（ACMP），其中粒子系统解的数值迭代即为消息传递传播。ACMP通过神经ODE求解器实现。", "result": "ACMP使得图神经网络能够达到一百层的深度，并理论上证明了Dirichlet能量的严格正下界，有效规避了过平滑问题。采用ACMP的GNN在同质和异质数据集上的真实世界节点分类任务中取得了最先进的性能。", "conclusion": "ACMP提供了一种能够有效解决过平滑问题的深层图神经网络模型，实现了更深的网络深度并在各种图数据集上取得了卓越的性能。", "translation": "神经消息传递是图结构数据的一种基本特征提取单元，它考虑了网络传播中从一层到下一层的邻居节点特征。我们通过一个具有吸引力和排斥力以及相变建模中产生的Allen-Cahn力的相互作用粒子系统来模拟这个过程。该系统的动力学是一个反应-扩散过程，可以分离粒子而不会发生爆炸。这为图神经网络引入了一种Allen-Cahn消息传递（ACMP），其中粒子系统解的数值迭代构成了消息传递传播。ACMP通过神经ODE求解器实现简单，可以将网络深度推进到一百层，并且理论上证明了Dirichlet能量的严格正下界。因此，它提供了一种深层GNN模型，规避了常见的GNN过平滑问题。采用ACMP的GNN在同质和异质数据集上的真实世界节点分类任务中取得了最先进的性能。代码可在https://github.com/ykiiiiii/ACMP获取。", "summary": "本文提出了一种名为Allen-Cahn消息传递（ACMP）的新型图神经网络（GNN）方法，它将消息传递建模为由Allen-Cahn动力学控制的相互作用粒子系统。这种通过神经ODE求解器实现的反应-扩散过程，使GNN能够达到前所未有的深度（高达100层），同时有效缓解常见的过平滑问题。ACMP增强的GNN在各种数据集的节点分类任务中表现出最先进的性能。", "keywords": "图神经网络, 消息传递, Allen-Cahn, 过平滑, 节点分类", "comments": "这篇论文提出了一种创新方法来解决图神经网络（GNNs）的根本限制，即过平滑问题，该问题阻碍了深层GNN架构的创建。通过将消息传递重新概念化为Allen-Cahn反应-扩散过程，作者引入了ACMP，使GNN能够扩展到显著更深的层次（高达100层），同时保持性能。这种利用物理模型进行图学习的新颖视角是一项重要贡献，为更深、更具表达力的GNN开辟了道路。"}}
{"id": "2507.02591", "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding", "authors": ["Weili Xu", "Enxin Song", "Wenhao Chai", "Xuexiang Wen", "Tian Ye", "Gaoang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Camera Ready", "url": "http://arxiv.org/abs/2507.02591v3", "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.", "comment": "ICCV 2025 Camera Ready", "pdf_url": "http://arxiv.org/pdf/2507.02591v3", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-23", "AI": {"title_translation": "AuroraLong：将RNN带回高效的开放式视频理解", "tldr": "AuroraLong利用线性RNN语言模型高效处理长视频理解，在降低计算成本的同时，实现了与大型Transformer模型相当的性能。", "motivation": "长视频理解面临计算复杂度和内存成本高昂的挑战，因为基于Transformer的大型语言模型所需的内存和计算量随输入序列长度呈二次方增长。", "method": "本文提出了AuroraLong，通过将多模态大型语言模型（MLLMs）中的LLM组件替换为线性RNN语言模型来解决这一挑战，该模型能够以恒定大小的隐藏状态处理任意长度的输入序列。为了进一步提高吞吐量和效率，我们将视觉token合并与线性RNN模型结合起来，通过按大小升序重新排列视觉token。", "result": "尽管AuroraLong只有20亿参数且仅在公共数据上进行训练，但它在多个视频基准测试中取得了与在私有数据集上训练的类似大小的基于Transformer的模型相当的性能。", "conclusion": "这表明高效的线性RNNs通过降低计算门槛，在普及长视频理解方面具有巨大潜力。据我们所知，我们是第一个在类LLaVA模型中使用基于线性RNN的LLM骨干进行开放式视频理解的研究。", "translation": "长视频理解的挑战在于其高计算复杂度和高昂的内存成本，因为基于Transformer的大型语言模型所需的内存和计算量随输入序列长度呈二次方增长。我们提出了AuroraLong来解决这一挑战，通过将多模态大型语言模型（MLLMs）中的LLM组件替换为线性RNN语言模型，该模型能够以恒定大小的隐藏状态处理任意长度的输入序列。为了进一步提高吞吐量和效率，我们将视觉token合并与线性RNN模型结合起来，通过按大小升序重新排列视觉token。尽管AuroraLong只有20亿参数且仅在公共数据上进行训练，但它在多个视频基准测试中取得了与在私有数据集上训练的类似大小的基于Transformer的模型相当的性能。这表明高效的线性RNNs通过降低计算门槛，在普及长视频理解方面具有巨大潜力。据我们所知，我们是第一个在类LLaVA模型中使用基于线性RNN的LLM骨干进行开放式视频理解的研究。", "summary": "AuroraLong是一个针对长视频理解的模型，通过将多模态大型语言模型（MLLMs）中的LLM替换为线性RNN来解决Transformer模型的计算和内存瓶颈。它能高效处理任意长度输入，并结合视觉token合并技术。尽管参数较少且仅用公共数据训练，AuroraLong在多个视频基准测试中表现与同等规模的Transformer模型相当，证明了线性RNN在普及长视频理解方面的潜力。", "keywords": "长视频理解, RNN, 线性RNN, 计算效率, 多模态大型语言模型", "comments": "本文通过重新引入线性RNN，为高效长视频理解提供了一种创新方法，直接解决了Transformer模型的二次方扩展问题。它能够以更少的参数和公共数据实现可比性能，这对于普及先进视频AI具有重要意义。声称是第一个在类LLaVA模型中使用基于线性RNN的LLM骨干进行开放式视频理解的研究，突显了其新颖性。"}}
{"id": "2507.16819", "title": "Assessing Medical Training Skills via Eye and Head Movements", "authors": ["Kayhan Latifzadeh", "Luis A. Leiva", "Klen Čopič Pucihar", "Matjaž Kljun", "Iztok Devetak", "Lili Steblovnik"], "categories": ["cs.HC", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16819v1", "summary": "We examined eye and head movements to gain insights into skill development in\nclinical settings. A total of 24 practitioners participated in simulated baby\ndelivery training sessions. We calculated key metrics, including pupillary\nresponse rate, fixation duration, or angular velocity. Our findings indicate\nthat eye and head tracking can effectively differentiate between trained and\nuntrained practitioners, particularly during labor tasks. For example,\nhead-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas\npupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results\nlay the groundwork for computational models that support implicit skill\nassessment and training in clinical settings by using commodity eye-tracking\nglasses as a complementary device to more traditional evaluation methods such\nas subjective scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16819v1", "cate": "cs.HC", "date": "2025-05-12", "updated": "2025-05-12", "AI": {"title_translation": "通过眼球和头部运动评估医疗培训技能", "tldr": "本研究表明，眼球和头部追踪能有效区分受训和未受训的医疗从业者，为临床技能评估提供了一种客观、补充性的方法。", "motivation": "为了深入了解临床环境中的技能发展，并探索通过眼球和头部运动来评估医疗培训技能的可能性。", "method": "24名从业者参与了模拟分娩训练。研究计算了瞳孔反应率、注视持续时间或角速度等关键指标，并利用眼球和头部追踪数据来区分受训和未受训的从业者。", "result": "眼球和头部追踪能有效区分受训和未受训的从业者，尤其是在分娩任务中。头部相关特征的F1分数为0.85，AUC为0.86；瞳孔相关特征的F1分数为0.77，AUC为0.85。", "conclusion": "研究结果为利用商品眼动追踪眼镜作为传统评估方法的补充，支持临床环境中隐性技能评估和培训的计算模型奠定了基础。", "translation": "我们检查了眼球和头部运动，以深入了解临床环境中的技能发展。共有24名从业者参与了模拟婴儿分娩训练。我们计算了关键指标，包括瞳孔反应率、注视持续时间或角速度。我们的发现表明，眼球和头部追踪可以有效区分受训和未受训的从业者，尤其是在分娩任务中。例如，头部相关特征的F1分数为0.85，AUC为0.86，而瞳孔相关特征的F1分数为0.77，AUC为0.85。这些结果为支持临床环境中隐性技能评估和培训的计算模型奠定了基础，通过使用商品眼动追踪眼镜作为主观评分等更传统评估方法的补充设备。", "summary": "本研究旨在通过分析眼球和头部运动，评估医疗培训中的技能发展。研究招募了24名从业者参与模拟分娩训练，并计算了瞳孔反应率、注视持续时间、角速度等指标。结果显示，眼球和头部追踪能够有效区分受训和未受训的从业者，特别是在分娩任务中，头部特征和瞳孔特征均表现出良好的区分能力（高F1分数和AUC）。这为开发基于商品眼动追踪设备的计算模型以支持临床技能的隐性评估和培训提供了基础。", "keywords": "眼动追踪, 头部运动, 技能评估, 医疗培训, 临床环境", "comments": "该研究创新性地利用商品眼动追踪眼镜作为一种客观工具，来评估医疗培训中的技能发展，为传统的主观评估方法提供了有价值的补充。其重要性在于有望提高技能评估的客观性和效率，并为未来开发智能培训系统奠定基础。"}}
{"id": "2507.17008", "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models", "authors": ["Gaston Gustavo Rios", "Pedro Dal Bianco", "Franco Ronchetti", "Facundo Quiroga", "Oscar Stanchi", "Santiago Ponte Ahón", "Waldo Hasperué"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 8 figures, to be published in Applied Soft Computing", "url": "http://arxiv.org/abs/2507.17008v1", "summary": "Most sign language handshape datasets are severely limited and unbalanced,\nposing significant challenges to effective model training. In this paper, we\nexplore the effectiveness of augmenting the training data of a handshape\nclassifier by generating synthetic data. We use an EfficientNet classifier\ntrained on the RWTH German sign language handshape dataset, which is small and\nheavily unbalanced, applying different strategies to combine generated and real\nimages. We compare two Generative Adversarial Networks (GAN) architectures for\ndata generation: ReACGAN, which uses label information to condition the data\ngeneration process through an auxiliary classifier, and SPADE, which utilizes\nspatially-adaptive normalization to condition the generation on pose\ninformation. ReACGAN allows for the generation of realistic images that align\nwith specific handshape labels, while SPADE focuses on generating images with\naccurate spatial handshape configurations. Our proposed techniques improve the\ncurrent state-of-the-art accuracy on the RWTH dataset by 5%, addressing the\nlimitations of small and unbalanced datasets. Additionally, our method\ndemonstrates the capability to generalize across different sign language\ndatasets by leveraging pose-based generation trained on the extensive HaGRID\ndataset. We achieve comparable performance to single-source trained classifiers\nwithout the need for retraining the generator.", "comment": "23 pages, 8 figures, to be published in Applied Soft Computing", "pdf_url": "http://arxiv.org/pdf/2507.17008v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "为手势形状分类带来平衡：通过生成模型缓解数据不平衡问题", "tldr": "该研究通过使用生成模型（ReACGAN和SPADE）生成合成数据来增强手势分类器的训练数据，有效缓解了小型和不平衡数据集的限制，并在RWTH数据集上将最新准确率提高了5%。", "motivation": "大多数手语手势数据集严重受限且不平衡，对手势分类模型的有效训练构成了重大挑战。", "method": "本研究使用EfficientNet分类器在小型且高度不平衡的RWTH德语手语手势数据集上进行训练。通过应用不同的策略结合生成图像和真实图像来扩充训练数据。比较了两种生成对抗网络（GAN）架构用于数据生成：ReACGAN（利用标签信息条件化数据生成）和SPADE（利用空间自适应归一化条件化姿态信息生成）。此外，还利用在HaGRID数据集上训练的基于姿态的生成方法来验证模型的泛化能力。", "result": "所提出的技术将RWTH数据集上的最新准确率提高了5%，解决了小型和不平衡数据集的限制。该方法还展示了跨不同手语数据集的泛化能力，在不重新训练生成器的情况下，实现了与单源训练分类器相当的性能。", "conclusion": "通过使用生成模型扩充训练数据是缓解手语手势分类中数据不平衡和数据集规模限制的有效方法，并且能够提高分类性能和模型的泛化能力。", "translation": "大多数手语手势数据集严重受限且不平衡，对手势分类的有效模型训练构成了重大挑战。在本文中，我们探讨了通过生成合成数据来增强手势分类器训练数据的有效性。我们使用在RWTH德语手语手势数据集上训练的EfficientNet分类器，该数据集规模小且高度不平衡，并应用不同的策略结合生成图像和真实图像。我们比较了两种用于数据生成的生成对抗网络（GAN）架构：ReACGAN，它使用标签信息通过辅助分类器来条件化数据生成过程；以及SPADE，它利用空间自适应归一化来条件化基于姿态信息的生成。ReACGAN允许生成与特定手势标签对齐的真实图像，而SPADE则专注于生成具有准确空间手势配置的图像。我们提出的技术将RWTH数据集上的最新准确率提高了5%，解决了小型和不平衡数据集的限制。此外，我们的方法通过利用在大量HaGRID数据集上训练的基于姿态的生成，展示了跨不同手语数据集的泛化能力。我们实现了与单源训练分类器相当的性能，而无需重新训练生成器。", "summary": "本论文旨在解决手语手势分类中数据集小且不平衡的问题。研究提出通过生成模型（ReACGAN和SPADE）生成合成数据来扩充训练集，并结合EfficientNet分类器进行训练。实验结果表明，该方法将RWTH数据集上的最新准确率提高了5%，有效缓解了数据不平衡的限制。此外，该方法通过利用基于姿态的生成，展示了在不同手语数据集上的良好泛化能力，无需重新训练生成器即可达到与单源训练模型相当的性能。", "keywords": "手势分类, 数据不平衡, 生成模型, GAN, 数据增强", "comments": "该论文的创新之处在于利用生成模型（特别是ReACGAN和SPADE）来解决手语手势分类中常见的数据不平衡问题。通过生成高质量的合成数据，有效扩充了训练集，显著提高了分类器的性能。此外，其在不同数据集上的泛化能力也增强了该方法的实用性。这项工作对于资源稀缺的领域具有重要意义。"}}
{"id": "2507.17252", "title": "Unsupervised Exposure Correction", "authors": ["Ruodai Cui", "Li Niu", "Guosheng Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17252v1", "summary": "Current exposure correction methods have three challenges, labor-intensive\npaired data annotation, limited generalizability, and performance degradation\nin low-level computer vision tasks. In this work, we introduce an innovative\nUnsupervised Exposure Correction (UEC) method that eliminates the need for\nmanual annotations, offers improved generalizability, and enhances performance\nin low-level downstream tasks. Our model is trained using freely available\npaired data from an emulated Image Signal Processing (ISP) pipeline. This\napproach does not need expensive manual annotations, thereby minimizing\nindividual style biases from the annotation and consequently improving its\ngeneralizability. Furthermore, we present a large-scale Radiometry Correction\nDataset, specifically designed to emphasize exposure variations, to facilitate\nunsupervised learning. In addition, we develop a transformation function that\npreserves image details and outperforms state-of-the-art supervised methods\n[12], while utilizing only 0.01% of their parameters. Our work further\ninvestigates the broader impact of exposure correction on downstream tasks,\nincluding edge detection, demonstrating its effectiveness in mitigating the\nadverse effects of poor exposure on low-level features. The source code and\ndataset are publicly available at https://github.com/BeyondHeaven/uec_code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17252v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "无监督曝光校正", "tldr": "本文提出了一种无监督曝光校正（UEC）方法，解决了传统方法对标注数据的依赖、泛化性差以及在低级视觉任务中性能下降的问题，并通过模拟ISP管线数据进行训练，同时引入了大规模辐射度校正数据集和保持图像细节的变换函数。", "motivation": "当前曝光校正方法面临三个挑战：劳动力密集型配对数据标注、泛化能力有限以及在低级计算机视觉任务中性能下降。", "method": "本文引入了一种创新的无监督曝光校正（UEC）方法。该模型利用从模拟图像信号处理（ISP）管线获得的免费配对数据进行训练，避免了昂贵的手动标注。此外，提出了一个大规模辐射度校正数据集，专门用于强调曝光变化以促进无监督学习。还开发了一个保持图像细节的变换函数。", "result": "UEC方法无需手动标注，提供了改进的泛化能力，并在低级下游任务中增强了性能。所开发的变换函数在利用仅0.01%参数的情况下，性能优于最先进的监督方法。研究还表明，曝光校正能有效减轻不良曝光对边缘检测等低级特征的不利影响。", "conclusion": "本文提出的无监督曝光校正（UEC）方法通过利用模拟ISP数据和创新的变换函数，有效解决了传统曝光校正方法在数据标注、泛化性和低级任务性能方面的挑战，显著提升了曝光校正的实用性和效果。", "translation": "当前曝光校正方法存在三个挑战：劳动密集型配对数据标注、泛化能力有限以及在低级计算机视觉任务中性能下降。在这项工作中，我们引入了一种创新的无监督曝光校正（UEC）方法，该方法消除了手动标注的需要，提供了改进的泛化能力，并增强了在低级下游任务中的性能。我们的模型使用来自模拟图像信号处理（ISP）管线的免费可用配对数据进行训练。这种方法不需要昂贵的手动标注，从而最大限度地减少了标注中的个体风格偏差，从而提高了其泛化能力。此外，我们提出了一个大规模辐射度校正数据集，专门设计用于强调曝光变化，以促进无监督学习。此外，我们开发了一个保持图像细节的变换函数，其性能优于最先进的监督方法[12]，同时仅使用了其0.01%的参数。我们的工作进一步研究了曝光校正对下游任务（包括边缘检测）的更广泛影响，证明了其在减轻不良曝光对低级特征的不利影响方面的有效性。源代码和数据集可在https://github.com/BeyondHeaven/uec_code公开获取。", "summary": "本文提出了一种名为无监督曝光校正（UEC）的新方法，旨在解决现有曝光校正方法中存在的标注依赖、泛化性差以及对低级视觉任务性能负面影响的问题。UEC模型通过利用模拟ISP管线生成的免费配对数据进行训练，避免了手动标注的成本和主观性，从而提高了泛化能力。研究还引入了一个大规模辐射度校正数据集，并开发了一个高效的图像细节保持变换函数，该函数在参数量极少的情况下超越了监督方法。此外，工作还探讨了曝光校正对边缘检测等下游任务的积极影响，证明了其在改善低级特征方面的有效性。", "keywords": "无监督学习, 曝光校正, 泛化能力, 低级视觉任务, 辐射度校正", "comments": "这项工作在无监督学习应用于图像曝光校正方面具有重要创新，尤其是在不依赖昂贵手动标注的情况下实现高性能。通过模拟ISP数据和引入专用数据集，该方法提升了实用性和泛化性。其在参数效率和对下游任务的积极影响方面也展现出显著优势。"}}
{"id": "2507.17717", "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17717v1", "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17717v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从反馈到清单：AI生成临床笔记的 grounded 评估", "tldr": "本文提出了一种将用户反馈转化为结构化清单的方法，用于评估AI生成的临床笔记，该方法在离线评估中表现优于基线方法，并与临床医生偏好高度一致。", "motivation": "AI生成的临床笔记在医疗保健中日益普及，但由于高度主观性和专家评审的可扩展性有限，评估其质量仍然是一个挑战。现有的自动化指标往往与真实的医生偏好不符。", "method": "我们提出了一种流水线，系统地将真实用户反馈提炼成结构化清单，用于笔记评估。这些清单被设计为可解释、基于人工反馈，并可由基于LLM的评估器强制执行。使用了来自21,000多次临床就诊的去识别化数据。", "result": "我们发现，我们从反馈中得出的清单在覆盖范围、多样性和对人工评分的预测能力方面优于基线方法。大量实验证实了清单对质量下降扰动的鲁棒性、与临床医生偏好的显著一致性以及作为评估方法的实用价值。", "conclusion": "在离线研究设置中，该清单可以帮助识别可能低于预设质量阈值的笔记。", "translation": "AI生成的临床笔记在医疗保健中日益普及，但由于高度主观性和专家评审的可扩展性有限，评估其质量仍然是一个挑战。现有自动化指标往往未能与真实世界的医生偏好保持一致。为了解决这个问题，我们提出了一种流水线，系统地将真实用户反馈提炼成结构化清单，用于笔记评估。这些清单被设计为可解释、基于人工反馈，并可由基于LLM的评估器强制执行。我们使用来自已部署AI医疗记录系统超过21,000次临床就诊的去识别化数据（根据HIPAA安全港标准准备），结果表明，我们从反馈中得出的清单在覆盖范围、多样性和对人工评分的预测能力方面优于我们的离线评估中的基线方法。大量实验证实了该清单对质量下降扰动的鲁棒性、与临床医生偏好的显著一致性以及作为评估方法的实用价值。在离线研究设置中，该清单可以帮助识别可能低于我们所选质量阈值的笔记。", "summary": "本文提出了一种新颖的方法，通过将真实用户反馈系统地转化为结构化评估清单，来解决AI生成临床笔记质量评估中存在的主观性和可扩展性问题。该研究利用了超过21,000次临床就诊的去识别化数据，并证明了其反馈衍生清单在覆盖率、多样性和对人类评分的预测能力方面优于现有基线方法。实验结果进一步验证了该清单的鲁棒性、与临床医生偏好的一致性及其作为有效评估工具的实际价值，尤其是在离线研究中识别低质量笔记的能力。", "keywords": "AI生成临床笔记, 质量评估, 用户反馈, 结构化清单, 大语言模型评估", "comments": "这项研究提出了一种创新且实用的方法来解决AI生成内容（特别是临床笔记）的评估难题。通过将主观的用户反馈转化为可量化、可解释的清单，并利用LLM进行评估，它有效地提升了评估的客观性和可扩展性。其创新点在于将人类中心的设计与自动化评估相结合，为未来AI在敏感领域（如医疗）的应用提供了更可靠的质量控制机制。这项工作对于确保AI在实际应用中的安全性和有效性具有重要意义。"}}
{"id": "2507.17726", "title": "Deep Generative Learning of Magnetic Frustration in Artificial Spin Ice from Magnetic Force Microscopy Images", "authors": ["Arnab Neogi", "Suryakant Mishra", "Prasad P Iyer", "Tzu-Ming Lu", "Ezra Bussmann", "Sergei Tretiak", "Andrew Crandall Jones", "Jian-Xin Zhu"], "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17726v1", "summary": "Increasingly large datasets of microscopic images with atomic resolution\nfacilitate the development of machine learning methods to identify and analyze\nsubtle physical phenomena embedded within the images. In this work, microscopic\nimages of honeycomb lattice spin-ice samples serve as datasets from which we\nautomate the calculation of net magnetic moments and directional orientations\nof spin-ice configurations. In the first stage of our workflow, machine\nlearning models are trained to accurately predict magnetic moments and\ndirections within spin-ice structures. Variational Autoencoders (VAEs), an\nemergent unsupervised deep learning technique, are employed to generate\nhigh-quality synthetic magnetic force microscopy (MFM) images and extract\nlatent feature representations, thereby reducing experimental and segmentation\nerrors. The second stage of proposed methodology enables precise identification\nand prediction of frustrated vertices and nanomagnetic segments, effectively\ncorrelating structural and functional aspects of microscopic images. This\nfacilitates the design of optimized spin-ice configurations with controlled\nfrustration patterns, enabling potential on-demand synthesis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17726v1", "cate": "cond-mat.dis-nn", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于磁力显微镜图像的深层生成学习人工自旋冰中的磁挫折", "tldr": "本文利用深度生成学习方法，从磁力显微镜图像中识别和分析人工自旋冰中的磁挫折，并自动化磁矩和方向的计算，从而实现对自旋冰构型的优化设计。", "motivation": "随着原子分辨率显微图像数据集的不断增长，机器学习方法在识别和分析图像中嵌入的微妙物理现象方面展现出巨大潜力。本研究的动机是自动化计算人工自旋冰构型的净磁矩和方向，并识别和预测磁挫折，以期设计出具有可控挫折模式的优化自旋冰构型。", "method": "本研究采用两阶段方法。第一阶段，训练机器学习模型以准确预测自旋冰结构中的磁矩和方向，并利用变分自编码器（VAEs）生成高质量的合成磁力显微镜（MFM）图像并提取潜在特征表示，从而减少实验和分割误差。第二阶段，所提出的方法能够精确识别和预测受挫顶点和纳米磁性片段，有效地关联微观图像的结构和功能方面。", "result": "研究成功实现了人工自旋冰构型中净磁矩和方向的自动化计算。利用VAEs生成了高质量的合成MFM图像并提取了潜在特征表示。该方法能够精确识别和预测受挫顶点和纳米磁性片段，并有效关联了微观图像的结构和功能方面。", "conclusion": "本研究提出的方法通过深度生成学习，能够自动化分析人工自旋冰中的磁挫折，并促进了具有可控挫折模式的优化自旋冰构型的设计，从而为潜在的按需合成提供了可能性。", "translation": "随着原子分辨率显微图像数据集的不断扩大，机器学习方法在识别和分析图像中嵌入的微妙物理现象方面得到了发展。在这项工作中，我们利用蜂窝状晶格自旋冰样本的微观图像作为数据集，从中自动化计算自旋冰构型的净磁矩和方向。在我们工作流程的第一阶段，训练机器学习模型以准确预测自旋冰结构中的磁矩和方向。变分自编码器（VAEs）作为一种新兴的无监督深度学习技术，被用来生成高质量的合成磁力显微镜（MFM）图像并提取潜在特征表示，从而减少实验和分割误差。所提出的方法在第二阶段能够精确识别和预测受挫顶点和纳米磁性片段，有效地关联微观图像的结构和功能方面。这有助于设计具有可控挫折模式的优化自旋冰构型，从而实现潜在的按需合成。", "summary": "本文提出了一种基于深度生成学习的方法，利用磁力显微镜图像数据自动化分析人工自旋冰中的磁挫折。该方法分两阶段进行：首先，训练机器学习模型预测自旋冰的磁矩和方向，并利用变分自编码器（VAEs）生成合成MFM图像并提取特征，以减少误差；其次，精确识别和预测受挫顶点和纳米磁性片段，将结构与功能关联。最终，该方法有助于设计具有可控挫折模式的优化自旋冰构型，为按需合成提供可能。", "keywords": "深度生成学习, 磁挫折, 人工自旋冰, 磁力显微镜, 变分自编码器", "comments": "本文的创新点在于将深度生成学习（特别是VAEs）应用于磁力显微镜图像分析，以自动化识别和量化人工自旋冰中的磁挫折现象。通过生成高质量合成图像和提取潜在特征，该方法有效地克服了实验和分割误差，提高了分析的准确性。其重要性在于，通过精确识别和预测挫折模式，为设计和潜在的按需合成具有特定磁学性质的自旋冰材料提供了新的途径，对磁性材料领域具有重要意义。"}}
{"id": "2507.16199", "title": "WAKENLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking", "authors": ["Zipeng Ling", "Yuehao Tang", "Shuliang Liu", "Junqi Yang", "Shenghong Fu", "Yao Wan", "Kejia Huang", "Chen Huang", "Zhichao Hou", "Xuming Hu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16199v2", "summary": "Large Language Models (LLMs) frequently output the label Unknown, yet current\nevaluations focus almost exclusively on whether such answers are honest rather\nthan why they arise. This blurs two distinct cases: (i) an input that is\ngenuinely indeterminate and (ii) a solvable problem that the model fails to\nresolve. We call this phenomenon Vague Perception. And thus we introduce a\nframework that quantifies the proportion of Unknown responses attributable to\nmodel incapacity and tests whether guided stimulation can convert them into\neither correct Known or correct Unknown with valid reasoning. By separating\nthese sources of uncertainty, our method provides a clearer picture of LLM\nreasoning limits and their potential for improvement. As we get a theoretical\naccuracy of reasoning task on different LLMs, we apply different methods to\ntest whether the model can reach the accuracy given a baseline framework. Our\nwork is meaningful in exploring the potential reasoning ability of LLMs and\nproviding a new perspective on solving the Vague Perception phenomenon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16199v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "WAKENLLM：通过细粒度基准测试评估LLM的推理潜力和稳定性", "tldr": "大型语言模型（LLM）输出“未知”可能源于模型能力不足而非真正的不确定性；本文提出WAKENLLM框架来区分并测试LLM的推理能力。", "motivation": "当前LLM评估主要关注“未知”回答的诚实性，而非其产生原因，这模糊了真正不确定性与模型未能解决可解问题的情况（即“模糊感知”现象），限制了对LLM推理局限性的理解。", "method": "引入WAKENLLM框架，量化归因于模型无能力的“未知”响应比例，并测试引导式刺激能否将其转化为带有有效推理的正确“已知”或正确“未知”响应。通过分离不确定性来源，并应用不同方法测试模型是否能达到理论准确性。", "result": "提供了LLM推理限制及其改进潜力的更清晰图景。获得了不同LLM在推理任务上的理论准确性。", "conclusion": "本工作在探索LLM的潜在推理能力和为解决“模糊感知”现象提供新视角方面具有重要意义。", "translation": "大型语言模型 (LLM) 经常输出“未知”标签，然而当前的评估几乎只关注此类回答是否诚实，而不是它们为何出现。这模糊了两种不同的情况：（i）输入本身确实不确定，以及（ii）模型未能解决的可解问题。我们称这种现象为“模糊感知”。因此，我们引入了一个框架，该框架量化了可归因于模型无能力的“未知”响应的比例，并测试引导式刺激是否能将其转化为带有有效推理的正确“已知”或正确“未知”。通过分离这些不确定性来源，我们的方法更清晰地描绘了LLM推理的局限性及其改进潜力。当我们获得了不同LLM在推理任务上的理论准确性时，我们应用不同的方法来测试模型是否能在给定基线框架的情况下达到该准确性。我们的工作在探索LLM的潜在推理能力和为解决“模糊感知”现象提供新视角方面具有重要意义。", "summary": "本文提出WAKENLLM，一个新颖的框架，旨在通过解决“模糊感知”现象来评估大型语言模型（LLM）的推理潜力和稳定性。它区分了真正反映不确定输入的“未知”响应和源于模型无法解决可解问题的“未知”响应。WAKENLLM量化了模型能力不足的程度，并测试引导式刺激是否能改善响应，从而更清晰地理解LLM的推理局限性和改进潜力。", "keywords": "LLM, 推理, 评估, 模糊感知, 基准测试", "comments": "该论文通过区分真正的不确定性和模型能力不足来评估LLM，提供了一个新颖的视角，这是一项重要的创新。这种对“未知”响应的细粒度分析有助于更具针对性地改进LLM的推理能力。"}}
{"id": "2507.17006", "title": "Quantitative Quantum Soundness for Bipartite Compiled Bell Games via the Sequential NPA Hierarchy", "authors": ["Igor Klep", "Connor Paddock", "Marc-Olivier Renou", "Simon Schmidt", "Lucas Tendick", "Xiangling Xu", "Yuming Zhao"], "categories": ["quant-ph", "cs.CR", "math-ph", "math.MP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      41 pages, 1 figure; comments welcome. We refer to Cui, Falor, Natarajan, and Zhang for an independent parallel work on the same topic", "url": "http://arxiv.org/abs/2507.17006v1", "summary": "Compiling Bell games under cryptographic assumptions replaces the need for\nphysical separation, allowing nonlocality to be probed with a single untrusted\ndevice. While Kalai et al. (STOC'23) showed that this compilation preserves\nquantum advantages, its quantitative quantum soundness has remained an open\nproblem. We address this gap with two primary contributions. First, we\nestablish the first quantitative quantum soundness bounds for every bipartite\ncompiled Bell game whose optimal quantum strategy is finite-dimensional: any\npolynomial-time prover's score in the compiled game is negligibly close to the\ngame's ideal quantum value. More generally, for all bipartite games we show\nthat the compiled score cannot significantly exceed the bounds given by a newly\nformalized sequential Navascu\\'es-Pironio-Ac\\'in (NPA) hierarchy. Second, we\nprovide a full characterization of this sequential NPA hierarchy, establishing\nit as a robust numerical tool that is of independent interest. Finally, for\ngames without finite-dimensional optimal strategies, we explore the necessity\nof NPA approximation error for quantitatively bounding their compiled scores,\nlinking these considerations to the complexity conjecture\n$\\mathrm{MIP}^{\\mathrm{co}}=\\mathrm{coRE}$ and open challenges such as quantum\nhomomorphic encryption correctness for \"weakly commuting\" quantum registers.", "comment": "41 pages, 1 figure; comments welcome. We refer to Cui, Falor,\n  Natarajan, and Zhang for an independent parallel work on the same topic", "pdf_url": "http://arxiv.org/pdf/2507.17006v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "通过序列NPA层级实现二分编译贝尔博弈的定量量子可靠性", "tldr": "本文通过引入并刻画序列NPA层级，首次为二分编译贝尔博弈提供了定量的量子可靠性界限，解决了单不可信设备下量子优势的度量问题。", "motivation": "Kalai等人（STOC'23）证明了加密假设下的贝尔博弈编译保留了量子优势，但其定量的量子可靠性仍然是一个开放问题。本文旨在填补这一空白。", "method": "本文引入并形式化了一个新的序列Navascués-Pironio-Acín (NPA) 层级，并对其进行了全面刻画，将其确立为一个独立的、鲁棒的数值工具。", "result": "1. 首次为所有最优量子策略是有限维的二分编译贝尔博弈建立了定量的量子可靠性界限：在编译博弈中，任何多项式时间证明者的得分都与博弈的理想量子值非常接近。2. 更普遍地，对于所有二分博弈，编译得分不会显著超过新形式化的序列NPA层级所给出的界限。3. 全面刻画了序列NPA层级。4. 对于没有有限维最优策略的博弈，探讨了NPA近似误差对于定量限制其编译得分的必要性，并将其与复杂性猜想$\\mathrm{MIP}^{\\mathrm{co}}=\\mathrm{coRE}$以及开放挑战（如“弱对易”量子寄存器的量子同态加密正确性）联系起来。", "conclusion": "本文成功地为二分编译贝尔博弈提供了定量的量子可靠性界限，并通过引入和刻画序列NPA层级提供了一个新的数值工具，为量子非局域性研究和量子复杂性理论带来了重要进展。", "translation": "在加密假设下编译贝尔博弈取代了对物理分离的需求，允许使用单个不可信设备探测非局域性。虽然Kalai等人（STOC'23）表明这种编译保留了量子优势，但其定量的量子可靠性仍然是一个开放问题。我们通过两项主要贡献解决了这一空白。首先，我们为所有最优量子策略是有限维的二分编译贝尔博弈建立了第一个定量的量子可靠性界限：在编译博弈中，任何多项式时间证明者的得分都与博弈的理想量子值非常接近。更普遍地，对于所有二分博弈，我们表明编译得分不会显著超过新形式化的序列Navascués-Pironio-Acín (NPA) 层级所给出的界限。其次，我们提供了对这个序列NPA层级的全面刻画，将其确立为一个独立的、鲁棒的数值工具。最后，对于没有有限维最优策略的博弈，我们探讨了NPA近似误差对于定量限制其编译得分的必要性，并将这些考虑与复杂性猜想$\\mathrm{MIP}^{\\mathrm{co}}=\\mathrm{coRE}$以及开放挑战（如“弱对易”量子寄存器的量子同态加密正确性）联系起来。", "summary": "本文研究了在加密假设下编译贝尔博弈的定量量子可靠性问题，该问题此前是一个开放挑战。作者首次为具有有限维最优量子策略的二分编译贝尔博弈建立了定量的量子可靠性界限，并证明了多项式时间证明者的得分接近理想量子值。为实现这一目标，文章引入并全面刻画了一个新的序列Navascués-Pironio-Acín (NPA) 层级，并将其确立为一个鲁棒的数值工具。此外，文章还探讨了NPA近似误差对于没有有限维最优策略的博弈的重要性，并将其与量子复杂性理论中的重要猜想和开放问题联系起来。", "keywords": "编译贝尔博弈, 量子可靠性, 序列NPA层级, 量子非局域性, 量子复杂性", "comments": "本文的创新之处在于首次为编译贝尔博弈提供了定量的量子可靠性分析，解决了该领域的一个关键开放问题。引入并全面刻画序列NPA层级是其重要贡献，不仅为该问题提供了新的分析工具，也为量子信息理论提供了新的数学工具，具有独立的理论价值。文章还将其结果与量子复杂性理论的深层问题联系起来，显示了其潜在的广泛影响。"}}
{"id": "2507.17542", "title": "AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests", "authors": ["Lara Khatib", "Noble Saji Mathews", "Meiyappan Nagappan"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17542v1", "summary": "Bug reproduction is critical in the software debugging and repair process,\nyet the majority of bugs in open-source and industrial settings lack executable\ntests to reproduce them at the time they are reported, making diagnosis and\nresolution more difficult and time-consuming. To address this challenge, we\nintroduce AssertFlip, a novel technique for automatically generating Bug\nReproducible Tests (BRTs) using large language models (LLMs). Unlike existing\nmethods that attempt direct generation of failing tests, AssertFlip first\ngenerates passing tests on the buggy behaviour and then inverts these tests to\nfail when the bug is present. We hypothesize that LLMs are better at writing\npassing tests than ones that crash or fail on purpose. Our results show that\nAssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a\nbenchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass\nsuccess rate of 43.6% on the SWT-Bench-Verified subset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17542v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "AssertFlip：通过反转LLM生成的通过测试来重现错误", "tldr": "AssertFlip是一种新颖的技术，通过反转LLM生成的通过测试来自动生成可重现错误的测试，并在SWT-Bench上表现优于现有技术。", "motivation": "软件调试和修复过程中，错误重现至关重要，但大多数错误在报告时缺乏可执行测试，导致诊断和解决困难且耗时。", "method": "AssertFlip首先生成针对错误行为的通过测试，然后反转这些测试，使其在错误存在时失败。其假设是LLM更擅长编写通过测试而非故意失败的测试。", "result": "AssertFlip在SWT-Bench基准测试中优于所有已知技术。具体来说，在SWT-Bench-Verified子集上，AssertFlip的失败-通过成功率达到43.6%。", "conclusion": "AssertFlip通过创新的反转通过测试方法，显著提高了自动生成错误可重现测试的效率和成功率，解决了当前调试中测试缺失的难题。", "translation": "错误重现是软件调试和修复过程中的关键环节，然而，在开源和工业环境中，大多数错误在报告时都缺乏可执行的测试来重现它们，这使得诊断和解决变得更加困难和耗时。为了应对这一挑战，我们引入了AssertFlip，这是一种利用大型语言模型（LLM）自动生成错误可重现测试（BRT）的新颖技术。与现有尝试直接生成失败测试的方法不同，AssertFlip首先针对错误行为生成通过测试，然后反转这些测试，使其在错误存在时失败。我们假设LLM更擅长编写通过测试，而不是故意崩溃或失败的测试。我们的结果表明，AssertFlip在SWT-Bench（一个为BRT策划的基准测试）的排行榜上超越了所有已知技术。具体来说，AssertFlip在SWT-Bench-Verified子集上的失败-通过成功率达到了43.6%。", "summary": "本文提出了AssertFlip，一种利用大型语言模型（LLM）自动生成错误可重现测试（BRT）的新方法。针对现有方法难以直接生成失败测试的问题，AssertFlip创新性地先生成通过测试，再将其反转为失败测试。实验结果表明，AssertFlip在SWT-Bench基准测试中表现优异，超越了现有技术，尤其在SWT-Bench-Verified子集上取得了43.6%的失败-通过成功率，有效解决了缺乏可执行测试导致错误重现困难的挑战。", "keywords": "错误重现, LLM, 测试生成, AssertFlip, 软件调试", "comments": "AssertFlip的创新之处在于其“反转通过测试”的策略，这与传统直接生成失败测试的方法形成对比，并利用了LLM在生成“正确”代码方面的优势。这一方法显著提高了错误重现测试的自动化水平和成功率，对于提高软件调试效率具有重要意义。"}}
{"id": "2507.16982", "title": "Extension of Simple and Accurate Inductance Estimation for Rectangular Planar Windings", "authors": ["Theofilos Papadopoulos", "Antonios Antonopoulos"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figures", "url": "http://arxiv.org/abs/2507.16982v1", "summary": "This paper proposes a method to generalize the equations estimating the\ninductance of square-shape planar windings to rectangle shape. This is done by\nutilizing the optimal p-norm of the Generalized Mean Value or Power Mean (PM).\nThree well-established equations with verified accuracy are examined, namely\nWheeler, Rosa, and the Monomial, which by definition consider only regular\npolygons. One critical parameter of the original equations is the outer-side\nlength of the winding, which for the rectangle case, can be substituted by the\nPM of the two outer-side lengths, without the need for any further\nmodifications. A methodology to select the optimal p-norm for the PM is\npresented in terms of achieving the best accuracy for this estimation. The\nselection of the optimal p is based on results from datasets containing more\nthan 2600 simulations of different rectangle-shaped windings. Finally, the\nestimation accuracy is verified by laboratory measurements for a selection of\nplanar inductors.", "comment": "9 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.16982v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "矩形平面绕组简单精确电感估算方法的扩展", "tldr": "本文提出了一种利用广义均值（幂均值）的最优p-范数，将方形平面绕组的电感估算方程推广到矩形绕组的方法，并通过2600多次仿真和实验室测量验证了其准确性。", "motivation": "现有用于估算方形平面绕组电感的方程，如Wheeler、Rosa和Monomial方程，仅适用于正多边形。本文的动机在于将这些简单精确的估算方法推广到矩形平面绕组，以提高其适用性。", "method": "本文提出了一种方法，通过使用广义均值或幂均值（PM）的最优p-范数，将方形平面绕组的电感估算方程推广到矩形形状。具体做法是将原始方程中的外侧边长替换为两个外侧边长的PM，无需其他修改。此外，还提出了一种选择PM最优p-范数的方法，以实现最佳估算精度，该选择基于2600多个不同矩形绕组仿真数据集的结果。最终，通过实验室测量对选定的平面电感器进行了估算精度的验证。", "result": "通过将原始方程中的外侧边长替换为两个外侧边长的幂均值，可以成功地将方形绕组的电感估算方程推广到矩形绕组。此外，还提出了一种选择最优p-范数的方法，该方法基于2600多次不同矩形绕组的仿真数据，确保了估算的最佳精度。最终，通过实验室测量验证了估算方法的准确性。", "conclusion": "本文成功提出了一种将方形平面绕组电感估算方程推广到矩形绕组的方法，该方法通过利用广义均值（幂均值）的最优p-范数替换关键参数，并通过大量仿真和实验室测量验证了其准确性。", "translation": "本文提出了一种将方形平面绕组电感估算方程推广到矩形的方法。这通过利用广义均值或幂均值（PM）的最优p-范数来实现。本文检验了三个经验证准确性的成熟方程，即Wheeler、Rosa和Monomial方程，这些方程根据定义仅考虑正多边形。原始方程的一个关键参数是绕组的外侧边长，对于矩形情况，可以用两个外侧边长的PM替代，无需任何进一步修改。本文提出了一种选择PM最优p-范数的方法，以实现这种估算的最佳精度。最优p的选择基于包含2600多个不同矩形绕组仿真数据集的结果。最后，通过对选定平面电感器的实验室测量验证了估算精度。", "summary": "本文提出了一种将方形平面绕组电感估算方程推广到矩形绕组的新方法。该方法利用广义均值（幂均值）的最优p-范数，将现有方程（如Wheeler、Rosa、Monomial）中的方形外侧边长替换为矩形的两条外侧边长的幂均值。研究通过2600多次仿真确定了最优p值，并通过实验室测量验证了该方法的估算精度。", "keywords": "电感估算, 矩形绕组, 幂均值, p-范数, 平面电感器", "comments": "本文的创新之处在于巧妙地利用了广义均值（幂均值）的概念，成功地将针对方形绕组的成熟电感估算方程推广到了更普遍的矩形绕组，极大地扩展了这些方法的适用范围。通过大量的仿真数据来确定最优参数，并结合实际测量进行验证，体现了研究的严谨性与实用性。"}}
{"id": "2507.17692", "title": "Joint Asymmetric Loss for Learning with Noisy Labels", "authors": ["Jialiang Wang", "Xianming Liu", "Xiong Zhou", "Gangfeng Hu", "Deming Zhai", "Junjun Jiang", "Xiangyang Ji"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.17692v1", "summary": "Learning with noisy labels is a crucial task for training accurate deep\nneural networks. To mitigate label noise, prior studies have proposed various\nrobust loss functions, particularly symmetric losses. Nevertheless, symmetric\nlosses usually suffer from the underfitting issue due to the overly strict\nconstraint. To address this problem, the Active Passive Loss (APL) jointly\noptimizes an active and a passive loss to mutually enhance the overall fitting\nability. Within APL, symmetric losses have been successfully extended, yielding\nadvanced robust loss functions. Despite these advancements, emerging\ntheoretical analyses indicate that asymmetric losses, a new class of robust\nloss functions, possess superior properties compared to symmetric losses.\nHowever, existing asymmetric losses are not compatible with advanced\noptimization frameworks such as APL, limiting their potential and\napplicability. Motivated by this theoretical gap and the prospect of asymmetric\nlosses, we extend the asymmetric loss to the more complex passive loss scenario\nand propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We\nrigorously establish the necessary and sufficient condition under which AMSE\nsatisfies the asymmetric condition. By substituting the traditional symmetric\npassive loss in APL with our proposed AMSE, we introduce a novel robust loss\nframework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate\nthe effectiveness of our method in mitigating label noise. Code available at:\nhttps://github.com/cswjl/joint-asymmetric-loss", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17692v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "联合非对称损失用于含噪标签学习", "tldr": "本文提出了一种名为联合非对称损失 (JAL) 的新框架，通过将新颖的非对称均方误差 (AMSE) 损失整合到主动被动损失 (APL) 框架中，有效解决了含噪标签学习中的问题，克服了对称损失和现有非对称损失的局限性。", "motivation": "在含噪标签学习中，现有对称损失函数存在欠拟合问题。尽管非对称损失函数在理论上具有更优的特性，但它们与像主动被动损失 (APL) 这样的先进优化框架不兼容，这限制了它们的潜力。本研究旨在弥补这一理论空白，并利用非对称损失的优势。", "method": "本文将非对称损失扩展到更复杂的被动损失场景，并提出了一种新颖的非对称损失函数——非对称均方误差 (AMSE)，严格确立了AMSE满足非对称条件的充要条件。通过用所提出的AMSE替换APL中传统的对称被动损失，引入了一种新的鲁棒损失框架，称为联合非对称损失 (JAL)。", "result": "广泛的实验证明了所提出的方法在缓解标签噪声方面的有效性。", "conclusion": "本文提出了联合非对称损失 (JAL) 框架，通过引入新型非对称损失 (AMSE) 并将其融入主动被动损失 (APL) 框架中，有效解决了含噪标签学习的挑战，表现出优于以往方法的性能。", "translation": "含噪标签学习是训练精确深度神经网络的关键任务。为了减轻标签噪声，先前的研究提出了各种鲁棒损失函数，特别是对称损失。然而，对称损失通常由于过于严格的约束而遭受欠拟合问题。为了解决这个问题，主动被动损失 (APL) 联合优化主动损失和被动损失，以相互增强整体拟合能力。在APL中，对称损失已成功扩展，产生了先进的鲁棒损失函数。尽管取得了这些进展，但新兴的理论分析表明，非对称损失——一类新的鲁棒损失函数——与对称损失相比具有更优越的特性。然而，现有的非对称损失与APL等先进优化框架不兼容，限制了它们的潜力和适用性。受这一理论空白和非对称损失前景的启发，我们将非对称损失扩展到更复杂的被动损失场景，并提出了非对称均方误差 (AMSE)，一种新颖的非对称损失。我们严格确立了AMSE满足非对称条件的充要条件。通过用我们提出的AMSE替换APL中传统的对称被动损失，我们引入了一种新的鲁棒损失框架，称为联合非对称损失 (JAL)。广泛的实验证明了我们方法在减轻标签噪声方面的有效性。代码可在：https://github.com/cswjl/joint-asymmetric-loss 获取。", "summary": "本论文旨在解决含噪标签学习的挑战，提出了一种名为联合非对称损失 (JAL) 的新框架。该研究引入了非对称均方误差 (AMSE) 这一新型非对称损失，并严格定义了其满足非对称性的条件。JAL 通过将 AMSE 整合到主动被动损失 (APL) 框架中，替代传统的对称被动损失而构建。这种方法成功克服了对称损失的欠拟合问题以及现有非对称损失与先进优化框架不兼容的局限性，在减轻标签噪声方面表现出卓越的性能。", "keywords": "含噪标签, 非对称损失, 鲁棒损失, 联合非对称损失, 主动被动损失", "comments": "该论文的创新之处在于成功地将理论上更优越的非对称损失函数融入到像APL这样的先进优化框架中，这在之前是一个挑战。这弥合了重要的理论和实践鸿沟，为含噪标签学习提供了一个更鲁棒的解决方案。"}}
{"id": "2507.17729", "title": "A Comprehensive Evaluation Framework for the Study of the Effects of Facial Filters on Face Recognition Accuracy", "authors": ["Kagan Ozturk", "Louisa Conwill", "Jacob Gutierrez", "Kevin Bowyer", "Walter J. Scheirer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17729v1", "summary": "Facial filters are now commonplace for social media users around the world.\nPrevious work has demonstrated that facial filters can negatively impact\nautomated face recognition performance. However, these studies focus on small\nnumbers of hand-picked filters in particular styles. In order to more\neffectively incorporate the wide ranges of filters present on various social\nmedia applications, we introduce a framework that allows for larger-scale study\nof the impact of facial filters on automated recognition. This framework\nincludes a controlled dataset of face images, a principled filter selection\nprocess that selects a representative range of filters for experimentation, and\na set of experiments to evaluate the filters' impact on recognition. We\ndemonstrate our framework with a case study of filters from the American\napplications Instagram and Snapchat and the Chinese applications Meitu and Pitu\nto uncover cross-cultural differences. Finally, we show how the filtering\neffect in a face embedding space can easily be detected and restored to improve\nface recognition performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17729v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "面部滤镜对人脸识别准确性影响的综合评估框架", "tldr": "本文提出了一个综合评估框架，用于大规模研究面部滤镜对人脸识别准确性的影响，揭示了跨文化差异，并展示了如何检测和恢复滤镜效果以提高识别性能。", "motivation": "先前的研究表明面部滤镜会对自动化人脸识别性能产生负面影响，但这些研究仅限于少数特定风格的手动选择滤镜，未能有效涵盖社交媒体应用中存在的广泛滤镜类型。因此，需要一个更全面的框架来大规模研究其影响。", "method": "本文引入了一个综合评估框架，用于大规模研究面部滤镜对自动化人脸识别的影响。该框架包括一个受控人脸图像数据集、一个原则性的滤镜选择过程（用于选择具有代表性的滤镜范围进行实验），以及一套评估滤镜对识别影响的实验。通过对来自美国应用Instagram和Snapchat以及中国应用美图和P图的滤镜进行案例研究，展示了该框架的应用。", "result": "研究展示了该框架能够揭示面部滤镜影响的跨文化差异。此外，还展示了人脸嵌入空间中的滤镜效应可以很容易地被检测和恢复，从而提高人脸识别性能。", "conclusion": "所提出的综合评估框架能够对人脸滤镜的影响进行大规模研究，揭示了跨文化差异，并提供了一种通过检测和恢复人脸嵌入空间中的滤镜效应来提高人脸识别性能的方法。", "translation": "面部滤镜如今已在全球社交媒体用户中普及。先前的研究表明，面部滤镜会对自动化人脸识别性能产生负面影响。然而，这些研究侧重于少数特定风格的手动选择滤镜。为了更有效地纳入各种社交媒体应用中存在的广泛滤镜，我们引入了一个框架，允许对人脸滤镜对自动化识别的影响进行更大规模的研究。该框架包括一个受控的人脸图像数据集、一个原则性的滤镜选择过程（用于选择具有代表性的滤镜范围进行实验），以及一套评估滤镜对识别影响的实验。我们通过对来自美国应用Instagram和Snapchat以及中国应用美图和P图的滤镜进行案例研究，展示了我们的框架，以揭示跨文化差异。最后，我们展示了人脸嵌入空间中的滤镜效应如何能够轻易地被检测和恢复，以提高人脸识别性能。", "summary": "本文提出了一个综合评估框架，用于研究各种面部滤镜对自动化人脸识别准确性的影响。该框架包含一个受控数据集、一个原则性的滤镜选择过程和一系列实验。通过对来自美国和中国流行社交媒体应用的滤镜进行案例研究，证明了其在揭示跨文化差异方面的有效性。此外，研究还表明，人脸嵌入空间中的滤镜效应可以被检测和恢复，从而提高人脸识别性能。", "keywords": "面部滤镜, 人脸识别, 评估框架, 跨文化, 滤镜恢复", "comments": "该论文的创新之处在于提供了一个系统化、大规模的框架来评估面部滤镜的影响，超越了以往研究的局限性。其重要性在于解决了由于滤镜广泛使用而给人脸识别系统带来的日益增长的现实挑战。能够检测和恢复被滤镜处理过的嵌入向量是解决实际问题的重大贡献。"}}
{"id": "2507.12006", "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12006v2", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12006v2", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "用于密集预测的频率动态注意力调制", "tldr": "本文提出频率动态注意力调制（FDAM）以解决Vision Transformers中注意力机制导致的频率消失问题，通过注意力反演和频率动态缩放来保留细节，并在多项任务上实现性能提升。", "motivation": "Vision Transformers (ViTs) 中的注意力机制使其每层都充当低通滤波器，导致堆叠层架构出现频率消失问题，从而丢失关键细节和纹理信息。", "method": "本文提出一种受电路理论启发的频率动态注意力调制（FDAM）策略，可即插即用于ViTs。FDAM通过注意力反演（AttInv）和频率动态缩放（FreqScale）两种技术直接调制ViTs的整体频率响应。AttInv通过反转注意力矩阵中的低通滤波器生成互补的高通滤波，并动态结合两者。FreqScale用于加权不同频率分量，以对目标响应函数进行细粒度调整。", "result": "FDAM避免了表征崩溃，并在SegFormer、DeiT和MaskDINO等多种模型上实现了持续的性能提升，这在语义分割、目标检测和实例分割等任务中表现明显。此外，该方法在遥感检测的单尺度设置中取得了最先进的结果。", "conclusion": "本文提出的频率动态注意力调制（FDAM）通过解决ViTs中的频率消失问题，有效保留了关键细节，并在多种视觉任务上取得了显著的性能提升，证明了其在密集预测领域的有效性。", "translation": "Vision Transformers (ViTs) 在计算机视觉领域取得了显著进展，在各种任务中表现出强大性能。然而，ViTs 中的注意力机制使每层都充当低通滤波器，现有 Transformer 的堆叠层架构存在频率消失问题。这导致关键细节和纹理的丢失。我们提出了一种新颖的、受电路理论启发的策略，称为频率动态注意力调制（FDAM），它可以轻松地即插即用于 ViTs。FDAM 直接调制 ViTs 的整体频率响应，由两种技术组成：注意力反演（AttInv）和频率动态缩放（FreqScale）。由于电路理论使用低通滤波器作为基本元件，我们引入了 AttInv，这是一种通过反转注意力矩阵中的低通滤波器来生成互补高通滤波并动态结合两者的方​​法。我们进一步设计了 FreqScale 来加权不同的频率分量，以对目标响应函数进行细粒度调整。通过特征相似性分析和有效秩评估，我们证明我们的方法避免了表示崩溃，从而在包括 SegFormer、DeiT 和 MaskDINO 在内的各种模型上实现了一致的性能改进。这些改进在语义分割、目标检测和实例分割等任务中表现明显。此外，我们将我们的方法应用于遥感检测，在单尺度设置中取得了最先进的结果。代码可在 https://github.com/Linwei-Chen/FDAM 获取。", "summary": "该论文提出了一种名为频率动态注意力调制（FDAM）的新方法，旨在解决Vision Transformers（ViTs）中因注意力机制导致的频率消失问题，该问题会导致关键细节丢失。FDAM受电路理论启发，包含注意力反演（AttInv）和频率动态缩放（FreqScale）两个核心技术，通过调节ViTs的频率响应来保留高频信息。实验结果表明，FDAM能有效避免表征崩溃，并在语义分割、目标检测和实例分割等多种密集预测任务上，对SegFormer、DeiT和MaskDINO等模型带来持续的性能提升，尤其在遥感检测中达到了SOTA水平。", "keywords": "频率动态注意力调制, Vision Transformers, 频率消失, 密集预测, 注意力反演", "comments": "该论文的创新点在于将电路理论中的频率响应概念引入到Vision Transformers的注意力机制中，通过设计AttInv和FreqScale两种巧妙的技术，有效解决了ViTs中高频信息丢失的问题。这种从信号处理角度优化深度学习模型的方法具有很强的启发性，对于提升ViTs在需要精细细节的任务（如密集预测）上的表现具有重要意义。其即插即用的特性也增加了其实用性。"}}
{"id": "2504.05018", "title": "Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning", "authors": ["Bibek Poudel", "Xuan Wang", "Weizi Li", "Lei Zhu", "Kevin Heaslip"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2504.05018v2", "summary": "Reinforcement learning (RL) holds significant promise for adaptive traffic\nsignal control. While existing RL-based methods demonstrate effectiveness in\nreducing vehicular congestion, their predominant focus on vehicle-centric\noptimization leaves pedestrian mobility needs and safety challenges\nunaddressed. In this paper, we present a deep RL framework for adaptive control\nof eight traffic signals along a real-world urban corridor, jointly optimizing\nboth pedestrian and vehicular efficiency. Our single-agent policy is trained\nusing real-world pedestrian and vehicle demand data derived from Wi-Fi logs and\nvideo analysis. The results demonstrate significant performance improvements\nover traditional fixed-time signals, reducing average wait times per pedestrian\nand per vehicle by up to 67% and 52% respectively, while simultaneously\ndecreasing total wait times for both groups by up to 67% and 53%. Additionally,\nour results demonstrate generalization capabilities across varying traffic\ndemands, including conditions entirely unseen during training, validating RL's\npotential for developing transportation systems that serve all road users.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2504.05018v2", "cate": "cs.LG", "date": "2025-04-07", "updated": "2025-07-23", "AI": {"title_translation": "使用强化学习在城市环境中联合优化行人和车辆交通", "tldr": "本研究提出了一种深度强化学习框架，用于城市交通信号的自适应控制，旨在同时优化行人和车辆的效率和安全，并在真实世界数据上实现了显著的等待时间减少和泛化能力。", "motivation": "现有基于强化学习的交通信号控制方法主要关注车辆优化，忽视了行人的出行需求和安全挑战。本研究旨在解决这一不足，实现行人和车辆交通的联合优化。", "method": "本研究提出了一个深度强化学习框架，用于控制真实城市走廊上的八个交通信号。该框架采用单智能体策略，并利用来自Wi-Fi日志和视频分析的真实世界行人和车辆需求数据进行训练。", "result": "与传统固定时间信号相比，该方法将每位行人和每辆车的平均等待时间分别减少了高达67%和52%，同时将两者的总等待时间分别减少了高达67%和53%。此外，该方法在不同交通需求（包括训练期间未见的条件）下表现出泛化能力。", "conclusion": "强化学习在开发服务于所有道路使用者的交通系统方面具有巨大潜力，本研究通过联合优化行人和车辆交通，证明了其在实际城市环境中的有效性和泛化能力。", "translation": "强化学习（RL）在自适应交通信号控制方面具有重要前景。尽管现有基于RL的方法在减少车辆拥堵方面表现出有效性，但它们主要侧重于以车辆为中心的优化，忽视了行人的出行需求和安全挑战。在本文中，我们提出了一个深度RL框架，用于控制真实城市走廊上的八个交通信号，共同优化行人和车辆的效率。我们的单智能体策略使用来自Wi-Fi日志和视频分析的真实世界行人和车辆需求数据进行训练。结果表明，与传统固定时间信号相比，性能显著提高，每位行人和每辆车的平均等待时间分别减少了高达67%和52%，同时两者的总等待时间分别减少了高达67%和53%。此外，我们的结果表明在不同交通需求下（包括训练期间完全未见的条件）具有泛化能力，验证了RL在开发服务于所有道路使用者的交通系统方面的潜力。", "summary": "本论文提出了一种深度强化学习框架，用于城市交通信号的自适应控制，旨在同时优化行人和车辆的效率和安全。该框架在真实城市走廊的八个交通信号上进行了测试，并使用真实世界数据进行训练。实验结果显示，与传统方法相比，行人和车辆的平均等待时间显著减少，并且该模型在不同交通需求下表现出良好的泛化能力，证明了强化学习在综合交通系统优化方面的潜力。", "keywords": "强化学习, 交通优化, 行人交通, 车辆交通, 交通信号控制", "comments": "该论文的创新点在于将强化学习应用于城市交通信号控制时，首次将行人和车辆的优化目标进行联合考虑，而非仅关注车辆。这对于构建更公平、更安全的城市交通系统具有重要意义。其使用真实世界数据进行训练和验证，并展示出在未见交通条件下的泛化能力，进一步增强了研究的实际应用价值和鲁棒性。"}}
{"id": "2501.02851", "title": "Exact Matching in Correlated Networks with Node Attributes for Improved Community Recovery", "authors": ["Joonhyuk Yang", "Hye Won Chung"], "categories": ["cs.SI", "cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      To appear at IEEE Transactions on Information Theory, 33 pages, 3 figures", "url": "http://arxiv.org/abs/2501.02851v2", "summary": "We study community detection in multiple networks with jointly correlated\nnode attributes and edges. This setting arises naturally in applications such\nas social platforms, where a shared set of users may exhibit both correlated\nfriendship patterns and correlated attributes across different platforms.\nExtending the classical Stochastic Block Model (SBM) and its contextual\ncounterpart (Contextual SBM or CSBM), we introduce the correlated CSBM, which\nincorporates structural and attribute correlations across graphs. To build\nintuition, we first analyze correlated Gaussian Mixture Models, wherein only\ncorrelated node attributes are available without edges, and identify the\nconditions under which an estimator minimizing the distance between attributes\nachieves exact matching of nodes across the two databases. For the correlated\nCSBMs, we develop a two-step procedure that first applies $k$-core matching to\nmost nodes using edge information, then refines the matching for the remaining\nunmatched nodes by leveraging their attributes with a distance-based estimator.\nWe identify the conditions under which the algorithm recovers the exact node\ncorrespondence, enabling us to merge the correlated edges and average the\ncorrelated attributes for enhanced community detection. Crucially, by aligning\nand combining graphs, we identify regimes in which community detection is\nimpossible in a single graph but becomes feasible when side information from\ncorrelated graphs is incorporated. Our results illustrate how the interplay\nbetween graph matching and community recovery can boost performance, broadening\nthe scope of multi-graph, attribute-based community detection.", "comment": "To appear at IEEE Transactions on Information Theory, 33 pages, 3\n  figures", "pdf_url": "http://arxiv.org/pdf/2501.02851v2", "cate": "cs.SI", "date": "2025-01-06", "updated": "2025-07-23", "AI": {"title_translation": "具有节点属性的相关网络中的精确匹配以改进社区恢复", "tldr": "提出了一种在具有相关节点属性和边的多个网络中进行社区检测的方法，通过精确匹配节点来提高性能，即使在单个网络中不可能检测到社区的情况下也能实现。", "motivation": "在社交平台等应用中，用户在不同平台上的友谊模式和属性都可能存在关联。现有的社区检测方法可能无法充分利用这些跨图结构和属性的相关性，尤其是在单个图信息不足以进行社区检测的情况下。", "method": "1. 引入了相关上下文随机块模型（correlated CSBM），它结合了图之间的结构和属性相关性。2. 首先分析了相关高斯混合模型，以理解在仅有属性的情况下如何实现节点精确匹配。3. 对于相关CSBMs，开发了一个两步程序：首先使用边信息对大多数节点应用k-core匹配，然后利用距离估计器，根据属性细化剩余未匹配节点的匹配。", "result": "1. 确定了算法恢复精确节点对应关系的条件。2. 通过对齐和组合图，识别出在单个图中社区检测不可能但通过结合来自相关图的侧面信息变得可行的区域。3. 结果表明，图匹配和社区恢复之间的相互作用可以提升性能，拓宽了多图、基于属性的社区检测的范围。", "conclusion": "通过在具有节点属性的相关网络中实现精确匹配，本研究证明了结合来自相关图的侧面信息可以显著增强社区检测的性能，甚至在单个图信息不足的情况下也能实现检测。", "translation": "我们研究在具有联合相关节点属性和边的多个网络中的社区检测。这种设置自然地出现在社交平台等应用中，其中一组共享用户可能在不同平台之间表现出相关的友谊模式和相关的属性。通过扩展经典的随机块模型（SBM）及其上下文对应模型（上下文SBM或CSBM），我们引入了相关CSBM，它整合了跨图的结构和属性相关性。为了建立直觉，我们首先分析了相关高斯混合模型，其中只有相关节点属性而没有边，并确定了最小化属性之间距离的估计器在两个数据库之间实现节点精确匹配的条件。对于相关CSBMs，我们开发了一个两步程序：首先使用边信息对大多数节点应用k-core匹配，然后利用其属性和基于距离的估计器对剩余未匹配节点进行匹配细化。我们确定了算法恢复精确节点对应关系的条件，这使我们能够合并相关边并平均相关属性以增强社区检测。关键的是，通过对齐和组合图，我们识别出在单个图中社区检测不可能，但当结合来自相关图的侧面信息时变得可行的区域。我们的结果说明了图匹配和社区恢复之间的相互作用如何提升性能，拓宽了多图、基于属性的社区检测的范围。", "summary": "本文研究了在具有相关节点属性和边的多个网络中的社区检测问题，该问题常见于社交平台等应用。通过扩展随机块模型，作者引入了相关上下文随机块模型（correlated CSBM），该模型整合了图间的结构和属性相关性。文章首先分析了相关高斯混合模型以建立直觉，然后提出了一种两步匹配程序：首先利用边信息进行k-core匹配，然后利用属性对未匹配节点进行精细化匹配。研究确定了算法实现精确节点对应关系的条件，并证明了通过对齐和组合图，可以在单个图无法检测社区的情况下，通过整合侧面信息实现社区检测。这表明图匹配和社区恢复的协同作用可以显著提升多图、基于属性的社区检测性能。", "keywords": "相关网络, 社区检测, 节点属性, 精确匹配, 上下文随机块模型", "comments": "这项研究通过引入相关CSBM并开发两步匹配过程，有效地解决了多网络环境下社区检测的挑战。其创新之处在于利用图间相关性，特别是节点属性，来实现精确匹配，从而在传统方法失效的场景下也能进行社区检测。这对于处理复杂真实世界数据（如社交网络）具有重要意义。"}}
{"id": "2507.17665", "title": "Perspective-Invariant 3D Object Detection", "authors": ["Ao Liang", "Lingdong Kong", "Dongyue Lu", "Youquan Liu", "Jian Fang", "Huaici Zhao", "Wei Tsang Ooi"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at this https URL", "url": "http://arxiv.org/abs/2507.17665v1", "summary": "With the rise of robotics, LiDAR-based 3D object detection has garnered\nsignificant attention in both academia and industry. However, existing datasets\nand methods predominantly focus on vehicle-mounted platforms, leaving other\nautonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,\nthe first benchmark featuring LiDAR data and 3D bounding box annotations\ncollected from multiple platforms: vehicle, quadruped, and drone, thereby\nfacilitating research in 3D object detection for non-vehicle platforms as well\nas cross-platform 3D detection. Based on Pi3DET, we propose a novel\ncross-platform adaptation framework that transfers knowledge from the\nwell-studied vehicle platform to other platforms. This framework achieves\nperspective-invariant 3D detection through robust alignment at both geometric\nand feature levels. Additionally, we establish a benchmark to evaluate the\nresilience and robustness of current 3D detectors in cross-platform scenarios,\nproviding valuable insights for developing adaptive 3D perception systems.\nExtensive experiments validate the effectiveness of our approach on challenging\ncross-platform tasks, demonstrating substantial gains over existing adaptation\nmethods. We hope this work paves the way for generalizable and unified 3D\nperception systems across diverse and complex environments. Our Pi3DET dataset,\ncross-platform benchmark suite, and annotation toolkit have been made publicly\navailable.", "comment": "ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at\n  https://pi3det.github.io", "pdf_url": "http://arxiv.org/pdf/2507.17665v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "透视不变的3D目标检测", "tldr": "引入Pi3DET，一个多平台3D检测基准和跨平台适应框架，用于实现车辆以外的鲁棒3D目标检测。", "motivation": "现有的激光雷达3D目标检测数据集和方法主要集中于车载平台，导致其他自主平台（如四足机器人和无人机）的研究不足，限制了3D检测技术的通用性。", "method": "本文引入了Pi3DET，这是第一个包含来自车辆、四足机器人和无人机等多平台激光雷达数据和3D边界框标注的基准。基于Pi3DET，提出了一种新颖的跨平台适应框架，该框架将知识从研究充分的车辆平台转移到其他平台，并通过在几何和特征层面进行鲁棒对齐，实现透视不变的3D检测。此外，还建立了一个基准来评估当前3D检测器在跨平台场景中的弹性和鲁棒性。", "result": "大量实验验证了所提出的方法在具有挑战性的跨平台任务上的有效性，并显示出比现有适应方法显著的提升。Pi3DET数据集、跨平台基准套件和标注工具已公开，有助于未来的研究。", "conclusion": "这项工作为在多样化和复杂环境中实现通用和统一的3D感知系统铺平了道路。", "translation": "随着机器人技术的兴起，基于激光雷达的3D目标检测在学术界和工业界都受到了广泛关注。然而，现有数据集和方法主要集中于车载平台，而其他自主平台的研究不足。为了弥补这一空白，我们引入了Pi3DET，这是第一个包含从多个平台（车辆、四足机器人和无人机）收集的激光雷达数据和3D边界框标注的基准，从而促进了非车辆平台以及跨平台3D检测的研究。基于Pi3DET，我们提出了一种新颖的跨平台适应框架，该框架将知识从研究充分的车辆平台转移到其他平台。该框架通过在几何和特征层面进行鲁棒对齐，实现了透视不变的3D检测。此外，我们建立了一个基准来评估当前3D检测器在跨平台场景中的弹性和鲁棒性，为开发自适应3D感知系统提供了宝贵的见解。大量实验验证了我们方法在具有挑战性的跨平台任务上的有效性，并显示出比现有适应方法显著的提升。我们希望这项工作能为在多样化和复杂环境中实现通用和统一的3D感知系统铺平道路。我们的Pi3DET数据集、跨平台基准套件和标注工具已公开。", "summary": "针对现有3D目标检测主要集中于车载平台的问题，本文引入了Pi3DET数据集，该数据集首次包含来自车辆、四足机器人和无人机等多平台的激光雷达数据。在此基础上，提出了一种新颖的跨平台适应框架，通过几何和特征层面的对齐，实现了透视不变的3D检测。研究还建立了一个基准来评估现有检测器在跨平台场景中的鲁棒性。实验证明了该方法在跨平台任务上的有效性，并优于现有方法。该工作旨在推动通用和统一的3D感知系统发展。", "keywords": "3D目标检测, 跨平台, 激光雷达, 透视不变, 数据集", "comments": "本文的创新点在于首次提出了一个包含多平台（车辆、四足机器人、无人机）激光雷达数据的3D目标检测基准Pi3DET，并针对性地提出了跨平台适应框架，解决了现有研究过度依赖车载平台的局限性。这种透视不变的检测方法对于机器人领域中更广泛的自主平台具有重要意义，有助于推动3D感知系统的泛化能力和鲁棒性。"}}
{"id": "2507.17436", "title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection", "authors": ["Yehao Lu", "Minghe Weng", "Zekang Xiao", "Rui Jiang", "Wei Su", "Guangcong Zheng", "Ping Lu", "Xi Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.17436v1", "summary": "The Mixture of Experts (MoE) architecture has excelled in Large\nVision-Language Models (LVLMs), yet its potential in real-time open-vocabulary\nobject detectors, which also leverage large-scale vision-language datasets but\nsmaller models, remains unexplored. This work investigates this domain,\nrevealing intriguing insights. In the shallow layers, experts tend to cooperate\nwith diverse peers to expand the search space. While in the deeper layers,\nfixed collaborative structures emerge, where each expert maintains 2-3 fixed\npartners and distinct expert combinations are specialized in processing\nspecific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding\nDINO 1.5 Edge from a dense model to a dynamic inference framework via an\nefficient MoE-Tuning strategy. Additionally, we design a granularity\ndecomposition mechanism to decompose the Feed-Forward Network (FFN) of base\nmodel into multiple smaller expert networks, expanding the subnet search space.\nTo prevent performance degradation at the start of fine-tuning, we further\npropose a pre-trained weight allocation strategy for the experts, coupled with\na specific router initialization. During inference, only the input-relevant\nexperts are activated to form a compact subnet. Experiments show that,\npretrained with merely 1.56M open-source data, Dynamic-DINO outperforms\nGrounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17436v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Dynamic-DINO：面向实时开放词汇目标检测的细粒度专家混合微调", "tldr": "本文提出了Dynamic-DINO，一个基于MoE的实时开放词汇目标检测器，它通过专家混合微调和粒度分解机制，在更少数据下超越了现有模型。", "motivation": "MoE架构在大型视觉语言模型中表现出色，但在利用大型视觉语言数据集但模型较小的实时开放词汇目标检测器中的潜力尚未被探索。本文旨在填补这一空白，并揭示MoE在该领域的内在机制。", "method": "本文提出了Dynamic-DINO，通过高效的MoE-Tuning策略将Grounding DINO 1.5 Edge从密集模型扩展到动态推理框架。设计了粒度分解机制，将FFN分解为多个更小的专家网络，以扩展子网搜索空间。为了防止微调初期性能下降，提出了预训练权重分配策略和特定的路由器初始化。推理时，只激活与输入相关的专家形成紧凑子网。", "result": "实验表明，Dynamic-DINO仅使用1.56M的开源数据进行预训练，就超越了使用私有Grounding20M数据集预训练的Grounding DINO 1.5 Edge。", "conclusion": "本文成功将MoE架构应用于实时开放词汇目标检测，并通过Dynamic-DINO展示了其有效性，证明了在更少数据下实现高性能的可能性。", "translation": "专家混合（MoE）架构在大型视觉语言模型（LVLM）中表现出色，但其在实时开放词汇目标检测器中的潜力仍未被探索，尽管后者也利用大规模视觉语言数据集但模型较小。这项工作研究了这一领域，揭示了有趣的见解。在浅层，专家倾向于与不同的同行合作以扩展搜索空间。而在深层，出现了固定的协作结构，每个专家保持2-3个固定伙伴，并且不同的专家组合专门用于处理特定模式。具体来说，我们提出了Dynamic-DINO，它通过高效的MoE-Tuning策略将Grounding DINO 1.5 Edge从密集模型扩展到动态推理框架。此外，我们设计了一种粒度分解机制，将基础模型的Feed-Forward Network (FFN) 分解为多个更小的专家网络，从而扩展了子网搜索空间。为了防止微调开始时性能下降，我们进一步提出了一种为专家分配预训练权重的策略，并结合了特定的路由器初始化。在推理过程中，只有与输入相关的专家被激活以形成一个紧凑的子网。实验表明，Dynamic-DINO仅用1.56M的开源数据进行预训练，就超越了在私有Grounding20M数据集上预训练的Grounding DINO 1.5 Edge。", "summary": "本文探索了专家混合（MoE）架构在实时开放词汇目标检测器中的应用潜力。研究发现MoE在模型不同层级展现出不同的协作模式。在此基础上，作者提出了Dynamic-DINO，一个基于Grounding DINO 1.5 Edge的动态推理框架，通过高效的MoE-Tuning策略、粒度分解机制以及预训练权重分配策略来提升性能。实验证明，Dynamic-DINO在仅使用少量开源数据的情况下，其性能超越了使用大规模私有数据集训练的现有模型，展示了其在资源受限场景下的优势。", "keywords": "专家混合, 开放词汇目标检测, 实时检测, Dynamic-DINO, 微调", "comments": "本文的创新点在于首次将MoE架构引入实时开放词汇目标检测领域，并揭示了MoE在模型不同层级的协作行为。其提出的Dynamic-DINO通过细粒度的专家混合微调、粒度分解和权重分配策略，在显著减少训练数据量的情况下实现了超越SOTA的性能，这对于资源受限或需要快速部署的应用具有重要意义。该研究为未来轻量级、高性能的视觉-语言模型设计提供了新的思路。"}}
{"id": "2410.20682", "title": "SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script", "authors": ["Eunwon Kim", "Chanho Park", "Buru Chang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.20682v3", "summary": "Shared memories between two individuals strengthen their bond and are crucial\nfor facilitating their ongoing conversations. This study aims to make long-term\ndialogue more engaging by leveraging these shared memories. To this end, we\nintroduce a new long-term dialogue dataset named SHARE, constructed from movie\nscripts, which are a rich source of shared memories among various\nrelationships. Our dialogue dataset contains the summaries of persona\ninformation and events of two individuals, as explicitly revealed in their\nconversation, along with implicitly extractable shared memories. We also\nintroduce EPISODE, a long-term dialogue framework based on SHARE that utilizes\nshared experiences between individuals. Through experiments using SHARE, we\ndemonstrate that shared memories between two individuals make long-term\ndialogues more engaging and sustainable, and that EPISODE effectively manages\nshared memories during dialogue. Our dataset and code are available at\nhttps://github.com/e1kim/SHARE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.20682v3", "cate": "cs.CL", "date": "2024-10-28", "updated": "2025-07-23", "AI": {"title_translation": "SHARE：基于电影剧本构建的共享记忆感知开放域长期对话数据集", "tldr": "本研究引入了一个名为SHARE的长期对话数据集，该数据集从电影剧本中构建，旨在通过利用个体间的共享记忆来增强长期对话的吸引力。研究还提出了一个名为EPISODE的对话框架，并实验证明共享记忆能使长期对话更具吸引力和可持续性，EPISODE能有效管理共享记忆。", "motivation": "个体间的共享记忆能够加强彼此的联系并促进持续对话。本研究旨在通过利用这些共享记忆，使长期对话更具吸引力。", "method": "研究引入了一个名为SHARE的新型长期对话数据集，该数据集从电影剧本中构建，包含人物信息和事件的摘要，以及可隐式提取的共享记忆。同时，研究还提出了一个基于SHARE的长期对话框架EPISODE，该框架利用个体间的共享经验。", "result": "通过使用SHARE数据集进行的实验表明，个体间的共享记忆能够使长期对话更具吸引力和可持续性，并且EPISODE框架能够有效地管理对话中的共享记忆。", "conclusion": "共享记忆对于提升长期对话的吸引力和可持续性至关重要，而SHARE数据集和EPISODE框架为实现这一目标提供了有效的方法和工具。", "translation": "个体间的共享记忆能够加强彼此的联系，对于促进持续对话至关重要。本研究旨在通过利用这些共享记忆，使长期对话更具吸引力。为此，我们引入了一个名为SHARE的新型长期对话数据集，该数据集从电影剧本中构建，电影剧本是各种关系中共享记忆的丰富来源。我们的对话数据集包含对话中明确揭示的两个人的人物信息和事件摘要，以及可隐式提取的共享记忆。我们还引入了EPISODE，一个基于SHARE的长期对话框架，它利用个体间的共享经验。通过使用SHARE进行的实验，我们证明了两个人之间的共享记忆使长期对话更具吸引力和可持续性，并且EPISODE能够有效地管理对话中的共享记忆。我们的数据集和代码可在https://github.com/e1kim/SHARE获取。", "summary": "本研究旨在通过利用共享记忆来增强长期对话的吸引力。为此，研究团队构建了一个名为SHARE的长期对话数据集，该数据集来源于电影剧本，包含了对话中显式的人物和事件信息以及隐式的共享记忆。此外，研究还提出了一个名为EPISODE的长期对话框架，该框架基于SHARE并利用个体间的共享经验。实验结果表明，共享记忆能够显著提升长期对话的吸引力和可持续性，且EPISODE框架在对话中有效管理了共享记忆。", "keywords": "长期对话, 共享记忆, 数据集, 电影剧本, EPISODE", "comments": "这项研究通过构建一个独特的、基于电影剧本的长期对话数据集SHARE，并在其中融入了共享记忆的概念，为提升对话系统的交互深度和真实感提供了创新思路。其创新点在于将电影剧本作为共享记忆的来源，这为对话系统提供了丰富且自然的情境记忆。同时，提出的EPISODE框架进一步验证了共享记忆在长期对话中的重要性及有效管理方式。这项工作对于推动开放域对话系统向更具情感和连贯性的方向发展具有重要意义。"}}
{"id": "2412.16867", "title": "A Parameter-Efficient Quantum Anomaly Detection Method on a Superconducting Quantum Processor", "authors": ["Maida Wang", "Jinyang Jiang", "Peter V. Coveney"], "categories": ["quant-ph", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      23 pages, 10 figures", "url": "http://arxiv.org/abs/2412.16867v4", "summary": "Quantum machine learning has gained attention for its potential to address\ncomputational challenges. However, whether those algorithms can effectively\nsolve practical problems and outperform their classical counterparts,\nespecially on current quantum hardware, remains a critical question. In this\nwork, we propose a novel quantum machine learning method, called\nParameter-Efficient Quantum Anomaly Detection (PEQAD), for practical image\nanomaly detection, which aims to achieve both parameter efficiency and superior\naccuracy compared to classical models. Emulation results indicate that PEQAD\ndemonstrates favourable recognition capabilities compared to classical\nbaselines, achieving an average accuracy of over 90% on benchmarks with\nsignificantly fewer trainable parameters. Theoretical analysis confirms that\nPEQAD has a comparable expressivity to classical counterparts while requiring\nonly a fraction of the parameters. Furthermore, we demonstrate the first\nimplementation of a quantum anomaly detection method for general image datasets\non a superconducting quantum processor. Specifically, we achieve an accuracy of\nover 80% with only 16 parameters on the device, providing initial evidence of\nPEQAD's practical viability in the noisy intermediate-scale quantum era and\nhighlighting its significant reduction in parameter requirements.", "comment": "23 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2412.16867v4", "cate": "quant-ph", "date": "2024-12-22", "updated": "2025-07-22", "AI": {"title_translation": "超导量子处理器上的一种参数高效量子异常检测方法", "tldr": "本文提出了一种名为PEQAD的参数高效量子异常检测方法，并在超导量子处理器上实现了通用图像异常检测，在保持高准确率的同时显著减少了参数需求，验证了其在噪声中等规模量子时代的实用性。", "motivation": "量子机器学习在解决计算挑战方面具有潜力，但其在当前量子硬件上能否有效解决实际问题并超越经典算法仍是一个关键问题。", "method": "提出了一种名为“参数高效量子异常检测 (PEQAD)”的新型量子机器学习方法，用于实际图像异常检测，旨在实现参数高效和优于经典模型的准确性。", "result": "仿真结果表明PEQAD在基准测试中平均准确率超过90%，且可训练参数显著减少。理论分析证实PEQAD具有与经典方法相当的表达能力，但参数量仅为一小部分。在超导量子处理器上首次实现了通用图像数据集的量子异常检测方法，仅用16个参数就达到了超过80%的准确率。", "conclusion": "PEQAD在噪声中等规模量子时代具有实际可行性，并显著降低了参数需求。", "translation": "量子机器学习因其解决计算挑战的潜力而受到关注。然而，这些算法能否有效解决实际问题并超越其经典对应算法，尤其是在当前的量子硬件上，仍然是一个关键问题。在这项工作中，我们提出了一种新颖的量子机器学习方法，称为参数高效量子异常检测（PEQAD），用于实际图像异常检测，旨在实现参数效率和优于经典模型的准确性。仿真结果表明，与经典基线相比，PEQAD表现出良好的识别能力，在基准测试中平均准确率超过90%，且可训练参数显著减少。理论分析证实，PEQAD具有与经典对应算法相当的表达能力，同时仅需要一小部分参数。此外，我们展示了在超导量子处理器上首次实现通用图像数据集的量子异常检测方法。具体来说，我们在设备上仅用16个参数就达到了超过80%的准确率，为PEQAD在噪声中等规模量子时代的实际可行性提供了初步证据，并突出了其在参数需求方面的显著降低。", "summary": "本文提出了一种名为PEQAD的参数高效量子异常检测方法，旨在解决量子机器学习在实际应用中参数效率和性能的挑战。该方法在图像异常检测任务中，通过仿真和超导量子处理器上的实验，证明了其在显著减少参数量的情况下，仍能达到与经典模型相当甚至更优的准确率，展现了其在噪声中等规模量子时代的实用潜力。", "keywords": "量子异常检测, 参数高效, 超导量子处理器, 量子机器学习, 图像异常检测", "comments": "该论文的创新点在于提出了参数高效的量子异常检测方法，并首次在超导量子处理器上实现了通用图像数据集的量子异常检测，这对于推动量子机器学习在实际硬件上的应用具有重要意义。其强调的参数效率对于克服当前量子硬件的限制（如噪声和有限的量子比特）尤为关键。"}}
{"id": "2507.16865", "title": "ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Huiru Zheng", "Lingxiang Zheng", "Yu Yang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16865v1", "summary": "Inertial Measurement Unit (IMU) has become a key technology for achieving\nlow-cost and precise positioning. However, traditional CNN-based inertial\npositioning methods struggle to capture the nonlinear motion characteristics\nand long-term dependencies in IMU data. To address this limitation, we propose\na novel inertial positioning network with a generic backbone called\nResChebyKAN, which leverages the nonlinear approximation capabilities of\nChebyshev polynomials to model complex motion patterns. Additionally, we\nintroduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively\ncapture contextual information and enhance long-term dependency modeling.\nExperimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,\nIMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory\nerror by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,\nwe release a preprocessed dataset and empirically show that removing the\ngravity component from acceleration data significantly improves inertial\npositioning performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16865v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "ResKACNNet: 一种用于惯性里程计的残差ChebyKAN网络", "tldr": "提出了一种名为ResChebyKAN的新型惯性定位网络，结合高效核自注意力模块，显著提升了惯性里程计的定位精度，并通过实验证明去除重力分量能进一步改善性能。", "motivation": "传统的基于CNN的惯性定位方法难以捕捉IMU数据的非线性运动特性和长期依赖性。", "method": "提出了一种名为ResChebyKAN的新型惯性定位网络，利用切比雪夫多项式的非线性逼近能力来建模复杂运动模式。此外，引入了高效核自注意力（EKSA）模块以有效捕获上下文信息并增强长期依赖性建模。", "result": "在多个公共数据集（如RIDI、RoNIN、RNIN-VIO、OxIOD、IMUNet和TLIO）上，该方法与现有基准方法相比，将绝对轨迹误差降低了3.79%至42.32%。同时，经验性地表明，从加速度数据中去除重力分量能显著提高惯性定位性能。", "conclusion": "ResKACNNet通过结合ResChebyKAN骨干网络和EKSA模块，有效解决了传统方法的局限性，显著提升了惯性里程计的定位精度，并且去除重力分量是提升性能的有效策略。", "translation": "惯性测量单元（IMU）已成为实现低成本和精确定位的关键技术。然而，传统的基于CNN的惯性定位方法难以捕捉IMU数据的非线性运动特性和长期依赖性。为了解决这一局限性，我们提出了一种新型的惯性定位网络，其通用骨干网络名为ResChebyKAN，该网络利用切比雪夫多项式的非线性逼近能力来建模复杂的运动模式。此外，我们引入了一种高效核自注意力（EKSA）模块，以有效捕获上下文信息并增强长期依赖性建模。在公共数据集（例如RIDI、RoNIN、RNIN-VIO、OxIOD、IMUNet和TLIO）上的实验结果表明，与现有基准方法相比，我们的方法将绝对轨迹误差降低了3.79%至42.32%。此外，我们发布了一个预处理数据集，并经验性地表明，从加速度数据中去除重力分量能显著提高惯性定位性能。", "summary": "本文提出了一种名为ResKACNNet的新型惯性定位网络，其核心是ResChebyKAN骨干网络，利用切比雪夫多项式处理非线性运动模式。为了增强长期依赖性建模，引入了高效核自注意力（EKSA）模块。实验结果表明，该方法在多个公共数据集上显著降低了绝对轨迹误差，与现有方法相比提升了3.79%至42.32%。研究还发现，从加速度数据中去除重力分量能进一步提高惯性定位性能。", "keywords": "惯性里程计, ChebyKAN网络, 残差网络, 自注意力, 非线性建模", "comments": "该论文通过引入基于切比雪夫多项式的ResChebyKAN网络和高效核自注意力模块，有效解决了惯性里程计中非线性运动特性和长期依赖性建模的挑战，展示了显著的性能提升。此外，发现去除加速度数据中的重力分量对性能有积极影响，为未来的研究提供了新的视角。"}}
{"id": "2507.17366", "title": "On Distributionally Robust Lossy Source Coding", "authors": ["Giuseppe Serra", "Photios A. Stavrou", "Marios Kountouris"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17366v1", "summary": "In this paper, we investigate the problem of distributionally robust source\ncoding, i.e., source coding under uncertainty in the source distribution,\ndiscussing both the coding and computational aspects of the problem. We propose\ntwo extensions of the so-called Strong Functional Representation Lemma (SFRL),\nconsidering the cases where, for a fixed conditional distribution, the marginal\ninducing the joint coupling belongs to either a finite set of distributions or\na Kullback-Leibler divergence sphere (KL-Sphere) centered at a fixed nominal\ndistribution. Using these extensions, we derive distributionally robust coding\nschemes for both the one-shot and asymptotic regimes, generalizing previous\nresults in the literature. Focusing on the case where the source distribution\nbelongs to a given KL-Sphere, we derive an implicit characterization of the\npoints attaining the robust rate-distortion function (R-RDF), which we later\nexploit to implement a novel algorithm for computing the R-RDF. Finally, we\ncharacterize the analytical expression of the R-RDF for Bernoulli sources,\nproviding a theoretical benchmark to evaluate the estimation performance of the\nproposed algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17366v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "关于分布鲁棒有损信源编码", "tldr": "本文研究了分布鲁棒有损信源编码问题，即源分布不确定性下的信源编码。它提出了强函数表示引理（SFRL）的两个扩展，推导了单次和渐近情况下的鲁棒编码方案，并针对属于KL-Sphere的源，推导了鲁棒率失真函数（R-RDF）的隐式特征，并利用其实现了一种计算R-RDF的新算法。最后，给出了伯努利源的R-RDF的解析表达式。", "motivation": "本文旨在研究分布鲁棒信源编码问题，即在信源分布不确定性下的信源编码，并讨论了该问题的编码和计算方面。", "method": "本文提出了强函数表示引理（SFRL）的两个扩展，考虑了在固定条件分布下，诱导联合耦合的边际分布属于有限分布集或以固定名义分布为中心的Kullback-Leibler散度球（KL-Sphere）的情况。利用这些扩展，推导了单次和渐近情况下的分布鲁棒编码方案。针对信源分布属于给定KL-Sphere的情况，推导了达到鲁棒率失真函数（R-RDF）的点的隐式特征，并利用其实现了一种计算R-RDF的新算法。最后，对伯努利源的R-RDF的解析表达式进行了表征。", "result": "1. 提出了强函数表示引理（SFRL）的两个扩展。2. 推导了单次和渐近情况下的分布鲁棒编码方案，推广了现有结果。3. 推导了达到鲁棒率失真函数（R-RDF）的点的隐式特征，用于信源分布属于KL-Sphere的情况。4. 实现了一种计算R-RDF的新算法。5. 表征了伯努利源的R-RDF的解析表达式，提供了评估所提出算法估计性能的理论基准。", "conclusion": "本文成功研究了分布鲁棒有损信源编码问题，通过扩展理论工具（SFRL），推导了鲁棒编码方案，并提供了一种计算鲁棒率失真函数（R-RDF）的算法，同时为伯努利源提供了理论基准。", "translation": "在本文中，我们研究了分布鲁棒信源编码问题，即在信源分布不确定性下的信源编码，讨论了该问题的编码和计算方面。我们提出了所谓的强函数表示引理（SFRL）的两个扩展，考虑了在固定条件分布下，诱导联合耦合的边际分布属于有限分布集或以固定名义分布为中心的Kullback-Leibler散度球（KL-Sphere）的情况。利用这些扩展，我们推导了单次和渐近情况下的分布鲁棒编码方案，推广了文献中的现有结果。针对信源分布属于给定KL-Sphere的情况，我们推导了达到鲁棒率失真函数（R-RDF）的点的隐式特征，我们随后利用其实现了一种计算R-RDF的新算法。最后，我们表征了伯努利信源的R-RDF的解析表达式，提供了一个理论基准来评估所提出算法的估计性能。", "summary": "本文研究了在信源分布不确定性下的分布鲁棒信源编码问题。文章提出了强函数表示引理的两个扩展，并基于此推导了单次和渐近情况下的鲁棒编码方案。对于信源分布属于Kullback-Leibler散度球的情况，文章推导了鲁棒率失真函数（R-RDF）的隐式特征，并据此实现了一种新的R-RDF计算算法。此外，文章还给出了伯努利信源R-RDF的解析表达式，为算法性能评估提供了理论基准。", "keywords": "分布鲁棒信源编码, 强函数表示引理, 率失真函数, Kullback-Leibler散度, 伯努利源", "comments": "本文的创新之处在于扩展了强函数表示引理（SFRL），推广了现有的鲁棒编码结果，并提供了一种计算鲁棒率失真函数（R-RDF）的具体算法，同时为伯努利源提供了分析基准，这对于实际评估至关重要。"}}
{"id": "2507.17216", "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul Röttger", "Dirk Hovy"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures", "url": "http://arxiv.org/abs/2507.17216v1", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "comment": "13 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.17216v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多元道德鸿沟：理解人类与大型语言模型之间的判断和价值观差异", "tldr": "研究发现大型语言模型（LLMs）在道德判断和价值观方面与人类存在“多元道德鸿沟”，尤其是在人类意见不一致时。作者提出了动态道德画像（DMP）方法，可显著提升LLMs与人类判断的对齐度并增加价值观多样性。", "motivation": "人们越来越依赖大型语言模型（LLMs）获取道德建议，这可能会影响人类的决策。然而，LLMs与人类道德判断的契合度知之甚少。", "method": "研究引入了“道德困境数据集”，这是一个包含1,618个真实世界道德困境的基准，并配有二元评估和自由文本理由形式的人类道德判断分布。将此问题视为多元分布对齐任务，比较LLM和人类判断在困境中的分布。并行使用从3,783个价值观表达中构建的60个价值观分类法。为弥合差距，引入了动态道德画像（DMP），这是一种基于狄利克雷采样的算法，根据人类衍生的价值观画像来调整模型输出。", "result": "研究发现模型仅在高度共识下才能再现人类判断；当人类分歧增加时，对齐度急剧下降。同时，LLMs依赖的道德价值观比人类狭窄。这些发现揭示了一个多元道德鸿沟：表达的价值观在分布和多样性上都存在不匹配。DMP将对齐度提高了64.3%，并增强了价值观多样性。", "conclusion": "动态道德画像（DMP）为LLMs提供更具多元性和与人类对齐的道德指导迈出了重要一步。", "translation": "人们越来越依赖大型语言模型（LLMs）获取道德建议，这可能会影响人类的决策。然而，LLMs与人类道德判断的契合度知之甚少。为了解决这个问题，我们引入了道德困境数据集，这是一个包含1,618个真实世界道德困境的基准，并配有二元评估和自由文本理由形式的人类道德判断分布。我们将此问题视为多元分布对齐任务，比较LLM和人类判断在困境中的分布。我们发现模型仅在高度共识下才能再现人类判断；当人类分歧增加时，对齐度急剧下降。同时，我们使用从3,783个价值观表达中构建的60个价值观分类法，表明LLMs依赖的道德价值观比人类狭窄。这些发现揭示了一个多元道德鸿沟：表达的价值观在分布和多样性上都存在不匹配。为了弥合这一差距，我们引入了动态道德画像（DMP），这是一种基于狄利克雷采样的算法，根据人类衍生的价值观画像来调整模型输出。DMP将对齐度提高了64.3%，并增强了价值观多样性，为LLMs提供更具多元性和与人类对齐的道德指导迈出了重要一步。", "summary": "该研究探讨了大型语言模型（LLMs）在提供道德建议时与人类道德判断和价值观的对齐问题。通过构建“道德困境数据集”并分析人类与LLMs的判断分布，发现LLMs仅在人类高度共识时才能良好复制人类判断，且其依赖的道德价值观比人类狭窄，揭示了“多元道德鸿沟”。为解决此问题，论文提出了动态道德画像（DMP）方法，该方法通过条件化模型输出于人类价值观画像，显著提升了LLMs与人类判断的对齐度（64.3%）并增强了价值观多样性，旨在使LLMs的道德指导更具多元性和人类对齐性。", "keywords": "大型语言模型, 道德判断, 价值观差异, 道德鸿沟, 动态道德画像", "comments": "该论文的创新之处在于明确提出了“多元道德鸿沟”的概念，并量化了大型语言模型在道德判断和价值观多样性上与人类的差异。其提出的动态道德画像（DMP）方法为弥合这一差距提供了一个有效的技术方案，对于提升LLMs在道德推理和指导方面的可靠性和可接受性具有重要意义。研究强调了在训练LLMs时考虑人类道德判断的细微差别和价值观多元性的必要性。"}}
{"id": "2412.05102", "title": "Exact Model Reduction for Continuous-Time Open Quantum Dynamics", "authors": ["Tommaso Grigoletto", "Yukuan Tao", "Francesco Ticozzi", "Lorenza Viola"], "categories": ["quant-ph", "cs.SY", "eess.SY", "math-ph", "math.MP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.05102v3", "summary": "We consider finite-dimensional many-body quantum systems described by\ntime-independent Hamiltonians and Markovian master equations, and present a\nsystematic method for constructing smaller-dimensional, reduced models that\nexactly reproduce the time evolution of a set of initial conditions or\nobservables of interest. Our approach exploits Krylov operator spaces and their\nextension to operator algebras, and may be used to obtain reduced linear models\nof minimal dimension, well-suited for simulation on classical computers, or\nreduced quantum models that preserve the structural constraints of physically\nadmissible quantum dynamics, as required for simulation on quantum computers.\nNotably, we prove that the reduced quantum-dynamical generator is still in\nLindblad form. By introducing a new type of observable-dependent symmetries, we\nshow that our method provides a non-trivial generalization of techniques that\nleverage symmetries, unlocking new reduction opportunities. We quantitatively\nbenchmark our method on paradigmatic open many-body systems of relevance to\ncondensed-matter and quantum-information physics. In particular, we demonstrate\nhow our reduced models can quantitatively describe decoherence dynamics in\ncentral-spin systems coupled to structured environments, magnetization\ntransport in boundary-driven dissipative spin chains, and unwanted error\ndynamics on information encoded in a noiseless quantum code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.05102v3", "cate": "quant-ph", "date": "2024-12-06", "updated": "2025-07-23", "AI": {"title_translation": "连续时间开放量子动力学的精确模型降阶", "tldr": "本文提出了一种系统方法，利用Krylov算子空间，为连续时间开放量子动力学构建精确的降维模型，可用于经典或量子模拟，并证明降阶生成器仍为Lindblad形式。", "motivation": "为了能够精确重现特定初始条件或感兴趣可观测量的时间演化，需要构建维度更小、计算效率更高的简化模型，以便在经典或量子计算机上进行模拟。", "method": "该方法利用Krylov算子空间及其到算子代数的扩展。通过引入一种新型的可观测依赖对称性，实现精确的模型降阶，并证明降阶后的量子动力学生成器仍保持Lindblad形式。", "result": "成功构建了可精确再现时间演化的最小维度线性模型和保持物理约束的量子模型。证明了降阶后的量子动力学生成器仍为Lindblad形式。通过引入可观测依赖对称性，实现了对称性利用技术的非平凡推广，解锁了新的降阶机会。在凝聚态和量子信息物理中的典型开放多体系统上进行了定量基准测试，成功描述了中心自旋系统中的退相干动力学、边界驱动耗散自旋链中的磁化传输以及无噪声量子编码中编码信息的错误动力学。", "conclusion": "该方法通过引入可观测依赖对称性，为利用对称性进行模型降阶的技术提供了非平凡的推广，从而开辟了新的模型降阶可能性，为在经典和量子计算机上模拟开放量子系统提供了有效的工具。", "translation": "我们考虑由时间无关哈密顿量和马尔可夫主方程描述的有限维多体量子系统，并提出了一种系统方法，用于构建维度更小的降阶模型，这些模型能够精确再现一组初始条件或感兴趣的可观测量的时间演化。我们的方法利用Krylov算子空间及其到算子代数的扩展，可用于获得最小维度的降阶线性模型，非常适合在经典计算机上进行模拟，或获得保留物理上允许的量子动力学结构约束的降阶量子模型，以满足量子计算机模拟的需求。值得注意的是，我们证明了降阶后的量子动力学生成器仍然是Lindblad形式。通过引入一种新型的可观测依赖对称性，我们表明我们的方法提供了对称性利用技术的非平凡推广，从而解锁了新的降阶机会。我们定量地在凝聚态和量子信息物理学中相关的典型开放多体系统上对我们的方法进行了基准测试。特别是，我们展示了我们的降阶模型如何定量描述与结构化环境耦合的中心自旋系统中的退相干动力学、边界驱动耗散自旋链中的磁化传输以及编码在无噪声量子代码中的信息上不必要的错误动力学。", "summary": "本文提出了一种利用Krylov算子空间和可观测依赖对称性，对连续时间开放量子动力学进行精确模型降阶的系统方法。该方法能够构建最小维度的线性模型或保留物理约束的量子模型，并证明降阶生成器仍为Lindblad形式。该研究通过推广对称性利用技术，为模拟复杂的开放多体系统提供了新的有效途径，并在多个物理相关系统上进行了验证。", "keywords": "模型降阶, 开放量子系统, Krylov算子空间, Lindblad形式, 可观测依赖对称性", "comments": "这项工作在开放量子系统建模方面具有重要创新性，特别在于提出了精确模型降阶的方法，并且证明了降阶后的动力学生成器依然保持Lindblad形式，这对于保证物理有效性至关重要。引入可观测依赖对称性是其核心贡献之一，它突破了传统对称性方法的局限，极大地扩展了模型降阶的应用范围，对于在经典和量子计算机上高效模拟大型量子系统具有重要意义。"}}
{"id": "2507.09068", "title": "Infinite Video Understanding", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09068v2", "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09068v2", "cate": "cs.CV", "date": "2025-07-11", "updated": "2025-07-23", "AI": {"title_translation": "无限视频理解", "tldr": "本文提出“无限视频理解”作为多媒体和AI研究的未来方向，旨在使模型能够连续处理任意时长视频，以克服当前模型在处理超长视频内容时面临的计算、内存和时间连贯性挑战。", "motivation": "尽管大型语言模型（LLMs）及其多模态扩展（MLLMs）在视频理解方面取得了显著进展，但现有模型在处理超长视频内容时面临根本性挑战。这些挑战包括显著的计算和内存限制，以及难以在长时间内保持时间连贯性、跟踪复杂事件和保留细粒度细节。", "method": "本文是一篇立场论文，提出了实现无限视频理解的关键研究方向，包括流式架构、持久内存机制、分层和自适应表示、以事件为中心的推理以及新颖的评估范式。论文概述了核心挑战和研究方向，但未提出具体的模型或算法。", "result": "Not mentioned in abstract", "conclusion": "本文提出“无限视频理解”作为多媒体和更广泛AI研究的下一个重要前沿，即模型能够连续处理、理解和推理任意时长、可能永无止境的视频数据。将其定位为一项远大的研究目标，旨在为相关研究社区提供重要指引，并推动流式架构、持久内存机制、分层表示和事件中心推理等领域的创新。", "translation": "大型语言模型（LLMs）及其多模态扩展（MLLMs）的快速发展，为视频理解带来了显著进步。然而，一个根本性挑战依然存在：如何有效地处理和理解时长超过数分钟或数小时的视频内容。尽管近期如Video-XL-2等工作展示了极高效率的新颖架构解决方案，以及HoPE和VideoRoPE++等位置编码的进步旨在改善长时间上下文中的时空理解，但当前最先进的模型在面对长序列中海量的视觉令牌时，仍会遇到显著的计算和内存限制。此外，尽管像Deep Video Discovery这样的智能体推理系统取得了进展，但在长时间内保持时间连贯性、跟踪复杂事件和保留细粒度细节仍然是严峻的障碍。这篇立场论文提出，多媒体研究一个合乎逻辑但雄心勃勃的下一个前沿是无限视频理解——模型能够连续处理、理解和推理任意时长、可能永无止境的视频数据的能力。我们认为，将无限视频理解定位为一个远大（blue-sky）的研究目标，为多媒体乃至更广泛的AI研究社区提供了一个重要的指路明灯，推动流式架构、持久内存机制、分层和自适应表示、以事件为中心的推理以及新颖评估范式等领域的创新。借鉴近期关于长/超长视频理解和几个密切相关领域的工作，我们概述了实现这一变革性能力的核心挑战和关键研究方向。", "summary": "本立场论文提出“无限视频理解”作为多媒体和AI研究的未来方向，旨在解决当前大型多模态模型在处理超长视频时面临的计算、内存限制以及难以保持时间连贯性和细节的问题。该能力将使模型能持续处理任意时长的视频数据。论文概述了实现这一目标的核心挑战和关键研究方向，包括流式架构、持久内存机制、分层表示和事件中心推理，旨在为相关领域创新提供指引。", "keywords": "视频理解, 无限视频, 长视频处理, 多模态AI, 计算限制", "comments": "这篇立场论文提出了一个极具前瞻性和挑战性的研究愿景——“无限视频理解”，这对于当前受限于计算和内存瓶颈的视频理解领域具有重要意义。它不仅指出了现有技术的局限性，更提出了一个宏大的“北极星”目标，为未来的多媒体和AI研究指明了方向，有助于推动流式处理、记忆机制和多层次表示等领域的突破性进展。其创新性在于将“无限”作为核心概念，而非简单地延长视频长度。"}}
{"id": "2507.07578", "title": "Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation", "authors": ["Chunyan Wang", "Dong Zhang", "Jinhui Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia", "url": "http://arxiv.org/abs/2507.07578v2", "summary": "Weakly-supervised semantic segmentation aims to assign category labels to\neach pixel using weak annotations, significantly reducing manual annotation\ncosts. Although existing methods have achieved remarkable progress in well-lit\nscenarios, their performance significantly degrades in low-light environments\ndue to two fundamental limitations: severe image quality degradation (e.g., low\ncontrast, noise, and color distortion) and the inherent constraints of weak\nsupervision. These factors collectively lead to unreliable class activation\nmaps and semantically ambiguous pseudo-labels, ultimately compromising the\nmodel's ability to learn discriminative feature representations. To address\nthese problems, we propose Diffusion-Guided Knowledge Distillation for\nWeakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel\nframework that synergistically combines Diffusion-Guided Knowledge Distillation\n(DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and\nlow-light features via diffusion-based denoising and knowledge distillation,\nwhile DGF2 integrates depth maps as illumination-invariant geometric priors to\nenhance structural feature learning. Extensive experiments demonstrate the\neffectiveness of DGKD-WLSS, which achieves state-of-the-art performance in\nweakly supervised semantic segmentation tasks under low-light conditions. The\nsource codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.", "comment": "Accepted by ACM Multimedia", "pdf_url": "http://arxiv.org/pdf/2507.07578v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-23", "AI": {"title_translation": "扩散引导知识蒸馏用于弱监督低光语义分割", "tldr": "针对低光弱监督语义分割中图像质量下降和弱监督限制导致的性能问题，本文提出DGKD-WLSS框架，结合扩散引导知识蒸馏和深度引导特征融合，实现了SOTA性能。", "motivation": "现有弱监督语义分割方法在低光环境下性能显著下降，原因是图像质量严重退化（低对比度、噪声、色彩失真）和弱监督的固有约束，导致类别激活图不可靠、伪标签模糊，最终损害模型学习判别性特征表示的能力。", "method": "提出DGKD-WLSS框架，结合扩散引导知识蒸馏（DGKD）和深度引导特征融合（DGF2）。DGKD通过基于扩散的去噪和知识蒸馏对齐正常光和低光特征；DGF2集成深度图作为光照不变的几何先验，以增强结构特征学习。", "result": "实验证明DGKD-WLSS的有效性，并在低光条件下的弱监督语义分割任务中实现了最先进的性能。", "conclusion": "DGKD-WLSS框架通过结合扩散引导知识蒸馏和深度引导特征融合，有效解决了低光弱监督语义分割中的挑战，并显著提升了性能。", "translation": "弱监督语义分割旨在为每个像素分配类别标签，使用弱标注显著降低了人工标注成本。尽管现有方法在光照充足的场景中取得了显著进展，但由于两个根本限制，它们在低光环境中的性能显著下降：严重的图像质量下降（例如，低对比度、噪声和色彩失真）以及弱监督的固有约束。这些因素共同导致了不可靠的类别激活图和语义模糊的伪标签，最终损害了模型学习判别性特征表示的能力。为了解决这些问题，我们提出了扩散引导知识蒸馏用于弱监督低光语义分割（DGKD-WLSS），这是一个新颖的框架，它协同结合了扩散引导知识蒸馏（DGKD）和深度引导特征融合（DGF2）。DGKD通过基于扩散的去噪和知识蒸馏对齐正常光和低光特征，而DGF2将深度图作为光照不变的几何先验来增强结构特征学习。广泛的实验证明了DGKD-WLSS的有效性，它在低光条件下的弱监督语义分割任务中实现了最先进的性能。源代码已发布在：https://github.com/ChunyanWang1/DGKD-WLSS。", "summary": "本文针对低光环境中弱监督语义分割性能下降的问题，提出了DGKD-WLSS框架。该框架通过结合扩散引导知识蒸馏（DGKD）来对齐正常光和低光特征，并利用深度引导特征融合（DGF2）引入深度图作为几何先验以增强结构特征学习。实验结果表明，DGKD-WLSS在低光弱监督语义分割任务中取得了最先进的性能。", "keywords": "弱监督语义分割, 低光环境, 知识蒸馏, 扩散模型, 深度引导特征融合", "comments": "这篇论文通过引入扩散模型进行特征对齐和利用深度信息作为几何先验，创新性地解决了低光环境下弱监督语义分割的挑战。其结合知识蒸馏和多模态信息（深度）的方法，为图像质量退化和弱监督限制下的像素级任务提供了新的思路，具有重要的研究价值和应用潜力。"}}
{"id": "2507.17118", "title": "HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study", "authors": ["Mandar Pitale", "Jelena Frtunikj", "Abhinaw Priyadershi", "Vasu Singh", "Maria Spence"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.17118v1", "summary": "AI has become integral to safety-critical areas like autonomous driving\nsystems (ADS) and robotics. The architecture of recent autonomous systems are\ntrending toward end-to-end (E2E) monolithic architectures such as large\nlanguage models (LLMs) and vision language models (VLMs). In this paper, we\nreview different architectural solutions and then evaluate the efficacy of\ncommon safety analyses such as failure modes and effect analysis (FMEA) and\nfault tree analysis (FTA). We show how these techniques can be improved for the\nintricate nature of the foundational models, particularly in how they form and\nutilize latent representations. We introduce HySAFE-AI, Hybrid Safety\nArchitectural Analysis Framework for AI Systems, a hybrid framework that adapts\ntraditional methods to evaluate the safety of AI systems. Lastly, we offer\nhints of future work and suggestions to guide the evolution of future AI safety\nstandards.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.17118v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HySafe-AI：面向AI系统的混合安全架构分析框架：一个案例研究", "tldr": "本文介绍了HySafe-AI，一个结合传统方法的混合框架，用于评估AI系统（特别是端到端AI）的安全，并探讨了现有安全分析方法的改进。", "motivation": "AI已成为自动驾驶系统和机器人等安全关键领域不可或缺的一部分。然而，当前自主系统正趋向于端到端的整体架构（如LLMs和VLMs），这使得传统的安全分析方法难以有效评估其安全性。", "method": "首先，回顾了不同的架构解决方案，然后评估了FMEA和FTA等常见安全分析方法的有效性。在此基础上，提出了HySafe-AI，一个混合安全架构分析框架，该框架调整传统方法以评估AI系统的安全性。", "result": "展示了如何改进传统安全分析技术，使其适用于基础模型的复杂性，特别是在它们如何形成和利用潜在表示方面。引入了HySAFE-AI框架。", "conclusion": "本文介绍了HySafe-AI框架，旨在通过适应传统方法来评估AI系统的安全性。研究还为未来的AI安全标准演进提供了未来工作的方向和建议。", "translation": "人工智能已成为自动驾驶系统（ADS）和机器人等安全关键领域不可或缺的一部分。最近的自主系统架构正趋向于端到端（E2E）的整体架构，例如大型语言模型（LLMs）和视觉语言模型（VLMs）。本文回顾了不同的架构解决方案，然后评估了常见的安全分析方法（如故障模式与影响分析（FMEA）和故障树分析（FTA））的有效性。我们展示了如何针对基础模型的复杂性，特别是它们如何形成和利用潜在表示，改进这些技术。我们引入了HySAFE-AI，即面向AI系统的混合安全架构分析框架，这是一个调整传统方法以评估AI系统安全性的混合框架。最后，我们提供了未来工作的提示和建议，以指导未来AI安全标准的发展。", "summary": "本文针对AI在安全关键领域日益增长的应用及其向端到端整体架构的趋势，探讨了传统安全分析方法（如FMEA和FTA）在评估AI系统安全性方面的局限性。作者提出HySafe-AI，一个混合安全架构分析框架，旨在通过改进和适应传统方法来有效评估AI系统的安全性，并特别关注基础模型中潜在表示的复杂性。论文还对未来的AI安全标准发展提出了建议。", "keywords": "AI安全, 混合框架, 安全分析, FMEA, FTA", "comments": "本文提出了HySafe-AI框架，旨在解决AI系统，特别是端到端大模型在安全关键领域应用时面临的安全评估挑战。其创新之处在于将传统安全分析方法与AI系统的特性相结合，以适应其复杂的内部机制，尤其是潜在表示的形成和利用。这项工作对于推动AI系统在安全关键领域的可靠部署具有重要意义，并为未来AI安全标准的发展提供了方向。"}}
{"id": "2507.17317", "title": "HuNavSim 2.0", "authors": ["Miguel Escudero-Jiménez", "Noé Pérez-Higueras", "Andrés Martínez-Silva", "Fernando Caballero", "Luis Merino"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Preprint submitted to the 8th Iberian Robotics Conference (ROBOT 2025)", "url": "http://arxiv.org/abs/2507.17317v1", "summary": "This work presents a new iteration of the Human Navigation Simulator\n(HuNavSim), a novel open-source tool for the simulation of different\nhuman-agent navigation behaviors in scenarios with mobile robots. The tool,\nprogrammed under the ROS 2 framework, can be used together with different\nwell-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main\ngoal is to facilitate the development and evaluation of human-aware robot\nnavigation systems in simulation. In this new version, several features have\nbeen improved and new ones added, such as the extended set of actions and\nconditions that can be combined in Behavior Trees to compound complex and\nrealistic human behaviors.", "comment": "Preprint submitted to the 8th Iberian Robotics Conference (ROBOT\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.17317v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HuNavSim 2.0", "tldr": "HuNavSim 2.0 是一个开源的人类导航模拟器新版本，旨在通过提供更丰富、逼真的人类行为建模能力，促进以人为本的机器人导航系统的开发和评估。", "motivation": "该工作的主要目标是促进在模拟环境中开发和评估以人为本的机器人导航系统。", "method": "该论文提出了HuNavSim 2.0，一个基于ROS 2框架的开源人类导航模拟工具。它可与Gazebo或NVidia Isaac Sim等机器人模拟器配合使用。新版本改进并增加了多项功能，例如扩展了可在行为树中组合的动作和条件集，以构建复杂逼真的人类行为。", "result": "新版本扩展了可在行为树中组合的动作和条件集，从而能够模拟更复杂和逼真的人类行为。", "conclusion": "HuNavSim 2.0通过改进和增加新功能，特别是行为树中动作和条件的扩展，显著提升了在模拟环境中开发和评估以人为本的机器人导航系统的能力。", "translation": "这项工作提出了人类导航模拟器（HuNavSim）的一个新迭代版本HuNavSim 2.0，这是一个新颖的开源工具，用于在有移动机器人的场景中模拟不同的人类-智能体导航行为。该工具在ROS 2框架下编程，可以与不同的知名机器人模拟器（如Gazebo或NVidia Isaac Sim）一起使用。主要目标是促进在模拟中开发和评估以人为本的机器人导航系统。在这个新版本中，改进了多项功能并增加了新的功能，例如扩展了可在行为树中组合的动作和条件集，以构成复杂逼真的人类行为。", "summary": "HuNavSim 2.0是一个基于ROS 2的开源人类导航模拟器的新版本，旨在促进以人为本的机器人导航系统的开发和评估。它与主流机器人模拟器兼容，并显著增强了通过行为树模拟复杂和逼真人类行为的能力。", "keywords": "人类导航模拟器, ROS 2, 机器人导航, 行为树, 仿真", "comments": "HuNavSim 2.0的创新之处在于其开源性质以及通过行为树机制极大地增强了人类行为建模的复杂性和真实性，这对于以人为本的机器人导航系统开发具有重要意义。与现有模拟器的兼容性也增加了其应用价值。"}}
{"id": "2507.17311", "title": "EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents", "authors": ["Zijie Guo", "Jiong Wang", "Xiaoyu Yue", "Wangxu Wei", "Zhe Jiang", "Wanghan Xu", "Ben Fei", "Wenlong Zhang", "Xinyu Gu", "Lijing Cheng", "Jing-Jia Luo", "Chao Li", "Yaqiang Wang", "Tao Chen", "Wanli Ouyang", "Fenghua Ling", "Lei Bai"], "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17311v1", "summary": "Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17311v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "地球之链：用自进化AI智能体解读气候信号", "tldr": "EarthLink是一个自进化的AI智能体，作为地球科学家的交互式副驾驶，自动化端到端研究工作流程，并能从用户互动中学习，其分析能力与人类初级研究员相当，旨在加速地球系统研究。", "motivation": "现代地球科学面临地球系统数据庞大、零碎、复杂以及分析需求日益精密的挑战，这些因素严重阻碍了科学发现的快速进展。", "method": "本文介绍了EarthLink，这是首个专为地球科学家设计的交互式AI智能体副驾驶。它能自动化端到端的研究工作流程，包括规划、代码生成和多场景分析。EarthLink不同于静态诊断工具，它能通过动态反馈循环从用户互动中持续学习和完善其能力。", "result": "EarthLink在多项气候变化核心科学任务（如模型-观测对比和复杂现象诊断）上验证了其性能。在多专家评估中，EarthLink生成了科学严谨的分析，并展现出与人类初级研究员工作流程中特定方面相当的分析能力。", "conclusion": "EarthLink的透明、可审计的工作流程和自然语言界面使科学家能够从繁重的手动执行转向战略性监督和假设生成。它标志着在全球变化加速的时代，地球系统研究迈向高效、可信赖和协作范式的关键一步。", "translation": "现代地球科学正处于一个转折点。地球系统数据庞大、零碎和复杂的性质，加上日益复杂的分析需求，给快速科学发现带来了严重的瓶颈。在此，我们介绍了EarthLink，这是首个被设计为地球科学家互动副驾驶的AI智能体。它自动化了端到端的研究工作流程，从规划和代码生成到多场景分析。与静态诊断工具不同，EarthLink可以从用户互动中学习，通过动态反馈循环不断完善其能力。我们验证了它在多项气候变化核心科学任务上的性能，范围从模型-观测比较到复杂现象的诊断。在多专家评估中，EarthLink产生了科学严谨的分析，并展示了与人类初级研究员工作流程中特定方面相当的分析能力。此外，其透明、可审计的工作流程和自然语言界面使科学家能够从繁重的手动执行转向战略性监督和假设生成。EarthLink标志着在全球变化加速的时代，地球系统研究迈向高效、可信赖和协作的关键一步。", "summary": "EarthLink是一个创新的AI智能体，旨在解决地球科学研究中数据复杂性和分析需求日益增长所带来的瓶颈。它作为地球科学家的交互式副驾驶，能够自动化端到端的研究工作流程，并具备通过用户互动进行自我进化的能力。经过验证，EarthLink在气候变化相关任务上表现出色，其分析能力可与人类初级研究员媲美。该系统通过提供透明和自然语言界面，赋能科学家更高效地进行战略性监督和假设生成，预示着地球系统研究迈向一个更高效、可信赖和协作的新范式。", "keywords": "AI智能体, 地球科学, 气候信号, 自进化, 研究自动化", "comments": "EarthLink的创新之处在于其“自进化AI智能体”的理念，使其能够从用户互动中学习并持续改进，这超越了传统静态工具的限制。其作为“交互式副驾驶”的定位，旨在将科学家从繁琐的执行工作中解放出来，专注于更高层次的战略思考和假设生成，这对于加速地球系统研究具有重要意义。该论文通过多专家评估验证了其分析能力与人类初级研究员相当，增强了其可信度。"}}
{"id": "2501.18409", "title": "Pinching-Antenna Systems (PASS): Architecture Designs, Opportunities, and Outlook", "authors": ["Yuanwei Liu", "Zhaolin Wang", "Xidong Mu", "Chongjun Ouyang", "Xiaoxia Xu", "Zhiguo Ding"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      7 pages; to appear in IEEE Communications Magazine", "url": "http://arxiv.org/abs/2501.18409v3", "summary": "Flexible-antenna systems have recently attracted significant research\nattention due to their potential to intelligently reconfigure wireless\nchannels. However, the current flexible-antenna systems still suffer from\nfundamental limitations, such as free-space path loss and line-of-sight\nblockage. This article introduces a novel flexible-antenna system, termed the\nPinching-Antenna SyStem (PASS). PASS adopts the dielectric waveguides as the\nprimary transmission medium and radiates signals into free space by flexibly\npinching discrete dielectric particles, referred to as pinching antennas, along\nthe waveguide. By combining the strengths of both wireless and wired\ncommunication, PASS effectively mitigates inherent wireless limitations while\noffering high antenna reconfigurability. This article reviews the key features\nof PASS in comparison with conventional wireless systems, analyzes its main\nadvantages, and discusses potential designs, transmission architectures, and\napplication scenarios. Finally, it outlines promising research directions and\nopen challenges associated with PASS.", "comment": "7 pages; to appear in IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2501.18409v3", "cate": "eess.SP", "date": "2025-01-30", "updated": "2025-07-23", "AI": {"title_translation": "捏合天线系统 (PASS)：架构设计、机遇与展望", "tldr": "本文介绍了一种新型捏合天线系统（PASS），它结合了无线和有线通信的优点，通过沿介质波导捏合介质粒子来辐射信号，旨在克服传统柔性天线系统的局限性。", "motivation": "现有的柔性天线系统存在自由空间路径损耗和视线阻塞等基本限制，因此需要一种能有效缓解这些限制并提供高天线可重构性的新系统。", "method": "PASS系统采用介质波导作为主要传输介质，并通过沿波导柔性捏合离散介质粒子（称为捏合天线）将信号辐射到自由空间。", "result": "PASS结合了无线和有线通信的优点，有效缓解了固有的无线限制，同时提供了高天线可重构性。文章回顾了PASS与传统无线系统的主要特点，分析了其主要优势。", "conclusion": "文章讨论了PASS的潜在设计、传输架构和应用场景，并概述了相关的有前景的研究方向和开放挑战。", "translation": "柔性天线系统因其智能重构无线信道的潜力，最近引起了广泛的研究关注。然而，当前的柔性天线系统仍面临基本限制，例如自由空间路径损耗和视线阻塞。本文介绍了一种新型柔性天线系统，称为捏合天线系统（PASS）。PASS采用介质波导作为主要传输介质，通过沿波导柔性捏合离散介质粒子（称为捏合天线）将信号辐射到自由空间。通过结合无线和有线通信的优势，PASS有效缓解了固有的无线限制，同时提供了高天线可重构性。本文回顾了PASS与传统无线系统的主要特点，分析了其主要优势，并讨论了潜在的设计、传输架构和应用场景。最后，本文概述了与PASS相关的有前景的研究方向和开放挑战。", "summary": "本文提出了一种名为捏合天线系统（PASS）的新型柔性天线系统，旨在克服现有柔性天线系统面临的自由空间路径损耗和视线阻塞等局限。PASS利用介质波导作为传输介质，通过捏合沿波导的离散介质粒子来辐射信号。这种混合方法结合了无线和有线通信的优点，能够有效缓解无线通信的固有缺陷，并提供高天线可重构性。文章详细探讨了PASS的特点、优势、潜在设计、架构和应用，并指出了未来的研究方向和挑战。", "keywords": "捏合天线系统, 柔性天线, 介质波导, 无线通信, 可重构性", "comments": "PASS系统通过结合介质波导的有线传输优势和离散捏合天线的无线辐射能力，提出了一种新颖的混合通信范式。这种方法在克服传统无线通信限制（如路径损耗和阻塞）方面具有创新性，并有望在智能可重构无线信道领域带来显著进步。"}}
{"id": "2507.17404", "title": "Univariate amenable functions", "authors": ["Carlos Beltrán"], "categories": ["math.NA", "cs.NA", "65Y04"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17404v1", "summary": "The concepts of amenable and compatible functions have been introduced in a\nrecent work, in order to state precise mathematical theorems that guarantee\nthat a backward stable algorithm is also forward stable, and that the\ncomposition of two stable algorithms results in an stable algorithm. In this\nwork, we elaborate in this theory for univariate real analytic functions,\nproviding simple tests for both concepts and producing tables for a number of\nelementary functions which are or fail to be amenable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17404v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "单变量适宜函数", "tldr": "本文针对单变量实解析函数，阐述了适宜函数和兼容函数的理论，并提供了简单的测试方法和基本函数的适宜性表格，以保证算法的稳定性。", "motivation": "旨在提出精确的数学定理，以保证向后稳定算法也是向前稳定的，并且两个稳定算法的组合仍然是稳定的。", "method": "针对单变量实解析函数，阐述了适宜函数和兼容函数的理论，提供了简单的测试方法，并制作了基本函数是否适宜的表格。", "result": "提供了适宜函数和兼容函数的简单测试方法。制作了许多基本函数是否适宜的表格。", "conclusion": "本文发展了单变量实解析函数的适宜函数和兼容函数理论，并提供了实用的测试方法和示例。", "translation": "适宜函数和兼容函数的概念在最近的一项工作中被引入，旨在阐述精确的数学定理，以保证一个向后稳定算法也是向前稳定的，并且两个稳定算法的组合仍然是一个稳定算法。在这项工作中，我们针对单变量实解析函数阐述了这一理论，为这两个概念提供了简单的测试，并为许多基本函数制作了它们是否适宜的表格。", "summary": "本文在先前为保证算法稳定性而引入的适宜函数和兼容函数理论基础上，针对单变量实解析函数进行了深入阐述。文章提供了这些概念的简单测试方法，并制作了多种基本函数的适宜性表格。", "keywords": "适宜函数, 兼容函数, 算法稳定性, 单变量函数, 实解析函数", "comments": "这篇论文通过引入并发展适宜函数和兼容函数的概念，特别是针对单变量实解析函数，为算法稳定性的理论理解做出了贡献。提供简单的测试和表格使得该理论具有实用性。"}}
{"id": "2507.16284", "title": "Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis", "authors": ["Paul-Andrei Pogăcean", "Sanda-Maria Avram"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16284v2", "summary": "The debate surrounding language identification has gained renewed attention\nin recent years, especially with the rapid evolution of AI-powered language\nmodels. However, the non-AI-based approaches to language identification have\nbeen overshadowed. This research explores a mathematical implementation of an\nalgorithm for language determinism by leveraging monograms and bigrams\nfrequency rankings derived from established linguistic research. The datasets\nused comprise texts varying in length, historical period, and genre, including\nshort stories, fairy tales, and poems. Despite these variations, the method\nachieves over 80\\% accuracy on texts shorter than 150 characters and reaches\n100\\% accuracy for longer texts. These results demonstrate that classical\nfrequency-based approaches remain effective and scalable alternatives to\nAI-driven models for language detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16284v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "基于闵可夫斯基范数的语言检测：通过字符二元组和频率分析进行识别", "tldr": "本研究提出了一种基于闵可夫斯基范数、字符二元组和频率分析的非AI语言检测算法，在短文本上准确率超过80%，长文本达到100%，证明经典方法仍有效。", "motivation": "近年来，随着AI语言模型的快速发展，语言识别的讨论再次受到关注，但非AI方法被忽视。本研究旨在探索一种数学实现的语言确定性算法。", "method": "本研究利用已有的语言学研究中得出的字母和二元组频率排名，通过闵可夫斯基范数实现了语言检测算法。数据集包含不同长度、历史时期和体裁的文本。", "result": "该方法在短于150个字符的文本上实现了超过80%的准确率，对于更长的文本达到了100%的准确率。", "conclusion": "经典基于频率的方法仍然是AI驱动模型进行语言检测的有效且可扩展的替代方案。", "translation": "近年来，围绕语言识别的争论再次受到关注，特别是随着人工智能驱动的语言模型的快速发展。然而，非人工智能的语言识别方法却被忽视了。本研究探索了一种利用已建立的语言学研究中得出的单字母和二元组频率排名，通过闵可夫斯基范数实现语言确定性算法的数学方法。使用的数据集包括长度、历史时期和体裁各异的文本，包括短篇小说、童话和诗歌。尽管存在这些差异，该方法在短于150个字符的文本上实现了超过80%的准确率，对于更长的文本达到了100%的准确率。这些结果表明，经典的基于频率的方法仍然是人工智能驱动模型进行语言检测的有效且可扩展的替代方案。", "summary": "本研究提出了一种基于闵可夫斯基范数、字符二元组和频率分析的非AI语言检测算法。该方法利用单字母和二元组的频率排名，并在包含不同长度、时期和体裁文本的数据集上进行了评估。结果显示，该算法在短于150字符的文本上准确率超过80%，在更长的文本上达到100%，证明了经典频率分析方法在语言检测领域依然有效且可扩展。", "keywords": "语言检测, 闵可夫斯基范数, 字符二元组, 频率分析, 非AI方法", "comments": "该论文的创新点在于重新审视并验证了非AI、基于数学和统计的经典方法在语言检测领域的有效性，尤其是在当前AI模型盛行的大背景下。其重要性在于提供了一个计算成本可能更低、解释性更强的替代方案，特别适用于对资源或模型复杂性有严格要求的场景。该研究证明了简单而强大的经典算法在特定任务上的竞争力。"}}
{"id": "2507.16833", "title": "Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs", "authors": ["Qiuyu Shi", "Kangming Li", "Yao Fehlis", "Daniel Persaud", "Robert Black", "Jason Hattrick-Simpers"], "categories": ["cs.LG", "physics.data-an"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16833v1", "summary": "Self-driving laboratories (SDLs) have shown promise to accelerate materials\ndiscovery by integrating machine learning with automated experimental\nplatforms. However, errors in the capture of input parameters may corrupt the\nfeatures used to model system performance, compromising current and future\ncampaigns. This study develops an automated workflow to systematically detect\nnoisy features, determine sample-feature pairings that can be corrected, and\nfinally recover the correct feature values. A systematic study is then\nperformed to examine how dataset size, noise intensity, and feature value\ndistribution affect both the detectability and recoverability of noisy\nfeatures. In general, high-intensity noise and large training datasets are\nconducive to the detection and correction of noisy features. Low-intensity\nnoise reduces detection and recovery but can be compensated for by larger clean\ntraining data sets. Detection and correction results vary between features with\ncontinuous and dispersed feature distributions showing greater recoverability\ncompared to features with discrete or narrow distributions. This systematic\nstudy not only demonstrates a model agnostic framework for rational data\nrecovery in the presence of noise, limited data, and differing feature\ndistributions but also provides a tangible benchmark of kNN imputation in\nmaterials data sets. Ultimately, it aims to enhance data quality and\nexperimental precision in automated materials discovery.", "comment": "15 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16833v1", "cate": "cs.LG", "date": "2025-07-15", "updated": "2025-07-15", "AI": {"title_translation": "探索kNN在自动驾驶实验室中噪声特征检测与恢复的前沿", "tldr": "本研究开发了一种自动工作流程，用于检测和恢复自动驾驶实验室（SDLs）中的噪声特征，并系统研究了数据集大小、噪声强度和特征分布对其可检测性和可恢复性的影响，旨在提高自动化材料发现中的数据质量。", "motivation": "自动驾驶实验室（SDLs）在加速材料发现方面显示出潜力，但输入参数的错误可能导致特征损坏，从而影响当前和未来的实验活动。", "method": "本研究开发了一个自动化工作流程，用于系统地检测噪声特征，确定可校正的样本-特征配对，并恢复正确的特征值。随后进行了一项系统研究，以检验数据集大小、噪声强度和特征值分布如何影响噪声特征的可检测性和可恢复性。", "result": "高强度噪声和大型训练数据集有利于噪声特征的检测和校正。低强度噪声会降低检测和恢复效果，但可以通过更大的干净训练数据集来补偿。连续和分散特征分布的特征比离散或狭窄分布的特征显示出更高的可恢复性。", "conclusion": "本研究不仅展示了一个在存在噪声、有限数据和不同特征分布情况下的理性数据恢复模型无关框架，还为材料数据集中的kNN插补提供了一个具体的基准。最终，它旨在提高自动化材料发现中的数据质量和实验精度。", "translation": "自动驾驶实验室（SDLs）通过将机器学习与自动化实验平台相结合，在加速材料发现方面显示出潜力。然而，输入参数捕获中的错误可能会损坏用于建模系统性能的特征，从而影响当前和未来的实验活动。本研究开发了一个自动化工作流程，以系统地检测噪声特征，确定可校正的样本-特征配对，并最终恢复正确的特征值。随后进行了一项系统研究，以检验数据集大小、噪声强度和特征值分布如何影响噪声特征的可检测性和可恢复性。总的来说，高强度噪声和大型训练数据集有利于噪声特征的检测和校正。低强度噪声会降低检测和恢复效果，但可以通过更大的干净训练数据集来补偿。连续和分散特征分布的特征比离散或狭窄分布的特征显示出更高的可恢复性。这项系统研究不仅展示了一个在存在噪声、有限数据和不同特征分布情况下的理性数据恢复模型无关框架，而且为材料数据集中的kNN插补提供了一个具体的基准。最终，它旨在提高自动化材料发现中的数据质量和实验精度。", "summary": "本论文提出了一种自动化工作流程，用于检测、识别和校正自动驾驶实验室（SDLs）输入参数中的噪声特征，这对于加速材料发现至关重要。通过一项系统研究，探讨了数据集大小、噪声强度和特征值分布如何影响这些噪声特征的可检测性和可恢复性。研究结果表明，高强度噪声和大型数据集通常能改善检测和校正效果，而低强度噪声可以通过更大的干净数据集来弥补。具有连续或分散分布的特征比离散或狭窄分布的特征表现出更好的可恢复性。这项研究提供了一个模型无关的数据恢复框架和kNN插补基准，旨在最终提升自动化材料发现中的数据质量和实验精度。", "keywords": "kNN, 噪声特征检测, 数据恢复, 自动驾驶实验室, 材料发现", "comments": "本论文解决了自动驾驶实验室中一个重要的实际挑战：输入数据中存在的噪声。其创新之处在于开发了一个系统化、自动化且模型无关的框架来检测和恢复这些噪声特征。对各种因素（数据集大小、噪声强度、特征分布）如何影响可恢复性进行的系统研究尤为宝贵，为自动化材料发现中的数据处理提供了实际见解。此外，为材料数据集提供kNN插补基准也是对该领域的一个具体贡献。"}}
{"id": "2507.16540", "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "categories": ["cs.CR", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16540v1", "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16540v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "C/C++中基于边缘感知图注意力网络的可解释漏洞检测", "tldr": "本文提出ExplainVulD，一个基于图的C/C++代码漏洞检测框架，通过边缘感知图注意力网络和双通道嵌入，解决了类别不平衡和可解释性问题，性能优于现有方法。", "motivation": "源代码中的安全漏洞检测仍然具有挑战性，特别是由于真实世界数据集中漏洞函数欠表示导致的类别不平衡问题。现有学习方法常优化召回率，导致高误报率和可用性降低。此外，许多方法缺乏可解释性，限制了它们在安全工作流程中的集成。", "method": "本文提出了ExplainVulD，一个基于图的C/C++代码漏洞检测框架。该方法构建代码属性图，并使用捕获语义和结构信息的双通道嵌入表示节点。这些节点由边缘感知注意力机制处理，该机制结合了边缘类型嵌入以区分程序关系。为解决类别不平衡问题，模型使用类别加权交叉熵损失进行训练。", "result": "ExplainVulD在ReVeal数据集上30次独立运行中实现了88.25%的平均准确率和48.23%的F1分数。与ReVeal模型（一种先前的基于学习的方法）相比，准确率相对提高了4.6%，F1分数相对提高了16.9%。该框架还优于静态分析工具，准确率相对提高了14.0%至14.1%，F1分数相对提高了132.2%至201.2%。", "conclusion": "ExplainVulD不仅提高了漏洞检测性能，还通过识别每个函数中最具影响力的代码区域来生成可解释的输出，从而支持安全分类的透明性和信任。", "translation": "在源代码中检测安全漏洞仍然具有挑战性，特别是由于真实世界数据集中易受攻击函数代表性不足导致的类别不平衡问题。现有的基于学习的方法通常优化召回率，导致高误报率并降低了在开发工作流程中的可用性。此外，许多方法缺乏可解释性，限制了它们在安全工作流程中的集成。本文提出了ExplainVulD，一个用于C/C++代码漏洞检测的基于图的框架。该方法构建代码属性图，并使用捕获语义和结构信息的双通道嵌入来表示节点。这些节点由边缘感知注意力机制处理，该机制结合了边缘类型嵌入以区分程序关系。为了解决类别不平衡问题，模型使用类别加权交叉熵损失进行训练。ExplainVulD在ReVeal数据集上30次独立运行中实现了88.25%的平均准确率和48.23%的F1分数。这些结果与ReVeal模型（一种先前的基于学习的方法）相比，准确率相对提高了4.6%，F1分数相对提高了16.9%。该框架还优于静态分析工具，准确率相对提高了14.0%至14.1%，F1分数相对提高了132.2%至201.2%。除了改进检测性能之外，ExplainVulD还通过识别每个函数中最具影响力的代码区域来生成可解释的输出，从而支持安全分类中的透明性和信任。", "summary": "本文提出了ExplainVulD，一个用于C/C++代码漏洞检测的图基框架，旨在解决现有方法中存在的类别不平衡和缺乏可解释性问题。ExplainVulD通过构建代码属性图、使用双通道嵌入捕获语义和结构信息，并结合边缘感知注意力机制来区分程序关系。为应对类别不平衡，模型采用类别加权交叉熵损失进行训练。实验结果表明，ExplainVulD在准确率和F1分数上均优于现有学习模型和静态分析工具，并且能够提供可解释的输出，提高安全分析的透明度和信任度。", "keywords": "漏洞检测, 图注意力网络, 可解释性, C/C++, 代码属性图", "comments": "本文的创新点在于提出了一个结合双通道嵌入和边缘感知图注意力机制的图基框架ExplainVulD，有效解决了C/C++代码漏洞检测中的类别不平衡问题。尤其值得关注的是，该方法不仅提升了检测性能，还提供了可解释的输出，这对于提高安全分析的透明度和信任度至关重要，是现有许多黑盒学习方法所不具备的优势。在实际开发和安全审查中，可解释性能够帮助开发者更快地理解漏洞原因并进行修复。"}}
{"id": "2507.17262", "title": "VisionTrap: Unanswerable Questions On Visual Data", "authors": ["Asir Saadat", "Syem Aziz", "Shahriar Mahmud", "Abdullah Ibne Masud Mahi", "Sabbir Ahmed"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17262v1", "summary": "Visual Question Answering (VQA) has been a widely studied topic, with\nextensive research focusing on how VLMs respond to answerable questions based\non real-world images. However, there has been limited exploration of how these\nmodels handle unanswerable questions, particularly in cases where they should\nabstain from providing a response. This research investigates VQA performance\non unrealistically generated images or asking unanswerable questions, assessing\nwhether models recognize the limitations of their knowledge or attempt to\ngenerate incorrect answers. We introduced a dataset, VisionTrap, comprising\nthree categories of unanswerable questions across diverse image types: (1)\nhybrid entities that fuse objects and animals, (2) objects depicted in\nunconventional or impossible scenarios, and (3) fictional or non-existent\nfigures. The questions posed are logically structured yet inherently\nunanswerable, testing whether models can correctly recognize their limitations.\nOur findings highlight the importance of incorporating such questions into VQA\nbenchmarks to evaluate whether models tend to answer, even when they should\nabstain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17262v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VisionTrap: 视觉数据上的不可回答问题", "tldr": "本研究探讨VQA模型如何处理不可回答问题，并引入VisionTrap数据集来评估模型是否知道何时应拒绝回答。", "motivation": "现有VQA研究主要关注模型如何回答基于真实图像的可回答问题，但很少探讨模型如何处理不可回答问题，特别是当它们应该拒绝提供响应时。", "method": "引入了一个名为VisionTrap的数据集，包含三类不可回答问题，涉及混合实体、非常规场景中的物体以及虚构人物。所提出的问题结构逻辑清晰但本质上不可回答，旨在测试模型识别其局限性的能力。", "result": "研究结果强调了将此类问题纳入VQA基准测试的重要性，以评估模型即使在应该拒绝回答时是否仍倾向于给出答案。", "conclusion": "将不可回答问题纳入VQA基准测试对于全面评估VQA模型何时应拒绝回答至关重要。", "translation": "视觉问答（VQA）是一个被广泛研究的课题，大量研究集中于视觉语言模型（VLM）如何基于真实世界图像回答可回答的问题。然而，对于这些模型如何处理不可回答问题，特别是在它们应该拒绝提供响应的情况下，探索有限。本研究调查了VQA在不真实生成的图像或提出不可回答问题时的性能，评估模型是否能识别其知识的局限性或试图生成不正确的答案。我们引入了一个名为VisionTrap的数据集，该数据集包含跨不同图像类型的三类不可回答问题：（1）融合了物体和动物的混合实体，（2）描绘在非常规或不可能场景中的物体，以及（3）虚构或不存在的人物。所提出的问题结构逻辑清晰但本质上不可回答，旨在测试模型能否正确识别其局限性。我们的研究结果强调了将此类问题纳入VQA基准测试的重要性，以评估模型即使在应该拒绝回答时是否仍倾向于给出答案。", "summary": "本研究探讨了视觉问答（VQA）模型在面对不可回答问题时的表现，填补了现有VQA研究主要关注可回答问题的空白。研究引入了VisionTrap数据集，该数据集包含三类设计为逻辑合理但本质上无解的视觉问题，旨在测试模型识别知识局限性和何时应拒绝回答的能力。研究发现，将此类不可回答问题纳入VQA评估对于全面理解模型行为至关重要，特别是它们在不应回答时是否仍倾向于生成答案。", "keywords": "视觉问答, 不可回答问题, VisionTrap, VQA基准测试, 模型局限性", "comments": "该论文的创新点在于提出了一个专门用于评估VQA模型处理不可回答问题的能力的新数据集VisionTrap。这对于提升VQA模型的鲁棒性和真实世界适用性至关重要，因为它迫使模型不仅要给出正确答案，还要知道何时保持沉默。这解决了现有基准测试中的一个重要盲点，即模型在遇到超出其知识范围或逻辑上不可能的问题时，往往会“幻觉”出答案而非承认无知。"}}
{"id": "2505.21767", "title": "Beyond Single-Channel: Multichannel Signal Imaging for PPG-to-ECG Reconstruction with Vision Transformers", "authors": ["Xiaoyan Li", "Shixin Xu", "Faisal Habib", "Arvind Gupta", "Huaxiong Huang"], "categories": ["eess.IV", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.21767v2", "summary": "Reconstructing ECG from PPG is a promising yet challenging task. While recent\nadvancements in generative models have significantly improved ECG\nreconstruction, accurately capturing fine-grained waveform features remains a\nkey challenge. To address this, we propose a novel PPG-to-ECG reconstruction\nmethod that leverages a Vision Transformer (ViT) as the core network. Unlike\nconventional approaches that rely on single-channel PPG, our method employs a\nfour-channel signal image representation, incorporating the original PPG, its\nfirst-order difference, second-order difference, and area under the curve. This\nmulti-channel design enriches feature extraction by preserving both temporal\nand physiological variations within the PPG. By leveraging the self-attention\nmechanism in ViT, our approach effectively captures both inter-beat and\nintra-beat dependencies, leading to more robust and accurate ECG\nreconstruction. Experimental results demonstrate that our method consistently\noutperforms existing 1D convolution-based approaches, achieving up to 29%\nreduction in PRD and 15% reduction in RMSE. The proposed approach also produces\nimprovements in other evaluation metrics, highlighting its robustness and\neffectiveness in reconstructing ECG signals. Furthermore, to ensure a\nclinically relevant evaluation, we introduce new performance metrics, including\nQRS area error, PR interval error, RT interval error, and RT amplitude\ndifference error. Our findings suggest that integrating a four-channel signal\nimage representation with the self-attention mechanism of ViT enables more\neffective extraction of informative PPG features and improved modeling of\nbeat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the\npotential of PPG as a viable alternative for heart activity monitoring, our\napproach opens new avenues for cyclic signal analysis and prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.21767v2", "cate": "eess.IV", "date": "2025-05-27", "updated": "2025-07-22", "AI": {"title_translation": "超越单通道：基于多通道信号成像和视觉Transformer的PPG到ECG重建", "tldr": "本文提出一种使用四通道信号图像表示和视觉Transformer进行PPG到ECG重建的新方法，显著提高了重建精度，并引入了临床相关评估指标。", "motivation": "从PPG重建ECG是一项有前景但具有挑战性的任务。尽管生成模型在ECG重建方面取得了显著进展，但准确捕获细粒度波形特征仍然是关键挑战。", "method": "本文提出一种新颖的PPG到ECG重建方法，其核心网络采用视觉Transformer（ViT）。与依赖单通道PPG的传统方法不同，该方法采用四通道信号图像表示，包括原始PPG、其一阶差分、二阶差分和曲线下面积。这种多通道设计通过保留PPG内的时间和生理变异性来丰富特征提取。通过利用ViT的自注意力机制，该方法有效捕获节拍间和节拍内依赖性。", "result": "实验结果表明，该方法始终优于现有的一维卷积方法，PRD降低高达29%，RMSE降低15%。该方法在其他评估指标上也取得了改进，突出了其在重建ECG信号方面的鲁棒性和有效性。此外，为确保临床相关评估，本文引入了新的性能指标，包括QRS面积误差、PR间期误差、RT间期误差和RT幅度差误差。", "conclusion": "研究结果表明，将四通道信号图像表示与ViT的自注意力机制相结合，能够更有效地提取信息丰富的PPG特征，并改进PPG到ECG映射中节拍间变异性的建模。该方法不仅展示了PPG作为心脏活动监测可行替代方案的潜力，还为循环信号分析和预测开辟了新途径。", "translation": "从PPG重建ECG是一项有前景但具有挑战性的任务。尽管生成模型在ECG重建方面取得了显著进展，但准确捕获细粒度波形特征仍然是关键挑战。为了解决这个问题，我们提出了一种新颖的PPG到ECG重建方法，该方法利用视觉Transformer（ViT）作为核心网络。与依赖单通道PPG的传统方法不同，我们的方法采用四通道信号图像表示，包括原始PPG、其一阶差分、二阶差分和曲线下面积。这种多通道设计通过保留PPG内的时间和生理变异性来丰富特征提取。通过利用ViT的自注意力机制，我们的方法有效地捕获了节拍间和节拍内依赖性，从而实现了更鲁棒和准确的ECG重建。实验结果表明，我们的方法始终优于现有的一维卷积方法，PRD降低高达29%，RMSE降低15%。所提出的方法在其他评估指标上也取得了改进，突出了其在重建ECG信号方面的鲁棒性和有效性。此外，为确保临床相关评估，我们引入了新的性能指标，包括QRS面积误差、PR间期误差、RT间期误差和RT幅度差误差。我们的发现表明，将四通道信号图像表示与ViT的自注意力机制相结合，能够更有效地提取信息丰富的PPG特征，并改进PPG到ECG映射中节拍间变异性的建模。除了展示PPG作为心脏活动监测可行替代方案的潜力外，我们的方法还为循环信号分析和预测开辟了新途径。", "summary": "本文提出一种新颖的PPG到ECG重建方法，通过采用四通道信号图像表示（包含原始PPG及其一阶、二阶差分和曲线下面积）并结合视觉Transformer（ViT）作为核心网络。该方法利用多通道设计丰富特征提取，并通过ViT的自注意力机制有效捕捉PPG信号中的时间、生理变异性以及节拍间和节拍内依赖性，从而实现更鲁棒和准确的ECG重建。实验结果表明，与现有的一维卷积方法相比，该方法在PRD和RMSE等指标上均有显著改善，并引入了临床相关评估指标，证明了其在心电信号重建方面的有效性和潜力。", "keywords": "PPG-to-ECG重建, 视觉Transformer, 多通道信号, 生理信号处理, 心电图", "comments": "这项研究的创新之处在于其独特的多通道信号图像表示方法，超越了传统的单通道处理，并成功将视觉Transformer应用于生理信号重建领域，有效利用了其自注意力机制来捕捉复杂的信号依赖性。引入临床相关评估指标也进一步提升了其实用价值和临床转化潜力。这为PPG到ECG重建领域带来了显著的性能提升和新的研究方向。"}}
{"id": "2308.15225", "title": "From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions", "authors": ["Mrugsen Nagsen Gopnarayan", "Jaan Aru", "Sebastian Gluth"], "categories": ["q-bio.NC", "cs.AI"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      Review paper, 17 pages, 2 figure", "url": "http://arxiv.org/abs/2308.15225v3", "summary": "Over the past decades, cognitive neuroscientists and behavioral economists\nhave recognized the value of describing the process of decision making in\ndetail and modeling the emergence of decisions over time. For example, the time\nit takes to decide can reveal more about an agent's true hidden preferences\nthan only the decision itself. Similarly, data that track the ongoing decision\nprocess such as eye movements or neural recordings contain critical information\nthat can be exploited, even if no decision is made. Here, we argue that\nartificial intelligence (AI) research would benefit from a stronger focus on\ninsights about how decisions emerge over time and incorporate related process\ndata to improve AI predictions in general and human-AI interactions in\nparticular. First, we introduce a highly established computational framework\nthat assumes decisions to emerge from the noisy accumulation of evidence, and\nwe present related empirical work in psychology, neuroscience, and economics.\nNext, we discuss to what extent current approaches in multi-agent AI do or do\nnot incorporate process data and models of decision making. Finally, we outline\nhow a more principled inclusion of the evidence-accumulation framework into the\ntraining and use of AI can help to improve human-AI interactions in the future.", "comment": "Review paper, 17 pages, 2 figure", "pdf_url": "http://arxiv.org/pdf/2308.15225v3", "cate": "q-bio.NC", "date": "2023-08-29", "updated": "2025-07-23", "AI": {"title_translation": "从DDMs到DNNs：利用决策过程数据和模型改进人机交互", "tldr": "本文认为AI研究应更多关注决策过程数据和决策模型，特别是证据积累框架，以提升AI预测能力和改善人机交互。", "motivation": "认知神经科学家和行为经济学家已认识到详细描述决策过程和建模决策随时间出现的重要性。本文论证，人工智能（AI）研究可以通过更强烈地关注决策如何随时间出现的见解，并结合相关的过程数据来普遍改进AI预测，特别是人机交互。", "method": "本文首先介绍了一个成熟的计算框架，该框架假设决策源于证据的嘈杂积累，并展示了心理学、神经科学和经济学中的相关实证工作。接着，讨论了当前多智能体AI方法是否以及在多大程度上整合了过程数据和决策模型。最后，概述了如何将证据积累框架更原则性地纳入AI的训练和使用中，以改进未来的人机交互。", "result": "Not mentioned in abstract", "conclusion": "通过将证据积累框架更原则性地纳入AI的训练和使用中，可以显著改善未来的人机交互。", "translation": "在过去的几十年里，认知神经科学家和行为经济学家已经认识到详细描述决策过程和建模决策随时间出现的重要性。例如，做出决策所需的时间比决策本身更能揭示代理人真实的隐藏偏好。同样，跟踪持续决策过程的数据，如眼球运动或神经记录，包含可以利用的关键信息，即使没有做出决策。本文认为，人工智能（AI）研究将受益于更强烈地关注决策如何随时间出现的见解，并结合相关的过程数据以普遍改进AI预测，特别是人机交互。首先，我们介绍了一个高度成熟的计算框架，该框架假设决策源于证据的嘈杂积累，并展示了心理学、神经科学和经济学中的相关实证工作。接下来，我们讨论了当前多智能体AI方法在多大程度上整合或未整合过程数据和决策模型。最后，我们概述了如何将证据积累框架更原则性地纳入AI的训练和使用中，从而有助于改进未来的人机交互。", "summary": "本文强调了将人类决策过程的详细描述和模型（特别是证据积累框架）整合到人工智能研究中的重要性。作者认为，通过关注决策如何随时间出现并利用过程数据（如反应时间、眼动或神经记录），可以显著提高AI的预测能力并优化人机交互。文章回顾了相关的人类决策研究，并探讨了当前AI方法对此类数据的整合情况，最终提出了将该框架应用于AI训练和使用的策略，以期改善未来的人机交互体验。", "keywords": "决策过程, 证据积累模型, 人机交互, 过程数据, 人工智能", "comments": "本文的创新之处在于提出将认知神经科学和行为经济学中成熟的决策过程模型（特别是证据积累模型）引入到AI研究中，以提升AI在理解和预测人类行为方面的能力，并改善人机交互。它强调了过程数据而非仅仅最终决策结果的重要性，为AI设计提供了一个新颖且有潜力的视角。其重要性在于为构建更智能、更具人类理解力的AI系统指明了方向。"}}
{"id": "2507.12828", "title": "Feature-Enhanced TResNet for Fine-Grained Food Image Classification", "authors": ["Lulu Liu", "Zhiyong Xiao"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12828v2", "summary": "Food is not only essential to human health but also serves as a medium for\ncultural identity and emotional connection. In the context of precision\nnutrition, accurately identifying and classifying food images is critical for\ndietary monitoring, nutrient estimation, and personalized health management.\nHowever, fine-grained food classification remains challenging due to the subtle\nvisual differences among similar dishes. To address this, we propose\nFeature-Enhanced TResNet (FE-TResNet), a novel deep learning model designed to\nimprove the accuracy of food image recognition in fine-grained scenarios. Built\non the TResNet architecture, FE-TResNet integrates a Style-based Recalibration\nModule (StyleRM) and Deep Channel-wise Attention (DCA) to enhance feature\nextraction and emphasize subtle distinctions between food items. Evaluated on\ntwo benchmark Chinese food datasets-ChineseFoodNet and CNFOOD-241-FE-TResNet\nachieved high classification accuracies of 81.37% and 80.29%, respectively.\nThese results demonstrate its effectiveness and highlight its potential as a\nkey enabler for intelligent dietary assessment and personalized recommendations\nin precision nutrition systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12828v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-23", "AI": {"title_translation": "用于细粒度食物图像分类的特征增强型TResNet", "tldr": "FE-TResNet模型通过集成StyleRM和DCA模块，在细粒度食物图像分类任务中取得了高精度。", "motivation": "在精准营养背景下，准确识别和分类食物图像对于膳食监测、营养估算和个性化健康管理至关重要。然而，由于相似菜肴之间细微的视觉差异，细粒度食物分类仍然具有挑战性。", "method": "我们提出了特征增强型TResNet (FE-TResNet)，这是一种基于TResNet架构的深度学习模型。FE-TResNet集成了基于风格的重新校准模块 (StyleRM) 和深度通道注意力 (DCA) 来增强特征提取并强调食物项目之间的细微区别。", "result": "在两个基准中文食物数据集（ChineseFoodNet和CNFOOD-241）上进行评估，FE-TResNet分别实现了81.37%和80.29%的高分类精度。", "conclusion": "这些结果表明了FE-TResNet的有效性，并突出了其作为精准营养系统中智能膳食评估和个性化推荐的关键推动者的潜力。", "translation": "食物不仅对人类健康至关重要，而且是文化认同和情感联系的媒介。在精准营养的背景下，准确识别和分类食物图像对于膳食监测、营养估算和个性化健康管理至关重要。然而，由于相似菜肴之间细微的视觉差异，细粒度食物分类仍然具有挑战性。为了解决这个问题，我们提出了一种新颖的深度学习模型——特征增强型TResNet (FE-TResNet)，旨在提高细粒度场景下食物图像识别的准确性。FE-TResNet建立在TResNet架构之上，集成了基于风格的重新校准模块 (StyleRM) 和深度通道注意力 (DCA)，以增强特征提取并强调食物项目之间的细微区别。在两个基准中文食物数据集——ChineseFoodNet和CNFOOD-241上进行评估，FE-TResNet分别实现了81.37%和80.29%的高分类精度。这些结果证明了其有效性，并突出了其作为精准营养系统中智能膳食评估和个性化推荐的关键推动者的潜力。", "summary": "本研究提出了一种名为特征增强型TResNet (FE-TResNet) 的新型深度学习模型，旨在解决细粒度食物图像分类的挑战。该模型基于TResNet架构，并通过集成Style-based Recalibration Module (StyleRM) 和 Deep Channel-wise Attention (DCA) 来增强特征提取能力，以区分相似食物的细微差异。在ChineseFoodNet和CNFOOD-241这两个中文食物数据集上，FE-TResNet分别达到了81.37%和80.29%的分类精度，验证了其在精准营养领域进行智能膳食评估和个性化推荐的潜力。", "keywords": "细粒度食物分类, TResNet, 特征增强, StyleRM, DCA", "comments": "该论文通过引入StyleRM和DCA模块对TResNet进行增强，有效提升了细粒度食物图像分类的精度，解决了相似食物难以区分的痛点。其创新性在于结合了现有先进架构和注意力机制，以更好地捕捉食物图像的细微特征。这对于精准营养和个性化健康管理具有重要的应用价值。"}}
{"id": "2507.17024", "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances", "authors": ["Chase Stokes", "Kylie Lin", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "H.5.0"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures, accepted to IEEE VIS", "url": "http://arxiv.org/abs/2507.17024v1", "summary": "A growing body of work on visualization affordances highlights how specific\ndesign choices shape reader takeaways from information visualizations. However,\nmapping the relationship between design choices and reader conclusions often\nrequires labor-intensive crowdsourced studies, generating large corpora of\nfree-response text for analysis. To address this challenge, we explored\nalternative scalable research methodologies to assess chart affordances. We\ntest four elicitation methods from human-subject studies: free response,\nvisualization ranking, conclusion ranking, and salience rating, and compare\ntheir effectiveness in eliciting reader interpretations of line charts, dot\nplots, and heatmaps. Overall, we find that while no method fully replicates\naffordances observed in free-response conclusions, combinations of ranking and\nrating methods can serve as an effective proxy at a broad scale. The two\nranking methodologies were influenced by participant bias towards certain chart\ntypes and the comparison of suggested conclusions. Rating conclusion salience\ncould not capture the specific variations between chart types observed in the\nother methods. To supplement this work, we present a case study with GPT-4o,\nexploring the use of large language models (LLMs) to elicit human-like chart\ninterpretations. This aligns with recent academic interest in leveraging LLMs\nas proxies for human participants to improve data collection and analysis\nefficiency. GPT-4o performed best as a human proxy for the salience rating\nmethodology but suffered from severe constraints in other areas. Overall, the\ndiscrepancies in affordances we found between various elicitation\nmethodologies, including GPT-4o, highlight the importance of intentionally\nselecting and combining methods and evaluating trade-offs.", "comment": "11 pages, 8 figures, accepted to IEEE VIS", "pdf_url": "http://arxiv.org/pdf/2507.17024v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "写、排序或评分：可视化可供性研究方法的比较", "tldr": "研究比较了四种评估可视化可供性的方法（自由响应、可视化排序、结论排序、显著性评分），发现排序和评分组合可作为自由响应的有效替代，并探讨了GPT-4o作为人类代理的潜力，强调了方法选择的重要性。", "motivation": "现有研究需要劳动密集型众包研究来映射设计选择与读者结论之间的关系，生成大量自由响应文本进行分析。本文旨在探索可扩展的替代研究方法来评估图表可供性。", "method": "测试了四种来自人类受试者研究的启发方法：自由响应、可视化排序、结论排序和显著性评分。比较了它们在引发读者对折线图、散点图和热力图解释方面的有效性。此外，还通过GPT-4o进行了案例研究，探索LLM在引发类人图表解释方面的应用，将其作为人类代理。", "result": "没有一种方法能完全复制自由响应结论中观察到的可供性。排序和评分方法的组合可以在广泛范围内作为有效替代。两种排序方法受参与者对特定图表类型和建议结论比较的偏见影响。结论显著性评分未能捕捉到其他方法中观察到的图表类型之间的具体差异。GPT-4o在显著性评分方法中表现最佳，但在其他方面存在严重限制。各种启发方法（包括GPT-4o）之间发现的可供性差异突出了有意选择和组合方法以及评估权衡的重要性。", "conclusion": "研究表明，没有单一方法能完全复制自由响应的可供性，但排序和评分方法的组合是可行的替代方案。LLM（如GPT-4o）在某些情况下可作为人类代理，但在其他方面仍有限制。方法选择和组合的权衡至关重要。", "translation": "关于可视化可供性日益增多的工作强调了特定的设计选择如何影响信息可视化的读者理解。然而，绘制设计选择与读者结论之间关系通常需要劳动密集型众包研究，产生大量自由响应文本进行分析。为了解决这一挑战，我们探索了评估图表可供性的替代可扩展研究方法。我们测试了来自人类受试者研究的四种启发方法：自由响应、可视化排序、结论排序和显著性评分，并比较了它们在引发读者对折线图、散点图和热力图解释方面的有效性。总的来说，我们发现虽然没有一种方法能完全复制自由响应结论中观察到的可供性，但排序和评分方法的组合可以在广泛范围内作为有效的代理。两种排序方法受到参与者对某些图表类型和建议结论比较的偏见影响。结论显著性评分未能捕捉到其他方法中观察到的图表类型之间的具体差异。为了补充这项工作，我们提出了一个使用GPT-4o的案例研究，探索使用大型语言模型（LLM）来引发类人图表解释。这与最近学术界对利用LLM作为人类参与者代理以提高数据收集和分析效率的兴趣相符。GPT-4o在显著性评分方法中作为人类代理表现最佳，但在其他方面受到严重限制。总的来说，我们在各种启发方法（包括GPT-4o）之间发现的可供性差异，突出了有意选择和组合方法以及评估权衡的重要性。", "summary": "本研究比较了四种评估可视化可供性的方法：自由响应、可视化排序、结论排序和显著性评分。旨在寻找替代劳动密集型众包研究的可扩展方法。结果显示，虽然没有单一方法能完全复制自由响应结果，但排序和评分方法的组合可作为有效替代。研究还探讨了GPT-4o作为人类代理在图表解释中的潜力，发现其在显著性评分中表现较好，但在其他方面有局限。最终强调了在研究可视化可供性时，精心选择和组合不同方法的重要性及其权衡。", "keywords": "可视化可供性, 研究方法, 大语言模型, GPT-4o, 数据可视化", "comments": "这项工作的重要性在于它提出了评估可视化可供性的可扩展替代方案，从而可能降低研究成本和时间。通过比较多种传统方法并引入LLM作为代理进行探索，该研究为未来可视化研究方法提供了宝贵的见解。其创新之处在于系统性地对比了不同启发方法的优缺点，并率先探讨了LLM在此领域的应用潜力。然而，研究也指出LLM在某些方面的局限性，提示其尚不能完全替代人类。这项工作对于推动可视化研究方法学的发展具有重要意义，特别是对于大规模研究设计。"}}
{"id": "2507.17071", "title": "Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation", "authors": ["Juntao Lin", "Xianghao Zhan"], "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY", "physics.ins-det"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.17071v1", "summary": "Due to environmental changes and sensor aging, sensor drift challenges the\nperformance of electronic nose systems in gas classification during real-world\ndeployment. Previous studies using the UCI Gas Sensor Array Drift Dataset\nreported promising drift compensation results but lacked robust statistical\nexperimental validation and may overcompensate for sensor drift, losing\nclass-related variance.To address these limitations and improve sensor drift\ncompensation with statistical rigor, we first designed two domain adaptation\ntasks based on the same electronic nose dataset: using the first batch to\npredict the remaining batches, simulating a controlled laboratory setting; and\npredicting the next batch using all prior batches, simulating continuous\ntraining data updates for online training. We then systematically tested three\nmethods: our proposed novel Knowledge Distillation (KD) method, the benchmark\nmethod Domain Regularized Component Analysis (DRCA), and a hybrid method\nKD-DRCA, across 30 random test set partitions on the UCI dataset. We showed\nthat KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%\nimprovement in accuracy and 15% in F1-score, demonstrating KD's superior\neffectiveness in drift compensation. This is the first application of KD for\nelectronic nose drift mitigation, significantly outperforming the previous\nstate-of-the-art DRCA method and enhancing the reliability of sensor drift\ncompensation in real-world environments.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.17071v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于知识蒸馏的电子鼻气体识别传感器漂移补偿", "tldr": "本研究首次将知识蒸馏（KD）应用于电子鼻传感器漂移补偿，相比现有方法显著提升了气体识别性能。", "motivation": "由于环境变化和传感器老化，传感器漂移严重影响了电子鼻系统在实际部署中气体分类的性能。以往的研究虽然在UCI气体传感器阵列漂移数据集上取得了有希望的漂移补偿结果，但缺乏稳健的统计实验验证，且可能过度补偿传感器漂移，导致丢失与类别相关的方差。", "method": "为了解决现有方法的局限性并提高传感器漂移补偿的统计严谨性，研究者首先基于同一电子鼻数据集设计了两种域适应任务：使用第一个批次预测剩余批次（模拟受控实验室设置），以及使用所有先前批次预测下一个批次（模拟在线训练的连续数据更新）。然后，系统地测试了三种方法：提出的知识蒸馏（KD）方法、基准方法域正则化主成分分析（DRCA）以及混合方法KD-DRCA，并在UCI数据集的30个随机测试集分区上进行了实验。", "result": "知识蒸馏（KD）方法始终优于DRCA和KD-DRCA，在准确率上最高提升了18%，在F1分数上最高提升了15%，证明了KD在漂移补偿方面的卓越有效性。", "conclusion": "知识蒸馏是首次应用于电子鼻漂移缓解的方法，它显著优于先前的最先进的DRCA方法，并增强了传感器漂移补偿在实际环境中的可靠性。", "translation": "由于环境变化和传感器老化，传感器漂移对电子鼻系统在实际部署中的气体分类性能构成了挑战。以往使用UCI气体传感器阵列漂移数据集的研究报告了有前景的漂移补偿结果，但缺乏稳健的统计实验验证，并且可能过度补偿传感器漂移，从而丢失与类别相关的方差。为了解决这些局限性并以统计学严谨性改进传感器漂移补偿，我们首先基于同一电子鼻数据集设计了两个域适应任务：使用第一个批次预测剩余批次，模拟受控实验室环境；以及使用所有先前批次预测下一个批次，模拟在线训练的连续训练数据更新。然后，我们系统地测试了三种方法：我们提出的新型知识蒸馏（KD）方法、基准方法域正则化主成分分析（DRCA）以及混合方法KD-DRCA，在UCI数据集的30个随机测试集分区上进行了实验。结果表明，KD始终优于DRCA和KD-DRCA，准确率最高提升了18%，F1分数最高提升了15%，证明了KD在漂移补偿方面的卓越有效性。这是知识蒸馏首次应用于电子鼻漂移缓解，显著优于先前的最先进的DRCA方法，并增强了传感器漂移补偿在实际环境中的可靠性。", "summary": "本研究旨在解决电子鼻系统在气体识别中因传感器漂移导致的性能下降问题。针对现有方法在统计验证和过度补偿方面的不足，作者设计了两种域适应任务并系统比较了知识蒸馏（KD）、域正则化主成分分析（DRCA）以及混合方法KD-DRCA。实验结果表明，KD方法在准确率和F1分数上均显著优于其他方法，最高分别提升18%和15%。这是知识蒸馏首次应用于电子鼻漂移补偿，为实际应用中的传感器漂移缓解提供了更可靠的解决方案。", "keywords": "传感器漂移, 电子鼻, 知识蒸馏, 域适应, 气体识别", "comments": "该论文的创新点在于首次将知识蒸馏（KD）应用于电子鼻传感器漂移补偿领域，并取得了显著优于现有SOTA方法DRCA的性能。其重要性体现在为实际部署中电子鼻系统的可靠性提供了新的、更有效的方法，解决了传感器漂移这一核心挑战。研究设计了两种域适应任务，模拟了不同的实际应用场景，增加了研究的普适性和严谨性。"}}
{"id": "2507.17011", "title": "An Energy-Autonomous and Battery-Free Resistive Sensor using a Time-Domain to Digital Conversion with Bluetooth Low Energy connectivity", "authors": ["Mario Costanza", "Antonino Pagano", "Samuel Margueron", "Ilenia Tinnirello", "Roberto La Rosa"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages, 6 figures. This work has been accepted for publication in IEEE ISCAS 2024. The final version is available at: this https URL", "url": "http://arxiv.org/abs/2507.17011v1", "summary": "This paper introduces an innovative Energy-Autonomous Wireless Sensing Node\n(EAWSN) that addresses power constraints by harnessing ambient light for\nenergy. It combines this energy harvesting capability with the Time Domain to\nDigital Conversion (TDDC) technique for efficient and accurate measurements of\nresistive sensors. Bluetooth Low Energy (BLE) communication ensures data can be\ntransmitted wirelessly to a base station, providing a promising solution for\nvarious applications, particularly in environments with limited access to wired\npower sources, enabling long-term, maintenance-free operation by eliminating\nbatteries. Experimental results showed a linear relationship between the test\nresistance R_m and the measured number of clock pulses N_m within the sensor's\noperating range.", "comment": "5 pages, 6 figures. This work has been accepted for publication in\n  IEEE ISCAS 2024. The final version is available at:\n  https://ieeexplore.ieee.org/abstract/document/10558483/authors#authors", "pdf_url": "http://arxiv.org/pdf/2507.17011v1", "cate": "eess.SY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "一种采用时域到数字转换和低功耗蓝牙连接的能量自给自足、无电池电阻传感器", "tldr": "本文介绍了一种创新的能量自给自足无线传感节点（EAWSN），它利用环境光获取能量，结合时域到数字转换（TDDC）技术进行电阻传感器测量，并通过低功耗蓝牙（BLE）进行无线数据传输，实现了无需电池的长期免维护运行。", "motivation": "该研究旨在解决传统无线传感节点面临的电源限制问题，特别是在难以获取有线电源的环境中，通过消除电池来实现传感器的长期、免维护运行。", "method": "该方法结合了环境光能量收集、用于电阻传感器高效精确测量的时域到数字转换（TDDC）技术，以及用于无线数据传输的低功耗蓝牙（BLE）通信。", "result": "实验结果表明，在传感器的工作范围内，测试电阻R_m与测得的时钟脉冲数N_m之间存在线性关系。", "conclusion": "该能量自给自足的无线传感节点为各种应用，特别是在电源受限环境中，提供了一个有前景的解决方案，通过消除电池实现了长期、免维护的运行。", "translation": "本文介绍了一种创新的能量自给自足无线传感节点（EAWSN），它通过利用环境光获取能量来解决电源限制问题。它将这种能量收集能力与时域到数字转换（TDDC）技术相结合，用于电阻传感器的高效准确测量。低功耗蓝牙（BLE）通信确保数据可以无线传输到基站，为各种应用提供了有前景的解决方案，特别是在有线电源访问受限的环境中，通过消除电池实现长期、免维护运行。实验结果表明，在传感器的工作范围内，测试电阻R_m与测得的时钟脉冲数N_m之间存在线性关系。", "summary": "本文提出了一种创新的能量自给自足无线传感节点（EAWSN），该节点通过环境光收集能量，并利用时域到数字转换（TDDC）技术对电阻传感器进行精确测量。结合低功耗蓝牙（BLE）通信，该系统无需电池即可实现数据的无线传输，为电源受限环境中的长期、免维护传感应用提供了有效方案。实验验证了其电阻测量结果的线性关系。", "keywords": "能量自给自足, 无电池, 电阻传感器, 时域到数字转换, 低功耗蓝牙", "comments": "该论文的创新点在于将环境光能量收集、时域到数字转换技术与低功耗蓝牙通信巧妙结合，实现了完全能量自给自足且无电池的无线电阻传感器，这对于需要长期部署且维护成本高的应用场景具有重要意义。其免维护和环保特性是显著优势。"}}
{"id": "2507.17288", "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "categories": ["cs.CL", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17288v1", "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17288v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Triple X：一个基于LLM的多语言语音识别系统，用于INTERSPEECH2025 MLC-SLM挑战赛", "tldr": "本文介绍了Triple X语音识别系统，该系统采用创新的编码器-适配器-LLM架构和多阶段训练策略，在多语言会话场景中实现了竞争性的语音识别准确率，并在MLC-SLM挑战赛中获得第二名。", "motivation": "该研究旨在通过优化多语言会话场景中的语音识别准确率，以应对多语言会话语音语言建模（MLC-SLM）挑战。", "method": "该系统采用创新的编码器-适配器-LLM（大型语言模型）架构，并结合领域特定适应。此外，还采用了精心设计的多阶段训练策略，利用大量多语言音频数据集来增强多语言识别性能。", "result": "实验结果表明，该方法在开发集和测试集上均获得了有竞争力的词错误率（WER）性能，并在挑战赛排名中获得第二名。", "conclusion": "Triple X系统通过其创新的架构和训练策略，在多语言会话语音识别任务中表现出色，达到了领先水平。", "translation": "本文介绍了我们提交给多语言会话语音语言建模（MLC-SLM）挑战赛任务1的Triple X语音识别系统。我们的工作重点是通过创新的编码器-适配器-LLM架构，优化多语言会话场景中的语音识别准确率。该框架利用了基于文本的大型语言模型的强大推理能力，同时融入了领域特定适应。为了进一步增强多语言识别性能，我们采用了精心设计的多阶段训练策略，利用了大量的多语言音频数据集。实验结果表明，我们的方法在开发集和测试集上均获得了有竞争力的词错误率（WER）性能，并在挑战赛排名中获得第二名。", "summary": "本文介绍了Triple X语音识别系统，该系统是为多语言会话语音语言建模（MLC-SLM）挑战赛任务1提交的。该系统专注于通过创新的编码器-适配器-LLM架构优化多语言会话场景中的语音识别准确率，该架构利用了基于文本的大型语言模型的强大推理能力，并融入了领域特定适应。为了进一步提升多语言识别性能，研究人员采用了精心设计的多阶段训练策略，并利用了大量的多语言音频数据集。实验结果表明，该方法在开发集和测试集上均获得了有竞争力的词错误率（WER）性能，并在挑战赛排名中获得第二名。", "keywords": "多语言语音识别, 大型语言模型（LLM）, 编码器-适配器架构, MLC-SLM挑战赛, 词错误率（WER）", "comments": "Triple X系统通过将LLM的强大推理能力与领域特定适应相结合的创新编码器-适配器-LLM架构，在多语言语音识别领域取得了显著进展。其在INTERSPEECH2025 MLC-SLM挑战赛中获得第二名的成绩，证明了其在实际多语言会话场景中的有效性和竞争力。该研究为未来基于LLM的多语言语音识别系统的发展提供了宝贵的经验。"}}
{"id": "2506.02951", "title": "Adaptive Graph Pruning for Multi-Agent Communication", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "categories": ["cs.CL", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ECAI 2025", "url": "http://arxiv.org/abs/2506.02951v3", "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "comment": "ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.02951v3", "cate": "cs.CL", "date": "2025-06-03", "updated": "2025-07-23", "AI": {"title_translation": "多智能体通信的自适应图剪枝", "tldr": "提出AGP框架，通过联合优化智能体数量和通信拓扑，使多智能体系统能自适应不同任务复杂度，显著提升性能并降低资源消耗。", "motivation": "当前LLM多智能体系统在处理不同任务复杂性时，因依赖固定数量的智能体和静态通信结构而缺乏适应性。", "method": "本文提出自适应图剪枝（AGP）框架，通过两阶段训练策略联合优化智能体数量（硬剪枝）和通信拓扑（软剪枝）。第一阶段独立训练不同智能体数量下的软剪枝网络以确定最佳图和位置掩码；第二阶段在最大完整图中联合优化硬剪枝和软剪枝，动态配置智能体数量及其通信拓扑。", "result": "1. 高性能：在六个基准测试中达到最先进水平，性能提升2.58%~9.84%，并能泛化到多个主流LLM架构。2. 任务自适应：能为特定任务动态构建优化的通信拓扑，在通用推理、数学推理和代码生成三类任务中表现出色。3. 节省Token：训练步骤更少，Token消耗降低90%以上。4. 训练高效：仅需少量训练步骤即可达到高性能，在约十个训练步骤后即可超越现有基线。", "conclusion": "自适应图剪枝（AGP）框架通过动态调整智能体数量和通信拓扑，显著提高了LLM多智能体系统在各种任务中的性能、适应性、Token效率和训练效率。", "translation": "基于大型语言模型（LLM）的多智能体系统在各种任务中表现出色，尤其是在通过协作通信增强时。然而，当前方法通常依赖于固定数量的智能体和静态通信结构，限制了它们适应不同任务复杂性的能力。在本文中，我们提出了自适应图剪枝（AGP），这是一种新颖的任务自适应多智能体协作框架，它联合优化智能体数量（硬剪枝）和通信拓扑（软剪枝）。具体而言，我们的方法采用两阶段训练策略：首先，独立训练不同智能体数量的软剪枝网络，以确定特定任务的最佳智能体数量相关完整图和位置掩码；然后，在最大完整图中联合优化硬剪枝和软剪枝，以动态配置每个任务的智能体数量及其通信拓扑。大量实验表明，我们的方法：（1）高性能：在六个基准测试中实现了最先进的结果，并持续泛化到多个主流LLM架构，性能提升了2.58%~9.84%；（2）任务自适应：为特定任务动态构建优化的通信拓扑，在所有三类任务（通用推理、数学推理和代码生成）中表现出极高的性能；（3）节省Token：同时具有更少的训练步骤和Token消耗，Token消耗减少了90%以上；（4）训练高效：与其他方法相比，仅用非常少的训练步骤就实现了高性能。在六个基准测试下，性能在大约十个训练步骤后将超越现有基线。", "summary": "本文提出了一种名为自适应图剪枝（AGP）的新型框架，旨在解决现有LLM多智能体系统在处理不同任务复杂性时缺乏适应性的问题。AGP通过两阶段训练策略，联合优化智能体数量（硬剪枝）和通信拓扑（软剪枝），从而动态调整多智能体系统的结构。实验证明，AGP在多个基准测试中实现了最先进的性能，显著提升了任务自适应性、Token效率和训练效率，且能泛化到不同的LLM架构。", "keywords": "多智能体系统, 图剪枝, 动态通信, LLM, 任务自适应", "comments": "这项工作通过引入动态的智能体数量和通信拓扑优化，有效解决了LLM多智能体系统在适应不同任务复杂性方面的局限性。其创新点在于结合了硬剪枝和软剪枝的策略，实现了性能提升和资源节约的双重优势，特别是在Token消耗上的显著降低，这对于大型模型应用至关重要。该方法在实际应用中具有很高的价值。"}}
{"id": "2507.17038", "title": "Transformer Based Building Boundary Reconstruction using Attraction Field Maps", "authors": ["Muhammad Kamran", "Mohammad Moein Sheikholeslami", "Andreas Wichmann", "Gunho Sohn"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17038v1", "summary": "In recent years, the number of remote satellites orbiting the Earth has grown\nsignificantly, streaming vast amounts of high-resolution visual data to support\ndiverse applications across civil, public, and military domains. Among these\napplications, the generation and updating of spatial maps of the built\nenvironment have become critical due to the extensive coverage and detailed\nimagery provided by satellites. However, reconstructing spatial maps from\nsatellite imagery is a complex computer vision task, requiring the creation of\nhigh-level object representations, such as primitives, to accurately capture\nthe built environment. While the past decade has witnessed remarkable\nadvancements in object detection and representation using visual data,\nprimitives-based object representation remains a persistent challenge in\ncomputer vision. Consequently, high-quality spatial maps often rely on\nlabor-intensive and manual processes. This paper introduces a novel deep\nlearning methodology leveraging Graph Convolutional Networks (GCNs) to address\nthese challenges in building footprint reconstruction. The proposed approach\nenhances performance by incorporating geometric regularity into building\nboundaries, integrating multi-scale and multi-resolution features, and\nembedding Attraction Field Maps into the network. These innovations provide a\nscalable and precise solution for automated building footprint extraction from\na single satellite image, paving the way for impactful applications in urban\nplanning, disaster management, and large-scale spatial analysis. Our model,\nDecoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,\ndemonstrating its ability to deliver accurate and regularized building\nfootprints across diverse and challenging scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17038v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于注意力场图的Transformer建筑物边界重建", "tldr": "本文提出了一种名为Decoupled-PolyGCN的新型深度学习方法，利用图卷积网络和注意力场图，从卫星图像中自动精确地重建建筑物边界，性能优于现有方法。", "motivation": "由于卫星图像覆盖范围广、细节丰富，生成和更新建筑环境的空间地图变得至关重要。然而，从卫星图像重建空间地图是一项复杂的计算机视觉任务，特别是基于基元的对象表示仍然是一个持续的挑战，导致高质量空间地图的生成往往依赖于劳动密集型的手动过程。", "method": "本文提出了一种新颖的深度学习方法，利用图卷积网络（GCNs）来解决建筑物足迹重建的挑战。该方法通过将几何规则性融入建筑物边界、整合多尺度和多分辨率特征，并将吸引场图（Attraction Field Maps）嵌入网络中来提高性能。该模型名为Decoupled-PolyGCN。", "result": "本文提出的Decoupled-PolyGCN模型在AP（平均精度）上比现有方法高出6%，在AR（平均召回率）上高出10%，这表明它能够在多样化和具有挑战性的场景中提供准确和规则化的建筑物足迹。", "conclusion": "本文提出的方法为从单一卫星图像中自动提取建筑物足迹提供了一个可扩展且精确的解决方案，为城市规划、灾害管理和大规模空间分析等重要应用铺平了道路。", "translation": "近年来，绕地球运行的遥感卫星数量显著增长，传输了大量高分辨率视觉数据，以支持民用、公共和军事领域的各种应用。在这些应用中，由于卫星提供了广泛的覆盖范围和详细的图像，建筑环境空间地图的生成和更新变得至关重要。然而，从卫星图像重建空间地图是一项复杂的计算机视觉任务，需要创建高级别的对象表示（例如基元）以准确捕获建筑环境。尽管过去十年在利用视觉数据进行对象检测和表示方面取得了显著进展，但基于基元的对象表示仍然是计算机视觉领域的一个持续挑战。因此，高质量的空间地图通常依赖于劳动密集型和手动过程。本文引入了一种新颖的深度学习方法，利用图卷积网络（GCNs）来解决建筑物足迹重建中的这些挑战。所提出的方法通过将几何规则性融入建筑物边界、整合多尺度和多分辨率特征，并将吸引场图嵌入网络中来提高性能。这些创新为从单一卫星图像中自动提取建筑物足迹提供了一个可扩展且精确的解决方案，为城市规划、灾害管理和大规模空间分析等重要应用铺平了道路。我们的模型Decoupled-PolyGCN在AP（平均精度）上比现有方法高出6%，在AR（平均召回率）上高出10%，这表明它能够在多样化和具有挑战性的场景中提供准确和规则化的建筑物足迹。", "summary": "本文提出了一种名为Decoupled-PolyGCN的新型深度学习方法，专门用于从高分辨率卫星图像中自动精确地重建建筑物边界。该方法利用图卷积网络（GCNs），并通过整合几何规则性、多尺度/多分辨率特征以及吸引场图来增强性能。实验结果表明，Decoupled-PolyGCN在平均精度（AP）和平均召回率（AR）方面分别比现有方法提高了6%和10%，为城市规划和灾害管理等领域提供了高效的建筑物足迹提取方案。", "keywords": "建筑物边界重建, 图卷积网络, 吸引场图, 卫星图像, 深度学习", "comments": "本文提出了一种创新的深度学习方法Decoupled-PolyGCN，通过结合GCNs和吸引场图，有效地解决了建筑物边界重建中基于基元表示的长期挑战。其在几何规则性、多尺度特征整合方面的创新，以及显著优于现有方法的性能提升，使其在自动化空间地图生成领域具有重要意义，尤其在城市规划和灾害管理等实际应用中潜力巨大。"}}
{"id": "2301.12309", "title": "On the Lipschitz Constant of Deep Networks and Double Descent", "authors": ["Matteo Gamba", "Hossein Azizpour", "Mårten Björkman"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2301.12309v5", "summary": "Existing bounds on the generalization error of deep networks assume some form\nof smooth or bounded dependence on the input variable, falling short of\ninvestigating the mechanisms controlling such factors in practice. In this\nwork, we present an extensive experimental study of the empirical Lipschitz\nconstant of deep networks undergoing double descent, and highlight\nnon-monotonic trends strongly correlating with the test error. Building a\nconnection between parameter-space and input-space gradients for SGD around a\ncritical point, we isolate two important factors -- namely loss landscape\ncurvature and distance of parameters from initialization -- respectively\ncontrolling optimization dynamics around a critical point and bounding model\nfunction complexity, even beyond the training data. Our study presents novels\ninsights on implicit regularization via overparameterization, and effective\nmodel complexity for networks trained in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2301.12309v5", "cate": "cs.LG", "date": "2023-01-28", "updated": "2025-07-23", "AI": {"title_translation": "深度网络和双重下降的Lipschitz常数研究", "tldr": "本文对深度网络在双重下降现象中经验Lipschitz常数进行了广泛的实验研究，发现其非单调趋势与测试误差强相关，并分离出控制优化动力学和模型复杂度的两个关键因素。", "motivation": "现有关于深度网络泛化误差的界限未能深入研究实践中控制平滑度或有界依赖性的机制，因此本文旨在通过实验研究经验Lipschitz常数来探究这些机制。", "method": "本文进行了广泛的实验研究，分析了深度网络在双重下降过程中经验Lipschitz常数的变化趋势。通过连接参数空间和输入空间梯度，分离出损失景观曲率和参数距初始化距离这两个关键因素。", "result": "研究发现经验Lipschitz常数呈现非单调趋势，并与测试误差强相关。损失景观曲率控制关键点附近的优化动力学，而参数距初始化距离则限制模型函数复杂度，甚至超出训练数据。", "conclusion": "本文对过参数化带来的隐式正则化以及实践中训练网络的有效模型复杂度提供了新的见解。", "translation": "现有关于深度网络泛化误差的界限假设了某种形式的平滑或对输入变量的有界依赖，未能深入研究实践中控制这些因素的机制。在这项工作中，我们对经历双重下降的深度网络的经验Lipschitz常数进行了广泛的实验研究，并强调了与测试误差强烈相关的非单调趋势。通过建立参数空间和输入空间梯度在临界点附近对SGD的连接，我们分离出两个重要因素——即损失景观曲率和参数距初始化的距离——分别控制临界点附近的优化动力学和限制模型函数复杂度，甚至超出了训练数据。我们的研究为过参数化带来的隐式正则化以及实践中训练网络的有效模型复杂度提供了新颖的见解。", "summary": "本文通过广泛的实验研究，探讨了深度网络在双重下降现象中的经验Lipschitz常数，并发现其非单调变化趋势与测试误差高度相关。研究进一步揭示了损失景观曲率和参数距初始化距离是影响优化动力学和模型复杂度的两个关键因素，为理解过参数化下的隐式正则化和有效模型复杂度提供了新颖的视角。", "keywords": "Lipschitz常数, 深度网络, 双重下降, 泛化误差, 隐式正则化", "comments": "本文通过实证研究揭示了深度网络Lipschitz常数在双重下降现象中的复杂行为，并识别出影响模型复杂度和优化过程的关键因素。这项工作为理解深度学习中的泛化能力和隐式正则化提供了宝贵的实验证据和理论洞察。"}}
{"id": "2504.03172", "title": "Bayesian Optimization of Robustness Measures under Input Uncertainty: A Randomized Gaussian Process Upper Confidence Bound Approach", "authors": ["Yu Inatsu"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      50 pages, 4 figures", "url": "http://arxiv.org/abs/2504.03172v2", "summary": "Bayesian optimization based on the Gaussian process upper confidence bound\n(GP-UCB) offers a theoretical guarantee for optimizing black-box functions. In\npractice, however, black-box functions often involve input uncertainty. To\nhandle such cases, GP-UCB can be extended to optimize evaluation criteria known\nas robustness measures. However, GP-UCB-based methods for robustness measures\nrequire a trade-off parameter, $\\beta$, which, as in the original GP-UCB, must\nbe set sufficiently large to ensure theoretical validity. In this study, we\npropose randomized robustness measure GP-UCB (RRGP-UCB), a novel method that\nsamples $\\beta$ from a chi-squared-based probability distribution. This\napproach eliminates the need to explicitly specify $\\beta$. Notably, the\nexpected value of $\\beta$ under this distribution is not excessively large.\nFurthermore, we show that RRGP-UCB provides tight bounds on the expected regret\nbetween the optimal and estimated solutions. Numerical experiments demonstrate\nthe effectiveness of the proposed method.", "comment": "50 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2504.03172v2", "cate": "stat.ML", "date": "2025-04-04", "updated": "2025-07-23", "AI": {"title_translation": "输入不确定性下鲁棒性度量的贝叶斯优化：一种随机高斯过程上置信界方法", "tldr": "针对输入不确定性下的鲁棒性度量优化问题，本文提出了一种新的随机化高斯过程上置信界方法（RRGP-UCB），通过从卡方分布中采样参数β，避免了手动设置该参数，并提供了紧密的后悔界限。", "motivation": "传统的基于高斯过程上置信界（GP-UCB）的贝叶斯优化方法在处理具有输入不确定性的黑盒函数时，需要优化鲁棒性度量。然而，这类方法需要手动设置一个权衡参数β，且为了保证理论有效性，该参数必须设置得足够大，这在实践中并不理想。", "method": "本研究提出了一种名为随机鲁棒性度量GP-UCB（RRGP-UCB）的新方法。该方法通过从基于卡方分布的概率分布中采样参数β来消除显式指定β的需要。", "result": "RRGP-UCB方法消除了手动指定参数β的需求，并且在此分布下，β的期望值不会过大。此外，该方法在最优解和估计解之间的预期后悔值上提供了紧密的界限。数值实验证明了所提出方法的有效性。", "conclusion": "RRGP-UCB是一种有效且理论上可靠的方法，用于在输入不确定性下优化鲁棒性度量，它通过随机采样关键参数β，解决了传统GP-UCB方法的局限性。", "translation": "基于高斯过程上置信界（GP-UCB）的贝叶斯优化为优化黑盒函数提供了理论保证。然而，在实践中，黑盒函数通常涉及输入不确定性。为了处理这种情况，GP-UCB可以扩展到优化被称为鲁棒性度量的评估标准。然而，基于GP-UCB的鲁棒性度量方法需要一个权衡参数β，与原始GP-UCB一样，该参数必须设置得足够大以确保理论有效性。在本研究中，我们提出了一种新的方法，即随机鲁棒性度量GP-UCB（RRGP-UCB），该方法从基于卡方分布的概率分布中采样β。这种方法消除了显式指定β的需要。值得注意的是，在该分布下β的期望值不会过大。此外，我们证明了RRGP-UCB在最优解和估计解之间的预期后悔值上提供了紧密的界限。数值实验证明了所提出方法的有效性。", "summary": "本文提出了一种新的贝叶斯优化方法——随机鲁棒性度量高斯过程上置信界（RRGP-UCB），用于在输入存在不确定性时优化鲁棒性度量。该方法通过从卡方分布中随机采样关键参数β，从而避免了传统GP-UCB方法中需要手动设置一个过大的β值的问题。研究表明，RRGP-UCB能够提供紧密的预期后悔界限，并且数值实验验证了其有效性。", "keywords": "贝叶斯优化, 高斯过程, 鲁棒性度量, 输入不确定性, 后悔界限", "comments": "本文的创新点在于提出了一种随机化方法来处理GP-UCB中关键参数β的设置问题，通过采样而非手动指定，解决了实践中β值难以确定且需设得过大的局限性。这对于在存在输入不确定性的实际应用中，提升贝叶斯优化的易用性和性能具有重要意义。"}}
{"id": "2404.19664", "title": "Towards Generalist Robot Learning from Internet Video: A Survey", "authors": ["Robert McCarthy", "Daniel C. H. Tan", "Dominik Schmidt", "Fernando Acero", "Nathan Herr", "Yilun Du", "Thomas G. Thuruthel", "Zhibin Li"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.19664v5", "summary": "Scaling deep learning to massive and diverse internet data has driven\nremarkable breakthroughs in domains such as video generation and natural\nlanguage processing. Robot learning, however, has thus far failed to replicate\nthis success and remains constrained by a scarcity of available data. Learning\nfrom videos (LfV) methods aim to address this data bottleneck by augmenting\ntraditional robot data with large-scale internet video. This video data\nprovides foundational information regarding physical dynamics, behaviours, and\ntasks, and can be highly informative for general-purpose robots.\n  This survey systematically examines the emerging field of LfV. We first\noutline essential concepts, including detailing fundamental LfV challenges such\nas distribution shift and missing action labels in video data. Next, we\ncomprehensively review current methods for extracting knowledge from\nlarge-scale internet video, overcoming LfV challenges, and improving robot\nlearning through video-informed training. The survey concludes with a critical\ndiscussion of future opportunities. Here, we emphasize the need for scalable\nfoundation model approaches that can leverage the full range of available\ninternet video and enhance the learning of robot policies and dynamics models.\nOverall, the survey aims to inform and catalyse future LfV research, driving\nprogress towards general-purpose robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.19664v5", "cate": "cs.RO", "date": "2024-04-30", "updated": "2025-07-23", "AI": {"title_translation": "迈向基于互联网视频的通用机器人学习：一项综述", "tldr": "本综述系统性地探讨了从互联网视频中学习（LfV）以克服机器人学习数据稀缺性的新兴领域，概述了挑战、现有方法，并强调了未来对可扩展基础模型的需求，以实现通用机器人。", "motivation": "机器人学习目前受限于数据稀缺性，未能像视频生成和自然语言处理那样取得突破性进展。从视频中学习（LfV）旨在通过利用大规模互联网视频来解决这一数据瓶颈，为通用机器人提供物理动力学、行为和任务的基础信息。", "method": "本综述系统性地审视了LfV这一新兴领域。首先，它概述了基本概念，包括详细阐述了如分布偏移和视频数据中缺少动作标签等LfV的根本挑战。其次，它全面回顾了当前从大规模互联网视频中提取知识、克服LfV挑战以及通过视频信息训练改进机器人学习的方法。", "result": "本综述全面概述了从视频中学习（LfV）的现状，包括其基本概念、面临的挑战（如分布偏移和动作标签缺失）以及克服这些挑战的现有方法。它还强调了未来利用可扩展基础模型来充分利用互联网视频以增强机器人策略和动力学模型学习的机会。", "conclusion": "本综述强调，未来的LfV研究需要可扩展的基础模型方法，以充分利用各种可用的互联网视频，并提升机器人策略和动力学模型的学习能力，从而推动通用机器人技术的发展。本综述旨在为未来的LfV研究提供信息并催化其进展。", "translation": "将深度学习扩展到大规模多样化的互联网数据，已在视频生成和自然语言处理等领域取得了显著突破。然而，机器人学习迄今未能复制这种成功，并且仍然受到可用数据稀缺性的限制。从视频中学习（LfV）方法旨在通过用大规模互联网视频补充传统机器人数据来解决这一数据瓶颈。这些视频数据提供了关于物理动力学、行为和任务的基础信息，对通用机器人非常有益。\n本综述系统地审视了LfV这一新兴领域。我们首先概述了基本概念，包括详细阐述了如分布偏移和视频数据中缺少动作标签等LfV的根本挑战。接下来，我们全面回顾了当前从大规模互联网视频中提取知识、克服LfV挑战以及通过视频信息训练改进机器人学习的方法。本综述最后对未来的机遇进行了批判性讨论。在此，我们强调需要可扩展的基础模型方法，这些方法可以利用全部可用的互联网视频并增强机器人策略和动力学模型的学习。总的来说，本综述旨在为未来的LfV研究提供信息并催化其进展，推动通用机器人技术的发展。", "summary": "本综述全面审视了从互联网视频中学习（LfV）这一新兴领域，旨在解决机器人学习面临的数据稀缺性问题。文章详细阐述了LfV的基本概念、挑战（如数据分布偏移和动作标签缺失），并系统回顾了现有方法，以从大规模互联网视频中提取知识并改进机器人学习。最后，综述强调了开发可扩展基础模型的重要性，以充分利用互联网视频资源，从而推动通用机器人技术的发展。", "keywords": "机器人学习, 互联网视频, 视频学习, 综述, 通用机器人", "comments": "这篇综述论文的重要性在于它系统地梳理了“从视频中学习”这一新兴且关键的领域，为机器人学习如何利用大规模互联网数据提供了路线图。其创新点在于明确指出了该领域面临的挑战（如分布偏移和缺少动作标签），并提出了未来研究方向，特别是对“可扩展的基础模型”的呼吁，这与当前AI领域的大模型趋势高度契合，有望加速通用机器人技术的发展。它为该领域的研究人员提供了宝贵的参考和启发。"}}
{"id": "2507.16947", "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Blog: this https URL", "url": "http://arxiv.org/abs/2507.16947v1", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "pdf_url": "http://arxiv.org/pdf/2507.16947v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于人工智能的基层医疗临床决策支持：一项真实世界研究", "tldr": "一项在肯尼亚基层医疗诊所进行的真实世界研究表明，基于大型语言模型（LLM）的AI咨询工具能够显著减少诊断和治疗错误，并提高护理质量。", "motivation": "该研究旨在评估基于大型语言模型的临床决策支持工具在真实医疗环境中对减少临床错误的影响，并为负责任的AI采纳提供实践框架。", "method": "研究与肯尼亚内罗毕的Penda Health合作，进行了一项质量改进研究。研究比较了15家诊所中，临床医生使用或不使用AI Consult工具的39,849次患者就诊结果。独立医生对就诊进行评估以识别临床错误。此外，还对使用AI Consult的临床医生进行了问卷调查。", "result": "使用AI Consult的临床医生诊断错误减少了16%，治疗错误减少了13%。在Penda Health，AI Consult每年可避免22,000次诊断错误和29,000次治疗错误。所有受访临床医生都表示AI Consult提升了护理质量，其中75%认为效果“显著”。这些结果的实现需要AI Consult与临床工作流程对齐的实施和积极部署。", "conclusion": "该研究表明，基于大型语言模型的临床决策支持工具在真实世界环境中具有减少医疗错误的潜力，并为推进负责任的AI采纳提供了实用框架。", "translation": "我们评估了在实际护理中基于大型语言模型的临床决策支持的影响。我们与肯尼亚内罗毕的基层医疗诊所网络Penda Health合作，研究了AI Consult，这是一种通过识别潜在的文件记录和临床决策错误来为临床医生提供安全网的工具。AI Consult整合到临床医生工作流程中，仅在需要时激活并保留临床医生自主权。我们进行了一项质量改进研究，比较了15家诊所中39,849次患者就诊的结果，这些就诊由可访问或不可访问AI Consult的临床医生执行。就诊由独立医生评估以识别临床错误。可访问AI Consult的临床医生错误相对较少：诊断错误减少了16%，治疗错误减少了13%。从绝对值来看，仅在Penda，引入AI Consult每年可避免22,000次诊断错误和29,000次治疗错误。在对使用AI Consult的临床医生进行的调查中，所有临床医生都表示AI Consult改善了他们提供的护理质量，其中75%表示效果“显著”。这些结果需要AI Consult与临床工作流程对齐的实施和积极部署，以鼓励临床医生采纳。我们希望这项研究能证明基于LLM的临床决策支持工具在真实世界环境中减少错误的潜力，并为推进负责任的采纳提供实用框架。", "summary": "本研究评估了在肯尼亚基层医疗环境中，基于大型语言模型（LLM）的AI Consult工具对临床决策支持的实际影响。通过一项质量改进研究，比较了使用和未使用该工具的近4万次患者就诊，发现AI Consult显著减少了诊断和治疗错误，分别降低了16%和13%。调查显示，所有临床医生都认为该工具提升了护理质量，且其成功部署依赖于与现有工作流程的良好整合。研究强调了LLM工具在真实世界中减少医疗错误和促进负责任AI采纳的潜力。", "keywords": "临床决策支持, 大型语言模型, 基层医疗, 医疗错误, 真实世界研究", "comments": "这项研究的创新之处在于其在真实世界基层医疗环境中的大规模实践验证，而非实验室或模拟环境。其重要性在于明确展示了LLM驱动的临床决策支持工具在提高医疗安全和质量方面的巨大潜力，特别是在资源可能有限的地区。同时，研究也强调了成功的AI部署需要与现有工作流程深度整合和积极的用户采纳策略，这为未来的AI医疗应用提供了宝贵的实践指导。"}}
{"id": "2411.10371", "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects", "authors": ["Qing Cheng", "Zefan Zeng", "Xingchen Hu", "Yuehang Si", "Zhong Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.10371v4", "summary": "Event Causality Identification (ECI) has emerged as a pivotal task in natural\nlanguage processing (NLP), aimed at automatically detecting causal\nrelationships between events in text. In this comprehensive survey, we\nsystematically elucidate the foundational principles and technical frameworks\nof ECI, proposing a novel classification framework to categorize and clarify\nexisting methods. {We discuss associated challenges, provide quantitative\nevaluations, and outline future directions for this dynamic and rapidly\nevolving field. We first delineate key definitions, problem formalization, and\nevaluation protocols of ECI. Our classification framework organizes ECI methods\nbased on two primary tasks: Sentence-level Event Causality Identification\n(SECI) and Document-level Event Causality Identification (DECI). For SECI, we\nreview methods including feature pattern-based matching, machine learning-based\nclassification, deep semantic encoding, prompt-based fine-tuning, and causal\nknowledge pre-training, alongside common data augmentation strategies. For\nDECI, we focus on techniques such as deep semantic encoding, event graph\nreasoning, and prompt-based fine-tuning. We dedicate specific discussions to\nadvancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI\nleveraging Large Language Models (LLMs). Furthermore, we analyze the strengths,\nlimitations, and unresolved challenges of each method. Extensive quantitative\nevaluations are conducted on four benchmark datasets to assess various ECI\nmethods. Finally, we explore future research directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.10371v4", "cate": "cs.CL", "date": "2024-11-15", "updated": "2025-07-23", "AI": {"title_translation": "事件因果关系识别综述：分类、挑战、评估与展望", "tldr": "这篇综合性综述系统地阐述了事件因果关系识别（ECI）的基础原理和技术框架，提出了一个新的分类框架来组织现有方法，并讨论了相关挑战、提供了定量评估并展望了未来方向。", "motivation": "事件因果关系识别（ECI）是自然语言处理（NLP）中的一个关键任务，旨在自动检测文本中事件之间的因果关系。本综述旨在系统地阐述ECI的原理和技术，并为现有方法提供新的分类框架，同时讨论挑战、评估方法并展望未来。", "method": "本综述系统地阐述了事件因果关系识别（ECI）的基础原理和技术框架，提出了一个新颖的分类框架，将ECI方法分为句子级（SECI）和文档级（DECI）。针对SECI，回顾了基于特征模式匹配、机器学习分类、深度语义编码、提示微调和因果知识预训练等方法；针对DECI，关注了深度语义编码、事件图推理和提示微调等技术。文中还讨论了多语言、跨语言和零样本ECI的进展，并分析了各种方法的优缺点和未解决的挑战。同时对四个人工基准数据集进行了广泛的定量评估。", "result": "本综述提出了一个新颖的分类框架，将ECI方法组织为句子级（SECI）和文档级（DECI），并详细回顾了各类方法的具体技术，包括基于特征模式、机器学习、深度语义编码、提示微调和因果知识预训练等。此外，对多语言、跨语言和零样本ECI的进展进行了讨论，并分析了现有方法的优缺点及挑战。通过对四个人工基准数据集的广泛定量评估，评估了各种ECI方法的效果。", "conclusion": "本综述全面阐述了事件因果关系识别（ECI）的原理和技术，提供了一个新的分类框架，并对现有方法进行了深入分析和定量评估。文章最后探讨了未来研究方向，指明了该领域动态和快速发展的趋势。", "translation": "事件因果关系识别（ECI）已成为自然语言处理（NLP）中的一项关键任务，旨在自动检测文本中事件之间的因果关系。在这项全面的综述中，我们系统地阐述了ECI的基础原理和技术框架，提出了一个新颖的分类框架来对现有方法进行分类和阐明。我们讨论了相关挑战，提供了定量评估，并概述了这一动态且快速发展领域的未来方向。我们首先描述了ECI的关键定义、问题形式化和评估协议。我们的分类框架根据两个主要任务组织ECI方法：句子级事件因果关系识别（SECI）和文档级事件因果关系识别（DECI）。对于SECI，我们回顾了包括基于特征模式匹配、基于机器学习的分类、深度语义编码、基于提示的微调和因果知识预训练在内的方法，以及常见的数据增强策略。对于DECI，我们专注于深度语义编码、事件图推理和基于提示的微调等技术。我们专门讨论了多语言和跨语言ECI以及利用大型语言模型（LLMs）的零样本ECI的进展。此外，我们分析了每种方法的优点、局限性和未解决的挑战。对四个基准数据集进行了广泛的定量评估，以评估各种ECI方法。最后，我们探讨了未来的研究方向。", "summary": "本综述全面审视了事件因果关系识别（ECI）领域，该领域是自然语言处理中的核心任务。文章提出了一个创新的分类框架，将ECI方法划分为句子级和文档级任务，并详细回顾了各类方法的原理、技术和数据增强策略，包括传统机器学习、深度学习和LLM相关方法。此外，综述还深入分析了现有方法的优缺点、未解决的挑战，并基于四个基准数据集进行了定量评估，最后展望了未来的研究方向。", "keywords": "事件因果关系识别, 自然语言处理, 综述, 分类框架, 大语言模型", "comments": "这篇综述提供了一个结构清晰、内容全面的事件因果关系识别（ECI）领域概览。其创新之处在于提出了一个新颖的分类框架，有助于更好地理解和组织现有方法。通过对各种方法的优缺点进行分析并进行定量评估，为研究人员提供了宝贵的参考。同时，对多语言、跨语言和零样本ECI的讨论，以及对LLM应用的关注，体现了其前瞻性。该综述对推动ECI领域的发展具有重要意义。"}}
{"id": "2507.16799", "title": "Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent", "authors": ["Xiaoyu Zhan", "Xinyu Fu", "Hao Sun", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16799v2", "summary": "The rapid advancement of large language models (LLMs) has enabled\nrole-playing language agents to demonstrate significant potential in various\napplications. However, relying solely on prompts and contextual inputs often\nproves insufficient for achieving deep immersion in specific roles,\nparticularly well-known fictional or public figures. On the other hand,\nfine-tuning-based approaches face limitations due to the challenges associated\nwith data collection and the computational resources required for training,\nthereby restricting their broader applicability. To address these issues, we\npropose Test-Time-Matching (TTM), a training-free role-playing framework\nthrough test-time scaling and context engineering. TTM uses LLM agents to\nautomatically decouple a character's features into personality, memory, and\nlinguistic style. Our framework involves a structured, three-stage generation\npipeline that utilizes these features for controlled role-playing. It achieves\nhigh-fidelity role-playing performance, also enables seamless combinations\nacross diverse linguistic styles and even variations in personality and memory.\nWe evaluate our framework through human assessment, and the results demonstrate\nthat our method achieves the outstanding performance in generating expressive\nand stylistically consistent character dialogues.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16799v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "测试时匹配：在基于LLM的角色扮演语言智能体中解耦个性、记忆和语言风格", "tldr": "提出Test-Time-Matching (TTM) 框架，通过测试时缩放和上下文工程，在不进行训练的情况下，自动解耦LLM角色扮演智能体的个性、记忆和语言风格，实现高保真和风格一致的角色对话。", "motivation": "现有LLM角色扮演方法依赖提示和上下文输入，难以实现深度沉浸，而基于微调的方法面临数据收集和计算资源限制，阻碍了广泛应用。", "method": "提出Test-Time-Matching (TTM) 框架，一种无需训练的角色扮演方法，通过测试时缩放和上下文工程实现。TTM使用LLM智能体自动将角色特征解耦为个性、记忆和语言风格。该框架包含一个结构化的三阶段生成管道，利用这些特征进行受控的角色扮演。", "result": "通过人工评估，结果表明该方法在生成富有表现力且风格一致的角色对话方面表现出色，并能实现不同语言风格甚至个性和记忆的无缝组合。", "conclusion": "Test-Time-Matching (TTM) 框架有效地解决了LLM角色扮演中深度沉浸和资源限制的问题，通过解耦角色特征，实现了高保真和灵活的角色对话生成。", "translation": "大型语言模型（LLMs）的快速发展使得角色扮演语言智能体在各种应用中展现出巨大潜力。然而，仅仅依靠提示和上下文输入往往不足以实现特定角色（尤其是知名虚构人物或公众人物）的深度沉浸。另一方面，基于微调的方法面临数据收集和训练所需计算资源的挑战，从而限制了其更广泛的应用。为了解决这些问题，我们提出了测试时匹配（Test-Time-Matching, TTM），一个通过测试时缩放和上下文工程实现的免训练角色扮演框架。TTM使用LLM智能体自动将角色特征解耦为个性、记忆和语言风格。我们的框架涉及一个结构化的三阶段生成管道，利用这些特征进行受控的角色扮演。它实现了高保真度的角色扮演性能，还能够实现跨不同语言风格甚至个性和记忆变化的无缝组合。我们通过人工评估了我们的框架，结果表明我们的方法在生成富有表现力且风格一致的角色对话方面取得了出色的表现。", "summary": "本研究提出Test-Time-Matching (TTM) 框架，旨在解决LLM角色扮演中深度沉浸不足和微调方法资源消耗大的问题。TTM是一种无需训练的方法，通过测试时缩放和上下文工程，将角色特征（个性、记忆、语言风格）自动解耦。该框架采用三阶段生成管道，利用这些解耦的特征实现高保真和受控的角色扮演。人工评估结果表明，TTM在生成富有表现力且风格一致的角色对话方面表现优异，并支持不同特征的灵活组合。", "keywords": "角色扮演, 大型语言模型, 测试时匹配, 特征解耦, 语言智能体", "comments": "TTM的创新之处在于其“免训练”的特性，通过测试时匹配和上下文工程，有效降低了传统微调方法的成本和复杂性。其将角色特征解耦为个性、记忆和语言风格的思路，为LLM在角色扮演领域的应用提供了更精细和灵活的控制能力，具有重要的实践意义。"}}
{"id": "2507.17017", "title": "Optimal Pure Differentially Private Sparse Histograms in Near-Linear Deterministic Time", "authors": ["Florian Kerschbaum", "Steven Lee", "Hao Wu"], "categories": ["cs.DS", "cs.CR"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17017v1", "summary": "We introduce an algorithm that releases a pure differentially private sparse\nhistogram over $n$ participants drawn from a domain of size $d \\gg n$. Our\nmethod attains the optimal $\\ell_\\infty$-estimation error and runs in strictly\n$O(n \\ln \\ln d)$ time in the word-RAM model, thereby improving upon the\nprevious best known deterministic-time bound of $\\tilde{O}(n^2)$ and resolving\nthe open problem of breaking this quadratic barrier (Balcer and Vadhan, 2019).\nCentral to our algorithm is a novel private item blanket technique with\ntarget-length padding, which transforms the approximate differentially private\nstability-based histogram algorithm into a pure differentially private one.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17017v1", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "最佳纯差分隐私稀疏直方图的近线性确定性时间算法", "tldr": "提出一种新的算法，以近线性确定性时间构建最佳纯差分隐私稀疏直方图，打破了之前的二次时间复杂度障碍。", "motivation": "该研究旨在改进现有差分隐私稀疏直方图算法的时间复杂度，并解决 Balcer 和 Vadhan (2019) 提出的打破二次时间障碍的开放问题。", "method": "本研究引入了一种新算法，其核心在于一种新颖的“私有项目覆盖技术”结合目标长度填充。该技术将近似差分隐私的基于稳定性的直方图算法转换为纯差分隐私算法。", "result": "该方法实现了最佳的 $\\ell_\\infty$ 估计误差，并在字-RAM模型中以严格的 $O(n \\ln \\ln d)$ 时间运行。这显著优于先前最佳的 $\\tilde{O}(n^2)$ 确定性时间界限，并成功解决了打破二次时间复杂度障碍的开放问题。", "conclusion": "该研究通过引入一种高效算法，解决了纯差分隐私稀疏直方图构建的开放问题，显著提升了算法性能，并实现了理论最优的估计误差。", "translation": "我们引入了一种算法，该算法能够从大小为 $d \\gg n$ 的域中，为 $n$ 个参与者发布一个纯差分隐私的稀疏直方图。我们的方法在字-RAM模型中达到了最佳的 $\\ell_\\infty$ 估计误差，并以严格的 $O(n \\ln \\ln d)$ 时间运行，从而改进了之前已知的最佳确定性时间界限 $\\tilde{O}(n^2)$，并解决了打破这个二次障碍的开放问题 (Balcer and Vadhan, 2019)。我们算法的核心是一种新颖的带有目标长度填充的私有项目覆盖技术，它将近似差分隐私的基于稳定性的直方图算法转换为纯差分隐私算法。", "summary": "本文提出了一种创新的算法，用于在近线性确定性时间内构建纯差分隐私的稀疏直方图。该算法针对 $n$ 个参与者和大小为 $d \\gg n$ 的域，实现了最优的 $\\ell_\\infty$ 估计误差。通过引入一种新颖的私有项目覆盖技术和目标长度填充，该方法将现有算法转化为纯差分隐私形式，并将运行时间从 $\\tilde{O}(n^2)$ 大幅优化至 $O(n \\ln \\ln d)$，成功解决了长期存在的二次时间复杂度瓶颈问题。", "keywords": "差分隐私, 稀疏直方图, 时间复杂度, 私有项目覆盖, 数据隐私", "comments": "这篇论文的创新点在于引入了一种新颖的“私有项目覆盖技术”并结合目标长度填充，从而将近似差分隐私算法转化为纯差分隐私算法。其重要性在于，它不仅在理论上解决了长期存在的二次时间复杂度开放问题，还将稀疏直方图的构建效率提升到了近线性时间，这对于大规模差分隐私数据发布具有重要实践意义。"}}
{"id": "2507.16941", "title": "Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection", "authors": ["Daniel Correa", "Tero Kaarlela", "Jose Fuentes", "Paulo Padrao", "Alain Duran", "Leonardo Bobadilla"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16941v1", "summary": "This paper presents a reinforcement learning (RL) environment for developing\nan autonomous underwater robotic coral sampling agent, a crucial coral reef\nconservation and research task. Using software-in-the-loop (SIL) and\nhardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)\ncontroller is developed using a digital twin (DT) in simulation and\nsubsequently verified in physical experiments. An underwater motion capture\n(MOCAP) system provides real-time 3D position and orientation feedback during\nverification testing for precise synchronization between the digital and\nphysical domains. A key novelty of this approach is the combined use of a\ngeneral-purpose game engine for simulation, deep RL, and real-time underwater\nmotion capture for an effective zero-shot sim-to-real strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16941v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "多智能体强化学习在机器人珊瑚礁样本采集中的应用", "tldr": "本文提出了一个用于开发水下机器人珊瑚采样智能体的强化学习环境，并使用数字孪生和实时水下运动捕捉实现了有效的零样本模拟到现实策略。", "motivation": "珊瑚礁保护和研究需要水下机器人进行珊瑚样本采集，因此开发自主水下机器人珊瑚采样智能体至关重要。", "method": "本文提出了一个强化学习（RL）环境，用于开发自主水下机器人珊瑚采样智能体。利用软件在环（SIL）和硬件在环（HIL）技术，通过在模拟中使用数字孪生（DT）开发了一个经过RL训练的人工智能（AI）控制器，随后在物理实验中进行了验证。水下运动捕捉（MOCAP）系统在验证测试期间提供实时3D位置和方向反馈，以实现数字和物理领域之间的精确同步。该方法的关键新颖之处在于结合使用了通用游戏引擎进行模拟、深度RL和实时水下运动捕捉，以实现有效的零样本模拟到现实策略。", "result": "成功开发了一个经过RL训练的AI控制器，并在物理实验中得到了验证，实现了数字和物理领域之间的精确同步。", "conclusion": "该研究成功展示了通过结合通用游戏引擎、深度强化学习和实时水下运动捕捉，可以实现有效的零样本模拟到现实策略，用于水下机器人珊瑚样本采集。", "translation": "本文提出了一个用于开发自主水下机器人珊瑚采样智能体的强化学习（RL）环境，这是一项重要的珊瑚礁保护和研究任务。通过软件在环（SIL）和硬件在环（HIL），利用模拟中的数字孪生（DT）开发了一个经过RL训练的人工智能（AI）控制器，随后在物理实验中进行了验证。水下运动捕捉（MOCAP）系统在验证测试期间提供实时3D位置和方向反馈，以实现数字和物理领域之间的精确同步。该方法的关键新颖之处在于结合使用了通用游戏引擎进行模拟、深度RL和实时水下运动捕捉，以实现有效的零样本模拟到现实策略。", "summary": "本文介绍了一个用于开发自主水下机器人珊瑚采样智能体的强化学习环境。该研究利用软件在环和硬件在环技术，通过数字孪生在模拟中训练AI控制器，并在物理实验中进行验证。一个关键创新是结合通用游戏引擎、深度强化学习和实时水下运动捕捉，实现了高效的零样本模拟到现实策略。", "keywords": "强化学习, 水下机器人, 珊瑚采样, 数字孪生, 模拟到现实", "comments": "该论文的创新点在于其独特的零样本模拟到现实（zero-shot sim-to-real）策略，通过整合通用游戏引擎、深度强化学习和实时水下运动捕捉系统，为水下机器人控制提供了一个高效且精确的解决方案。这对于在复杂水下环境中部署机器人具有重要意义，减少了实际硬件测试的需求，加速了开发周期。"}}
{"id": "2507.17401", "title": "The Wilhelm Tell Dataset of Affordance Demonstrations", "authors": ["Rachel Ringe", "Mihai Pomarlan", "Nikolaos Tsiogkas", "Stefano De Giorgis", "Maria Hedblom", "Rainer Malaka"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      \\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "url": "http://arxiv.org/abs/2507.17401v1", "summary": "Affordances - i.e. possibilities for action that an environment or objects in\nit provide - are important for robots operating in human environments to\nperceive. Existing approaches train such capabilities on annotated static\nimages or shapes. This work presents a novel dataset for affordance learning of\ncommon household tasks. Unlike previous approaches, our dataset consists of\nvideo sequences demonstrating the tasks from first- and third-person\nperspectives, along with metadata about the affordances that are manifested in\nthe task, and is aimed towards training perception systems to recognize\naffordance manifestations. The demonstrations were collected from several\nparticipants and in total record about seven hours of human activity. The\nvariety of task performances also allows studying preparatory maneuvers that\npeople may perform for a task, such as how they arrange their task space, which\nis also relevant for collaborative service robots.", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "pdf_url": "http://arxiv.org/pdf/2507.17401v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "威廉·泰尔人机交互演示数据集", "tldr": "提出了一个名为“Wilhelm Tell”的新型仿人行为视频数据集，用于机器人学习感知家庭任务中的“示能性”，包含第一和第三人称视角以及元数据，共七小时人类活动，旨在改进现有基于静态图像的示能性学习方法。", "motivation": "机器人需要在人类环境中感知“示能性”（环境或物体提供的行动可能性）。现有方法主要基于带注释的静态图像或形状进行训练，存在局限性。", "method": "本文提出了一个名为“Wilhelm Tell”的新型示能性学习数据集。该数据集包含视频序列，从第一人称和第三人称视角展示了常见的家庭任务，并附带了任务中体现的示能性元数据。这些演示是从多位参与者那里收集的，总共记录了大约七小时的人类活动。", "result": "成功构建了一个包含约七小时人类活动视频序列的“Wilhelm Tell”数据集。该数据集独特之处在于其视频格式、第一和第三人称视角以及详细的示能性元数据，这些都旨在训练感知系统识别示能性表现。", "conclusion": "该数据集不仅能用于训练机器人感知系统识别示能性表现，其任务执行的多样性还允许研究人类在任务中可能进行的准备性操作（如整理任务空间），这对协作服务机器人也具有重要意义。", "translation": "“示能性”——即环境或其中物体所提供的行动可能性——对于在人类环境中运行的机器人来说，感知它们至关重要。现有方法通过注释的静态图像或形状来训练这种能力。这项工作提出了一个新颖的数据集，用于学习常见家庭任务的示能性。与以前的方法不同，我们的数据集由视频序列组成，从第一人称和第三人称视角演示了这些任务，并附带了任务中表现出的示能性的元数据，旨在训练感知系统识别示能性表现。这些演示是从多位参与者那里收集的，总共记录了大约七小时的人类活动。任务执行的多样性也允许研究人们可能为任务执行的准备性操作，例如他们如何安排任务空间，这对于协作服务机器人也具有相关性。", "summary": "本文介绍了一个名为“Wilhelm Tell”的新型示能性学习数据集，旨在解决机器人感知人类环境中物体“示能性”的问题。不同于以往基于静态图像的方法，该数据集包含来自第一和第三人称视角的视频演示序列，记录了约七小时的常见家庭任务，并附带了详细的示能性元数据。该数据集旨在训练机器人感知系统识别示能性，其多样的任务表现也为研究人类准备性操作提供了可能，从而对协作服务机器人研究具有重要价值。", "keywords": "示能性, 机器人感知, 数据集, 视频序列, 家庭任务", "comments": "这篇论文的创新点在于构建了一个基于视频序列的示能性数据集，突破了传统静态图像的局限。通过包含第一和第三人称视角以及详细元数据，它为机器人更真实、动态地理解人类行为和环境提供了宝贵资源。数据集的规模（七小时人类活动）和对准备性操作的研究潜力，也显示了其在推动协作服务机器人发展方面的潜在重要性。"}}
{"id": "2507.17548", "title": "CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning", "authors": ["Lingxiao Tang", "He Ye", "Zhongxin Liu", "Xiaoxue Ren", "Lingfeng Bao"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17548v1", "summary": "Code reasoning is a fundamental capability for large language models (LLMs)\nin the code domain. It involves understanding and predicting a program's\nexecution behavior, such as determining the output for a given input or whether\na specific statement will be executed. This capability is essential for\ndownstream tasks like debugging, code generation, and program repair. Prior\napproaches mainly rely on supervised fine-tuning to improve performance in code\nreasoning tasks. However, they often show limited gains and fail to generalize\nacross diverse scenarios. We argue this is due to two core issues: the low\nquality of training data and the limitations of supervised fine-tuning, which\nstruggles to teach general reasoning skills. To address these challenges, we\npropose CodeReasoner, a framework that spans both dataset construction and a\ntwo-stage training process. First, we introduce a method to construct datasets\nthat focus on the core execution logic of Python programs. Next, we apply\ninstruction tuning to inject execution-specific knowledge distilled from a\npowerful teacher model. We then enhance reasoning and generalization through\nGRPO reinforcement learning on top of the fine-tuned model. Experiments on\nthree widely-used code reasoning benchmarks show that CodeReasoner improves\nperformance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the\n7B model matches GPT-4o on key tasks like input/output and coverage prediction.\nWhen scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.\nAblation studies confirm the effectiveness of each training stage and highlight\nthe importance of reasoning chains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17548v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CodeReasoner：利用强化学习增强代码推理能力", "tldr": "CodeReasoner通过构建高质量数据集和两阶段训练（指令微调+强化学习）显著提升了LLMs的代码推理能力，在多个基准测试中超越了现有方法，甚至在14B模型上超越了GPT-4o。", "motivation": "大型语言模型在代码领域的代码推理能力至关重要，但现有方法（主要依赖监督微调）在性能提升和泛化能力上受限。这主要是由于训练数据质量低和监督微调难以教授通用推理技能。", "method": "提出CodeReasoner框架，包含数据集构建和两阶段训练。首先，构建专注于Python程序核心执行逻辑的数据集。其次，通过指令微调注入从强大教师模型中提取的执行特定知识。最后，在微调模型基础上应用GRPO强化学习来增强推理和泛化能力。", "result": "在三个广泛使用的代码推理基准测试中，CodeReasoner使用7B模型比现有方法性能提升27.1%至40.2%。7B模型在输入/输出和覆盖率预测等关键任务上与GPT-4o持平。扩展到14B时，CodeReasoner在所有基准测试中均优于GPT-4o。消融研究证实了每个训练阶段的有效性，并强调了推理链的重要性。", "conclusion": "CodeReasoner通过其新颖的数据集构建和两阶段强化学习训练方法，显著提升了大型语言模型在代码推理任务上的性能和泛化能力，甚至在某些情况下超越了最先进的商业模型。", "translation": "代码推理是大型语言模型（LLMs）在代码领域的一项基本能力。它涉及理解和预测程序的执行行为，例如确定给定输入的输出或特定语句是否会被执行。这项能力对于调试、代码生成和程序修复等下游任务至关重要。现有方法主要依赖监督微调来提高代码推理任务的性能。然而，它们通常表现出有限的增益，并且难以泛化到不同的场景。我们认为这归因于两个核心问题：训练数据质量低和监督微调的局限性，后者难以教授通用的推理技能。为了解决这些挑战，我们提出了CodeReasoner，一个涵盖数据集构建和两阶段训练过程的框架。首先，我们引入了一种构建数据集的方法，该数据集专注于Python程序的核心执行逻辑。接下来，我们应用指令微调来注入从强大教师模型中提取的执行特定知识。然后，我们在微调模型的基础上，通过GRPO强化学习来增强推理和泛化能力。在三个广泛使用的代码推理基准测试上的实验表明，CodeReasoner使用7B模型比现有方法性能提高了27.1%到40.2%。值得注意的是，7B模型在输入/输出和覆盖率预测等关键任务上与GPT-4o持平。当扩展到14B时，CodeReasoner在所有基准测试中均优于GPT-4o。消融研究证实了每个训练阶段的有效性，并强调了推理链的重要性。", "summary": "本文提出了CodeReasoner框架，旨在提升大型语言模型的代码推理能力。针对现有方法在数据质量和泛化能力上的不足，CodeReasoner通过构建专注于核心执行逻辑的高质量数据集，并采用两阶段训练方法：首先进行指令微调注入执行知识，然后通过GRPO强化学习增强推理和泛化。实验结果表明，CodeReasoner在多个代码推理基准测试上显著优于现有方法，其7B模型在关键任务上可与GPT-4o媲美，而14B模型则全面超越GPT-4o。", "keywords": "代码推理, 强化学习, 大型语言模型, 数据集构建, GRPO", "comments": "CodeReasoner的创新点在于结合了高质量数据集构建、指令微调和强化学习，解决了现有方法在代码推理任务中数据质量和泛化能力不足的问题。其通过GRPO强化学习在微调模型上进一步提升推理和泛化能力，并实现了超越GPT-4o的性能，这对于提升LLMs在代码领域的实用性具有重要意义。特别是其在相对较小的模型（7B/14B）上达到甚至超越顶尖商业模型的能力，证明了其方法的有效性和潜力。"}}
{"id": "2507.17224", "title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes", "authors": ["Feng Cao", "Zishuo Feng"], "categories": ["eess.SP", "cs.AI", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, 6 tables", "url": "http://arxiv.org/abs/2507.17224v1", "summary": "Extracellular recordings are brief voltage fluctuations recorded near\nneurons, widely used in neuroscience as the basis for decoding brain activity\nat single-neuron resolution. Spike sorting, which assigns each spike to its\nsource neuron, is a critical step in brain sensing pipelines. However, it\nremains challenging under low signal-to-noise ratio (SNR), electrode drift, and\ncross-session variability. In this paper, we propose HuiduRep, a robust\nself-supervised representation learning framework that extracts discriminative\nand generalizable features from extracellular spike waveforms. By combining\ncontrastive learning with a denoising autoencoder, HuiduRep learns latent\nrepresentations that are robust to noise and drift. Built on HuiduRep, we\ndevelop a spike sorting pipeline that clusters spike representations without\nsupervision. Experiments on hybrid and real-world datasets demonstrate that\nHuiduRep achieves strong robustness and the pipeline matches or outperforms\nstate-of-the-art tools such as KiloSort4 and MountainSort5. These findings\ndemonstrate the potential of self-supervised spike representation learning as a\nfoundational tool for robust and generalizable processing of extracellular\nrecordings.", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.17224v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HuiduRep：一个用于从细胞外尖峰学习神经表征的鲁棒自监督框架", "tldr": "HuiduRep是一个鲁棒的自监督框架，通过结合对比学习和去噪自编码器，从细胞外尖峰波形中提取判别性和可泛化的特征，以解决低信噪比、电极漂移和会话间变异性等挑战，并在尖峰分选任务中表现优异。", "motivation": "在神经科学中，尖峰分选是将每个尖峰分配到其源神经元的关键步骤，但其在低信噪比（SNR）、电极漂移和会话间变异性等挑战下仍然面临困难。", "method": "本文提出了HuiduRep，一个鲁棒的自监督表征学习框架。它通过结合对比学习与去噪自编码器，学习对噪声和漂移具有鲁棒性的潜在表征。在此基础上，开发了一个无需监督的尖峰分选流程，通过聚类尖峰表征来完成任务。", "result": "在混合和真实世界数据集上的实验表明，HuiduRep具有强大的鲁棒性，并且其尖峰分选流程与KiloSort4和MountainSort5等最先进的工具相当或优于它们。", "conclusion": "自监督尖峰表征学习作为一种基础工具，在处理细胞外记录方面具有鲁棒性和泛化潜力。", "translation": "细胞外记录是神经元附近记录到的短暂电压波动，在神经科学中被广泛用作解码单神经元分辨率脑活动的基础。尖峰分选，即将每个尖峰分配到其源神经元，是脑感知流程中的关键步骤。然而，在低信噪比（SNR）、电极漂移和会话间变异性下，它仍然具有挑战性。在本文中，我们提出了HuiduRep，一个鲁棒的自监督表征学习框架，可以从细胞外尖峰波形中提取判别性和可泛化的特征。通过结合对比学习与去噪自编码器，HuiduRep学习到对噪声和漂移具有鲁棒性的潜在表征。基于HuiduRep，我们开发了一个无需监督的尖峰分选流程，该流程通过聚类尖峰表征来完成。在混合和真实世界数据集上的实验表明，HuiduRep实现了强大的鲁棒性，并且该流程与KiloSort4和MountainSort5等最先进的工具相当或优于它们。这些发现证明了自监督尖峰表征学习作为一种基础工具，在细胞外记录的鲁棒和可泛化处理方面的潜力。", "summary": "本文提出了HuiduRep，一个用于从细胞外尖峰波形中学习鲁棒神经表征的自监督框架。通过结合对比学习和去噪自编码器，HuiduRep能够提取对噪声和电极漂移具有鲁棒性的判别性特征。在此基础上开发的无监督尖峰分选流程，在实验中表现出强大的鲁棒性，并能与现有最先进的工具媲美或超越，展示了自监督表征学习在神经科学记录处理中的巨大潜力。", "keywords": "自监督学习, 神经表征, 尖峰分选, 对比学习, 去噪自编码器", "comments": "HuiduRep的创新之处在于将对比学习与去噪自编码器结合起来，以解决细胞外尖峰记录中常见的低信噪比、电极漂移和会话间变异性问题。这种自监督方法避免了对大量标注数据的依赖，提高了特征的鲁棒性和泛化能力。其在尖峰分选任务中超越现有SOTA工具的性能，证明了其在神经科学领域的重要应用价值。"}}
{"id": "2507.17003", "title": "PPAAS: PVT and Pareto Aware Analog Sizing via Goal-conditioned Reinforcement Learning", "authors": ["Seunggeun Kim", "Ziyi Wang", "Sungyoung Lee", "Youngmin Oh", "Hanqing Zhu", "Doyun Kim", "David Z. Pan"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted to the 44th International Conference on Computer-Aided Design (ICCAD 2025); 9 pages, 10 figures", "url": "http://arxiv.org/abs/2507.17003v1", "summary": "Device sizing is a critical yet challenging step in analog and mixed-signal\ncircuit design, requiring careful optimization to meet diverse performance\nspecifications. This challenge is further amplified under process, voltage, and\ntemperature (PVT) variations, which cause circuit behavior to shift across\ndifferent corners. While reinforcement learning (RL) has shown promise in\nautomating sizing for fixed targets, training a generalized policy that can\nadapt to a wide range of design specifications under PVT variations requires\nmuch more training samples and resources. To address these challenges, we\npropose a \\textbf{Goal-conditioned RL framework} that enables efficient policy\ntraining for analog device sizing across PVT corners, with strong\ngeneralization capability. To improve sample efficiency, we introduce\nPareto-front Dominance Goal Sampling, which constructs an automatic curriculum\nby sampling goals from the Pareto frontier of previously achieved goals. This\nstrategy is further enhanced by integrating Conservative Hindsight Experience\nReplay, which assigns relabeled goals with conservative virtual rewards to\nstabilize training and accelerate convergence. To reduce simulation overhead,\nour framework incorporates a Skip-on-Fail simulation strategy, which skips\nfull-corner simulations when nominal-corner simulation fails to meet target\nspecifications. Experiments on benchmark circuits demonstrate $\\sim$1.6$\\times$\nimprovement in sample efficiency and $\\sim$4.1$\\times$ improvement in\nsimulation efficiency compared to existing sizing methods. Code and benchmarks\nare publicly available at https://github.com/SeunggeunKimkr/PPAAS", "comment": "Accepted to the 44th International Conference on Computer-Aided\n  Design (ICCAD 2025); 9 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.17003v1", "cate": "eess.SP", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "PPAAS：基于目标条件强化学习的PVT和帕累托感知模拟尺寸确定", "tldr": "该论文提出了一种名为PPAAS的目标条件强化学习框架，用于在PVT变化下高效地进行模拟器件尺寸确定。该框架通过帕累托前沿支配目标采样、保守事后经验回放和失败跳过仿真策略，显著提高了样本效率和仿真效率，并具有强大的泛化能力。", "motivation": "模拟和混合信号电路设计中的器件尺寸确定是关键但具有挑战性的一步，需要在工艺、电压和温度（PVT）变化下进行优化以满足各种性能指标。现有的强化学习方法在固定目标下表现出潜力，但训练一个能适应广泛设计规范并在PVT变化下泛化的策略需要大量的训练样本和资源。", "method": "我们提出了一个目标条件强化学习框架，该框架通过引入帕累托前沿支配目标采样（构建自动课程）、集成保守事后经验回放（稳定训练和加速收敛）以及采用失败跳过仿真策略（减少仿真开销），实现了在PVT角点下模拟器件尺寸确定的高效策略训练和强大的泛化能力。", "result": "在基准电路上进行的实验表明，与现有尺寸确定方法相比，该框架在样本效率方面提高了约1.6倍，在仿真效率方面提高了约4.1倍。", "conclusion": "PPAAS框架通过创新的目标采样、经验回放和仿真策略，显著提高了模拟器件尺寸确定在PVT变化下的效率和泛化能力，为模拟电路设计自动化提供了有效的解决方案。", "translation": "器件尺寸确定是模拟和混合信号电路设计中一个关键但具有挑战性的步骤，需要仔细优化以满足各种性能规范。在工艺、电压和温度（PVT）变化下，电路行为会在不同角点之间发生偏移，这进一步加剧了这一挑战。虽然强化学习（RL）在自动化固定目标的尺寸确定方面显示出前景，但训练一个能够适应PVT变化下广泛设计规范的通用策略需要更多的训练样本和资源。为了应对这些挑战，我们提出了一个**目标条件强化学习框架**，该框架能够高效地训练模拟器件尺寸确定在PVT角点下的策略，并具有强大的泛化能力。为了提高样本效率，我们引入了帕累托前沿支配目标采样，通过从先前实现目标的帕累托前沿采样目标来构建自动课程。通过集成保守事后经验回放进一步增强了这一策略，该回放为重新标记的目标分配保守的虚拟奖励，以稳定训练并加速收敛。为了减少仿真开销，我们的框架包含了失败跳过仿真策略，当标称角点仿真未能满足目标规范时，会跳过全角点仿真。在基准电路上进行的实验表明，与现有尺寸确定方法相比，样本效率提高了约1.6倍，仿真效率提高了约4.1倍。代码和基准可在https://github.com/SeunggeunKimkr/PPAAS公开获取。", "summary": "本论文提出了一种名为PPAAS的目标条件强化学习框架，旨在解决模拟器件尺寸确定在PVT变化下的效率和泛化挑战。该框架通过引入帕累托前沿支配目标采样、保守事后经验回放和失败跳过仿真策略，显著提升了样本效率和仿真效率，并在实验中展现出优于现有方法的性能，为模拟电路设计自动化提供了有效途径。", "keywords": "模拟尺寸确定, 强化学习, PVT变化, 目标条件RL, 帕累托前沿", "comments": "PPAAS框架的创新之处在于其结合了目标条件强化学习与多种优化策略，以应对模拟电路设计中PVT变化下的尺寸确定难题。特别是帕累托前沿支配目标采样和保守事后经验回放，有效提升了学习效率和泛化能力，而失败跳过仿真策略则显著降低了计算成本。这对于自动化模拟电路设计具有重要意义。"}}
{"id": "2507.17673", "title": "Stable Iterative Solvers for Ill-conditioned Linear Systems", "authors": ["Vasileios Kalantzis", "Mark S. Squillante", "Chai Wah Wu"], "categories": ["math.NA", "cs.DS", "cs.NA", "65F22", "F.2.1"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      7 pages, 13 figures", "url": "http://arxiv.org/abs/2507.17673v1", "summary": "Iterative solvers for large-scale linear systems such as Krylov subspace\nmethods can diverge when the linear system is ill-conditioned, thus\nsignificantly reducing the applicability of these iterative methods in practice\nfor high-performance computing solutions of such large-scale linear systems. To\naddress this fundamental problem, we propose general algorithmic frameworks to\nmodify Krylov subspace iterative solution methods which ensure that the\nalgorithms are stable and do not diverge. We then apply our general frameworks\nto current implementations of the corresponding iterative methods in SciPy and\ndemonstrate the efficacy of our stable iterative approach with respect to\nnumerical experiments across a wide range of synthetic and real-world\nill-conditioned linear systems.", "comment": "7 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.17673v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "针对病态线性系统的稳定迭代求解器", "tldr": "提出通用的算法框架，改进Krylov子空间迭代方法，使其在求解病态线性系统时稳定不发散，并通过实验验证了其有效性。", "motivation": "现有的Krylov子空间等迭代求解器在处理病态线性系统时可能发散，这大大限制了它们在高性能计算中解决大规模线性系统时的应用。", "method": "提出了通用的算法框架，用于修改Krylov子空间迭代求解方法，以确保算法的稳定性和不发散性。并将这些框架应用于SciPy中现有迭代方法的实现。", "result": "通过对各种合成和真实世界病态线性系统的数值实验，证明了所提出的稳定迭代方法的有效性。", "conclusion": "所提出的通用算法框架能够有效地解决病态线性系统中迭代求解器发散的问题，提高了迭代方法在实际应用中的稳定性。", "translation": "大规模线性系统的迭代求解器，如Krylov子空间方法，在线性系统病态时可能会发散，从而显著降低了这些迭代方法在高性能计算中解决此类大规模线性系统时的实际适用性。为了解决这个根本问题，我们提出了通用的算法框架来修改Krylov子空间迭代求解方法，以确保算法稳定且不发散。然后，我们将我们的通用框架应用于SciPy中相应迭代方法的当前实现，并通过对各种合成和真实世界病态线性系统的数值实验，证明了我们稳定迭代方法的有效性。", "summary": "本文针对Krylov子空间等迭代求解器在处理病态线性系统时可能发散的问题，提出了通用的算法框架来改进这些方法，以确保其稳定性。研究人员将这些框架应用于SciPy中的现有实现，并通过广泛的数值实验验证了所提出的稳定迭代方法在解决合成和真实世界病态线性系统时的有效性。", "keywords": "迭代求解器, 病态线性系统, Krylov子空间方法, 稳定性, 算法框架", "comments": "本文提出了一种创新的通用算法框架，有效解决了迭代求解器在病态线性系统下发散的关键问题，显著提升了Krylov子空间方法在高性能计算中的实用性。其将通用框架应用于现有库的实践方法也增加了其潜在影响力。"}}
{"id": "2507.16869", "title": "Controllable Video Generation: A Survey", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      project page: this https URL", "url": "http://arxiv.org/abs/2507.16869v1", "summary": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.", "comment": "project page:\n  https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "pdf_url": "http://arxiv.org/pdf/2507.16869v1", "cate": "cs.GR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "可控视频生成：一项综述", "tldr": "这篇综述系统地回顾了可控视频生成领域，分析了如何通过结合非文本条件来增强视频生成模型的控制能力。", "motivation": "随着AI生成内容（AIGC）和视频生成基础模型的快速发展，现有文本到视频生成模型单独的文本提示不足以表达复杂、多模态和细粒度的用户意图，导致难以精确控制视频生成，因此需要探索和发展可控视频生成方法。", "method": "本文通过系统综述的方式，涵盖了可控视频生成的理论基础和最新进展。具体方法包括：介绍关键概念和常用开源视频生成模型；重点分析视频扩散模型中的控制机制，以及不同类型的条件如何融入去噪过程以指导生成；根据其利用的控制信号类型（包括单条件生成、多条件生成和通用可控生成）对现有方法进行分类。", "result": "提供了对可控视频生成领域的系统回顾，介绍了关键概念、常用模型，分析了视频扩散模型中的控制机制，并根据控制信号类型对现有方法进行了分类，旨在增强AIGC驱动的视频生成系统的灵活性和实际适用性。", "conclusion": "Not mentioned in abstract", "translation": "随着AI生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。特别是，视频生成基础模型的进步导致对能够更准确反映用户意图的可控视频生成方法的需求不断增长。大多数现有基础模型专为文本到视频生成设计，其中单独的文本提示通常不足以表达复杂、多模态和细粒度的用户需求。这一限制使得用户难以使用当前模型精确控制视频生成。为了解决这个问题，最近的研究探索了整合额外的非文本条件，如摄像机运动、深度图和人体姿态，以扩展预训练的视频生成模型，实现更可控的视频合成。这些方法旨在增强AIGC驱动的视频生成系统的灵活性和实际适用性。在本综述中，我们系统回顾了可控视频生成，涵盖了该领域的理论基础和最新进展。我们首先介绍了关键概念和常用的开源视频生成模型。然后，我们重点关注视频扩散模型中的控制机制，分析不同类型的条件如何融入去噪过程以指导生成。最后，我们根据其利用的控制信号类型对现有方法进行分类，包括单条件生成、多条件生成和通用可控生成。有关可控视频生成文献的完整列表，请访问我们的精选存储库：https://github.com/mayuelala/Awesome-Controllable-Video-Generation。", "summary": "本文综述了可控视频生成领域，指出当前文本到视频模型难以满足复杂的用户意图。为解决此问题，研究探索了整合非文本条件以增强控制。该综述系统回顾了该领域，涵盖理论基础、最新进展、关键概念、开源模型，并详细分析了视频扩散模型中的控制机制，最后根据控制信号类型对现有方法进行了分类。", "keywords": "可控视频生成, 视频扩散模型, AIGC, 非文本条件, 综述", "comments": "这篇综述的重要性在于它系统地梳理了可控视频生成这一新兴且关键的AIGC子领域。它不仅识别了当前文本到视频生成模型的局限性，即难以精确表达复杂用户意图，还详细介绍了通过整合非文本条件来增强控制的方法。对视频扩散模型控制机制的深入分析和对现有方法的分类，为研究人员和开发者提供了宝贵的结构化知识和参考，有助于推动该领域的进一步发展和实际应用。"}}
{"id": "2312.03584", "title": "Context Diffusion: In-Context Aware Image Generation", "authors": ["Ivona Najdenkoska", "Animesh Sinha", "Abhimanyu Dubey", "Dhruv Mahajan", "Vignesh Ramanathan", "Filip Radenovic"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.03584v2", "summary": "We propose Context Diffusion, a diffusion-based framework that enables image\ngeneration models to learn from visual examples presented in context. Recent\nwork tackles such in-context learning for image generation, where a query image\nis provided alongside context examples and text prompts. However, the quality\nand context fidelity of the generated images deteriorate when the prompt is not\npresent, demonstrating that these models cannot truly learn from the visual\ncontext. To address this, we propose a novel framework that separates the\nencoding of the visual context and the preservation of the desired image\nlayout. This results in the ability to learn from the visual context and\nprompts, but also from either of them. Furthermore, we enable our model to\nhandle few-shot settings, to effectively address diverse in-context learning\nscenarios. Our experiments and human evaluation demonstrate that Context\nDiffusion excels in both in-domain and out-of-domain tasks, resulting in an\noverall enhancement in image quality and context fidelity compared to\ncounterpart models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.03584v2", "cate": "cs.CV", "date": "2023-12-06", "updated": "2025-07-23", "AI": {"title_translation": "上下文扩散：上下文感知图像生成", "tldr": "Context Diffusion是一个新型扩散框架，通过分离视觉上下文编码和图像布局保留，解决了现有模型在无文本提示下上下文学习不足的问题，显著提高了图像生成质量和上下文保真度。", "motivation": "现有图像生成模型在缺乏文本提示时，其生成图像的质量和上下文保真度会下降，表明它们无法真正从视觉上下文中有效学习。", "method": "本文提出了Context Diffusion框架，其核心在于分离视觉上下文的编码和所需图像布局的保留。这种设计使得模型能够同时从视觉上下文和文本提示中学习，或仅从其中之一学习。此外，该模型还被设计为能够处理少样本设置，以适应多样化的上下文学习场景。", "result": "实验和人工评估表明，Context Diffusion在域内和域外任务中均表现出色，与现有同类模型相比，显著提升了图像质量和上下文保真度。", "conclusion": "Context Diffusion通过其创新的框架设计，成功解决了现有图像生成模型在无文本提示下上下文学习能力不足的问题，并在各种复杂场景下实现了高质量的图像生成，具有广泛的应用潜力。", "translation": "我们提出了Context Diffusion，一个基于扩散的框架，它使图像生成模型能够从上下文中呈现的视觉示例中学习。最近的工作解决了图像生成的这种上下文学习问题，其中查询图像与上下文示例和文本提示一起提供。然而，当提示不存在时，生成图像的质量和上下文保真度会下降，这表明这些模型无法真正从视觉上下文中学习。为了解决这个问题，我们提出了一个新颖的框架，该框架分离了视觉上下文的编码和所需图像布局的保留。这使得模型能够从视觉上下文和提示中学习，也可以仅从其中之一学习。此外，我们使我们的模型能够处理少样本设置，以有效地解决各种上下文学习场景。我们的实验和人工评估表明，Context Diffusion在域内和域外任务中均表现出色，与现有模型相比，整体上提高了图像质量和上下文保真度。", "summary": "Context Diffusion是一个新颖的扩散框架，旨在解决现有图像生成模型在无文本提示时上下文学习能力不足的问题。该框架通过分离视觉上下文的编码和所需图像布局的保留，使模型能够高效地从视觉上下文、文本提示或两者中学习，并支持少样本设置。实验结果表明，Context Diffusion在图像质量和上下文保真度方面均优于现有模型，在域内和域外任务中均表现出色。", "keywords": "上下文扩散, 图像生成, 上下文学习, 扩散模型, 少样本学习", "comments": "该论文的创新点在于提出了Context Diffusion框架，特别是其分离视觉上下文编码和图像布局保留的设计，这有效解决了现有模型在无文本提示下上下文学习的局限性。这对于提高生成模型的泛化能力和实用性具有重要意义，使其在更多真实世界场景中可用。"}}
{"id": "2507.17423", "title": "A new data-driven energy-stable Evolve-Filter-Relax model for turbulent flow simulation", "authors": ["Anna Ivagnes", "Toby van Gastelen", "Syver Døving Agdestein", "Benjamin Sanderse", "Giovanni Stabile", "Gianluigi Rozza"], "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17423v1", "summary": "We present a novel approach to define the filter and relax steps in the\nevolve-filter-relax (EFR) framework for simulating turbulent flows. The EFR\nmain advantages are its ease of implementation and computational efficiency.\nHowever, as it only contains two parameters (one for the filter step and one\nfor the relax step) its flexibility is rather limited. In this work, we propose\na data-driven approach in which the optimal filter is found based on DNS data\nin the frequency domain. The optimization step is computationally efficient and\nonly involves one-dimensional least-squares problems for each wavenumber.\nAcross both decaying turbulence and Kolmogorov flow, our learned filter\ndecisively outperforms the standard differential filter and the Smagorinsky\nmodel, yielding significantly improved accuracy in energy spectra and in the\ntemporal evolution of both energy and enstrophy. In addition, the relax\nparameter is determined by requiring energy and/or enstrophy conservation,\nwhich enforces stability of the method and reduces the appearance of numerical\nwiggles, especially when the filter is built in scarce data regimes. Applying\nthe learned filter is also more computationally efficient compared to\ntraditional differential filters, as it circumvents solving a linear system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17423v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "湍流模拟中一种新的数据驱动型能量稳定演化-滤波-松弛模型", "tldr": "提出一种数据驱动的EFR模型，通过DNS数据优化滤波器和松弛参数，显著提高了湍流模拟的准确性和效率，同时保证了能量稳定性。", "motivation": "现有的演化-滤波-松弛（EFR）框架虽然易于实现且计算高效，但由于其仅包含两个参数，灵活性受限，导致模拟精度不足。", "method": "本文提出一种数据驱动方法，利用DNS数据在频域中寻找最优滤波器，该优化过程计算高效，仅涉及每个波数的一维最小二乘问题。松弛参数通过要求能量和/或涡量守恒来确定，这确保了方法的稳定性并减少了数值振荡，尤其在数据稀疏时。", "result": "学习到的滤波器在衰减湍流和Kolmogorov流中显著优于标准微分滤波器和Smagorinsky模型，在能量谱以及能量和涡量的瞬态演化方面提供了显著提高的精度。此外，应用学习到的滤波器比传统微分滤波器更具计算效率，因为它避免了求解线性系统。", "conclusion": "提出的数据驱动型能量稳定演化-滤波-松弛模型通过优化滤波器和松弛参数，显著提升了湍流模拟的准确性和计算效率，同时保证了数值稳定性。", "translation": "我们提出了一种定义演化-滤波-松弛（EFR）框架中滤波和松弛步骤的新方法，用于模拟湍流。EFR的主要优点是易于实现和计算效率高。然而，由于它只包含两个参数（一个用于滤波步骤，一个用于松弛步骤），其灵活性相当有限。在这项工作中，我们提出了一种数据驱动方法，其中基于频域中的DNS数据找到最优滤波器。优化步骤计算效率高，并且每个波数只涉及一维最小二乘问题。在衰减湍流和Kolmogorov流中，我们学习到的滤波器明显优于标准微分滤波器和Smagorinsky模型，在能量谱以及能量和涡量的瞬态演化方面产生了显著提高的精度。此外，松弛参数通过要求能量和/或涡量守恒来确定，这强制了方法的稳定性并减少了数值振荡的出现，特别是当滤波器在数据稀疏的情况下构建时。与传统的微分滤波器相比，应用学习到的滤波器也更具计算效率，因为它避免了求解线性系统。", "summary": "本文提出了一种新颖的数据驱动型演化-滤波-松弛（EFR）模型，用于湍流模拟，旨在克服传统EFR模型参数灵活性受限的问题。通过在频域利用DNS数据优化滤波器，并基于能量/涡量守恒确定松弛参数，该模型显著提升了模拟精度和计算效率。实验结果表明，该数据驱动模型在能量谱和能量/涡量演化方面优于现有模型，并有效减少了数值振荡。", "keywords": "湍流模拟, 数据驱动, 演化-滤波-松弛模型, 能量稳定, 频域优化", "comments": "这项工作创新性地将数据驱动方法引入EFR框架，解决了传统EFR模型灵活性不足的问题。通过在频域进行高效优化，并结合物理守恒律来确定参数，该模型在保持计算效率的同时显著提升了湍流模拟的精度和稳定性，为复杂流体模拟提供了新的思路。其优势在于结合了数据驱动的灵活性和物理约束的稳定性。"}}
{"id": "2507.14000", "title": "Photonic Fabric Platform for AI Accelerators", "authors": ["Jing Ding", "Trung Diep"], "categories": ["cs.PF", "cs.AI", "C.4"], "primary_category": "Subjects:       Performance (cs.PF)", "pdf_link": null, "comments": "Comments:      12 pages, 14 figures, 5 tables", "url": "http://arxiv.org/abs/2507.14000v3", "summary": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.", "comment": "12 pages, 14 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.14000v3", "cate": "cs.PF", "date": "2025-07-18", "updated": "2025-07-23", "AI": {"title_translation": "用于AI加速器的光子互联平台", "tldr": "本文介绍了Photonic FabricTM和PFA，这是一种光子使能的交换和内存子系统，通过消除固定内存与计算比的限制，显著提升了AI加速器（特别是大型语言模型）的性能和能效。", "motivation": "当前的XPU加速器设计普遍存在固定内存与计算比的硅片前端限制，这限制了AI训练和推理的并行策略效率以及内存容量和带宽的扩展。本文旨在通过引入光子互联平台来解决这一瓶颈。", "method": "本文提出了Photonic FabricTM和Photonic Fabric ApplianceTM (PFA)，这是一种集成了HBM3E内存、片上光子交换机和外部DDR5的2.5D电光系统级封装。PFA通过用连接到Photonic Fabric的芯粒取代XPU上的本地HBM堆栈，从而扩展内存容量和带宽。研究人员还引入了CelestiSim，一个轻量级分析模拟器，并在NVIDIA H100和H200系统上进行了验证，用于评估PFA在大型语言模型性能和能效方面的表现。", "result": "模拟结果显示，在405B参数的LLM推理中，吞吐量提高了3.66倍，延迟改善了1.40倍；在1T参数的LLM推理中，吞吐量提高了7.04倍，延迟改善了1.41倍。此外，在所有LLM训练场景中，重型集合操作的数据移动能耗节省了60-90%。", "conclusion": "Photonic Fabric Appliance (PFA) 通过提供灵活的内存扩展路径，显著提升了大型语言模型（LLM）的推理吞吐量、延迟以及训练阶段的数据移动能效，克服了当前AI加速器固定内存与计算比的根本性限制，并可应用于其他AI加速器设计。", "translation": "本文介绍了Photonic FabricTM和Photonic Fabric ApplianceTM (PFA)，这是一种光子使能的交换和内存子系统，具有低延迟、高带宽和低比特能耗的特点。通过在2.5D电光系统级封装中集成高带宽HBM3E内存、片上光子交换机和外部DDR5，PFA提供了高达32 TB的共享内存以及115 Tbps的全对全数字交换能力。Photonic FabricTM使分布式AI训练和推理能够更高效地执行并行策略。Photonic Fabric消除了几乎所有当前XPU加速器设计中观察到的限制固定内存与计算比的硅片前端约束。用连接到Photonic Fabric的芯粒取代XPU上的本地HBM堆栈，可以增加其内存容量和相应的内存带宽，从而提供一种灵活的扩展路径，远超片上HBM的限制。我们引入了CelestiSim，一个在NVIDIA H100和H200系统上验证的轻量级分析模拟器。它用于评估PFA在LLM参考性能和能耗方面的表现，而无需对GPU核心设计进行任何重大更改。PFA的模拟结果显示，在405B参数的LLM推理中，吞吐量提高了3.66倍，延迟改善了1.40倍；在1T参数的LLM推理中，吞吐量提高了7.04倍，延迟改善了1.41倍；在所有LLM训练场景中，重型集合操作的数据移动能耗节省了60-90%。虽然这些结果是针对NVIDIA GPU显示的，但它们同样适用于其他具有相同固定内存与计算根本限制的AI加速器设计（XPU）。", "summary": "本文提出了一种名为Photonic FabricTM和Photonic Fabric ApplianceTM (PFA) 的光子互联平台，旨在解决现有AI加速器中固定内存与计算比的限制。PFA通过集成光子交换和内存技术，提供了高带宽、低延迟和低能耗的共享内存系统，最高可达32 TB。通过用连接到Photonic Fabric的芯粒替换本地HBM，该平台能够显著扩展内存容量和带宽。基于CelestiSim模拟器在LLM上的评估显示，PFA在推理吞吐量和延迟方面有显著提升（最高7.04倍吞吐量和1.41倍延迟改善），并在训练阶段的数据移动中实现60-90%的能耗节省，证明了其在未来AI加速器中的巨大潜力。", "keywords": "光子互联, AI加速器, 共享内存, 大型语言模型, 性能提升", "comments": "这篇论文提出了一种创新的方法来解决当前AI加速器面临的内存瓶颈问题，即通过光子互联技术实现内存与计算单元的解耦。其主要创新点在于Photonic FabricTM和PFA的设计，通过光子交换和共享内存池，打破了传统硅片前端对内存容量和带宽的限制。这对于需要大规模内存和数据传输的AI工作负载，尤其是大型语言模型，具有重要意义。性能提升和显著的能耗节省表明了该技术在提高AI系统效率方面的巨大潜力。该研究的普适性（适用于其他XPU）也增强了其影响力。"}}
{"id": "2507.09184", "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ACM MM 2025", "url": "http://arxiv.org/abs/2507.09184v2", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "comment": "Accepted in ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.09184v2", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-23", "AI": {"title_translation": "MCA-LLaVA：曼哈顿因果注意力减少大型视觉语言模型中的幻觉", "tldr": "大型视觉语言模型（LVLMs）中的幻觉是一个重大挑战，本文发现旋转位置编码（RoPE）的长期衰减导致了图像对齐偏差。为解决此问题，论文提出了MCA-LLaVA，该模型基于曼哈顿距离，将长期衰减扩展为二维多向空间衰减，并整合图像令牌的一维序列顺序和二维空间位置进行位置建模，从而有效减轻了幻觉。实验结果证明了其有效性和通用性。", "motivation": "大型视觉语言模型（LVLMs）中的幻觉是一个重大挑战，其主要原因在于多模态特征之间的错位。具体而言，用于LVLMs位置建模的旋转位置编码（RoPE）的长期衰减对多模态对齐产生了负面影响，导致指令令牌对图像令牌的感知存在偏见（优先感知右下区域），从而引起图像对齐偏差，导致图像-指令交互不足和次优的多模态对齐。", "method": "提出MCA-LLaVA模型，该模型基于曼哈顿距离，将旋转位置编码（RoPE）的长期衰减扩展为二维、多方向的空间衰减。MCA-LLaVA通过整合图像令牌的一维序列顺序和二维空间位置进行位置建模，以增强指令对不同空间位置图像令牌的感知，从而减轻图像对齐偏差。", "result": "MCA-LLaVA在各种幻觉和通用基准测试中均表现出有效性和通用性。", "conclusion": "MCA-LLaVA通过缓解图像对齐偏差，成功减轻了大型视觉语言模型中的幻觉问题。", "translation": "幻觉对大型视觉语言模型（LVLMs）构成了重大挑战，多模态特征之间的错位被认为是关键因素。本文揭示了用于LVLMs位置建模的旋转位置编码（RoPE）中长期衰减对多模态对齐的负面影响。具体而言，在长期衰减下，指令令牌对位于二维空间中不同位置的图像令牌表现出不均匀的感知：优先感知右下区域的图像令牌，因为在一维序列中，这些令牌在位置上更接近指令令牌。这种有偏见的感知导致图像-指令交互不足和次优的多模态对齐。我们将这种现象称为图像对齐偏差。为了增强指令对不同空间位置图像令牌的感知，我们提出了基于曼哈顿距离的MCA-LLaVA，它将长期衰减扩展为二维、多方向的空间衰减。MCA-LLaVA整合了图像令牌的一维序列顺序和二维空间位置进行位置建模，通过缓解图像对齐偏差来减轻幻觉。MCA-LLaVA在各种幻觉和通用基准测试中的实验结果证明了其有效性和通用性。代码可在https://github.com/ErikZ719/MCA-LLaVA访问。", "summary": "大型视觉语言模型（LVLMs）中的幻觉问题源于多模态特征错位，本文发现旋转位置编码（RoPE）的长期衰减导致指令令牌对图像令牌的感知存在图像对齐偏差。为解决此问题，研究提出了MCA-LLaVA，该模型基于曼哈顿距离，将RoPE的衰减扩展为二维多向空间衰减，并整合图像令牌的序列顺序和空间位置进行建模。实验证明，MCA-LLaVA有效减轻了幻觉并提升了模型通用性。", "keywords": "大型视觉语言模型, 幻觉, 旋转位置编码, 曼哈顿因果注意力, 多模态对齐", "comments": "这篇论文的创新点在于发现了旋转位置编码（RoPE）在LVLMs中导致图像对齐偏差这一新问题，并提出了基于曼哈顿距离的二维多向空间衰减来解决。这种方法通过更精细地建模图像令牌的空间位置，有效提升了多模态对齐，为减少LVLMs幻觉提供了一个新颖且有效的路径。其重要性在于直接解决了LVLMs部署中的一个关键障碍。"}}
{"id": "2507.17168", "title": "Improving LLMs' Generalized Reasoning Abilities by Graph Problems", "authors": ["Qifan Zhang", "Nuo Chen", "Zehua Li", "Miao Peng", "Jing Tang", "Jia Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      COLM2025", "url": "http://arxiv.org/abs/2507.17168v1", "summary": "Large Language Models (LLMs) have made remarkable strides in reasoning tasks,\nyet their performance often falters on novel and complex problems.\nDomain-specific continued pretraining (CPT) methods, such as those tailored for\nmathematical reasoning, have shown promise but lack transferability to broader\nreasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning\n(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,\nspanning pathfinding, network analysis, numerical computation, and topological\nreasoning, require sophisticated logical and relational reasoning, making them\nideal for teaching diverse reasoning patterns. To achieve this, we introduce\nGraphPile, the first large-scale corpus specifically designed for CPT using GPR\ndata. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes\nchain-of-thought, program-of-thought, trace of execution, and real-world graph\ndata. Using GraphPile, we train GraphMind on popular base models Llama 3 and\n3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in\nmathematical reasoning and up to 21.2 percent improvement in non-mathematical\nreasoning tasks such as logical and commonsense reasoning. By being the first\nto harness GPR for enhancing reasoning patterns and introducing the first\ndataset of its kind, our work bridges the gap between domain-specific\npretraining and universal reasoning capabilities, advancing the adaptability\nand robustness of LLMs.", "comment": "COLM2025", "pdf_url": "http://arxiv.org/pdf/2507.17168v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过图问题提升LLM的通用推理能力", "tldr": "大型语言模型（LLMs）在推理任务上表现出色，但在新颖和复杂问题上仍有不足。本文开创性地使用图问题推理（GPR）来增强LLMs的通用推理能力。为此，我们引入了GraphPile，这是首个专为GPR数据设计的持续预训练大型语料库。利用GraphPile，我们在流行的基础模型上训练了GraphMind，在数学和非数学推理任务中都取得了显著的准确性提升。", "motivation": "大型语言模型（LLMs）在推理任务上取得了显著进展，但其在新颖和复杂问题上的表现常常不佳。领域特定的持续预训练（CPT）方法（如针对数学推理的方法）虽然有前景，但缺乏向更广泛推理任务的迁移能力。", "method": "本文开创性地使用图问题推理（GPR）来增强LLMs的通用推理能力。GPR任务涵盖寻路、网络分析、数值计算和拓扑推理，需要复杂的逻辑和关系推理。为此，我们引入了GraphPile，这是第一个专门为使用GPR数据进行持续预训练而设计的大规模语料库，包含109亿个tokens，涵盖23个图任务，包括思维链、程序思维、执行轨迹和真实世界图数据。我们使用GraphPile在流行的基础模型Llama 3、3.1和Gemma 2上训练了GraphMind。", "result": "使用GraphPile，我们在数学推理中实现了高达4.9%的准确率提升，在逻辑和常识推理等非数学推理任务中实现了高达21.2%的改进。", "conclusion": "本文首次利用GPR来增强推理模式，并引入了同类中的第一个数据集，弥合了领域特定预训练和通用推理能力之间的鸿沟，提升了LLMs的适应性和鲁棒性。", "translation": "大型语言模型（LLMs）在推理任务上取得了显著进展，但其在新颖和复杂问题上的表现常常不佳。领域特定的持续预训练（CPT）方法，例如那些为数学推理量身定制的方法，已经显示出前景但缺乏向更广泛推理任务的迁移能力。在这项工作中，我们开创性地使用图问题推理（GPR）来增强LLMs的通用推理能力。GPR任务涵盖寻路、网络分析、数值计算和拓扑推理，需要复杂的逻辑和关系推理，使其成为教授多样化推理模式的理想选择。为了实现这一点，我们引入了GraphPile，这是第一个专门为使用GPR数据进行CPT而设计的大规模语料库。该数据集包含109亿个tokens，涵盖23个图任务，包括思维链、程序思维、执行轨迹和真实世界图数据。使用GraphPile，我们在流行的基础模型Llama 3和3.1以及Gemma 2上训练了GraphMind，在数学推理中实现了高达4.9%的准确率提升，在逻辑和常识推理等非数学推理任务中实现了高达21.2%的改进。通过首次利用GPR来增强推理模式并引入同类中的第一个数据集，我们的工作弥合了领域特定预训练和通用推理能力之间的鸿沟，提升了LLMs的适应性和鲁棒性。", "summary": "本文旨在通过引入图问题推理（GPR）来提升大型语言模型（LLMs）的通用推理能力，以解决LLMs在处理新颖和复杂问题时的不足。研究人员构建了GraphPile，一个包含109亿tokens和23个图任务的大规模数据集，用于对LLMs进行持续预训练。他们利用GraphPile在Llama 3/3.1和Gemma 2等基础模型上训练了GraphMind，结果显示在数学推理上准确率提高了4.9%，在逻辑和常识推理等非数学任务上提高了21.2%，证明了GPR在增强LLMs通用推理能力方面的有效性。", "keywords": "图问题推理, LLM, 通用推理, 持续预训练, GraphPile", "comments": "本文的创新点在于首次将图问题推理（GPR）应用于提升大型语言模型的通用推理能力，并为此构建了首个大规模、多任务的GPR数据集GraphPile。这项工作对于弥合领域特定预训练与通用推理之间的差距具有重要意义，有望显著增强LLM的适应性和鲁棒性，使其能更好地应对未见过的复杂推理任务。"}}
{"id": "2505.17764", "title": "Dynamic Graph Embedding Through Hub-aware Random Walks", "authors": ["Aleksandar Tomčić", "Miloš Savić", "Dušan Simić", "Miloš Radovanović"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17764v2", "summary": "The role of high-degree nodes, or hubs, in shaping graph dynamics and\nstructure is well-recognized in network science, yet their influence remains\nunderexplored in the context of dynamic graph embedding. Recent advances in\nrepresentation learning for graphs have shown that random walk-based methods\ncan capture both structural and temporal patterns, but often overlook the\nimpact of hubs on walk trajectories and embedding stability. In this paper, we\nintroduce DeepHub, a method for dynamic graph embedding that explicitly\nintegrates hub sensitivity into random walk sampling strategies. Focusing on\ndynnode2vec as a representative dynamic embedding method, we systematically\nanalyze the effect of hub-biased walks across nine real-world temporal\nnetworks. Our findings reveal that standard random walks tend to overrepresent\nhub nodes, leading to embeddings that underfit the evolving local context of\nless-connected nodes. By contrast, hub-aware walks can balance exploration,\nresulting in embeddings that better preserve temporal neighborhood structure\nand improve downstream task performance. These results suggest that\nhub-awareness is an important yet overlooked factor in dynamic graph embedding,\nand our work provides a foundation for more robust, structure-sensitive\nrepresentation learning in evolving networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17764v2", "cate": "cs.SI", "date": "2025-05-23", "updated": "2025-07-23", "AI": {"title_translation": "通过中心感知随机游走进行动态图嵌入", "tldr": "DeepHub是一种动态图嵌入方法，通过将中心节点敏感性整合到随机游走采样策略中，解决了现有方法对中心节点过度表示的问题，提高了嵌入质量和下游任务性能。", "motivation": "现有图表示学习中的随机游走方法在动态图嵌入中往往忽视高度节点（中心节点）的影响，导致嵌入对不常连接节点的局部上下文拟合不足，且中心节点在随机游走中被过度表示。", "method": "本论文介绍了DeepHub，一种动态图嵌入方法，它将中心节点敏感性明确地整合到随机游走采样策略中。研究以dynnode2vec为例，系统分析了九个真实世界时间网络中中心节点偏向性游走的效果。", "result": "研究发现，标准随机游走倾向于过度表示中心节点，导致嵌入对连接较少节点的演变局部上下文拟合不足。相比之下，中心感知游走可以平衡探索，从而使嵌入更好地保留时间邻域结构并提高下游任务性能。", "conclusion": "中心感知是动态图嵌入中一个重要但被忽视的因素。本工作为演化网络中更鲁棒、结构敏感的表示学习奠定了基础。", "translation": "高阶节点（或称中心节点）在塑造图的动态和结构方面的作用在网络科学中得到了广泛认可，但它们在动态图嵌入背景下的影响力仍未得到充分探索。图表示学习的最新进展表明，基于随机游走的方法可以捕获结构和时间模式，但往往忽略了中心节点对游走轨迹和嵌入稳定性的影响。在本文中，我们引入了DeepHub，一种动态图嵌入方法，它明确地将中心节点敏感性整合到随机游走采样策略中。我们以dynnode2vec作为代表性的动态嵌入方法，系统分析了中心节点偏向性游走在九个真实世界时间网络中的效果。我们的发现表明，标准随机游走倾向于过度表示中心节点，导致嵌入对连接较少节点的演变局部上下文拟合不足。相比之下，中心感知游走可以平衡探索，从而使嵌入更好地保留时间邻域结构并提高下游任务性能。这些结果表明，中心感知是动态图嵌入中一个重要但被忽视的因素，我们的工作为演化网络中更鲁棒、结构敏感的表示学习提供了基础。", "summary": "本论文提出DeepHub，一种新的动态图嵌入方法，通过在随机游走采样中融入对高度节点（中心节点）的敏感性，解决了现有方法对中心节点过度表示的问题。研究以dynnode2vec为基础，在多个真实世界时间网络上验证了其有效性。结果显示，与标准随机游走相比，DeepHub的中心感知游走能更好地平衡图探索，从而生成更能保留时间邻域结构、并提升下游任务性能的嵌入。这强调了中心感知在动态图嵌入中的重要性。", "keywords": "动态图嵌入, 中心节点, 随机游走, 表示学习, 时间网络", "comments": "本文的创新点在于明确地将中心节点敏感性整合到动态图嵌入的随机游走策略中，解决了现有方法中中心节点过度表示导致嵌入质量下降的问题。这为动态网络中的表示学习提供了更鲁棒和结构敏感的基础，具有重要的实际意义。"}}
{"id": "2507.17328", "title": "A Learning-based Domain Decomposition Method", "authors": ["Rui Wu", "Nikola Kovachki", "Burigede Liu"], "categories": ["cs.LG", "math-ph", "math.MP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17328v1", "summary": "Recent developments in mechanical, aerospace, and structural engineering have\ndriven a growing need for efficient ways to model and analyse structures at\nmuch larger and more complex scales than before. While established numerical\nmethods like the Finite Element Method remain reliable, they often struggle\nwith computational cost and scalability when dealing with large and\ngeometrically intricate problems. In recent years, neural network-based methods\nhave shown promise because of their ability to efficiently approximate\nnonlinear mappings. However, most existing neural approaches are still largely\nlimited to simple domains, which makes it difficult to apply to real-world PDEs\ninvolving complex geometries. In this paper, we propose a learning-based domain\ndecomposition method (L-DDM) that addresses this gap. Our approach uses a\nsingle, pre-trained neural operator-originally trained on simple domains-as a\nsurrogate model within a domain decomposition scheme, allowing us to tackle\nlarge and complicated domains efficiently. We provide a general theoretical\nresult on the existence of neural operator approximations in the context of\ndomain decomposition solution of abstract PDEs. We then demonstrate our method\nby accurately approximating solutions to elliptic PDEs with discontinuous\nmicrostructures in complex geometries, using a physics-pretrained neural\noperator (PPNO). Our results show that this approach not only outperforms\ncurrent state-of-the-art methods on these challenging problems, but also offers\nresolution-invariance and strong generalization to microstructural patterns\nunseen during training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17328v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种基于学习的域分解方法", "tldr": "本文提出了一种名为L-DDM的基于学习的域分解方法，它利用预训练的神经网络算子作为代理模型，有效解决了复杂几何结构中偏微分方程的建模和计算成本问题，并在性能上超越了现有先进方法，同时具备分辨率不变性和对未知微结构模式的泛化能力。", "motivation": "机械、航空航天和结构工程领域对更大、更复杂结构的建模和分析效率需求日益增长。传统数值方法如有限元法在处理大型、几何复杂的结构时面临计算成本和可扩展性挑战。现有神经网络方法大多局限于简单域，难以应用于涉及复杂几何的实际偏微分方程。", "method": "本文提出了一种基于学习的域分解方法（L-DDM）。该方法利用一个单一的、预训练的神经网络算子（最初在简单域上训练）作为域分解方案中的代理模型，从而能够高效处理大型和复杂的域。文章还提供了在抽象偏微分方程的域分解求解背景下，神经网络算子近似存在性的通用理论结果。", "result": "该方法通过使用物理预训练的神经网络算子（PPNO）准确逼近复杂几何中具有不连续微结构的椭圆偏微分方程的解。结果表明，该方法不仅在这些挑战性问题上优于当前的先进方法，而且还提供了分辨率不变性和对训练期间未见的微结构模式的强大泛化能力。", "conclusion": "本文提出的基于学习的域分解方法（L-DDM）有效解决了复杂几何结构中偏微分方程的计算挑战，通过结合预训练神经网络算子和域分解，实现了高效、高精度、分辨率不变和强泛化的性能，超越了现有技术水平。", "translation": "机械、航空航天和结构工程领域的最新发展推动了对以比以往更大、更复杂规模建模和分析结构的有效方法的需求日益增长。尽管有限元法等成熟的数值方法仍然可靠，但它们在处理大型和几何复杂的问​​题时，常常面临计算成本和可扩展性的困扰。近年来，基于神经网络的方法因其能够有效逼近非线性映射而显示出前景。然而，大多数现有神经网络方法仍然主要局限于简单域，这使得它们难以应用于涉及复杂几何的实际偏微分方程。在本文中，我们提出了一种基于学习的域分解方法（L-DDM），以解决这一空白。我们的方法使用一个单一的、预训练的神经网络算子（最初在简单域上训练）作为域分解方案中的代理模型，使我们能够高效处理大型和复杂的域。我们提供了一个关于在抽象偏微分方程的域分解解背景下，神经网络算子近似存在性的通用理论结果。然后，我们通过使用物理预训练的神经网络算子（PPNO）准确逼近复杂几何中具有不连续微结构的椭圆偏微分方程的解来演示我们的方法。我们的结果表明，这种方法不仅在这些挑战性问题上优于当前的先进方法，而且还提供了分辨率不变性和对训练期间未见的微结构模式的强大泛化能力。", "summary": "本文提出了一种名为L-DDM的基于学习的域分解方法，旨在解决传统数值方法在处理复杂、大规模结构时面临的计算成本和可扩展性问题，以及现有神经网络方法在复杂几何域上的局限性。L-DDM将单一的预训练神经网络算子作为域分解框架中的代理模型，从而高效地处理复杂域。研究提供了神经网络算子近似在域分解求解抽象偏微分方程中的存在性理论，并通过求解复杂几何中具有不连续微结构的椭圆偏微分方程进行验证。实验结果表明，L-DDM在性能上超越了现有先进方法，并展现出分辨率不变性和对未知微结构模式的强大泛化能力。", "keywords": "域分解, 神经网络算子, 复杂几何, 偏微分方程, 机器学习", "comments": "这篇论文的创新点在于将神经网络算子与经典的域分解方法相结合，有效克服了传统数值方法和现有深度学习方法在处理复杂几何和大规模问题时的局限性。其提出的L-DDM方法不仅提高了计算效率和精度，还通过分辨率不变性和对未知微结构的泛化能力，展现了在工程应用中的巨大潜力。理论结果的提供也增强了方法的严谨性。"}}
{"id": "2312.15234", "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems", "authors": ["Xupeng Miao", "Gabriele Oliaro", "Zhihao Zhang", "Xinhao Cheng", "Hongyi Jin", "Tianqi Chen", "Zhihao Jia"], "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ACM Computing Surveys", "url": "http://arxiv.org/abs/2312.15234v2", "summary": "In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.", "comment": "ACM Computing Surveys", "pdf_url": "http://arxiv.org/pdf/2312.15234v2", "cate": "cs.LG", "date": "2023-12-23", "updated": "2025-07-23", "AI": {"title_translation": "迈向高效生成式大型语言模型服务：从算法到系统的综述", "tldr": "大型语言模型（LLMs）的部署面临效率挑战，本综述从算法到系统层面探讨了高效LLM服务的方法。", "motivation": "生成式大型语言模型（LLMs）的部署在服务效率方面面临巨大的计算强度和内存消耗挑战，尤其是在需要低延迟和高吞吐量的场景中。因此，迫切需要高效的LLM服务方法。", "method": "本综述从机器学习系统（MLSys）研究的角度，对高效LLM服务方法进行了深入分析，涵盖了从前沿算法修改到系统设计突破性变革的一系列解决方案。", "result": "Not mentioned in abstract", "conclusion": "本综述旨在提供对高效LLM服务现状和未来方向的全面理解，为研究人员和实践者克服有效LLM部署的障碍提供宝贵见解，从而重塑AI的未来。", "translation": "在快速发展的人工智能（AI）领域中，生成式大型语言模型（LLMs）处于前沿，正在彻底改变我们与数据交互的方式。然而，部署这些模型的计算强度和内存消耗在服务效率方面带来了巨大挑战，特别是在需要低延迟和高吞吐量的场景中。本综述从机器学习系统（MLSys）研究的角度，解决了对高效LLM服务方法的迫切需求，这处于先进AI创新和实际系统优化的关键点。我们提供了深入分析，涵盖了一系列解决方案，从前沿算法修改到系统设计的突破性变革。本综述旨在提供对高效LLM服务现状和未来方向的全面理解，为研究人员和实践者克服有效LLM部署的障碍提供宝贵见解，从而重塑AI的未来。", "summary": "本综述探讨了生成式大型语言模型（LLMs）部署中面临的效率挑战，尤其是在计算强度和内存消耗方面。文章从机器学习系统（MLSys）的视角，深入分析了从算法优化到系统设计改进的各种解决方案，旨在全面理解高效LLM服务的当前状态和未来发展方向，为研究人员和实践者提供克服部署障碍的宝贵见解。", "keywords": "大型语言模型, LLM服务, 效率, 算法, 系统, 综述", "comments": "本论文及时且重要，因为它针对当前大型语言模型部署中面临的核心效率问题进行了全面的综述。其创新之处在于涵盖了从底层算法到上层系统设计的广泛解决方案，为理解和解决LLM服务瓶颈提供了系统性的视角。对于推动LLM在实际应用中的落地具有重要指导意义。"}}
{"id": "2507.17725", "title": "On the Interaction of Compressibility and Adversarial Robustness", "authors": ["Melih Barsbey", "Antônio H. Ribeiro", "Umut Şimşekli", "Tolga Birdal"], "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17725v1", "summary": "Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17725v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "可压缩性与对抗性鲁棒性之间的相互作用", "tldr": "本文分析了神经网络中不同形式的可压缩性如何影响对抗性鲁棒性，发现压缩会产生可被对抗者利用的脆弱性，揭示了可压缩性与鲁棒性之间存在根本性张力。", "motivation": "现代神经网络需同时满足准确性、泛化能力、效率和鲁棒性等特性。尽管可压缩性与鲁棒性已被广泛研究，但两者之间的相互作用尚不明确。本文旨在统一理解它们之间的关系。", "method": "开发了一个有原则的框架来分析神经元级稀疏性和谱可压缩性如何影响对抗性鲁棒性。通过理论分析推导出一个鲁棒性界限，并通过合成和现实任务的经验评估来证实理论预测。", "result": "研究表明，压缩可以在表示空间中诱导少数高度敏感的方向，对抗者可以利用这些方向构建有效扰动。分析得出了一个简单但有启发性的鲁棒性界限，揭示了神经元和谱可压缩性如何通过它们对学习表示的影响来影响$L_\\infty$和$L_2$鲁棒性。这些脆弱性与压缩实现方式无关，并且在对抗性训练和迁移学习下依然存在，并促成了普遍对抗性扰动的出现。", "conclusion": "结构化可压缩性与鲁棒性之间存在根本性的张力。研究结果为设计既高效又安全的模型提供了新途径。", "translation": "现代神经网络需要同时满足一系列理想的特性：准确拟合训练数据、泛化到未见输入、参数和计算效率以及对对抗性扰动的鲁棒性。虽然可压缩性和鲁棒性都已被广泛研究，但对它们之间相互作用的统一理解仍然难以捉摸。在这项工作中，我们开发了一个有原则的框架来分析不同形式的可压缩性——例如神经元级稀疏性和谱可压缩性——如何影响对抗性鲁棒性。我们表明，这些形式的压缩可以在表示空间中诱导少数高度敏感的方向，对抗者可以利用这些方向来构建有效的扰动。我们的分析得出了一个简单但有启发性的鲁棒性界限，揭示了神经元和谱可压缩性如何通过它们对学习表示的影响来影响$L_\\infty$和$L_2$鲁棒性。至关重要的是，我们识别出的漏洞的出现与压缩的实现方式无关——无论是通过正则化、架构偏差还是隐式学习动力学。通过对合成和现实任务的经验评估，我们证实了我们的理论预测，并进一步证明这些漏洞在对抗性训练和迁移学习下仍然存在，并导致了普遍对抗性扰动的出现。我们的发现表明结构化可压缩性与鲁棒性之间存在根本性的张力，并为设计既高效又安全的模型提供了新途径。", "summary": "本文探讨了神经网络中可压缩性与对抗性鲁棒性之间未被充分探索的相互作用。它提出了一个框架来分析不同形式的压缩（例如，稀疏性、谱压缩）如何在表示空间中创建敏感方向，从而使模型容易受到对抗性攻击。这项工作提供了一个鲁棒性界限，并通过经验验证，这些固有的压缩脆弱性即使在对抗性训练下也依然存在，强调了模型效率和安全性之间的根本性权衡。", "keywords": "可压缩性, 对抗性鲁棒性, 神经网络, 稀疏性, 谱压缩", "comments": "这篇论文通过系统地分析模型可压缩性与对抗性鲁棒性之间的权衡，做出了重要的理论和实证贡献。发现压缩本身会产生脆弱性，无论压缩方法如何，这一点至关重要。它为设计既鲁棒又高效的模型开辟了新途径，挑战了这些特性可以同时优化的普遍假设，揭示了其间存在的根本性张力。"}}
{"id": "2504.06566", "title": "Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure", "authors": ["Minshuo Chen", "Renyuan Xu", "Yumin Xu", "Ruixun Zhang"], "categories": ["q-fin.ST", "cs.LG", "q-fin.MF"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06566v4", "summary": "Financial scenario simulation is essential for risk management and portfolio\noptimization, yet it remains challenging especially in high-dimensional and\nsmall data settings common in finance. We propose a diffusion factor model that\nintegrates latent factor structure into generative diffusion processes,\nbridging econometrics with modern generative AI to address the challenges of\nthe curse of dimensionality and data scarcity in financial simulation. By\nexploiting the low-dimensional factor structure inherent in asset returns, we\ndecompose the score function--a key component in diffusion models--using\ntime-varying orthogonal projections, and this decomposition is incorporated\ninto the design of neural network architectures. We derive rigorous statistical\nguarantees, establishing nonasymptotic error bounds for both score estimation\nat O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4}\nn^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather\nthan the number of assets d, surpassing the dimension-dependent limits in the\nclassical nonparametric statistics literature and making the framework viable\nfor markets with thousands of assets. Numerical studies confirm superior\nperformance in latent subspace recovery under small data regimes. Empirical\nanalysis demonstrates the economic significance of our framework in\nconstructing mean-variance optimal portfolios and factor portfolios. This work\npresents the first theoretical integration of factor structure with diffusion\nmodels, offering a principled approach for high-dimensional financial\nsimulation with limited data. Our code is available at\nhttps://github.com/xymmmm00/diffusion_factor_model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06566v4", "cate": "q-fin.ST", "date": "2025-04-09", "updated": "2025-07-23", "AI": {"title_translation": "扩散因子模型：生成具有因子结构的高维收益", "tldr": "本文提出了一个扩散因子模型，将潜在因子结构整合到生成扩散过程中，以解决金融模拟中高维和小数据设置的挑战。该模型利用资产收益中固有的低维因子结构，分解得分函数，并将其融入神经网络设计中，提供了严格的统计保证，并在数值和实证研究中表现出优越性能，首次将因子结构与扩散模型理论结合，为有限数据下的高维金融模拟提供了原则性方法。", "motivation": "金融情景模拟对风险管理和投资组合优化至关重要，但在金融领域常见的高维和小数据设置中，这仍然是一个挑战，存在维度诅咒和数据稀缺问题。", "method": "提出了一种扩散因子模型，将潜在因子结构整合到生成扩散过程中。通过利用资产收益中固有的低维因子结构，该模型使用时变正交投影分解得分函数，并将这种分解整合到神经网络架构的设计中。", "result": "该模型推导了严格的统计保证，建立了得分估计（O(d^{5/2} n^{-2/(k+5)})）和生成分布（O(d^{5/4} n^{-1/2(k+5)})）的非渐近误差界，主要由内在因子维度k驱动，而非资产数量d。数值研究证实了在小数据条件下潜在子空间恢复的优越性能。实证分析表明该框架在构建均值-方差最优投资组合和因子投资组合方面具有经济意义。", "conclusion": "本文首次将因子结构与扩散模型进行了理论整合，为有限数据下的高维金融模拟提供了一种原则性方法。", "translation": "金融情景模拟对于风险管理和投资组合优化至关重要，但在金融领域常见的高维和小数据设置中，这仍然是一个挑战。我们提出了一种扩散因子模型，将潜在因子结构整合到生成扩散过程中，将计量经济学与现代生成式AI相结合，以解决金融模拟中维度诅咒和数据稀缺的挑战。通过利用资产收益中固有的低维因子结构，我们使用时变正交投影分解得分函数——扩散模型中的一个关键组成部分，并将这种分解整合到神经网络架构的设计中。我们推导了严格的统计保证，建立了得分估计（O(d^{5/2} n^{-2/(k+5)})）和生成分布（O(d^{5/4} n^{-1/2(k+5)})）的非渐近误差界，主要由内在因子维度k驱动，而非资产数量d，这超越了经典非参数统计文献中依赖维度的限制，使得该框架适用于拥有数千种资产的市场。数值研究证实了在小数据条件下潜在子空间恢复的优越性能。实证分析表明我们的框架在构建均值-方差最优投资组合和因子投资组合方面具有经济意义。这项工作首次将因子结构与扩散模型进行了理论整合，为有限数据下的高维金融模拟提供了一种原则性方法。我们的代码可在https://github.com/xymmmm00/diffusion_factor_model 获取。", "summary": "该研究提出了一种扩散因子模型，旨在解决金融领域高维和小数据情景下的金融模拟挑战。该模型通过将潜在因子结构融入生成扩散过程，并利用时变正交投影分解得分函数来设计神经网络。它提供了严格的统计保证，其误差界主要取决于内在因子维度而非资产数量。数值和实证研究均验证了该模型在潜在子空间恢复和投资组合构建方面的优越性能和经济意义，标志着因子结构与扩散模型理论结合的首次尝试。", "keywords": "扩散因子模型, 金融模拟, 高维数据, 因子结构, 生成式AI", "comments": "本文的创新之处在于将计量经济学的因子结构与现代生成扩散模型相结合，为高维金融数据模拟提供了新的解决方案。其理论贡献在于首次建立了因子结构与扩散模型的整合，并提供了严格的统计误差界，这对于处理金融领域特有的维度诅咒和数据稀缺问题具有重要意义。该框架超越了传统方法的维度限制，使其能够应用于大规模市场，具有很高的实用价值。"}}
{"id": "2507.16844", "title": "TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning", "authors": ["Jie He", "Vincent Theo Willem Kenbeek", "Zhantao Yang", "Meixun Qu", "Ezio Bartocci", "Dejan Ničković", "Radu Grosu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16844v1", "summary": "We introduce TD-Interpreter, a specialized ML tool that assists engineers in\nunderstanding complex timing diagrams (TDs), originating from a third party,\nduring their design and verification process. TD-Interpreter is a visual\nquestion-answer environment which allows engineers to input a set of TDs and\nask design and verification queries regarding these TDs. We implemented\nTD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B\nMultimodal Large Language Model (MLLM). To address limited training data\navailability, we developed a synthetic data generation workflow that aligns\nvisual information with its textual interpretation. Our experimental evaluation\ndemonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o\nby a large margin on the evaluated benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16844v1", "cate": "cs.LG", "date": "2025-07-20", "updated": "2025-07-20", "AI": {"title_translation": "TD-Interpreter：通过视觉-语言学习增强对时序图的理解", "tldr": "TD-Interpreter是一个专门的机器学习工具，通过微调LLaVA并利用合成数据生成，帮助工程师理解复杂的时序图，并在评估中显著优于未优化的GPT-4o。", "motivation": "在设计和验证过程中，工程师理解第三方提供的复杂时序图存在困难，需要一个工具来辅助。", "method": "TD-Interpreter通过多模态学习实现，具体是通过微调轻量级7B多模态大型语言模型（MLLM）LLaVA。为了解决训练数据有限的问题，开发了一个合成数据生成工作流，将视觉信息与其文本解释对齐。", "result": "实验评估表明TD-Interpreter的实用性，在评估的基准测试中，它以很大的优势超越了未经微调的GPT-4o。", "conclusion": "TD-Interpreter是一个有效且实用的工具，能够显著提升工程师对复杂时序图的理解，其性能优于通用大型模型。", "translation": "我们引入了TD-Interpreter，一个专门的机器学习工具，旨在帮助工程师在设计和验证过程中理解来自第三方的复杂时序图（TDs）。TD-Interpreter是一个视觉问答环境，允许工程师输入一组时序图并提出关于这些时序图的设计和验证查询。我们通过微调LLaVA（一个轻量级7B多模态大型语言模型（MLLM））实现了TD-Interpreter的多模态学习。为了解决训练数据可用性有限的问题，我们开发了一个合成数据生成工作流，将视觉信息与其文本解释对齐。我们的实验评估证明了TD-Interpreter的实用性，它在评估的基准测试中以很大的优势超越了未经微调的GPT-4o。", "summary": "TD-Interpreter是一款专为工程师设计的机器学习工具，旨在协助他们理解复杂的第三方时序图。该工具提供一个视觉问答环境，允许用户输入时序图并进行设计和验证查询。其实现基于对LLaVA（一个7B多模态大语言模型）的微调，并通过开发合成数据生成工作流来克服训练数据稀缺的问题。实验结果显示，TD-Interpreter在性能上显著优于未经优化的GPT-4o。", "keywords": "时序图理解, 视觉-语言学习, TD-Interpreter, LLaVA, 合成数据生成", "comments": "本文提出了一种创新性的解决方案，将视觉-语言学习应用于工程领域中理解时序图的特定痛点。通过微调轻量级MLLM和开发合成数据生成流程，有效解决了专业领域数据稀缺的问题，并展示了其在特定任务上超越通用大型模型的潜力，具有重要的实用价值。"}}
{"id": "2507.17426", "title": "Information Entropy-Based Scheduling for Communication-Efficient Decentralized Learning", "authors": ["Jaiprakash Nagar", "Zheng Chen", "Marios Kountouris", "Photios A. Stavrou"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17426v1", "summary": "This paper addresses decentralized stochastic gradient descent (D-SGD) over\nresource-constrained networks by introducing node-based and link-based\nscheduling strategies to enhance communication efficiency. In each iteration of\nthe D-SGD algorithm, only a few disjoint subsets of nodes or links are randomly\nactivated, subject to a given communication cost constraint. We propose a novel\nimportance metric based on information entropy to determine node and link\nscheduling probabilities. We validate the effectiveness of our approach through\nextensive simulations, comparing it against state-of-the-art methods, including\nbetweenness centrality (BC) for node scheduling and \\textit{MATCHA} for link\nscheduling. The results show that our method consistently outperforms the\nBC-based method in the node scheduling case, achieving faster convergence with\nup to 60\\% lower communication budgets. At higher communication budgets (above\n60\\%), our method maintains comparable or superior performance. In the link\nscheduling case, our method delivers results that are superior to or on par\nwith those of \\textit{MATCHA}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17426v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于信息熵的通信高效去中心化学习调度", "tldr": "本文提出了一种基于信息熵的去中心化学习调度方法，以提高通信效率，其性能优于或与现有先进方法相当。", "motivation": "本文旨在解决资源受限网络上分布式随机梯度下降（D-SGD）的通信效率问题。", "method": "提出了一种基于信息熵的新颖重要性度量，用于确定节点和链路的调度概率。在D-SGD算法的每次迭代中，根据通信成本约束，随机激活少数不相交的节点或链路子集。", "result": "在节点调度方面，该方法在通信预算降低高达60%的情况下，收敛速度比基于BC的方法更快，并且在更高通信预算下保持相当或更优的性能。在链路调度方面，该方法的结果优于或与MATCHA相当。", "conclusion": "本文提出的基于信息熵的调度方法能够有效提高去中心化学习的通信效率和性能，优于或与现有先进方法相当。", "translation": "本文通过引入基于节点和基于链路的调度策略来提高通信效率，从而解决了资源受限网络上的去中心化随机梯度下降（D-SGD）问题。在D-SGD算法的每次迭代中，只有少数不相交的节点或链路子集在给定通信成本约束下被随机激活。我们提出了一种基于信息熵的新颖重要性度量来确定节点和链路调度概率。我们通过广泛的模拟验证了我们方法的有效性，并将其与最先进的方法进行了比较，包括用于节点调度的介数中心性（BC）和用于链路调度的MATCHA。结果表明，在节点调度情况下，我们的方法始终优于基于BC的方法，在通信预算降低高达60%的情况下实现更快的收敛。在更高的通信预算（高于60%）下，我们的方法保持相当或更优的性能。在链路调度情况下，我们的方法提供优于或与MATCHA相当的结果。", "summary": "本文提出了一种基于信息熵的调度方法，用于解决资源受限网络中去中心化随机梯度下降（D-SGD）的通信效率问题。该方法通过基于信息熵的重要性度量来确定节点和链路的激活概率。广泛的模拟结果表明，与介数中心性（BC）和MATCHA等现有先进方法相比，该方法在节点和链路调度方面均表现出优异或相当的性能，能够以更低的通信成本实现更快的收敛。", "keywords": "信息熵, 去中心化学习, 调度, 通信效率, 随机梯度下降", "comments": "该论文创新性地将信息熵应用于去中心化学习中的调度概率确定，为解决资源受限环境下的通信效率问题提供了一种新颖途径。其与现有方法相比所展现出的优异或相当的性能，突显了其潜在的实际应用价值。"}}
{"id": "2507.17268", "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis", "authors": ["Kailong Zhang", "Youwei Lyu", "Heng Guo", "Si Li", "Zhanyu Ma", "Boxin Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.17268v1", "summary": "Polarization images facilitate image enhancement and 3D reconstruction tasks,\nbut the limited accessibility of polarization cameras hinders their broader\napplication. This gap drives the need for synthesizing photorealistic\npolarization images.The existing polarization simulator Mitsuba relies on a\nparametric polarization image formation model and requires extensive 3D assets\ncovering shape and PBR materials, preventing it from generating large-scale\nphotorealistic images. To address this problem, we propose PolarAnything,\ncapable of synthesizing polarization images from a single RGB input with both\nphotorealism and physical accuracy, eliminating the dependency on 3D asset\ncollections. Drawing inspiration from the zero-shot performance of pretrained\ndiffusion models, we introduce a diffusion-based generative framework with an\neffective representation strategy that preserves the fidelity of polarization\nproperties. Experiments show that our model generates high-quality polarization\nimages and supports downstream tasks like shape from polarization.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.17268v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PolarAnything：基于扩散的偏振图像合成", "tldr": "PolarAnything提出了一种基于扩散模型的方法，仅通过RGB输入即可合成逼真且物理精确的偏振图像，解决了偏振相机获取受限和现有模拟器依赖大量3D资产的问题。", "motivation": "偏振图像有助于图像增强和3D重建，但偏振相机的可及性有限阻碍了其广泛应用。现有的偏振模拟器（如Mitsuba）依赖参数化模型和大量3D资产，难以生成大规模逼真的图像。", "method": "我们提出了PolarAnything，一个基于扩散的生成框架，它采用有效的表示策略来合成具有光照真实感和物理准确性的偏振图像，仅需单个RGB输入，并消除了对3D资产收集的依赖。", "result": "实验表明，我们的模型能够生成高质量的偏振图像，并支持偏振形状恢复等下游任务。", "conclusion": "PolarAnything成功地从单一RGB输入合成了高质量的偏振图像，解决了偏振图像数据获取的难题，并支持多种下游应用。", "translation": "偏振图像有助于图像增强和3D重建任务，但偏振相机的有限可及性阻碍了其更广泛的应用。这一空白促使了对合成逼真偏振图像的需求。现有的偏振模拟器Mitsuba依赖参数化偏振图像形成模型，并需要涵盖形状和PBR材料的广泛3D资产，这使其无法生成大规模逼真的图像。为了解决这个问题，我们提出了PolarAnything，它能够从单一RGB输入合成具有光照真实感和物理准确性的偏振图像，消除了对3D资产集合的依赖。借鉴预训练扩散模型的零样本性能，我们引入了一个基于扩散的生成框架，该框架具有有效的表示策略，可保持偏振特性的保真度。实验表明，我们的模型生成了高质量的偏振图像，并支持偏振形状恢复等下游任务。", "summary": "PolarAnything是一种新颖的基于扩散的生成框架，旨在解决偏振图像获取困难的问题。该模型能够仅从单个RGB输入合成高质量、逼真且物理精确的偏振图像，从而克服了现有偏振相机可及性有限以及传统模拟器对大量3D资产的依赖。通过借鉴预训练扩散模型的零样本能力，PolarAnything有效保留了偏振特性，并已在实验中证明其在生成图像质量和支持偏振形状恢复等下游任务方面的有效性。", "keywords": "偏振图像, 图像合成, 扩散模型, RGB输入, 物理准确性", "comments": "本文的创新之处在于利用扩散模型从单一RGB图像合成偏振图像，这显著降低了对昂贵偏振相机和大量3D资产的需求。这种方法有望极大促进偏振图像在图像增强和3D重建领域的应用，具有重要的实际意义。"}}
{"id": "2507.17706", "title": "HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging", "authors": ["Taha Ceritli", "Ondrej Bohdal", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17706v1", "summary": "Large language models (LLMs) often leverage adapters, such as low-rank-based\nadapters, to achieve strong performance on downstream tasks. However, storing a\nseparate adapter for each task significantly increases memory requirements,\nposing a challenge for resource-constrained environments such as mobile\ndevices. Although model merging techniques can reduce storage costs, they\ntypically result in substantial performance degradation. In this work, we\nintroduce HydraOpt, a new model merging technique that capitalizes on the\ninherent similarities between the matrices of low-rank adapters. Unlike\nexisting methods that produce a fixed trade-off between storage size and\nperformance, HydraOpt allows us to navigate this spectrum of efficiency and\nperformance. Our experiments show that HydraOpt significantly reduces storage\nsize (48% reduction) compared to storing all adapters, while achieving\ncompetitive performance (0.2-1.8% drop). Furthermore, it outperforms existing\nmerging techniques in terms of performance at the same or slightly worse\nstorage efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17706v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HydraOpt：探索适配器合并的效率-性能权衡", "tldr": "HydraOpt是一种新的模型合并技术，通过利用低秩适配器矩阵的相似性，显著减少了大型语言模型（LLMs）适配器的存储空间（48%），同时保持了有竞争力的性能（仅下降0.2-1.8%），优于现有合并技术，实现了效率和性能之间的灵活权衡。", "motivation": "大型语言模型（LLMs）使用适配器在下游任务上表现出色，但为每个任务存储单独的适配器会显著增加内存需求，这对于资源受限的环境（如移动设备）是一个挑战。现有的模型合并技术虽然可以降低存储成本，但通常会导致性能大幅下降。", "method": "本文引入了HydraOpt，一种新的模型合并技术。它利用低秩适配器矩阵之间固有的相似性。与现有方法不同，HydraOpt允许研究人员在存储大小和性能之间进行灵活的权衡，而不是产生固定的权衡。", "result": "实验表明，与存储所有适配器相比，HydraOpt显著减少了存储大小（减少48%），同时实现了具有竞争力的性能（仅下降0.2-1.8%）。此外，在相同或略低的存储效率下，它在性能方面优于现有的合并技术。", "conclusion": "HydraOpt提供了一种灵活且高效的方法来管理适配器存储，同时保持了竞争力性能，解决了大型语言模型在资源受限环境中应用时适配器存储和性能之间的权衡问题。", "translation": "大型语言模型（LLMs）通常利用适配器（例如基于低秩的适配器）在下游任务上实现强大的性能。然而，为每个任务存储单独的适配器会显著增加内存需求，这给资源受限的环境（如移动设备）带来了挑战。尽管模型合并技术可以降低存储成本，但它们通常会导致性能大幅下降。在这项工作中，我们引入了HydraOpt，一种新的模型合并技术，它利用了低秩适配器矩阵之间固有的相似性。与现有方法产生固定的存储大小和性能权衡不同，HydraOpt允许我们在这个效率和性能的范围内进行探索。我们的实验表明，与存储所有适配器相比，HydraOpt显著减少了存储大小（减少48%），同时实现了具有竞争力的性能（下降0.2-1.8%）。此外，在相同或略低的存储效率下，它在性能方面优于现有的合并技术。", "summary": "HydraOpt是一种新颖的模型合并技术，旨在解决大型语言模型（LLMs）在资源受限设备上因存储大量适配器而产生的内存挑战。该方法利用低秩适配器矩阵的固有相似性，允许在存储效率和模型性能之间进行灵活的权衡，这与现有固定权衡的方法不同。实验结果显示，HydraOpt可将存储空间减少48%，而性能仅下降0.2-1.8%，且在相同或略低的存储效率下，其性能优于其他现有合并技术。", "keywords": "适配器合并, LLMs, 存储效率, 性能权衡, HydraOpt", "comments": "HydraOpt的创新之处在于它能够“探索效率和性能的范围”，提供了灵活的权衡选择，而非固定的权衡，这相对于现有模型合并技术是一个显著的进步。它在将LLMs部署到移动设备等资源受限环境方面具有重要的实际应用价值。"}}
{"id": "2312.03885", "title": "Gathering and Exploiting Higher-Order Information when Training Large Structured Models", "authors": ["Pierre Wolinski"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.03885v4", "summary": "When training large models, such as neural networks, the full derivatives of\norder 2 and beyond are usually inaccessible, due to their computational cost.\nTherefore, among the second-order optimization methods, it is common to bypass\nthe computation of the Hessian by using first-order information, such as the\ngradient of the parameters (e.g., quasi-Newton methods) or the activations\n(e.g., K-FAC). In this paper, we focus on the exact and explicit computation of\nprojections of the Hessian and higher-order derivatives on well-chosen\nsubspaces relevant for optimization. Namely, for a given partition of the set\nof parameters, we compute tensors that can be seen as \"higher-order derivatives\naccording to the partition\", at a reasonable cost as long as the number of\nsubsets of the partition remains small. Then, we give some examples of how\nthese tensors can be used. First, we show how to compute a learning rate per\nsubset of parameters, which can be used for hyperparameter tuning. Second, we\nshow how to use these tensors at order 2 to construct an optimization method\nthat uses information contained in the Hessian. Third, we show how to use these\ntensors at order 3 (information contained in the third derivative of the loss)\nto regularize this optimization method. The resulting training step has several\ninteresting properties, including: it takes into account long-range\ninteractions between the layers of the trained neural network, which is usually\nnot the case in similar methods (e.g., K-FAC); the trajectory of the\noptimization is invariant under affine layer-wise reparameterization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.03885v4", "cate": "cs.LG", "date": "2023-12-06", "updated": "2025-07-23", "AI": {"title_translation": "在大规模结构化模型训练中收集和利用高阶信息", "tldr": "本文提出了一种计算和利用Hessian和高阶导数在特定子空间上的精确投影的方法，以在训练大型模型时获得更有效的优化和超参数调整。", "motivation": "在训练大型模型（如神经网络）时，由于计算成本高昂，通常无法获取二阶及更高阶的完整导数。现有的二阶优化方法（如准牛顿法、K-FAC）通常通过使用一阶信息来规避Hessian矩阵的计算。", "method": "本文专注于精确且显式地计算Hessian和高阶导数在精心选择的、与优化相关的子空间上的投影。具体而言，对于参数集合的给定划分，本文计算了可以视为“根据划分的高阶导数”的张量，只要划分的子集数量较少，成本是合理的。这些张量被用于：1. 为每个参数子集计算学习率，用于超参数调整；2. 构建一个利用Hessian信息的二阶优化方法；3. 使用三阶导数信息（损失函数的三阶导数）来正则化该优化方法。", "result": "所提出的训练步骤具有以下特性：它考虑了训练神经网络层之间的长程相互作用（这在类似方法中通常不常见，例如K-FAC）；优化轨迹在逐层仿射重参数化下是不变的。", "conclusion": "通过精确计算高阶导数在特定子空间上的投影，可以有效地在大型模型训练中利用高阶信息，从而实现更有效的超参数调整和具有理想性质的优化方法。", "translation": "当训练大型模型，例如神经网络时，由于计算成本高昂，通常无法获取二阶及更高阶的完整导数。因此，在二阶优化方法中，通常通过使用一阶信息（例如参数的梯度，如准牛顿法；或激活，如K-FAC）来规避Hessian矩阵的计算。在本文中，我们专注于精确且显式地计算Hessian和高阶导数在精心选择的、与优化相关的子空间上的投影。具体而言，对于参数集合的给定划分，我们计算了可以视为“根据划分的高阶导数”的张量，只要划分的子集数量较少，成本是合理的。然后，我们给出了一些如何使用这些张量的例子。首先，我们展示了如何为每个参数子集计算学习率，这可以用于超参数调整。其次，我们展示了如何使用这些二阶张量来构建一个利用Hessian信息的优化方法。第三，我们展示了如何使用这些三阶张量（损失函数三阶导数中包含的信息）来正则化这种优化方法。所得到的训练步骤具有几个有趣的特性，包括：它考虑了训练神经网络层之间的长程相互作用，这在类似方法（例如K-FAC）中通常不常见；优化轨迹在逐层仿射重参数化下是不变的。", "summary": "本文提出了一种在训练大型结构化模型时有效利用高阶导数信息的方法。鉴于完整高阶导数计算成本高昂，研究人员提出计算Hessian和更高阶导数在特定子空间上的精确投影。这些投影被组织成“根据划分的高阶导数”的张量，并被证明可用于多种用途：为参数子集计算学习率以进行超参数调整，构建利用Hessian信息的二阶优化方法，以及使用三阶导数信息对优化方法进行正则化。该方法在优化过程中能捕捉层间的长程相互作用，并具有仿射重参数化不变性。", "keywords": "高阶信息, 神经网络训练, 二阶优化, Hessian投影, 超参数调整", "comments": "这项研究的创新之处在于，它提供了一种在计算成本可控的前提下，精确且显式地利用高阶导数信息的方法。传统上，高阶导数因其计算复杂性而被规避。本文通过在精心选择的子空间上投影高阶导数，为优化和超参数调整提供了新的工具。其提出的优化方法能捕捉长程相互作用并具有重参数化不变性，这对于深度学习模型的训练具有重要意义。"}}
{"id": "2507.09984", "title": "Latent Diffusion Models with Masked AutoEncoders", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09984v2", "summary": "In spite of the remarkable potential of Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoders. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09984v2", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-23", "AI": {"title_translation": "带有掩码自编码器的潜在扩散模型", "tldr": "论文分析了潜在扩散模型（LDMs）中自编码器的关键特性，发现现有自编码器不足，并提出了基于掩码自编码器的新型变分自编码器（VMAE），将其集成到LDM中形成LDMAE。", "motivation": "尽管潜在扩散模型（LDMs）在图像生成方面潜力巨大，但自编码器的理想特性和优化设计尚未得到充分探索。", "method": "作者分析了LDMs中自编码器的作用，确定了潜在平滑度、感知压缩质量和重建质量三个关键特性。在此基础上，提出了利用掩码自编码器分层特征的变分掩码自编码器（VMAEs），并将其整合到LDM框架中，形成了带有掩码自编码器的潜在扩散模型（LDMAEs）。", "result": "现有自编码器未能同时满足潜在平滑度、感知压缩质量和重建质量这三个关键特性。论文提出了VMAEs，并将其成功集成到LDM框架中。", "conclusion": "论文通过提出VMAEs并集成到LDM中，旨在解决现有自编码器在LDMs中未能同时满足关键特性的问题，从而改进LDMs的性能。", "translation": "尽管潜在扩散模型（LDM）在图像生成方面具有显著潜力，但自编码器的理想特性和优化设计尚未得到充分探索。在这项工作中，我们分析了自编码器在LDM中的作用，并确定了三个关键特性：潜在平滑度、感知压缩质量和重建质量。我们证明了现有自编码器未能同时满足所有这三个特性，并提出了变分掩码自编码器（VMAE），它利用了掩码自编码器所保持的分层特征。我们将VMAE集成到LDM框架中，引入了带有掩码自编码器的潜在扩散模型（LDMAE）。", "summary": "本文深入分析了潜在扩散模型（LDM）中自编码器的作用，并识别出潜在平滑度、感知压缩质量和重建质量三个关键特性。研究发现现有自编码器无法同时满足这些特性。为此，作者提出了一种新型的变分掩码自编码器（VMAE），该模型利用了掩码自编码器的分层特征，并将其集成到LDM框架中，从而引入了带有掩码自编码器（LDMAE）的潜在扩散模型，旨在提升图像生成性能。", "keywords": "潜在扩散模型, 自编码器, 掩码自编码器, 图像生成, 变分掩码自编码器", "comments": "这篇论文的创新点在于系统地分析了潜在扩散模型中自编码器的关键特性，并针对现有自编码器的不足，提出了结合掩码自编码器优势的变分掩码自编码器（VMAE）。这种方法有望优化LDM的性能，尤其是在平衡潜在表示的平滑性、感知质量和重建精度方面。"}}
{"id": "2507.17139", "title": "Evaluation of the effects of frame time variation on VR task performance", "authors": ["Benjamin Watson", "Victoria Spaulding", "Neff Walker", "William Ribarsky"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17139v1", "summary": "We present a first study of the effects of frame time variations, in both\ndeviation around mean frame times and period of fluctuation, on task\nperformance in a virtual environment (VE). Chosen are open and closed loop\ntasks that are typical for current applications or likely to be prominent in\nfuture ones. The results show that at frame times in the range deemed\nacceptable for many applications, fairly large deviations in amplitude over a\nfairly wide range of periods do not significantly affect task performance.\nHowever, at a frame time often considered a minimum for immersive VR, frame\ntime variations do produce significant effects on closed loop task performance.\nThe results will be of use to designers of VEs and immersive applications, who\noften must control frame time variations due to large fluctuations of\ncomplexity (graphical and otherwise) in the VE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17139v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "评估帧时间变化对VR任务性能的影响", "tldr": "本研究首次探讨了帧时间变化（包括偏差和波动周期）对虚拟环境（VE）中任务性能的影响。发现在多数可接受的帧时间范围内，较大的帧时间波动对任务性能无显著影响；但在沉浸式VR的最低帧时间下，帧时间变化会对闭环任务性能产生显著影响。", "motivation": "本研究旨在首次系统性地评估帧时间变化（包括围绕平均帧时间的偏差和波动周期）对虚拟环境（VE）中任务性能的影响。这对于虚拟环境和沉浸式应用的设计者至关重要，因为他们经常需要应对虚拟环境中复杂性（图形或其他方面）的剧烈波动导致的帧时间变化。", "method": "研究通过在虚拟环境中测试开环和闭环任务，评估了帧时间变化（包括平均帧时间的偏差和波动周期）对任务性能的影响。", "result": "结果表明，在许多应用认为可接受的帧时间范围内，相当大的振幅偏差在相当宽的周期范围内不会显著影响任务性能。然而，在通常被认为是沉浸式VR最低要求的帧时间下，帧时间变化确实对闭环任务性能产生了显著影响。", "conclusion": "帧时间变化对VR任务性能的影响取决于基线帧率，尤其是在较低但仍具沉浸感的帧率下，会对闭环任务性能产生显著影响。这些发现对虚拟环境和沉浸式应用的设计者具有实用价值。", "translation": "我们首次研究了帧时间变化（包括平均帧时间的偏差和波动周期）对虚拟环境（VE）中任务性能的影响。选择的任务是当前应用中典型或未来可能突出的开环和闭环任务。结果显示，在许多应用认为可接受的帧时间范围内，相当大的振幅偏差在相当宽的周期范围内不会显著影响任务性能。然而，在通常被认为是沉浸式VR最低要求的帧时间下，帧时间变化确实对闭环任务性能产生了显著影响。这些结果将对虚拟环境和沉浸式应用程序的设计者有用，他们经常需要控制由于虚拟环境复杂性（图形或其他方面）的剧烈波动而导致的帧时间变化。", "summary": "本论文研究了帧时间变化（包括偏差和波动周期）如何影响虚拟环境中的用户表现。研究通过使用开环和闭环任务发现，在可接受的帧率范围内，较大的帧时间偏差通常不会影响性能；但在被认为是沉浸式VR最低要求的帧率下，帧时间变化会显著降低闭环任务的性能。这些见解对需要管理虚拟应用中图形复杂性的设计者具有重要价值。", "keywords": "帧时间变化, VR任务性能, 虚拟环境, 沉浸式VR, 闭环任务", "comments": "该论文具有创新性，因为它首次系统地研究了帧时间变化（不仅仅是平均帧率）对VR任务性能的具体影响，并区分了开环和闭环任务。其重要性在于提供了实证数据，这对于VR应用设计者优化性能和用户体验至关重要，尤其是在复杂虚拟环境中平衡图形保真度与保持稳定帧时间方面。"}}
{"id": "2507.17338", "title": "Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks", "authors": ["Corrado Pezzato", "Ozan Çatal", "Toon Van de Maele", "Riddhi J. Pitliya", "Tim Verbelen"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17338v1", "summary": "Despite growing interest in active inference for robotic control, its\napplication to complex, long-horizon tasks remains untested. We address this\ngap by introducing a fully hierarchical active inference architecture for\ngoal-directed behavior in realistic robotic settings. Our model combines a\nhigh-level active inference model that selects among discrete skills realized\nvia a whole-body active inference controller. This unified approach enables\nflexible skill composition, online adaptability, and recovery from task\nfailures without requiring offline training. Evaluated on the Habitat Benchmark\nfor mobile manipulation, our method outperforms state-of-the-art baselines\nacross the three long-horizon tasks, demonstrating for the first time that\nactive inference can scale to the complexity of modern robotics benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17338v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于主动推理的移动操作用于长周期重排任务", "tldr": "本文提出了一种全分层主动推理架构，用于机器人长周期重排任务中的目标导向行为，该方法在Habitat基准测试中优于现有技术，首次证明了主动推理可以扩展到现代机器人基准测试的复杂性。", "motivation": "尽管主动推理在机器人控制中的应用日益增长，但其在复杂、长周期任务中的应用尚未经过测试。本文旨在解决这一空白。", "method": "本文引入了一种全分层主动推理架构，用于在真实的机器人环境中实现目标导向行为。该模型结合了一个高层主动推理模型，该模型通过全身主动推理控制器选择离散技能，实现了灵活的技能组合、在线适应性以及从任务失败中恢复，而无需离线训练。", "result": "在针对移动操作的Habitat基准测试中，本文提出的方法在三个长周期任务中均优于现有基线。", "conclusion": "本文首次证明了主动推理可以扩展到现代机器人基准测试的复杂性。", "translation": "尽管对机器人控制中的主动推理的兴趣日益增长，但其在复杂、长周期任务中的应用仍未得到检验。我们通过引入一个完全分层的主动推理架构来弥补这一空白，该架构用于在真实的机器人环境中实现目标导向行为。我们的模型结合了一个高级主动推理模型，该模型通过全身主动推理控制器选择离散技能。这种统一的方法实现了灵活的技能组合、在线适应性以及从任务失败中恢复，而无需离线训练。在针对移动操作的Habitat基准测试中进行评估，我们的方法在三个长周期任务中均优于现有技术基线，首次证明了主动推理可以扩展到现代机器人基准测试的复杂性。", "summary": "本文提出了一种新颖的全分层主动推理架构，用于解决机器人领域中复杂、长周期重排任务的移动操作问题。该架构结合了高层技能选择和低层全身控制，实现了无需离线训练的灵活技能组合、在线适应性和任务失败恢复。实验结果表明，该方法在Habitat基准测试中表现优异，超越了现有技术，证明了主动推理在现代机器人复杂任务中的可扩展性。", "keywords": "主动推理, 机器人控制, 移动操作, 长周期任务, 分层架构", "comments": "本文的创新之处在于首次将主动推理扩展到复杂、长周期的机器人重排任务，并提出了一种实用的全分层架构。其重要性在于无需离线训练即可实现在线适应和故障恢复，这对于实际机器人部署具有重要意义。该研究为主动推理在复杂机器人控制中的应用开辟了新方向。"}}
{"id": "2507.17110", "title": "Transient Stability-Driven Planning for the Optimal Sizing of Resilient AC/DC Hybrid Microgrids", "authors": ["Yi Wang", "Goran Strbac"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17110v1", "summary": "This paper proposes a transient stability-driven planning framework for the\noptimal sizing problem of resilient AC/DC hybrid microgrids (HMGs) under\ndifferent types of contingencies, capturing frequency and voltage stability\nrequirements as well as the frequency-voltage coupling dynamics of AC/DC\ninterlinking converters (ICs). The planning model is formulated into a\ndefender-attacker-defender (DAD) architecture, which can be further merged into\ntwo levels, i.e., upper-level and low-level problems, and then iteratively\nsolved by an enhanced genetic algorithm with sparsity calculation and local\nsearch. Regarding the operation stage, a novel transient stability-constrained\noptimal power flow (TSC-OPF) algorithm is proposed for static and transient\noperations of HMGs, capturing governor dynamics and automatic voltage regulator\nof conventional generators as well as the droop control dynamics of\ninverter-based resources (IBRs) for frequency control and voltage control,\nrespectively. Furthermore, a Lyapunov optimisation approach is developed to\ncapture the time-coupling property of energy storages (ESs) and then allow the\nTSC-OPF to be solved on an hourly basis with a second-scale resolution,\nachieving the co-optimisation of static and transient stability requirements.\nCase studies have been conducted to verify the effectiveness of the proposed\nplanning framework in obtaining cost-effective investment decisions for various\nresources while respecting transient stability requirements under different\ncontingencies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17110v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "瞬态稳定性驱动的弹性交直流混合微电网优化规模规划", "tldr": "本文提出了一种瞬态稳定性驱动的规划框架，用于弹性交直流混合微电网在不同偶发事件下的优化规模确定，并开发了考虑瞬态稳定性的潮流算法和 Lyapunov 优化方法。", "motivation": "现有微电网规划可能未充分考虑瞬态稳定性要求以及交直流互联转换器的频率-电压耦合动态，导致在偶发事件下系统弹性不足。因此，需要一个能够兼顾频率和电压稳定性要求，并捕获交直流耦合动态的优化规划框架。", "method": "本文提出了一个瞬态稳定性驱动的规划框架，用于弹性交直流混合微电网的优化规模确定。该规划模型被构造成一个攻防-防卫者（DAD）架构，并进一步合并为两级问题（上层和下层），通过结合稀疏性计算和局部搜索的增强型遗传算法迭代求解。在运行阶段，提出了一种新的瞬态稳定性约束最优潮流（TSC-OPF）算法，用于混合微电网的静态和瞬态运行，该算法捕获了常规发电机的调速器动态和自动电压调节器，以及基于逆变器资源（IBRs）的下垂控制动态。此外，开发了一种 Lyapunov 优化方法来捕获储能（ESs）的时间耦合特性，使得 TSC-OPF 能够以小时为基础、秒级分辨率求解，实现静态和瞬态稳定性要求的协同优化。", "result": "案例研究验证了所提出的规划框架在不同偶发事件下，在尊重瞬态稳定性要求的同时，获得各种资源具有成本效益的投资决策的有效性。", "conclusion": "本文提出的瞬态稳定性驱动的规划框架能够有效解决弹性交直流混合微电网的优化规模问题，并在考虑瞬态稳定性要求的同时，实现具有成本效益的投资决策。", "translation": "本文提出了一种瞬态稳定性驱动的规划框架，用于弹性交直流混合微电网（HMGs）在不同类型偶发事件下的优化规模问题，该框架捕获了频率和电压稳定性要求以及交直流互联转换器（ICs）的频率-电压耦合动态。该规划模型被构造成一个攻防-防卫者（DAD）架构，可以进一步合并为两级问题，即上层和下层问题，然后通过结合稀疏性计算和局部搜索的增强型遗传算法迭代求解。在运行阶段，提出了一种新颖的瞬态稳定性约束最优潮流（TSC-OPF）算法，用于HMGs的静态和瞬态运行，该算法分别捕获了常规发电机的调速器动态和自动电压调节器，以及基于逆变器资源（IBRs）用于频率控制和电压控制的下垂控制动态。此外，开发了一种Lyapunov优化方法来捕获储能（ESs）的时间耦合特性，并允许TSC-OPF以小时为基础、秒级分辨率求解，从而实现静态和瞬态稳定性要求的协同优化。案例研究已进行，以验证所提出的规划框架在不同偶发事件下，在尊重瞬态稳定性要求的同时，获得各种资源具有成本效益的投资决策的有效性。", "summary": "本文提出了一种瞬态稳定性驱动的规划框架，用于优化弹性交直流混合微电网的规模。该框架考虑了不同偶发事件下的频率和电压稳定性，以及交直流互联转换器的耦合动态。规划模型采用攻防-防卫者（DAD）架构，并通过增强型遗传算法求解。同时，提出了一种新的瞬态稳定性约束最优潮流（TSC-OPF）算法，结合Lyapunov优化方法，实现对微电网静态和瞬态运行的协同优化，并能以小时为单位、秒级分辨率处理储能的时间耦合特性。案例研究验证了该框架在实现成本效益投资决策的同时满足瞬态稳定性的有效性。", "keywords": "瞬态稳定性, 交直流混合微电网, 优化规模, 攻防-防卫者, 稳定性约束最优潮流", "comments": "本文的创新点在于将瞬态稳定性深度融入到交直流混合微电网的规划和运行中，特别是在考虑频率-电压耦合动态、攻防-防卫者架构以及小时级、秒级分辨率的Lyapunov优化方法，实现了静态和瞬态稳定性的协同优化。这对于提升微电网在复杂偶发事件下的韧性具有重要意义。"}}
{"id": "2507.17456", "title": "Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection", "authors": ["Francesco Tonini", "Lorenzo Vaquero", "Alessandro Conti", "Cigdem Beyan", "Elisa Ricci"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.17456v1", "summary": "Human-Object Interaction (HOI) detection aims to identify humans and objects\nwithin images and interpret their interactions. Existing HOI methods rely\nheavily on large datasets with manual annotations to learn interactions from\nvisual cues. These annotations are labor-intensive to create, prone to\ninconsistency, and limit scalability to new domains and rare interactions. We\nargue that recent advances in Vision-Language Models (VLMs) offer untapped\npotential, particularly in enhancing interaction representation. While prior\nwork has injected such potential and even proposed training-free methods, there\nremain key gaps. Consequently, we propose a novel training-free HOI detection\nframework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively\nutilizes textual and visual interaction representations within a multimodal\nregistry, enabling robust and nuanced interaction understanding. This registry\nincorporates a small set of visual cues and uses innovative interaction\nsignatures to improve the semantic alignment of verbs, facilitating effective\ngeneralization to rare interactions. Additionally, we propose a unique\nmulti-head attention mechanism that adaptively weights the contributions of the\nvisual and textual features. Experimental results demonstrate that our DYSCO\nsurpasses training-free state-of-the-art models and is competitive with\ntraining-based approaches, particularly excelling in rare interactions. Code is\navailable at https://github.com/francescotonini/dysco.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.17456v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "具有增强语义的动态评分用于免训练人-物交互检测", "tldr": "本文提出DYSCO，一个免训练的人-物交互检测框架，通过利用视觉-语言模型和多模态注册表，在稀有交互方面表现出色。", "motivation": "现有的人-物交互（HOI）检测方法严重依赖于大型手动标注数据集，这些数据集创建成本高昂、容易不一致，并且限制了模型向新领域和稀有交互的扩展性。视觉-语言模型（VLMs）的最新进展提供了尚未开发的潜力，但此前的工作仍存在关键差距。", "method": "本文提出DYSCO，一个新颖的免训练HOI检测框架。它通过多模态注册表有效地利用文本和视觉交互表示，以实现鲁棒且细致的交互理解。该注册表结合了少量视觉线索，并使用创新的交互签名来改善动词的语义对齐，从而促进对稀有交互的有效泛化。此外，还提出了一种独特的多头注意力机制，以自适应地加权视觉和文本特征的贡献。", "result": "实验结果表明，DYSCO超越了现有的免训练模型，并与基于训练的方法具有竞争力，尤其在检测稀有交互方面表现出色。", "conclusion": "DYSCO框架通过有效利用视觉-语言模型，实现了鲁棒的人-物交互检测，尤其在无需大量训练数据的情况下，对稀有交互具有卓越的性能。", "translation": "人-物交互（HOI）检测旨在识别图像中的人与物并解释它们之间的交互。现有的HOI方法严重依赖带有手动标注的大型数据集来从视觉线索中学习交互。这些标注的创建劳动密集，容易不一致，并限制了向新领域和稀有交互的扩展性。我们认为，视觉-语言模型（VLMs）的最新进展提供了尚未开发的潜力，特别是在增强交互表示方面。尽管以前的工作已经注入了这种潜力，甚至提出了免训练方法，但仍存在关键差距。因此，我们提出了一种新颖的免训练HOI检测框架，即具有增强语义的动态评分（DYSCO），它有效地利用多模态注册表中的文本和视觉交互表示，从而实现鲁棒和细致的交互理解。该注册表包含少量视觉线索，并使用创新的交互签名来改善动词的语义对齐，从而促进对稀有交互的有效泛化。此外，我们提出了一种独特的多头注意力机制，该机制自适应地加权视觉和文本特征的贡献。实验结果表明，我们的DYSCO超越了免训练的最新模型，并与基于训练的方法具有竞争力，尤其在稀有交互方面表现出色。代码可在https://github.com/francescotonini/dysco获取。", "summary": "本文提出了一种名为DYSCO的新型免训练人-物交互（HOI）检测框架，旨在解决现有方法对大量手动标注数据集的依赖以及在稀有交互上的泛化限制。DYSCO利用视觉-语言模型（VLMs）的潜力，通过多模态注册表有效整合文本和视觉交互表示。该框架引入了创新的交互签名以增强动词的语义对齐，并设计了独特的多头注意力机制来自适应地权衡视觉和文本特征。实验证明，DYSCO在免训练模型中达到领先水平，并能与基于训练的方法相媲美，尤其在检测稀有交互方面表现突出。", "keywords": "人-物交互检测, 免训练, 视觉-语言模型, 动态评分, 稀有交互", "comments": "本文的创新点在于提出了一个无需训练的HOI检测框架DYSCO，有效利用了视觉-语言模型的潜力，并通过多模态注册表和创新的交互签名解决了数据标注成本高昂和稀有交互泛化能力不足的问题。其免训练特性和在稀有交互上的卓越性能使其在实际应用中具有重要意义和吸引力。"}}
{"id": "2406.10447", "title": "The BabyView dataset: High-resolution egocentric videos of infants' and young children's everyday experiences", "authors": ["Bria Long", "Robert Z. Sparks", "Violet Xiang", "Stefan Stojanov", "Zi Yin", "Grace E. Keene", "Alvin W. M. Tan", "Steven Y. Feng", "Chengxu Zhuang", "Virginia A. Marchman", "Daniel L. K. Yamins", "Michael C. Frank"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, 4 tables and Appendix. Published in the Proceedings of the 8th Annual Conference on Cognitive Computational Neuroscience", "url": "http://arxiv.org/abs/2406.10447v2", "summary": "Human children far exceed modern machine learning algorithms in their sample\nefficiency, achieving high performance in key domains with much less data than\ncurrent models. This ''data gap'' is a key challenge both for building\nintelligent artificial systems and for understanding human development.\nEgocentric video capturing children's experience--their ''training data''--is a\nkey ingredient for comparison of humans and models and for the development of\nalgorithmic innovations to bridge this gap. Yet there are few such datasets\navailable, and extant data are low-resolution, have limited metadata, and\nimportantly, represent only a small set of children's experiences. Here, we\nprovide the first release of a large developmental egocentric video\ndataset--the BabyView dataset--recorded using a high-resolution camera with a\nlarge vertical field-of-view and gyroscope/accelerometer data. This 868 hour\ndataset includes egocentric videos from children spanning 6 months to 3 years\nof age in longitudinal, at-home contexts. We provide gold-standard annotations\nfor the evaluation of speech transcription, speaker diarization, and human pose\nestimation, and evaluate models in each of these domains. We train\nself-supervised language and vision models and evaluate their transfer to\nout-of-distribution tasks, including syntactic structure learning, object\nrecognition, depth estimation, and image segmentation. Although performance in\neach domain scales with dataset size, overall performance is relatively lower\nthan when models are trained on curated datasets, especially in the visual\ndomain. Our dataset stands as an open challenge for robust, human-like AI\nsystems: how can such systems achieve human-levels of success on the same scale\nand distribution of training data as humans?", "comment": "9 pages, 3 figures, 4 tables and Appendix. Published in the\n  Proceedings of the 8th Annual Conference on Cognitive Computational\n  Neuroscience", "pdf_url": "http://arxiv.org/pdf/2406.10447v2", "cate": "cs.CV", "date": "2024-06-14", "updated": "2025-07-22", "AI": {"title_translation": "BabyView数据集：婴儿和幼儿日常经验的高分辨率自我中心视频", "tldr": "该论文介绍了BabyView数据集，一个包含高分辨率婴儿和幼儿自我中心视频的大型数据集，旨在弥合人类和机器学习模型之间的数据效率差距，并为类人AI系统提供挑战。", "motivation": "人类儿童在数据效率方面远超现代机器学习算法，而现有数据集中高分辨率、高质量的儿童自我中心视频稀缺，且元数据有限，无法有效弥补机器与人类在学习效率上的“数据鸿沟”。", "method": "研究团队发布了BabyView数据集，该数据集包含868小时的6个月至3岁儿童的家庭自我中心视频，使用高分辨率摄像头录制，并包含陀螺仪/加速度计数据。数据集提供了语音转录、说话人识别和人体姿态估计的黄金标准注释。研究人员还训练了自监督语言和视觉模型，并评估了它们在域外任务（如句法结构学习、物体识别、深度估计和图像分割）上的迁移能力。", "result": "BabyView数据集包含868小时的视频，涵盖6个月至3岁儿童的家庭场景。模型评估显示，性能随数据集大小的增加而提升，但在视觉领域，总体性能低于在精选数据集上训练的模型。", "conclusion": "BabyView数据集为开发强大、类人的人工智能系统提出了一个开放的挑战：即AI系统如何在与人类相同规模和分布的训练数据上实现人类水平的成功。", "translation": "人类儿童在样本效率方面远远超过现代机器学习算法，在关键领域以远少于当前模型所需的数据量实现高性能。这种“数据鸿沟”对于构建智能人工系统和理解人类发展都是一个关键挑战。捕捉儿童经验（他们的“训练数据”）的自我中心视频是比较人类和模型以及开发算法创新以弥合这一鸿沟的关键要素。然而，此类数据集很少，现有数据分辨率低，元数据有限，更重要的是，仅代表了儿童经验的一小部分。在此，我们首次发布了一个大型的、发展性的自我中心视频数据集——BabyView数据集，该数据集使用高分辨率摄像头录制，具有大垂直视野和陀螺仪/加速度计数据。这个868小时的数据集包括6个月至3岁儿童在家庭环境中的纵向自我中心视频。我们为语音转录、说话人识别和人体姿态估计的评估提供了黄金标准注释，并评估了这些领域中的模型。我们训练了自监督语言和视觉模型，并评估了它们向分布外任务的迁移，包括句法结构学习、物体识别、深度估计和图像分割。尽管在每个领域中的性能都随数据集大小而扩展，但总体性能相对低于在精选数据集上训练的模型，尤其是在视觉领域。我们的数据集为强大、类人的人工智能系统提出了一个开放的挑战：这些系统如何在与人类相同规模和分布的训练数据上实现人类水平的成功？", "summary": "该论文介绍了BabyView数据集，这是一个大型、高分辨率的儿童自我中心视频数据集，旨在解决机器学习模型与人类在数据效率上的“数据鸿沟”。该数据集包含868小时6个月至3岁儿童的家庭视频，并提供语音转录、说话人识别和人体姿态估计的黄金标准注释。研究人员利用该数据集训练并评估了自监督语言和视觉模型在多项任务上的表现，发现性能随数据量增加而提升，但在视觉任务上仍低于精选数据集。该数据集为实现类人AI系统提供了新的研究挑战。", "keywords": "BabyView数据集, 自我中心视频, 儿童发展, 数据集, 机器学习", "comments": "BabyView数据集的创新之处在于其大规模、高分辨率和包含陀螺仪/加速度计数据的儿童自我中心视频，这为理解人类发展和构建更类人AI系统提供了独特的“训练数据”。其重要性在于弥补了现有数据集的不足，为研究儿童学习机制和AI的数据效率问题提供了宝贵资源。论文也坦承了当前模型在未经精选数据上的局限性，提出了一个值得深思的开放性挑战。"}}
{"id": "2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wei Wang", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16802v2", "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16802v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "Agentar-Fin-R1：通过领域专业知识、训练效率和高级推理增强金融智能", "tldr": "Agentar-Fin-R1是一个新的金融大语言模型系列，通过优化训练效率、增强领域专业性和推理能力，并在新提出的Finova基准测试中表现出色，解决了现有LLM在金融应用中推理和信任度不足的问题。", "motivation": "现有的大语言模型在金融应用中，面对需要复杂推理能力、严格可信度标准和高效领域适应性的场景时，普遍存在局限性。", "method": "本文引入了基于Qwen3基础模型的Agentar-Fin-R1系列金融大语言模型（8B和32B参数），旨在增强金融应用的推理能力、可靠性和领域专业化。优化方法整合了高质量、系统化的金融任务标签系统和全面的多层可信度保障框架，该框架包括高质量可信知识工程、多智能体可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练流程和动态归因系统，显著提高了训练效率。模型在Fineva、FinEval和FinanceIQ等主流金融基准以及MATH-500和GPQA-diamond等通用推理数据集上进行了全面评估，并创新性地提出了Finova评估基准，专注于智能体级别的金融推理和合规性验证。", "result": "实验结果表明，Agentar-Fin-R1不仅在金融任务上取得了最先进的性能，而且展现出卓越的通用推理能力。", "conclusion": "Agentar-Fin-R1被验证为高风险金融应用的有效且值得信赖的解决方案。", "translation": "大型语言模型（LLMs）在金融应用中展现出巨大的潜力；然而，现有模型在面对需要复杂推理能力、严格可信度标准和高效适应领域特定需求的场景时，经常表现出局限性。我们引入了Agentar-Fin-R1系列金融大型语言模型（8B和32B参数），该模型专门基于Qwen3基础模型进行工程设计，旨在增强金融应用的推理能力、可靠性和领域专业化。我们的优化方法整合了一个高质量、系统化的金融任务标签系统和一个全面的多层可信度保障框架。该框架包括高质量的可信知识工程、多智能体可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练流程和动态归因系统，我们实现了训练效率的显著提升。我们的模型在Fineva、FinEval和FinanceIQ等主流金融基准以及MATH-500和GPQA-diamond等通用推理数据集上进行了全面评估。为了彻底评估实际部署能力，我们创新性地提出了Finova评估基准，该基准专注于智能体级别的金融推理和合规性验证。实验结果表明，Agentar-Fin-R1不仅在金融任务上取得了最先进的性能，而且展现出卓越的通用推理能力，验证了其作为高风险金融应用的可信解决方案的有效性。Finova基准可在https://github.com/antgroup/Finova获取。", "summary": "Agentar-Fin-R1是一个针对金融领域设计的新型大语言模型系列（8B和32B参数），旨在解决现有LLM在金融应用中推理能力、可信度和领域适应性不足的问题。该模型基于Qwen3构建，通过整合高质量的金融任务标签系统、多层可信度保障框架（包括知识工程、数据合成和验证）以及优化训练流程（如两阶段训练和动态归因系统），显著提升了训练效率、领域专业性和推理能力。模型在多个主流金融及通用推理基准上进行了全面评估，并提出了创新的Finova基准用于智能体级别的金融推理和合规性验证。实验结果表明，Agentar-Fin-R1在金融任务上达到了最先进的性能，并展现出卓越的通用推理能力，证明其是高风险金融应用中一个有效且可信赖的解决方案。", "keywords": "金融大语言模型, 推理能力, 可信度, 领域专业化, Finova", "comments": "该论文的创新点在于提出了专门针对金融领域优化的Agentar-Fin-R1模型，并强调了在金融应用中至关重要的可信度保障框架。通过引入Finova这一专注于智能体级别金融推理和合规性验证的新评估基准，填补了现有评估体系的空白，对于推动金融AI的实际部署具有重要意义。模型的两阶段训练和动态归因系统也体现了对训练效率的关注。其重要性在于为高风险金融应用提供了一个更可靠、更智能的LLM解决方案。"}}
{"id": "2507.17188", "title": "LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks", "authors": ["Lijie Zheng", "Ji He", "Shih Yu Chang", "Yulong Shen", "Dusit Niyato"], "categories": ["cs.NI", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Mobile Computing", "url": "http://arxiv.org/abs/2507.17188v1", "summary": "This work tackles the physical layer security (PLS) problem of maximizing the\nsecrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy\nconstraints. Unlike prior studies that assume uniform UAV capabilities or\noverlook energy-security trade-offs, we consider a realistic scenario where\nUAVs with diverse payloads and computation resources collaborate to serve\nground terminals in the presence of eavesdroppers. To manage the complex\ncoupling between UAV motion and communication, we propose a hierarchical\noptimization framework. The inner layer uses a semidefinite relaxation\n(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex\n(d.c.) programming to solve the secrecy precoding problem with fixed UAV\npositions. The outer layer introduces a Large Language Model (LLM)-guided\nheuristic multi-agent reinforcement learning approach (LLM-HeMARL) for\ntrajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics\npolicy generated by the LLM, enabling UAVs to learn energy-aware,\nsecurity-driven trajectories without the inference overhead of real-time LLM\ncalls. The simulation results show that our method outperforms existing\nbaselines in secrecy rate and energy efficiency, with consistent robustness\nacross varying UAV swarm sizes and random seeds.", "comment": "Submitted to IEEE Transactions on Mobile Computing", "pdf_url": "http://arxiv.org/pdf/2507.17188v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大语言模型遇上天空：用于安全异构无人机网络的启发式多智能体强化学习", "tldr": "本文提出了一种名为LLM-HeMARL的启发式多智能体强化学习方法，结合大语言模型指导，用于解决异构无人机网络中物理层安全问题，以在能量约束下最大化保密速率，并在仿真中表现优于现有基线。", "motivation": "该工作旨在解决异构无人机网络（HetUAVNs）中最大化保密速率的物理层安全（PLS）问题，同时考虑推进能量约束。与之前假设无人机能力统一或忽视能量-安全权衡的研究不同，本文考虑了无人机具有不同载荷和计算资源的现实场景，以服务地面终端并对抗窃听者。", "method": "本文提出了一个分层优化框架。内层使用基于半定松弛（SDR）的S2DC算法，结合惩罚函数和凸差（d.c.）规划，在固定无人机位置下解决保密预编码问题。外层引入了一种由大语言模型（LLM）引导的启发式多智能体强化学习方法（LLM-HeMARL）进行轨迹优化。LLM-HeMARL有效结合了LLM生成的专家启发式策略，使无人机能够在没有实时LLM调用推理开销的情况下学习能量感知、安全驱动的轨迹。", "result": "仿真结果表明，本文提出的方法在保密速率和能量效率方面优于现有基线，并且在不同无人机群规模和随机种子下均表现出一致的鲁棒性。", "conclusion": "本文提出的LLM-HeMARL框架通过优化保密预编码和无人机轨迹，有效解决了异构无人机网络中的物理层安全问题，从而实现了卓越的保密速率和能量效率，并保持了鲁棒性。", "translation": "这项工作解决了在推进能量约束下最大化异构无人机网络（HetUAVNs）中保密速率的物理层安全（PLS）问题。与之前假设无人机能力统一或忽视能量-安全权衡的研究不同，我们考虑了一个现实场景，即具有不同载荷和计算资源的无人机协同为存在窃听者的地面终端提供服务。为了管理无人机运动和通信之间复杂的耦合，我们提出了一个分层优化框架。内层使用基于半定松弛（SDR）的S2DC算法，结合惩罚函数和凸差（d.c.）规划来解决固定无人机位置下的保密预编码问题。外层引入了一种由大语言模型（LLM）引导的启发式多智能体强化学习方法（LLM-HeMARL）进行轨迹优化。LLM-HeMARL有效地结合了LLM生成的专家启发式策略，使无人机能够在没有实时LLM调用推理开销的情况下学习能量感知、安全驱动的轨迹。仿真结果表明，我们的方法在保密速率和能量效率方面优于现有基线，并且在不同无人机群规模和随机种子下均表现出一致的鲁棒性。", "summary": "本文针对异构无人机网络（HetUAVNs）中在能量约束下最大化保密速率的物理层安全问题，提出了一种分层优化框架。内层利用基于SDR的S2DC算法解决固定无人机位置下的保密预编码；外层则引入LLM引导的启发式多智能体强化学习方法（LLM-HeMARL）进行轨迹优化。LLM-HeMARL通过整合LLM生成的专家启发式策略，实现了能量感知和安全驱动的无人机轨迹学习，同时避免了实时LLM调用的开销。仿真结果验证了该方法在保密速率和能量效率上均优于现有基线，并展现出良好的鲁棒性。", "keywords": "异构无人机网络, 物理层安全, 多智能体强化学习, 大语言模型, 轨迹优化", "comments": "该论文的创新之处在于将大语言模型（LLM）生成的启发式策略融入多智能体强化学习框架，用于无人机轨迹优化，以解决物理层安全问题。这种方法有效避免了实时LLM推理带来的高计算开销，同时利用了LLM的知识。分层优化框架的设计也有效地解耦了复杂的耦合问题。"}}
{"id": "2507.16826", "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models", "authors": ["Qikai Wei", "Huansheng Ning", "Chunlong Han", "Jianguo Ding"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16826v1", "summary": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising\nparadigm for enhancing the accuracy and factual consistency of content\ngenerated by large language models (LLMs). However, existing RAG studies\nprimarily focus on retrieving isolated segments using similarity-based matching\nmethods, while overlooking the intrinsic connections between them. This\nlimitation hampers performance in RAG tasks. To address this, we propose QMKGF,\na Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\nRetrieval Augmented Generation. First, we design prompt templates and employ\ngeneral-purpose LLMs to extract entities and relations, thereby generating a\nknowledge graph (KG) efficiently. Based on the constructed KG, we introduce a\nmulti-path subgraph construction strategy that incorporates one-hop relations,\nmulti-hop relations, and importance-based relations, aiming to improve the\nsemantic relevance between the retrieved documents and the user query.\nSubsequently, we designed a query-aware attention reward model that scores\nsubgraph triples based on their semantic relevance to the query. Then, we\nselect the highest score subgraph and enrich subgraph with additional triples\nfrom other subgraphs that are highly semantically relevant to the query.\nFinally, the entities, relations, and triples within the updated subgraph are\nutilised to expand the original query, thereby enhancing its semantic\nrepresentation and improving the quality of LLMs' generation. We evaluate QMKGF\non the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA\ndataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the\nBGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%).\nExperimental results demonstrate the effectiveness and superiority of the QMKGF\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16826v1", "cate": "cs.IR", "date": "2025-07-07", "updated": "2025-07-07", "AI": {"title_translation": "一种查询感知多路径知识图谱融合方法，用于增强大型语言模型中的检索增强生成", "tldr": "QMKGF是一种查询感知的多路径知识图谱融合方法，通过构建KG、多路径子图和查询感知注意力奖励模型来增强RAG，显著提高了LLM的生成质量。", "motivation": "现有检索增强生成（RAG）研究主要侧重于使用基于相似性的匹配方法检索孤立的片段，而忽略了它们之间的内在联系，这限制了RAG任务的性能。", "method": "1. 设计提示模板并使用通用LLM提取实体和关系以生成知识图谱（KG）。2. 基于KG引入多路径子图构建策略，包含一跳、多跳和基于重要性的关系。3. 设计查询感知注意力奖励模型，根据语义相关性对子图三元组进行评分。4. 选择最高分数的子图，并用其他与查询高度相关的子图中的三元组进行丰富。5. 利用更新后的子图中的实体、关系和三元组扩展原始查询，增强其语义表示。", "result": "在HotpotQA数据集上，QMKGF的ROUGE-1分数达到64.98%，比BGE-Rerank方法提高了9.72个百分点（从55.26%到64.98%）。在SQuAD、IIRC、Culture和MuSiQue数据集上也进行了评估。", "conclusion": "实验结果表明QMKGF方法有效且优越，能够通过增强查询的语义表示来提高大型语言模型的生成质量。", "translation": "检索增强生成（RAG）已逐渐成为一种很有前途的范式，用于提高大型语言模型（LLM）生成内容的准确性和事实一致性。然而，现有RAG研究主要侧重于使用基于相似性的匹配方法检索孤立的片段，而忽略了它们之间的内在联系。这一局限性阻碍了RAG任务的性能。为了解决这个问题，我们提出了QMKGF，一种查询感知多路径知识图谱融合方法，用于增强检索增强生成。首先，我们设计提示模板并采用通用LLM来提取实体和关系，从而高效地生成知识图谱（KG）。基于构建的KG，我们引入了一种多路径子图构建策略，该策略结合了一跳关系、多跳关系和基于重要性的关系，旨在提高检索文档和用户查询之间的语义相关性。随后，我们设计了一个查询感知注意力奖励模型，根据三元组与查询的语义相关性对其进行评分。然后，我们选择得分最高的子图，并用来自其他与查询高度语义相关的子图的额外三元组丰富该子图。最后，利用更新后的子图中的实体、关系和三元组来扩展原始查询，从而增强其语义表示并提高LLM的生成质量。我们在SQuAD、IIRC、Culture、HotpotQA和MuSiQue数据集上评估了QMKGF。在HotpotQA数据集上，我们的方法获得了64.98%的ROUGE-1分数，比BGE-Rerank方法提高了9.72个百分点（从55.26%到64.98%）。实验结果证明了QMKGF方法的有效性和优越性。", "summary": "该论文提出了QMKGF，一种查询感知多路径知识图谱融合方法，旨在解决现有RAG方法忽略检索片段间内在联系的问题。QMKGF通过高效构建知识图谱、采用多路径子图构建策略以及设计查询感知注意力奖励模型来增强查询的语义表示，从而提高大型语言模型的生成质量。实验结果表明，QMKGF在多个数据集上表现出优越性，尤其在HotpotQA数据集上显著优于基线方法。", "keywords": "检索增强生成, 知识图谱, 大型语言模型, 查询感知, 多路径融合", "comments": "该论文的创新点在于引入了查询感知和多路径知识图谱融合的概念，克服了传统RAG方法仅关注孤立片段的局限性。通过构建和利用知识图谱，并结合查询感知的注意力机制来选择和丰富相关信息，显著提升了检索增强生成的语义相关性和生成质量。该方法为RAG领域提供了新的视角和有效的解决方案。"}}
{"id": "2507.17106", "title": "Efficient and Distortion-less Spectrum Multiplexer via Neural Network-based Filter Banks", "authors": ["Jiazhao Wang", "Wenchao Jiang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17106v1", "summary": "Spectrum multiplexer enables simultaneous transmission of multiple\nnarrow-band IoT signals through gateway devices, thereby enhancing overall\nspectrum utilization. We propose a novel solution based on filter banks that\noffer increased efficiency and minimal distortion compared with conventional\nmethods. We follow a model-driven approach to integrate the neural networks\ninto the filter bank design by interpreting the neural network models as filter\nbanks. The proposed NN-based filter banks can leverage advanced learning\ncapabilities to achieve distortionless multiplexing and harness hardware\nacceleration for high efficiency. Then, we evaluate the performance of the\nspectrum multiplexer implemented by NN-based filter banks for various types of\nsignals and environmental conditions. The results show that it can achieve a\nlow distortion level down to $-39$dB normalized mean squared error.\nFurthermore, it achieves up to $35$ times execution efficiency gain and $10$dB\nSNR gain compared with the conventional methods. The field applications show\nthat it can handle both the heterogeneous and homogeneous IoT networks,\nresulting in high packet reception ratio at the standard receivers up to\n$98\\%$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17106v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于神经网络滤波器组的高效无失真频谱复用器", "tldr": "该论文提出了一种基于神经网络滤波器组的新型频谱复用器，与传统方法相比，它能实现高效、无失真的多信号传输，并在物联网应用中表现出色。", "motivation": "为了提高频谱利用率，实现网关设备同时传输多个窄带物联网信号。", "method": "该论文采用模型驱动的方法，将神经网络模型解释为滤波器组，并将其集成到滤波器组设计中。所提出的基于神经网络的滤波器组利用了高级学习能力来实现无失真复用，并利用硬件加速实现高效率。", "result": "所提出的频谱复用器实现了低至-39dB归一化均方误差的低失真水平。与传统方法相比，执行效率提高了35倍，信噪比提高了10dB。在现场应用中，它能处理异构和同构物联网网络，标准接收器的分组接收率高达98%。", "conclusion": "基于神经网络滤波器组的频谱复用器能够实现高效、无失真的多信号传输，并在物联网网络中表现出优越的性能，显著提高了频谱利用率和数据接收率。", "translation": "频谱复用器通过网关设备实现多个窄带物联网信号的同时传输，从而提高整体频谱利用率。我们提出了一种基于滤波器组的新型解决方案，与传统方法相比，它具有更高的效率和最小的失真。我们遵循模型驱动的方法，通过将神经网络模型解释为滤波器组，将神经网络集成到滤波器组设计中。所提出的基于神经网络的滤波器组可以利用先进的学习能力来实现无失真复用，并利用硬件加速实现高效率。然后，我们评估了由基于神经网络的滤波器组实现的频谱复用器在各种信号类型和环境条件下的性能。结果表明，它可以实现低至-39dB归一化均方误差的低失真水平。此外，与传统方法相比，它实现了高达35倍的执行效率增益和10dB的信噪比增益。现场应用表明，它可以处理异构和同构物联网网络，在标准接收器上实现高达98%的高分组接收率。", "summary": "该论文提出了一种基于神经网络滤波器组的新型频谱复用器，旨在提高物联网信号传输的频谱利用率。通过将神经网络集成到滤波器组设计中，该方法实现了高效、无失真的多信号复用。实验结果表明，与传统方法相比，该复用器在失真、执行效率和信噪比方面均有显著提升，并在实际物联网应用中实现了高分组接收率。", "keywords": "频谱复用器, 神经网络, 滤波器组, 物联网, 无失真", "comments": "该论文的创新点在于将神经网络模型创造性地应用于滤波器组设计，从而解决了传统频谱复用器在效率和失真方面的局限性。通过利用神经网络的学习能力和硬件加速，该方案在性能上取得了显著突破，特别是在物联网领域展现了巨大的应用潜力。"}}
{"id": "2507.00358", "title": "Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems", "authors": ["Yilie Huang", "Xun Yu Zhou"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      37 pages, 10 figures", "url": "http://arxiv.org/abs/2507.00358v2", "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "comment": "37 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00358v2", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-23", "AI": {"title_translation": "针对一类连续时间不定线性二次强化学习问题的数据驱动探索", "tldr": "本文提出了一种自适应、数据驱动的探索机制，用于连续时间线性二次强化学习问题，该机制实现了与最佳固定探索方法匹配的次线性遗憾界，并显著提高了学习效率和收敛速度。", "motivation": "现有连续时间随机线性二次强化学习问题中的探索策略（如\n\\cite{huang2024sublinear}中的恒定或确定性探索计划）需要大量调优且忽略学习进度，导致学习效率低下。本文旨在提出一种更高效、更少调优的探索机制，以克服这些局限性。", "method": "本文提出了一种无模型、数据驱动的探索机制。该机制通过评论家自适应地调整熵正则化，并通过行动者自适应地调整策略方差。", "result": "该方法实现了与该类LQ问题中已知最佳无模型结果相匹配的次线性遗憾界，而这些结果此前仅通过固定探索计划获得。数值实验表明，与非自适应的无模型和基于模型的对应方法相比，自适应探索加速了收敛并改善了遗憾性能。", "conclusion": "本文提出的自适应探索机制在连续时间线性二次强化学习问题中表现出卓越的性能，不仅在理论上达到了最先进的遗憾界限，而且在实践中显著提高了学习效率和收敛速度，证明了其在解决现有探索策略局限性方面的有效性。", "translation": "我们研究了与\\cite{huang2024sublinear}中相同的连续时间随机线性二次（LQ）控制问题类别的强化学习（RL），其中波动性取决于状态和控制，而状态是标量值且缺少运行控制奖励。我们提出了一种无模型、数据驱动的探索机制，该机制通过评论家自适应地调整熵正则化，并通过行动者自适应地调整策略方差。与\\cite{huang2024sublinear}中采用的恒定或确定性探索计划不同，这些计划需要大量的实现调优并忽略迭代过程中的学习进展，我们的自适应探索方法以最小的调优提高了学习效率。尽管具有灵活性，我们的方法实现了次线性遗憾界，这与该类LQ问题中已知最佳的无模型结果相匹配，而这些结果此前仅通过固定探索计划获得。数值实验表明，与非自适应的无模型和基于模型的对应方法相比，自适应探索加速了收敛并改善了遗憾性能。", "summary": "本文针对一类连续时间随机线性二次（LQ）强化学习问题，提出了一种创新的无模型、数据驱动的自适应探索机制。该机制通过动态调整熵正则化和策略方差来优化探索过程，解决了现有固定探索策略需要大量调优且效率低下的问题。研究表明，该方法不仅在理论上达到了与最佳固定探索方法相同的次线性遗憾界，而且在数值实验中显著加速了收敛并改善了遗憾性能。", "keywords": "强化学习, 线性二次控制, 连续时间, 数据驱动, 自适应探索", "comments": "本文的创新点在于提出了一个自适应的数据驱动探索机制，解决了传统固定探索策略在连续时间LQ强化学习中需要大量手动调优且效率不高的问题。通过动态调整探索参数，该方法在保持理论性能（匹配最佳次线性遗憾界）的同时，显著提高了实际学习效率和收敛速度，对于实际应用具有重要意义。"}}
{"id": "2502.12988", "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs", "authors": ["Zixiao Wang", "Duzhen Zhang", "Ishita Agrawal", "Shen Gao", "Le Song", "Xiuying Chen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      19 pages, 3 figures, ACL 2025 Findings", "url": "http://arxiv.org/abs/2502.12988v2", "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.", "comment": "19 pages, 3 figures, ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2502.12988v2", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-07-23", "AI": {"title_translation": "超越表象：从表面事实到大语言模型中深度人格模拟", "tldr": "以往的大语言模型人格模拟流于表面。本文介绍了CharacterBot，一个旨在模拟角色深层思维和语言模式的模型，以鲁迅为例进行研究，并在语言准确性和观点理解方面显著优于基线。", "motivation": "以往人格模拟大语言模型的方法通常依赖于学习基本的传记信息或使用有限的角色扮演对话数据集，无法捕捉个体包括深层思想和思维在内的整体表征。", "method": "本文引入了CharacterBot模型，旨在复制角色的语言模式和独特的思维过程。以中国作家鲁迅为例，从其17部散文集中提取了四个训练任务：一个专注于掌握外部语言结构和知识的预训练任务，以及三个微调任务（多项选择问答、生成式问答和风格迁移），每个任务都使大语言模型与鲁迅的内在思想和写作风格保持一致。为优化跨任务学习，引入了CharLoRA参数更新机制，其中一个通用语言风格专家与其他特定任务专家协作，以更好地研究语言风格和对深层思想的理解。", "result": "CharacterBot在语言准确性和观点理解的三个任务上，通过自适应指标评估，显著优于基线模型。", "conclusion": "这项工作有望启发未来关于深度角色人格模拟大语言模型的研究。", "translation": "以前的人格模拟大语言模型（LLMs）方法通常依赖于学习基本的传记信息，或使用有限的角色扮演对话数据集来捕捉角色的回应。然而，一个人的整体表征不仅仅是表面事实或对话，还包括更深层次的思想和思维。在这项工作中，我们引入了CharacterBot，一个旨在复制角色语言模式和独特思维过程的模型。我们以中国著名作家鲁迅为例，提出了从他17部散文集中提取的四个训练任务。其中包括一个专注于掌握外部语言结构和知识的预训练任务，以及三个微调任务：多项选择问答、生成式问答和风格迁移，每个任务都使LLM与鲁迅的内在思想和写作风格保持一致。为了优化这些任务的学习，我们引入了一种CharLoRA参数更新机制，其中一个通用语言风格专家与其他特定任务专家协作，以更好地研究语言风格和对深层思想的理解。我们在三个任务上评估了CharacterBot的语言准确性和观点理解能力，结果表明它在我们的自适应指标上显著优于基线。我们希望这项工作能启发未来关于深度角色人格模拟LLM的研究。", "summary": "本文提出了CharacterBot，一个用于深度人格模拟的大语言模型，旨在超越表面事实，捕捉角色的语言模式和思维过程。该模型以鲁迅的散文为案例，采用一个预训练任务学习语言结构，并结合三个微调任务（问答、生成式问答和风格迁移）以模拟其内在思想。通过CharLoRA机制优化学习，结合通用和任务特定专家。评估结果表明CharacterBot在语言准确性和观点理解方面显著优于现有基线，为更深层次的角色模拟铺平了道路。", "keywords": "人格模拟, 大语言模型, CharacterBot, 深度思维, 鲁迅, CharLoRA", "comments": "创新点在于CharacterBot模型通过模拟深层思维过程而非仅限于表面事实，以及引入CharLoRA机制整合语言和思想学习。以鲁迅这样复杂的知名作家作为案例研究，增强了研究的深度和挑战性。这项工作解决了当前人格模拟大语言模型的关键局限性，推动了更真实、更全面的人物表征，对虚拟助手、创意写作和历史模拟等应用具有重要意义。"}}
{"id": "2507.16988", "title": "RAPTAR: Radar Radiation Pattern Acquisition through Automated Collaborative Robotics", "authors": ["Maaz Qureshi", "Mohammad Omid Bagheri", "Abdelrahman Elbadrawy", "William Melek", "George Shaker"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 Pages, IEEE Journal", "url": "http://arxiv.org/abs/2507.16988v1", "summary": "Accurate characterization of modern on-chip antennas remains challenging, as\ncurrent probe-station techniques offer limited angular coverage, rely on\nbespoke hardware, and require frequent manual alignment. This research\nintroduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a\nportable, state-of-the-art, and autonomous system based on collaborative\nrobotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar\nmodules without dedicated anechoic facilities. The system is designed to\naddress the challenges of testing radar modules mounted in diverse real-world\nconfigurations, including vehicles, UAVs, AR/VR headsets, and biomedical\ndevices, where traditional measurement setups are impractical. A\n7-degree-of-freedom Franka cobot holds the receiver probe and performs\ncollision-free manipulation across a hemispherical spatial domain, guided by\nreal-time motion planning and calibration accuracy with RMS error below 0.9 mm.\nThe system achieves an angular resolution upto 2.5 degree and integrates\nseamlessly with RF instrumentation for near- and far-field power measurements.\nExperimental scans of a 60 GHz radar module show a mean absolute error of less\nthan 2 dB compared to full-wave electromagnetic simulations ground truth.\nBenchmarking against baseline method demonstrates 36.5% lower mean absolute\nerror, highlighting RAPTAR accuracy and repeatability.", "comment": "8 Pages, IEEE Journal", "pdf_url": "http://arxiv.org/pdf/2507.16988v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "RAPTAR：通过自动化协作机器人获取雷达辐射图", "tldr": "RAPTAR是一种便携式、自动化的协作机器人系统，用于在非消声室环境中精确测量集成雷达模块的3D辐射图，解决了传统方法的局限性。", "motivation": "现代片上天线的精确特性表征仍然具有挑战性，因为现有的探针台技术角度覆盖范围有限、依赖定制硬件且需要频繁手动对准。此外，对于安装在车辆、无人机、AR/VR头戴设备和生物医学设备等多样化真实世界配置中的雷达模块，传统测量设置不切实际。", "method": "本研究引入了RAPTAR（通过机器人自动化获取辐射图），一个基于协作机器人的便携式、先进且自主的系统。它使用一个7自由度Franka协作机器人，通过实时运动规划和小于0.9毫米RMS误差的校准精度，在半球形空间域内进行无碰撞操作，并与射频仪器无缝集成，进行近场和远场功率测量。", "result": "RAPTAR系统实现了高达2.5度的角度分辨率。对一个60 GHz雷达模块的实验扫描显示，与全波电磁仿真真值相比，平均绝对误差小于2 dB。与基线方法相比，平均绝对误差降低了36.5%，突出了RAPTAR的准确性和可重复性。", "conclusion": "RAPTAR系统提供了一种准确、可重复且便携的解决方案，用于在各种实际场景中自动测量集成雷达模块的3D辐射图，克服了传统方法的局限性。", "translation": "现代片上天线的精确特性表征仍然具有挑战性，因为当前的探针台技术角度覆盖范围有限，依赖定制硬件，并需要频繁手动对准。本研究引入了RAPTAR（通过机器人自动化获取辐射图），一个基于协作机器人的便携式、先进且自主的系统。RAPTAR无需专用消声设施即可实现集成雷达模块的3D辐射图测量。该系统旨在解决在车辆、无人机、AR/VR头戴设备和生物医学设备等多样化真实世界配置中安装的雷达模块的测试挑战，在这些场景中传统测量设置不切实际。一个7自由度Franka协作机器人持有接收探头，通过实时运动规划和低于0.9毫米RMS误差的校准精度，在半球形空间域内执行无碰撞操作。该系统实现了高达2.5度的角度分辨率，并与射频仪器无缝集成，用于近场和远场功率测量。对一个60 GHz雷达模块的实验扫描显示，与全波电磁仿真真值相比，平均绝对误差小于2 dB。与基线方法相比，平均绝对误差降低了36.5%，突出了RAPTAR的准确性和可重复性。", "summary": "RAPTAR是一种创新的便携式自动化系统，利用协作机器人技术来解决现代片上天线和集成雷达模块3D辐射图测量的挑战。该系统无需传统消声室，并能适应车辆、无人机等实际应用场景。通过7自由度机器人和实时运动规划，RAPTAR实现了高精度和高分辨率的测量，实验结果表明其相较于传统方法在准确性和可重复性方面有显著提升。", "keywords": "雷达辐射图, 协作机器人, 天线测量, 自动化, 便携式系统", "comments": "该论文提出了一种创新的、基于协作机器人的自动化系统RAPTAR，用于雷达辐射图测量。其主要创新点在于将机器人技术应用于传统上需要大型、昂贵且手动操作的测试环境，实现了便携性、自动化和对真实世界复杂配置的适应性。这对于集成雷达模块的快速、准确测试具有重要意义，尤其是在物联网、自动驾驶和医疗设备等领域。"}}
{"id": "2507.14811", "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Yuying Liu", "Xinkui Zhao", "Kingsum Chow", "Gang Xiong", "Shuiguang Deng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14811v2", "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14811v2", "cate": "cs.CV", "date": "2025-07-20", "updated": "2025-07-23", "AI": {"title_translation": "SegQuant：一种面向扩散模型的语义感知和通用量化框架", "tldr": "SegQuant是一个统一的量化框架，通过结合语义感知和双尺度量化策略，解决了现有扩散模型量化方法通用性差的问题，实现了高性能和广泛适用性。", "motivation": "扩散模型计算密集，难以在资源受限或延迟敏感环境中部署。现有针对扩散模型的后训练量化（PTQ）方法通常依赖于特定架构的启发式方法，这限制了它们的通用性并阻碍了与工业部署流程的集成。", "method": "我们提出了SegQuant，一个统一的量化框架，它自适应地结合了互补技术以增强跨模型通用性。SegQuant包含一个分段感知、基于图的量化策略（SegLinear），它捕获结构语义和空间异质性；以及一个双尺度量化方案（DualScale），它保留了对维持生成输出视觉保真度至关重要的极性不对称激活。", "result": "SegQuant广泛适用于基于Transformer的扩散模型之外，实现了强大的性能，同时确保了与主流部署工具的无缝兼容性。", "conclusion": "SegQuant通过其统一的框架和自适应的量化策略，成功解决了扩散模型量化中通用性差的问题，为资源受限环境下的模型部署提供了高效且兼容的解决方案。", "translation": "扩散模型展现出卓越的生成能力，但计算密集，对在资源受限或延迟敏感环境中部署带来了重大挑战。量化提供了一种有效的方法来减少模型大小和计算成本，其中后训练量化（PTQ）因其与预训练模型的兼容性而无需重新训练或训练数据而特别吸引人。然而，现有针对扩散模型的PTQ方法通常依赖于特定架构的启发式方法，这限制了它们的通用性并阻碍了与工业部署流程的集成。为了解决这些限制，我们提出了SegQuant，一个统一的量化框架，它自适应地结合了互补技术以增强跨模型通用性。SegQuant包含一个分段感知、基于图的量化策略（SegLinear），它捕获结构语义和空间异质性，以及一个双尺度量化方案（DualScale），它保留了对维持生成输出视觉保真度至关重要的极性不对称激活。SegQuant广泛适用于基于Transformer的扩散模型之外，实现了强大的性能，同时确保了与主流部署工具的无缝兼容性。", "summary": "SegQuant是一个针对扩散模型提出的统一量化框架，旨在解决现有量化方法通用性差和部署困难的问题。它通过结合分段感知、基于图的量化策略（SegLinear）和保留极性不对称激活的双尺度量化方案（DualScale），实现了对模型结构语义和空间异质性的有效捕获。SegQuant不仅适用于Transformer之外的扩散模型，还能在保持高性能的同时，确保与主流部署工具的无缝兼容性，从而降低了扩散模型在资源受限环境中的部署成本。", "keywords": "扩散模型, 量化, 后训练量化, SegQuant, 通用性", "comments": "SegQuant的创新之处在于其“语义感知”和“通用化”的特性，通过SegLinear和DualScale两个核心组件，克服了现有量化方法对特定架构的依赖，显著提升了跨模型适用性。这对于扩散模型在实际工业部署中的推广具有重要意义，因为它解决了兼容性和部署效率的关键瓶颈。"}}
{"id": "2507.17096", "title": "ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search", "authors": ["Olivia Dry", "Timothy L. Molloy", "Wanxin Jin", "Iman Shames"], "categories": ["cs.LG", "cs.NA", "cs.SY", "eess.SY", "math.NA", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17096v1", "summary": "We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations\n(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of\nconstrained optimal control problems, in both continuous and discrete time, to\nbe learned from expert demonstrations without requiring smoothness of the\nlearning-loss landscape. In contrast, existing state-of-the-art first-order\nmethods require the existence and computation of gradients of the costs,\nconstraints, dynamics, and learning loss with respect to states, controls\nand/or parameters. Most existing methods are also tailored to discrete time,\nwith constrained problems in continuous time receiving only cursory attention.\nWe demonstrate that ZORMS-LfD matches or surpasses the performance of\nstate-of-the-art methods in terms of both learning loss and compute time across\na variety of benchmark problems. On unconstrained continuous-time benchmark\nproblems, ZORMS-LfD achieves similar loss performance to state-of-the-art\nfirst-order methods with an over $80$\\% reduction in compute time. On\nconstrained continuous-time benchmark problems where there is no specialized\nstate-of-the-art method, ZORMS-LfD is shown to outperform the commonly used\ngradient-free Nelder-Mead optimization method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17096v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "ZORMS-LfD：基于零阶随机矩阵搜索的示教学习", "tldr": "ZORMS-LfD是一种新的示教学习方法，它通过零阶随机矩阵搜索，无需梯度即可学习受约束最优控制问题，并在计算时间和学习损失方面超越现有方法，尤其在连续时间约束问题上表现突出。", "motivation": "现有最先进的一阶方法需要计算成本、约束、动力学和学习损失的梯度，并且大多针对离散时间问题，对连续时间约束问题关注不足。", "method": "本文提出了ZORMS-LfD（零阶随机矩阵搜索示教学习）方法。该方法通过零阶随机矩阵搜索，无需学习损失函数的光滑性，即可从专家演示中学习连续和离散时间受约束最优控制问题的成本、约束和动力学。", "result": "ZORMS-LfD在各种基准问题上，在学习损失和计算时间方面与最先进的方法持平或超越。在无约束连续时间基准问题上，ZORMS-LfD实现了与最先进的一阶方法相似的损失性能，但计算时间减少了80%以上。在没有专门最先进方法的约束连续时间基准问题上，ZORMS-LfD优于常用的无梯度Nelder-Mead优化方法。", "conclusion": "ZORMS-LfD是一种高效且无需梯度的示教学习方法，能够有效处理连续和离散时间的受约束最优控制问题，并在性能上超越或匹配现有最先进的方法，尤其在连续时间问题上具有显著优势。", "translation": "我们提出了用于示教学习的零阶随机矩阵搜索（ZORMS-LfD）。ZORMS-LfD使得在连续和离散时间下的受约束最优控制问题的成本、约束和动力学都可以从专家演示中学习，而无需学习损失函数景观的光滑性。相比之下，现有最先进的一阶方法需要成本、约束、动力学以及学习损失相对于状态、控制和/或参数的梯度的存在和计算。大多数现有方法也都是针对离散时间定制的，对连续时间受约束问题的关注仅是粗略的。我们证明了ZORMS-LfD在各种基准问题上，无论是学习损失还是计算时间方面，都与最先进的方法持平或超越。在无约束连续时间基准问题上，ZORMS-LfD实现了与最先进的一阶方法相似的损失性能，计算时间减少了80%以上。在没有专门最先进方法的约束连续时间基准问题上，ZORMS-LfD被证明优于常用的无梯度Nelder-Mead优化方法。", "summary": "本文提出了一种名为ZORMS-LfD的示教学习方法，它利用零阶随机矩阵搜索来解决连续和离散时间下的受约束最优控制问题。与需要梯度的现有方法不同，ZORMS-LfD无需损失函数的光滑性，即可从专家演示中学习成本、约束和动力学。实验结果表明，ZORMS-LfD在学习损失和计算时间方面均能匹配或超越现有最先进方法，尤其在无约束连续时间问题上可大幅减少计算时间，并在约束连续时间问题上优于传统无梯度方法。", "keywords": "零阶优化, 示教学习, 随机矩阵搜索, 受约束控制, 连续时间", "comments": "ZORMS-LfD的创新之处在于其采用零阶随机矩阵搜索，避免了对梯度计算的依赖，这在学习损失函数非光滑或梯度难以获取的复杂控制问题中具有重要意义。它同时解决了连续和离散时间下的约束问题，填补了现有方法在连续时间约束问题上的空白。其在计算效率上的显著提升，尤其是对计算资源要求较高的实时控制系统而言，具有重要的应用价值。"}}
{"id": "2407.18661", "title": "Optimizing Design and Control Methods for Using Collaborative Robots in Upper-Limb Rehabilitation", "authors": ["Dario Onfiani", "Marco Caramaschi", "Luigi Biagiotti", "Fabio Pini"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.18661v2", "summary": "In this paper, we address the development of a robotic rehabilitation system\nfor the upper limbs based on collaborative end-effector solutions. The use of\ncommercial collaborative robots offers significant advantages for this task, as\nthey are optimized from an engineering perspective and ensure safe physical\ninteraction with humans. However, they also come with noticeable drawbacks,\nsuch as the limited range of sizes available on the market and the standard\ncontrol modes, which are primarily oriented towards industrial or service\napplications. To address these limitations, we propose an optimization-based\ndesign method to fully exploit the capability of the cobot in performing\nrehabilitation tasks. Additionally, we introduce a novel control architecture\nbased on an admittance-type Virtual Fixture method, which constrains the motion\nof the robot along a prescribed path. This approach allows for an intuitive\ndefinition of the task to be performed via Programming by Demonstration and\nenables the system to operate both passively and actively. In passive mode, the\nsystem supports the patient during task execution with additional force, while\nin active mode, it opposes the motion with a braking force. Experimental\nresults demonstrate the effectiveness of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.18661v2", "cate": "cs.RO", "date": "2024-07-26", "updated": "2025-07-23", "AI": {"title_translation": "上肢康复中协作机器人设计与控制方法优化", "tldr": "本文提出了一种优化的设计和基于导纳型虚拟夹具的控制方法，以克服商用协作机器人在上肢康复应用中的局限性，并实验证明了其有效性。", "motivation": "商用协作机器人在工程优化和人机安全交互方面具有优势，但其尺寸范围有限且标准控制模式主要面向工业或服务应用，不完全适用于上肢康复任务。", "method": "提出了一种基于优化的设计方法以充分利用协作机器人在康复任务中的能力；引入了一种基于导纳型虚拟夹具方法的新型控制架构，该架构可将机器人运动限制在预定路径上，支持通过示教编程直观定义任务，并能以被动和主动模式运行。", "result": "实验结果表明所提出的方法是有效的。", "conclusion": "本文提出的优化设计和基于导纳型虚拟夹具的控制方法能够有效克服商用协作机器人在上肢康复应用中的局限性，并充分发挥其康复潜力。", "translation": "在本文中，我们致力于开发一种基于协作末端执行器解决方案的上肢机器人康复系统。使用商用协作机器人对此任务具有显著优势，因为它们从工程角度进行了优化，并确保与人类安全地进行物理交互。然而，它们也存在明显的缺点，例如市场上可用的尺寸范围有限以及主要面向工业或服务应用的标准控制模式。为了解决这些局限性，我们提出了一种基于优化的设计方法，以充分利用协作机器人在执行康复任务中的能力。此外，我们引入了一种基于导纳型虚拟夹具方法的新型控制架构，该架构将机器人运动约束在预设路径上。这种方法允许通过示教编程直观地定义要执行的任务，并使系统能够以被动和主动两种模式运行。在被动模式下，系统在任务执行期间通过附加力支持患者，而在主动模式下，它通过制动力抵抗运动。实验结果证明了所提出方法的有效性。", "summary": "本研究旨在开发一种用于上肢康复的协作机器人系统，以克服商用协作机器人现有尺寸和控制模式的局限性。为此，论文提出了一种基于优化的设计方法，以充分发挥协作机器人在康复任务中的潜力。同时，引入了一种基于导纳型虚拟夹具的新型控制架构，该架构允许通过示教编程直观地定义任务，并支持被动（提供辅助力）和主动（提供阻力）两种操作模式。实验结果验证了所提出方法的有效性。", "keywords": "协作机器人, 上肢康复, 优化设计, 虚拟夹具, 控制方法", "comments": "该论文创新性地结合了优化设计与新型控制架构，解决了商用协作机器人在上肢康复应用中的实际局限性。其提出的导纳型虚拟夹具方法，通过示教编程实现直观任务定义，并支持被动与主动模式，极大地提升了康复系统的灵活性和用户友好性。这项研究对于推动协作机器人在医疗康复领域的应用具有重要意义。"}}
{"id": "2504.19126", "title": "Coherent Source Enumeration with Compact ULAs", "authors": ["Dibakar Sil", "Sunder Ram Krishnan", "Kumar Vijay Mishra"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      5 pages, 1 figure", "url": "http://arxiv.org/abs/2504.19126v2", "summary": "Source enumeration typically relies on subspace-based techniques that require\naccurate separation of signal and noise subspaces. However, prior works do not\naddress coherent sources in small uniform linear arrays, where ambiguities\narise in the spatial spectrum. We address this by decomposing the\nforward-backward smoothed covariance matrix into a sum of a rank-constrained\nToeplitz matrix and a diagonal matrix with non-negative entries representing\nthe signal and noise subspaces, respectively. The resulting non-convex\noptimization problem is solved by proposing Toeplitz approach for rank-based\ntarget estimation (TARgEt) that employs the alternating direction method of\nmultipliers. Numerical results on both synthetic and real-world datasets\ndemonstrate the effectiveness and robustness of TARgEt over the\nstate-of-the-art.", "comment": "5 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2504.19126v2", "cate": "eess.SP", "date": "2025-04-27", "updated": "2025-07-23", "AI": {"title_translation": "紧凑型均匀线性阵列的相干源枚举", "tldr": "针对小型均匀线性阵列中相干源枚举的挑战，本文提出了一种基于协方差矩阵分解和优化算法的新方法TARgEt，并表现出优越性能。", "motivation": "传统子空间源枚举方法在小型均匀线性阵列处理相干源时存在局限性，导致空间频谱模糊，未能有效分离信号和噪声子空间。", "method": "通过将前向-后向平滑协方差矩阵分解为一个秩约束的Toeplitz矩阵（代表信号子空间）和一个对角矩阵（代表噪声子空间）之和。通过提出基于秩的目标估计的Toeplitz方法（TARgEt），并采用交替方向乘子法来解决由此产生的非凸优化问题。", "result": "在合成和真实世界数据集上的数值结果表明，TARgEt比现有最先进的方法更有效和鲁棒。", "conclusion": "TARgEt方法能够有效且鲁棒地解决小型均匀线性阵列中相干源枚举的挑战，并优于现有技术。", "translation": "信源枚举通常依赖于子空间技术，这些技术需要精确分离信号和噪声子空间。然而，现有工作并未解决小型均匀线性阵列中的相干源问题，其中空间频谱会产生模糊。我们通过将前向-后向平滑协方差矩阵分解为一个秩约束的Toeplitz矩阵（代表信号子空间）和一个具有非负条目的对角矩阵（分别代表噪声子空间）之和来解决此问题。由此产生的非凸优化问题通过提出基于秩的目标估计的Toeplitz方法（TARgEt）来解决，该方法采用了交替方向乘子法。在合成和真实世界数据集上的数值结果表明，TARgEt比现有最先进的方法更有效和更鲁棒。", "summary": "本论文旨在解决小型均匀线性阵列中相干源枚举时传统子空间方法遇到的空间频谱模糊问题。为此，提出了一种名为TARgEt的新方法，该方法通过将前向-后向平滑协方差矩阵分解为秩约束Toeplitz矩阵和对角矩阵之和来表征信号和噪声子空间。通过交替方向乘子法解决由此产生的非凸优化问题。实验结果表明，TARgEt在合成和真实数据集上均优于现有技术，表现出更高的有效性和鲁棒性。", "keywords": "信源枚举, 相干源, 均匀线性阵列, Toeplitz矩阵, 协方差矩阵分解", "comments": "该论文的创新之处在于提出了一种新颖的协方差矩阵分解方法，并结合优化算法（TARgEt）来解决小型均匀线性阵列中相干源枚举的挑战，尤其克服了传统子空间方法在处理相干源时的局限性。通过将非凸问题转化为可求解的形式，并利用ADMM算法，提高了方法的鲁棒性和有效性，填补了该领域的空白。"}}
{"id": "2507.17569", "title": "Error estimates and adaptivity for a least-squares method applied to the Monge-Ampère equation", "authors": ["Alexandre Caboussat", "Anna Peruso", "Marco Picasso"], "categories": ["math.NA", "cs.NA", "65M60 (Primary) 65M50 (Secondary)", "G.1.8; G.1.6"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      29 pages, 46 figures. Submitted to IMA Journal of Numerical Analysis", "url": "http://arxiv.org/abs/2507.17569v1", "summary": "We introduce novel a posteriori error indicators for a nonlinear\nleast-squares solver for smooth solutions of the Monge--Amp\\`ere equation on\nconvex polygonal domains in $\\mathbb{R}^2$. At each iteration, our iterative\nscheme decouples the problem into (i) a pointwise nonlinear minimization\nproblem and (ii) a linear biharmonic variational problem. For the latter, we\nderive an equivalence to a biharmonic problem with Navier boundary conditions\nand solve it via mixed piecewise-linear finite elements. Reformulating this as\na coupled second-order system, we derive a priori and a posteriori\n$\\mathbb{P}^1$ finite element error estimators and we design a robust adaptive\nmesh refinement strategy. Numerical tests confirm that errors in different\nnorms scale appropriately. Finally, we demonstrate the effectiveness of our a\nposteriori indicators in guiding mesh refinement.", "comment": "29 pages, 46 figures. Submitted to IMA Journal of Numerical Analysis", "pdf_url": "http://arxiv.org/pdf/2507.17569v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Monge-Ampère方程最小二乘法的误差估计与自适应性", "tldr": "本文为Monge-Ampère方程的最小二乘求解器引入了新的误差指示器和自适应网格细化策略，并通过数值测试证实了其有效性。", "motivation": "为Monge-Ampère方程的非线性最小二乘求解器开发新颖的后验误差指示器和鲁棒的自适应网格细化策略。", "method": "本文引入了一种用于Monge-Ampère方程的非线性最小二乘求解器。该迭代方案将问题解耦为点式非线性最小化问题和线性双调和变分问题。后者被证明等价于具有Navier边界条件的双调和问题，并通过混合分段线性有限元求解。通过将其重新表述为耦合的二阶系统，推导了先验和后验$\\mathbb{P}^1$有限元误差估计器，并设计了鲁棒的自适应网格细化策略。", "result": "数值测试证实了不同范数下的误差适当缩放。后验指示器在指导网格细化方面是有效的。", "conclusion": "所引入的后验误差指示器和自适应网格细化策略对于Monge-Ampère方程的最小二乘求解器是有效的，并得到了数值测试的证实。", "translation": "我们为在$\\mathbb{R}^2$中的凸多边形域上光滑解的Monge-Ampère方程的非线性最小二乘求解器引入了新颖的后验误差指示器。在每次迭代中，我们的迭代方案将问题分解为 (i) 点式非线性最小化问题和 (ii) 线性双调和变分问题。对于后者，我们推导了与具有Navier边界条件的双调和问题的等价性，并通过混合分段线性有限元解决它。将其重新表述为耦合的二阶系统，我们推导了先验和后验$\\mathbb{P}^1$有限元误差估计器，并设计了一种鲁棒的自适应网格细化策略。数值测试证实了不同范数下的误差适当缩放。最后，我们证明了我们的后验指示器在指导网格细化方面的有效性。", "summary": "本文提出了一种针对在凸多边形域上Monge-Ampère方程非线性最小二乘求解器的新颖后验误差指示器和鲁棒的自适应网格细化策略。迭代求解器将问题解耦为点式非线性最小化和线性双调和变分问题，并使用混合有限元进行求解。作者推导了误差估计器，并通过数值测试证实了其在指导网格细化方面的有效性。", "keywords": "Monge-Ampère方程, 最小二乘法, 误差估计, 自适应网格细化, 双调和问题", "comments": "该论文在为具有挑战性的Monge-Ampère方程提供误差估计和自适应网格细化新方法方面具有创新性，特别是通过其解耦策略和最小二乘方法的使用。推导出的误差估计器和自适应策略可以显著提高此类非线性偏微分方程数值解的效率和准确性。"}}
{"id": "2504.20129", "title": "A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States", "authors": ["Arun M. Saranathan", "Mahmoud Saeedimoghaddam", "Brandon Smith", "Deepthi Raghunandan", "Grey Nearing", "Craig Pelissier"], "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "Subjects:       Atmospheric and Oceanic Physics (physics.ao-ph)", "pdf_link": null, "comments": "Comments:      Preprint of journal paper in preparation. Details: 24 pages, 8 figures", "url": "http://arxiv.org/abs/2504.20129v2", "summary": "Snow is an essential input for various land surface models. Seasonal snow\nestimates are available as snow water equivalent (SWE) from process-based\nreanalysis products or locally from in situ measurements. While the reanalysis\nproducts are computationally expensive and available at only fixed spatial and\ntemporal resolutions, the in situ measurements are highly localized and sparse.\nTo address these issues and enable the analysis of the effect of a large suite\nof physical, morphological, and geological conditions on the presence and\namount of snow, we build a Long Short-Term Memory (LSTM) network, which is able\nto estimate the SWE based on time series input of the various\nphysical/meteorological factors as well static spatial/morphological factors.\nSpecifically, this model breaks down the SWE estimation into two separate\ntasks: (i) a classification task that indicates the presence/absence of snow on\na specific day and (ii) a regression task that indicates the height of the SWE\non a specific day in the case of snow presence. The model is trained using\nphysical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows\nin the western United States. We will show that trained LSTM models have a\nclassification accuracy of $\\geq 93\\%$ for the presence of snow and a\ncoefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We\nwill also demonstrate that the models can generalize both spatially and\ntemporally to previously unseen data.", "comment": "Preprint of journal paper in preparation. Details: 24 pages, 8\n  figures", "pdf_url": "http://arxiv.org/pdf/2504.20129v2", "cate": "physics.ao-ph", "date": "2025-04-28", "updated": "2025-07-23", "AI": {"title_translation": "美国大陆雪水当量估算的物理驱动长短期记忆模型", "tldr": "该研究开发了一个基于物理驱动的长短期记忆（LSTM）模型，用于准确估计美国大陆的雪水当量（SWE），解决了现有方法计算成本高或数据稀疏的问题，并实现了高分类精度和回归相关性。", "motivation": "现有的雪水当量（SWE）估算方法存在局限性：过程驱动的再分析产品计算成本高且空间和时间分辨率固定；原位测量数据则高度局部化且稀疏。为解决这些问题并分析多种物理、形态和地质条件对雪存在和数量的影响，本研究旨在开发一种新的估算模型。", "method": "本研究构建了一个长短期记忆（LSTM）网络，该网络能够根据各种物理/气象因素的时间序列输入以及静态空间/形态因素来估算雪水当量（SWE）。该模型将SWE估算分解为两个独立任务：(i) 判断特定日期雪的出现/缺失的分类任务；(ii) 在雪出现的情况下，估算特定日期SWE高度的回归任务。模型使用来自美国西部SNOTEL雪枕的物理/原位SWE测量数据进行训练。", "result": "训练后的LSTM模型在雪存在分类任务上具有≥93%的准确率，在雪水当量（SWE）估算方面具有约0.9的相关系数。模型还展示了对先前未见过数据的空间和时间泛化能力。", "conclusion": "本研究开发的物理驱动长短期记忆（LSTM）模型能够有效解决传统雪水当量（SWE）估算方法的局限性，提供高精度且可泛化的SWE估算，为大陆尺度雪情分析提供了有力的工具。", "translation": "雪是各种陆地表面模型的重要输入。季节性雪情估算可以从基于过程的再分析产品中获取雪水当量（SWE），或者从本地的原位测量中获取。再分析产品计算成本高昂，且只能在固定的空间和时间分辨率下提供，而原位测量则高度局部化且稀疏。为了解决这些问题并分析大量物理、形态和地质条件对雪的存在和数量的影响，我们构建了一个长短期记忆（LSTM）网络，它能够根据各种物理/气象因素的时间序列输入以及静态空间/形态因素来估算SWE。具体来说，该模型将SWE估算分解为两个独立的任务：(i) 一个分类任务，指示特定日期雪的出现/缺失；(ii) 一个回归任务，指示在有雪的情况下特定日期SWE的高度。该模型使用来自美国西部SNOTEL雪枕的物理/原位SWE测量数据进行训练。我们将展示，经过训练的LSTM模型在雪存在分类方面具有≥93%的准确率，并且其SWE估算的相关系数约为0.9。我们还将证明，这些模型可以在空间和时间上泛化到以前未见过的数据。", "summary": "针对现有雪水当量（SWE）估算方法计算成本高昂、数据稀疏等问题，本研究提出了一种物理驱动的长短期记忆（LSTM）网络模型。该模型结合了物理/气象时间序列输入和静态空间/形态因素，将SWE估算分为雪存在/缺失的分类任务和SWE高度的回归任务。模型在美国西部SNOTEL雪枕数据上进行训练，在雪存在分类上达到了93%以上的准确率，SWE估算相关系数约为0.9，并展现出良好的空间和时间泛化能力，为大范围SWE估算提供了高效准确的解决方案。", "keywords": "雪水当量, 长短期记忆网络, 机器学习, 雪情估算, 物理驱动模型", "comments": "该论文的创新之处在于利用物理驱动的LSTM模型来克服传统雪水当量（SWE）估算方法的局限性，特别是将SWE估算分解为分类和回归两个子任务，这种方法学上的创新提高了模型的实用性和精度。其重要性体现在为土地地表模型提供了更准确、更高效的季节性雪情输入，有助于深入分析各种物理条件对雪情的影响。模型展示的良好泛化能力也预示其在大尺度应用中的巨大潜力。"}}
{"id": "2507.17214", "title": "Our Cars Can Talk: How IoT Brings AI to Vehicles", "authors": ["Amod Kant Agrawal"], "categories": ["cs.AI", "cs.CY", "cs.NI", "cs.SY", "eess.SY", "I.2; B.8; C.2; I.5; J.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      3 pages, 1 figure; To appear in IEEE Computer (Nov 2025)", "url": "http://arxiv.org/abs/2507.17214v1", "summary": "Bringing AI to vehicles and enabling them as sensing platforms is key to\ntransforming maintenance from reactive to proactive. Now is the time to\nintegrate AI copilots that speak both languages: machine and driver. This\narticle offers a conceptual and technical perspective intended to spark\ninterdisciplinary dialogue and guide future research and development in\nintelligent vehicle systems, predictive maintenance, and AI-powered user\ninteraction.", "comment": "3 pages, 1 figure; To appear in IEEE Computer (Nov 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17214v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "我们的汽车会说话：物联网如何将人工智能带入车辆", "tldr": "将AI引入车辆作为传感平台，实现主动维护和智能交互，激发跨学科研究。", "motivation": "将AI引入车辆并使其成为传感平台是实现从被动维护到主动维护转变的关键。现在是集成能理解机器和驾驶员两种“语言”的AI副驾驶的最佳时机。", "method": "本文提供了一个概念和技术视角，旨在激发跨学科对话，并指导智能车辆系统、预测性维护和AI驱动的用户交互领域的未来研究和开发。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "将人工智能引入车辆并使其成为传感平台是实现从被动维护到主动维护转变的关键。现在是集成同时掌握机器和驾驶员两种“语言”的AI副驾驶的最佳时机。本文提供了一个概念和技术视角，旨在激发跨学科对话，并指导智能车辆系统、预测性维护和AI驱动的用户交互领域的未来研究和开发。", "summary": "本文探讨了将人工智能引入车辆并使其成为传感平台的重要性，旨在将维护从被动转变为主动。文章强调了集成能够理解机器和驾驶员“语言”的AI副驾驶的需求，并提供了一个概念和技术视角，以期激发关于智能车辆系统、预测性维护和AI驱动用户交互的跨学科对话和未来研究开发。", "keywords": "物联网, 人工智能, 智能车辆, 预测性维护, AI副驾驶", "comments": "这篇文章的创新点在于提出了将AI深度集成到车辆中，使其成为会“说话”的传感平台，从而实现革命性的主动维护。其重要性在于它旨在激发跨学科的思考和合作，为未来智能汽车和AI交互的发展指明方向，而非仅仅呈现技术成果。"}}
{"id": "2507.17481", "title": "AI in Design Education at College Level-Educators' Perspectives and Challenges", "authors": ["Lizhu Zhang", "Cecilia X. Wang"], "categories": ["cs.CY", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17481v1", "summary": "Artificial intelligence has deeply permeated numerous fields, especially the\ndesign area which relies on technology as a tool for innovation. This change\nnaturally extends to the field of design education, which is closest to design\npractice. This has led to further exploration of the impact of AI on\ncollege-level education in the design discipline. This study aims to examine\nhow current design educators perceive the role of AI in college-level design\neducation, their perspectives on integrating AI into teaching and research, and\ntheir concerns regarding its potential challenges in design education and\nresearch. Through qualitative, semi-structured, in-depth interviews with seven\nfaculties in U.S. design colleges, the findings reveal that AI, as a tool and\nsource of information, has become an integral part of design education. AI-\nderived functionalities are increasingly utilized in design software, and\neducators are actively incorporating AI as a theoretical framework in their\nteaching. Educators can guide students in using AI tools, but only if they\nfirst acquire a strong foundation in basic design principles and skills. This\nstudy also indicates the importance of promoting a cooperative relationship\nbetween design educators and AI. At the same time, educators express\nanticipation for advancements in ethical standards, authenticity, and the\nresolution of copyright issues related to AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17481v1", "cate": "cs.CY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大学设计教育中的人工智能——教育者的视角与挑战", "tldr": "研究探讨了大学设计教育者对人工智能的看法、整合方式及面临的挑战。", "motivation": "人工智能已深入渗透设计领域和设计教育。本研究旨在探讨当前设计教育者如何看待人工智能在大学设计教育中的作用、他们对将人工智能整合到教学和研究中的看法，以及他们对人工智能在设计教育和研究中潜在挑战的担忧。", "method": "本研究采用定性研究方法，通过对美国设计学院的七名教师进行半结构化深度访谈。", "result": "研究发现，人工智能作为工具和信息来源，已成为设计教育不可或缺的一部分。人工智能衍生的功能越来越多地应用于设计软件，教育者也积极将人工智能作为理论框架融入教学。教育者可以指导学生使用人工智能工具，但前提是学生必须首先掌握扎实的基本设计原则和技能。本研究还指出，促进设计教育者与人工智能之间合作关系的重要性。同时，教育者也对人工智能相关的伦理标准、真实性和版权问题的进步表示期待。", "conclusion": "人工智能已深度融入大学设计教育，教育者对其持积极态度并积极整合，但同时面临伦理、真实性和版权等挑战，需要促进合作并完善相关标准。学生掌握基础设计技能是有效利用人工智能的前提。", "translation": "人工智能已深入渗透众多领域，特别是依赖技术作为创新工具的设计领域。这一变化自然延伸到最接近设计实践的设计教育领域。这促使人们进一步探索人工智能对大学设计学科教育的影响。本研究旨在探讨当前设计教育者如何看待人工智能在大学设计教育中的作用，他们对将人工智能整合到教学和研究中的看法，以及他们对人工智能在设计教育和研究中潜在挑战的担忧。通过对美国设计学院的七名教师进行定性、半结构化深度访谈，研究结果显示，人工智能作为一种工具和信息来源，已成为设计教育不可或缺的一部分。人工智能衍生的功能越来越多地应用于设计软件，教育者也积极将人工智能作为理论框架融入教学。教育者可以指导学生使用人工智能工具，但前提是学生必须首先掌握扎实的基本设计原则和技能。本研究还指出，促进设计教育者与人工智能之间合作关系的重要性。同时，教育者也对人工智能相关的伦理标准、真实性和版权问题的进步表示期待。", "summary": "本研究探讨了大学设计教育者对人工智能在设计教育中作用的看法、将其整合到教学和研究中的观点，以及对潜在挑战的担忧。通过对美国七名设计学院教师的深度访谈发现，AI已成为设计教育的重要组成部分，作为工具和理论框架被广泛应用。研究强调了学生掌握基础设计技能的重要性，以及促进教育者与AI合作的必要性。同时，教育者也关注AI伦理、真实性和版权等问题。", "keywords": "人工智能, 设计教育, 大学教育, 教育者视角, 挑战", "comments": "这篇论文通过访谈深入探讨了设计教育者在AI时代面临的实际问题和看法，具有较强的现实意义。其创新之处在于从教育者的视角出发，揭示了AI在设计教育中的应用现状、积极影响以及挑战，为未来设计教育的改革提供了宝贵的经验和方向。"}}
{"id": "2507.17232", "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task", "authors": ["Mashiro Toyooka", "Kiyoharu Aizawa", "Yoko Yamakata"], "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.17232v1", "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.17232v1", "cate": "cs.MM", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一个带有食材状态标注的高度干净的食谱数据集，用于状态探测任务", "tldr": "本文提出了一个带有食材状态标注的新型日文食谱数据集，并设计了三个任务来评估大型语言模型在烹饪过程中识别食材中间状态的能力，实验表明学习食材状态知识能提高模型的理解能力。", "motivation": "大型语言模型虽然在大量程序文本上进行训练，但它们无法直接观察真实世界现象。在烹饪食谱中，食材的中间状态常被省略，导致模型难以准确跟踪食材状态和理解食谱。", "method": "本文将状态探测（一种评估语言模型对世界理解的方法）应用于烹饪领域。研究者构建了一个新的日文食谱数据集，其中包含清晰准确的食材状态变化标注，数据来源于结构良好且受控的食谱文本。利用该数据集，设计了三个新颖的任务来评估大型语言模型是否能跟踪食材状态转换并识别中间步骤中存在的食材。", "result": "使用Llama3.1-70B和Qwen2.5-72B等广泛使用的LLM进行的实验表明，学习食材状态知识可以提高它们对烹饪过程的理解，达到了与商业LLM相当的性能。", "conclusion": "通过引入带有食材状态标注的干净数据集和设计相应的探测任务，可以显著提高大型语言模型对烹饪过程中食材状态变化的理解能力。", "translation": "大型语言模型（LLMs）在大量的程序文本上进行训练，但它们不直接观察真实世界的现象。在烹饪食谱的背景下，这带来了一个挑战，因为食材的中间状态经常被省略，使得模型难以跟踪食材状态并准确理解食谱。在本文中，我们将状态探测（一种评估语言模型对世界理解的方法）应用于烹饪领域。我们提出了一个新的任务和数据集，用于评估LLMs在烹饪过程中识别中间食材状态的能力。我们首先构建了一个新的日文食谱数据集，其中包含清晰准确的食材状态变化标注，这些数据收集自结构良好且受控的食谱文本。利用这个数据集，我们设计了三个新颖的任务来评估LLMs是否能跟踪食材状态转换并识别中间步骤中存在的食材。我们使用Llama3.1-70B和Qwen2.5-72B等广泛使用的LLMs进行的实验表明，学习食材状态知识可以提高它们对烹饪过程的理解，达到了与商业LLMs相当的性能。", "summary": "本文针对大型语言模型在理解烹饪食谱中食材中间状态的挑战，提出了一个用于状态探测的新任务和日文数据集。该数据集包含清晰标注的食材状态变化。研究者设计了三个任务来评估LLM跟踪食材状态转换和识别中间食材的能力。实验结果表明，通过学习食材状态知识，LLM对烹饪过程的理解能力显著提高，性能可与商业LLM媲美。", "keywords": "食谱数据集, 食材状态, 状态探测, 大型语言模型, 烹饪过程", "comments": "该论文通过构建一个高质量、带有详细食材状态标注的食谱数据集，并提出相应的状态探测任务，为提升大型语言模型对真实世界过程的理解提供了一个新颖且重要的方向。其创新性在于将“状态探测”这一概念应用于烹饪领域，并证明了明确的食材状态知识对模型理解能力的积极影响。"}}
{"id": "2506.12186", "title": "MRI-CORE: A Foundation Model for Magnetic Resonance Imaging", "authors": ["Haoyu Dong", "Yuwen Chen", "Hanxue Gu", "Nicholas Konz", "Yaqian Chen", "Qihang Li", "Maciej A. Mazurowski"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      36 pages, under review", "url": "http://arxiv.org/abs/2506.12186v2", "summary": "The widespread use of Magnetic Resonance Imaging (MRI) in combination with\ndeep learning shows promise for many high-impact automated diagnostic and\nprognostic tools. However, training new models requires large amounts of\nlabeled data, a challenge due to high cost of precise annotations and data\nprivacy. To address this issue, we introduce the MRI-CORE, a vision foundation\nmodel trained using more than 6 million slices from over 110 thousand MRI\nvolumes across 18 body locations. Our experiments show notable improvements in\nperformance over state-of-the-art methods in 13 data-restricted segmentation\ntasks, as well as in image classification, and zero-shot segmentation, showing\nthe strong potential of MRI-CORE to enable data-efficient development of\nartificial intelligence models. We also present data on which strategies yield\nmost useful foundation models and a novel analysis relating similarity between\npre-training and downstream task data with transfer learning performance. Our\nmodel is publicly available with a permissive license.", "comment": "36 pages, under review", "pdf_url": "http://arxiv.org/pdf/2506.12186v2", "cate": "eess.IV", "date": "2025-06-13", "updated": "2025-07-22", "AI": {"title_translation": "MRI-CORE：一种磁共振成像基础模型", "tldr": "MRI-CORE是一个针对磁共振成像的基础模型，通过大量未标注数据训练，显著提高了数据受限分割、图像分类和零样本分割任务的性能，解决了深度学习在MRI应用中对大量标注数据的需求。", "motivation": "深度学习在MRI诊断和预后工具中潜力巨大，但训练新模型需要大量标注数据，这导致标注成本高昂且存在数据隐私问题。因此，需要一个能够解决数据限制的基础模型。", "method": "研究引入了MRI-CORE，一个视觉基础模型。该模型使用来自18个身体部位的超过11万个MRI体素的600多万张切片进行训练。实验通过在13个数据受限分割任务、图像分类和零样本分割中与现有最先进方法进行比较来评估其性能。此外，还分析了哪些策略能产生最有用的基础模型，并探讨了预训练数据与下游任务数据相似性与迁移学习性能的关系。", "result": "MRI-CORE在13个数据受限分割任务、图像分类和零样本分割中，性能显著优于现有最先进的方法。研究还提供了关于何种策略能产生最有用的基础模型的数据，以及预训练和下游任务数据相似性与迁移学习性能之间关系的新颖分析。", "conclusion": "MRI-CORE展现出强大的潜力，能够实现人工智能模型的数据高效开发，有效解决了深度学习在MRI应用中对大量标注数据的依赖问题。该模型已公开发布。", "translation": "磁共振成像（MRI）与深度学习的广泛结合，有望为许多具有高影响力的自动化诊断和预后工具带来前景。然而，训练新模型需要大量标注数据，由于精确标注成本高昂和数据隐私问题，这是一个挑战。为了解决这个问题，我们引入了MRI-CORE，这是一个视觉基础模型，使用来自18个身体部位的超过11万个MRI体素的600多万张切片进行训练。我们的实验表明，在13个数据受限分割任务以及图像分类和零样本分割中，性能比最先进的方法有显著提高，显示了MRI-CORE在实现人工智能模型数据高效开发方面的强大潜力。我们还提供了关于哪些策略能产生最有用的基础模型的数据，以及预训练和下游任务数据相似性与迁移学习性能之间关系的新颖分析。我们的模型已获得宽松许可并公开发布。", "summary": "MRI-CORE是一个为磁共振成像设计的基础模型，旨在解决深度学习在MRI应用中对大量标注数据的依赖。该模型通过大规模的MRI数据（超过600万张切片）进行预训练，并在数据受限的分割、图像分类和零样本分割任务中取得了优于现有技术的显著性能提升。研究还探讨了优化基础模型的策略以及预训练数据与迁移学习性能的关系，证明了MRI-CORE在推动MRI领域AI模型数据高效开发方面的巨大潜力，并且该模型已开源。", "keywords": "磁共振成像, 基础模型, 深度学习, 图像分割, 迁移学习", "comments": "MRI-CORE的创新之处在于其作为MRI领域首个大规模预训练的基础模型，有效解决了医疗影像领域标注数据稀缺的痛点。其在多项任务上表现出的优越性能，特别是对数据受限场景的提升，预示着其在临床诊断和研究中具有广泛的应用前景。模型的开源性也极大地促进了社区的进一步研究和应用。然而，其泛化能力在更广泛的临床数据集上的表现以及在不同疾病诊断中的具体效用仍需进一步验证。"}}
{"id": "2404.16417", "title": "Constructing Optimal Noise Channels for Enhanced Robustness in Quantum Machine Learning", "authors": ["David Winderl", "Nicola Franco", "Jeanette Miriam Lorenz"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      QML technical track at IEEE QCE 2025", "url": "http://arxiv.org/abs/2404.16417v2", "summary": "With the rapid advancement of Quantum Machine Learning (QML), the critical\nneed to enhance security measures against adversarial attacks and protect QML\nmodels becomes increasingly evident. In this work, we outline the connection\nbetween quantum noise channels and differential privacy (DP), by constructing a\nfamily of noise channels which are inherently $\\epsilon$-DP: $(\\alpha,\n\\gamma)$-channels. Through this approach, we successfully replicate the\n$\\epsilon$-DP bounds observed for depolarizing and random rotation channels,\nthereby affirming the broad generality of our framework. Additionally, we use a\nsemi-definite program to construct an optimally robust channel. In a\nsmall-scale experimental evaluation, we demonstrate the benefits of using our\noptimal noise channel over depolarizing noise, particularly in enhancing\nadversarial accuracy. Moreover, we assess how the variables $\\alpha$ and\n$\\gamma$ affect the certifiable robustness and investigate how different\nencoding methods impact the classifier's robustness.", "comment": "QML technical track at IEEE QCE 2025", "pdf_url": "http://arxiv.org/pdf/2404.16417v2", "cate": "quant-ph", "date": "2024-04-25", "updated": "2025-07-23", "AI": {"title_translation": "构建最优噪声信道以增强量子机器学习的鲁棒性", "tldr": "本文通过构建固有的ε-差分隐私量子噪声信道（(α, γ)-信道）并利用半定规划构建最优鲁棒信道，以增强量子机器学习模型对抗性攻击的鲁棒性。", "motivation": "随着量子机器学习（QML）的快速发展，增强对抗性攻击的安全措施和保护QML模型的关键需求日益突出。", "method": "本文通过构建一种固有的ε-差分隐私（DP）的噪声信道族——(α, γ)-信道，来阐明量子噪声信道与差分隐私之间的联系。此外，还使用半定规划来构建一个最优鲁棒信道。", "result": "成功复制了去极化和随机旋转信道的ε-DP界限，证实了该框架的广泛通用性。在小规模实验评估中，证明了使用最优噪声信道比去极化噪声更能提高对抗性准确性。此外，评估了变量α和γ如何影响可认证的鲁棒性，并研究了不同编码方法如何影响分类器的鲁棒性。", "conclusion": "通过构建和优化量子噪声信道，可以有效提升量子机器学习模型对抗性攻击的鲁棒性和安全性。", "translation": "随着量子机器学习（QML）的快速发展，增强对抗性攻击的安全措施和保护QML模型的关键需求日益突出。在这项工作中，我们通过构建一个固有的ε-差分隐私（DP）噪声信道族——(α, γ)-信道，阐明了量子噪声信道与差分隐私之间的联系。通过这种方法，我们成功复制了去极化和随机旋转信道所观察到的ε-DP界限，从而证实了我们框架的广泛通用性。此外，我们使用半定规划来构建一个最优鲁棒信道。在小规模实验评估中，我们展示了使用我们的最优噪声信道相较于去极化噪声的优势，特别是在提高对抗性准确性方面。此外，我们评估了变量α和γ如何影响可认证的鲁棒性，并研究了不同编码方法如何影响分类器的鲁棒性。", "summary": "本研究旨在通过构建最优量子噪声信道来增强量子机器学习（QML）模型的鲁棒性，以应对对抗性攻击。文章阐述了量子噪声信道与差分隐私（DP）之间的联系，并构建了一族固有的ε-DP (α, γ)-信道。通过此方法，成功复制了现有噪声信道的DP界限，验证了框架的通用性。此外，利用半定规划构建了一个最优鲁棒信道，并在实验中展示了其在提高对抗性准确性方面的优势。研究还探讨了信道参数和编码方法对模型鲁棒性的影响。", "keywords": "量子机器学习, 噪声信道, 差分隐私, 鲁棒性, 对抗性攻击", "comments": "本文的创新之处在于将量子噪声信道与差分隐私理论相结合，并提出了一种构建最优鲁棒信道的方法。这为提升量子机器学习模型的安全性和对抗性鲁棒性提供了新的视角和实用方案。其方法具有通用性，并通过实验验证了有效性，对QML的安全研究具有重要意义。"}}
{"id": "2507.13708", "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ECAI 2025", "url": "http://arxiv.org/abs/2507.13708v2", "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.", "comment": "ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.13708v2", "cate": "cs.CV", "date": "2025-07-18", "updated": "2025-07-23", "AI": {"title_translation": "PoemTale Diffusion：最小化诗歌到图像生成中的信息损失与多阶段提示词优化", "tldr": "PoemTale Diffusion提出了一种无需训练的方法，通过多阶段提示词优化和修改扩散模型的自注意力机制，以最小化诗歌到图像生成中的信息损失，并引入了P4I数据集。", "motivation": "现有的文本到图像扩散模型在处理复杂、抽象或高度描述性的创意表达（尤其是诗歌）时，难以准确解释文本提示，导致信息损失。", "method": "本研究提出了PoemTale Diffusion方法，通过将多阶段提示词优化循环整合到语言模型中，以增强诗歌文本的可解释性，从而最小化诗歌到图像转换中的信息损失。此外，通过修改现有最先进扩散模型的自注意力机制，采用一致的自注意力技术生成多个一致图像，并引入了包含1111首诗歌的P4I（PoemForImage）数据集。", "result": "人类和定量评估结果均验证了该方法的有效性，并为诗歌到图像生成提供了一个新颖的视角，增强了生成图像中的信息捕获能力。", "conclusion": "PoemTale Diffusion通过多阶段提示词优化和修改扩散模型自注意力机制，有效提升了诗歌到图像生成的质量，最小化了信息损失，并为该领域的研究做出了贡献。", "translation": "最近文本到图像扩散模型的进步在生成逼真和多样化的视觉内容方面取得了显著成功。这个过程中的一个关键因素是模型准确解释文本提示的能力。然而，这些模型在处理创意表达时常常遇到困难，特别是那些涉及复杂、抽象或高度描述性语言的表达。在这项工作中，我们引入了一种新颖的、无需训练的方法，专门用于改善一种独特的创意语言形式——诗歌的图像生成，诗歌经常具有分层、抽象和双重含义。我们提出的PoemTale Diffusion方法旨在通过将多阶段提示词优化循环整合到语言模型中，以增强诗歌文本的可解释性，从而最小化诗意文本到图像转换过程中丢失的信息。为了支持这一点，我们通过修改其自注意力机制，采用一致的自注意力技术来生成多个一致的图像，然后集体使用这些图像来传达诗歌的意义，从而适应了现有的最先进扩散模型。此外，为了鼓励诗歌领域的研究，我们引入了P4I（PoemForImage）数据集，该数据集包含从多个在线和离线资源中获取的1111首诗歌。我们邀请了一组诗歌专家进行定性评估。人类和定量评估的结果都验证了我们方法的有效性，并为诗歌到图像生成提供了一个新颖的视角，增强了生成图像中的信息捕获。", "summary": "本文提出PoemTale Diffusion，一种无需训练的诗歌到图像生成方法，旨在通过多阶段提示词优化和修改扩散模型的自注意力机制来最小化信息损失。该方法通过增强语言模型对诗歌文本的解释性，并利用一致的自注意力技术生成多张图像以共同表达诗意。为促进研究，论文还发布了P4I诗歌图像数据集。实验结果表明该方法有效提升了诗歌到图像生成的质量。", "keywords": "诗歌到图像生成, 扩散模型, 提示词优化, 信息损失, P4I数据集", "comments": "PoemTale Diffusion的创新之处在于其“无需训练”的方法，这降低了部署成本和复杂性。多阶段提示词优化与修改自注意力机制的结合，有效解决了诗歌这种复杂文本信息在图像生成中的损失问题，具有重要的实践意义。P4I数据集的引入对推动诗歌到图像生成这一新兴领域的研究具有显著贡献。"}}
{"id": "2507.16864", "title": "Reinforcement Learning in hyperbolic space for multi-step reasoning", "authors": ["Tao Xu", "Dung-Yang Lee", "Momiao Xiong"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      53 pages, 5 figures", "url": "http://arxiv.org/abs/2507.16864v1", "summary": "Multi-step reasoning is a fundamental challenge in artificial intelligence,\nwith applications ranging from mathematical problem-solving to decision-making\nin dynamic environments. Reinforcement Learning (RL) has shown promise in\nenabling agents to perform multi-step reasoning by optimizing long-term\nrewards. However, conventional RL methods struggle with complex reasoning tasks\ndue to issues such as credit assignment, high-dimensional state\nrepresentations, and stability concerns. Recent advancements in Transformer\narchitectures and hyperbolic geometry have provided novel solutions to these\nchallenges. This paper introduces a new framework that integrates hyperbolic\nTransformers into RL for multi-step reasoning. The proposed approach leverages\nhyperbolic embeddings to model hierarchical structures effectively. We present\ntheoretical insights, algorithmic details, and experimental results that\ninclude Frontier Math and nonlinear optimal control problems. Compared to RL\nwith vanilla transformer, the hyperbolic RL largely improves accuracy by\n(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control\nbenchmark, while achieving impressive reduction in computational time by\n(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control\nbenchmark. Our work demonstrates the potential of hyperbolic Transformers in\nreinforcement learning, particularly for multi-step reasoning tasks that\ninvolve hierarchical structures.", "comment": "53 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16864v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "双曲空间中用于多步推理的强化学习", "tldr": "本文提出了一种将双曲Transformer集成到强化学习中的新框架，用于解决多步推理任务，该方法在准确性和计算效率上均显著优于传统方法。", "motivation": "多步推理是人工智能中的一个基本挑战，传统的强化学习方法在处理复杂推理任务时面临信用分配、高维状态表示和稳定性等问题。", "method": "本文引入了一个新框架，将双曲Transformer集成到强化学习中，用于多步推理。该方法利用双曲嵌入有效地建模分层结构。", "result": "与使用普通Transformer的强化学习相比，双曲强化学习在FrontierMath基准测试中将准确性提高了32%~44%，在非线性最优控制基准测试中提高了43%~45%。同时，在FrontierMath基准测试中计算时间减少了16%~32%，在非线性最优控制基准测试中减少了16%~17%。", "conclusion": "本文工作证明了双曲Transformer在强化学习中的潜力，特别适用于涉及分层结构的多步推理任务。", "translation": "多步推理是人工智能中的一个基本挑战，应用范围从数学问题求解到动态环境中的决策制定。强化学习（RL）通过优化长期奖励，在使智能体执行多步推理方面展现出潜力。然而，传统的RL方法由于信用分配、高维状态表示和稳定性问题，难以处理复杂的推理任务。Transformer架构和双曲几何的最新进展为这些挑战提供了新颖的解决方案。本文介绍了一个新框架，将双曲Transformer集成到RL中，用于多步推理。所提出的方法利用双曲嵌入有效地建模分层结构。我们提出了理论见解、算法细节和实验结果，包括前沿数学和非线性最优控制问题。与使用普通Transformer的RL相比，双曲RL在FrontierMath基准测试中将准确性大幅提高了（32%~44%），在非线性最优控制基准测试中提高了（43%~45%），同时在FrontierMath基准测试中计算时间显著减少了（16%~32%），在非线性最优控制基准测试中减少了（16%~17%）。我们的工作证明了双曲Transformer在强化学习中的潜力，特别是对于涉及分层结构的多步推理任务。", "summary": "本文针对强化学习在多步推理中面临的挑战，提出了一种将双曲Transformer集成到RL的新框架。该方法利用双曲嵌入有效处理高维和分层结构，并在FrontierMath和非线性最优控制等复杂任务上，显著提高了准确性并降低了计算时间，证明了双曲Transformer在处理多步推理任务中的巨大潜力。", "keywords": "强化学习, 双曲空间, 多步推理, Transformer, 分层结构", "comments": "该论文的创新点在于将双曲几何（特别是双曲Transformer）引入强化学习，以解决传统RL在复杂多步推理中遇到的高维表示和分层结构建模难题。其重要性体现在通过双曲嵌入有效处理了分层数据，并在实验中展现出显著的性能提升，为未来在复杂AI任务中结合几何结构与深度学习提供了新的方向。"}}
{"id": "2507.17346", "title": "DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD", "authors": ["Rongwei Lu", "Jingyan Jiang", "Chunyang Li", "Haotian Dong", "Xingguang Wei", "Delin Cai", "Zhi Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17346v1", "summary": "Distributed machine learning in high end-to-end latency and low, varying\nbandwidth network environments undergoes severe throughput degradation. Due to\nits low communication requirements, distributed SGD (D-SGD) remains the\nmainstream optimizer in such challenging networks, but it still suffers from\nsignificant throughput reduction. To mitigate these limitations, existing\napproaches typically employ gradient compression and delayed aggregation to\nalleviate low bandwidth and high latency, respectively. To address both\nchallenges simultaneously, these strategies are often combined, introducing a\ncomplex three-way trade-off among compression ratio, staleness (delayed\nsynchronization steps), and model convergence rate. To achieve the balance\nunder varying bandwidth conditions, an adaptive policy is required to\ndynamically adjust these parameters. Unfortunately, existing works rely on\nstatic heuristic strategies due to the lack of theoretical guidance, which\nprevents them from achieving this goal. This study fills in this theoretical\ngap by introducing a new theoretical tool, decomposing the joint optimization\nproblem into a traditional convergence rate analysis with multiple analyzable\nnoise terms. We are the first to reveal that staleness exponentially amplifies\nthe negative impact of gradient compression on training performance, filling a\ncritical gap in understanding how compressed and delayed gradients affect\ntraining. Furthermore, by integrating the convergence rate with a network-aware\ntime minimization condition, we propose DeCo-SGD, which dynamically adjusts the\ncompression ratio and staleness based on the real-time network condition and\ntraining task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and\nstatic strategy in high-latency and low, varying bandwidth networks,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17346v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DeCo-SGD: 分布式SGD中延迟陈旧度与梯度压缩比的联合优化", "tldr": "DeCo-SGD通过动态调整梯度压缩和延迟同步，解决了分布式SGD在高延迟低带宽网络中的性能问题，首次揭示了陈旧度对梯度压缩负面影响的指数级放大作用，并实现了显著的加速。", "motivation": "在高端到端延迟和低、可变带宽的网络环境中，分布式机器学习的吞吐量严重下降。尽管分布式SGD (D-SGD) 因其低通信要求而成为主流优化器，但在这些具有挑战性的网络中仍面临显著的吞吐量降低。现有方法通常采用梯度压缩和延迟聚合来分别缓解低带宽和高延迟问题，但这些策略结合后引入了压缩比、陈旧度（延迟同步步骤）和模型收敛速度之间复杂的权衡，且缺乏理论指导导致现有工作依赖静态启发式策略，无法实现动态调整以适应带宽变化。", "method": "本研究通过引入一种新的理论工具，将联合优化问题分解为具有多个可分析噪声项的传统收敛率分析，填补了理论空白。首次揭示了陈旧度对梯度压缩对训练性能的负面影响具有指数级放大作用。此外，通过将收敛率与网络感知的最小化时间条件相结合，提出了DeCo-SGD，它根据实时网络条件和训练任务动态调整压缩比和陈旧度。", "result": "DeCo-SGD在高延迟和低、可变带宽网络中，相比D-SGD和静态策略分别实现了高达5.07倍和1.37倍的加速。", "conclusion": "DeCo-SGD通过理论指导下的动态调整策略，有效解决了分布式SGD在高延迟低带宽网络中的性能瓶颈，显著提升了训练效率，并首次揭示了陈旧度与梯度压缩的相互作用机制。", "translation": "在高端到端延迟和低、可变带宽的网络环境中，分布式机器学习的吞吐量严重下降。由于其低通信要求，分布式SGD（D-SGD）在这种具有挑战性的网络中仍然是主流优化器，但它仍然面临显著的吞吐量降低。为了缓解这些限制，现有方法通常采用梯度压缩和延迟聚合来分别缓解低带宽和高延迟。为了同时解决这两个挑战，这些策略通常结合使用，引入了压缩比、陈旧度（延迟同步步骤）和模型收敛速度之间复杂的三向权衡。为了在变化的带宽条件下实现平衡，需要一种自适应策略来动态调整这些参数。不幸的是，由于缺乏理论指导，现有工作依赖静态启发式策略，这阻碍了它们实现这一目标。本研究通过引入一种新的理论工具填补了这一理论空白，将联合优化问题分解为具有多个可分析噪声项的传统收敛率分析。我们首次揭示了陈旧度指数级地放大了梯度压缩对训练性能的负面影响，填补了理解压缩和延迟梯度如何影响训练的关键空白。此外，通过将收敛率与网络感知的最小化时间条件相结合，我们提出了DeCo-SGD，它根据实时网络条件和训练任务动态调整压缩比和陈旧度。DeCo-SGD在高延迟和低、可变带宽网络中，相比D-SGD和静态策略分别实现了高达5.07倍和1.37倍的加速。", "summary": "本文针对高延迟和低带宽网络中分布式SGD (D-SGD) 的吞吐量下降问题，提出了一种名为DeCo-SGD的新方法。现有方法通常结合梯度压缩和延迟聚合，但缺乏动态调整的理论指导。DeCo-SGD通过引入新的理论工具，首次揭示了陈旧度会指数级放大梯度压缩对训练性能的负面影响。该方法将收敛率与网络感知的时间最小化条件相结合，能够根据实时网络条件和训练任务动态调整梯度压缩比和陈旧度。实验结果表明，DeCo-SGD在高延迟和低带宽网络中分别比D-SGD和静态策略实现了显著的加速。", "keywords": "分布式SGD, 梯度压缩, 延迟聚合, 联合优化, 网络感知", "comments": "DeCo-SGD的创新之处在于首次为分布式SGD中的梯度压缩和延迟聚合的联合优化提供了理论指导，特别是揭示了陈旧度对梯度压缩负面影响的指数级放大作用，这填补了该领域的一个关键理论空白。其提出的动态调整策略能够有效适应网络变化，显著提升了分布式机器学习在挑战性网络环境下的效率和性能。"}}
{"id": "2507.17047", "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding", "authors": ["Kuleen Sasse", "Efsun Sarioglu Kayi", "Arun Reddy"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17047v1", "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17047v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "可控混合字幕生成器，用于改进长篇视频理解", "tldr": "本文提出了一种可控混合字幕生成器，能够高效地为长篇视频生成动作和场景描述，从而提高视频理解能力并扩展可回答问题的范围。", "motivation": "长篇视频数据量巨大且高维，难以直接处理。文本摘要提供了一种紧凑且可供大型语言模型（LLM）处理的方式，以实现视频内容推理并回答复杂自然语言查询。然而，现有视频字幕主要关注人类动作，而查询可能涉及场景信息，因此需要丰富文本记忆。", "method": "该研究通过分段处理视频并逐步构建文本记忆。它探索了改进短视频字幕质量的方法，并利用视觉语言模型（VLM）丰富了静态场景描述。研究系统结合LaViLa视频字幕生成器和LLM来回答视频问题。具体方法包括：1) 探索视频分段方式以更准确反映内容结构；2) 使用LLaVA VLM将静态场景描述整合到字幕生成流程中；3) 成功微调LaViLa视频字幕生成器以同时生成动作和场景字幕，并通过特殊输入令牌根据视频中检测到的场景变化来切换字幕类型。", "result": "研究结果是生成了更详细、更完整的字幕日志，扩展了可以从文本记忆中回答问题的范围。此外，与为两项任务使用单独字幕模型相比，该方法显著提高了字幕生成流程的效率。", "conclusion": "本文成功开发了一种可控混合字幕生成器，能够高效地生成动作和场景字幕，显著提升了长篇视频的理解能力和基于文本记忆的问答范围。", "translation": "视频数据，尤其是长篇视频，极其密集且高维。基于文本的视频内容摘要提供了一种比原始视频更紧凑地表示查询相关内容的方式。此外，文本表示易于被最先进的大型语言模型（LLM）吸收，从而能够对视频内容进行推理以回答复杂的自然语言查询。为了解决这个问题，我们依赖于视频字幕生成器在视频较短片段上逐步构建基于文本的记忆，在这些片段上时空建模在计算上是可行的。我们探索了改进仅由短视频字幕组成的活动日志质量的方法。因为视频字幕往往侧重于人类动作，而问题可能涉及场景中的其他信息，我们试图使用视觉语言模型（VLM）通过静态场景描述来丰富记忆。我们的视频理解系统依靠LaViLa视频字幕生成器结合LLM来回答有关视频的问题。我们首先探索了将视频划分为有意义片段的不同方法，使得文本描述更准确地反映视频内容的结构。此外，我们使用LLaVA VLM将静态场景描述整合到字幕生成流程中，从而生成了更详细、更完整的字幕日志，并扩展了可以从文本记忆中回答问题的范围。最后，我们成功地微调了LaViLa视频字幕生成器，使其能够同时生成动作和场景字幕，与为两项任务使用单独字幕模型相比，显著提高了字幕生成流程的效率。我们的模型，即可控混合字幕生成器，可以根据视频中检测到的场景变化的特殊输入令牌在不同类型的字幕之间进行切换。", "summary": "本文提出了一种可控混合字幕生成器，旨在解决长篇视频内容理解的挑战。该系统通过分段处理视频并结合LaViLa视频字幕生成器与LLM来构建文本记忆。为弥补现有字幕对场景信息关注不足的问题，研究引入了静态场景描述（通过LLaVA VLM），并成功微调LaViLa模型使其能同时生成动作和场景字幕，并能根据视频场景变化智能切换字幕类型。这不仅生成了更详细完整的字幕日志，扩展了可回答问题的范围，还显著提升了字幕生成的效率。", "keywords": "长篇视频理解, 视频字幕, 混合字幕生成器, 大型语言模型, 视觉语言模型", "comments": "这项研究的创新之处在于其提出的“可控混合字幕生成器”，它能够在一个统一的框架内高效地生成视频的动作和静态场景描述。通过微调单一模型以处理两种类型的字幕，并利用场景变化信号进行智能切换，极大地提高了字幕生成的效率和质量。这对于提升长篇视频理解、扩展大型语言模型在视频内容推理方面的应用具有重要意义。该方法有效解决了视频数据密度高以及现有字幕局限于动作描述的问题，为构建更全面、更具上下文感知的视频文本记忆提供了新的途径。"}}
{"id": "2507.17527", "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Seed-LiveInterpret 2.0 Technical Report", "url": "http://arxiv.org/abs/2507.17527v1", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "comment": "Seed-LiveInterpret 2.0 Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.17527v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Seed LiveInterpret 2.0：端到端同步语音到语音翻译，支持您的声音", "tldr": "Seed-LiveInterpret 2.0是一个端到端的同步语音到语音翻译模型，通过语音克隆实现高保真、超低延迟的翻译，并在翻译质量和延迟方面显著优于现有商业解决方案。", "motivation": "同步口译（SI）是翻译行业中最具挑战性的领域之一，现有产品级自动化系统面临着转录和翻译质量不佳、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀（尤其是在长篇话语中）等难题。", "method": "本文提出了Seed-LiveInterpret 2.0，一个端到端SI模型，通过其新颖的双工语音到语音理解-生成框架来解决现有挑战。该模型通过大规模预训练和强化学习进行训练。", "result": "Seed-LiveInterpret 2.0在翻译准确性和延迟之间取得了显著更好的平衡，经人工译员验证，在复杂场景下的正确率超过70%。它在翻译质量上显著优于商业SI解决方案，并将克隆语音的平均延迟从近10秒大幅缩短至近实时的3秒（约70%的缩减）。", "conclusion": "Seed-LiveInterpret 2.0作为一个完全可操作的产品级解决方案，成功解决了同步口译中的关键挑战，实现了高保真、超低延迟的语音到语音翻译，并具有语音克隆能力，显著提升了实际可用性。", "translation": "同步口译（SI）代表着翻译行业中最严峻的前沿之一，产品级自动化系统长期以来一直受到难以解决的挑战困扰：转录和翻译质量不佳、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀，尤其是在长篇话语中。在本研究中，我们介绍了Seed-LiveInterpret 2.0，这是一个端到端SI模型，可提供高保真、超低延迟的语音到语音生成，并具有语音克隆功能。作为一款完全可操作的产品级解决方案，Seed-LiveInterpret 2.0通过我们新颖的双工语音到语音理解-生成框架直接解决了这些挑战。实验结果表明，通过大规模预训练和强化学习，该模型在翻译准确性和延迟之间取得了显著更好的平衡，经人工译员验证，在复杂场景下的正确率超过70%。值得注意的是，Seed-LiveInterpret 2.0在翻译质量上显著优于商业SI解决方案，同时将克隆语音的平均延迟从近10秒大幅缩减至近实时的3秒，这大约是近70%的缩减，极大地增强了实际可用性。", "summary": "Seed-LiveInterpret 2.0是一个端到端的同步语音到语音翻译（SI）模型，旨在解决现有SI系统面临的转录和翻译质量差、实时性不足、多说话人识别困难及语音膨胀等问题。该模型采用新颖的双工理解-生成框架，结合大规模预训练和强化学习，实现了高保真、超低延迟的语音到语音翻译，并支持语音克隆。实验证明，其在翻译准确性与延迟之间取得了更好的平衡，在复杂场景下正确率超70%，且在翻译质量上显著优于商业解决方案，同时将语音延迟大幅降低了约70%，显著提升了实用性。", "keywords": "同步口译, 语音到语音翻译, 语音克隆, 端到端, 低延迟", "comments": "该论文提出了一种创新的端到端同步口译解决方案，通过结合语音克隆技术和优化的理解-生成框架，显著提升了翻译质量和实时性。其在延迟上的巨大改进（70%的降低）是重要的突破，使其更接近实际应用需求。作为产品级解决方案的定位也表明了其实用价值。"}}
{"id": "2401.01100", "title": "Sampling-enabled scalable manifold learning unveils discriminative cluster structure of high-dimensional data", "authors": ["Dehua Peng", "Zhipeng Gui", "Wenzhang Wei", "Fa Li", "Jie Gui", "Huayi Wu", "Jianya Gong"], "categories": ["cs.LG", "I.5.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      80 pages, 37 figures", "url": "http://arxiv.org/abs/2401.01100v3", "summary": "As a pivotal branch of machine learning, manifold learning uncovers the\nintrinsic low-dimensional structure within complex nonlinear manifolds in\nhigh-dimensional space for visualization, classification, clustering, and\ngaining key insights. Although existing techniques have achieved remarkable\nsuccesses, they suffer from extensive distortions of cluster structure, which\nhinders the understanding of underlying patterns. Scalability issues also limit\ntheir applicability for handling large-scale data. We hence propose a\nsampling-based Scalable manifold learning technique that enables Uniform and\nDiscriminative Embedding, namely SUDE, for large-scale and high-dimensional\ndata. It starts by seeking a set of landmarks to construct the low-dimensional\nskeleton of the entire data, and then incorporates the non-landmarks into the\nlearned space based on the constrained locally linear embedding (CLLE). We\nempirically validated the effectiveness of SUDE on synthetic datasets and\nreal-world benchmarks, and applied it to analyze single-cell data and detect\nanomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage\nin scalability with respect to data size and embedding dimension, and has\npromising performance in cluster separation, integrity, and global structure\npreservation. The experiments also demonstrate notable robustness in embedding\nquality as the sampling rate decreases.", "comment": "80 pages, 37 figures", "pdf_url": "http://arxiv.org/pdf/2401.01100v3", "cate": "cs.LG", "date": "2024-01-02", "updated": "2025-07-23", "AI": {"title_translation": "采样驱动的可伸缩流形学习揭示高维数据的判别性聚类结构", "tldr": "SUDE是一种基于采样的可伸缩流形学习技术，解决了现有流形学习方法在处理高维数据时存在的聚类结构扭曲和可伸缩性问题，能有效揭示数据的判别性聚类结构。", "motivation": "现有的流形学习技术在揭示高维数据内在低维结构时，面临聚类结构严重扭曲和可伸缩性差的问题，这阻碍了对底层模式的理解，并限制了它们在大规模数据处理中的应用。", "method": "本文提出了一种名为SUDE（Sampling-based Scalable manifold learning technique that enables Uniform and Discriminative Embedding）的采样驱动可伸缩流形学习技术。该方法首先通过寻找一组地标点来构建整个数据的低维骨架，然后基于约束局部线性嵌入（CLLE）将非地标点整合到学习到的空间中。", "result": "SUDE在合成数据集和真实世界基准测试中验证了其有效性，并成功应用于单细胞数据分析和心电图（ECG）信号异常检测。SUDE在数据大小和嵌入维度方面表现出显著的可伸缩性优势，并在聚类分离、完整性和全局结构保持方面具有良好的性能。实验还表明，随着采样率的降低，嵌入质量仍具有显著的鲁棒性。", "conclusion": "SUDE是一种有效且可伸缩的流形学习技术，能够揭示高维数据的判别性聚类结构，解决现有方法的局限性，并对采样率变化具有鲁棒性。", "translation": "流形学习作为机器学习的一个关键分支，旨在揭示高维空间中复杂非线性流形的内在低维结构，以实现可视化、分类、聚类和获取关键洞察。尽管现有技术取得了显著成功，但它们存在聚类结构严重扭曲的问题，这阻碍了对底层模式的理解。可伸缩性问题也限制了它们处理大规模数据的适用性。因此，我们提出了一种名为SUDE（Sampling-enabled Scalable manifold learning technique that enables Uniform and Discriminative Embedding）的采样驱动可伸缩流形学习技术，用于处理大规模高维数据。它首先通过寻找一组地标点来构建整个数据的低维骨架，然后基于约束局部线性嵌入（CLLE）将非地标点整合到学习到的空间中。我们通过合成数据集和真实世界基准测试经验性地验证了SUDE的有效性，并将其应用于单细胞数据分析和心电图（ECG）信号异常检测。SUDE在数据大小和嵌入维度方面表现出显著的可伸缩性优势，并在聚类分离、完整性和全局结构保持方面具有良好的性能。实验还表明，随着采样率的降低，嵌入质量仍具有显著的鲁棒性。", "summary": "本文提出了一种名为SUDE的采样驱动可伸缩流形学习技术，旨在解决现有方法在处理高维数据时，聚类结构扭曲和可伸缩性差的问题。SUDE通过选择地标点构建数据骨架，并利用约束局部线性嵌入整合非地标点。实验证明，SUDE在可伸缩性、聚类分离、结构保持和对采样率变化的鲁棒性方面表现优异，并成功应用于单细胞数据和ECG信号分析。", "keywords": "流形学习, 高维数据, 可伸缩性, 聚类结构, 采样", "comments": "SUDE的创新点在于结合了采样策略与改进的嵌入方法（CLLE），有效地解决了大规模高维数据流形学习中的两大挑战：聚类结构失真和可伸缩性不足。其在真实世界数据集上的应用展示了其潜在的实用价值。该研究为高维数据的可视化和模式识别提供了新的有效工具。"}}
{"id": "2507.16535", "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion", "authors": ["Shang Liu", "Chenjie Cao", "Chaohui Yu", "Wen Qian", "Jing Wang", "Fan Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Models and codes will be released at this https URL: this https URL", "url": "http://arxiv.org/abs/2507.16535v2", "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/", "comment": "Models and codes will be released at this https URL:\n  https://github.com/whiteinblue/EarthCrafter", "pdf_url": "http://arxiv.org/pdf/2507.16535v2", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "EarthCrafter: 通过双稀疏潜在扩散实现可扩展的三维地球生成", "tldr": "EarthCrafter通过引入最大的3D航拍数据集Aerial-Earth3D和一种将结构与纹理生成分离的双稀疏潜在扩散框架，实现了可扩展的大规模三维地球生成。", "motivation": "现有的三维生成方法难以扩展到地理尺度，例如模拟数千平方公里的地球表面，这是一个开放的挑战。", "method": "本研究通过数据基础设施和模型架构的双重创新来解决问题。首先，引入了迄今为止最大的3D航拍数据集Aerial-Earth3D，包含5万个精心策划的场景（每个600m x 600m），涵盖美国大陆，包含45M多视图Google Earth帧，并提供姿态标注的多视图图像、深度图、法线、语义分割和相机姿态，且有明确的质量控制。其次，提出了EarthCrafter框架，一个通过稀疏解耦潜在扩散进行大规模3D地球生成的定制框架。其架构分离了结构和纹理生成：1）双稀疏3D-VAEs将高分辨率几何体素和纹理2D高斯散斑（2DGS）压缩到紧凑的潜在空间，以缓解大规模地理尺度的计算成本。2）提出了在混合输入（语义、图像或两者皆无）上训练的条件感知流匹配模型，以独立灵活地建模潜在几何和纹理特征。", "result": "EarthCrafter在超大规模生成方面表现显著更好。该框架进一步支持多种应用，从语义引导的城市布局生成到无条件地形合成，同时通过Aerial-Earth3D的丰富数据先验保持地理合理性。", "conclusion": "EarthCrafter通过其创新的数据基础设施和模型架构，有效解决了大规模三维地球生成的挑战，并在超大规模生成和多功能应用方面展现出卓越性能。", "translation": "尽管最近的三维生成工作取得了显著进展，但将这些方法扩展到地理范围，例如模拟数千平方公里的地球表面，仍然是一个开放的挑战。我们通过数据基础设施和模型架构的双重创新来解决这个问题。首先，我们引入了迄今为止最大的三维航拍数据集Aerial-Earth3D，它包含在美国大陆捕获的5万个精心策划的场景（每个测量600米 x 600米），总计4500万张多视图Google Earth帧。每个场景都提供姿态标注的多视图图像、深度图、法线、语义分割和相机姿态，并进行明确的质量控制以确保地形多样性。在此基础上，我们提出了EarthCrafter，一个通过稀疏解耦潜在扩散进行大规模三维地球生成的定制框架。我们的架构将结构和纹理生成分离：1）双稀疏3D-VAEs将高分辨率几何体素和纹理2D高斯散斑（2DGS）压缩到紧凑的潜在空间，这在很大程度上缓解了大规模地理尺度带来的昂贵计算，同时保留了关键信息。2）我们提出了在混合输入（语义、图像或两者皆无）上训练的条件感知流匹配模型，以独立灵活地建模潜在几何和纹理特征。广泛的实验表明，EarthCrafter在超大规模生成方面表现显著更好。该框架进一步支持多种应用，从语义引导的城市布局生成到无条件地形合成，同时通过我们从Aerial-Earth3D获得的丰富数据先验保持地理合理性。我们的项目页面可在https://whiteinblue.github.io/earthcrafter/访问。", "summary": "EarthCrafter提出了一种可扩展的三维地球生成方法，通过构建迄今为止最大的3D航拍数据集Aerial-Earth3D，并开发了一种双稀疏潜在扩散框架。该框架将几何结构和纹理生成解耦，利用双稀疏3D-VAEs进行高效压缩，并采用条件感知流匹配模型独立建模特征，从而显著提升了大规模地理范围的生成效率和质量，并支持多样化的应用。", "keywords": "3D Earth Generation, Latent Diffusion, Large-scale, Aerial-Earth3D, Dual-Sparse", "comments": "这篇论文通过构建迄今为止最大的3D航拍数据集Aerial-Earth3D，为大规模三维地球生成提供了坚实的数据基础。其核心创新在于EarthCrafter框架，特别是双稀疏潜在扩散方法，它有效地将结构和纹理生成分离，并通过压缩潜在空间显著降低了计算成本，解决了传统方法难以扩展到地理尺度的痛点。这种方法不仅提高了生成效率，还保持了地理合理性，为城市规划、地形建模等领域提供了新的可能性。"}}
{"id": "2507.17209", "title": "HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery", "authors": ["Haoran Jiang", "Shaohan Shi", "Yunjie Yao", "Chang Jiang", "Quan Li"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17209v1", "summary": "Modern scientific discovery faces growing challenges in integrating vast and\nheterogeneous knowledge critical to breakthroughs in biomedicine and drug\ndevelopment. Traditional hypothesis-driven research, though effective, is\nconstrained by human cognitive limits, the complexity of biological systems,\nand the high cost of trial-and-error experimentation. Deep learning models,\nespecially graph neural networks (GNNs), have accelerated prediction\ngeneration, but the sheer volume of outputs makes manual selection for\nvalidation unscalable. Large language models (LLMs) offer promise in filtering\nand hypothesis generation, yet suffer from hallucinations and lack grounding in\nstructured knowledge, limiting their reliability. To address these issues, we\npropose HypoChainer, a collaborative visualization framework that integrates\nhuman expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance\nhypothesis generation and validation. HypoChainer operates in three stages:\nFirst, exploration and contextualization -- experts use retrieval-augmented\nLLMs (RAGs) and dimensionality reduction to navigate large-scale GNN\npredictions, assisted by interactive explanations. Second, hypothesis chain\nformation -- experts iteratively examine KG relationships around predictions\nand semantically linked entities, refining hypotheses with LLM and KG\nsuggestions. Third, validation prioritization -- refined hypotheses are\nfiltered based on KG-supported evidence to identify high-priority candidates\nfor experimentation, with visual analytics further strengthening weak links in\nreasoning. We demonstrate HypoChainer's effectiveness through case studies in\ntwo domains and expert interviews, highlighting its potential to support\ninterpretable, scalable, and knowledge-grounded scientific discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17209v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HypoChainer：一个结合大型语言模型和知识图谱的协作系统，用于假设驱动的科学发现", "tldr": "HypoChainer是一个结合LLMs、知识图谱和人类专业知识的协作可视化框架，旨在解决传统科学发现中知识整合、预测筛选和LLM幻觉问题，通过探索、假设链形成和验证优先级排序三个阶段，支持可解释、可扩展且基于知识的科学发现。", "motivation": "现代科学发现面临整合大量异构知识的挑战，尤其是在生物医学和药物开发领域。传统的假设驱动研究受限于人类认知、生物系统复杂性和高昂的试错成本。深度学习模型（如GNNs）加速了预测生成，但手动筛选验证不可扩展。大型语言模型（LLMs）在过滤和假设生成方面有潜力，但存在幻觉且缺乏结构化知识的 grounding，限制了其可靠性。", "method": "我们提出了HypoChainer，一个结合人类专业知识、LLM驱动推理和知识图谱（KGs）的协作可视化框架，以增强假设生成和验证。HypoChainer分三个阶段操作：1. 探索和情境化：专家使用检索增强型LLMs（RAGs）和降维技术导航大规模GNN预测，并辅以交互式解释。2. 假设链形成：专家迭代检查围绕预测和语义链接实体的知识图谱关系，利用LLM和KG的建议完善假设。3. 验证优先级排序：基于KG支持的证据过滤完善的假设，以识别高优先级实验候选，并通过视觉分析进一步加强推理中的薄弱环节。", "result": "通过在两个领域的案例研究和专家访谈，我们证明了HypoChainer的有效性，突出了其支持可解释、可扩展且基于知识的科学发现的潜力。", "conclusion": "HypoChainer通过整合LLMs、知识图谱和人类专业知识，提供了一个有效的协作可视化框架，显著提升了假设生成和验证过程，克服了传统方法和单一AI模型的局限性，实现了更可靠和高效的科学发现。", "translation": "现代科学发现面临着整合对生物医学和药物开发突破至关重要的海量异构知识的日益增长的挑战。传统的假设驱动研究虽然有效，但受限于人类认知能力、生物系统的复杂性以及试错实验的高昂成本。深度学习模型，特别是图神经网络（GNNs），加速了预测的生成，但庞大的输出量使得人工选择进行验证变得不可扩展。大型语言模型（LLMs）在过滤和假设生成方面展现出前景，但它们存在幻觉且缺乏结构化知识的支撑，限制了其可靠性。为了解决这些问题，我们提出了HypoChainer，一个协作可视化框架，它整合了人类专业知识、LLM驱动的推理和知识图谱（KGs），以增强假设生成和验证。HypoChainer分三个阶段运行：首先，探索和情境化——专家使用检索增强型LLMs（RAGs）和降维技术来导航大规模GNN预测，并辅以交互式解释。其次，假设链形成——专家迭代检查围绕预测和语义链接实体的知识图谱关系，利用LLM和KG的建议完善假设。第三，验证优先级排序——基于知识图谱支持的证据过滤完善的假设，以识别高优先级实验候选，并通过视觉分析进一步加强推理中的薄弱环节。我们通过在两个领域的案例研究和专家访谈证明了HypoChainer的有效性，突出了其支持可解释、可扩展且基于知识的科学发现的潜力。", "summary": "HypoChainer是一个创新的协作可视化系统，旨在通过整合大型语言模型（LLMs）、知识图谱（KGs）和人类专业知识，解决现代科学发现中面临的知识整合、预测筛选和LLM可靠性问题。该系统分三个阶段运行：首先，利用RAGs和降维技术进行探索和情境化；其次，通过LLM和KG建议形成和完善假设链；最后，基于KG证据进行验证优先级排序。通过案例研究和专家访谈，该研究证明了HypoChainer在支持可解释、可扩展和知识驱动的科学发现方面的有效性。", "keywords": "LLMs, 知识图谱, 科学发现, 假设生成, 可视化", "comments": "HypoChainer的创新之处在于其将LLMs的生成能力与知识图谱的结构化知识和人类专家的洞察力相结合，有效地解决了LLMs的“幻觉”问题和大规模预测筛选的难题。其提出的三阶段工作流为假设驱动的科学发现提供了一个系统化、可解释且可扩展的框架，对于加速生物医学和药物开发等领域的突破具有重要意义。"}}
{"id": "2507.17690", "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary Study", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      The 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)", "url": "http://arxiv.org/abs/2507.17690v1", "summary": "A commit message describes the main code changes in a commit and plays a\ncrucial role in software maintenance. Existing commit message generation (CMG)\napproaches typically frame it as a direct mapping which inputs a code diff and\nproduces a brief descriptive sentence as output. However, we argue that relying\nsolely on the code diff is insufficient, as raw code diff fails to capture the\nfull context needed for generating high-quality and informative commit\nmessages. In this paper, we propose a contextual code retrieval-based method\ncalled C3Gen to enhance CMG by retrieving commit-relevant code snippets from\nthe repository and incorporating them into the model input to provide richer\ncontextual information at the repository scope. In the experiments, we\nevaluated the effectiveness of C3Gen across various models using four objective\nand three subjective metrics. Meanwhile, we design and conduct a human\nevaluation to investigate how C3Gen-generated commit messages are perceived by\nhuman developers. The results show that by incorporating contextual code into\nthe input, C3Gen enables models to effectively leverage additional information\nto generate more comprehensive and informative commit messages with greater\npractical value in real-world development scenarios. Further analysis\nunderscores concerns about the reliability of similaritybased metrics and\nprovides empirical insights for CMG.", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM)", "pdf_url": "http://arxiv.org/pdf/2507.17690v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于提交消息生成的上下文代码检索：一项初步研究", "tldr": "本文提出C3Gen方法，通过检索上下文代码片段来增强提交消息生成，解决了仅依赖代码差异的不足，并实验证明其能生成更全面、信息更丰富的提交消息。", "motivation": "现有的提交消息生成（CMG）方法通常将CMG视为直接映射，仅依赖代码差异。然而，原始代码差异不足以捕获生成高质量、信息丰富的提交消息所需的完整上下文。", "method": "本文提出了一种基于上下文代码检索的方法C3Gen，通过从代码库中检索提交相关的代码片段并将其整合到模型输入中，以提供更丰富的代码库范围内的上下文信息，从而增强提交消息生成。", "result": "实验结果表明，通过将上下文代码纳入输入，C3Gen使模型能够有效利用额外信息，生成更全面、信息更丰富的提交消息，在实际开发场景中具有更大的实用价值。进一步分析强调了对基于相似性指标可靠性的担忧，并为CMG提供了经验见解。", "conclusion": "通过整合上下文代码，C3Gen显著提升了提交消息生成的质量和信息量，解决了现有方法仅依赖代码差异的局限性，并为CMG领域提供了宝贵的经验见解。", "translation": "提交消息描述了提交中主要的的代码更改，并在软件维护中起着关键作用。现有的提交消息生成（CMG）方法通常将其框定为一个直接映射，即将代码差异作为输入，并产生一个简短的描述性句子作为输出。然而，我们认为仅仅依赖代码差异是不够的，因为原始代码差异未能捕获生成高质量和信息丰富的提交消息所需的完整上下文。在本文中，我们提出了一种基于上下文代码检索的方法C3Gen，通过从代码库中检索提交相关的代码片段并将其整合到模型输入中，以提供更丰富的代码库范围内的上下文信息，从而增强CMG。在实验中，我们使用四项客观和三项主观指标评估了C3Gen在各种模型中的有效性。同时，我们设计并进行了一项人工评估，以调查人类开发者如何感知C3Gen生成的提交消息。结果表明，通过将上下文代码纳入输入，C3Gen使模型能够有效利用额外信息，生成更全面、信息更丰富的提交消息，在实际开发场景中具有更大的实用价值。进一步分析强调了对基于相似性指标可靠性的担忧，并为CMG提供了经验见解。", "summary": "本文提出C3Gen，一种基于上下文代码检索的提交消息生成方法。该方法通过从代码库中检索相关代码片段并将其作为额外上下文输入模型，以弥补现有方法仅依赖代码差异的不足。实验结果表明，C3Gen能帮助模型生成更全面、信息量更大的提交消息，提高了实用价值，并对现有评估指标的可靠性提出了见解。", "keywords": "提交消息生成, 上下文代码检索, C3Gen, 代码维护, 软件工程", "comments": "本文的创新点在于引入了“上下文代码检索”的概念，以解决传统提交消息生成方法中代码差异信息不足的问题。通过从整个代码库中检索相关上下文，C3Gen能够为模型提供更丰富的语义信息，从而生成更高质量的提交消息。这项研究对于提高软件维护效率和代码可读性具有重要意义，同时也对现有评估指标的局限性提出了反思。"}}
{"id": "2507.17281", "title": "Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation", "authors": ["Huanli Zhuo", "Leilei Ma", "Haifeng Zhao", "Shiwei Zhou", "Dengdi Sun", "Yanping Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This manuscript has been accepted for presentation at the IEEE International Conference on Systems, Man, and Cybernetics (IEEE SMC 2025) and is copyrighted by IEEE", "url": "http://arxiv.org/abs/2507.17281v1", "summary": "Although SAM-based single-source domain generalization models for medical\nimage segmentation can mitigate the impact of domain shift on the model in\ncross-domain scenarios, these models still face two major challenges. First,\nthe segmentation of SAM is highly dependent on domain-specific expert-annotated\nprompts, which prevents SAM from achieving fully automated medical image\nsegmentation and therefore limits its application in clinical settings. Second,\nproviding poor prompts (such as bounding boxes that are too small or too large)\nto the SAM prompt encoder can mislead SAM into generating incorrect mask\nresults. Therefore, we propose the FA-SAM, a single-source domain\ngeneralization framework for medical image segmentation that achieves fully\nautomated SAM. FA-SAM introduces two key innovations: an Auto-prompted\nGeneration Model (AGM) branch equipped with a Shallow Feature Uncertainty\nModeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module\nintegrated into the SAM mask decoder. Specifically, AGM models the uncertainty\ndistribution of shallow features through the SUFM module to generate bounding\nbox prompts for the target domain, enabling fully automated segmentation with\nSAM. The IPEF module integrates multiscale information from SAM image\nembeddings and prompt embeddings to capture global and local details of the\ntarget object, enabling SAM to mitigate the impact of poor prompts. Extensive\nexperiments on publicly available prostate and fundus vessel datasets validate\nthe effectiveness of FA-SAM and highlight its potential to address the above\nchallenges.", "comment": "This manuscript has been accepted for presentation at the IEEE\n  International Conference on Systems, Man, and Cybernetics (IEEE SMC 2025) and\n  is copyrighted by IEEE", "pdf_url": "http://arxiv.org/pdf/2507.17281v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于医学图像分割的单源域泛化全自动SAM", "tldr": "FA-SAM是一个全自动的SAM框架，通过引入自动提示生成模型（AGM）和图像-提示嵌入融合（IPEF）模块，解决了SAM在医学图像分割中对专家标注提示的依赖以及不良提示导致分割错误的问题，实现了全自动化和鲁棒性。", "motivation": "尽管基于SAM的单源域泛化模型可以减轻跨域场景中域偏移对模型的影响，但它们仍面临两大挑战：1. SAM的分割高度依赖于特定领域的专家标注提示，限制了其在临床应用中的全自动化。2. 提供不佳的提示（如过小或过大的边界框）会误导SAM生成不正确的掩膜结果。", "method": "我们提出了FA-SAM，一个用于医学图像分割的单源域泛化框架，旨在实现全自动SAM。FA-SAM引入了两项关键创新：1. 一个配备浅层特征不确定性建模（SUFM）模块的自动提示生成模型（AGM）分支。AGM通过SUFM模块建模浅层特征的不确定性分布，为目标域生成边界框提示，从而实现SAM的全自动分割。2. 一个集成到SAM掩膜解码器中的图像-提示嵌入融合（IPEF）模块。IPEF模块整合SAM图像嵌入和提示嵌入的多尺度信息，以捕获目标对象的全局和局部细节，使SAM能够减轻不良提示的影响。", "result": "在公开可用的前列腺和眼底血管数据集上进行的广泛实验验证了FA-SAM的有效性，并突出了其解决上述挑战的潜力。", "conclusion": "FA-SAM通过自动生成提示和融合图像-提示嵌入，成功解决了SAM在医学图像分割中对专家标注提示的依赖性以及不良提示导致的分割错误问题，实现了全自动的域泛化分割，具有重要的临床应用潜力。", "translation": "尽管基于SAM的单源域泛化模型可以减轻跨域场景中域偏移对模型的影响，但这些模型仍然面临两大主要挑战。首先，SAM的分割高度依赖于领域特定的专家标注提示，这使得SAM无法实现全自动医学图像分割，从而限制了其在临床环境中的应用。其次，向SAM提示编码器提供不良提示（例如过小或过大的边界框）可能会误导SAM生成不正确的掩膜结果。因此，我们提出了FA-SAM，一个用于医学图像分割的单源域泛化框架，实现了全自动SAM。FA-SAM引入了两项关键创新：一个配备浅层特征不确定性建模（SUFM）模块的自动提示生成模型（AGM）分支，以及一个集成到SAM掩膜解码器中的图像-提示嵌入融合（IPEF）模块。具体而言，AGM通过SUFM模块对浅层特征的不确定性分布进行建模，为目标域生成边界框提示，从而实现SAM的全自动分割。IPEF模块整合了SAM图像嵌入和提示嵌入的多尺度信息，以捕获目标对象的全局和局部细节，使SAM能够减轻不良提示的影响。在公开可用的前列腺和眼底血管数据集上进行的广泛实验验证了FA-SAM的有效性，并突出了其解决上述挑战的潜力。", "summary": "本研究提出了FA-SAM，一个针对医学图像分割的单源域泛化框架，旨在实现全自动的SAM应用。该框架解决了SAM在医学图像分割中对专家标注提示的依赖以及不良提示导致分割精度下降的问题。FA-SAM通过引入自动提示生成模型（AGM）及其浅层特征不确定性建模（SUFM）模块，实现了目标域的自动边界框提示生成。同时，图像-提示嵌入融合（IPEF）模块被集成到SAM掩膜解码器中，以有效整合多尺度信息，减轻不良提示的影响。实验证明FA-SAM在公共数据集上表现出有效性。", "keywords": "SAM, 域泛化, 医学图像分割, 自动化, 提示工程", "comments": "FA-SAM的创新之处在于其通过AGM模块实现了SAM提示的自动化生成，摆脱了对人工标注的依赖，极大地提升了SAM在医学图像分割中的实用性。同时，IPEF模块增强了模型对不良提示的鲁棒性，进一步提高了分割的准确性。这对于推动SAM在临床医学图像全自动化分割中的应用具有重要意义。"}}
{"id": "2507.16242", "title": "Toward a Lightweight and Robust Design for Caching", "authors": ["Peng Chen", "Hailiang Zhao", "Jiaji Zhang", "Xueyan Tang", "Yixuan Wang", "Shuiguang Deng"], "categories": ["cs.DS", "cs.LG"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2507.16242v2", "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2507.16242v2", "cate": "cs.DS", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "迈向轻量级和鲁棒的缓存设计", "tldr": "提出Guard框架，为学习增强型缓存算法提供轻量级且鲁棒的增强，同时保持一致性并降低开销。", "motivation": "在线缓存问题中，朴素的学习增强型缓存算法缺乏鲁棒性，而现有鲁棒化方法要么牺牲1-一致性，要么引入显著的计算开销。", "method": "引入Guard，一个轻量级鲁棒化框架，用于增强广泛的学习增强型缓存算法的鲁棒性。", "result": "Guard将鲁棒性提升到$2H_k + 2$，同时保持$1$-一致性，并仅增加$O(1)$的额外每请求开销，从而保持原有时间复杂度。在多个真实世界数据集和预测模型上的广泛实验验证了Guard在实践中的有效性。", "conclusion": "Guard在一致性和鲁棒性之间实现了当前已知最佳的权衡，并在实践中被证明有效。", "translation": "在线缓存问题旨在有限的缓存大小下，在处理请求序列时最小化缓存未命中。虽然朴素的学习增强型缓存算法实现了理想的1-一致性，但它们缺乏鲁棒性保证。现有的鲁棒化方法要么牺牲1-一致性，要么引入显著的计算开销。在本文中，我们引入了Guard，一个轻量级鲁棒化框架，它将广泛的学习增强型缓存算法的鲁棒性增强到$2H_k + 2$，同时保持其1-一致性。Guard在一致性和鲁棒性之间实现了当前已知最佳的权衡，每请求仅增加$O(1)$的额外开销，从而保持了基础算法的原始时间复杂度。在多个真实世界数据集和预测模型上的广泛实验验证了Guard在实践中的有效性。", "summary": "本文针对在线缓存问题中学习增强型算法缺乏鲁棒性和现有鲁棒化方法效率低下的问题，提出了一个名为Guard的轻量级鲁棒化框架。Guard能够显著提升学习增强型缓存算法的鲁棒性至$2H_k + 2$，同时保持其理想的$1$-一致性，并且仅引入极低的$O(1)$额外计算开销。实验结果表明，Guard在实际应用中表现出色，实现了当前一致性和鲁棒性之间的最优权衡。", "keywords": "缓存, 鲁棒性, 学习增强, 在线算法, Guard", "comments": "这篇论文提出了一种创新的、轻量级的鲁棒化框架Guard，有效解决了学习增强型缓存算法在鲁棒性方面的缺陷，同时避免了传统方法中牺牲一致性或引入高开销的问题。其在保持$1$-一致性的同时，实现了当前最佳的鲁棒性与一致性权衡，并且仅有$O(1)$的额外开销，这对于实际系统部署具有重要意义。"}}
{"id": "2408.14672", "title": "Optimizing against Infeasible Inclusions from Data for Semantic Segmentation through Morphology", "authors": ["Shamik Basu", "Luc Van Gool", "Christos Sakaridis"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.14672v4", "summary": "State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel or per-segment classification\nobjectives on their training data. This purely data-driven paradigm often leads\nto absurd segmentations, especially when the domain of input images is shifted\nfrom the one encountered during training. For instance, state-of-the-art models\nmay assign the label \"road\" to a segment that is included by another segment\nthat is respectively labeled as \"sky\". However, the ground truth of the\nexisting dataset at hand dictates that such inclusion is not feasible. Our\nmethod, Infeasible Semantic Inclusions (InSeIn), first extracts explicit\ninclusion constraints that govern spatial class relations from the semantic\nsegmentation training set at hand in an offline, data-driven fashion, and then\nenforces a morphological yet differentiable loss that penalizes violations of\nthese constraints during training to promote prediction feasibility. InSeIn is\na light-weight plug-and-play method, constitutes a novel step towards\nminimizing infeasible semantic inclusions in the predictions of learned\nsegmentation models, and yields consistent and significant performance\nimprovements over diverse state-of-the-art networks across the ADE20K,\nCityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn/tree/main", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.14672v4", "cate": "cs.CV", "date": "2024-08-26", "updated": "2025-07-23", "AI": {"title_translation": "通过形态学优化语义分割中数据导致的不可行包含问题", "tldr": "本文提出InSeIn方法，通过提取空间类别包含约束并引入可微形态学损失，解决了语义分割模型在域偏移下产生不合理分割的问题，显著提升了分割性能。", "motivation": "现有的语义分割模型通常纯粹以数据驱动的方式优化，仅最小化像素级或片段级分类目标。这种范式常导致不合理的分割结果，尤其是在输入图像域发生偏移时，例如将“道路”标签分配给被“天空”包含的区域，这与现有数据集的真实情况不符。", "method": "本文提出的Infeasible Semantic Inclusions (InSeIn) 方法，首先离线地、数据驱动地从语义分割训练集中提取管理空间类别关系的显式包含约束，然后引入一个形态学且可微的损失函数，在训练过程中惩罚违反这些约束的行为，以提高预测的可行性。InSeIn是一种轻量级的即插即用方法。", "result": "InSeIn方法在ADE20K、Cityscapes和ACDC数据集上，对各种先进网络均产生了持续且显著的性能改进。", "conclusion": "InSeIn方法是朝着最小化学习分割模型预测中不可行语义包含迈出的新颖一步，能够有效提升分割预测的合理性。", "translation": "最先进的语义分割模型通常以数据驱动的方式进行优化，仅最小化训练数据上的逐像素或逐片段分类目标。这种纯粹的数据驱动范式常常导致荒谬的分割结果，特别是当输入图像的域与训练时遇到的域发生偏移时。例如，最先进的模型可能会将“道路”标签分配给被另一个标签为“天空”的片段所包含的片段。然而，现有数据集的真实情况表明这种包含是不可行的。我们的方法，即不可行语义包含（InSeIn），首先以离线、数据驱动的方式从现有语义分割训练集中提取管理空间类别关系的显式包含约束，然后强制执行一种形态学但可微的损失，在训练期间惩罚这些约束的违规行为，以促进预测的可行性。InSeIn是一种轻量级的即插即用方法，是朝着最小化学习分割模型预测中不可行语义包含的新一步，并在ADE20K、Cityscapes和ACDC数据集上，对各种最先进的网络均产生了持续且显著的性能改进。https://github.com/SHAMIK-97/InSeIn/tree/main", "summary": "本文提出了一种名为InSeIn（Infeasible Semantic Inclusions）的新方法，旨在解决语义分割模型在域偏移下生成不合理分割结果的问题。该方法首先离线地从训练数据中提取空间类别包含约束，然后通过引入一个可微的形态学损失函数，在训练过程中惩罚违反这些约束的行为，从而强制模型生成更符合常识的预测。InSeIn是一种轻量级且即插即用的方案，在多个主流数据集上显著提升了现有最先进语义分割模型的性能。", "keywords": "语义分割, 不可行包含, 形态学, 约束, InSeIn", "comments": "该论文的创新点在于引入了明确的形态学约束来解决语义分割中“不可行包含”的问题，即模型预测出不符合现实空间关系的分割结果。这种方法通过将领域知识（空间包含约束）融入到数据驱动的训练范式中，有效地提高了模型的鲁棒性和预测的合理性，尤其是在面对领域偏移时。其“即插即用”的特性使其具有较高的实用价值和普适性。该工作为提升语义分割模型的可靠性提供了一个有价值的视角。"}}
{"id": "2505.20863", "title": "Leveraging Diffusion Models for Parameterized Quantum Circuit Generation", "authors": ["Daniel Barta", "Darya Martyniuk", "Johannes Jung", "Adrian Paschke"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      This work has been accepted for presentation at IEEE Quantum Week 2025: IEEE International Conference on Quantum Computing and Engineering (QCE)", "url": "http://arxiv.org/abs/2505.20863v3", "summary": "Quantum computing holds immense potential, yet its practical success depends\non multiple factors, including advances in quantum circuit design. In this\npaper, we introduce a generative approach based on denoising diffusion models\n(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent\ndiffusion model pipeline of F\\\"urrutter et al. [1], our model effectively\nconditions the synthesis process, enabling the simultaneous generation of\ncircuit architectures and their continuous gate parameters. We demonstrate our\napproach in synthesizing PQCs optimized for generating high-fidelity\nGreenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum\nmachine learning (QML) classification tasks. Our results indicate a strong\ngeneralization across varying gate sets and scaling qubit counts, highlighting\nthe versatility and computational efficiency of diffusion-based methods. This\nwork illustrates the potential of generative models as a powerful tool for\naccelerating and optimizing the design of PQCs, supporting the development of\nmore practical and scalable quantum applications.", "comment": "This work has been accepted for presentation at IEEE Quantum Week\n  2025: IEEE International Conference on Quantum Computing and Engineering\n  (QCE)", "pdf_url": "http://arxiv.org/pdf/2505.20863v3", "cate": "quant-ph", "date": "2025-05-27", "updated": "2025-07-23", "AI": {"title_translation": "利用扩散模型生成参数化量子电路", "tldr": "本文引入了一种基于去噪扩散模型（DMs）的生成方法来合成参数化量子电路（PQCs），能够同时生成电路架构和门参数。实验证明其在生成高保真GHZ态和量子机器学习分类任务中表现良好，并具有强大的泛化能力和计算效率。", "motivation": "量子计算的实际成功依赖于包括量子电路设计在内的多方面进步。", "method": "本文提出了一种基于去噪扩散模型（DMs）的生成方法来合成参数化量子电路（PQCs）。该方法扩展了F\"urrutter等人的扩散模型流程，能够有效地调节合成过程，同时生成电路架构及其连续的门参数。", "result": "该方法在合成优化用于生成高保真Greenberger-Horne-Zeilinger (GHZ) 态的PQC以及在量子机器学习 (QML) 分类任务中实现高精度方面得到了验证。结果表明，该方法在不同的门集和扩展的量子比特数量上都具有很强的泛化能力，突出了基于扩散方法的通用性和计算效率。", "conclusion": "这项工作表明，生成模型作为加速和优化PQC设计、支持更实用和可扩展量子应用开发的强大工具的潜力。", "translation": "量子计算具有巨大的潜力，但其实际成功取决于多种因素，包括量子电路设计的进步。在本文中，我们引入了一种基于去噪扩散模型（DMs）的生成方法来合成参数化量子电路（PQCs）。我们的模型扩展了F\"urrutter等人[1]最近的扩散模型流程，有效地调节了合成过程，实现了电路架构及其连续门参数的同时生成。我们展示了我们的方法在合成优化用于生成高保真Greenberger-Horne-Zeilinger（GHZ）态的PQC以及在量子机器学习（QML）分类任务中实现高精度方面的应用。我们的结果表明，该方法在不同的门集和扩展的量子比特数量上都具有很强的泛化能力，突出了基于扩散方法的通用性和计算效率。这项工作说明了生成模型作为加速和优化PQC设计的强大工具的潜力，支持了更实用和可扩展量子应用的开发。", "summary": "本文提出了一种利用去噪扩散模型（DMs）生成参数化量子电路（PQCs）的新方法。该模型能够同时合成电路架构和其连续的门参数，并通过生成高保真GHZ态和在量子机器学习分类任务中实现高精度来验证其有效性。研究结果表明，该方法在不同门集和量子比特数量扩展方面均表现出强大的泛化能力、通用性和计算效率，预示着生成模型在量子电路设计优化中的巨大潜力。", "keywords": "扩散模型, 参数化量子电路, 量子计算, 量子机器学习, 生成模型", "comments": "该论文的创新之处在于将扩散模型引入参数化量子电路的生成，实现了电路架构和连续门参数的同步合成。其重要性体现在为量子电路设计提供了一种高效、通用的自动化工具，有望加速量子应用的开发。其优势在于展现了良好的泛化能力和计算效率。"}}
{"id": "2507.16849", "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery", "authors": ["Yi-Shan Chu", "Hsuan-Cheng Wei"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16849v1", "summary": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16849v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "基于Vision Transformer (ViT) 的EVAP模型利用Sentinel-2和Formosat-5影像进行灾后受灾区域分割", "tldr": "该研究提出了一个基于Vision Transformer (ViT) 的深度学习框架，用于利用遥感影像改进灾后受灾区域分割，以支持和增强台湾太空中心 (TASA) 开发的应急增值产品 (EVAP)。该框架通过弱监督训练和多阶段损失策略，提高了分割结果的平滑度和可靠性，尤其适用于缺少准确地面真值的情况。", "motivation": "该研究旨在改进遥感影像中的灾后受灾区域分割，以支持和增强台湾太空中心 (TASA) 开发的应急增值产品 (EVAP)。现有方法可能在缺乏准确地面真值时面临挑战。", "method": "研究提出了一个基于Vision Transformer (ViT) 的深度学习框架。首先，利用少量手动标注区域，通过主成分分析 (PCA) 的特征空间分析和置信度指数 (CI) 构建弱监督训练集。然后，使用这些扩展的标签训练基于ViT的编码器-解码器模型，输入为Sentinel-2和Formosat-5的多波段影像。该架构支持多种解码器变体和多阶段损失策略，以在有限监督下提高性能。", "result": "在对2022年鄱阳湖干旱和2023年罗德岛野火的案例研究中，该框架改善了分割结果的平滑度和可靠性。模型预测与更高分辨率的EVAP输出进行比较，以评估空间一致性和分割连贯性。", "conclusion": "该框架为灾害测绘提供了一种可扩展的方法，尤其是在无法获得准确地面真值的情况下，能够提高灾后受灾区域分割的平滑度和可靠性。", "translation": "我们提出了一个基于Vision Transformer (ViT) 的深度学习框架，旨在从遥感影像中细化灾后受灾区域分割，以支持和增强台湾太空中心 (TASA) 开发的应急增值产品 (EVAP)。该过程始于一小部分手动标注区域。然后，我们应用基于主成分分析 (PCA) 的特征空间分析并构建置信度指数 (CI) 来扩展这些标签，生成一个弱监督训练集。这些扩展的标签随后用于训练基于ViT的编码器-解码器模型，输入来自Sentinel-2和Formosat-5影像的多波段数据。我们的架构支持多种解码器变体和多阶段损失策略，以在有限监督下提高性能。在评估期间，模型预测与更高分辨率的EVAP输出进行比较，以评估空间一致性和分割一致性。对2022年鄱阳湖干旱和2023年罗德岛野火的案例研究表明，我们的框架提高了分割结果的平滑度和可靠性，为在缺乏准确地面真值的情况下进行灾害测绘提供了一种可扩展的方法。", "summary": "该论文提出了一种基于Vision Transformer (ViT) 的深度学习框架，用于改进遥感影像中的灾后受灾区域分割，旨在支持台湾太空中心 (TASA) 的EVAP产品。该方法利用少量手动标注数据，结合PCA和置信度指数生成弱监督训练集，并使用Sentinel-2和Formosat-5多波段影像训练ViT编码器-解码器模型。通过多阶段损失策略，该框架在鄱阳湖干旱和罗德岛野火案例中展示了对分割结果平滑度和可靠性的提升，提供了一种在地面真值稀缺时可扩展的灾害测绘方案。", "keywords": "灾后分割, Vision Transformer, 弱监督学习, 遥感影像, EVAP", "comments": "该论文的创新点在于结合了Vision Transformer和弱监督学习，以解决灾害遥感中地面真值数据稀缺的问题。通过利用PCA和置信度指数生成训练数据，以及多阶段损失策略，有效地提高了分割结果的质量和可靠性。这对于快速响应和评估灾情具有重要意义，尤其是在紧急情况下，数据标注往往是瓶颈。"}}
{"id": "2506.13343", "title": "TwiUSD: A Benchmark Dataset and Structure-Aware LLM Framework for User Stance Detection", "authors": ["Fuqiang Niu", "Zini Chen", "Zhiyu Xie", "Hu Huang", "Genan Dai", "Bowen Zhang"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.13343v2", "summary": "User-level stance detection (UserSD) remains challenging due to the lack of\nhigh-quality benchmarks that jointly capture linguistic and social structure.\nIn this paper, we introduce TwiUSD, the first large-scale, manually annotated\nUserSD benchmark with explicit followee relationships, containing 16,211 users\nand 47,757 tweets. TwiUSD enables rigorous evaluation of stance models by\nintegrating tweet content and social links, with superior scale and annotation\nquality. Building on this resource, we propose MRFG: a structure-aware\nframework that uses LLM-based relevance filtering and feature routing to\naddress noise and context heterogeneity. MRFG employs multi-scale filtering and\nadaptively routes features through graph neural networks or multi-layer\nperceptrons based on topological informativeness. Experiments show MRFG\nconsistently outperforms strong baselines (including PLMs, graph-based models,\nand LLM prompting) in both in-target and cross-target evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.13343v2", "cate": "cs.SI", "date": "2025-06-16", "updated": "2025-07-23", "AI": {"title_translation": "TwiUSD：一个用于用户立场检测的基准数据集和结构感知LLM框架", "tldr": "本文提出了TwiUSD数据集和MRFG框架，用于解决用户级立场检测中高质量基准数据集缺失和模型性能不足的问题，并在实验中取得了优异表现。", "motivation": "用户级立场检测（UserSD）由于缺乏高质量的、同时捕获语言和社会结构的基准数据集而仍然具有挑战性。", "method": "本文引入了TwiUSD，一个大规模、人工标注的用户级立场检测基准数据集，包含16,211名用户和47,757条推文，并明确了关注者关系。在此基础上，提出了MRFG框架，一个结构感知框架，利用基于LLM的相关性过滤和特征路由来解决噪声和上下文异质性。MRFG采用多尺度过滤，并根据拓扑信息自适应地通过图神经网络或多层感知器路由特征。", "result": "实验表明，MRFG在目标内和跨目标评估中始终优于强大的基线模型（包括PLM、基于图的模型和LLM提示）。", "conclusion": "TwiUSD数据集的发布为用户级立场检测提供了急需的高质量基准，而MRFG框架则通过其结构感知和LLM驱动的机制，显著提升了该任务的性能，为未来的研究奠定了坚实基础。", "translation": "用户级立场检测（UserSD）由于缺乏同时捕获语言和社会结构的高质量基准数据集而仍然具有挑战性。在本文中，我们引入了TwiUSD，这是第一个大规模、人工标注的用户级立场检测基准数据集，包含明确的关注者关系，共有16,211名用户和47,757条推文。TwiUSD通过整合推文内容和社交链接，以卓越的规模和标注质量，实现了对立场模型的严格评估。在此资源的基础上，我们提出了MRFG：一个结构感知框架，它利用基于大型语言模型（LLM）的相关性过滤和特征路由来解决噪声和上下文异质性。MRFG采用多尺度过滤，并根据拓扑信息通过图神经网络或多层感知器自适应地路由特征。实验表明，MRFG在目标内和跨目标评估中始终优于强大的基线模型（包括预训练语言模型、基于图的模型和LLM提示）。", "summary": "本文针对用户级立场检测中高质量基准数据集的缺失，推出了TwiUSD数据集，该数据集结合了语言内容和社交结构，规模大且标注精良。在此基础上，作者提出了MRFG框架，该框架利用LLM进行相关性过滤和特征路由，以应对数据噪声和异质性。实验证明，MRFG在各项评估中均显著优于现有主流基线方法。", "keywords": "用户立场检测, TwiUSD, MRFG, 基准数据集, 结构感知框架", "comments": "创新点在于首次构建了大规模、包含明确社交关系的用户立场检测基准数据集TwiUSD，填补了该领域高质量数据的空白。同时，提出的MRFG框架结合了LLM进行相关性过滤和结构感知特征路由，有效处理了社交数据中的噪声和异质性，是一种新颖且高效的方法。这项工作为用户级立场检测提供了重要的资源和高性能模型。"}}
{"id": "2507.17155", "title": "Multi-Angle Rotational Actuation in a 0.8-mm-Thick Preload-Free Piezoelectric Micromotor", "authors": ["Haijia Yu", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SY", "cs.SY", "physics.med-ph"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17155v1", "summary": "Micro motors can be used in numerous fields like Micro medical testing and\ntreatment. To achieve a smaller size, micro piezoelectric motors in\nlaboratories often omit the outer casing, which can lead to functional defects\nsuch as rotation only in one fixed direction or the need for external weights\n(which are not counted within the motors volume) to increase preload. However,\nthis significantly reduces the practical value of micro piezoelectric motors.\nThis paper proposes a new driving principle for piezoelectric motors to design\na micro piezoelectric motor that can rotate at a wide range of angles (e.g. up\nto 80)without increasing the motors casing and does not require external\nweights, with a stator thickness of only 0.8 mm. This motor has significant\napplication potential in OCT endoscopes and thrombectomy grinding heads", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17155v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "0.8毫米厚无预载压电微电机中的多角度旋转驱动", "tldr": "本文提出了一种新型驱动原理，设计了一种0.8毫米厚、无需预载且能实现多角度旋转（例如高达80度）的压电微电机，解决了现有微电机功能缺陷和实用性低的问题。", "motivation": "为了实现更小的尺寸，实验室中的微压电电机通常省略外壳，但这导致了功能缺陷，如只能单向固定旋转或需要外部配重来增加预载（这些配重不计入电机体积），这大大降低了微压电电机的实用价值。", "method": "本文提出了一种新的压电电机驱动原理，旨在设计一种无需增加电机外壳且不需要外部配重的微压电电机，该电机能够实现大范围角度（例如高达80度）的旋转。", "result": "该论文设计并实现了一种定子厚度仅为0.8毫米的微压电电机，该电机无需外部配重，且能在不增加电机外壳的情况下实现大范围角度（例如高达80度）的旋转。", "conclusion": "该电机在OCT内窥镜和取栓磨头等领域具有重要的应用潜力。", "translation": "微电机可用于微型医疗检测和治疗等众多领域。为了实现更小的尺寸，实验室中的微压电电机通常省略外壳，但这可能导致功能缺陷，例如只能在一个固定方向旋转，或者需要外部配重（不计入电机体积）来增加预载。然而，这显著降低了微压电电机的实用价值。本文提出了一种新的压电电机驱动原理，旨在设计一种无需增加电机外壳且不需要外部配重的微压电电机，该电机能够实现大范围角度（例如高达80度）的旋转，且定子厚度仅为0.8毫米。该电机在OCT内窥镜和取栓磨头等领域具有重要的应用潜力。", "summary": "本文针对现有微压电电机在小型化过程中出现的单向旋转、需要外部预载配重等功能缺陷，提出了一种新型驱动原理。研究成功设计并实现了一种定子厚度仅0.8毫米的微压电电机，该电机无需外部配重，且能在保持紧凑体积的同时实现高达80度的多角度旋转。该创新电机在医疗器械如OCT内窥镜和取栓磨头等领域展现出巨大的应用前景。", "keywords": "压电微电机, 多角度驱动, 无预载, 0.8毫米厚度", "comments": "该论文的创新点在于提出了新的驱动原理，解决了微型压电电机在无外壳或薄型化设计中常见的单向旋转和预载依赖问题。通过实现0.8mm的超薄、无预载和多角度旋转能力，极大地提升了微电机的实用价值和集成度，特别是在微创医疗器械领域具有重要意义。"}}
{"id": "2507.16951", "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16951v1", "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16951v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "利用RLHF实现LLM中鲁棒的不可回答性识别和可信响应生成", "tldr": "本文提出SALU，一种将不可回答性检测深度集成到LLM生成过程中的方法，通过多任务学习和RLHF训练，显著提高了LLM在回答或拒绝回答问题时的准确性和可靠性，并大幅减少了幻觉。", "motivation": "对话式信息检索（CIR）系统面临一个重大挑战：如何可靠地处理不可回答的问题，以防止生成误导性或幻觉内容。传统方法依赖外部分类器，这可能与核心生成式大型语言模型（LLM）产生不一致。", "method": "本文提出了一种名为“自我感知LLM不可回答性”（SALU）的新方法，将不可回答性检测直接深度集成到LLM的生成过程中。SALU通过多任务学习框架进行训练，既处理标准问答（QA），也为不可回答的查询明确生成弃权。它特别融入了一个置信度分数引导的强化学习与人类反馈（RLHF）阶段，该阶段明确惩罚幻觉响应并奖励适当的弃权，从而培养对知识边界的内在自我感知。", "result": "通过在我们定制构建的C-IR_Answerability数据集上进行的大量实验，SALU在正确回答或弃权问题方面的整体准确性上始终优于包括混合LLM-分类器系统在内的强大基线。人类评估进一步证实了SALU卓越的可靠性，在事实性、适当弃权方面取得了高分，最重要的是，幻觉现象大幅减少。", "conclusion": "SALU展示了其在LLM中实现鲁棒的“知道何时说‘我不知道’”的能力，显著提高了其可靠性，并大幅减少了幻觉。", "translation": "对话式信息检索（CIR）系统虽然提供了直观的信息访问方式，但面临一个重大挑战：如何可靠地处理不可回答的问题，以防止生成误导性或幻觉内容。传统方法通常依赖外部分类器，这可能与核心生成式大型语言模型（LLM）产生不一致。本文介绍了一种名为“自我感知LLM不可回答性”（SALU）的新方法，它将不可回答性检测直接深度集成到LLM的生成过程中。SALU通过多任务学习框架进行训练，既用于标准问答（QA），也用于为不可回答的查询明确生成弃权。关键是，它结合了一个置信度分数引导的强化学习与人类反馈（RLHF）阶段，该阶段明确惩罚幻觉响应并奖励适当的弃权，从而培养对知识边界的内在自我感知。通过在我们定制构建的C-IR_Answerability数据集上进行的大量实验，SALU在正确回答或弃权问题方面的整体准确性上始终优于包括混合LLM-分类器系统在内的强大基线。人类评估进一步证实了SALU卓越的可靠性，在事实性、适当弃权方面取得了高分，最重要的是，幻觉现象大幅减少，展示了其鲁棒地“知道何时说‘我不知道’”的能力。", "summary": "本文提出了一种名为“自我感知LLM不可回答性”（SALU）的新方法，旨在解决大型语言模型（LLMs）在对话式信息检索（CIR）系统中处理不可回答问题时产生误导或幻觉内容的挑战。与传统依赖外部分类器的方法不同，SALU将不可回答性检测深度集成到LLM的生成过程中。它采用多任务学习框架进行问答和明确的弃权生成，并结合了置信度分数引导的强化学习与人类反馈（RLHF），以惩罚幻觉并奖励适当的弃权，从而培养LLM的内在知识边界感知。实验证明，SALU在准确性、可靠性和减少幻觉方面均优于现有基线。", "keywords": "LLM, RLHF, 不可回答性识别, 幻觉, 可信度", "comments": "本文的创新之处在于将不可回答性检测深度集成到LLM的生成过程中，并巧妙地利用了RLHF来明确惩罚幻觉和奖励弃权，从而增强了LLM的“自我感知”能力。这对于提升LLM在实际应用中的可信度和鲁棒性具有重要意义，尤其是在需要高准确性和低幻觉率的场景。其方法有望成为未来构建更可靠LLM的基石。"}}
{"id": "2502.15910", "title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models", "authors": ["Zheyuan Liu", "Guangyao Dou", "Xiangchi Yuan", "Chunhui Zhang", "Zhaoxuan Tan", "Meng Jiang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Main Conference", "url": "http://arxiv.org/abs/2502.15910v3", "summary": "Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility.", "comment": "ACL 2025 Main Conference", "pdf_url": "http://arxiv.org/pdf/2502.15910v3", "cate": "cs.CL", "date": "2025-02-21", "updated": "2025-07-23", "AI": {"title_translation": "多模态大语言模型中用于遗忘的模态感知神经元剪枝", "tldr": "MANU是一种针对多模态大语言模型(MLLMs)的遗忘框架，通过模态感知神经元剪枝，实现对敏感信息的选择性遗忘，同时保持模型性能。", "motivation": "现有大语言模型（LLMs）和多模态大语言模型（MLLMs）在海量数据集上训练时可能记忆并无意中泄露敏感信息，引发伦理和隐私问题。对于MLLMs而言，由于知识在模态间的纠缠性，全面遗忘更具挑战性。", "method": "本文提出了模态感知神经元遗忘（MANU）框架，旨在解决MLLMs中的遗忘挑战。MANU包括两个阶段：重要神经元选择和选择性剪枝。第一阶段识别并收集与目标遗忘知识相关的跨模态最具影响力的神经元，第二阶段则剪枝这些选定的神经元。", "result": "实验表明，MANU能够在不显著影响模型整体效用的情况下，在每个模态中实现更平衡和全面的遗忘。", "conclusion": "MANU框架能够有效地隔离并移除多模态大语言模型中对遗忘数据贡献最大的神经元，同时保留现有知识的完整性，从而在不大幅影响模型整体效用的前提下，实现更平衡和全面的跨模态遗忘。", "translation": "大型语言模型（LLMs）和多模态大型语言模型（MLLMs）等生成模型在海量数据集上训练时，可能导致它们记忆并无意中泄露敏感信息，从而引发伦理和隐私问题。虽然此前的一些工作已经在LLMs的背景下探讨了这个问题，但对于MLLMs来说，由于知识在模态间纠缠的特性，这带来了独特的挑战，使得全面遗忘更加困难。为了应对这一挑战，我们提出了模态感知神经元遗忘（MANU），这是一种新颖的MLLMs遗忘框架，旨在根据神经元对目标遗忘数据（针对不同模态进行整理）的相对重要性进行选择性剪枝。具体来说，MANU包括两个阶段：重要神经元选择和选择性剪枝。第一阶段识别并收集与目标遗忘知识相关的跨模态最具影响力的神经元，而第二阶段则专门用于剪枝这些选定的神经元。MANU有效地隔离并移除了每个模态中对遗忘数据贡献最大的神经元，同时保留了现有知识的完整性。我们在各种MLLM架构上进行的实验表明，MANU可以在不大幅影响模型整体效用的情况下，在每个模态中实现更平衡和全面的遗忘。", "summary": "本文提出了一种名为MANU（模态感知神经元遗忘）的新型框架，旨在解决多模态大语言模型（MLLMs）中敏感信息遗忘的挑战。由于MLLMs中知识的模态间纠缠，遗忘变得复杂。MANU通过两个阶段工作：首先识别与目标遗忘数据最相关的跨模态重要神经元，然后对这些神经元进行选择性剪枝。实验证明，MANU能够有效地在每个模态中实现更平衡和全面的遗忘，同时不显著影响模型的整体效用，从而在移除敏感信息的同时保持模型性能和现有知识的完整性。", "keywords": "多模态大语言模型, 神经元剪枝, 模型遗忘, 隐私保护, 模态感知", "comments": "该论文提出了一种新颖的模态感知神经元剪枝方法，解决了多模态大语言模型中遗忘的独特挑战。其创新性在于考虑了多模态知识的纠缠性，并通过选择性剪枝来隔离和移除与敏感信息相关的神经元，同时保持模型其他知识的完整性，这对于大模型的隐私保护具有重要意义。"}}
{"id": "2507.15401", "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15401v2", "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15401v2", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "重新思考FER中的遮挡问题：一种语义感知视角及超越", "tldr": "ORSANet通过引入多模态语义引导、多尺度交叉交互模块和动态对抗排斥增强损失，显著提升了面部表情识别在遮挡条件下的性能，并构建了首个遮挡导向的FER数据集。", "motivation": "由于普遍存在的遮挡和数据集偏差，面部表情识别（FER）是一项具有挑战性的任务。特别是当面部信息部分被遮挡时，现有FER模型难以提取有效的面部特征，导致分类不准确。", "method": "本文提出了ORSANet模型，包含三个主要贡献：1）引入辅助多模态语义引导来消除面部遮挡的歧义并学习高级语义知识，包括语义分割图（作为密集语义先验生成语义增强的面部表示）和面部地标（作为稀疏几何先验以减轻身份和性别偏差等固有噪声）。2）设计了多尺度交叉交互模块（MCM），以有效地融合不同尺度下的地标特征和语义增强表示。3）设计了动态对抗排斥增强损失（DARELoss），动态调整模糊类别的裕度，进一步增强模型区分相似表情的能力。此外，作者构建了首个遮挡导向的FER数据集Occlu-FER，用于专门分析各种真实世界遮挡条件下的鲁棒性。", "result": "在公共基准测试和Occlu-FER数据集上的大量实验表明，所提出的ORSANet实现了最先进（SOTA）的识别性能。", "conclusion": "ORSANet通过创新的多模态语义融合和损失设计，有效解决了面部表情识别在遮挡情况下的挑战，并达到了领先的识别性能，为未来的研究提供了新的数据集。", "translation": "面部表情识别（FER）由于普遍存在的遮挡和数据集偏差而成为一项具有挑战性的任务。特别是当面部信息部分被遮挡时，现有的FER模型难以提取有效的面部特征，导致分类不准确。为此，我们提出了ORSANet，它引入了以下三个关键贡献：首先，我们引入辅助多模态语义引导来消除面部遮挡的歧义并学习高级语义知识，这包括两个方面：1）我们引入语义分割图作为密集语义先验来生成语义增强的面部表示；2）我们引入面部地标作为稀疏几何先验来减轻FER中的固有噪声，例如身份和性别偏差。其次，为了促进这两种多模态先验的有效结合，我们定制了一个多尺度交叉交互模块（MCM），以自适应地融合不同尺度下的地标特征和语义增强表示。第三，我们设计了一种动态对抗排斥增强损失（DARELoss），该损失动态调整模糊类别的裕度，进一步增强模型区分相似表情的能力。我们进一步构建了第一个面向遮挡的FER数据集，以促进对各种真实世界遮挡条件的专门鲁棒性分析，该数据集名为Occlu-FER。在公共基准测试和Occlu-FER上的大量实验表明，我们提出的ORSANet实现了SOTA识别性能。代码已在https://github.com/Wenyuzhy/ORSANet-master上公开。", "summary": "本文针对面部表情识别（FER）中普遍存在的遮挡问题，提出了ORSANet模型。该模型通过引入多模态语义引导（结合语义分割图和面部地标）来增强特征表示，设计了多尺度交叉交互模块（MCM）以有效融合多模态信息，并提出了动态对抗排斥增强损失（DARELoss）以提高相似表情的区分能力。此外，研究团队还构建了首个遮挡导向的FER数据集Occlu-FER。实验结果表明，ORSANet在公共基准和Occlu-FER数据集上均达到了最先进的识别性能。", "keywords": "面部表情识别, 遮挡, 语义感知, 多模态融合, Occlu-FER", "comments": "该论文在解决遮挡条件下的面部表情识别问题上具有显著创新性。通过引入多模态语义信息（语义分割和地标）来增强特征表示，并设计专门的模块和损失函数进行有效融合和优化，提升了模型的鲁棒性。此外，构建首个遮挡导向的FER数据集Occlu-FER，为该领域的未来研究提供了宝贵的资源和评估标准，体现了其重要性。"}}
{"id": "2507.17580", "title": "Enhancing Quantum Federated Learning with Fisher Information-Based Optimization", "authors": ["Amandeep Singh Bhatia", "Sabre Kais"], "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.ET", "quant-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17580v1", "summary": "Federated Learning (FL) has become increasingly popular across different\nsectors, offering a way for clients to work together to train a global model\nwithout sharing sensitive data. It involves multiple rounds of communication\nbetween the global model and participating clients, which introduces several\nchallenges like high communication costs, heterogeneous client data, prolonged\nprocessing times, and increased vulnerability to privacy threats. In recent\nyears, the convergence of federated learning and parameterized quantum circuits\nhas sparked significant research interest, with promising implications for\nfields such as healthcare and finance. By enabling decentralized training of\nquantum models, it allows clients or institutions to collaboratively enhance\nmodel performance and outcomes while preserving data privacy. Recognizing that\nFisher information can quantify the amount of information that a quantum state\ncarries under parameter changes, thereby providing insight into its geometric\nand statistical properties. We intend to leverage this property to address the\naforementioned challenges. In this work, we propose a Quantum Federated\nLearning (QFL) algorithm that makes use of the Fisher information computed on\nlocal client models, with data distributed across heterogeneous partitions.\nThis approach identifies the critical parameters that significantly influence\nthe quantum model's performance, ensuring they are preserved during the\naggregation process. Our research assessed the effectiveness and feasibility of\nQFL by comparing its performance against other variants, and exploring the\nbenefits of incorporating Fisher information in QFL settings. Experimental\nresults on ADNI and MNIST datasets demonstrate the effectiveness of our\napproach in achieving better performance and robustness against the quantum\nfederated averaging method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17580v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "量子联邦学习中基于费雪信息优化的增强", "tldr": "提出一种基于费雪信息的量子联邦学习（QFL）算法，通过识别关键参数来提升性能和鲁棒性，克服传统FL挑战。", "motivation": "联邦学习面临高通信成本、数据异构、处理时间长和隐私威胁等挑战。量子联邦学习虽有前景，但仍需解决这些问题。费雪信息可以量化量子态信息，有望解决上述挑战。", "method": "提出一种量子联邦学习（QFL）算法，该算法利用在本地客户端模型上计算的费雪信息。此方法在聚合过程中识别并保留显著影响量子模型性能的关键参数，数据分布在异构分区。", "result": "在ADNI和MNIST数据集上的实验结果表明，该方法在性能和对抗量子联邦平均方法的鲁棒性方面优于其他变体。", "conclusion": "通过利用费雪信息识别和保留关键参数，所提出的QFL算法有效提升了量子联邦学习的性能和鲁棒性，成功应对了传统联邦学习的挑战。", "translation": "联邦学习（FL）在不同领域越来越受欢迎，它提供了一种客户端协作训练全局模型而不共享敏感数据的方式。它涉及全局模型和参与客户端之间的多轮通信，这带来了高通信成本、异构客户端数据、处理时间延长和隐私威胁增加等挑战。近年来，联邦学习与参数化量子电路的融合引发了重要的研究兴趣，对医疗保健和金融等领域具有广阔的前景。通过实现量子模型的去中心化训练，它允许客户端或机构在保护数据隐私的同时协作提升模型性能和结果。认识到费雪信息可以量化量子态在参数变化下携带的信息量，从而深入了解其几何和统计特性。我们打算利用这一特性来解决上述挑战。在这项工作中，我们提出了一种量子联邦学习（QFL）算法，该算法利用在本地客户端模型上计算的费雪信息，数据分布在异构分区中。这种方法识别出显著影响量子模型性能的关键参数，确保它们在聚合过程中得到保留。我们的研究通过比较其性能与其他变体，并探索在QFL设置中结合费雪信息的优势，评估了QFL的有效性和可行性。在ADNI和MNIST数据集上的实验结果表明，我们的方法在实现更好的性能和对抗量子联邦平均方法的鲁棒性方面是有效的。", "summary": "本文提出了一种新颖的量子联邦学习（QFL）算法，旨在通过利用费雪信息来解决传统联邦学习（FL）中的挑战，如高通信成本和数据异构性。该算法通过计算本地客户端模型的费雪信息来识别并保留影响量子模型性能的关键参数，从而在聚合过程中增强模型表现。在ADNI和MNIST数据集上的实验证明，该方法在性能和鲁棒性上优于现有的量子联邦平均方法。", "keywords": "量子联邦学习, 费雪信息, 分布式训练, 异构数据, 模型优化", "comments": "该论文的创新点在于将费雪信息引入量子联邦学习，以智能地识别和保留对模型性能至关重要的参数，从而有效缓解了联邦学习中数据异构性、通信成本和隐私保护等核心挑战。这种基于信息理论的优化方法为提升分布式量子模型的效率和鲁棒性提供了新的视角。"}}
{"id": "2507.17427", "title": "Learning to Write on Dirty Paper", "authors": ["Ezgi Ozyilkan", "Oğuzhan Kubilay Ülger", "Elza Erkip"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      accepter for publication at 2025 IEEE Information Theory Workshop (ITW)", "url": "http://arxiv.org/abs/2507.17427v1", "summary": "Dirty paper coding (DPC) is a classical problem in information theory that\nconsiders communication in the presence of channel state known only at the\ntransmitter. While the theoretical impact of DPC has been substantial,\npractical realizations of DPC, such as Tomlinson-Harashima precoding (THP) or\nlattice-based schemes, often rely on specific modeling assumptions about the\ninput, state and channel. In this work, we explore whether modern\nlearning-based approaches can offer a complementary path forward by revisiting\nthe DPC problem. We propose a data-driven solution in which both the encoder\nand decoder are parameterized by neural networks. Our proposed model operates\nwithout prior knowledge of the state (also referred to as \"interference\"),\nchannel or input statistics, and recovers nonlinear mappings that yield\neffective interference pre-cancellation. To the best of our knowledge, this is\nthe first interpretable proof-of-concept demonstrating that learning-based DPC\nschemes can recover characteristic features of well-established solutions, such\nas THP and lattice-based precoding, and outperform them in several regimes.", "comment": "accepter for publication at 2025 IEEE Information Theory Workshop\n  (ITW)", "pdf_url": "http://arxiv.org/pdf/2507.17427v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "学习在脏纸上书写", "tldr": "本文提出了一种基于神经网络的数据驱动方法来解决脏纸编码（DPC）问题，该方法无需先验知识，能恢复非线性映射，并在某些情况下优于传统方案。", "motivation": "脏纸编码（DPC）在信息论中具有重要理论影响，但其实际实现（如THP或基于格的方案）通常依赖于对输入、状态和信道的特定建模假设。本研究旨在探索现代基于学习的方法能否为DPC问题提供一种无需这些假设的补充解决方案。", "method": "提出了一种数据驱动的解决方案，其中编码器和解码器都由神经网络参数化。该模型无需事先了解状态（干扰）、信道或输入统计信息，并能够恢复产生有效干扰预消除的非线性映射。", "result": "首次证明了基于学习的DPC方案可以恢复诸如THP和基于格预编码等成熟解决方案的特征，并在多个场景下超越它们。", "conclusion": "基于学习的DPC方案是一种可行且有效的替代方法，能够克服传统DPC实现中对先验知识的依赖，并在某些情况下展现出更优的性能。", "translation": "脏纸编码（DPC）是信息论中的一个经典问题，它考虑了在发射端已知信道状态下的通信。虽然DPC的理论影响巨大，但DPC的实际实现，例如Tomlinson-Harashima预编码（THP）或基于格的方案，通常依赖于对输入、状态和信道的特定建模假设。在这项工作中，我们通过重新审视DPC问题，探索现代基于学习的方法是否能提供一条互补的途径。我们提出了一种数据驱动的解决方案，其中编码器和解码器都由神经网络参数化。我们提出的模型在没有状态（也称为“干扰”）、信道或输入统计信息的先验知识的情况下运行，并恢复产生有效干扰预消除的非线性映射。据我们所知，这是第一个可解释的概念验证，表明基于学习的DPC方案可以恢复成熟解决方案（如THP和基于格预编码）的特征，并在多个场景下超越它们。", "summary": "本文提出了一种新颖的基于学习的脏纸编码（DPC）方法，通过使用神经网络作为编码器和解码器，克服了传统DPC方案对信道和输入统计信息等先验知识的依赖。该数据驱动模型能够自动学习并恢复有效的非线性干扰预消除映射，实验结果表明它不仅能复现传统方法的特性，还在某些情况下表现出更优的性能，为DPC的实际应用提供了新的方向。", "keywords": "脏纸编码, 神经网络, 干扰消除, 机器学习, 预编码", "comments": "这篇论文的创新点在于将深度学习引入到经典的信息论问题——脏纸编码中，提供了一种无需特定建模假设的数据驱动解决方案。其重要性在于，它展示了机器学习在复杂通信系统中的潜力，尤其是在处理未知或复杂信道条件下的干扰消除问题。该工作为DPC的实际部署提供了一条更灵活、更自适应的路径，未来可以探索其在更复杂信道模型和大规模MIMO系统中的表现。"}}
{"id": "2507.17153", "title": "Stacked Intelligent Metasurface Assisted Multiuser Communications: From a Rate Fairness Perspective", "authors": ["Junjie Fang", "Chao Zhang", "Jiancheng An", "Hongwen Yu", "Qingqing Wu", "Mérouane Debbah", "Chau Yuen"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17153v1", "summary": "Stacked intelligent metasurface (SIM) extends the concept of single-layer\nreconfigurable holographic surfaces (RHS) by incorporating a multi-layered\nstructure, thereby providing enhanced control over electromagnetic wave\npropagation and improved signal processing capabilities. This study\ninvestigates the potential of SIM in enhancing the rate fairness in multiuser\ndownlink systems by addressing two key optimization problems: maximizing the\nminimum rate (MR) and maximizing the geometric mean of rates (GMR). {The former\nstrives to enhance the minimum user rate, thereby ensuring fairness among\nusers, while the latter relaxes fairness requirements to strike a better\ntrade-off between user fairness and system sum-rate (SR).} For the MR\nmaximization, we adopt a consensus alternating direction method of multipliers\n(ADMM)-based approach, which decomposes the approximated problem into\nsub-problems with closed-form solutions. {For GMR maximization, we develop an\nalternating optimization (AO)-based algorithm that also yields closed-form\nsolutions and can be seamlessly adapted for SR maximization. Numerical results\nvalidate the effectiveness and convergence of the proposed algorithms.}\nComparative evaluations show that MR maximization ensures near-perfect\nfairness, while GMR maximization balances fairness and system SR. Furthermore,\nthe two proposed algorithms respectively outperform existing related works in\nterms of MR and SR performance. Lastly, SIM with lower power consumption\nachieves performance comparable to that of multi-antenna digital beamforming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17153v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "堆叠智能超表面辅助多用户通信：从速率公平性视角", "tldr": "本文研究了堆叠智能超表面（SIM）在多用户下行系统中提升速率公平性的潜力，提出了基于ADMM和AO的算法来解决最小速率最大化和几何平均速率最大化问题，并证明了其有效性和优越性。", "motivation": "本文旨在探究堆叠智能超表面（SIM）在多用户下行系统中提升速率公平性的潜力，通过解决最大化最小速率（MR）和最大化速率几何平均（GMR）两个关键优化问题来实现。", "method": "对于最小速率（MR）最大化问题，采用了一种基于共识交替方向乘子法（ADMM）的方法，将近似问题分解为具有闭合解的子问题。对于速率几何平均（GMR）最大化问题，开发了一种基于交替优化（AO）的算法，该算法也产生了闭合解，并且可以无缝地适用于系统和速率（SR）最大化。", "result": "数值结果验证了所提出算法的有效性和收敛性。比较评估表明，MR最大化确保了接近完美的公平性，而GMR最大化则平衡了公平性和系统和速率。此外，所提出的两种算法在MR和SR性能方面分别优于现有的相关工作。最后，功耗更低的SIM实现了与多天线数字波束成形相当的性能。", "conclusion": "堆叠智能超表面（SIM）通过所提出的优化算法，能够有效提升多用户下行系统中的速率公平性与整体性能，并在功耗方面展现出优势，性能可媲美传统数字波束成形。", "translation": "堆叠智能超表面（SIM）通过整合多层结构，扩展了单层可重构全息表面（RHS）的概念，从而增强了对电磁波传播的控制并改进了信号处理能力。本研究通过解决两个关键优化问题：最大化最小速率（MR）和最大化速率几何平均（GMR），探讨了SIM在增强多用户下行系统中速率公平性方面的潜力。前者致力于提高最小用户速率，从而确保用户间的公平性，而后者则放宽了公平性要求，以在用户公平性和系统和速率（SR）之间取得更好的权衡。对于MR最大化，我们采用了一种基于共识交替方向乘子法（ADMM）的方法，该方法将近似问题分解为具有闭合解的子问题。对于GMR最大化，我们开发了一种基于交替优化（AO）的算法，该算法也产生了闭合解，并且可以无缝地适用于SR最大化。数值结果验证了所提出算法的有效性和收敛性。比较评估表明，MR最大化确保了接近完美的公平性，而GMR最大化则平衡了公平性和系统和速率。此外，所提出的两种算法在MR和SR性能方面分别优于现有的相关工作。最后，功耗更低的SIM实现了与多天线数字波束成形相当的性能。", "summary": "本文研究了堆叠智能超表面（SIM）在多用户下行通信中提升速率公平性的应用。通过解决最小速率最大化（MR）和速率几何平均最大化（GMR）问题，作者提出了基于ADMM和AO的优化算法。研究结果表明，这些算法能有效提高用户公平性或平衡公平性与系统总速率，并且在性能上优于现有方法，同时SIM在低功耗下能达到与多天线数字波束成形相媲美的效果。", "keywords": "堆叠智能超表面, 速率公平性, 多用户通信, ADMM, 交替优化", "comments": "本文创新性地将堆叠智能超表面应用于多用户通信中的速率公平性优化问题，并通过提出两种有效的优化算法（ADMM和AO）解决了复杂的非凸问题，提供了闭合解。其重要性在于展示了SIM在提升系统公平性和效率方面的潜力，并证明了其在低功耗下可与传统数字波束成形媲美的性能，为未来无线通信系统设计提供了新思路。"}}
{"id": "2507.17184", "title": "A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis", "authors": ["Hui Zhao"], "categories": ["cs.GR", "cs.NA", "math.NA"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      in Chinese and English", "url": "http://arxiv.org/abs/2507.17184v1", "summary": "In the current practices of both industry and academia, the convergence and\naccuracy of finite element calculations are closely related to the methods and\nquality of mesh generation. For years, the research on high-quality mesh\ngeneration in the domestic academic field has mainly referred to the local\nquality of quadrilaterals and hexahedrons approximating that of squares and\ncubes. The main contribution of this paper is to propose a brand-new research\ndirection and content: it is necessary to explore and study the influence of\nthe overall global arrangement structure and pattern of super structured\nquadrilateral meshes on the convergence and calculation accuracy of finite\nelement calculations. Through the research in this new field, it can help solve\nthe non-rigorous state of serious reliance on \"experience\" in the mesh\ngeneration stage during simulation in the current industry and academia, and\nmake clear judgments on which global arrangements of mesh generation can ensure\nthe convergence of finite element calculations. In order to generate and design\nsuper-structured quadrilateral meshes with controllable overall arrangement\nstructures, a large number of modern two-dimensional and three-dimensional\ngeometric topology theories are required, such as moduli space, Teichm\\\"uller\nspace, harmonic foliations, dynamical systems, surface mappings, meromorphic\nquadratic differentials, surface mappings, etc.", "comment": "in Chinese and English", "pdf_url": "http://arxiv.org/pdf/2507.17184v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一个科学问题：超结构四边形网格对有限元分析收敛性和精度的影响研究", "tldr": "本文提出了一个新研究方向，旨在探索超结构四边形网格的整体全局排列结构对有限元计算收敛性和精度的影响，以减少网格生成对经验的依赖。", "motivation": "当前工业界和学术界在有限元计算中，网格生成严重依赖经验，导致收敛性和精度难以保证。本文旨在解决这一非严谨状态，并明确网格的全局排列如何影响有限元计算的收敛性。", "method": "提出研究超结构四边形网格的整体全局排列结构和模式对有限元计算收敛性和计算精度的影响。为实现可控的网格生成，需要运用模空间、Teichmüller空间、调和叶状结构、动力系统、曲面映射、亚纯二次微分等现代几何拓扑理论。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在当前工业界和学术界的实践中，有限元计算的收敛性和精度与网格生成的方法和质量密切相关。多年来，国内学术界关于高质量网格生成的研究主要集中在四边形和六面体的局部质量接近正方形和立方体。本文的主要贡献是提出了一个全新的研究方向和内容：有必要探索和研究超结构四边形网格的整体全局排列结构和模式对有限元计算收敛性和计算精度的影响。通过这个新领域的研究，可以帮助解决当前工业界和学术界在模拟过程中网格生成阶段严重依赖“经验”的非严谨状态，并对哪些全局排列的网格生成可以确保有限元计算的收敛性做出明确判断。为了生成和设计具有可控整体排列结构的超结构四边形网格，需要大量的现代二维和三维几何拓扑理论，例如模空间、Teichmüller空间、调和叶状结构、动力系统、曲面映射、亚纯二次微分等。", "summary": "本文提出一个全新的研究方向，旨在探讨超结构四边形网格的整体全局排列结构对有限元计算收敛性和精度的影响。与以往侧重局部网格质量的研究不同，该工作强调全局结构的重要性，旨在解决当前网格生成中过度依赖经验的问题，并为确保有限元计算收敛性提供清晰判断。实现这一目标需要整合现代二维和三维几何拓扑理论。", "keywords": "有限元分析, 四边形网格, 网格生成, 全局排列, 收敛性", "comments": "本文的创新之处在于提出了一个全新的研究视角，将有限元网格生成的研究重点从传统的局部质量提升到整体全局排列结构。这有望解决当前工业界和学术界在网格生成中过度依赖经验的痛点，具有重要的理论和实践意义。"}}
{"id": "2507.17685", "title": "Data assimilation using a global Girsanov nudged particle filter", "authors": ["Maneesh Kumar Singh", "Joshua Hope-Collins", "Colin J. Cotter", "Dan Crisan"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17685v1", "summary": "We present a particle filtering algorithm for stochastic models on infinite\ndimensional state space, making use of Girsanov perturbations to nudge the\nensemble of particles into regions of higher likelihood. We argue that the\noptimal control problem needs to couple control variables for all of the\nparticles to maintain an ensemble with good effective sample size (ESS). We\nprovide an optimisation formulation that separates the problem into three\nstages, separating the nonlinearity in the ESS term in the functional with the\nnonlinearity due to the forward problem, and allowing independent parallel\ncomputation for each particle when calculations are performed over control\nvariable space. The particle filter is applied to the stochastic\nKuramoto-Sivashinsky equation, and compared with the temper-jitter particle\nfilter approach. We observe that whilst the nudging filter is over spread\ncompared to the temper-jitter filter, it responds to extreme events in the\nassimilated data more quickly and robustly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17685v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用全局Girsanov微扰粒子滤波的数据同化", "tldr": "本文提出了一种新的粒子滤波算法，利用Girsanov微扰将粒子群推向更高似然区域，并优化了控制问题以保持有效的样本大小。该算法在随机Kuramoto-Sivashinsky方程上的应用显示，虽然其分布较广，但对极端事件响应更快、更鲁棒。", "motivation": "在无限维状态空间上对随机模型进行粒子滤波时，需要将粒子群推向更高似然区域以提高准确性。同时，为了保持良好的有效样本大小（ESS），需要解决最优控制问题中所有粒子控制变量的耦合问题。", "method": "本文提出了一种粒子滤波算法，利用Girsanov微扰将粒子群推向更高似然区域。为此，他们提供了一个优化公式，将问题分为三个阶段，分离了ESS项中的非线性和正向问题中的非线性，并允许在控制变量空间上对每个粒子进行独立的并行计算。该粒子滤波器应用于随机Kuramoto-Sivashinsky方程，并与temper-jitter粒子滤波方法进行比较。", "result": "研究发现，尽管Girsanov微扰粒子滤波器与temper-jitter滤波器相比分布更广（over spread），但它对同化数据中的极端事件响应更快、更鲁棒。", "conclusion": "Girsanov微扰粒子滤波算法在处理无限维状态空间上的随机模型时，特别是在应对极端事件方面，表现出较快的响应速度和更高的鲁棒性，尽管其 ensemble 可能会更分散。", "translation": "我们提出了一种用于无限维状态空间上随机模型的粒子滤波算法，该算法利用Girsanov微扰将粒子群推向更高似然区域。我们认为，最优控制问题需要耦合所有粒子的控制变量，以保持具有良好有效样本大小（ESS）的粒子群。我们提供了一个优化公式，将问题分为三个阶段，分离了泛函中ESS项的非线性与正向问题引起的非线性，并允许在控制变量空间上进行计算时，每个粒子进行独立的并行计算。该粒子滤波器应用于随机Kuramoto-Sivashinsky方程，并与temper-jitter粒子滤波方法进行比较。我们观察到，虽然微扰滤波器与temper-jitter滤波器相比分布更广，但它对同化数据中的极端事件响应更快、更鲁棒。", "summary": "本文介绍了一种新的粒子滤波算法，该算法通过Girsanov微扰技术将粒子群引导至高似然区域，以应用于无限维状态空间上的随机模型。为解决有效样本大小（ESS）问题，论文提出了一种三阶段优化公式，实现了控制变量空间中粒子计算的独立并行化。将该滤波器应用于随机Kuramoto-Sivashinsky方程，并与现有方法对比，结果表明，尽管新算法的粒子分布可能更广，但在处理极端事件时表现出更快速和鲁棒的响应能力。", "keywords": "粒子滤波, Girsanov微扰, 数据同化, 无限维状态空间, 有效样本大小", "comments": "本文的创新点在于引入Girsanov微扰来“微扰”粒子群，使其向高似然区域移动，并提出了一种分离式优化框架，解决了在无限维空间中保持有效样本大小的挑战，同时实现了并行计算。尽管存在“over spread”的局限性，但其在处理极端事件时的快速和鲁棒响应能力，使其在数据同化领域具有潜在的应用价值。"}}
{"id": "2506.15643", "title": "Revisiting Randomization in Greedy Model Search", "authors": ["Xin Chen", "Jason M. Klusowski", "Yan Shuo Tan", "Chang Yu"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15643v2", "summary": "Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15643v2", "cate": "stat.ML", "date": "2025-06-18", "updated": "2025-07-22", "AI": {"title_translation": "重新审视贪婪模型搜索中的随机化", "tldr": "本文提出了一种基于特征子采样的随机贪婪前向选择估计器集成方法，通过动态规划显著提高了计算效率，并在稀疏线性回归中表现优于Lasso和Elastic Net。研究还揭示了随机集成可以同时降低训练误差和自由度，从而改变偏差-方差权衡曲线，并增强了对随机森林和隐式正则化的理解。", "motivation": "组合随机估计器（如随机森林）在现代数据科学中是基础技术，但计算成本高昂，且其提高预测性能的机制尚不清楚。本文旨在解决这些问题。", "method": "本文提出并分析了一种通过特征子采样实现随机化的贪婪前向选择估计器集成方法。在每次迭代中，从随机子集中选择最佳特征。设计了一种基于动态规划的新颖实现，以大幅提高计算效率。", "result": "数值实验表明，该方法在各种设置下均优于Lasso和Elastic Net等流行方法。研究还发现，与普遍认为随机集成类似于收缩的观点相反，它能够同时减少训练误差和自由度，从而改变基础估计器的整个偏差-方差权衡曲线。在正交特征设置下，理论证明了集成估计器通过两参数Logistic权重族重新缩放普通最小二乘系数，从而扩大了模型搜索空间。", "conclusion": "这些结果增强了我们对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的影响。", "translation": "将随机估计器组合成集成模型，例如通过随机森林，已成为现代数据科学中的一项基本技术，但其计算成本可能很高。此外，其提高预测性能的机制也知之甚少。我们通过在稀疏线性回归的背景下提出和分析一种贪婪前向选择估计器集成方法来解决这些问题，该方法通过特征子采样进行随机化——在每次迭代中，从随机子集中选择最佳特征。我们设计了一种基于动态规划的新颖实现，极大地提高了其计算效率。此外，我们通过仔细的数值实验表明，我们的方法在各种设置下均优于Lasso和Elastic Net等流行方法。接下来，与普遍认为随机集成类似于收缩的观点相反，我们通过数值实验表明，它能够同时减少训练误差和自由度，从而改变基础估计器的整个偏差-方差权衡曲线。我们在正交特征设置下严格证明了这一事实，在这种情况下，集成估计器通过一个两参数的Logistic权重族重新缩放普通最小二乘系数，从而扩大了模型搜索空间。这些结果增强了我们对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的影响。", "summary": "本文针对随机集成估计器（如随机森林）计算成本高和性能提升机制不明的问题，在稀疏线性回归背景下提出了一种创新的随机贪婪前向选择集成方法。该方法通过特征子采样实现随机化，并采用动态规划优化计算效率。实验证明，该方法在性能上超越了Lasso和Elastic Net。更重要的是，研究揭示随机集成能够同时降低训练误差和自由度，从而改变偏差-方差权衡曲线，这与现有认知不同。理论分析进一步表明，在特定条件下，集成估计器能通过重新缩放系数来扩大模型搜索空间。这些发现加深了对随机森林的理解，并暗示隐式正则化可能比显式正则化具有更复杂的效应。", "keywords": "随机化, 贪婪模型搜索, 特征子采样, 偏差-方差权衡, 隐式正则化", "comments": "本文在理解随机化集成模型（特别是随机森林）的内在机制方面取得了重要进展。通过提出一种计算效率更高的新型贪婪模型搜索方法，并深入分析其对偏差-方差权衡的影响，挑战了关于随机化等同于收缩的传统观点。研究成果不仅提供了性能更优的算法，更重要的是，它揭示了隐式正则化可能比显式正则化具有更复杂且更积极的作用，这对于机器学习理论和实践都具有重要的启发意义。"}}
{"id": "2507.17577", "title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors", "authors": ["Chen Ma", "Xinjie Xu", "Shuyu Cheng", "Qi Xuan"], "categories": ["cs.CV", "cs.CR", "cs.LG", "I.2.6; I.5.1; G.1.6"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published at ICLR 2025 (Spotlight paper)", "url": "http://arxiv.org/abs/2507.17577v1", "summary": "One of the most practical and challenging types of black-box adversarial\nattacks is the hard-label attack, where only the top-1 predicted label is\navailable. One effective approach is to search for the optimal ray direction\nfrom the benign image that minimizes the $\\ell_p$-norm distance to the\nadversarial region. The unique advantage of this approach is that it transforms\nthe hard-label attack into a continuous optimization problem. The objective\nfunction value is the ray's radius, which can be obtained via binary search at\na high query cost. Existing methods use a \"sign trick\" in gradient estimation\nto reduce the number of queries. In this paper, we theoretically analyze the\nquality of this gradient estimation and propose a novel prior-guided approach\nto improve ray search efficiency both theoretically and empirically.\nSpecifically, we utilize the transfer-based priors from surrogate models, and\nour gradient estimators appropriately integrate them by approximating the\nprojection of the true gradient onto the subspace spanned by these priors and\nrandom directions, in a query-efficient manner. We theoretically derive the\nexpected cosine similarities between the obtained gradient estimators and the\ntrue gradient, and demonstrate the improvement achieved by incorporating\npriors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that\nour approach significantly outperforms 11 state-of-the-art methods in terms of\nquery efficiency.", "comment": "Published at ICLR 2025 (Spotlight paper)", "pdf_url": "http://arxiv.org/pdf/2507.17577v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "利用基于迁移的先验提升硬标签攻击中的射线搜索过程", "tldr": "本文提出了一种新的先验引导方法，通过利用替代模型的迁移先验，显著提高了硬标签攻击中射线搜索的查询效率，超越了现有SOTA方法。", "motivation": "硬标签攻击是一种实用且具有挑战性的黑盒对抗攻击，现有方法通过射线搜索将问题转化为连续优化，但梯度估计的查询成本很高。本文旨在提高射线搜索的效率并降低查询成本。", "method": "本文理论分析了现有梯度估计的质量，并提出了一种新的先验引导方法。具体来说，利用替代模型的迁移先验，以查询高效的方式近似真实梯度在这些先验和随机方向所张子空间上的投影，从而整合先验信息改进梯度估计。", "result": "理论上，本文推导了所得梯度估计器与真实梯度之间预期的余弦相似度，并证明了结合先验所带来的改进。在ImageNet和CIFAR-10数据集上的大量实验表明，该方法在查询效率方面显著优于11种最先进的方法。", "conclusion": "本文通过引入基于迁移的先验，有效提高了硬标签攻击中射线搜索的效率，显著降低了查询成本，并超越了现有最先进的方法，为黑盒攻击提供了更实用的解决方案。", "translation": "最实用和最具挑战性的黑盒对抗攻击类型之一是硬标签攻击，其中只有top-1预测标签可用。一种有效的方法是从良性图像中搜索最佳射线方向，以最小化到对抗区域的$\\ell_p$-范数距离。这种方法的独特优势在于它将硬标签攻击转化为一个连续优化问题。目标函数值是射线的半径，可以通过二分搜索以高查询成本获得。现有方法在梯度估计中使用“符号技巧”来减少查询次数。在本文中，我们理论分析了这种梯度估计的质量，并提出了一种新颖的先验引导方法，以在理论和经验上提高射线搜索效率。具体来说，我们利用来自替代模型的基于迁移的先验，并且我们的梯度估计器通过近似真实梯度在这些先验和随机方向所张子空间上的投影，以查询高效的方式适当整合它们。我们理论推导了所得梯度估计器与真实梯度之间预期的余弦相似度，并证明了结合先验所实现的改进。在ImageNet和CIFAR-10数据集上的大量实验表明，我们的方法在查询效率方面显著优于11种最先进的方法。", "summary": "本文针对黑盒硬标签对抗攻击中射线搜索的高查询成本问题，提出了一种新颖的先验引导方法。通过理论分析现有梯度估计的局限性，并利用替代模型的迁移先验来改进梯度估计，该方法能够以查询高效的方式近似真实梯度。实验结果表明，该方法在ImageNet和CIFAR-10数据集上显著提升了查询效率，超越了多种现有先进方法。", "keywords": "硬标签攻击, 射线搜索, 迁移先验, 查询效率, 对抗攻击", "comments": "本文针对黑盒硬标签攻击中查询效率这一核心挑战，提出了一种创新性的解决方案。其亮点在于将迁移学习的先验知识融入到梯度估计中，通过理论推导和实验验证，证明了这种结合的有效性。这不仅提升了攻击效率，也为未来黑盒攻击的研究提供了新的思路，具有重要的实践意义和理论价值。"}}
{"id": "2507.17376", "title": "An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness", "authors": ["Tianshu Ruan", "Aniketh Ramesh", "Rustam Stolkin", "Manolis Chiou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17376v1", "summary": "In this paper, we investigate the impact of high-level semantics (evaluation\nof the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction\n(HRI) in the context of mobile robot deployments. Although semantics has been\nwidely researched in AI, how high-level semantics can benefit the HRT paradigm\nis underexplored, often fuzzy, and intractable. We applied a semantics-based\nframework that could reveal different indicators of the environment (i.e. how\nmuch semantic information exists) in a mock-up disaster response mission. In\nsuch missions, semantics are crucial as the HRT should handle complex\nsituations and respond quickly with correct decisions, where humans might have\na high workload and stress. Especially when human operators need to shift their\nattention between robots and other tasks, they will struggle to build\nSituational Awareness (SA) quickly. The experiment suggests that the presented\nsemantics: 1) alleviate the perceived workload of human operators; 2) increase\nthe operator's trust in the SA; and 3) help to reduce the reaction time in\nswitching the level of autonomy when needed. Additionally, we find that\nparticipants with higher trust in the system are encouraged by high-level\nsemantics to use teleoperation mode more.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17376v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于语义情境感知的人机交互探索性研究", "tldr": "本研究探索了高级语义如何帮助提高移动机器人部署中人机团队的情境感知能力，减轻操作员工作量，提高信任度，并缩短反应时间。", "motivation": "尽管语义在AI领域已被广泛研究，但高级语义如何有益于人机团队范式却未被充分探索，并且通常模糊且难以处理。在灾难响应任务中，语义至关重要，因为人机团队需要处理复杂情况并快速做出正确决策，而人类可能面临高工作量和压力，尤其是在需要快速建立情境感知时。", "method": "本研究应用了一个基于语义的框架，该框架能够在模拟灾难响应任务中揭示不同的环境指标（即存在多少语义信息）。", "result": "实验结果表明，所提出的语义：1）减轻了人类操作员的感知工作量；2）提高了操作员对情境感知的信任度；3）有助于在需要时缩短切换自主水平的反应时间。此外，研究发现对系统信任度较高的参与者更受高级语义的鼓励，更多地使用遥操作模式。", "conclusion": "高级语义能够显著改善移动机器人部署中的人机团队表现，通过减轻操作员工作量、提高信任度并缩短反应时间来增强情境感知。", "translation": "在本文中，我们研究了在移动机器人部署背景下，高级语义（环境评估）对人机团队（HRT）和人机交互（HRI）的影响。尽管语义在人工智能领域已被广泛研究，但高级语义如何有益于人机团队范式却未被充分探索，通常模糊且难以处理。我们应用了一个基于语义的框架，该框架可以在模拟灾难响应任务中揭示不同的环境指标（即存在多少语义信息）。在这样的任务中，语义至关重要，因为人机团队应该处理复杂情况并迅速做出正确决策，而人类可能面临高工作量和压力。特别是当人类操作员需要在机器人和其他任务之间转移注意力时，他们将难以快速建立情境感知（SA）。实验表明，所提出的语义：1）减轻了人类操作员的感知工作量；2）提高了操作员对SA的信任度；3）有助于在需要时缩短切换自主水平的反应时间。此外，我们发现对系统信任度较高的参与者更受高级语义的鼓励，更多地使用遥操作模式。", "summary": "本研究探讨了高级语义在移动机器人部署中对人机团队和人机交互的影响。研究指出，尽管语义在AI中广泛研究，但在人机团队中应用高级语义仍有不足。通过在模拟灾难响应任务中应用基于语义的框架，实验结果表明高级语义能有效减轻操作员感知工作量、提高对情境感知的信任度，并缩短自主水平切换的反应时间。此外，高信任度的参与者在高级语义的鼓励下更倾向于使用遥操作模式。", "keywords": "人机交互, 情境感知, 高级语义, 移动机器人, 灾难响应", "comments": "这项研究的创新之处在于探索了高级语义在人机交互和团队协作中的具体应用，尤其是在高压、复杂的情境感知场景下。它提供了一个实证基础，证明了语义信息能够直接改善操作员的体验和团队效率。其重要性在于为未来设计更智能、更人性化的人机协作系统提供了方向，尤其是在灾难响应等关键领域。该研究的局限性可能在于其“探索性”性质，实验是在“模拟”环境下进行的，实际部署中的复杂性和不可预测性可能需要进一步验证。"}}
{"id": "2410.17491", "title": "X-MOBILITY: End-To-End Generalizable Navigation via World Modeling", "authors": ["Wei Liu", "Huihua Zhao", "Chenran Li", "Joydeep Biswas", "Billy Okal", "Pulkit Goyal", "Yan Chang", "Soha Pouya"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.17491v3", "summary": "General-purpose navigation in challenging environments remains a significant\nproblem in robotics, with current state-of-the-art approaches facing myriad\nlimitations. Classical approaches struggle with cluttered settings and require\nextensive tuning, while learning-based methods face difficulties generalizing\nto out-of-distribution environments. This paper introduces X-Mobility, an\nend-to-end generalizable navigation model that overcomes existing challenges by\nleveraging three key ideas. First, X-Mobility employs an auto-regressive world\nmodeling architecture with a latent state space to capture world dynamics.\nSecond, a diverse set of multi-head decoders enables the model to learn a rich\nstate representation that correlates strongly with effective navigation skills.\nThird, by decoupling world modeling from action policy, our architecture can\ntrain effectively on a variety of data sources, both with and without expert\npolicies: off-policy data allows the model to learn world dynamics, while\non-policy data with supervisory control enables optimal action policy learning.\nThrough extensive experiments, we demonstrate that X-Mobility not only\ngeneralizes effectively but also surpasses current state-of-the-art navigation\napproaches. Additionally, X-Mobility also achieves zero-shot Sim2Real\ntransferability and shows strong potential for cross-embodiment generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.17491v3", "cate": "cs.RO", "date": "2024-10-23", "updated": "2025-07-22", "AI": {"title_translation": "X-MOBILITY：通过世界建模实现端到端通用导航", "tldr": "X-Mobility是一种端到端通用导航模型，通过世界建模和解耦学习，在各种环境中表现出色，并实现Sim2Real零样本迁移。", "motivation": "现有导航方法在复杂环境中面临泛化性挑战，经典方法需大量调整，学习方法难以泛化到分布外环境。", "method": "X-Mobility通过三个关键思想实现：1. 采用具有潜在状态空间的自回归世界建模架构捕获世界动态。2. 多样化的多头解码器使模型学习与有效导航技能强相关的丰富状态表示。3. 将世界建模与动作策略解耦，允许在有无专家策略的多种数据源上有效训练，利用离策略数据学习世界动态，在策略数据学习最优动作策略。", "result": "X-Mobility不仅能有效泛化，而且超越了当前最先进的导航方法。此外，它还实现了零样本Sim2Real迁移，并显示出强大的跨实体泛化潜力。", "conclusion": "X-Mobility通过其创新的世界建模和解耦学习方法，为通用导航问题提供了一个有效且具有优越泛化能力的解决方案，克服了现有方法的局限性。", "translation": "在复杂环境中实现通用导航仍然是机器人领域的一个重大问题，当前最先进的方法面临着诸多限制。经典方法在杂乱环境中表现不佳且需要大量调整，而基于学习的方法则难以泛化到分布外环境。本文介绍了X-Mobility，这是一种端到端可泛化的导航模型，它通过利用三个关键思想克服了现有挑战。首先，X-Mobility采用具有潜在状态空间的自回归世界建模架构来捕获世界动态。其次，多样化的多头解码器使模型能够学习与有效导航技能强相关的丰富状态表示。第三，通过将世界建模与动作策略解耦，我们的架构可以在各种数据源上有效训练，无论是否有专家策略：离策略数据允许模型学习世界动态，而带监督控制的在策略数据则能实现最优动作策略学习。通过大量实验，我们证明X-Mobility不仅能有效泛化，而且超越了当前最先进的导航方法。此外，X-Mobility还实现了零样本Sim2Real迁移，并显示出强大的跨实体泛化潜力。", "summary": "X-Mobility是一种新型的端到端通用导航模型，旨在解决机器人导航在复杂环境中的泛化难题。它通过结合自回归世界建模、多头解码器学习丰富状态表示以及解耦世界建模与动作策略的训练机制，实现了对多种数据源的有效利用。实验证明，X-Mobility在泛化能力上超越了现有SOTA方法，并展示了零样本Sim2Real和跨实体泛化的潜力。", "keywords": "通用导航, 世界建模, 端到端学习, 泛化, Sim2Real", "comments": "本文的创新点在于其端到端的世界建模方法以及世界建模与动作策略的解耦训练，这极大地增强了模型的泛化能力和数据利用效率。零样本Sim2Real迁移和跨实体泛化是其重要的亮点，显示了该方法在实际机器人应用中的巨大潜力。"}}
{"id": "2507.17731", "title": "Flow Matching Meets Biology and Life Science: A Survey", "authors": ["Zihao Li", "Zhichen Zeng", "Xiao Lin", "Feihao Fang", "Yanru Qu", "Zhe Xu", "Zhining Liu", "Xuying Ning", "Tianxin Wei", "Ge Liu", "Hanghang Tong", "Jingrui He"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint, 27 pages", "url": "http://arxiv.org/abs/2507.17731v1", "summary": "Over the past decade, advances in generative modeling, such as generative\nadversarial networks, masked autoencoders, and diffusion models, have\nsignificantly transformed biological research and discovery, enabling\nbreakthroughs in molecule design, protein generation, drug discovery, and\nbeyond. At the same time, biological applications have served as valuable\ntestbeds for evaluating the capabilities of generative models. Recently, flow\nmatching has emerged as a powerful and efficient alternative to diffusion-based\ngenerative modeling, with growing interest in its application to problems in\nbiology and life sciences. This paper presents the first comprehensive survey\nof recent developments in flow matching and its applications in biological\ndomains. We begin by systematically reviewing the foundations and variants of\nflow matching, and then categorize its applications into three major areas:\nbiological sequence modeling, molecule generation and design, and peptide and\nprotein generation. For each, we provide an in-depth review of recent progress.\nWe also summarize commonly used datasets and software tools, and conclude with\na discussion of potential future directions. The corresponding curated\nresources are available at\nhttps://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.", "comment": "Preprint, 27 pages", "pdf_url": "http://arxiv.org/pdf/2507.17731v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "流匹配遇上生物学与生命科学：一项综述", "tldr": "这篇综述首次全面概述了流匹配模型在生物学和生命科学领域的最新进展及其应用，涵盖了其基础、变体以及在生物序列建模、分子生成与设计以及肽和蛋白质生成方面的应用。", "motivation": "生成式模型（如GAN、MAE、扩散模型）在生物研究和发现中取得了显著进展，而流匹配作为扩散模型的强大替代品，在生物学和生命科学领域的应用日益增长。因此，需要对流匹配在这些领域的最新发展进行首次全面综述。", "method": "本文首先系统地回顾了流匹配的基础和变体，然后将其应用分为三大主要领域：生物序列建模、分子生成与设计以及肽和蛋白质生成。对每个领域都进行了深入回顾，并总结了常用的数据集和软件工具。", "result": "本文提供了流匹配及其在生物领域应用的首次全面综述，系统地分类并深入回顾了其在生物序列建模、分子生成与设计、肽和蛋白质生成方面的最新进展，并总结了常用数据集和软件工具。", "conclusion": "文章最后讨论了潜在的未来发展方向。", "translation": "在过去十年中，生成式建模的进步，如生成对抗网络、掩码自编码器和扩散模型，显著改变了生物研究和发现，在分子设计、蛋白质生成、药物发现等方面取得了突破。与此同时，生物应用也成为了评估生成模型能力的宝贵试验平台。最近，流匹配已成为基于扩散的生成式建模的一种强大而高效的替代方案，其在生物学和生命科学问题中的应用兴趣日益增长。本文首次全面综述了流匹配的最新发展及其在生物领域的应用。我们首先系统地回顾了流匹配的基础和变体，然后将其应用分为三大主要领域：生物序列建模、分子生成与设计，以及肽和蛋白质生成。对于每个领域，我们都深入回顾了最新进展。我们还总结了常用的数据集和软件工具，并以对潜在未来方向的讨论作为结束。相应的精选资源可在 https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology 获取。", "summary": "本文是首篇全面综述流匹配模型在生物学与生命科学领域应用的论文。它系统地回顾了流匹配的基础理论和多种变体，并将其在生物领域的应用划分为生物序列建模、分子生成与设计以及肽和蛋白质生成三大类，对每个领域内的最新进展进行了深入探讨。此外，文章还总结了相关的数据集和软件工具，并展望了未来的研究方向。", "keywords": "流匹配, 生物学, 生命科学, 生成模型, 综述", "comments": "这是一篇重要的综述性论文，因为它首次系统地梳理了新兴的流匹配技术在生物学和生命科学交叉领域的应用。这对于研究人员了解该领域的最新进展、潜在应用和未来挑战具有重要价值。其系统性的分类和对常用资源（数据集、工具）的总结也极具实用性。"}}
{"id": "2409.01534", "title": "Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign Recognition in the Wild", "authors": ["Yaozong Gan", "Guang Li", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published by Knowledge-Based Systems", "url": "http://arxiv.org/abs/2409.01534v2", "summary": "In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve\nzero-shot fine-grained traffic sign recognition (TSR) performance in the wild.\nZero-shot fine-grained TSR in the wild is challenging due to the cross-domain\nproblem between clean template traffic signs and real-world counterparts, and\nexisting approaches particularly struggle with cross-country TSR scenarios,\nwhere traffic signs typically differ between countries. The proposed CdMT\nframework tackles these challenges by leveraging the multi-step reasoning\ncapabilities of large multimodal models (LMMs). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for LMMs. Context descriptions, which are enhanced by center\ncoordinate prompt optimization, enable the precise localization of target\ntraffic signs in complex road images and filter irrelevant responses via novel\nprior traffic sign hypotheses. Characteristic descriptions, which are derived\nfrom in-context learning with template traffic signs, bridge cross-domain gaps\nand enhance fine-grained TSR. Differential descriptions refine the multimodal\nreasoning ability of LMMs by distinguishing subtle differences among similar\nsigns. CdMT is independent of training data and requires only simple and\nuniform instructions, enabling it to achieve cross-country TSR. We conducted\nextensive experiments on three benchmark datasets and two real-world datasets\nfrom different countries. The proposed CdMT framework achieved superior\nperformance compared with other state-of-the-art methods on all five datasets,\nwith recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB,\nBTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.", "comment": "Published by Knowledge-Based Systems", "pdf_url": "http://arxiv.org/pdf/2409.01534v2", "cate": "cs.CV", "date": "2024-09-03", "updated": "2025-07-23", "AI": {"title_translation": "跨域多步思考：野外零样本细粒度交通标志识别", "tldr": "提出跨域多步思考（CdMT）框架，利用大型多模态模型的推理能力，通过上下文、特征和差异描述，在无需训练数据的情况下显著提升野外零样本细粒度交通标志识别，尤其解决了跨国识别挑战。", "motivation": "现有的零样本细粒度交通标志识别在野外（尤其跨国场景）面临挑战，主要源于干净模板标志与真实世界标志之间的跨域问题，导致识别性能不佳。", "method": "提出跨域多步思考（CdMT）框架，利用大型多模态模型（LMMs）的多步推理能力。该框架引入了三种描述：1. 上下文描述：通过中心坐标提示优化增强，实现复杂道路图像中交通标志的精确定位，并通过先验交通标志假设过滤无关响应。2. 特征描述：通过模板交通标志的上下文学习获得，弥合跨域差距并增强细粒度识别。3. 差异描述：通过区分相似标志间的细微差异来提升LMMs的多模态推理能力。CdMT独立于训练数据，仅需简单统一的指令即可实现跨国识别。", "result": "CdMT框架在三个基准数据集和两个来自不同国家的真实世界数据集上进行了广泛实验，性能优于所有最先进的方法。在GTSRB、BTSD、TT-100K、Sapporo和Yokohama数据集上的识别准确率分别为0.93、0.89、0.97、0.89和0.85。", "conclusion": "提出的跨域多步思考（CdMT）框架通过利用大型多模态模型的推理能力，有效解决了野外零样本细粒度交通标志识别中的跨域和跨国挑战，并在多个数据集上取得了优异的性能，展示了其在实际应用中的潜力。", "translation": "在这项研究中，我们提出了跨域多步思考（CdMT）框架，以提高野外零样本细粒度交通标志识别（TSR）的性能。野外零样本细粒度TSR具有挑战性，原因在于干净的模板交通标志与真实世界对应物之间存在跨域问题，现有方法尤其在跨国TSR场景中表现不佳，因为不同国家的交通标志通常有所不同。所提出的CdMT框架通过利用大型多模态模型（LMMs）的多步推理能力来应对这些挑战。我们引入了上下文、特征和差异描述来设计LMMs的多个思考过程。上下文描述通过中心坐标提示优化得到增强，能够精确地定位复杂道路图像中的目标交通标志，并通过新颖的先验交通标志假设过滤无关响应。特征描述源自模板交通标志的上下文学习，弥合了跨域差距并增强了细粒度TSR。差异描述通过区分相似标志之间的细微差异来细化LMMs的多模态推理能力。CdMT独立于训练数据，只需要简单统一的指令，使其能够实现跨国TSR。我们在三个基准数据集和两个来自不同国家的真实世界数据集上进行了广泛实验。所提出的CdMT框架在所有五个数据集上均取得了优于其他最先进方法的性能，在GTSRB、BTSD、TT-100K、Sapporo和Yokohama数据集上的识别准确率分别为0.93、0.89、0.97、0.89和0.85。", "summary": "本文提出了跨域多步思考（CdMT）框架，旨在解决野外零样本细粒度交通标志识别中的跨域和跨国挑战。CdMT利用大型多模态模型的多步推理能力，通过引入上下文、特征和差异描述来精确定位、弥合域差距并区分相似标志。该框架无需训练数据，仅依赖简单指令即可实现跨国识别。实验结果表明，CdMT在多个基准和真实世界数据集上均取得了优于现有先进方法的识别性能。", "keywords": "零样本学习, 交通标志识别, 跨域, 多模态模型, 细粒度识别", "comments": "这项研究的创新之处在于提出了一个无需训练数据、仅通过多步推理和描述（上下文、特征、差异）就能有效处理跨域和跨国交通标志识别的框架。其利用大型多模态模型的零样本能力，为复杂现实世界场景下的识别问题提供了一种新颖且高效的解决方案，尤其在数据稀缺或领域差异大的情况下具有重要意义。"}}
{"id": "2507.15680", "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "authors": ["Yongkang Hou", "Jiarun Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15680v3", "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15680v3", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "用于图像质量评估的视觉-语言模型知识蒸馏方法", "tldr": "本文提出了一种视觉-语言模型知识蒸馏方法，旨在解决CLIP在图像质量评估中参数负担过大和局部失真特征识别能力不足的问题，通过知识蒸馏使学生模型在降低复杂度的同时超越现有方法。", "motivation": "现有的基于视觉-语言模型（如CLIP）的图像质量评估（IQA）方法存在参数负担过大和识别局部失真特征能力不足的问题。", "method": "本研究提出了一种视觉-语言模型知识蒸馏方法。首先，设计了质量分级提示模板以引导CLIP输出质量分数。其次，对CLIP进行微调以增强其在IQA任务中的能力。最后，提出了一种模态自适应知识蒸馏策略，实现CLIP教师模型对学生模型的指导。", "result": "在多个IQA数据集上的实验结果表明，所提出的方法显著降低了模型复杂度，同时性能优于现有IQA方法。", "conclusion": "该方法在降低模型复杂度的同时，提升了图像质量评估的性能，并展示了强大的实际部署潜力。", "translation": "图像质量评估（IQA）是计算机视觉中的一项核心任务。基于视觉-语言模型（如CLIP）的多模态方法在IQA任务中展现出卓越的泛化能力。为了解决CLIP在IQA中参数负担过重和识别局部失真特征能力不足的问题，本研究提出了一种视觉-语言模型知识蒸馏方法，旨在利用CLIP的IQA知识指导具有架构优势的模型的训练。首先，设计了质量分级提示模板以引导CLIP输出质量分数。然后，对CLIP进行微调以增强其在IQA任务中的能力。最后，提出了一种模态自适应知识蒸馏策略，以实现CLIP教师模型对学生模型的指导。我们的实验在多个IQA数据集上进行，结果表明所提出的方法显著降低了模型复杂度，同时性能优于现有IQA方法，显示出强大的实际部署潜力。", "summary": "本文针对视觉-语言模型（如CLIP）在图像质量评估（IQA）中存在的参数冗余和局部特征识别不足问题，提出了一种视觉-语言模型知识蒸馏方法。该方法通过设计质量分级提示模板、对CLIP进行微调以及引入模态自适应知识蒸馏策略，将CLIP的IQA知识迁移到更轻量级的学生模型。实验证明，该方法在显著降低模型复杂度的同时，实现了超越现有IQA方法的性能。", "keywords": "图像质量评估, 视觉-语言模型, 知识蒸馏, CLIP, 模型压缩", "comments": "该论文的创新点在于提出了一个针对视觉-语言模型（如CLIP）在图像质量评估中应用时遇到的实际问题（参数负担和局部特征识别）的解决方案。通过知识蒸馏，成功地将大型模型的强大泛化能力与轻量级模型的效率相结合，这对于实际部署具有重要意义。模态自适应知识蒸馏策略是其核心创新，有望在其他多模态任务中也得到应用。"}}
{"id": "2507.17055", "title": "Shared Control of Holonomic Wheelchairs through Reinforcement Learning", "authors": ["Jannis Bähler", "Diego Paez-Granados", "Jorge Peña-Queralta"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17055v1", "summary": "Smart electric wheelchairs can improve user experience by supporting the\ndriver with shared control. State-of-the-art work showed the potential of\nshared control in improving safety in navigation for non-holonomic robots.\nHowever, for holonomic systems, current approaches often lead to unintuitive\nbehavior for the user and fail to utilize the full potential of omnidirectional\ndriving. Therefore, we propose a reinforcement learning-based method, which\ntakes a 2D user input and outputs a 3D motion while ensuring user comfort and\nreducing cognitive load on the driver. Our approach is trained in Isaac Gym and\ntested in simulation in Gazebo. We compare different RL agent architectures and\nreward functions based on metrics considering cognitive load and user comfort.\nWe show that our method ensures collision-free navigation while smartly\norienting the wheelchair and showing better or competitive smoothness compared\nto a previous non-learning-based method. We further perform a sim-to-real\ntransfer and demonstrate, to the best of our knowledge, the first real-world\nimplementation of RL-based shared control for an omnidirectional mobility\nplatform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17055v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于强化学习的全向轮椅共享控制", "tldr": "本文提出了一种基于强化学习的方法，用于全向轮椅的共享控制，解决了现有方法导致的用户体验不佳问题，并实现了首次基于强化学习的全向移动平台共享控制的实物部署。", "motivation": "当前的全向系统共享控制方法常导致用户操作不直观，未能充分利用全向驱动的潜力，且可能增加用户的认知负荷。", "method": "提出了一种基于强化学习的方法，该方法将2D用户输入转换为3D运动输出，同时确保用户舒适度并降低驾驶员的认知负荷。该方法在Isaac Gym中训练，并在Gazebo中进行仿真测试，比较了不同的RL智能体架构和奖励函数。", "result": "所提出的方法确保了无碰撞导航，能够智能地调整轮椅方向，并且与之前的非学习方法相比，在平滑性方面表现更好或具有竞争力。实现了首次基于强化学习的全向移动平台共享控制的实物部署（从仿真到真实世界的迁移）。", "conclusion": "本文成功开发并验证了一种基于强化学习的全向轮椅共享控制方法，该方法显著改善了用户体验，并首次将此技术应用于实际的全向移动平台。", "translation": "智能电动轮椅通过共享控制支持驾驶员，可以改善用户体验。最先进的研究表明，共享控制在提高非全向机器人导航安全性方面的潜力。然而，对于全向系统，目前的方案常常导致用户行为不直观，并且未能充分利用全向驱动的全部潜力。因此，我们提出了一种基于强化学习的方法，该方法接收2D用户输入并输出3D运动，同时确保用户舒适度并降低驾驶员的认知负荷。我们的方法在Isaac Gym中进行训练，并在Gazebo中进行仿真测试。我们根据考虑认知负荷和用户舒适度的指标，比较了不同的RL智能体架构和奖励函数。我们表明，我们的方法确保了无碰撞导航，同时智能地调整轮椅方向，并且与之前的非学习方法相比，显示出更好或有竞争力的平滑度。我们进一步进行了从仿真到真实世界的迁移，并据我们所知，展示了首个基于强化学习的全向移动平台共享控制的真实世界实现。", "summary": "本研究提出了一种基于强化学习的创新方法，旨在改进全向轮椅的共享控制系统。该方法克服了现有技术在全向系统中导致操作不直观和未能充分利用其潜力的缺点。通过将2D用户输入映射到3D运动，并优化用户舒适度和降低认知负荷，该系统在仿真和实际部署中均表现出优异的无碰撞导航能力、智能方向调整和更好的平滑度。值得注意的是，这是首次将强化学习应用于全向移动平台共享控制的真实世界案例。", "keywords": "共享控制, 全向轮椅, 强化学习, 机器人导航, 仿真到真实迁移", "comments": "本文的创新点在于首次将强化学习应用于全向轮椅的共享控制，并成功实现了从仿真到真实世界的迁移。这对于提升全向移动平台的可用性和用户体验具有重要意义。该方法通过优化用户舒适度和降低认知负荷，解决了现有方案的痛点，为智能轮椅的发展提供了新思路。"}}
{"id": "2507.16829", "title": "You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control", "authors": ["Giovanni De Toni", "Erasmo Purificato", "Emilia Gómez", "Bruno Lepri", "Andrea Passerini", "Cristian Consonni"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at the 19th ACM Conference on Recommender Systems (RecSys 2025)", "url": "http://arxiv.org/abs/2507.16829v1", "summary": "Recommenders are significantly shaping online information consumption. While\neffective at personalizing content, these systems increasingly face criticism\nfor propagating irrelevant, unwanted, and even harmful recommendations. Such\ncontent degrades user satisfaction and contributes to significant societal\nissues, including misinformation, radicalization, and erosion of user trust.\nAlthough platforms offer mechanisms to mitigate exposure to undesired content,\nthese mechanisms are often insufficiently effective and slow to adapt to users'\nfeedback. This paper introduces an intuitive, model-agnostic, and\ndistribution-free method that uses conformal risk control to provably bound\nunwanted content in personalized recommendations by leveraging simple binary\nfeedback on items. We also address a limitation of traditional conformal risk\ncontrol approaches, i.e., the fact that the recommender can provide a smaller\nset of recommended items, by leveraging implicit feedback on consumed items to\nexpand the recommendation set while ensuring robust risk mitigation. Our\nexperimental evaluation on data coming from a popular online video-sharing\nplatform demonstrates that our approach ensures an effective and controllable\nreduction of unwanted recommendations with minimal effort. The source code is\navailable here: https://github.com/geektoni/mitigating-harm-recsys.", "comment": "Accepted at the 19th ACM Conference on Recommender Systems (RecSys\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.16829v1", "cate": "cs.IR", "date": "2025-07-09", "updated": "2025-07-09", "AI": {"title_translation": "你没有给我带来鲜花：通过保形风险控制减轻不必要的推荐", "tldr": "推荐系统常产生不必要内容。本文提出一种模型无关的方法，利用保形风险控制可证明地限制不必要的推荐，并在真实数据上证明其有效性。", "motivation": "推荐系统在个性化内容方面虽然有效，但因传播不相关、不必要甚至有害的推荐内容而日益受到批评。此类内容会降低用户满意度，并导致严重的社会问题，包括错误信息、激进化和用户信任受损。尽管平台提供了减轻 undesired 内容暴露的机制，但这些机制往往不够有效，且适应用户反馈的速度缓慢。", "method": "本文引入了一种直观、模型无关且无分布限制的方法，该方法利用保形风险控制，通过利用简单的项目二元反馈，可证明地限制个性化推荐中的不必要内容。该方法还通过利用已消费项目的隐式反馈来扩展推荐集，同时确保稳健的风险缓解，从而解决了传统保形风险控制方法（即推荐器可以提供较小推荐集）的局限性。", "result": "在来自一个流行在线视频分享平台的数据进行的实验评估表明，该方法以最小的努力确保了不必要推荐的有效和可控的减少。", "conclusion": "所提出的方法能够以最小的努力有效且可控地减少不必要的推荐，解决了推荐系统中的一个关键问题。", "translation": "推荐系统正在显著影响在线信息消费。尽管在个性化内容方面表现出色，但这些系统因传播不相关、不必要甚至有害的推荐而日益受到批评。此类内容会降低用户满意度，并导致严重的社会问题，包括错误信息、激进化和用户信任受损。尽管平台提供了减轻 undesired 内容暴露的机制，但这些机制往往不够有效，且适应用户反馈的速度缓慢。本文引入了一种直观、模型无关且无分布限制的方法，该方法利用保形风险控制，通过利用简单的项目二元反馈，可证明地限制个性化推荐中的不必要内容。我们还通过利用已消费项目的隐式反馈来扩展推荐集，同时确保稳健的风险缓解，从而解决了传统保形风险控制方法（即推荐器可以提供较小推荐集）的局限性。我们在来自一个流行在线视频分享平台的数据进行的实验评估表明，我们的方法以最小的努力确保了不必要推荐的有效和可控的减少。源代码可在此处获取：https://github.com/geektoni/mitigating-harm-recsys。", "summary": "推荐系统经常产生不必要内容，对用户体验和社会福祉产生负面影响。本文提出了一种新颖、模型无关的方法，利用保形风险控制可证明地限制不必要的推荐。通过结合二元和隐式反馈，该方法有效地扩展了推荐集，同时确保了稳健的风险缓解。在视频平台上的实验证明了其在减少不必要推荐方面的有效性。", "keywords": "推荐系统, 不必要推荐, 保形风险控制, 用户反馈, 错误信息", "comments": "该论文的创新之处在于将保形风险控制这一可证明稳健的方法应用于减轻不必要的推荐，解决了推荐系统中的一个重要实际和社会问题。其模型无关和无分布限制的特性使其具有广泛的适用性。"}}
{"id": "2507.17291", "title": "Integrating Belief Domains into Probabilistic Logic Programs", "authors": ["Damiano Azzolini", "Fabrizio Riguzzi", "Theresa Swift"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Under consideration in Theory and Practice of Logic Programming (TPLP)", "url": "http://arxiv.org/abs/2507.17291v1", "summary": "Probabilistic Logic Programming (PLP) under the Distribution Semantics is a\nleading approach to practical reasoning under uncertainty. An advantage of the\nDistribution Semantics is its suitability for implementation as a Prolog or\nPython library, available through two well-maintained implementations, namely\nProbLog and cplint/PITA. However, current formulations of the Distribution\nSemantics use point-probabilities, making it difficult to express epistemic\nuncertainty, such as arises from, for example, hierarchical classifications\nfrom computer vision models. Belief functions generalize probability measures\nas non-additive capacities, and address epistemic uncertainty via interval\nprobabilities. This paper introduces interval-based Capacity Logic Programs\nbased on an extension of the Distribution Semantics to include belief\nfunctions, and describes properties of the new framework that make it amenable\nto practical applications.", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "pdf_url": "http://arxiv.org/pdf/2507.17291v1", "cate": "cs.LO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "将信念域集成到概率逻辑程序中", "tldr": "本文通过将信念函数集成到分布语义中，引入了基于区间的容量逻辑程序，以解决概率逻辑程序中难以表达认知不确定性的问题。", "motivation": "当前的概率逻辑程序（PLP）在分布语义下使用点概率，难以表达认知不确定性，例如来自计算机视觉模型的层次分类。", "method": "本文通过将分布语义扩展到包含信念函数，引入了基于区间的容量逻辑程序。", "result": "描述了新框架的特性，使其适用于实际应用。", "conclusion": "通过引入基于区间的容量逻辑程序，并将其与信念函数结合，可以有效地解决概率逻辑程序中认知不确定性的表达问题，并使其更具实用性。", "translation": "概率逻辑编程（PLP）在分布语义下是处理不确定性实用推理的一种领先方法。分布语义的一个优点是其适合作为Prolog或Python库实现，通过ProbLog和cplint/PITA这两个维护良好的实现可用。然而，当前分布语义的公式使用点概率，使得难以表达认知不确定性，例如来自计算机视觉模型的层次分类。信念函数将概率度量推广为非加性容量，并通过区间概率解决认知不确定性。本文引入了基于区间的容量逻辑程序，它基于分布语义的扩展以包含信念函数，并描述了新框架的特性，使其适用于实际应用。", "summary": "本文针对概率逻辑程序（PLP）在分布语义下难以表达认知不确定性的问题，提出了一种新的解决方案。通过将分布语义扩展以包含信念函数，引入了基于区间的容量逻辑程序。这种新框架利用区间概率来处理认知不确定性，并被证明具有适用于实际应用的特性。", "keywords": "概率逻辑编程, 信念函数, 认知不确定性, 分布语义, 区间概率", "comments": "这篇论文通过将信念函数引入到概率逻辑编程中，创新性地解决了传统PLP在处理认知不确定性方面的局限性。通过使用区间概率，它提供了一种更灵活和鲁棒的方式来建模不确定性，这对于需要处理模糊或不完整信息的实际应用（如计算机视觉）具有重要意义。"}}
{"id": "2507.17746", "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains", "authors": ["Anisha Gunjal", "Anthony Wang", "Elaine Lau", "Vaskar Nath", "Bing Liu", "Sean Hendryx"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17746v1", "summary": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17746v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "评分标准作为奖励：超越可验证领域的强化学习", "tldr": "引入“评分标准作为奖励”（RaR），一个使用结构化评分标准作为可解释奖励信号的框架，用于解决真实世界任务中强化学习的奖励定义问题，并在HealthBench-1k上取得了显著提升。", "motivation": "将强化学习扩展到真实世界任务时，常常需要平衡客观和主观评估标准。然而，许多此类任务缺乏单一、明确的“真值”，难以定义可靠的奖励信号。传统的基于偏好的方法依赖于不透明的奖励函数，难以解释且容易产生虚假关联。", "method": "本文引入了“评分标准作为奖励”（RaR）框架，该框架使用结构化、清单式的评分标准作为可解释的奖励信号，用于GRPO的在线策略训练。", "result": "与简单的基于Likert的方法相比，最佳的RaR方法在HealthBench-1k上取得了高达28%的相对改进，同时与专家编写的参考资料衍生的奖励信号的性能相当或超越。", "conclusion": "通过将评分标准视为结构化奖励信号，RaR使小规模的判断模型能够更好地与人类偏好保持一致，并在不同模型规模下保持稳健的性能。", "translation": "将强化学习与可验证奖励（RLVR）扩展到真实世界任务通常需要平衡客观和主观评估标准。然而，许多此类任务缺乏单一、明确的真实性——这使得难以定义用于训练后语言模型的可靠奖励信号。虽然传统的基于偏好的方法提供了一种变通方案，但它们依赖于不透明的奖励函数，这些函数难以解释且容易产生虚假关联。我们引入了“评分标准作为奖励”（RaR），这是一个使用结构化、清单式评分标准作为可解释奖励信号的框架，用于GRPO的在线策略训练。我们最好的RaR方法在HealthBench-1k上比简单的基于Likert的方法取得了高达28%的相对改进，同时与专家编写的参考资料衍生的奖励信号的性能相当或超越。通过将评分标准视为结构化奖励信号，我们表明RaR使小规模的判断模型能够更好地与人类偏好保持一致，并在不同模型规模下保持稳健的性能。", "summary": "本论文提出了“评分标准作为奖励”（RaR）框架，旨在解决真实世界任务中强化学习奖励信号定义困难的问题。针对传统方法依赖不透明奖励函数的问题，RaR利用结构化、清单式的评分标准作为可解释的奖励信号进行在线策略训练。实验结果表明，RaR在HealthBench-1k数据集上比传统Likert方法有显著提升（28%），并能匹配或超越专家参考标准，从而使小型判断模型更好地与人类偏好对齐并保持鲁棒性能。", "keywords": "强化学习, 奖励信号, 评分标准, 可解释性, 人类偏好", "comments": "该论文提出了一种新颖且有前景的方法，通过利用结构化评分标准作为强化学习的奖励信号，有效解决了真实世界任务中奖励信号难以定义和解释的问题。其创新点在于将人类可理解的评估标准转化为机器可用的奖励，提高了模型的可解释性和对齐性。这项工作对于将强化学习应用于更广泛、更主观的领域具有重要意义。"}}
{"id": "2404.11944", "title": "Trusted Multi-view Learning under Noisy Supervision", "authors": ["Yilin Zhang", "Cai Xu", "Han Jiang", "Ziyu Guan", "Wei Zhao", "Xiaofei He", "Murat Sensoy"], "categories": ["cs.LG", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      v2 12 pages, accepted at IJCAI 2024; v3 is currently under review", "url": "http://arxiv.org/abs/2404.11944v3", "summary": "Multi-view learning methods often focus on improving decision accuracy while\nneglecting the decision uncertainty, which significantly restricts their\napplications in safety-critical scenarios. To address this, trusted multi-view\nlearning methods estimate prediction uncertainties by learning class\ndistributions from each instance. However, these methods heavily rely on high\nquality ground-truth labels. This motivates us to delve into a new problem: how\nto develop a reliable multi-view learning model under the guidance of noisy\nlabels? We propose the Trusted Multi view Noise Refining (TMNR) method to\naddress this challenge by modeling label noise arising from low-quality data\nfeatures and easily-confused classes. TMNR employs evidential deep neural\nnetworks to construct view-specific opinions that capture both beliefs and\nuncertainty. These opinions are then transformed through noise correlation\nmatrices to align with the noisy supervision, where matrix elements are\nconstrained by sample uncertainty to reflect label reliability. Furthermore,\nconsidering the challenge of jointly optimizing the evidence network and noise\ncorrelation matrices under noisy supervision, we further propose Trusted\nMulti-view Noise Re-Refining (TMNR^2 ), which disentangles this complex\nco-training problem by establishing different training objectives for distinct\nmodules. TMNR^2 identifies potentially mislabeled samples through\nevidence-label consistency and generates pseudo-labels from neighboring\ninformation. By assigning clean samples to optimize evidential networks and\nnoisy samples to guide noise correlation matrices, respectively, TMNR^2 reduces\nmapping interference and achieves stabilizes training. Experimental results\ndemonstrate that TMNR^2 significantly outperforms baseline methods, with\naverage accuracy improvements of 7% on datasets with 50% label noise.", "comment": "v2 12 pages, accepted at IJCAI 2024; v3 is currently under review", "pdf_url": "http://arxiv.org/pdf/2404.11944v3", "cate": "cs.LG", "date": "2024-04-18", "updated": "2025-07-23", "AI": {"title_translation": "噪声监督下的可信多视图学习", "tldr": "本文提出了TMNR和TMNR^2方法，旨在解决噪声标签下可信多视图学习的问题，通过建模噪声并稳定训练，显著提升了决策准确性。", "motivation": "多视图学习方法通常侧重于提高决策准确性而忽略决策不确定性，这严重限制了它们在安全关键场景中的应用。现有的可信多视图学习方法高度依赖高质量的真实标签。因此，本文旨在解决如何在噪声标签指导下开发可靠的多视图学习模型的问题。", "method": "本文提出了两种方法：\n1. 可信多视图噪声提炼（TMNR）方法：通过建模源于低质量数据特征和易混淆类别的标签噪声来解决挑战。TMNR采用证据深度神经网络构建视图特定意见，捕获信念和不确定性。这些意见通过噪声相关矩阵转换以与噪声监督对齐，其中矩阵元素受样本不确定性约束以反映标签可靠性。\n2. 可信多视图噪声再提炼（TMNR^2）方法：为解决在噪声监督下联合优化证据网络和噪声相关矩阵的挑战，TMNR^2通过为不同模块建立不同的训练目标来解耦这个复杂的协同训练问题。TMNR^2通过证据-标签一致性识别潜在的错误标记样本，并从邻近信息生成伪标签。通过将干净样本分配给证据网络优化，将噪声样本分配给噪声相关矩阵指导，TMNR^2减少了映射干扰并实现了训练稳定。", "result": "实验结果表明，TMNR^2显著优于基线方法，在标签噪声为50%的数据集上，平均准确率提高了7%。", "conclusion": "本文提出的TMNR^2方法有效解决了噪声监督下可信多视图学习的挑战，显著提高了模型的准确性和训练稳定性。", "translation": "多视图学习方法通常侧重于提高决策准确性而忽略决策不确定性，这严重限制了它们在安全关键场景中的应用。为了解决这个问题，可信多视图学习方法通过学习每个实例的类别分布来估计预测不确定性。然而，这些方法高度依赖高质量的真实标签。这促使我们深入研究一个新问题：如何在噪声标签的指导下开发可靠的多视图学习模型？我们提出了可信多视图噪声提炼（TMNR）方法来解决这一挑战，通过建模源于低质量数据特征和易混淆类别的标签噪声。TMNR采用证据深度神经网络构建视图特定意见，捕获信念和不确定性。这些意见随后通过噪声相关矩阵进行转换，以与噪声监督对齐，其中矩阵元素受样本不确定性约束以反映标签可靠性。此外，考虑到在噪声监督下联合优化证据网络和噪声相关矩阵的挑战，我们进一步提出了可信多视图噪声再提炼（TMNR^2），它通过为不同模块建立不同的训练目标来解耦这个复杂的协同训练问题。TMNR^2通过证据-标签一致性识别潜在的错误标记样本，并从邻近信息生成伪标签。通过将干净样本分配给证据网络优化，将噪声样本分配给噪声相关矩阵指导，TMNR^2减少了映射干扰并实现了训练稳定。实验结果表明，TMNR^2显著优于基线方法，在标签噪声为50%的数据集上，平均准确率提高了7%。", "summary": "本文针对噪声监督下可信多视图学习中现有方法依赖高质量标签的问题，提出了两种新方法：可信多视图噪声提炼（TMNR）和可信多视图噪声再提炼（TMNR^2）。TMNR通过证据深度神经网络建模视图特定意见和噪声相关矩阵来处理标签噪声和不确定性。TMNR^2进一步优化了这一过程，通过解耦证据网络和噪声相关矩阵的联合优化，识别错误标记样本并生成伪标签，从而稳定训练并减少干扰。实验证明，TMNR^2在存在高噪声的场景下，其性能显著优于基线方法。", "keywords": "多视图学习, 噪声监督, 不确定性, 可信学习, 证据深度学习", "comments": "本文的创新点在于解决了多视图学习在安全关键场景中受噪声标签限制的问题，特别是通过引入证据深度网络来量化不确定性，并提出TMNR^2来有效处理复杂噪声环境下的模型训练。TMNR^2通过解耦训练目标和利用证据-标签一致性识别噪声样本，提供了一种新颖且实用的方法来提高模型的鲁棒性和准确性，这对于实际应用具有重要意义。"}}
{"id": "2507.17258", "title": "Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?", "authors": ["Andreas Scholl", "Natalie Kiesler"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at PPIG 2025", "url": "http://arxiv.org/abs/2507.17258v1", "summary": "Building on prior research on Generative AI (GenAI) and related tools for\nprogramming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,\nto support novice learners. SCRIPT allows for open-ended interactions and\nstructured guidance through predefined prompts. We evaluated the tool via an\nexperiment with 136 students from an introductory programming course at a large\nGerman university and analyzed how students interacted with SCRIPT while\nsolving programming tasks with a focus on their feedback preferences. The\nresults reveal that students' feedback requests seem to follow a specific\nsequence. Moreover, the chatbot responses aligned well with students' requested\nfeedback types (in 75%), and it adhered to the system prompt constraints. These\ninsights inform the design of GenAI-based learning support systems and\nhighlight challenges in balancing guidance and flexibility in AI-assisted\ntools.", "comment": "Accepted at PPIG 2025", "pdf_url": "http://arxiv.org/pdf/2507.17258v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "学生反馈请求与SCRIPT聊天机器人交互：他们得到了想要的吗？", "tldr": "本研究开发了基于ChatGPT-4o-mini的SCRIPT聊天机器人，以支持编程初学者。实验结果显示，学生反馈请求遵循特定顺序，且聊天机器人75%的回复与学生请求的反馈类型一致，并遵守了系统提示约束。这些发现为GenAI学习支持系统的设计提供了见解。", "motivation": "基于先前关于生成式AI（GenAI）及其在编程教育中相关工具的研究，本研究开发了SCRIPT聊天机器人以支持编程初学者。", "method": "开发了基于ChatGPT-4o-mini的SCRIPT聊天机器人，允许开放式交互和通过预定义提示进行的结构化指导。通过一项包含136名德国大型大学入门编程课程学生的实验对该工具进行了评估，并分析了学生在解决编程任务时与SCRIPT的交互，重点关注他们的反馈偏好。", "result": "结果显示，学生的反馈请求似乎遵循特定顺序。此外，聊天机器人的回复与学生请求的反馈类型高度一致（75%），并且遵守了系统提示约束。", "conclusion": "这些见解为基于GenAI的学习支持系统的设计提供了信息，并强调了在AI辅助工具中平衡指导和灵活性的挑战。", "translation": "基于先前关于生成式AI (GenAI) 及其在编程教育中相关工具的研究，我们开发了SCRIPT，一个基于ChatGPT-4o-mini的聊天机器人，以支持编程初学者。SCRIPT允许开放式交互并通过预定义提示提供结构化指导。我们通过一项在德国一所大型大学的入门编程课程中进行的包含136名学生的实验对该工具进行了评估，并分析了学生在解决编程任务时与SCRIPT的交互，重点关注他们的反馈偏好。结果显示，学生的反馈请求似乎遵循特定顺序。此外，聊天机器人的回复与学生请求的反馈类型高度一致（75%），并且遵守了系统提示约束。这些见解为基于GenAI的学习支持系统的设计提供了信息，并强调了在AI辅助工具中平衡指导和灵活性的挑战。", "summary": "本研究开发了SCRIPT，一个基于ChatGPT-4o-mini的聊天机器人，旨在支持编程初学者。该工具通过一项涉及136名学生的实验进行评估，重点分析了学生在编程任务中与SCRIPT的交互及其反馈偏好。研究发现，学生的反馈请求呈现特定序列，且SCRIPT的响应在75%的情况下与学生的请求类型一致，并遵循了预设约束。这些发现为设计未来的GenAI学习系统提供了实践指导，并指出了在AI辅助学习中平衡指导与灵活性的重要性。", "keywords": "GenAI, 编程教育, 聊天机器人, 学生反馈, SCRIPT", "comments": "该论文的创新之处在于开发并评估了一个专门用于编程教育的GenAI聊天机器人SCRIPT，并深入分析了学生与AI的交互模式和反馈请求偏好。其重要性在于为未来GenAI学习支持系统的设计提供了实证见解，尤其是在平衡AI指导和用户灵活性的方面。研究结果表明AI在满足学生反馈需求方面具有潜力，但也暗示了仍有25%的提升空间，这可能是未来研究的限制或方向。"}}
{"id": "2507.17107", "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models", "authors": ["Andrii Balashov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17107v1", "summary": "Reinforcement learning (RL) is a key post-pretraining step for aligning large\nlanguage models (LLMs) with complex tasks and human preferences. While it is\noften assumed that RL fine-tuning requires updating most of a model's\nparameters, we challenge this assumption with a surprising finding: RL\nfine-tuning consistently modifies only a small subnetwork (typically 5-30% of\nweights), leaving most parameters unchanged. We call this phenomenon RL-induced\nparameter update sparsity. It arises naturally, without any sparsity\nconstraints or parameter-efficient tuning, and appears across multiple RL\nalgorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,\nMeta, and open-source LLMs). Moreover, the subnetworks updated by RL show\nsubstantial overlap across different seeds, datasets, and algorithms-far\nexceeding chance-suggesting a partially transferable structure in the\npretrained model. We show that fine-tuning only this sparse subnetwork recovers\nfull model performance and yields parameters nearly identical to the fully\nfine-tuned model. Our analysis suggests this sparsity emerges because RL\noperates near the model's original distribution, requiring only targeted\nchanges. KL penalties, gradient clipping, and on-policy dynamics have limited\neffect on the sparsity pattern. These findings shed new light on how RL adapts\nmodels: not by shifting all weights, but by focusing training on a small,\nconsistently updated subnetwork. This insight enables more efficient RL methods\nand reframes sparsity through the lens of the lottery ticket hypothesis.", "comment": "16 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17107v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "强化学习在大型语言模型中微调稀疏子网络", "tldr": "研究发现，强化学习（RL）微调大型语言模型（LLMs）时，只更新模型参数的一小部分（5-30%），而非全部，这种稀疏更新现象自然发生，并且这些稀疏子网络在不同设置下具有重叠性，仅微调这些子网络即可恢复完整模型性能。", "motivation": "挑战了强化学习（RL）微调大型语言模型（LLMs）需要更新大部分模型参数的普遍假设。", "method": "通过在多种RL算法（如PPO, DPO, SimPO, PRIME）和模型家族（如OpenAI, Meta, 开源LLMs）上进行实验，观察到RL微调过程中参数更新的稀疏性。分析了这种稀疏性产生的原因，并验证了仅微调稀疏子网络的效果。", "result": "RL微调始终只修改一小部分子网络（通常为5-30%的权重），这种RL诱导的参数更新稀疏性是自然发生的，无需任何稀疏性约束或参数高效微调。此外，RL更新的子网络在不同种子、数据集和算法之间显示出显著的重叠。仅微调这个稀疏子网络即可恢复完整的模型性能，并且参数与完全微调的模型几乎相同。分析表明这种稀疏性出现是因为RL在模型原始分布附近操作，只需要有针对性的改变。KL惩罚、梯度裁剪和在策略动态对稀疏模式的影响有限。", "conclusion": "强化学习（RL）适应模型的方式不是通过改变所有权重，而是通过将训练集中在一个小而持续更新的子网络上。这一发现为更高效的RL方法提供了可能，并从彩票假说的角度重新审视了稀疏性。", "translation": "强化学习（RL）是大型语言模型（LLMs）与复杂任务和人类偏好对齐的关键后预训练步骤。虽然通常认为RL微调需要更新模型的大部分参数，但我们通过一个令人惊讶的发现挑战了这一假设：RL微调始终只修改一小部分子网络（通常为5-30%的权重），而大部分参数保持不变。我们称之为RL诱导的参数更新稀疏性。它自然产生，无需任何稀疏性约束或参数高效微调，并且出现在多种RL算法（例如PPO、DPO、SimPO、PRIME）和模型家族（例如OpenAI、Meta和开源LLMs）中。此外，RL更新的子网络在不同种子、数据集和算法之间显示出显著的重叠——远超偶然——这表明预训练模型中存在部分可迁移的结构。我们表明，仅微调这个稀疏子网络即可恢复完整的模型性能，并且参数与完全微调的模型几乎相同。我们的分析表明，这种稀疏性出现是因为RL在模型的原始分布附近操作，只需要有针对性的改变。KL惩罚、梯度裁剪和在策略动态对稀疏模式的影响有限。这些发现为RL如何适应模型提供了新的视角：不是通过改变所有权重，而是通过将训练集中在一个小而持续更新的子网络上。这一洞察力使得更高效的RL方法成为可能，并从彩票假说的角度重新审视了稀疏性。", "summary": "本文挑战了强化学习（RL）微调大型语言模型（LLMs）需要更新大部分参数的普遍认知。研究发现，RL微调实际上只修改了模型中一小部分（5-30%）的稀疏子网络，这种现象自然发生，且在不同RL算法和模型家族中普遍存在。这些被更新的稀疏子网络在不同设置下表现出显著重叠。重要的是，仅微调这些稀疏子网络就能实现与完全微调相同的性能。研究指出，这种稀疏性是由于RL在模型原始分布附近进行微调，仅需进行有针对性的改变。这一发现为开发更高效的RL微调方法提供了新思路，并为彩票假说提供了新的视角。", "keywords": "强化学习, 大型语言模型, 微调, 稀疏性, 子网络", "comments": "这项研究揭示了强化学习在大型语言模型微调中一个出人意料且重要的现象——参数更新的稀疏性。其创新之处在于，它挑战了RL微调需要大量参数更新的传统观念，并首次发现这种稀疏性是自然发生的，无需额外约束。其重要性在于，它不仅解释了RL如何高效地调整模型，还为开发更节能、更高效的RL微调方法提供了理论基础。将这种稀疏性与“彩票假说”联系起来，也为未来研究提供了新的方向。"}}
{"id": "2507.17348", "title": "TOC-UCO: a comprehensive repository of tabular ordinal classification datasets", "authors": ["Rafael Ayllón-Gavilán", "David Guijo-Rubio", "Antonio Manuel Gómez-Orellana", "David Guijo-Rubio", "Francisco Bérchez-Moreno", "Víctor Manuel Vargas-Yun", "Pedro A. Gutiérrez"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 single column pages, 5 figures, 7 tables", "url": "http://arxiv.org/abs/2507.17348v1", "summary": "An ordinal classification (OC) problem corresponds to a special type of\nclassification characterised by the presence of a natural order relationship\namong the classes. This type of problem can be found in a number of real-world\napplications, motivating the design and development of many ordinal\nmethodologies over the last years. However, it is important to highlight that\nthe development of the OC field suffers from one main disadvantage: the lack of\na comprehensive set of datasets on which novel approaches to the literature\nhave to be benchmarked. In order to approach this objective, this manuscript\nfrom the University of C\\'ordoba (UCO), which have previous experience on the\nOC field, provides the literature with a publicly available repository of\ntabular data for a robust validation of novel OC approaches, namely TOC-UCO\n(Tabular Ordinal Classification repository of the UCO). Specifically, this\nrepository includes a set of $46$ tabular ordinal datasets, preprocessed under\na common framework and ensured to have a reasonable number of patterns and an\nappropriate class distribution. We also provide the sources and preprocessing\nsteps of each dataset, along with details on how to benchmark a novel approach\nusing the TOC-UCO repository. For this, indices for $30$ different randomised\ntrain-test partitions are provided to facilitate the reproducibility of the\nexperiments.", "comment": "25 single column pages, 5 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.17348v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "TOC-UCO：一个综合的表格序数分类数据集存储库", "tldr": "TOC-UCO是一个新的表格序数分类数据集存储库，旨在解决现有数据集缺乏的问题，促进新方法的基准测试和可复现性。", "motivation": "序数分类（OC）领域的发展面临一个主要劣势：缺乏一个全面的数据集，用于对文献中的新方法进行基准测试。", "method": "本手稿提供了一个名为TOC-UCO的公开可用的表格数据存储库，其中包含46个表格序数数据集。这些数据集经过通用框架预处理，确保有合理的模式数量和适当的类别分布。同时提供了每个数据集的来源、预处理步骤以及如何使用TOC-UCO进行基准测试的详细信息，包括30个不同随机训练-测试分区的索引，以方便实验的可复现性。", "result": "创建了一个名为TOC-UCO的综合表格序数分类数据集存储库，其中包含46个经过统一预处理、具有合理模式数量和适当类别分布的数据集。同时提供了数据集来源、预处理步骤和用于基准测试的30个随机训练-测试分区索引。", "conclusion": "TOC-UCO存储库为新型序数分类方法的稳健验证提供了支持，并通过提供预定义的训练-测试分区索引，极大地促进了实验的可复现性，从而解决了该领域数据集缺乏的问题。", "translation": "序数分类（OC）问题是一种特殊类型的分类问题，其特点是类别之间存在自然的顺序关系。这类问题存在于许多现实世界的应用中，这促使近年来设计和开发了许多序数方法。然而，重要的是要强调，OC领域的发展面临一个主要劣势：缺乏一个全面的数据集，用于对文献中的新方法进行基准测试。为了实现这一目标，这份来自科尔多瓦大学（UCO）的手稿，该大学在OC领域拥有丰富的经验，为文献提供了一个公开可用的表格数据存储库，用于新型OC方法的稳健验证，即TOC-UCO（UCO的表格序数分类存储库）。具体而言，该存储库包含46个表格序数数据集，这些数据集在通用框架下进行预处理，并确保具有合理的模式数量和适当的类别分布。我们还提供了每个数据集的来源和预处理步骤，以及如何使用TOC-UCO存储库对新方法进行基准测试的详细信息。为此，提供了30个不同随机训练-测试分区的索引，以方便实验的可复现性。", "summary": "本论文介绍了TOC-UCO，一个由科尔多瓦大学开发的综合表格序数分类（OC）数据集存储库。该存储库旨在解决OC领域缺乏统一基准测试数据集的痛点。TOC-UCO包含了46个经过标准化预处理的表格OC数据集，这些数据集具有充足的模式数量和适当的类别分布。为了确保实验的可复现性，TOC-UCO还提供了每个数据集的来源、预处理步骤以及30个预定义的随机训练-测试分区索引，从而为OC新方法的稳健验证提供了重要的资源。", "keywords": "序数分类, 数据集存储库, 表格数据, 基准测试, 可复现性", "comments": "这项工作通过提供一个统一且全面的表格序数分类数据集存储库，解决了该领域长期存在的数据集碎片化和缺乏标准基准的问题。其创新之处在于不仅收集了大量数据集，还对它们进行了标准化预处理，并提供了详细的元数据和预定义的训练-测试分区，这对于确保研究成果的可复现性和促进新方法的公平比较至关重要。这对于推动序数分类领域的研究进展具有重要意义。"}}
{"id": "2507.17462", "title": "ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents", "authors": ["Chang Nie", "Guangming Wang", "Zhe Lie", "Hesheng Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17462v1", "summary": "Robot imitation learning relies on 4D multi-view sequential images. However,\nthe high cost of data collection and the scarcity of high-quality data severely\nconstrain the generalization and application of embodied intelligence policies\nlike Vision-Language-Action (VLA) models. Data augmentation is a powerful\nstrategy to overcome data scarcity, but methods for editing 4D multi-view\nsequential images for manipulation tasks are currently lacking. Thus, we\npropose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation\nframework that efficiently edits an entire multi-view sequence based on\nsingle-frame editing and robot state conditions. This task presents three core\nchallenges: (1) maintaining geometric and appearance consistency across dynamic\nviews and long time horizons; (2) expanding the working window with low\ncomputational costs; and (3) ensuring the semantic integrity of critical\nobjects like the robot arm. ERMV addresses these challenges through a series of\ninnovations. First, to ensure spatio-temporal consistency in motion blur, we\nintroduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that\nlearns pixel shift caused by movement before applying geometric constraints.\nSecond, to maximize the editing working window, ERMV pioneers a Sparse\nSpatio-Temporal (STT) module, which decouples the temporal and spatial views\nand remodels a single-frame multi-view problem through sparse sampling of the\nviews to reduce computational demands. Third, to alleviate error accumulation,\nwe incorporate a feedback intervention Mechanism, which uses a Multimodal Large\nLanguage Model (MLLM) to check editing inconsistencies and request targeted\nexpert guidance only when necessary. Extensive experiments demonstrate that\nERMV-augmented data significantly boosts the robustness and generalization of\nVLA models in both simulated and real-world environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17462v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "ERMV：编辑4D机器人多视角图像以增强具身智能体", "tldr": "ERMV是一个新颖的数据增强框架，通过高效编辑4D机器人多视角序列图像来解决数据稀缺问题，从而显著提升VLA模型的鲁棒性和泛化能力。", "motivation": "机器人模仿学习依赖4D多视角序列图像，但高昂的数据收集成本和高质量数据稀缺严重限制了具身智能策略（如VLA模型）的泛化和应用。目前缺乏用于操作任务的4D多视角序列图像编辑方法。", "method": "提出ERMV（Editing Robotic Multi-View 4D data），一个数据增强框架，基于单帧编辑和机器人状态条件高效编辑整个多视角序列。它通过以下创新解决挑战：1. 引入Epipolar Motion-Aware Attention (EMA-Attn) 机制，学习运动引起的像素偏移，确保运动模糊中的时空一致性。2. 开创Sparse Spatio-Temporal (STT) 模块，解耦时间视图和空间视图，通过稀疏采样视图将单帧多视角问题重新建模，以降低计算成本。3. 整合反馈干预机制，使用多模态大型语言模型（MLLM）检查编辑不一致性，并在必要时请求专家指导，以减轻错误累积。", "result": "经过ERMV增强的数据显著提升了VLA模型在模拟和真实世界环境中的鲁棒性和泛化能力。", "conclusion": "ERMV有效解决了机器人模仿学习中4D多视角数据稀缺的问题，通过其创新的编辑框架，显著增强了具身智能体的性能。", "translation": "机器人模仿学习依赖于4D多视角序列图像。然而，数据收集的高成本和高质量数据的稀缺严重限制了具身智能策略（如视觉-语言-动作（VLA）模型）的泛化和应用。数据增强是克服数据稀缺的强大策略，但目前缺乏用于操作任务的4D多视角序列图像编辑方法。因此，我们提出了ERMV（编辑机器人多视角4D数据），一个新颖的数据增强框架，它基于单帧编辑和机器人状态条件高效编辑整个多视角序列。这项任务提出了三个核心挑战：(1) 在动态视图和长时间范围内保持几何和外观一致性；(2) 以低计算成本扩展工作窗口；(3) 确保机器人手臂等关键对象的语义完整性。ERMV通过一系列创新解决了这些挑战。首先，为了确保运动模糊中的时空一致性，我们引入了一种新颖的极线运动感知注意力（EMA-Attn）机制，该机制在应用几何约束之前学习由运动引起的像素偏移。其次，为了最大化编辑工作窗口，ERMV开创了一个稀疏时空（STT）模块，该模块解耦了时间和空间视图，并通过视图的稀疏采样将单帧多视角问题重新建模，以降低计算需求。第三，为了减轻错误累积，我们整合了一个反馈干预机制，该机制使用多模态大型语言模型（MLLM）检查编辑不一致性，并仅在必要时请求有针对性的专家指导。大量实验表明，经过ERMV增强的数据显著提升了VLA模型在模拟和真实世界环境中的鲁棒性和泛化能力。", "summary": "本文提出了ERMV，一个新颖的4D机器人多视角图像数据增强框架，旨在解决机器人模仿学习中数据稀缺和高质量数据获取成本高的问题。ERMV通过创新的Epipolar Motion-Aware Attention机制、Sparse Spatio-Temporal模块和基于MLLM的反馈干预机制，有效应对了跨视图一致性、计算效率和语义完整性等挑战。实验证明，ERMV增强的数据能显著提升VLA模型在模拟和真实环境中的鲁棒性和泛化能力。", "keywords": "4D数据增强, 机器人模仿学习, 多视角图像编辑, 具身智能体, VLA模型", "comments": "ERMV的创新之处在于其针对4D多视角图像的独特编辑框架，特别是引入了Epipolar Motion-Aware Attention和Sparse Spatio-Temporal模块来解决时空一致性和计算效率问题。此外，结合MLLM进行错误检测和专家指导的反馈机制也很有趣，为数据增强的质量控制提供了一种新颖的方法。这项工作对于推动具身智能体的泛化能力具有重要意义。"}}
{"id": "2409.19638", "title": "BadHMP: Backdoor Attack against Human Motion Prediction", "authors": ["Chaohui Xu", "Si Wang", "Chip-Hong Chang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.19638v2", "summary": "Precise future human motion prediction over sub-second horizons from past\nobservations is crucial for various safety-critical applications. To date, only\na few studies have examined the vulnerability of skeleton-based neural networks\nto evasion and backdoor attacks. In this paper, we propose BadHMP, a novel\nbackdoor attack that targets specifically human motion prediction tasks. Our\napproach involves generating poisoned training samples by embedding a localized\nbackdoor trigger in one limb of the skeleton, causing selected joints to follow\npredefined motion in historical time steps. Subsequently, the future sequences\nare globally modified that all the joints move following the target\ntrajectories. Our carefully designed backdoor triggers and targets guarantee\nthe smoothness and naturalness of the poisoned samples, making them stealthy\nenough to evade detection by the model trainer while keeping the poisoned model\nunobtrusive in terms of prediction fidelity to untainted sequences. The target\nsequences can be successfully activated by the designed input sequences even\nwith a low poisoned sample injection ratio. Experimental results on two\ndatasets (Human3.6M and CMU-Mocap) and two network architectures (LTD and HRI)\ndemonstrate the high-fidelity, effectiveness, and stealthiness of BadHMP.\nRobustness of our attack against fine-tuning defense is also verified.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.19638v2", "cate": "cs.CV", "date": "2024-09-29", "updated": "2025-07-23", "AI": {"title_translation": "BadHMP：针对人体运动预测的后门攻击", "tldr": "本文提出了BadHMP，一种专门针对人体运动预测模型的后门攻击。该攻击通过在训练数据中嵌入隐蔽触发器，使中毒模型在特定触发输入下预测出预设的异常运动轨迹，且具有高隐蔽性和鲁棒性。", "motivation": "精确的未来人体运动预测对于各种安全关键应用至关重要，然而，现有研究很少关注基于骨架的神经网络在人体运动预测任务中对规避和后门攻击的脆弱性。", "method": "BadHMP通过生成中毒训练样本来实现攻击。具体方法是在骨架的一个肢体中嵌入局部后门触发器，导致选定的关节在历史时间步中遵循预定义的运动。随后，未来的序列被全局修改，使得所有关节都按照目标轨迹移动。该方法旨在保证中毒样本的平滑性和自然性，使其能够隐蔽地逃避检测。", "result": "实验结果表明，BadHMP在Human3.6M和CMU-Mocap两个数据集以及LTD和HRI两种网络架构上都展现出高保真度、有效性和隐蔽性。即使在低中毒样本注入率下，目标序列也能被成功激活。此外，该攻击对微调防御也表现出鲁棒性。", "conclusion": "本文成功证明了人体运动预测模型对后门攻击的脆弱性，并展示了所提出的BadHMP在多种设置下的有效性、隐蔽性和对防御措施的鲁棒性。", "translation": "从过去的观测中精确预测亚秒级未来人体运动对于各种安全关键应用至关重要。迄今为止，只有少数研究检查了基于骨架的神经网络对规避和后门攻击的脆弱性。在本文中，我们提出了BadHMP，一种专门针对人体运动预测任务的新型后门攻击。我们的方法涉及通过在骨架的一个肢体中嵌入局部后门触发器来生成中毒训练样本，导致选定的关节在历史时间步中遵循预定义的运动。随后，未来序列被全局修改，所有关节都按照目标轨迹移动。我们精心设计的后门触发器和目标保证了中毒样本的平滑性和自然性，使其足够隐蔽，可以逃避模型训练者的检测，同时保持中毒模型在对未受污染序列的预测保真度方面不引人注目。即使在低中毒样本注入率下，目标序列也可以通过设计的输入序列成功激活。在两个数据集（Human3.6M和CMU-Mocap）和两种网络架构（LTD和HRI）上的实验结果证明了BadHMP的高保真度、有效性和隐蔽性。我们攻击对微调防御的鲁棒性也得到了验证。", "summary": "本文提出了BadHMP，一种针对人体运动预测任务的新型后门攻击方法。该方法通过在训练样本中嵌入局部且隐蔽的触发器，使受感染的模型在特定输入下预测出预设的异常运动轨迹。实验证明，BadHMP在不同数据集和网络架构上均表现出高保真度、有效性和隐蔽性，即使在低中毒样本注入率下也能成功激活，并且对微调防御具有鲁棒性。", "keywords": "后门攻击, 人体运动预测, 骨架神经网络, 脆弱性, 隐蔽性", "comments": "该论文创新性地将后门攻击的概念引入到人体运动预测领域，揭示了此类模型在安全关键应用中可能存在的潜在漏洞。其设计的触发器和目标能够保证中毒样本的平滑性和自然性，使得攻击具有高度隐蔽性，不易被模型训练者察觉。这项研究对于未来开发更安全、更鲁棒的人体运动预测系统具有重要的警示和指导意义。"}}
{"id": "2507.16867", "title": "Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization", "authors": ["Yunyi Zhao", "Wei Zhang", "Cheng Xiang", "Hongyang Du", "Dusit Niyato", "Shuhua Gao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures", "url": "http://arxiv.org/abs/2507.16867v1", "summary": "This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware\nreinforcement learning algorithm for intelligent operation of multi-microgrid\nsystems. With the growing integration of renewables and increasing system\ncomplexity, microgrid communities face significant challenges in real-time\nenergy scheduling and optimization under uncertainty. DiffCarl integrates a\ndiffusion model into a deep reinforcement learning (DRL) framework to enable\nadaptive energy scheduling under uncertainty and explicitly account for carbon\nemissions and operational risk. By learning action distributions through a\ndenoising generation process, DiffCarl enhances DRL policy expressiveness and\nenables carbon- and risk-aware scheduling in dynamic and uncertain microgrid\nenvironments. Extensive experimental studies demonstrate that it outperforms\nclassic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower\noperational cost. It also achieves 28.7% lower carbon emissions than those of\nits carbon-unaware variant and reduces performance variability. These results\nhighlight DiffCarl as a practical and forward-looking solution. Its flexible\ndesign allows efficient adaptation to different system configurations and\nobjectives to support real-world deployment in evolving energy systems.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16867v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于扩散模型的碳与风险感知微电网优化强化学习", "tldr": "DiffCarl是一种基于扩散模型的强化学习算法，用于多微电网系统的智能运行，旨在解决不确定性下的能源调度挑战，同时考虑碳排放和运行风险，并显著降低运营成本和碳排放。", "motivation": "随着可再生能源的日益整合和系统复杂性的增加，微电网社区在不确定性下的实时能源调度和优化面临重大挑战。", "method": "本文提出了DiffCarl算法，它将扩散模型集成到深度强化学习（DRL）框架中，以实现不确定性下的自适应能源调度，并明确考虑碳排放和运行风险。DiffCarl通过去噪生成过程学习动作分布，增强了DRL策略的表达能力，实现了动态不确定微电网环境中的碳和风险感知调度。", "result": "实验研究表明，DiffCarl优于经典算法和最先进的DRL解决方案，运营成本降低了2.3-30.1%。与未感知碳排放的变体相比，它还实现了28.7%的碳排放降低，并减少了性能变异性。", "conclusion": "DiffCarl被证明是一种实用且前瞻性的解决方案。其灵活的设计允许高效适应不同的系统配置和目标，以支持在不断发展的能源系统中的实际部署。", "translation": "本文介绍了DiffCarl，一种基于扩散模型的碳和风险感知强化学习算法，用于多微电网系统的智能运行。随着可再生能源日益整合和系统复杂性增加，微电网社区在不确定性下的实时能源调度和优化面临重大挑战。DiffCarl将扩散模型集成到深度强化学习（DRL）框架中，以实现在不确定性下的自适应能源调度，并明确考虑碳排放和运行风险。通过去噪生成过程学习动作分布，DiffCarl增强了DRL策略的表达能力，并实现了动态不确定微电网环境中的碳和风险感知调度。广泛的实验研究表明，它优于经典算法和最先进的DRL解决方案，运营成本降低了2.3-30.1%。与未感知碳排放的变体相比，它还实现了28.7%的碳排放降低，并减少了性能变异性。这些结果突出显示DiffCarl是一种实用且前瞻性的解决方案。其灵活的设计允许高效适应不同的系统配置和目标，以支持在不断发展的能源系统中的实际部署。", "summary": "本文提出了一种名为DiffCarl的基于扩散模型的强化学习算法，专为多微电网系统的智能运行设计。该算法通过将扩散模型融入深度强化学习框架，解决了可再生能源集成和系统复杂性带来的不确定性下能源调度挑战，并明确考虑碳排放和运行风险。实验证明，DiffCarl在降低运营成本（2.3-30.1%）和碳排放（28.7%）方面优于现有方法，并能减少性能变异性，展现了其在实际能源系统部署中的潜力。", "keywords": "扩散模型, 强化学习, 微电网优化, 碳感知, 风险感知", "comments": "DiffCarl的创新之处在于将扩散模型与深度强化学习相结合，以应对微电网优化中的不确定性、碳排放和风险管理。这种方法通过学习动作分布，提升了策略的表达能力，使其在复杂的能源调度问题中表现出色。其在降低成本和碳排放方面的显著效果，以及减少性能变异性的能力，使其成为一个具有重要实际应用价值的前瞻性解决方案。"}}
{"id": "2507.17718", "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17718v1", "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17718v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "AI电话调查：利用AI访谈员实现定量数据收集自动化", "tldr": "该研究提出并验证了一种基于大型语言模型、自动语音识别和语音合成技术的AI电话调查系统，用于自动化定量数据收集。实验结果表明，缩短问卷和提高AI访谈员的响应速度有助于提升问卷完成率、降低中断率并提高受访者满意度。", "motivation": "随着语音AI系统的兴起，定量调查研究人员获得了一种新的数据收集模式：AI电话调查。该方法旨在扩展定量研究规模，同时平衡类人交互性和方法学严谨性，克服了传统交互式语音应答（IVR）技术的局限性，提供更自然、适应性强的受访体验。", "method": "研究团队构建并测试了一个基于大型语言模型（LLM）、自动语音识别（ASR）和语音合成技术的AI系统，用于进行定量调查。该系统专为定量研究设计，并严格遵守问卷顺序随机化、答案顺序随机化和精确措辞等研究最佳实践。为验证系统有效性，研究人员通过SSRS舆论小组进行了两次试点调查，并随后进行了一项单独的人工调查以评估受访者体验，测量了问卷完成率、中断率和受访者满意度三项关键指标。", "result": "研究结果表明，更短的问卷和响应更快的AI访谈员有助于改善所研究的所有三项指标：问卷完成率、中断率和受访者满意度。", "conclusion": "AI电话调查是一种有前景的定量数据收集方法，通过优化问卷长度和AI访谈员的响应速度，可以显著提高数据收集效率和受访者体验。", "translation": "随着语音AI系统的兴起，定量调查研究人员获得了一种新的数据收集模式：AI电话调查。通过使用AI进行电话访谈，研究人员可以在扩展定量研究规模的同时，平衡类人交互性和方法学严谨性双重目标。与早期使用交互式语音应答（IVR）技术自动化这些调查的努力不同，语音AI能够提供更自然、适应性强的受访体验，因为它对中断、纠正和人类语音的其他特性更具鲁棒性。\n我们构建并测试了一个基于大型语言模型（LLM）、自动语音识别（ASR）和语音合成技术的AI系统，用于进行定量调查。该系统专为定量研究设计，并严格遵守问卷顺序随机化、答案顺序随机化和精确措辞等研究最佳实践。\n为验证系统有效性，我们通过SSRS舆论小组进行了两次试点调查，并随后进行了一项单独的人工调查以评估受访者体验。我们测量了三个关键指标：问卷完成率、中断率和受访者满意度。我们的结果表明，更短的问卷和响应更快的AI访谈员可能有助于改善所研究的所有三项指标。", "summary": "本文介绍了AI电话调查作为一种新型定量数据收集模式，它利用大型语言模型、自动语音识别和语音合成技术来自动化电话访谈过程。该系统旨在遵循严格的研究最佳实践，并通过试点调查进行了有效性验证。研究发现，缩短问卷长度和提高AI访谈员的响应速度，可以显著提高问卷完成率、降低中断率并提升受访者满意度。", "keywords": "AI电话调查, 定量数据收集, 大型语言模型, 调查自动化, 受访者体验", "comments": "该论文将现代AI技术（LLM、ASR、语音合成）创新性地应用于传统的电话调查方法，有效解决了大规模数据收集中的可扩展性和交互性挑战。其对方法学严谨性的强调和通过实证研究进行验证的做法，使其成为该领域的重要贡献。特别是关于“更短的问卷和响应更快的AI”的发现，为未来AI调查系统的设计和优化提供了宝贵的实践指导。"}}
{"id": "2507.17296", "title": "PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud Pretraining", "authors": ["Xuanyu Lin", "Xiaona Zeng", "Xianwei Zheng", "Xutao Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17296v1", "summary": "Mamba has recently gained widespread attention as a backbone model for point\ncloud modeling, leveraging a state-space architecture that enables efficient\nglobal sequence modeling with linear complexity. However, its lack of local\ninductive bias limits its capacity to capture fine-grained geometric structures\nin 3D data. To address this limitation, we propose \\textbf{PointLAMA}, a point\ncloud pretraining framework that combines task-aware point cloud serialization,\na hybrid encoder with integrated Latent Attention and Mamba blocks, and a\nconditional diffusion mechanism built upon the Mamba backbone. Specifically,\nthe task-aware point cloud serialization employs Hilbert/Trans-Hilbert\nspace-filling curves and axis-wise sorting to structurally align point tokens\nfor classification and segmentation tasks, respectively. Our lightweight Latent\nAttention block features a Point-wise Multi-head Latent Attention (PMLA)\nmodule, which is specifically designed to align with the Mamba architecture by\nleveraging the shared latent space characteristics of PMLA and Mamba. This\nenables enhanced local context modeling while preserving overall efficiency. To\nfurther enhance representation learning, we incorporate a conditional diffusion\nmechanism during pretraining, which denoises perturbed feature sequences\nwithout relying on explicit point-wise reconstruction. Experimental results\ndemonstrate that PointLAMA achieves competitive performance on multiple\nbenchmark datasets with minimal parameter count and FLOPs, validating its\neffectiveness for efficient point cloud pretraining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17296v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PointLAMA：潜注意力结合Mamba实现高效点云预训练", "tldr": "PointLAMA结合了Mamba和潜注意力机制，通过任务感知序列化和条件扩散，解决了Mamba在点云建模中缺乏局部归纳偏差的问题，实现了高效且高性能的点云预训练。", "motivation": "Mamba作为点云建模的骨干模型，虽然能高效进行全局序列建模，但其缺乏局部归纳偏差，限制了其捕获三维数据中细粒度几何结构的能力。", "method": "PointLAMA是一个点云预训练框架，包含：1. 任务感知点云序列化，利用Hilbert/Trans-Hilbert空间填充曲线和轴向排序对点令牌进行结构对齐。2. 混合编码器，集成了潜注意力块和Mamba块。其中，轻量级潜注意力块包含点级多头潜注意力（PMLA）模块，与Mamba架构共享潜在空间特性，增强局部上下文建模。3. 基于Mamba骨干的条件扩散机制，在预训练期间去噪扰动的特征序列，不依赖显式点级重建。", "result": "PointLAMA在多个基准数据集上取得了有竞争力的性能，且参数量和FLOPs极小。", "conclusion": "PointLAMA通过结合Mamba和潜注意力机制，并引入任务感知序列化和条件扩散，有效解决了Mamba在点云建模中局部特征捕获不足的问题，实现了高效且高性能的点云预训练。", "translation": "Mamba最近作为点云建模的骨干模型受到了广泛关注，它利用状态空间架构，能够以线性复杂度实现高效的全局序列建模。然而，它缺乏局部归纳偏差，限制了其捕获三维数据中细粒度几何结构的能力。为了解决这一限制，我们提出了PointLAMA，一个点云预训练框架，它结合了任务感知的点云序列化、一个集成了潜注意力块和Mamba块的混合编码器，以及一个基于Mamba骨干的条件扩散机制。具体来说，任务感知的点云序列化采用Hilbert/Trans-Hilbert空间填充曲线和轴向排序，分别用于分类和分割任务，以结构化地对齐点令牌。我们轻量级的潜注意力块具有点级多头潜注意力（PMLA）模块，该模块专门设计用于通过利用PMLA和Mamba共享的潜在空间特性来与Mamba架构对齐。这使得在保持整体效率的同时增强了局部上下文建模。为了进一步增强表示学习，我们在预训练期间引入了条件扩散机制，该机制对扰动的特征序列进行去噪，而不依赖于显式的点级重建。实验结果表明，PointLAMA在多个基准数据集上取得了有竞争力的性能，且参数量和FLOPs极小，验证了其在高效点云预训练方面的有效性。", "summary": "PointLAMA是一个高效的点云预训练框架，旨在解决Mamba模型在点云建模中缺乏局部归纳偏差的问题。它通过引入任务感知点云序列化、结合潜注意力与Mamba的混合编码器，以及基于Mamba的条件扩散机制，增强了对局部几何结构的捕获能力。实验证明，PointLAMA在保持高效率的同时，在多个基准数据集上取得了优异的性能。", "keywords": "点云预训练, Mamba, 潜注意力, 序列化, 条件扩散", "comments": "PointLAMA的创新点在于将Mamba的全局序列建模能力与潜注意力机制的局部上下文建模能力相结合，有效解决了现有Mamba模型在点云处理中对细粒度几何结构捕获不足的局限性。其提出的任务感知序列化和条件扩散机制也进一步提升了预训练的效率和表示学习能力。该方法在参数量和计算量极小的情况下实现了有竞争力的性能，对于推动高效点云预训练具有重要意义。"}}
{"id": "2507.14219", "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14219v2", "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14219v2", "cate": "cs.LG", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "人工智能用于绿色氢气产量预测和场地适宜性评估，采用基于SHAP的综合指数：以阿曼为例", "tldr": "本研究提出了一种基于AI和SHAP的框架，用于在数据稀缺地区预测绿色氢气产量和评估场地适宜性，解决了缺乏地面真实数据的问题。", "motivation": "各国寻求可持续的替代能源，绿色氢气是脱碳的有前景途径，但确定最佳生产地点需要整合复杂因素，且直接氢气产量数据有限。", "method": "本研究提出了一个新颖的人工智能（AI）框架，利用平均绝对SHAP（SHapley Additive exPlanations）值计算绿色氢气产量和场地适宜性指数。该框架包含一个多阶段流程：无监督多变量聚类、监督机器学习分类器和SHAP算法。该流程在综合的气象、地形和时间数据集上进行训练。", "result": "模型预测准确率为98%。结果显示，水体邻近度、海拔和季节性变化是影响阿曼绿色氢气场地适宜性的最主要因素，其平均绝对SHAP值分别为2.470891、2.376296和1.273216。结果还揭示了适宜性的独特空间模式和变量的相对影响力。", "conclusion": "该研究为行业利益相关者和政策制定者提供了一个可复制、可扩展的工具，用于数据稀缺地区的绿色氢气基础设施规划和其他决策，提供了一种客观、可再现的替代方案，克服了主观专家权重法的局限性。", "translation": "随着各国寻求化石燃料的可持续替代品，绿色氢气已成为一条有前景的脱碳战略途径，特别是在太阳能资源丰富的干旱地区。然而，确定最佳的氢气生产地点需要整合复杂的环境、大气和基础设施因素，而直接的氢气产量数据往往有限。本研究提出了一种新颖的人工智能（AI）框架，利用平均绝对SHAP（SHapley Additive exPlanations）值计算绿色氢气产量和场地适宜性指数。该框架由无监督多变量聚类、监督机器学习分类器和SHAP算法组成的多阶段流程。该流程在综合的气象、地形和时间数据集上进行训练，结果揭示了适宜性的独特空间模式和变量的相对影响力。模型预测准确率为98%，结果还表明，水体邻近度、海拔和季节性变化是决定阿曼绿色氢气场地适宜性的最具影响力的因素，其平均绝对SHAP值分别为2.470891、2.376296和1.273216。鉴于许多具有绿色氢气前景和雄心壮志的国家缺乏或没有地面真实产量数据，本研究提供了一种客观且可再现的替代方案，取代了主观的专家加权法，从而让数据本身说话，并有可能在没有预设假设的情况下发现新颖的潜在分组。本研究为行业利益相关者和政策制定者提供了一个可复制和可扩展的工具，用于绿色氢气基础设施规划和数据稀缺地区的其他决策。", "summary": "本文提出了一种新颖的人工智能（AI）框架，结合无监督聚类、监督机器学习和SHAP算法，用于预测绿色氢气产量和评估场地适宜性。该框架通过分析气象、地形和时间数据集，在阿曼地区实现了98%的预测准确率，并识别出水体邻近度、海拔和季节性变化是关键影响因素。该研究为数据稀缺地区提供了一种客观、可再现的绿色氢气基础设施规划工具。", "keywords": "绿色氢气, 人工智能, SHAP, 场地适宜性, 阿曼", "comments": "本文的创新之处在于提出了一个结合AI和SHAP值的综合框架，解决了绿色氢气产量数据稀缺的问题。通过SHAP值量化了各因素的影响力，增加了模型的可解释性。其客观性和可复制性对于数据受限地区的绿色氢气规划具有重要意义。"}}
{"id": "2503.02382", "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning", "authors": ["Wei Sun", "Qianlong Du", "Fuwei Cui", "Jiajun Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.02382v2", "summary": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.02382v2", "cate": "cs.CL", "date": "2025-03-04", "updated": "2025-07-23", "AI": {"title_translation": "一种用于数学推理中过程监督奖励模型的高效精确训练数据构建框架", "tldr": "提出EpicPRM框架，通过量化贡献和自适应二分搜索高效构建高质量过程监督训练数据Epic50k，显著提升数学推理PRM性能。", "motivation": "现有过程监督训练数据构建方法（如手动标注和每步蒙特卡洛估计）成本高昂或质量差，阻碍了大型语言模型（LLM）数学推理能力的提升。", "method": "本文引入了一个名为EpicPRM的框架，该框架根据每个中间推理步骤的量化贡献进行标注，并使用自适应二分搜索算法来提高标注精度和效率。", "result": "通过EpicPRM框架高效构建了一个包含5万个标注中间步骤的高质量过程监督训练数据集Epic50k。与在其他公开数据集上训练的PRM模型相比，在Epic50k上训练的PRM模型表现出显著更优的性能。", "conclusion": "EpicPRM框架能够高效、精确地构建高质量的过程监督训练数据，从而显著提升过程监督奖励模型在数学推理任务上的表现。", "translation": "增强大型语言模型（LLMs）的数学推理能力具有重要的科学和实践意义。研究人员通常采用过程监督奖励模型（PRMs）来指导推理过程，有效提升模型的推理能力。然而，现有的过程监督训练数据构建方法，如手动标注和每步蒙特卡洛估计，往往成本高昂或质量不佳。为了解决这些挑战，本文引入了一个名为EpicPRM的框架，该框架根据每个中间推理步骤的量化贡献进行标注，并使用自适应二分搜索算法来提高标注精度和效率。通过这种方法，我们高效地构建了一个名为Epic50k的高质量过程监督训练数据集，其中包含5万个标注的中间步骤。与其他公开数据集相比，在Epic50k上训练的PRM模型表现出显著更优的性能。可在https://github.com/xiaolizh1/EpicPRM获取Epic50k。", "summary": "本文针对现有过程监督奖励模型（PRMs）训练数据构建成本高、质量差的问题，提出了一个名为EpicPRM的框架。该框架通过量化中间推理步骤的贡献并结合自适应二分搜索算法，高效且精确地构建了高质量的过程监督训练数据集Epic50k（包含5万个标注步骤）。实验结果表明，在Epic50k上训练的PRM在数学推理任务上取得了显著优于其他公开数据集的性能。", "keywords": "数学推理, 过程监督奖励模型, 训练数据构建, EpicPRM, Epic50k", "comments": "这篇论文的创新点在于提出了一个高效且精确的数据构建框架EpicPRM，解决了过程监督奖励模型训练数据获取的痛点。通过量化贡献和自适应二分搜索，极大地提升了标注的效率和质量，为提升LLM的数学推理能力提供了高质量的数据基础。其构建的Epic50k数据集及其带来的性能提升证明了该方法的有效性和重要性。"}}
{"id": "2507.17218", "title": "OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena", "authors": ["Yang Ouyang", "Yuchen Wu", "Xiyuan Wang", "Laixin Xie", "Weicong Cheng", "Jianping Gan", "Quan Li", "Xiaojuan Ma"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear at the IEEE VIS Conference 2025", "url": "http://arxiv.org/abs/2507.17218v1", "summary": "Communicating the complexity of oceanic phenomena-such as hypoxia and\nacidification-poses a persistent challenge for marine science. Despite advances\nin sensing technologies and computational models, conventional formats like\nstatic visualizations and text-based reports often fall short in conveying the\ndynamics of ocean changes. To address this gap, we present OceanVive, an\nimmersive and interactive visualization system that transforms complex ocean\ndatasets into navigable spatial narratives. OceanVive incorporates an\nexploratory panel on a table-sized tablet for managing immersive content on a\nlarge screen and integrates adaptive visual encodings, contextual storytelling,\nand intuitive navigation pathways to support effective communication. We\nvalidate the system through expert interviews, demonstrating its potential to\nenhance science communication and promote deeper public understanding.", "comment": "To appear at the IEEE VIS Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.17218v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "OceanVive：一个用于交流复杂海洋现象的沉浸式可视化系统", "tldr": "OceanVive是一个沉浸式交互式可视化系统，旨在通过将复杂的海洋数据集转换为可导航的空间叙事来改善海洋科学的交流和公众理解。", "motivation": "尽管传感技术和计算模型有所进步，但传统的静态可视化和文本报告在传达海洋变化的动态方面存在不足，难以有效交流缺氧和酸化等复杂海洋现象。", "method": "本研究提出了OceanVive系统，一个沉浸式和交互式可视化系统。它包括一个用于管理大屏幕沉浸式内容的桌面大小平板上的探索面板，并集成了自适应视觉编码、情境叙事和直观导航路径。", "result": "通过专家访谈验证了该系统，结果表明它有潜力增强科学交流并促进公众更深入的理解。", "conclusion": "OceanVive系统能够有效提升海洋科学的交流，并加深公众对复杂海洋现象的理解。", "translation": "海洋现象（如缺氧和酸化）的复杂性交流对海洋科学来说是一个持续的挑战。尽管传感技术和计算模型取得了进步，但像静态可视化和基于文本的报告等传统格式往往无法有效传达海洋变化的动态。为了解决这一差距，我们提出了OceanVive，一个沉浸式和交互式可视化系统，它将复杂的海洋数据集转化为可导航的空间叙事。OceanVive结合了一个用于在大屏幕上管理沉浸式内容的桌面大小平板上的探索面板，并集成了自适应视觉编码、情境叙事和直观导航路径，以支持有效的交流。我们通过专家访谈验证了该系统，展示了其增强科学交流和促进公众更深入理解的潜力。", "summary": "OceanVive是一个创新的沉浸式可视化系统，旨在解决传统方法在传达复杂海洋现象方面的不足。该系统通过将复杂的海洋数据集转化为交互式的空间叙事，并结合了平板控制面板、自适应视觉编码和直观导航等功能。通过专家访谈验证，OceanVive被证明能有效提升科学交流和公众对海洋变化的理解。", "keywords": "海洋现象, 沉浸式可视化, 科学交流, OceanVive, 数据叙事", "comments": "OceanVive的创新之处在于其将复杂海洋数据转化为沉浸式、交互式空间叙事的能力，这显著优于传统的静态可视化。通过结合桌面平板控制和大型屏幕显示，它提供了一个直观且引人入胜的平台，有望极大地改善科学传播和公众参与度。该系统的重要性在于它能够弥合科学界与公众之间的理解鸿沟，促进对关键环境问题的更深层次认识。"}}
{"id": "2507.17050", "title": "Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models", "authors": ["Tz-Ying Wu", "Tahani Trigui", "Sharath Nittur Sridhar", "Anand Bodas", "Subarna Tripathi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to CVAM Workshop at ICCV 2025", "url": "http://arxiv.org/abs/2507.17050v1", "summary": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.", "comment": "Accepted to CVAM Workshop at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17050v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "迈向可扩展的视频叙述：一种使用多模态大型语言模型的免训练方法", "tldr": "VideoNarrator是一种无需训练的管道，利用现成的多模态大型语言模型和视觉语言模型，生成带有时间戳的密集视频字幕，有效减少幻觉并改善时间对齐，从而增强视频理解。", "motivation": "尽管多模态大型语言模型在视频理解方面取得了进展，但它们在时间对齐的叙述方面表现不佳，并且容易产生幻觉，尤其是在不熟悉的场景中。", "method": "本文提出了VideoNarrator，一个无需训练的管道，通过利用现成的多模态大型语言模型（MLLMs）和视觉语言模型（VLMs）作为字幕生成器、上下文提供者或字幕验证器来生成密集的视频字幕。", "result": "实验结果表明，VideoNarrator中各组件的协同作用显著提高了视频叙述的质量和准确性，有效减少了幻觉并改善了时间对齐。", "conclusion": "VideoNarrator的结构化方法不仅增强了视频理解，还促进了视频摘要和视频问答等下游任务，并可能扩展到广告和营销应用。", "translation": "在本文中，我们介绍了VideoNarrator，这是一种新颖的免训练管道，旨在生成密集的视频字幕，提供视频内容的结构化快照。这些字幕提供带有精确时间戳的详细叙述，捕捉视频每个片段中存在的细微之处。尽管多模态大型语言模型（MLLMs）在视频理解方面取得了进展，但这些模型在时间对齐的叙述方面常常遇到困难，并且倾向于产生幻觉，特别是在不熟悉的场景中。VideoNarrator通过利用灵活的管道解决了这些挑战，在该管道中，现成的MLLMs和视觉语言模型（VLMs）可以充当字幕生成器、上下文提供者或字幕验证器。我们的实验结果表明，这些组件的协同作用显著提高了视频叙述的质量和准确性，有效减少了幻觉并改善了时间对齐。这种结构化方法不仅增强了视频理解，而且促进了视频摘要和视频问答等下游任务，并可能扩展到广告和营销应用。", "summary": "VideoNarrator是一种新颖的免训练管道，旨在生成带有精确时间戳的密集视频字幕，从而提供视频内容的结构化快照。它通过利用现成的多模态大型语言模型（MLLMs）和视觉语言模型（VLMs）作为字幕生成器、上下文提供者或字幕验证器，解决了现有MLLMs在时间对齐叙述和幻觉方面的问题。实验证明，该方法显著提高了视频叙述的质量和准确性，减少了幻觉并改善了时间对齐，进而增强了视频理解并支持下游任务。", "keywords": "视频叙述, 多模态大型语言模型, 免训练, 视频理解, 时间对齐", "comments": "本文提出了一种创新的免训练方法VideoNarrator，通过巧妙地整合现有的MLLMs和VLMs来解决视频叙述中的幻觉和时间对齐问题。其模块化和灵活的管道设计是其主要创新点，使得系统能够有效利用现有模型的能力，而无需进行耗时的模型训练。这种方法对于推动视频理解和相关下游应用具有重要意义。"}}
{"id": "2507.17156", "title": "Maintenance-free condition monitoring system based on lora", "authors": ["Honglin Zhang", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17156v1", "summary": "With the rising volume of railroad transportation, the traditional track\ninspection mainly relies on manual inspection and large-scale inspection\nequipment, which not only has low inspection frequency and lagging response,\nbut also has the defects of high risk, high cost and easy to miss inspection.\nTo this end, this study designs and realizes a maintenance-free railroad track\nwireless monitoring system based on LoRa module LM401. Each monitoring node\nconsists of an STM32 microcontroller, an LM401 LoRa transceiver, a low-power\nADXL362 triaxial acceleration sensor, a digital temperature sensor (LMT85), and\na digital barometric pressure sensor (RSCM17100KP101). The system collects\nvibration data through the SPI1 interface at the node end, periodically reads\nthe temperature and barometric pressure information, and packages and sends the\ndata to a centralized gateway within a range of 500 m using the LoRa star\ntopology; the gateway then uploads the data in real time to a cloud server\nthrough a 4G module, which supports the MQTT protocol. MQTT protocol is\nsupported. Laboratory tests and field deployments show that the system can\nrealize acceleration resolution of 0.01 g, reduce maintenance cost by about\n70%, and improve monitoring efficiency by more than 5 times. The system\nprovides a reliable means for intelligent rail health management, and in the\nfuture, it is planned to introduce RF energy collection technology to realize\nautomatic wake-up without battery, and expand to urban bridges, tunnels and\nenvironmental monitoring and other multi-scenario applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17156v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于LoRa的免维护状态监测系统", "tldr": "该研究设计并实现了一种基于LoRa模块的免维护铁路轨道无线监测系统，用于解决传统人工检测效率低、成本高的问题，并通过实验验证了其在降低维护成本和提高监测效率方面的显著优势。", "motivation": "传统的铁路轨道检测主要依赖人工和大型检测设备，存在检测频率低、响应滞后、风险高、成本高以及易漏检等缺陷。", "method": "本研究设计并实现了一个基于LoRa模块LM401的免维护铁路轨道无线监测系统。每个监测节点包含STM32微控制器、LM401 LoRa收发器、低功耗ADXL362三轴加速度传感器、数字温度传感器（LMT85）和数字气压传感器（RSCM17100KP101）。系统通过SPI1接口采集振动数据，周期性读取温湿度信息，并将数据打包通过LoRa星形拓扑发送至500米范围内的集中网关；网关通过4G模块实时上传数据至支持MQTT协议的云服务器。", "result": "实验室测试和现场部署表明，该系统能实现0.01g的加速度分辨率，维护成本降低约70%，监测效率提高5倍以上。", "conclusion": "该系统为智能轨道健康管理提供了可靠手段。未来计划引入射频能量收集技术实现无电池自动唤醒，并扩展到城市桥梁、隧道和环境监测等多场景应用。", "translation": "随着铁路运输量的不断增长，传统的轨道检测主要依靠人工检测和大型检测设备，不仅检测频率低、响应滞后，而且存在风险高、成本高、易漏检等缺陷。为此，本研究设计并实现了一种基于LoRa模块LM401的免维护铁路轨道无线监测系统。每个监测节点由STM32微控制器、LM401 LoRa收发器、低功耗ADXL362三轴加速度传感器、数字温度传感器（LMT85）和数字气压传感器（RSCM17100KP101）组成。系统在节点端通过SPI1接口采集振动数据，定期读取温度和气压信息，并使用LoRa星形拓扑将数据打包发送到500米范围内的集中网关；网关随后通过4G模块将数据实时上传到支持MQTT协议的云服务器。实验室测试和现场部署表明，该系统可以实现0.01g的加速度分辨率，维护成本降低约70%，监测效率提高5倍以上。该系统为智能轨道健康管理提供了可靠手段，未来计划引入射频能量收集技术以实现无电池自动唤醒，并扩展到城市桥梁、隧道和环境监测等多场景应用。", "summary": "本研究针对传统铁路轨道检测效率低、成本高的问题，设计并实现了一种基于LoRa模块的免维护无线监测系统。该系统由集成多种传感器的监测节点和数据上传网关组成，通过LoRa技术将采集的振动、温度、气压数据传输至云端。实验结果表明，该系统显著降低了维护成本并提高了监测效率，为智能轨道健康管理提供了有效方案，并展望了未来引入能量收集和多场景应用的可能性。", "keywords": "LoRa, 铁路轨道监测, 免维护, 无线传感器网络, 状态监测", "comments": "该论文提出了一种创新的免维护铁路轨道监测系统，利用LoRa技术实现了低功耗、远距离的数据传输，有效解决了传统检测方式的痛点。其核心创新在于将多种传感器与LoRa模块集成，并实现了显著的成本降低和效率提升。未来引入RF能量收集技术将是其重要突破，有望进一步提升系统的独立性和适用性。"}}
{"id": "2506.07770", "title": "Channel Estimation for RIS-Assisted mmWave Systems via Diffusion Models", "authors": ["Yang Wang", "Yin Xu", "Cixiao Zhang", "Zhiyong Chen", "Mingzeng Dai", "Haiming Wang", "Bingchao Liu", "Dazhi He", "Meixia Tao"], "categories": ["eess.SP", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2506.07770v2", "summary": "Reconfigurable intelligent surface (RIS) has been recognized as a promising\ntechnology for next-generation wireless communications. However, the\nperformance of RIS-assisted systems critically depends on accurate channel\nstate information (CSI). To address this challenge, this letter proposes a\nnovel channel estimation method for RIS-aided millimeter-wave (mmWave) systems\nbased on diffusion models (DMs). Specifically, the forward diffusion process of\nthe original signal is formulated to model the received signal as a noisy\nobservation within the framework of DMs. Subsequently, the channel estimation\ntask is formulated as the reverse diffusion process, and a sampling algorithm\nbased on denoising diffusion implicit models (DDIMs) is developed to enable\neffective inference. Furthermore, a lightweight neural network, termed BRCNet,\nis introduced to replace the conventional U-Net, significantly reducing the\nnumber of parameters and computational complexity. Extensive experiments\nconducted under various scenarios demonstrate that the proposed method\nconsistently outperforms existing baselines.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2506.07770v2", "cate": "eess.SP", "date": "2025-06-09", "updated": "2025-07-23", "AI": {"title_translation": "基于扩散模型的RIS辅助毫米波系统信道估计", "tldr": "本文提出了一种基于扩散模型的新型信道估计方法，用于RIS辅助的毫米波系统，并通过引入轻量级神经网络BRCNet显著降低了复杂性，实验证明其性能优于现有基线。", "motivation": "可重构智能表面（RIS）是下一代无线通信的有前景技术，但RIS辅助系统的性能严重依赖于精确的信道状态信息（CSI）。", "method": "提出了一种基于扩散模型（DMs）的RIS辅助毫米波系统信道估计方法。具体来说，将原始信号的正向扩散过程建模为DM框架内的噪声观测接收信号；将信道估计任务表述为逆向扩散过程，并开发了一种基于去噪扩散隐式模型（DDIMs）的采样算法进行有效推理。此外，引入了一个名为BRCNet的轻量级神经网络来取代传统的U-Net，以显著减少参数数量和计算复杂性。", "result": "在各种场景下进行的广泛实验表明，所提出的方法始终优于现有基线。", "conclusion": "本文提出了一种基于扩散模型的RIS辅助毫米波系统信道估计新方法，通过引入轻量级网络显著降低了复杂性，并在实验中展现出优越性能，有效解决了RIS系统中的信道估计挑战。", "translation": "可重构智能表面（RIS）已被认为是下一代无线通信的一项有前景的技术。然而，RIS辅助系统的性能严重依赖于精确的信道状态信息（CSI）。为了应对这一挑战，本文提出了一种基于扩散模型（DMs）的RIS辅助毫米波（mmWave）系统的新型信道估计方法。具体来说，将原始信号的正向扩散过程公式化，以在DMs框架内将接收信号建模为噪声观测。随后，将信道估计任务表述为逆向扩散过程，并开发了一种基于去噪扩散隐式模型（DDIMs）的采样算法以实现有效推理。此外，引入了一个轻量级神经网络，称为BRCNet，以取代传统的U-Net，显著减少了参数数量和计算复杂性。在各种场景下进行的广泛实验表明，所提出的方法始终优于现有基线。", "summary": "本研究针对RIS辅助毫米波系统中信道估计对系统性能的关键影响，提出了一种基于扩散模型（DMs）的新型信道估计方法。该方法将信道估计任务建模为逆向扩散过程，并利用去噪扩散隐式模型（DDIMs）进行有效推理。为降低计算复杂度，论文引入了轻量级神经网络BRCNet替代传统U-Net。实验结果表明，所提方法在多种场景下均优于现有基线。", "keywords": "信道估计, RIS, 毫米波, 扩散模型, BRCNet", "comments": "本文的创新点在于首次将扩散模型引入RIS辅助毫米波系统的信道估计任务中，并结合了轻量级神经网络BRCNet，有效解决了传统方法的复杂性问题，同时保持了优越的性能，为未来无线通信中的信道估计提供了新思路。"}}
{"id": "2507.17723", "title": "Application of new conformal cooling layouts to the green injection molding of complex slender polymeric parts with high dimensional specifications", "authors": ["Abelardo Torres Alba", "Jorge Manuel Mercado Colmenero", "Juan de Dios Caballero Garcia", "Cristina Martin Donate"], "categories": ["math.NA", "cond-mat.mtrl-sci", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17723v1", "summary": "Eliminating warpage in injection molded polymeric parts is one of the most\nimportant problems in the injection molding industry today. This situation is\ncritical in geometries that are particularly susceptible to warping due to\ntheir geometric features, and this occurs with topologies of great length and\nslenderness with high changes in thickness. These features are, in these\nspecial geometries, impossible to manufacture with traditional technologies to\nmeet the dimensional and sustainable requirements of the industry. This paper\npresents an innovative green conformal cooling system that is specifically\ndesigned for parts with slender geometric shapes that are highly susceptible to\nwarping. Additionally, the work presented by the authors investigates the\nimportance of using highly conductive inserts made of steel alloys in\ncombination with the use of additively manufactured conformal channels for\nreducing influential parameters, such as warpage, cooling time, and residual\nstresses in the complex manufacturing of long and slender parts. The results of\nthis real industrial case study indicated that the use of conformal cooling\nlayouts decreased the cycle time by 175.1 s 66% below the current cooling time;\nthe temperature gradient by 78.5% specifically, 18.16 C; the residual stress by\n39.78 MPa or 81.88%; and the warpage by 6.9 mm or 90.5%. In this way, it was\npossible to achieve a final warping in the complex geometry studied of 0.72 mm,\nwhich was under the maximum value required at the industrial level of 1 mm. The\nresulting values obtained by the researchers present a turning point from which\nthe manufacturing and sustainability in the injection molding of said plastic\ngeometries is possible, and they take into account that the geometric\nmanufacturing features analyzed will present a great demand in the coming years\nin the auto parts manufacturing industry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17723v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "应用于复杂细长高尺寸规格聚合物零件绿色注塑成型的新型随形冷却布局", "tldr": "该研究提出了一种新型随形冷却系统，结合高导电性钢合金嵌件和增材制造通道，显著减少了复杂细长聚合物注塑件的翘曲、冷却时间和残余应力，使其达到工业标准并提高可持续性。", "motivation": "当前注塑行业面临的主要问题是消除聚合物注塑件的翘曲，特别是对于几何特征使其极易翘曲的细长复杂零件。传统技术无法满足这些零件的尺寸和可持续性要求。", "method": "本文提出了一种创新的绿色随形冷却系统，专门针对易翘曲的细长几何形状零件。该系统结合使用高导电性钢合金嵌件和增材制造的随形通道，以减少翘曲、冷却时间和残余应力等影响参数。", "result": "实际工业案例研究结果显示：\n* 循环时间减少了175.1秒，比当前冷却时间缩短了66%。\n* 温度梯度降低了78.5%，具体为18.16°C。\n* 残余应力降低了39.78 MPa，即81.88%。\n* 翘曲减少了6.9毫米，即90.5%，最终翘曲为0.72毫米，低于工业要求的最大值1毫米。", "conclusion": "研究结果表明，该随形冷却技术为注塑成型此类塑料几何形状的制造和可持续性带来了转折点，并且考虑到所分析的几何制造特征在未来几年将在汽车零部件制造行业中呈现巨大需求。", "translation": "消除注塑聚合物零件的翘曲是当今注塑行业最重要的难题之一。在几何特征使其特别容易翘曲的几何形状中，这种情况尤为严峻，这发生在具有大长度、细长且厚度变化大的拓扑结构中。这些特征在这些特殊几何形状中，使用传统技术无法满足行业的尺寸和可持续性要求。本文提出了一种创新的绿色随形冷却系统，该系统专为易于翘曲的细长几何形状零件设计。此外，作者所展示的工作研究了使用由钢合金制成的高导电性嵌件与增材制造的随形通道相结合的重要性，以减少复杂细长零件制造中的翘曲、冷却时间以及残余应力等影响参数。这项真实工业案例研究的结果表明，使用随形冷却布局将循环时间缩短了175.1秒，比当前冷却时间减少了66%；温度梯度降低了78.5%，具体为18.16°C；残余应力降低了39.78 MPa，即81.88%；翘曲减少了6.9毫米，即90.5%。通过这种方式，所研究的复杂几何形状的最终翘曲能够达到0.72毫米，低于工业水平要求的最大值1毫米。研究人员获得的结果提供了一个转折点，从此类塑料几何形状的注塑成型制造和可持续性成为可能，并且他们考虑到所分析的几何制造特征在未来几年将在汽车零部件制造行业中呈现巨大需求。", "summary": "本文介绍了一种创新的绿色随形冷却系统，旨在解决复杂细长聚合物注塑件的翘曲问题。该系统结合了高导电性钢合金嵌件和增材制造的随形通道，成功减少了翘曲、冷却时间和残余应力。一项工业案例研究显示，该技术显著缩短了循环时间、降低了温度梯度、残余应力，并将翘曲控制在工业标准以内，为高尺寸精度和可持续的注塑制造提供了可行方案，特别适用于汽车零部件等对复杂几何形状有高需求的领域。", "keywords": "随形冷却, 注塑成型, 翘曲, 细长零件, 增材制造", "comments": "该论文的创新点在于将新型随形冷却布局与高导电性嵌件及增材制造技术相结合，针对传统方法难以解决的复杂细长零件翘曲问题提供了有效的解决方案。其重要性体现在显著提高了生产效率（缩短循环时间）、改善了产品质量（减少翘曲和残余应力）并促进了绿色制造（可持续性）。这些定量结果非常令人信服，表明该技术具有巨大的工业应用潜力，尤其是在对精度要求高的汽车零部件制造领域。"}}
{"id": "2507.16850", "title": "Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors", "authors": ["Mohamed Adjel"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE ICRA 2025 (workshop: Enhancing Human Mobility: From Computer Vision-Based Motion Tracking to Wearable Assistive Robot Control), May 2025, Atlanta (Georgia), United States", "url": "http://arxiv.org/abs/2507.16850v1", "summary": "Monocular 3D human pose estimation remains a challenging and ill-posed\nproblem, particularly in real-time settings and unconstrained environments.\nWhile direct imageto-3D approaches require large annotated datasets and heavy\nmodels, 2D-to-3D lifting offers a more lightweight and flexible\nalternative-especially when enhanced with prior knowledge. In this work, we\npropose a framework that combines real-time 2D keypoint detection with\ngeometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics\nand subject-specific anatomical priors. Our approach builds on recent advances\nin self-calibration and biomechanically-constrained inverse kinematics to\ngenerate large-scale, plausible 2D-3D training pairs from MoCap and synthetic\ndatasets. We discuss how these ingredients can enable fast, personalized, and\naccurate 3D pose estimation from monocular images without requiring specialized\nhardware. This proposal aims to foster discussion on bridging data-driven\nlearning and model-based priors to improve accuracy, interpretability, and\ndeployability of 3D human motion capture on edge devices in the wild.", "comment": "IEEE ICRA 2025 (workshop: Enhancing Human Mobility: From Computer\n  Vision-Based Motion Tracking to Wearable Assistive Robot Control), May 2025,\n  Atlanta (Georgia), United States", "pdf_url": "http://arxiv.org/pdf/2507.16850v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "基于几何先验的实时精确单目3D人体姿态估计框架", "tldr": "提出一种结合实时2D关键点检测和几何感知的2D到3D提升框架，利用相机内参和解剖学先验，实现快速、个性化、准确的单目3D人体姿态估计。", "motivation": "单目3D人体姿态估计在实时和无约束环境下仍然是一个具有挑战性的不适定问题。直接图像到3D的方法需要大型标注数据集和重型模型，而2D到3D提升方法更轻量灵活，尤其在结合先验知识时。", "method": "提出一个框架，结合实时2D关键点检测和几何感知的2D到3D提升，明确利用已知的相机内参和主体特定的解剖学先验。该方法基于自校准和生物力学约束逆运动学的最新进展，从MoCap和合成数据集中生成大规模、合理的2D-3D训练对。", "result": "能够实现快速、个性化和准确的单目图像3D姿态估计，无需专用硬件。", "conclusion": "旨在促进数据驱动学习和基于模型的先验知识的结合，以提高3D人体运动捕捉在边缘设备上的准确性、可解释性和可部署性。", "translation": "单目3D人体姿态估计仍然是一个具有挑战性且不适定问题，尤其是在实时设置和无约束环境中。虽然直接的图像到3D方法需要大型标注数据集和重型模型，但2D到3D提升提供了一种更轻量级和灵活的替代方案——尤其是在结合先验知识时。在这项工作中，我们提出了一个框架，将实时2D关键点检测与几何感知的2D到3D提升相结合，明确利用已知的相机内参和主体特定的解剖学先验。我们的方法建立在自校准和生物力学约束逆运动学的最新进展之上，从MoCap和合成数据集中生成大规模、合理的2D-3D训练对。我们讨论了这些要素如何能够在不需要专用硬件的情况下，从单目图像中实现快速、个性化和准确的3D姿态估计。本提议旨在促进关于桥接数据驱动学习和基于模型的先验知识的讨论，以提高3D人体运动捕捉在边缘设备上在野外环境中的准确性、可解释性和可部署性。", "summary": "本文提出一个用于实时、准确单目3D人体姿态估计的框架。该框架结合了实时2D关键点检测和几何感知的2D到3D提升，并通过利用相机内参和解剖学先验知识来增强。通过结合自校准和生物力学约束逆运动学，该方法能够生成大量的2D-3D训练对，从而实现无需专用硬件的快速、个性化和准确的3D姿态估计，旨在提升边缘设备上3D人体运动捕捉的性能。", "keywords": "Monocular 3D human pose estimation, Geometric priors, Real-time, 2D-to-3D lifting, Inverse kinematics", "comments": "这篇论文的创新点在于其结合了数据驱动的2D关键点检测与模型驱动的几何先验（相机内参和解剖学先验），以解决单目3D姿态估计的挑战。通过生成大规模的合成训练数据，它减少了对真实世界大型标注数据集的需求，并提高了在无约束环境下的实时性能。其重要性在于为边缘设备上的3D人体运动捕捉提供了更高效、准确且可部署的解决方案。"}}
{"id": "2507.17743", "title": "Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence", "authors": ["Andre Menolli", "Bruno Strik"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17743v1", "summary": "Object-Oriented programming is frequently challenging for undergraduate\nComputer Science students, particularly in understanding abstract concepts such\nas encapsulation, inheritance, and polymorphism. Although the literature\noutlines various methods to identify potential design and coding issues in\nobject-oriented programming through source code analysis, such as code smells\nand SOLID principles, few studies explore how these code-level issues relate to\nlearning difficulties in Object-Oriented Programming. In this study, we explore\nthe relationship of the code issue indicators with common challenges\nencountered during the learning of object-oriented programming. Using\nqualitative analysis, we identified the main categories of learning\ndifficulties and, through a literature review, established connections between\nthese difficulties, code smells, and violations of the SOLID principles. As a\nresult, we developed a conceptual map that links code-related issues to\nspecific learning challenges in Object-Oriented Programming. The model was then\nevaluated by an expert who applied it in the analysis of the student code to\nassess its relevance and applicability in educational contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17743v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从代码中获得的教育见解：通过基于代码的证据绘制面向对象编程的学习挑战", "tldr": "本研究通过定性分析和文献综述，将面向对象编程中的代码问题指标（如代码异味和SOLID原则）与学生学习困难联系起来，并开发了一个概念图，以帮助在教育背景下识别和解决这些挑战。", "motivation": "面向对象编程对于本科计算机科学学生来说常常具有挑战性，特别是在理解封装、继承和多态等抽象概念时。尽管现有文献探讨了通过源代码分析识别潜在设计和编码问题的方法，但很少有研究探讨这些代码级别问题如何与面向对象编程中的学习困难相关联。", "method": "研究采用定性分析方法，识别了主要的学习困难类别。通过文献综述，建立了这些困难与代码异味和SOLID原则违规之间的联系。然后开发了一个概念图，将代码相关问题与面向对象编程中的特定学习挑战联系起来。最后，该模型由专家评估，专家将其应用于学生代码分析以评估其在教育环境中的相关性和适用性。", "result": "结果是开发了一个概念图，该图将代码相关问题与面向对象编程中的特定学习挑战联系起来。该模型还经过了专家评估，证明了其在教育背景下的相关性和适用性。", "conclusion": "该研究成功地建立并验证了代码级别问题与面向对象编程学习困难之间的联系，为教育者提供了识别和解决学生学习挑战的工具。", "translation": "面向对象编程对于本科计算机科学学生来说常常具有挑战性，特别是在理解封装、继承和多态等抽象概念时。尽管现有文献概述了通过源代码分析（例如代码异味和SOLID原则）识别面向对象编程中潜在设计和编码问题的方法，但很少有研究探讨这些代码级别问题如何与面向对象编程中的学习困难相关联。在本研究中，我们探讨了代码问题指标与面向对象编程学习过程中遇到的常见挑战之间的关系。通过定性分析，我们确定了主要的学习困难类别，并通过文献综述，建立了这些困难与代码异味以及SOLID原则违规之间的联系。因此，我们开发了一个概念图，将代码相关问题与面向对象编程中的特定学习挑战联系起来。该模型随后由一位专家进行评估，该专家将其应用于学生代码分析，以评估其在教育环境中的相关性和适用性。", "summary": "本文旨在探索面向对象编程中代码层面的问题（如代码异味和SOLID原则违规）与学生学习困难之间的关系。研究通过定性分析识别了学习挑战，并通过文献综述建立了这些挑战与代码问题的联系，最终开发了一个将两者关联起来的概念图。该概念图经专家评估，证明了其在教育背景下分析学生代码以识别学习困难的有效性。", "keywords": "面向对象编程, 学习挑战, 代码异味, SOLID原则, 概念图", "comments": "这项研究通过将编程中的具体代码问题与学生的抽象学习困难联系起来，为面向对象编程教学提供了新颖的视角。它提出了一种基于代码证据识别学习挑战的方法，对教育者具有重要的实践指导意义。其创新之处在于构建了一个概念图，将技术细节与教学痛点相结合。"}}
{"id": "2507.09898", "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images", "authors": ["Alireza Golkarieh", "Kiana Kiashemshaki", "Sajjad Rezvani Boroujeni", "Nasibeh Asadi Isakan"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      This manuscript has 20 pages and 10 figures. It is submitted to the Journal 'Scientific Reports'", "url": "http://arxiv.org/abs/2507.09898v2", "summary": "This study investigates the effectiveness of U-Net architectures integrated\nwith various convolutional neural network (CNN) backbones for automated lung\ncancer detection and segmentation in chest CT images, addressing the critical\nneed for accurate diagnostic tools in clinical settings. A balanced dataset of\n832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed\nusing Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to\n128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,\nVGG16, and Xception, to segment lung regions. After segmentation, CNN-based\nclassifiers and hybrid models combining CNN feature extraction with traditional\nmachine learning classifiers (Support Vector Machine, Random Forest, and\nGradient Boosting) were evaluated using 5-fold cross-validation. Metrics\nincluded accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.\nU-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:\n0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for\nnon-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For\nclassification, the CNN model using U-Net with Xception achieved 99.1 percent\naccuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid\nCNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent\nF1-score. Compared to prior methods, our framework consistently outperformed\nexisting models. In conclusion, combining U-Net with advanced CNN backbones\nprovides a powerful method for both segmentation and classification of lung\ncancer in CT scans, supporting early diagnosis and clinical decision-making.", "comment": "This manuscript has 20 pages and 10 figures. It is submitted to the\n  Journal 'Scientific Reports'", "pdf_url": "http://arxiv.org/pdf/2507.09898v2", "cate": "eess.IV", "date": "2025-07-14", "updated": "2025-07-22", "AI": {"title_translation": "结合CNN骨干网络的先进U-Net架构在胸部CT图像中实现肺癌自动检测与分割", "tldr": "本研究探讨了将U-Net架构与ResNet50、VGG16和Xception等不同CNN骨干网络结合，用于胸部CT图像中的肺癌自动检测和分割，并取得了高精度性能。", "motivation": "为了满足临床环境中对精确诊断工具的迫切需求，本研究旨在调查结合各种卷积神经网络（CNN）骨干网络的U-Net架构在胸部CT图像中自动检测和分割肺癌的有效性。", "method": "研究使用了包含832张胸部CT图像（416张癌症和416张非癌症）的平衡数据集。图像经过对比度受限自适应直方图均衡化（CLAHE）预处理并调整大小为128x128像素。开发了结合ResNet50、VGG16和Xception三种CNN骨干网络的U-Net模型用于肺区域分割。分割后，使用CNN分类器和结合CNN特征提取与传统机器学习分类器（支持向量机、随机森林和梯度提升）的混合模型进行评估，采用5折交叉验证。评估指标包括准确率、精确率、召回率、F1分数、Dice系数和ROC-AUC。", "result": "在癌变肺分割方面，结合ResNet50的U-Net表现最佳（Dice系数：0.9495，准确率：0.9735）。在非癌变肺分割方面，结合VGG16的U-Net表现最佳（Dice系数：0.9532，准确率：0.9513）。在分类方面，使用结合Xception的U-Net的CNN模型达到了99.1%的准确率、99.74%的召回率和99.42%的F1分数。混合CNN-SVM-Xception模型达到了96.7%的准确率和97.88%的F1分数。与现有方法相比，该框架表现更优。", "conclusion": "结合U-Net与先进的CNN骨干网络为CT扫描中肺癌的分割和分类提供了一种强大的方法，支持早期诊断和临床决策。", "translation": "本研究调查了将U-Net架构与各种卷积神经网络（CNN）骨干网络结合，用于胸部CT图像中肺癌自动检测和分割的有效性，解决了临床环境中对精确诊断工具的关键需求。一个包含832张胸部CT图像（416张癌变和416张非癌变）的平衡数据集经过对比度受限自适应直方图均衡化（CLAHE）预处理并调整大小为128x128像素。开发了结合ResNet50、VGG16和Xception三种CNN骨干网络的U-Net模型用于肺区域分割。分割后，使用CNN分类器和结合CNN特征提取与传统机器学习分类器（支持向量机、随机森林和梯度提升）的混合模型进行评估，采用5折交叉验证。评估指标包括准确率、精确率、召回率、F1分数、Dice系数和ROC-AUC。结合ResNet50的U-Net在癌变肺分割方面表现最佳（Dice系数：0.9495，准确率：0.9735），而结合VGG16的U-Net在非癌变分割方面表现最佳（Dice系数：0.9532，准确率：0.9513）。在分类方面，使用结合Xception的U-Net的CNN模型达到了99.1%的准确率、99.74%的召回率和99.42%的F1分数。混合CNN-SVM-Xception模型达到了96.7%的准确率和97.88%的F1分数。与现有方法相比，我们的框架始终优于现有模型。总之，结合U-Net与先进的CNN骨干网络为CT扫描中肺癌的分割和分类提供了一种强大的方法，支持早期诊断和临床决策。", "summary": "本研究提出了一种结合U-Net架构与多种CNN骨干网络（ResNet50, VGG16, Xception）的方法，用于胸部CT图像中的肺癌自动检测与分割。通过对832张CT图像数据集进行预处理和5折交叉验证评估，结果显示U-Net与ResNet50在癌变肺分割上表现最佳，而U-Net与VGG16在非癌变肺分割上表现最佳。在分类任务中，结合Xception的CNN模型和混合CNN-SVM-Xception模型均取得了高精度，表明该集成框架在肺癌诊断中具有显著潜力。", "keywords": "肺癌检测, 图像分割, U-Net, CNN, CT图像", "comments": "该论文通过结合U-Net与多种先进CNN骨干网络（ResNet50、VGG16、Xception）来解决肺癌检测和分割这一关键临床问题，具有创新性。其亮点在于同时处理了分割和分类两个任务，并展示了在相对平衡但规模有限的数据集上取得的优异性能。这种集成方法为早期肺癌诊断提供了强大的工具。然而，数据集规模相对较小，未来可考虑在更大、更多样化的数据集上进行验证，以增强模型的泛化能力和临床适用性。"}}
{"id": "2507.17154", "title": "Design of a Noval Wearable ECG Monitoring Device", "authors": ["Ruihua Wang", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17154v1", "summary": "The aim of this project is to develop a new wireless powered wearable ECG\nmonitoring device. The main goal of the project is to provide a wireless,\nsmall-sized ECG monitoring device that can be worn for a long period of time by\nthe monitored person. Electrocardiogram ECG reflects physiological and\npathological information about heart activity and is commonly used to diagnose\nheart disease. Existing wearable smart ECG solutions suffer from high power\nconsumption in both ECG diagnosis and transmission for high accuracy.\nMonitoring of ECG devices is mainly done by data extraction and acquisition,\npre-processing, feature extraction, processing and analysis, visualisation and\nauxiliary procedures. During the pre-processing of the information, different\nkinds of noise generated during the signal collection need to be taken into\naccount. The quality of the signal-to-noise ratio can usually be improved by\noptimising algorithms and reducing the noise power. The choice of electrodes\nusually has a direct impact on the signal-to-noise ratio and the user\nexperience, and conventional Ag/AgCl gel electrodes are not suitable for\nlong-term and dynamic monitoring as they are prone to skin irritation,\ninflammation and allergic reactions. Therefore, a completely new way of\ncombining electrodes and wires will be used in the report. The electrodes and\nwires are cut in one piece from a silver-plated fabric. The wire portion is cut\ninto a curved structure close to an S shape to ensure that it has good\nductility for comfort and signal integrity during daily movement of the\ngarment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17154v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "新型可穿戴心电监护设备的设计", "tldr": "本文旨在开发一种新型无线供电的可穿戴心电监护设备，解决了现有设备的功耗高和电极不适问题，采用一体式镀银织物电极和弯曲导线设计。", "motivation": "现有可穿戴智能心电解决方案在诊断和传输高精度心电信号时功耗高。此外，传统Ag/AgCl凝胶电极不适合长期动态监测，易引起皮肤刺激。因此，需要开发一种无线、小型化、可长期佩戴且舒适的心电监护设备。", "method": "本项目旨在开发一种新型无线供电可穿戴心电监护设备。该设备采用一体式镀银织物电极和导线，其中导线部分被切割成S形弯曲结构，以确保在日常活动中具有良好的延展性、舒适性和信号完整性。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本项目的目标是开发一种新型无线供电的可穿戴心电监护设备。该项目的主要目标是提供一种无线、小尺寸的心电监护设备，供被监测者长期佩戴。心电图（ECG）反映心脏活动的生理和病理信息，常用于诊断心脏病。现有可穿戴智能心电解决方案在心电诊断和高精度传输方面都存在功耗高的问题。心电设备的监测主要通过数据提取和采集、预处理、特征提取、处理和分析、可视化和辅助程序来完成。在信息预处理过程中，需要考虑信号采集过程中产生的各种噪声。通常可以通过优化算法和降低噪声功率来提高信噪比的质量。电极的选择通常直接影响信噪比和用户体验，传统的Ag/AgCl凝胶电极不适用于长期动态监测，因为它们容易引起皮肤刺激、炎症和过敏反应。因此，本报告将采用一种全新的电极和导线组合方式。电极和导线由镀银织物一体切割而成。导线部分被切割成接近S形的弯曲结构，以确保在日常穿着的日常活动中具有良好的延展性，从而保证舒适性和信号完整性。", "summary": "本文旨在开发一款新型无线供电可穿戴心电监护设备，以解决现有方案功耗高及传统电极不适宜长期佩戴的问题。该设备通过一体式镀银织物电极和S形弯曲导线设计，旨在提供无线、小型化、舒适且信号完整的心电监测解决方案。", "keywords": "可穿戴设备, 心电监测, 无线供电, 镀银织物电极, 信号完整性", "comments": "该论文提出了一种创新的可穿戴心电监护设备设计，特别是在电极材料和结构上。采用一体式镀银织物电极和S形弯曲导线有望解决传统凝胶电极的皮肤刺激问题，并提高长期佩戴的舒适性和信号稳定性，对于可穿戴医疗设备领域具有重要意义。"}}
{"id": "2507.17087", "title": "Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs", "authors": ["Anjiang Wei", "Rohan Yadav", "Hang Song", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "categories": ["cs.DC", "cs.PL"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17087v1", "summary": "Optimizing parallel programs for distributed heterogeneous systems remains a\ncomplex task, often requiring significant code modifications. Task-based\nprogramming systems improve modularity by separating performance decisions from\ncore application logic, but their mapping interfaces are often too low-level.\nIn this work, we introduce Mapple, a high-level, declarative programming\ninterface for mapping distributed applications. Mapple provides transformation\nprimitives to resolve dimensionality mismatches between iteration and processor\nspaces, including a key primitive, decompose, that helps minimize communication\nvolume. We implement Mapple on top of the Legion runtime by translating Mapple\nmappers into its low-level C++ interface. Across nine applications, including\nsix matrix multiplication algorithms and three scientific computing workloads,\nMapple reduces mapper code size by 14X and enables performance improvements of\nup to 1.34X over expert-written C++ mappers. In addition, the decompose\nprimitive achieves up to 1.83X improvement over existing\ndimensionality-resolution heuristics. These results demonstrate that Mapple\nsimplifies the development of high-performance mappers for distributed\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17087v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Mapple：一种用于映射分布式异构并行程序的领域特定语言", "tldr": "Mapple是一种高级声明式编程接口，用于映射分布式应用程序，它通过提供转换原语（包括分解）来简化高性能映射器开发，并在多种应用中显著减少代码量并提高性能。", "motivation": "优化分布式异构系统上的并行程序是一项复杂任务，通常需要大量的代码修改。现有的基于任务的编程系统虽然提高了模块化，但其映射接口往往过于底层。", "method": "引入了Mapple，一个高级、声明式编程接口，用于映射分布式应用程序。Mapple提供转换原语来解决迭代空间和处理器空间之间的维度不匹配问题，其中一个关键原语是“分解”（decompose），有助于最小化通信量。Mapple在Legion运行时之上实现，通过将其映射器转换为Legion的底层C++接口。", "result": "在包括六种矩阵乘法算法和三种科学计算工作负载在内的九个应用程序中，Mapple将映射器代码量减少了14倍，并且比专家编写的C++映射器性能提高了高达1.34倍。此外，分解原语比现有维度解析启发式方法实现了高达1.83倍的改进。", "conclusion": "Mapple简化了分布式应用程序高性能映射器的开发。", "translation": "优化分布式异构系统上的并行程序仍然是一项复杂的任务，通常需要大量的代码修改。基于任务的编程系统通过将性能决策与核心应用程序逻辑分离来提高模块化，但它们的映射接口往往过于底层。在这项工作中，我们引入了Mapple，一个用于映射分布式应用程序的高级声明式编程接口。Mapple提供了转换原语来解决迭代空间和处理器空间之间的维度不匹配问题，其中包括一个关键原语“分解”（decompose），有助于最小化通信量。我们在Legion运行时之上实现了Mapple，通过将其映射器转换为其底层的C++接口。在包括六种矩阵乘法算法和三种科学计算工作负载在内的九个应用程序中，Mapple将映射器代码量减少了14倍，并且比专家编写的C++映射器性能提高了高达1.34倍。此外，分解原语比现有维度解析启发式方法实现了高达1.83倍的改进。这些结果表明，Mapple简化了分布式应用程序高性能映射器的开发。", "summary": "Mapple是一种新的领域特定语言和高级声明式编程接口，旨在简化分布式异构并行程序的映射优化。它通过提供转换原语（特别是“分解”原语）来解决维度不匹配和最小化通信，从而改进了现有低级映射接口的复杂性。在实验中，Mapple显著减少了映射器代码量并提升了应用程序性能，证明了其在开发高性能分布式应用方面的有效性。", "keywords": "分布式系统, 并行编程, 领域特定语言, 映射, Mapple", "comments": "Mapple的创新之处在于提供了一个高级、声明式的接口来解决分布式异构系统中的复杂映射问题，这显著降低了开发人员的负担。其“分解”原语在优化通信量方面表现出色，是其重要贡献。该工作通过集成到Legion运行时并展示出优于专家级C++代码的性能，验证了其实用性和有效性。"}}
{"id": "2410.18489", "title": "LLM as a code generator in Agile Model Driven Development", "authors": ["Ahmed R. Sadik", "Sebastian Brulin", "Markus Olhofer"], "categories": ["cs.AI", "cs.ET", "cs.RO", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.18489v2", "summary": "Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.18489v2", "cate": "cs.AI", "date": "2024-10-24", "updated": "2025-07-22", "AI": {"title_translation": "大型语言模型在敏捷模型驱动开发中作为代码生成器", "tldr": "本研究提出了一种敏捷模型驱动开发（AMDD）方法，利用GPT4作为代码生成器，通过整合UML、OCL和FIPA本体语言来减少自然语言歧义，从而提高代码自动生成的灵活性和可伸缩性。", "motivation": "大型语言模型（LLM）在代码自动生成方面取得了显著进展，但自然语言描述的模糊性给生成可部署、结构化代码带来了挑战。", "method": "本研究提出了一种敏捷模型驱动开发（AMDD）方法，将GPT4用作代码生成器。该方法通过使用统一建模语言（UML）对多智能体无人机编队（UVF）系统进行建模，并通过集成对象约束语言（OCL）进行代码结构元建模，以及FIPA本体语言进行通信语义元建模，显著减少了模型歧义。然后应用GPT4自动生成与JADE和PADE框架兼容的Java和Python代码。", "result": "研究成功生成了与JADE和PADE框架兼容的Java和Python代码。评估验证了自动生成代码与预期行为的一致性，并发现了智能体交互的增强。结构复杂性分析表明，本体约束的元模型产生的代码更为复杂，但其圈复杂度仍保持在可管理水平，表明可以整合额外的元模型约束而不会超过高风险复杂度阈值。", "conclusion": "本研究表明，将大型语言模型与敏捷模型驱动开发相结合，并通过引入OCL和FIPA本体等元模型约束来减少歧义，可以有效生成高质量的代码，同时保持代码复杂性在可控范围内，从而提高了代码生成的灵活性和可伸缩性。", "translation": "大型语言模型（LLM）如GPT4在代码自动生成方面的应用代表了重大进展，但并非没有挑战。软件自然语言描述固有的模糊性给生成可部署、结构化的人工制品带来了实质性障碍。本研究倡导模型驱动开发（MDD）作为克服这些挑战的可行策略，提出了一种敏捷模型驱动开发（AMDD）方法，该方法利用GPT4作为代码生成器。这种方法增强了代码自动生成过程的灵活性和可伸缩性，并提供了敏捷性，允许无缝适应模型或部署环境的变化。我们通过使用统一建模语言（UML）对多智能体无人机编队（UVF）系统进行建模来阐述这一点，通过集成对象约束语言（OCL）进行代码结构元建模和FIPA本体语言进行通信语义元建模，显著减少了模型歧义。应用GPT4的自动生成能力产生了分别与JADE和PADE框架兼容的Java和Python代码。我们对自动生成代码的彻底评估验证了其与预期行为的一致性，并确定了智能体交互的增强。在结构上，我们评估了仅受OCL元模型约束的模型所派生代码的复杂性，与受OCL和FIPA本体元模型影响的代码的复杂性。结果表明，本体约束的元模型产生了本质上更复杂的代码，但其圈复杂度保持在可管理水平内，表明可以整合额外的元模型约束而不会超过高风险的复杂度阈值。", "summary": "本研究提出了一种敏捷模型驱动开发（AMDD）方法，旨在解决大型语言模型（LLM）在代码自动生成中遇到的自然语言歧义问题。该方法利用GPT4作为代码生成器，并通过结合UML、OCL（用于代码结构元建模）和FIPA本体语言（用于通信语义元建模）来增强模型的精确性。通过对多智能体无人机编队系统进行建模，研究成功生成了与JADE和PADE框架兼容的Java和Python代码。评估结果显示，生成的代码符合预期行为，并改进了智能体交互。此外，研究分析了引入本体约束对代码复杂性的影响，发现虽然增加了代码复杂性，但仍处于可管理范围内，证明了该方法在提高代码生成质量和适应性方面的潜力。", "keywords": "大型语言模型, 敏捷模型驱动开发, 代码生成, UML, OCL, FIPA本体", "comments": "这项研究的创新之处在于将LLM（特别是GPT4）与AMDD框架相结合，并通过引入OCL和FIPA本体等元模型约束来系统性地解决自然语言描述的模糊性问题。这为LLM在生成复杂、结构化代码方面提供了更可靠的路径，克服了LLM在直接从模糊自然语言生成代码时的固有局限性。其重要性在于为未来LLM驱动的软件开发提供了更精确、更可控的方法，特别是在需要高准确性和适应性的领域。"}}
{"id": "2507.15460", "title": "Privacy-Preserving Multimodal News Recommendation through Federated Learning", "authors": ["Mehdi Khalaj", "Shahrzad Golestani Najafabadi", "Julita Vassileva"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15460v3", "summary": "Personalized News Recommendation systems (PNR) have emerged as a solution to\ninformation overload by predicting and suggesting news items tailored to\nindividual user interests. However, traditional PNR systems face several\nchallenges, including an overreliance on textual content, common neglect of\nshort-term user interests, and significant privacy concerns due to centralized\ndata storage. This paper addresses these issues by introducing a novel\nmultimodal federated learning-based approach for news recommendation. First, it\nintegrates both textual and visual features of news items using a multimodal\nmodel, enabling a more comprehensive representation of content. Second, it\nemploys a time-aware model that balances users' long-term and short-term\ninterests through multi-head self-attention networks, improving recommendation\naccuracy. Finally, to enhance privacy, a federated learning framework is\nimplemented, enabling collaborative model training without sharing user data.\nThe framework divides the recommendation model into a large server-maintained\nnews model and a lightweight user model shared between the server and clients.\nThe client requests news representations (vectors) and a user model from the\ncentral server, then computes gradients with user local data, and finally sends\ntheir locally computed gradients to the server for aggregation. The central\nserver aggregates gradients to update the global user model and news model. The\nupdated news model is further used to infer news representation by the server.\nTo further safeguard user privacy, a secure aggregation algorithm based on\nShamir's secret sharing is employed. Experiments on a real-world news dataset\ndemonstrate strong performance compared to existing systems, representing a\nsignificant advancement in privacy-preserving personalized news recommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15460v3", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "基于联邦学习的隐私保护多模态新闻推荐", "tldr": "本文提出了一种新颖的基于联邦学习的多模态新闻推荐方法，通过整合文本和视觉特征、考虑用户短期兴趣以及采用联邦学习和安全聚合技术来解决传统个性化新闻推荐系统面临的隐私、过度依赖文本和忽视短期兴趣等问题。", "motivation": "传统的个性化新闻推荐系统面临多项挑战，包括过度依赖文本内容、常忽视用户短期兴趣，以及由于集中式数据存储带来的严重隐私问题。", "method": "本文提出了一种新颖的多模态联邦学习新闻推荐方法。首先，它使用多模态模型整合新闻的文本和视觉特征。其次，它采用时间感知模型，通过多头自注意力网络平衡用户的长期和短期兴趣。最后，为增强隐私，实现了联邦学习框架，将推荐模型分为服务器维护的新闻模型和轻量级用户模型，并采用基于Shamir秘密共享的安全聚合算法来进一步保护用户隐私。", "result": "在真实世界新闻数据集上的实验表明，与现有系统相比，该方法表现出强大的性能。", "conclusion": "本文提出的基于联邦学习的多模态新闻推荐方法，在隐私保护的个性化新闻推荐方面取得了显著进展，有效解决了传统系统的局限性。", "translation": "个性化新闻推荐系统（PNR）已成为解决信息过载的方案，通过预测和推荐符合个人用户兴趣的新闻项目。然而，传统的PNR系统面临多项挑战，包括过度依赖文本内容、常忽视用户短期兴趣，以及由于集中式数据存储带来的严重隐私问题。本文通过引入一种新颖的基于联邦学习的多模态新闻推荐方法来解决这些问题。首先，它使用多模态模型整合新闻项目的文本和视觉特征，从而实现更全面的内容表示。其次，它采用时间感知模型，通过多头自注意力网络平衡用户的长期和短期兴趣，从而提高推荐准确性。最后，为增强隐私，实现了联邦学习框架，使得在不共享用户数据的情况下进行协作模型训练。该框架将推荐模型分为一个由服务器维护的大型新闻模型和一个在服务器和客户端之间共享的轻量级用户模型。客户端从中央服务器请求新闻表示（向量）和用户模型，然后使用用户本地数据计算梯度，最后将其本地计算的梯度发送到服务器进行聚合。中央服务器聚合梯度以更新全局用户模型和新闻模型。更新后的新闻模型进一步由服务器用于推断新闻表示。为了进一步保护用户隐私，采用了基于Shamir秘密共享的安全聚合算法。在真实世界新闻数据集上的实验表明，与现有系统相比，该方法表现出强大的性能，代表了隐私保护个性化新闻推荐方面的重大进步。", "summary": "本文提出了一种新颖的隐私保护多模态新闻推荐系统，旨在解决传统PNR系统在内容表示、用户兴趣捕获和隐私保护方面的不足。该系统通过整合新闻的文本和视觉特征实现更全面的内容理解；采用时间感知模型利用多头自注意力网络平衡用户的长期和短期兴趣；并引入联邦学习框架，结合Shamir秘密共享的安全聚合算法，在不共享原始用户数据的情况下进行模型训练，有效保护用户隐私。实验结果验证了其优越性能。", "keywords": "联邦学习, 多模态, 新闻推荐, 隐私保护, 安全聚合", "comments": "该论文的创新点在于将多模态学习、时间感知用户兴趣建模与联邦学习和安全聚合技术相结合，以解决个性化新闻推荐中的多重挑战。其重要性在于提供了一个在保护用户隐私的同时，提升推荐准确性和内容理解能力的解决方案，对于未来隐私计算在推荐系统中的应用具有指导意义。"}}
{"id": "2411.00107", "title": "First, Learn What You Don't Know: Active Information Gathering for Driving at the Limits of Handling", "authors": ["Alexander Davydov", "Franck Djeumou", "Marcus Greiff", "Makoto Suminaka", "Michael Thompson", "John Subosits", "Thomas Lew"], "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.00107v2", "summary": "Combining data-driven models that adapt online and model predictive control\n(MPC) has enabled effective control of nonlinear systems. However, when\ndeployed on unstable systems, online adaptation may not be fast enough to\nensure reliable simultaneous learning and control. For example, a controller on\na vehicle executing highly dynamic maneuvers--such as drifting to avoid an\nobstacle--may push the vehicle's tires to their friction limits, destabilizing\nthe vehicle and allowing modeling errors to quickly compound and cause a loss\nof control. To address this challenge, we present an active information\ngathering framework for identifying vehicle dynamics as quickly as possible. We\npropose an expressive vehicle dynamics model that leverages Bayesian last-layer\nmeta-learning to enable rapid online adaptation. The model's uncertainty\nestimates are used to guide informative data collection and quickly improve the\nmodel prior to deployment. Dynamic drifting experiments on a Toyota Supra show\nthat (i) the framework enables reliable control of a vehicle at the edge of\nstability, (ii) online adaptation alone may not suffice for zero-shot control\nand can lead to undesirable transient errors or spin-outs, and (iii) active\ndata collection helps achieve reliable performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.00107v2", "cate": "cs.RO", "date": "2024-10-31", "updated": "2025-07-23", "AI": {"title_translation": "首先，学习你不知道的：用于极限操控驾驶的主动信息收集", "tldr": "该研究提出一种主动信息收集框架，结合贝叶斯元学习模型，以快速识别车辆动力学并实现极限操控下的可靠控制，避免失控。", "motivation": "现有结合数据驱动模型和MPC的方法在不稳定系统上部署时，在线适应可能不够快，导致学习和控制不同步，例如在车辆极限操控（如漂移避障）时，建模误差会迅速累积并导致失控。", "method": "提出一种主动信息收集框架，用于尽快识别车辆动力学。该框架使用表达性车辆动力学模型，利用贝叶斯最后一层元学习实现快速在线适应。模型的不确定性估计用于指导信息量大的数据收集，并在部署前快速改进模型。", "result": "在丰田Supra上的动态漂移实验表明：(i) 该框架能够可靠地控制处于稳定边缘的车辆；(ii) 仅靠在线适应可能不足以实现零样本控制，并可能导致不良瞬态误差或失控；(iii) 主动数据收集有助于实现可靠性能。", "conclusion": "该主动信息收集框架通过结合贝叶斯元学习和不确定性引导的数据收集，有效解决了车辆在极限操控下快速识别动力学并保持可靠控制的挑战。", "translation": "将在线自适应的数据驱动模型与模型预测控制（MPC）相结合，使得非线性系统的有效控制成为可能。然而，当部署在不稳定系统上时，在线自适应可能不够快，无法确保可靠的同步学习和控制。例如，在车辆执行高度动态机动（如漂移以避开障碍物）时，控制器可能会将车辆轮胎推向其摩擦极限，从而使车辆失稳，并允许建模误差迅速累积并导致失控。为了解决这一挑战，我们提出了一种主动信息收集框架，用于尽快识别车辆动力学。我们提出了一种表达性车辆动力学模型，该模型利用贝叶斯最后一层元学习来实现快速在线自适应。模型的不确定性估计用于指导信息量大的数据收集，并在部署前快速改进模型。在丰田Supra上的动态漂移实验表明：（i）该框架能够可靠地控制处于稳定边缘的车辆；（ii）仅靠在线自适应可能不足以实现零样本控制，并可能导致不良瞬态误差或失控；（iii）主动数据收集有助于实现可靠性能。", "summary": "本文提出了一种用于车辆极限操控的主动信息收集框架，旨在快速识别车辆动力学并确保可靠控制。针对现有在线适应在不稳定系统上可能不足以实现同步学习和控制的问题，该框架引入了基于贝叶斯最后一层元学习的表达性车辆动力学模型。该模型利用其不确定性估计来指导信息收集，从而在部署前快速优化模型。实验证明，该框架能有效控制处于稳定边缘的车辆，并强调了主动数据收集在实现可靠性能方面的重要性，克服了单一在线适应的局限性。", "keywords": "极限操控, 主动信息收集, 贝叶斯元学习, 车辆动力学, 模型预测控制", "comments": "该论文的创新点在于提出了一个结合贝叶斯元学习和主动信息收集的框架，以解决车辆在极限操控下快速识别动力学和保持控制的难题。通过利用模型的不确定性来指导数据收集，显著提高了模型适应速度和控制可靠性，这对于自动驾驶等需要高动态性能和安全性的应用具有重要意义。"}}
{"id": "2507.17563", "title": "BoSS: Beyond-Semantic Speech", "authors": ["Qing Wang", "Zehan Li", "Hang Lv", "Hongjie Chen", "Yaodong Song", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17563v1", "summary": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17563v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "BoSS：超越语义的语音", "tldr": "现代语音技术未能捕捉人类交流中的超越语义维度，BoSS提出一个框架来表征和分析这些隐式信号，以实现更丰富的人机交互。", "motivation": "人类交流不仅包含显式语义，还涉及关键的隐式信号和上下文线索，而现代语音技术（如ASR和TTS）往往无法捕捉这些超越语义的维度。", "method": "本文引入了口语交互系统能力级别（L1-L5）这一分层框架，并提出了超越语义的语音（BoSS）概念。研究者提出了一个BoSS的形式化框架，利用认知相关性理论和机器学习模型来分析时序和上下文语音动态。", "result": "对BoSS相关属性在五个不同维度上的评估显示，当前的口语语言模型（SLMs）难以完全解释超越语义的信号。", "conclusion": "研究结果强调了推进BoSS研究的必要性，以实现更丰富、更具上下文意识的人机通信。", "translation": "人类交流不仅仅涉及显式语义，隐式信号和上下文线索在塑造意义方面发挥着关键作用。然而，现代语音技术，如自动语音识别（ASR）和文本到语音（TTS），往往未能捕捉这些超越语义的维度。为了更好地表征和基准语音智能的进展，我们引入了口语交互系统能力级别（L1-L5），这是一个分层框架，阐述了口语对话系统从基本命令识别到类人社交互动的演变。为了支持这些高级能力，我们提出了超越语义的语音（BoSS），它指的是语音通信中包含但超越显式语义的信息集。它通过情感线索、上下文动态和隐式语义等多维特征来传达情感、上下文，并修改或扩展意义，从而增强对交流意图和场景的理解。我们提出了一个BoSS的形式化框架，利用认知相关性理论和机器学习模型来分析时序和上下文语音动态。我们评估了BoSS相关的五个不同维度的属性，揭示了当前的口语语言模型（SLMs）难以完全解释超越语义的信号。这些发现强调了推进BoSS研究的必要性，以实现更丰富、更具上下文意识的人机通信。", "summary": "本文提出了“超越语义的语音”（BoSS）概念和“口语交互系统能力级别”（L1-L5）框架，旨在解决当前语音技术在捕捉人类交流中隐式、超越语义维度方面的不足。BoSS涵盖了通过情感线索、上下文动态和隐式语义等多维特征传达的情感和上下文信息。研究者提出了一个BoSS的形式化框架，结合认知相关性理论和机器学习模型进行分析。评估结果表明，当前的口语语言模型难以完全解释这些超越语义的信号，这突显了深入研究BoSS对于实现更丰富、更具上下文意识的人机通信的重要性。", "keywords": "超越语义语音, 口语交互, 人机通信, 隐式信号, 语音技术", "comments": "该论文通过引入BoSS和能力级别框架，解决了当前语音技术在处理人类交流中非语义、隐式信息方面的关键空白。它提供了一个结构化的方法来理解和衡量语音智能的进步。研究发现当前口语模型难以处理超越语义信号，这指明了一个重要的研究挑战和未来的发展方向。"}}
{"id": "2409.13684", "title": "The FIX Benchmark: Extracting Features Interpretable to eXperts", "authors": ["Helen Jin", "Shreya Havaldar", "Chaehyeon Kim", "Anton Xue", "Weiqiu You", "Helen Qu", "Marco Gatti", "Daniel A Hashimoto", "Bhuvnesh Jain", "Amin Madani", "Masao Sako", "Lyle Ungar", "Eric Wong"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.13684v4", "summary": "Feature-based methods are commonly used to explain model predictions, but\nthese methods often implicitly assume that interpretable features are readily\navailable. However, this is often not the case for high-dimensional data, and\nit can be hard even for domain experts to mathematically specify which features\nare important. Can we instead automatically extract collections or groups of\nfeatures that are aligned with expert knowledge? To address this gap, we\npresent FIX (Features Interpretable to eXperts), a benchmark for measuring how\nwell a collection of features aligns with expert knowledge. In collaboration\nwith domain experts, we propose FIXScore, a unified expert alignment measure\napplicable to diverse real-world settings across cosmology, psychology, and\nmedicine domains in vision, language, and time series data modalities. With\nFIXScore, we find that popular feature-based explanation methods have poor\nalignment with expert-specified knowledge, highlighting the need for new\nmethods that can better identify features interpretable to experts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.13684v4", "cate": "cs.LG", "date": "2024-09-20", "updated": "2025-07-22", "AI": {"title_translation": "FIX基准：提取专家可解释的特征", "tldr": "FIX是一个衡量特征集合与专家知识对齐程度的基准，研究发现当前流行的基于特征的解释方法与专家知识对齐性较差，表明需要新的方法来识别专家可解释的特征。", "motivation": "基于特征的方法常用于解释模型预测，但这些方法通常隐含假设可解释特征易于获得，而对于高维数据并非如此，甚至领域专家也难以数学地指定哪些特征重要。因此，需要一种方法来自动提取与专家知识对齐的特征集合或组。", "method": "本文提出了FIX（Features Interpretable to eXperts），一个用于衡量特征集合与专家知识对齐程度的基准。与领域专家合作，提出了FIXScore，一个统一的专家对齐度量，适用于宇宙学、心理学和医学领域中视觉、语言和时间序列数据模态的各种真实世界设置。", "result": "使用FIXScore，我们发现流行的基于特征的解释方法与专家指定的知识对齐性较差。", "conclusion": "研究结果突出了需要新的方法来更好地识别专家可解释的特征。", "translation": "基于特征的方法通常用于解释模型预测，但这些方法往往隐含假设可解释特征是现成的。然而，对于高维数据而言，情况往往并非如此，即使是领域专家也很难通过数学方式指定哪些特征是重要的。我们能否自动提取与专家知识对齐的特征集合或组呢？为了解决这一差距，我们提出了FIX（Features Interpretable to eXperts），一个用于衡量特征集合与专家知识对齐程度的基准。通过与领域专家合作，我们提出了FIXScore，一个统一的专家对齐度量，适用于宇宙学、心理学和医学领域中视觉、语言和时间序列数据模态的各种真实世界设置。通过FIXScore，我们发现流行的基于特征的解释方法与专家指定的知识对齐性较差，这突出表明需要新的方法来更好地识别专家可解释的特征。", "summary": "该论文提出了FIX（Features Interpretable to eXperts），一个评估特征集合与专家知识对齐程度的基准，并引入了FIXScore作为统一的专家对齐度量。研究发现，当前流行的基于特征的解释方法在与专家知识对齐方面表现不佳，强调了开发新方法以识别专家可解释特征的必要性。", "keywords": "FIX, FIXScore, 可解释特征, 专家知识, 特征解释", "comments": "该论文通过引入FIX基准和FIXScore，创新性地解决了可解释AI领域中一个关键但常被忽视的问题：如何量化特征解释与领域专家知识的对齐程度。这对于推动可解释AI的发展至关重要，因为它提供了一个客观的评估工具，可以指导未来研究开发出真正对人类专家有用的解释方法。其重要性在于，它揭示了现有解释方法的局限性，并为未来研究指明了方向。"}}
{"id": "2507.17589", "title": "Encrypted-State Quantum Compilation Scheme Based on Quantum Circuit Obfuscation", "authors": ["Chenyi Zhang", "Tao Shang", "Xueyi Guo"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17589v1", "summary": "With the rapid advancement of quantum computing, quantum compilation has\nbecome a crucial layer connecting high-level algorithms with physical hardware.\nIn quantum cloud computing, compilation is performed on the cloud side, which\nexposes user circuits to potential risks such as structural leakage and output\npredictability. To address these issues, we propose the encrypted-state quantum\ncompilation scheme based on quantum circuit obfuscation (ECQCO), the first\nsecure compilation framework tailored for the co-location of compilers and\nquantum hardware. It applies quantum homomorphic encryption to conceal output\nstates and instantiates a structure obfuscation mechanism based on quantum\nindistinguishability obfuscation, effectively protecting both functionality and\ntopology of the circuit. Additionally, an adaptive decoupling obfuscation\nalgorithm is designed to suppress potential idle errors while inserting pulse\noperations. The proposed scheme achieves information-theoretic security and\nguarantees computational indistinguishability under the quantum random oracle\nmodel. Experimental results on benchmark datasets show that ECQCO achieves a\nTVD of up to 0.7 and a normalized GED of 0.88, enhancing compilation-stage\nsecurity. Moreover, it introduces only a slight increase in circuit depth,\nwhile keeping the average fidelity change within 1%, thus achieving a practical\nbalance between security and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17589v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于量子电路混淆的加密态量子编译方案", "tldr": "本文提出了一种名为ECQCO的加密态量子编译方案，通过应用量子同态加密和量子不可区分混淆，解决了量子云计算中用户电路的安全风险，并在保证效率的同时提升了编译阶段的安全性。", "motivation": "在量子云计算中，编译在云端进行，这使得用户电路面临结构泄露和输出可预测性等潜在安全风险。", "method": "本文提出了基于量子电路混淆的加密态量子编译方案（ECQCO），该方案首次为编译器和量子硬件的协同定位量身定制了安全编译框架。它应用量子同态加密来隐藏输出状态，并实例化了基于量子不可区分混淆的结构混淆机制，有效保护了电路的功能和拓扑结构。此外，还设计了一种自适应去耦混淆算法，以在插入脉冲操作时抑制潜在的空闲错误。", "result": "ECQCO在基准数据集上的实验结果显示，其TVD高达0.7，归一化GED为0.88，增强了编译阶段的安全性。此外，它只引入了电路深度上的轻微增加，同时保持平均保真度变化在1%以内，从而在安全性和效率之间实现了实用平衡。", "conclusion": "所提出的ECQCO方案通过应用量子同态加密和量子不可区分混淆，有效解决了量子云计算中的电路安全问题，并在保证信息论安全和计算不可区分性的同时，实现了安全性和效率的良好平衡。", "translation": "随着量子计算的快速发展，量子编译已成为连接高级算法与物理硬件的关键层。在量子云计算中，编译在云端进行，这使得用户电路面临结构泄露和输出可预测性等潜在风险。为解决这些问题，我们提出了一种基于量子电路混淆的加密态量子编译方案（ECQCO），这是首个为编译器和量子硬件协同定位量身定制的安全编译框架。它应用量子同态加密来隐藏输出状态，并实例化了基于量子不可区分混淆的结构混淆机制，有效保护了电路的功能和拓扑结构。此外，还设计了一种自适应去耦混淆算法，以在插入脉冲操作时抑制潜在的空闲错误。所提出的方案实现了信息论安全，并在量子随机预言模型下保证了计算不可区分性。在基准数据集上的实验结果表明，ECQCO的TVD高达0.7，归一化GED为0.88，增强了编译阶段的安全性。此外，它只引入了电路深度上的轻微增加，同时保持平均保真度变化在1%以内，从而在安全性和效率之间实现了实用平衡。", "summary": "本文提出了一种名为ECQCO的加密态量子编译方案，旨在解决量子云计算中用户电路面临的安全风险，如结构泄露和输出可预测性。该方案通过结合量子同态加密隐藏输出状态和基于量子不可区分混淆的结构混淆机制来保护电路的功能和拓扑。此外，它还包含一个自适应去耦混淆算法以减少错误。实验证明，ECQCO在提高编译阶段安全性的同时，仅对电路深度和保真度产生轻微影响，实现了安全与效率的平衡。", "keywords": "量子编译, 量子混淆, 量子同态加密, 量子安全", "comments": "该论文提出了一种创新的加密态量子编译方案，首次将量子同态加密和量子不可区分混淆应用于量子编译的安全领域，有效解决了量子云计算中用户电路的隐私和安全问题。其在信息论安全和计算不可区分性方面的理论保证，以及实验结果展示的安全性和效率平衡，都显示了该方案的实用性和重要性。"}}
{"id": "2507.17432", "title": "Non-Asymptotic Achievable Rate-Distortion Region for Indirect Wyner-Ziv Source Coding", "authors": ["Jiahui Wei", "Philippe Mary", "Elsa Dupraz"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 3 pages' appendix", "url": "http://arxiv.org/abs/2507.17432v1", "summary": "In the Wyner-Ziv source coding problem, a source $X$ has to be encoded while\nthe decoder has access to side information $Y$. This paper investigates the\nindirect setup, in which a latent source $S$, unobserved by both the encoder\nand the decoder, must also be reconstructed at the decoder. This scenario is\nincreasingly relevant in the context of goal-oriented communications, where $S$\ncan represent semantic information obtained from $X$. This paper derives the\nindirect Wyner-Ziv rate-distortion function in asymptotic regime and provides\nan achievable region in finite block-length. Furthermore, a Blahut-Arimoto\nalgorithm tailored for the indirect Wyner-Ziv setup, is proposed. This\nalgorithm is then used to give a numerical evaluation of the achievable\nindirect rate-distortion region when $S$ is treated as a classification label.", "comment": "8 pages, 2 figures, 3 pages' appendix", "pdf_url": "http://arxiv.org/pdf/2507.17432v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "非渐近可达率失真区域用于间接Wyner-Ziv信源编码", "tldr": "本文研究了间接Wyner-Ziv信源编码的非渐近可达率失真区域，并提出了相应的算法。", "motivation": "在Wyner-Ziv信源编码问题中，解决编码器和解码器都未观测到的潜在源S必须在解码器处重建的问题。这种场景在面向目标的通信中，S可以代表从X获得的语义信息，因此越来越重要。", "method": "推导了渐近状态下的间接Wyner-Ziv率失真函数；提供了有限块长度下的可达区域；提出了一个为间接Wyner-Ziv设置量身定制的Blahut-Arimoto算法。", "result": "成功推导了渐近状态下的间接Wyner-Ziv率失真函数；提供了有限块长度下的可达区域；并利用提出的Blahut-Arimoto算法对当S被视为分类标签时可达的间接率失真区域进行了数值评估。", "conclusion": "本文成功推导了间接Wyner-Ziv源编码的率失真函数和有限块长度下的可达区域，并提供了一种数值评估方法，证明了其在面向目标通信中的潜力。", "translation": "在Wyner-Ziv信源编码问题中，源X必须被编码，而解码器可以访问侧信息Y。本文研究了间接设置，其中编码器和解码器都未观测到的潜在源S也必须在解码器处重建。这种场景在面向目标的通信中越来越相关，其中S可以表示从X获得的语义信息。本文推导了渐近状态下的间接Wyner-Ziv率失真函数，并提供了有限块长度下的可达区域。此外，还提出了一种为间接Wyner-Ziv设置量身定制的Blahut-Arimoto算法。然后，当S被视为分类标签时，该算法被用于对可达的间接率失真区域进行数值评估。", "summary": "本文研究了间接Wyner-Ziv信源编码问题，即在编码器和解码器均无法观测潜在源S的情况下，需要重建S。该问题在语义信息传输等面向目标的通信中具有重要意义。文章推导了渐近条件下的间接Wyner-Ziv率失真函数，并给出了有限块长度下的可达区域。为数值评估，提出了一种适用于间接Wyner-Ziv设置的Blahut-Arimoto算法，并利用该算法对S作为分类标签时的间接率失真区域进行了数值评估。", "keywords": "Wyner-Ziv编码, 率失真, 间接编码, 非渐近, Blahut-Arimoto算法", "comments": "本文将Wyner-Ziv编码扩展到更复杂的间接设置，引入了潜在源S的概念，这对于面向目标的通信具有创新性。通过理论推导和算法实现，为理解和应用间接Wyner-Ziv编码提供了基础。其对有限块长度的研究也增加了实用价值。"}}
{"id": "2507.17297", "title": "On Temporal Guidance and Iterative Refinement in Audio Source Separation", "authors": ["Tobias Morocutti", "Jonathan Greif", "Paul Primus", "Florian Schmid", "Gerhard Widmer"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17297v1", "summary": "Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17297v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "关于音频源分离中的时间引导和迭代细化", "tldr": "本文提出了一种新的空间声场景语义分割方法，通过时间引导和迭代细化显著提高了音频标记和源分离性能。", "motivation": "传统的空间声场景语义分割系统依赖于两阶段流程（音频标记后进行标签条件源分离），但往往受限于缺乏细粒度时间信息，这对于有效分离至关重要。本文旨在解决这一限制。", "method": "本文引入了一种新的空间声场景语义分割方法，增强了事件检测和源分离阶段之间的协同作用。主要贡献有三点：1. 微调一个预训练的Transformer来检测活跃声类。2. 利用另一个微调的Transformer实例执行声音事件检测，为分离模块提供详细、时变的引导。3. 实现一个迭代细化机制，通过递归重用前一次迭代的分离器输出，逐步提高分离质量。", "result": "这些改进显著提高了音频标记和源分离性能，系统在DCASE Challenge 2025的Task 4中获得了第二名。", "conclusion": "通过引入时间引导和迭代细化机制，本文提出的方法有效解决了传统系统在空间声场景语义分割中缺乏细粒度时间信息的问题，显著提升了音频标记和源分离的性能。", "translation": "空间声场景语义分割（S5）涉及准确识别活跃声类并从复杂的声学混合物中精确分离其声源。传统系统依赖于两阶段流程——音频标记后进行标签条件源分离——但往往受限于缺乏细粒度时间信息，这对于有效分离至关重要。在这项工作中，我们通过引入一种新的S5方法来解决这一限制，该方法增强了事件检测和源分离阶段之间的协同作用。我们的主要贡献有三点。首先，我们微调了一个预训练的Transformer来检测活跃声类。其次，我们利用这个微调的Transformer的独立实例来执行声音事件检测（SED），为分离模块提供详细、时变的引导。第三，我们实现了一个迭代细化机制，通过递归重用分离器在先前迭代中的输出，逐步提高分离质量。这些进步显著提高了音频标记和源分离性能，正如我们的系统在DCASE Challenge 2025的Task 4中获得第二名所证明的那样。我们的实现和模型检查点可在我们的GitHub仓库中获取：https://github.com/theMoro/dcase25task4。", "summary": "本文提出了一种用于空间声场景语义分割（S5）的新方法，旨在解决传统方法中缺乏细粒度时间信息的问题。该方法通过微调Transformer进行声类检测和声音事件检测，为源分离提供时变引导，并引入迭代细化机制以逐步提高分离质量。实验结果表明，该方法在音频标记和源分离性能上均取得了显著提升，并在DCASE Challenge 2025的Task 4中获得第二名。", "keywords": "音频源分离, 时间引导, 迭代细化, 空间声场景语义分割, Transformer", "comments": "本文的创新之处在于引入了时间引导和迭代细化机制来增强音频事件检测和源分离之间的协同作用，有效解决了传统两阶段方法的局限性。特别是，将Transformer用于细粒度声音事件检测并结合迭代细化，为复杂声场景的分离提供了新的思路和显著的性能提升。其在DCASE Challenge中的优异表现也证明了该方法的有效性和实用性。"}}
{"id": "2507.16989", "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "authors": ["Giulio Pelosio", "Devesh Batra", "Noémie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16989v1", "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16989v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "隐而不消：通过基于名称的偏见基准评估大型语言模型中的国籍偏见", "tldr": "大型语言模型（LLMs）即使在没有明确人口学标记的情况下，也会表现出潜在的国籍偏见。研究引入了一种基于名称的基准测试方法，发现小型模型比大型模型准确性更低，偏见更大。", "motivation": "本文旨在通过将明确的国籍标签替换为具有文化指示性的名称（这种场景更符合现实世界中LLM的应用），来调查大型语言模型（LLMs）中即使在没有明确人口学标记时也可能存在的潜在国籍偏见。", "method": "研究引入了一种新颖的、基于名称的基准测试方法，该方法源自“问答偏见基准（BBQ）”数据集。通过用具有文化指示性的名称替代明确的国籍标签，该方法评估了OpenAI、Google和Anthropic等行业领先的LLM在偏见程度和准确性方面受到的影响。", "result": "实验结果显示，与大型模型相比，小型模型准确性较低，且表现出更大的偏见。例如，在基于名称的数据集和模糊语境中，Claude Haiku的刻板印象偏见得分高达9%，而其大型对应模型Claude Sonnet仅为3.5%，后者在准确性上还比前者高出117.7%。此外，研究发现小型模型在这些模糊语境中保留了更大比例的现有错误，例如，GPT-4o保留了68%的错误率，而GPT-4o-mini则保留了76%。", "conclusion": "本研究强调了大型语言模型中偏见的顽固韧性，突显了其对在多样化全球背景下开发和部署AI系统的深远影响。", "translation": "大型语言模型（LLMs）即使在没有明确人口学标记的情况下，也会对特定国籍表现出潜在偏见。在这项工作中，我们引入了一种新颖的、基于名称的基准测试方法，该方法源自问答偏见基准（BBQ）数据集，旨在调查用具有文化指示性的名称替代明确国籍标签的影响，这种场景更符合现实世界中LLM的应用。我们新颖的方法考察了这种替代如何影响OpenAI、Google和Anthropic等行业领先的LLM在偏见程度和准确性上的表现。我们的实验表明，与大型模型相比，小型模型准确性较低，且表现出更大的偏见。例如，在我们的基于名称的数据集和模糊语境中（正确选择未揭示），Claude Haiku的刻板印象偏见得分高达9%，而其大型对应模型Claude Sonnet仅为3.5%，后者在准确性上还比前者高出117.7%。此外，我们发现小型模型在这些模糊语境中保留了更大比例的现有错误。例如，在用名称替代明确国籍指代后，GPT-4o保留了68%的错误率，而GPT-4o-mini则保留了76%，其他模型提供商也有类似发现。我们的研究强调了LLM中偏见的顽固韧性，突显了其对在多样化全球背景下开发和部署AI系统的深远影响。", "summary": "本文引入了一种基于名称的基基准测试方法，该方法源自BBQ数据集，旨在评估大型语言模型（LLMs）在用具有文化指示性的名称替代明确国籍标签时所表现出的潜在国籍偏见。实验结果表明，与大型模型相比，小型模型展现出更高的偏见水平和更低的准确性，并且在模糊语境中保留了更大比例的现有错误。这项研究强调了LLM中偏见的持续存在性，对其在多样化全球背景下AI系统的开发和部署具有重要意义。", "keywords": "国籍偏见, 大型语言模型, 基于名称的基准, 潜在偏见, 模型规模", "comments": "该论文的创新之处在于采用了基于名称的基准测试，这比使用明确标签更能反映真实世界的应用场景。它为LLM中偏见的顽固性，特别是小型模型中的偏见，提供了关键见解，这对公平性和负责任的AI开发具有重要意义。"}}
{"id": "2405.15632", "title": "Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning", "authors": ["Dario Fenoglio", "Gabriele Dominici", "Pietro Barbiero", "Alberto Tonda", "Martin Gjoreski", "Marc Langheinrich"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      [v3] Pre-print of the paper accepted to NeurIPS 2024 (30 pages)", "url": "http://arxiv.org/abs/2405.15632v3", "summary": "Federated Learning (FL), a privacy-aware approach in distributed deep\nlearning environments, enables many clients to collaboratively train a model\nwithout sharing sensitive data, thereby reducing privacy risks. However,\nenabling human trust and control over FL systems requires understanding the\nevolving behaviour of clients, whether beneficial or detrimental for the\ntraining, which still represents a key challenge in the current literature. To\naddress this challenge, we introduce Federated Behavioural Planes (FBPs), a\nnovel method to analyse, visualise, and explain the dynamics of FL systems,\nshowing how clients behave under two different lenses: predictive performance\n(error behavioural space) and decision-making processes (counterfactual\nbehavioural space). Our experiments demonstrate that FBPs provide informative\ntrajectories describing the evolving states of clients and their contributions\nto the global model, thereby enabling the identification of clusters of clients\nwith similar behaviours. Leveraging the patterns identified by FBPs, we propose\na robust aggregation technique named Federated Behavioural Shields to detect\nmalicious or noisy client models, thereby enhancing security and surpassing the\nefficacy of existing state-of-the-art FL defense mechanisms. Our code is\npublicly available on GitHub.", "comment": "[v3] Pre-print of the paper accepted to NeurIPS 2024 (30 pages)", "pdf_url": "http://arxiv.org/pdf/2405.15632v3", "cate": "cs.LG", "date": "2024-05-24", "updated": "2025-07-23", "AI": {"title_translation": "联邦行为平面：解释联邦学习中客户端行为的演变", "tldr": "提出联邦行为平面（FBP）来分析和解释联邦学习中客户端行为的演变，并利用其模式开发联邦行为盾（FBS）以检测恶意客户端并增强安全性。", "motivation": "在联邦学习中，理解客户端行为的演变（无论是好是坏）对于建立人类信任和控制FL系统至关重要，但目前仍是一个关键挑战。", "method": "引入联邦行为平面（FBPs）来分析、可视化和解释FL系统的动态，从预测性能和决策过程两个维度展示客户端行为。在此基础上，提出联邦行为盾（Federated Behavioural Shields, FBS）作为一种鲁棒的聚合技术，用于检测恶意或噪声客户端模型。", "result": "实验表明，FBPs提供了描述客户端演变状态及其对全局模型贡献的信息轨迹，从而能够识别具有相似行为的客户端集群。利用FBPs识别的模式，联邦行为盾（FBS）能够检测恶意或噪声客户端模型，增强了安全性并超越了现有最先进的FL防御机制的效力。", "conclusion": "联邦行为平面（FBPs）和联邦行为盾（FBS）为理解联邦学习中客户端行为的演变提供了新方法，并显著提升了FL系统的安全性和鲁棒性。", "translation": "联邦学习（FL）是一种在分布式深度学习环境中注重隐私的方法，它使得许多客户端能够在不共享敏感数据的情况下协同训练模型，从而降低了隐私风险。然而，要使人类信任和控制FL系统，就需要理解客户端不断演变的行为，无论这种行为对训练是有益还是有害，这在当前文献中仍然是一个关键挑战。为了解决这一挑战，我们引入了联邦行为平面（Federated Behavioural Planes, FBPs），这是一种分析、可视化和解释FL系统动态的新颖方法，它从两个不同的视角展示客户端的行为：预测性能（误差行为空间）和决策过程（反事实行为空间）。我们的实验表明，FBPs提供了描述客户端演变状态及其对全局模型贡献的信息轨迹，从而能够识别具有相似行为的客户端集群。利用FBPs识别的模式，我们提出了一种名为联邦行为盾（Federated Behavioural Shields）的鲁棒聚合技术，用于检测恶意或噪声客户端模型，从而增强了安全性并超越了现有最先进的FL防御机制的效力。我们的代码已在GitHub上公开。", "summary": "本文针对联邦学习中理解客户端行为演变的关键挑战，提出了联邦行为平面（FBPs）作为一种新颖的方法，用于分析、可视化和解释FL系统的动态，从预测性能和决策过程两个维度揭示客户端行为。实验证明FBPs能提供客户端行为轨迹并识别相似行为的集群。在此基础上，作者进一步提出了联邦行为盾（FBS），一种基于FBPs模式的鲁棒聚合技术，旨在检测恶意或噪声客户端模型，从而显著提升联邦学习的安全性，并超越现有先进防御机制。", "keywords": "联邦学习, 客户端行为, 联邦行为平面, 恶意检测, 模型聚合", "comments": "这篇论文的创新点在于提出了“联邦行为平面”这一概念，通过可视化和分析客户端行为的动态演变，为理解联邦学习系统提供了新的视角。在此基础上，进一步开发了“联邦行为盾”来增强FL的安全性，这显示了从行为理解到实际防御机制的完整链条。该方法有望提高联邦学习系统的透明度、可信赖性和鲁棒性，尤其是在应对恶意攻击方面具有重要意义。"}}
{"id": "2507.17085", "title": "Deformable Cluster Manipulation via Whole-Arm Policy Learning", "authors": ["Jayadeep Jacob", "Wenzheng Zhang", "Houston Warren", "Paulo Borges", "Tirthankar Bandyopadhyay", "Fabio Ramos"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17085v1", "summary": "Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17085v1", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "通过全身策略学习的可变形簇群操作", "tldr": "本文提出了一种用于可变形物体簇群全身操作的无模型强化学习框架，该框架结合了3D点云和本体触觉信息，并通过零样本模拟到现实迁移成功应用于电力线清理任务。", "motivation": "操作可变形物体簇群是一项具有广泛应用但极具挑战性的任务，它需要丰富的接触式全身交互。现有解决方案面临着模型合成能力有限、感知不确定性高以及缺乏高效空间抽象等问题。", "method": "本文提出了一种新颖的无模型策略学习框架，该框架整合了两种模态：3D点云和本体触觉指示器，并强调了全身接触感知，超越了传统的末端执行器模式。该强化学习框架利用核均值嵌入辅助的分布状态表示，以提高训练效率和实时推理能力。此外，还提出了一种与上下文无关的遮挡启发式方法，用于从目标区域清除可变形物体以进行暴露任务。", "result": "该框架被部署在电力线清理场景中，观察到智能体生成了利用多个机械臂连杆进行去遮挡的创造性策略。最终，实现了零样本模拟到现实的策略迁移，使得机械臂能够清除具有未知遮挡模式、未见拓扑结构和不确定动力学的真实树枝。", "conclusion": "本文提出的框架成功实现了可变形簇群的全身操作，并在复杂任务（如清除遮挡）中展示了鲁棒的模拟到现实迁移能力。", "translation": "操作可变形物体簇群是一个具有广泛适用性的巨大挑战，但它需要富含接触的全身交互。一个潜在的解决方案必须解决现实模型合成能力有限、感知高度不确定性以及缺乏高效空间抽象等问题。我们提出了一种新颖的无模型策略学习框架，该框架整合了两种模态：3D点云和本体触觉指示器，强调了具有全身接触意识的操作，超越了传统的末端执行器模式。我们的强化学习框架利用核均值嵌入辅助的分布状态表示，以实现改进的训练效率和实时推理。此外，我们提出了一种新颖的与上下文无关的遮挡启发式方法，用于从目标区域清除可变形物体以进行暴露任务。我们将该框架部署在电力线清理场景中，并观察到智能体生成了利用多个机械臂连杆进行去遮挡的创造性策略。最后，我们执行了零样本模拟到现实的策略迁移，使机械臂能够清除具有未知遮挡模式、未见拓扑结构和不确定动力学的真实树枝。", "summary": "本文提出了一种新颖的无模型强化学习框架，用于解决可变形物体簇群的全身操作挑战。该框架结合了3D点云和本体触觉信息，并利用分布状态表示和核均值嵌入来提高效率。通过引入一种上下文无关的遮挡启发式方法，该系统能够生成创造性策略以清除遮挡。实验在电力线清理场景中进行，并成功实现了零样本模拟到现实的策略迁移，证明了其在处理复杂现实世界任务中的有效性。", "keywords": "可变形操作, 全身控制, 强化学习, 模拟到现实, 点云", "comments": "本文的创新点在于提出了一个结合3D点云和本体触觉信息的全身策略学习框架，突破了传统末端执行器操作的限制。其采用的分布状态表示和核均值嵌入提高了训练效率，而零样本模拟到现实的迁移能力则大大增强了其实用性和泛化能力，为可变形物体操作领域带来了重要进展。"}}
{"id": "2507.17379", "title": "Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models", "authors": ["Shen Tan", "Dong Zhou", "Xiangyu Shao", "Junqiao Wang", "Guanghui Sun"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IJCAI 2025", "url": "http://arxiv.org/abs/2507.17379v1", "summary": "Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.", "comment": "IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.17379v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于预训练模型的语言条件开放词汇移动操作", "tldr": "本文提出了LOVMM框架，利用大型语言模型（LLM）和视觉语言模型（VLM）来解决家庭环境中涉及新颖物体的开放词汇移动操作（OVMM）任务，并在模拟实验中展示了强大的零样本泛化和多任务学习能力。", "motivation": "开放词汇移动操作（OVMM）涉及在不同工作空间处理新颖和未见过的物体，这对于现实世界的机器人应用来说仍然是一个重大挑战。", "method": "本文提出了一种名为LOVMM的新型语言条件开放词汇移动操作框架，该框架结合了大型语言模型（LLM）和视觉语言模型（VLM）来处理家庭环境中的各种移动操作任务。该方法能够通过自由形式的自然语言指令解决各种OVMM任务。", "result": "在复杂的家庭环境中进行的广泛模拟实验表明，LOVMM具有强大的零样本泛化和多任务学习能力。此外，该方法还可以泛化到多个桌面操作任务，并比其他最先进的方法取得更好的成功率。", "conclusion": "LOVMM框架通过结合LLM和VLM，有效解决了开放词汇移动操作的挑战，并在零样本泛化和多任务学习方面表现出色，优于现有SOTA方法。", "translation": "开放词汇移动操作（OVMM）涉及在不同工作空间处理新颖和未见过的物体，这对于现实世界的机器人应用来说仍然是一个重大挑战。在本文中，我们提出了一种名为LOVMM的新型语言条件开放词汇移动操作框架，该框架结合了大型语言模型（LLM）和视觉语言模型（VLM）来处理家庭环境中的各种移动操作任务。我们的方法能够通过自由形式的自然语言指令解决各种OVMM任务（例如“把办公室桌上的食物盒子扔到角落的垃圾桶里”和“把床上的瓶子装到客房的盒子里”）。在复杂家庭环境中进行的广泛模拟实验表明，LOVMM具有强大的零样本泛化和多任务学习能力。此外，我们的方法还可以泛化到多个桌面操作任务，并比其他最先进的方法取得更好的成功率。", "summary": "本文提出了一种名为LOVMM的语言条件开放词汇移动操作框架，旨在解决机器人在家庭环境中处理新颖物体的挑战。LOVMM集成了大型语言模型（LLM）和视觉语言模型（VLM），使其能够理解自由形式的自然语言指令并执行各种移动操作任务。模拟实验证明，LOVMM在零样本泛化和多任务学习方面表现出强大的能力，并且在多桌面操作任务上优于现有最先进的方法。", "keywords": "开放词汇移动操作, 大型语言模型, 视觉语言模型, 机器人操作, 零样本泛化", "comments": "LOVMM框架的创新之处在于其将LLM和VLM集成到移动操作任务中，从而实现了对自由形式自然语言指令的理解和处理。这大大提高了机器人在开放词汇环境中的泛化能力，使其能够处理未见过的物体和场景。其在零样本泛化和多任务学习方面的表现是其重要性所在，预示了未来更灵活、更智能的机器人应用。"}}
{"id": "2507.17365", "title": "DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning", "authors": ["Chuzhan Hao", "Wenfeng Feng", "Yuewei Zhang", "Hao Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures", "url": "http://arxiv.org/abs/2507.17365v1", "summary": "Multi-step agentic retrieval systems based on large language models (LLMs)\nhave demonstrated remarkable performance in complex information search tasks.\nHowever, these systems still face significant challenges in practical\napplications, particularly in generating factually inconsistent intermediate\nqueries and inefficient search trajectories, which can lead to reasoning\ndeviations or redundant computations. To address these issues, we propose\nDynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs\nand multi-reward reinforcement learning (RL). Specifically, our system\nleverages knowledge graphs as external structured knowledge to guide the search\nprocess by explicitly modeling entity relationships, thereby ensuring factual\nconsistency in intermediate queries and mitigating biases from irrelevant\ninformation. Furthermore, we employ a multi-reward RL framework for\nfine-grained control over training objectives such as retrieval accuracy,\nefficiency, and response quality. This framework promotes the generation of\nhigh-quality intermediate queries and comprehensive final answers, while\ndiscouraging unnecessary exploration and minimizing information omissions or\nredundancy. Experimental results demonstrate that our approach achieves\nstate-of-the-art answer accuracy on six multi-hop question answering datasets,\nmatching frontier LLMs while using only small-scale models and limited\ncomputational resources. Furthermore, our approach demonstrates strong\ngeneralization and robustness across diverse retrieval environments and\nlarger-scale models, highlighting its broad applicability.", "comment": "10 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.17365v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DynaSearcher：基于多奖励强化学习的动态知识图谱增强搜索代理", "tldr": "DynaSearcher提出了一种通过动态知识图谱和多奖励强化学习增强的搜索代理，解决了现有LLM代理在信息搜索中事实不一致和搜索效率低下的问题，并在多跳问答任务中取得了最先进的性能。", "motivation": "现有的基于大型语言模型（LLMs）的多步代理检索系统在复杂信息搜索任务中存在挑战，特别是生成事实不一致的中间查询和低效的搜索轨迹，这可能导致推理偏差或冗余计算。", "method": "我们提出了DynaSearcher，一个通过动态知识图谱和多奖励强化学习（RL）增强的创新搜索代理。具体来说，系统利用知识图谱作为外部结构化知识，通过明确建模实体关系来指导搜索过程，从而确保中间查询的事实一致性并减轻不相关信息的偏差。此外，我们采用多奖励RL框架对检索准确性、效率和响应质量等训练目标进行细粒度控制。", "result": "实验结果表明，我们的方法在六个多跳问答数据集上实现了最先进的答案准确性，与前沿LLM模型匹敌，同时仅使用小型模型和有限的计算资源。此外，我们的方法在不同的检索环境和更大规模的模型上表现出强大的泛化性和鲁棒性。", "conclusion": "DynaSearcher通过结合动态知识图谱和多奖励强化学习，有效解决了LLM代理在信息搜索中面临的事实不一致和效率低下问题，并在多跳问答任务中展现出卓越的性能、泛化性和鲁棒性。", "translation": "基于大型语言模型（LLMs）的多步代理检索系统在复杂信息搜索任务中表现出卓越的性能。然而，这些系统在实际应用中仍面临重大挑战，特别是在生成事实不一致的中间查询和低效的搜索轨迹方面，这可能导致推理偏差或冗余计算。为了解决这些问题，我们提出了DynaSearcher，一个通过动态知识图谱和多奖励强化学习（RL）增强的创新搜索代理。具体来说，我们的系统利用知识图谱作为外部结构化知识，通过明确建模实体关系来指导搜索过程，从而确保中间查询的事实一致性并减轻不相关信息的偏差。此外，我们采用多奖励RL框架对检索准确性、效率和响应质量等训练目标进行细粒度控制。该框架促进了高质量中间查询和全面最终答案的生成，同时阻止不必要的探索并最小化信息遗漏或冗余。实验结果表明，我们的方法在六个多跳问答数据集上实现了最先进的答案准确性，与前沿LLM模型匹敌，同时仅使用小型模型和有限的计算资源。此外，我们的方法在不同的检索环境和更大规模的模型上表现出强大的泛化性和鲁棒性，突显了其广泛适用性。", "summary": "本文提出了DynaSearcher，一个创新的搜索代理，旨在解决基于LLM的检索系统在复杂信息搜索中遇到的事实不一致和效率低下问题。DynaSearcher通过利用动态知识图谱指导搜索过程以确保事实一致性，并采用多奖励强化学习框架优化检索准确性、效率和响应质量。实验证明，该方法在多跳问答数据集上实现了最先进的准确性，且具有强大的泛化性和鲁棒性，同时资源消耗较低。", "keywords": "知识图谱, 强化学习, 搜索代理, 大型语言模型, 多跳问答", "comments": "DynaSearcher的创新点在于结合了动态知识图谱来增强搜索过程中的事实一致性，并通过多奖励强化学习实现对搜索行为的精细控制。其重要性体现在能够在仅使用小型模型和有限资源的情况下，达到甚至超越现有大型语言模型的性能，这对于资源受限的应用场景具有重要意义。该方法在泛化性和鲁棒性方面的表现也值得关注。"}}
{"id": "2507.17336", "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting", "authors": ["Hyeongmin Lee", "Kyungjune Baek"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      21 pages, 10 figures", "url": "http://arxiv.org/abs/2507.17336v1", "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91x compression compared to the original Ex4DGS model while\nmaintaining high visual fidelity. These results highlight the applicability of\nour framework for real-time dynamic scene rendering in diverse scenarios, from\nresource-constrained edge devices to high-performance environments.", "comment": "21 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.17336v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "时间平滑感知率失真优化4D高斯泼溅", "tldr": "本文提出了一种新颖的端到端率失真优化压缩框架，专为4D高斯泼溅（4DGS）设计，通过引入小波变换来利用时间平滑性，显著减少存储需求并实现高达91倍的压缩比，同时保持高视觉保真度，适用于实时动态场景渲染。", "motivation": "动态4D高斯泼溅（4DGS）尽管能高效渲染体视频，但其庞大的高斯数量、显著的时间冗余以及缺乏熵感知压缩框架导致巨大的存储需求，这给实际部署、高效边缘设备处理和数据传输带来了重大挑战。", "method": "本文提出了一种新颖的端到端率失真（RD）优化压缩框架，专为4DGS量身定制。该方法以Ex4DGS为基线，在兼容现有3DGS压缩方法的同时，特别通过使用小波变换来反映真实世界的时间平滑先验，而不是独立存储每个点的运动轨迹，从而显著提高存储效率。", "result": "该方法与原始Ex4DGS模型相比，实现了高达91倍的压缩比，同时保持了高视觉保真度，并提供了用户可控的压缩效率与渲染质量之间的平衡。", "conclusion": "本文提出的框架通过显著降低4DGS的存储需求，使其能够实现灵活、高保真渲染，并适用于从资源受限的边缘设备到高性能环境的各种实时动态场景渲染。", "translation": "动态4D高斯泼溅（4DGS）有效地将3D高斯泼溅（3DGS）的高速渲染能力扩展到表示体视频。然而，大量的高斯点、显著的时间冗余，尤其是缺乏熵感知压缩框架，导致了巨大的存储需求。因此，这给实际部署、高效边缘设备处理和数据传输带来了重大挑战。在本文中，我们引入了一种新颖的、端到端率失真（RD）优化的压缩框架，专为4DGS量身定制，旨在实现跨不同计算平台的灵活、高保真渲染。我们以最先进的4DGS方法之一——全显式动态高斯泼溅（Ex4DGS）作为基线，从现有的3DGS压缩方法开始，以保持兼容性，同时有效解决时间轴带来的额外挑战。特别是，我们不独立存储每个点的运动轨迹，而是采用小波变换来反映真实世界的平滑先验，显著提高了存储效率。这种方法显著提高了压缩比，并提供了用户可控的压缩效率和渲染质量之间的平衡。广泛的实验证明了我们方法的有效性，与原始Ex4DGS模型相比，实现了高达91倍的压缩，同时保持了高视觉保真度。这些结果突出了我们框架在从资源受限的边缘设备到高性能环境等各种场景中实时动态场景渲染的适用性。", "summary": "本文针对动态4D高斯泼溅（4DGS）巨大的存储需求，提出了一种新颖的端到端率失真（RD）优化压缩框架。该框架以Ex4DGS为基线，通过引入小波变换来利用时间平滑性，而非独立存储运动轨迹，从而显著提升了存储效率。实验结果表明，该方法在保持高视觉保真度的前提下，实现了高达91倍的压缩比，为实时动态场景在多种计算平台上的高效渲染提供了解决方案。", "keywords": "4D高斯泼溅, 视频压缩, 率失真优化, 时间平滑性, 小波变换", "comments": "本文的创新点在于将时间平滑性作为先验知识引入4DGS的压缩框架中，通过小波变换有效处理时间轴上的冗余。这种方法不仅解决了4DGS在存储和部署上的实际挑战，而且在大幅提高压缩比的同时保持了高质量的视觉效果，对于推动4DGS在边缘设备和实时应用中的普及具有重要意义。其用户可控的平衡机制也增加了实用性。"}}
{"id": "2410.20519", "title": "Fractal Signatures: Securing AI-Generated Pollock-Style Art via Intrinsic Watermarking and Blockchain", "authors": ["Yiquan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages, 7 figures", "url": "http://arxiv.org/abs/2410.20519v4", "summary": "The digital art market faces unprecedented challenges in authenticity\nverification and copyright protection. This study introduces an integrated\nframework to address these issues by combining neural style transfer, fractal\nanalysis, and blockchain technology. We generate abstract artworks inspired by\nJackson Pollock, using their inherent mathematical complexity to create robust,\nimperceptible watermarks. Our method embeds these watermarks, derived from\nfractal and turbulence features, directly into the artwork's structure. This\napproach is then secured by linking the watermark to NFT metadata, ensuring\nimmutable proof of ownership. Rigorous testing shows our feature-based\nwatermarking achieves a 76.2% average detection rate against common attacks,\nsignificantly outperforming traditional methods (27.8-44.0%). This work offers\na practical solution for digital artists and collectors, enhancing security and\ntrust in the digital art ecosystem.", "comment": "19 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2410.20519v4", "cate": "cs.CV", "date": "2024-10-27", "updated": "2025-07-23", "AI": {"title_translation": "分形签名：通过内在水印和区块链保护AI生成的波洛克风格艺术", "tldr": "本研究提出一个结合神经网络风格迁移、分形分析和区块链技术的集成框架，通过内在水印和NFT元数据来保护AI生成的波洛克风格数字艺术的真实性和版权。", "motivation": "数字艺术市场在真实性验证和版权保护方面面临前所未有的挑战。", "method": "本研究结合神经风格迁移、分形分析和区块链技术。通过利用杰克逊·波洛克风格艺术固有的数学复杂性，从分形和湍流特征中提取并直接嵌入鲁棒、不可感知的水印到艺术品结构中。然后，将水印与NFT元数据链接，以确保所有权不可篡改的证明。", "result": "特征水印在对抗常见攻击时实现了76.2%的平均检测率，显著优于传统方法（27.8%-44.0%）。", "conclusion": "这项工作为数字艺术家和收藏家提供了一个实用的解决方案，增强了数字艺术生态系统中的安全性和信任。", "translation": "数字艺术市场在真实性验证和版权保护方面面临前所未有的挑战。本研究引入了一个集成框架，通过结合神经网络风格迁移、分形分析和区块链技术来解决这些问题。我们生成了受杰克逊·波洛克启发的抽象艺术品，并利用其固有的数学复杂性来创建鲁棒、不可感知的水印。我们的方法将这些源自分形和湍流特征的水印直接嵌入到艺术品的结构中。然后，通过将水印与NFT元数据链接来确保这种方法，从而提供不可篡改的所有权证明。严格的测试表明，我们的基于特征的水印在对抗常见攻击时实现了76.2%的平均检测率，显著优于传统方法（27.8%-44.0%）。这项工作为数字艺术家和收藏家提供了一个实用的解决方案，增强了数字艺术生态系统中的安全性和信任。", "summary": "本研究提出了一个集成的框架，旨在解决数字艺术市场的真实性验证和版权保护问题。该框架结合了神经网络风格迁移、分形分析和区块链技术，通过利用AI生成的波洛克风格艺术固有的数学复杂性，创建并嵌入基于分形和湍流特征的内在水印。这些水印随后与NFT元数据关联，以提供不可篡改的所有权证明。实验结果表明，该方法在检测率上显著优于传统水印技术，为数字艺术生态系统提供了增强的安全性和信任。", "keywords": "分形签名, 数字水印, 区块链, AI艺术, 版权保护", "comments": "这项研究的创新之处在于将分形分析与神经网络风格迁移相结合，用于生成艺术品并创建内在水印，然后利用区块链技术确保版权和真实性。其提出的基于特征的水印方法在检测率上表现出色，为数字艺术领域面临的版权和验证挑战提供了一个有前景的解决方案。该方法利用了艺术品本身的数学特性，而非简单地叠加信息，增强了水印的鲁棒性和隐蔽性。"}}
{"id": "2507.17289", "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments", "authors": ["Shitong Zhu", "Chenhao Fang", "Derek Larson", "Neel Reddy Pochareddy", "Rajeev Rao", "Sophie Zeng", "Yanqing Peng", "Wendy Summer", "Alex Goncalves", "Arya Pudota", "Herve Robert"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17289v1", "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17289v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "合规大脑助手：用于协助企业环境中合规任务的对话式智能AI", "tldr": "Compliance Brain Assistant (CBA) 是一种对话式智能AI助手，通过智能查询路由器（FastTrack和FullAgentic模式）显著提高企业合规任务的效率和响应质量，优于现有大型语言模型。", "motivation": "在企业环境中，日常合规任务的效率低下是需要解决的问题。本文旨在通过开发一个对话式智能AI助手来提升这些任务的效率。", "method": "本文提出了合规大脑助手（CBA），一个对话式智能AI助手。它设计了一个用户查询路由器，可以智能地在两种模式之间选择：(i) FastTrack模式处理简单请求，仅需从知识库中检索相关上下文；(ii) FullAgentic模式处理复杂请求，需要复合操作和工具调用来主动发现各种合规工件中的上下文，并/或调用其他API/模型。一个典型例子是根据用户查询找到特定实体，然后利用实体信息查询其他API来丰富AI响应。", "result": "实验评估显示，CBA在平均关键词匹配率（83.7% vs. 41.7%）和LLM-judge通过率（82.0% vs. 20.0%）等指标上，显著优于开箱即用的大型语言模型。此外，与`fast-track only`和`full-agentic`模式相比，完整的路由设计在保持运行时大致相同的情况下，具有更好的平均匹配率和通过率。", "conclusion": "路由机制在响应质量和延迟之间取得了良好的平衡，验证了CBA设计中路由机制的有效性，并显著提升了合规任务的效率。", "translation": "本文介绍了合规大脑助手（CBA），一个对话式智能AI助手，旨在提高企业环境中人员日常合规任务的效率。为了在响应质量和延迟之间取得良好的平衡，我们设计了一个用户查询路由器，可以智能地在以下两种模式之间进行选择：(i) FastTrack模式：用于处理只需要从知识语料库中检索额外相关上下文的简单请求；(ii) FullAgentic模式：用于处理需要复合操作和工具调用以主动发现各种合规工件中的上下文，和/或涉及其他API/模型以适应请求的复杂请求。一个典型的例子是，从用户查询开始，使用其描述找到一个特定实体，然后使用该实体的信息查询其他API，以整理和丰富最终的AI响应。\n我们的实验评估将CBA与开箱即用的大型语言模型在针对不同角色的各种真实世界隐私/合规相关查询上进行了比较。我们发现CBA在平均关键词匹配率（83.7% vs. 41.7%）和LLM-judge通过率（82.0% vs. 20.0%）等指标上显著提升了普通大型语言模型的性能。我们还将完整的基于路由的设计与“仅快速通道”和“完全智能体”模式进行了比较，发现它具有更好的平均匹配率和通过率，同时保持了大致相同的运行时间。这一发现验证了我们的假设，即路由机制在两种模式之间实现了良好的权衡。", "summary": "本文提出了合规大脑助手（CBA），一个旨在提高企业合规任务效率的对话式智能AI。CBA通过一个智能查询路由器在FastTrack（简单请求）和FullAgentic（复杂请求）模式间动态切换，以平衡响应质量和延迟。实验结果表明，CBA在关键词匹配率和LLM-judge通过率上显著优于现有大型语言模型，且其路由机制在保持效率的同时优化了性能。", "keywords": "合规助手, 对话式AI, 智能体AI, 路由机制, 企业合规", "comments": "CBA的创新之处在于其双模式路由机制，这有效地解决了大型语言模型在处理不同复杂程度任务时面临的响应质量与延迟之间的权衡问题。该方法通过智能地选择处理路径，显著提升了企业合规任务的效率和准确性，具有重要的实际应用价值。"}}
{"id": "2507.16834", "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription", "authors": ["Jordan Madden", "Matthew Stone", "Dimitri Johnson", "Daniel Geddez"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16834v1", "summary": "Although Jamaican Patois is a widely spoken language, current speech\nrecognition systems perform poorly on Patois music, producing inaccurate\ncaptions that limit accessibility and hinder downstream applications. In this\nwork, we take a data-centric approach to this problem by curating more than 40\nhours of manually transcribed Patois music. We use this dataset to fine-tune\nstate-of-the-art automatic speech recognition (ASR) models, and use the results\nto develop scaling laws for the performance of Whisper models on Jamaican\nPatois audio. We hope that this work will have a positive impact on the\naccessibility of Jamaican Patois music and the future of Jamaican Patois\nlanguage modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16834v1", "cate": "eess.AS", "date": "2025-07-15", "updated": "2025-07-15", "AI": {"title_translation": "牙买加土语音乐转录鲁棒语音识别研究", "tldr": "当前语音识别系统对牙买加土语音乐表现不佳，本研究通过构建大型手动转录数据集并微调ASR模型，以提高其识别性能并探索Whisper模型的性能扩展规律，旨在提升牙买加土语音乐的可及性。", "motivation": "当前语音识别系统在牙买加土语音乐上的表现不佳，导致字幕不准确，限制了可及性并阻碍了后续应用。因此，需要提高牙买加土语音乐的语音识别准确性。", "method": "本研究采用以数据为中心的方法，整理了超过40小时的手动转录牙买加土语音乐数据集。利用该数据集微调了最先进的自动语音识别（ASR）模型，并利用结果开发了Whisper模型在牙买加土语音频上性能的扩展规律。", "result": "开发了Whisper模型在牙买加土语音频上性能的扩展规律。", "conclusion": "这项工作有望对牙买加土语音乐的可及性以及牙买加土语语言建模的未来产生积极影响。", "translation": "尽管牙买加土语是一种广泛使用的语言，但当前的语音识别系统在土语音乐上的表现不佳，生成不准确的字幕，这限制了可及性并阻碍了下游应用程序。在这项工作中，我们通过整理超过40小时的手动转录土语音乐，对这个问题采取了以数据为中心的方法。我们使用这个数据集来微调最先进的自动语音识别（ASR）模型，并使用结果来开发Whisper模型在牙买加土语音频上性能的扩展规律。我们希望这项工作将对牙买加土语音乐的可及性以及牙买加土语语言建模的未来产生积极影响。", "summary": "本研究旨在解决当前语音识别系统在牙买加土语音乐转录方面表现不佳的问题。通过构建一个包含超过40小时手动转录土语音乐的大型数据集，研究人员微调了最先进的ASR模型，并在此基础上探索了Whisper模型在牙买加土语音频上的性能扩展规律。这项工作有望显著提升牙买加土语音乐的可访问性，并促进该语言的未来建模研究。", "keywords": "牙买加土语, 语音识别, 音乐转录, ASR, Whisper模型", "comments": "本论文的创新点在于构建了一个大型、高质量的牙买加土语音乐转录数据集，并利用该数据集对现有ASR模型进行微调，以解决特定语言和音乐领域的识别难题。其重要性在于提升了牙买加土语音乐的可及性，并为低资源语言的语音识别研究提供了数据驱动的范例。通过探索Whisper模型的扩展规律，也为未来模型优化提供了方向。"}}
{"id": "2507.17304", "title": "Learning-based Stage Verification System in Manual Assembly Scenarios", "authors": ["Xingjian Zhang", "Yutong Duan", "Zaishu Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17304v1", "summary": "In the context of Industry 4.0, effective monitoring of multiple targets and\nstates during assembly processes is crucial, particularly when constrained to\nusing only visual sensors. Traditional methods often rely on either multiple\nsensor types or complex hardware setups to achieve high accuracy in monitoring,\nwhich can be cost-prohibitive and difficult to implement in dynamic industrial\nenvironments. This study presents a novel approach that leverages multiple\nmachine learning models to achieve precise monitoring under the limitation of\nusing a minimal number of visual sensors. By integrating state information from\nidentical timestamps, our method detects and confirms the current stage of the\nassembly process with an average accuracy exceeding 92%. Furthermore, our\napproach surpasses conventional methods by offering enhanced error detection\nand visuali-zation capabilities, providing real-time, actionable guidance to\noperators. This not only improves the accuracy and efficiency of assembly\nmonitoring but also re-duces dependency on expensive hardware solutions, making\nit a more practical choice for modern industrial applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17304v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于学习的手动装配场景阶段验证系统", "tldr": "本研究提出了一种基于多机器学习模型的方法，仅使用少量视觉传感器即可在手动装配场景中实现高精度阶段监控，准确率超过92%，并增强了错误检测和可视化能力。", "motivation": "在工业4.0背景下，装配过程中的多目标和状态监控至关重要，尤其是在仅限于使用视觉传感器的情况下。传统方法通常依赖多种传感器或复杂硬件，成本高昂且难以在动态工业环境中实施。", "method": "本研究提出了一种新颖的方法，利用多个机器学习模型，在仅使用少量视觉传感器的情况下实现精确监控。通过整合相同时间戳的状态信息，该方法检测并确认装配过程的当前阶段。", "result": "该方法检测并确认装配过程的当前阶段，平均准确率超过92%。此外，该方法在错误检测和可视化能力方面超越了传统方法，能为操作员提供实时、可操作的指导。", "conclusion": "本研究提出的系统不仅提高了装配监控的准确性和效率，还减少了对昂贵硬件解决方案的依赖，使其成为现代工业应用中更实用的选择。", "translation": "在工业4.0背景下，在装配过程中有效监控多个目标和状态至关重要，尤其是在仅限于使用视觉传感器的情况下。传统方法通常依赖多种传感器类型或复杂的硬件设置来实现高精度监控，这可能成本过高且难以在动态工业环境中实施。本研究提出了一种新颖的方法，利用多个机器学习模型，在仅使用少量视觉传感器的情况下实现精确监控。通过整合相同时间戳的状态信息，我们的方法检测并确认装配过程的当前阶段，平均准确率超过92%。此外，我们的方法通过增强错误检测和可视化能力超越了传统方法，为操作员提供实时、可操作的指导。这不仅提高了装配监控的准确性和效率，还减少了对昂贵硬件解决方案的依赖，使其成为现代工业应用中更实用的选择。", "summary": "本研究针对工业4.0背景下手动装配过程中的视觉传感器局限性，提出了一种创新的基于多机器学习模型的阶段验证系统。该系统通过整合时间戳状态信息，实现了超过92%的平均准确率来检测和确认装配阶段。与传统方法相比，它在错误检测和可视化方面表现更优，能提供实时指导，从而提高效率并降低对昂贵硬件的依赖，为现代工业应用提供了更实用的解决方案。", "keywords": "机器学习, 阶段验证, 视觉传感器, 工业4.0, 智能制造", "comments": "该研究的创新之处在于，在仅使用少量视觉传感器的情况下，通过整合多个机器学习模型和时间戳信息，实现了高精度的装配阶段监控。其重要性在于，它提供了一种成本效益高且易于实施的解决方案，解决了传统方法在动态工业环境中成本高昂和复杂性大的问题，对于推动工业4.0的智能制造具有实际意义。"}}
{"id": "2507.17745", "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "authors": ["Yiwen Chen", "Zhihao Li", "Yikai Wang", "Hu Zhang", "Qin Li", "Chi Zhang", "Guosheng Lin"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.17745v1", "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.", "comment": "Project Page: https://buaacyw.github.io/ultra3d/", "pdf_url": "http://arxiv.org/pdf/2507.17745v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Ultra3D：基于部分注意力的N高效高保真3D生成", "tldr": "Ultra3D 提出了一种高效的3D生成框架，通过引入紧凑的VecSet表示和几何感知的局部注意力机制Part Attention，显著加速了稀疏体素建模，同时保持了高质量的3D内容生成。", "motivation": "现有3D内容生成框架由于两阶段扩散管道中注意力机制的二次复杂度，导致计算效率低下，无法有效处理高分辨率3D建模。", "method": "Ultra3D 采用紧凑的VecSet表示在第一阶段生成粗糙对象布局，减少了token数量并加速了体素坐标预测。在第二阶段，引入了Part Attention，这是一种几何感知的局部注意力机制，将注意力计算限制在语义一致的部分区域内，以优化每个体素的潜在特征。此外，还构建了一个可扩展的部分注释管道，将原始网格转换为部分标记的稀疏体素。", "result": "Ultra3D 在潜在生成方面实现了高达6.7倍的速度提升，支持1024分辨率的高分辨率3D生成，并在视觉保真度和用户偏好方面达到了最先进的性能。", "conclusion": "Ultra3D 通过创新的VecSet表示和Part Attention机制，有效解决了现有3D生成框架的计算效率问题，实现了高质量和高效率的3D内容生成。", "translation": "稀疏体素表示的最新进展显著提高了3D内容生成的质量，实现了高分辨率建模和精细几何。然而，现有框架由于其两阶段扩散管道中注意力机制的二次复杂度而遭受严重的计算效率低下问题。在这项工作中，我们提出了Ultra3D，一个高效的3D生成框架，在不牺牲质量的情况下显著加速了稀疏体素建模。我们的方法利用紧凑的VecSet表示在第一阶段高效生成粗糙对象布局，减少了token数量并加速了体素坐标预测。为了在第二阶段细化每个体素的潜在特征，我们引入了Part Attention，这是一种几何感知的局部注意力机制，将注意力计算限制在语义一致的部分区域内。这种设计保留了结构连续性，同时避免了不必要的全局注意力，在潜在生成方面实现了高达6.7倍的速度提升。为了支持这种机制，我们构建了一个可扩展的部分注释管道，将原始网格转换为部分标记的稀疏体素。大量实验表明，Ultra3D 支持1024分辨率的高分辨率3D生成，并在视觉保真度和用户偏好方面达到了最先进的性能。", "summary": "Ultra3D 是一种新型高效的3D生成框架，旨在解决现有稀疏体素表示方法中注意力机制导致的计算效率低下问题。该方法通过在第一阶段使用紧凑的VecSet表示来加速粗糙对象布局的生成，并在第二阶段引入几何感知的局部注意力机制Part Attention来优化体素特征，从而避免了全局注意力计算。Ultra3D 支持高分辨率3D生成，并在实验中展现出显著的速度提升和卓越的生成质量，达到最先进的性能。", "keywords": "3D生成, 稀疏体素, 注意力机制, 效率, 高保真", "comments": "Ultra3D 的创新点在于其结合了VecSet表示和Part Attention机制，有效解决了3D生成中的计算效率瓶颈，尤其是在处理高分辨率数据时。Part Attention的局部化设计，即在语义一致的部分区域内进行注意力计算，是其效率提升的关键。此外，构建可扩展的部分注释管道也为该方法的实用性提供了保障。这项工作对于推动高效高保真3D内容生成具有重要意义。"}}
{"id": "2503.02832", "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "authors": ["Songming Zhang", "Xue Zhang", "Tong Zhang", "Bojie Hu", "Yufeng Chen", "Jinan Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Main Conference, code available at: this https URL", "url": "http://arxiv.org/abs/2503.02832v3", "summary": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.", "comment": "ACL 2025 Main Conference, code available at:\n  https://github.com/songmzhang/AlignDistil", "pdf_url": "http://arxiv.org/pdf/2503.02832v3", "cate": "cs.CL", "date": "2025-03-04", "updated": "2025-07-23", "AI": {"title_translation": "AlignDistil：作为自适应策略蒸馏的令牌级语言模型对齐", "tldr": "AlignDistil提出了一种令牌级的语言模型对齐方法，通过将DPO奖励引入RLHF目标并设计自适应机制，解决了现有方法中稀疏响应级奖励导致的问题，实现了更优的性能和更快的收敛速度。", "motivation": "现有的大型语言模型对齐方法（如RLHF和DPO）通常使用稀疏的、响应级的奖励或偏好标注来优化所有令牌，忽略了令牌级的奖励。这种做法可能错误地惩罚高质量令牌或鼓励低质量令牌，导致次优性能和收敛速度慢。", "method": "本研究提出了AlignDistil，一种RLHF等效的蒸馏方法，用于令牌级奖励优化。具体而言，它将DPO学习到的奖励引入RLHF目标，并从理论上证明了该目标与令牌级蒸馏过程的等效性，其中教师分布线性结合了DPO模型和参考模型的logits。此外，通过构建一个包含正向和反向DPO模型的对比DPO奖励，弥合了DPO模型奖励与纯奖励模型之间的准确性差距。为了避免不同令牌的欠优化和过优化，设计了一种令牌自适应的logit外推机制来构建每个令牌的适当教师分布。", "result": "实验结果表明，AlignDistil优于现有方法，并且由于其令牌级分布奖励优化，展示了快速收敛的特性。", "conclusion": "AlignDistil通过引入令牌级奖励优化和自适应策略蒸馏，有效解决了现有LLM对齐方法的局限性，实现了卓越的性能和更快的收敛速度。", "translation": "在现代大型语言模型（LLMs）中，LLM对齐至关重要，通常通过人类反馈强化学习（RLHF）和直接偏好优化（DPO）等方法实现。然而，在大多数现有的LLM对齐方法中，响应中的所有令牌都使用稀疏的、响应级奖励或偏好标注进行优化。对令牌级奖励的忽视可能会错误地惩罚高质量令牌或鼓励低质量令牌，导致次优性能和收敛速度慢。为了解决这个问题，我们提出了AlignDistil，一种用于令牌级奖励优化的RLHF等效蒸馏方法。具体而言，我们将DPO学习到的奖励引入到RLHF目标中，并从理论上证明了该目标与令牌级蒸馏过程的等效性，其中教师分布线性结合了DPO模型和参考模型的logits。在此基础上，我们通过构建一个包含正向和反向DPO模型的对比DPO奖励，进一步弥合了DPO模型奖励与纯奖励模型之间的准确性差距。此外，为了避免不同令牌的欠优化和过优化，我们设计了一种令牌自适应的logit外推机制来为每个令牌构建适当的教师分布。实验结果表明，我们的AlignDistil优于现有方法，并由于其令牌级分布奖励优化而展示出快速收敛。", "summary": "该论文提出了AlignDistil，一种新颖的令牌级语言模型对齐方法。针对现有LLM对齐方法（如RLHF和DPO）中因稀疏的响应级奖励而导致的性能次优和收敛慢的问题，AlignDistil将DPO奖励整合到RLHF目标中，并从理论上证明了其与令牌级蒸馏的等效性。该方法通过构建对比DPO奖励和设计令牌自适应的logit外推机制，实现了令牌级的优化。实验证明AlignDistil优于现有方法，并具有更快的收敛速度。", "keywords": "LLM对齐, 策略蒸馏, 令牌级优化, DPO, RLHF", "comments": "AlignDistil的创新点在于将LLM对齐从传统的响应级优化提升到令牌级，有效解决了现有方法中奖励稀疏性导致的问题。其理论证明了DPO奖励与RLHF目标的等效性，并结合自适应策略蒸馏，为LLM对齐提供了一个更精细、更高效的范式。这种令牌级优化有望在未来提升LLM的生成质量和训练效率。"}}
{"id": "2507.16871", "title": "Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks", "authors": ["Pietro Giuseppe Fré", "Federico Milanesio", "Guido Sanguinetti", "Matteo Santoro"], "categories": ["cs.LG", "hep-th"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      59 pages, 2 figures", "url": "http://arxiv.org/abs/2507.16871v1", "summary": "Recent work has identified non-compact symmetric spaces U/H as a promising\nclass of homogeneous manifolds to develop a geometrically consistent theory of\nneural networks. An initial implementation of these concepts has been presented\nin a twin paper under the moniker of Cartan Neural Networks, showing both the\nfeasibility and the performance of these geometric concepts in a machine\nlearning context. The current paper expands on the mathematical structures\nunderpinning Cartan Neural Networks, detailing the geometric properties of the\nlayers and how the maps between layers interact with such structures to make\nCartan Neural Networks covariant and geometrically interpretable. Together,\nthese twin papers constitute a first step towards a fully geometrically\ninterpretable theory of neural networks exploiting group-theoretic structures", "comment": "59 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.16871v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "非紧对称空间上的导航：Cartan神经网络的数学视角", "tldr": "本文从数学角度详细阐述了Cartan神经网络的几何结构，解释了其层间的协变性和可解释性，是利用群论结构构建几何可解释神经网络理论的第一步。", "motivation": "近期研究发现非紧对称空间U/H是发展几何一致神经网络理论的有前景的齐次流形。一篇姊妹论文已初步实现了基于这些概念的Cartan神经网络并展示了其可行性和性能，而本文旨在扩展其背后的数学结构。", "method": "本文扩展了支撑Cartan神经网络的数学结构，详细阐述了层的几何特性以及层间映射如何与这些结构相互作用，从而使Cartan神经网络具有协变性和几何可解释性。", "result": "本文详细阐述了Cartan神经网络的数学结构，包括层的几何特性以及层间映射如何与这些结构相互作用，以实现网络的协变性和几何可解释性。", "conclusion": "本文与一篇姊妹论文共同构成了利用群论结构构建完全几何可解释神经网络理论的第一步。", "translation": "最近的工作已将非紧对称空间U/H确定为一类有前景的齐次流形，可用于发展几何一致的神经网络理论。这些概念的初步实现在一篇名为“Cartan神经网络”的姊妹论文中得到了展示，显示了这些几何概念在机器学习背景下的可行性和性能。本论文扩展了支撑Cartan神经网络的数学结构，详细阐述了层的几何特性以及层间映射如何与这些结构相互作用，从而使Cartan神经网络具有协变性和几何可解释性。这两篇姊妹论文共同构成了利用群论结构构建完全几何可解释神经网络理论的第一步。", "summary": "本文从数学角度深入探讨了Cartan神经网络的几何基础，阐述了非紧对称空间U/H作为其构建框架的潜力。它详细分析了神经网络层的几何特性以及层间映射如何确保网络的协变性和几何可解释性。结合一篇姊妹论文，这项工作为发展基于群论结构的完全几何可解释神经网络理论奠定了基础。", "keywords": "Cartan神经网络, 非紧对称空间, 几何深度学习, 协变性, 群论结构", "comments": "本文的创新之处在于其深入探讨了神经网络的几何结构，特别是基于非紧对称空间U/H的Cartan神经网络。它不仅仅停留在概念层面，而是详细解释了如何通过几何特性和层间映射实现网络的协变性和可解释性，这对于构建更具原理性和可信赖的AI模型具有重要意义。与传统的基于欧几里得空间的神经网络相比，这种方法为处理复杂数据结构提供了新的视角，并有望推动几何深度学习的发展。"}}
{"id": "2507.17116", "title": "Probabilistic Graphical Models: A Concise Tutorial", "authors": ["Jacqueline Maasch", "Willie Neiswanger", "Stefano Ermon", "Volodymyr Kuleshov"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.17116v1", "summary": "Probabilistic graphical modeling is a branch of machine learning that uses\nprobability distributions to describe the world, make predictions, and support\ndecision-making under uncertainty. Underlying this modeling framework is an\nelegant body of theory that bridges two mathematical traditions: probability\nand graph theory. This framework provides compact yet expressive\nrepresentations of joint probability distributions, yielding powerful\ngenerative models for probabilistic reasoning.\n  This tutorial provides a concise introduction to the formalisms, methods, and\napplications of this modeling framework. After a review of basic probability\nand graph theory, we explore three dominant themes: (1) the representation of\nmultivariate distributions in the intuitive visual language of graphs, (2)\nalgorithms for learning model parameters and graphical structures from data,\nand (3) algorithms for inference, both exact and approximate.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.17116v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "概率图模型：简明教程", "tldr": "本教程简要介绍了概率图模型，涵盖了其形式、方法和应用，特别是多元分布表示、模型学习算法和推理算法。", "motivation": "本教程旨在为概率图模型这一建模框架提供一个简明的介绍，涵盖其形式、方法和应用。", "method": "教程首先回顾了基础概率论和图论，然后探讨了三个主要主题：使用图的直观视觉语言表示多元分布、从数据中学习模型参数和图结构的算法，以及精确和近似推理算法。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "概率图建模是机器学习的一个分支，它使用概率分布来描述世界、进行预测并支持不确定性下的决策。该建模框架的基础是一个优雅的理论体系，它连接了概率论和图论这两个数学传统。这个框架提供了紧凑而富有表现力的联合概率分布表示，为概率推理产生了强大的生成模型。\n本教程提供了对该建模框架的形式、方法和应用的简明介绍。在回顾了基础概率论和图论之后，我们探讨了三个主要主题：（1）用图的直观视觉语言表示多元分布，（2）从数据中学习模型参数和图结构的算法，以及（3）精确和近似推理算法。", "summary": "本教程对概率图模型进行了简明介绍，概率图模型是机器学习的一个分支，它利用概率分布描述世界并支持不确定性下的决策。该框架结合了概率论和图论，提供了紧凑且富有表现力的联合概率分布表示。教程内容涵盖了基础概率论和图论的复习，并深入探讨了三大主题：多元分布的图形表示、模型参数和结构的学习算法，以及精确和近似推理算法。", "keywords": "概率图模型, 机器学习, 概率论, 图论, 推理", "comments": "这是一篇针对初学者的优秀教程，它以清晰和结构化的方式介绍了概率图模型的核心概念。它的创新之处在于将复杂的理论以易于理解的方式呈现，为读者构建扎实的知识基础。其重要性在于为机器学习领域中日益重要的概率图模型提供了一个快速入门的途径。"}}
{"id": "2507.17748", "title": "Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility", "authors": ["Melih Barsbey", "Lucas Prieto", "Stefanos Zafeiriou", "Tolga Birdal"], "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025, 23 pages", "url": "http://arxiv.org/abs/2507.17748v1", "summary": "Robustness and resource-efficiency are two highly desirable properties for\nmodern machine learning models. However, achieving them jointly remains a\nchallenge. In this paper, we position high learning rates as a facilitator for\nsimultaneously achieving robustness to spurious correlations and network\ncompressibility. We demonstrate that large learning rates also produce\ndesirable representation properties such as invariant feature utilization,\nclass separation, and activation sparsity. Importantly, our findings indicate\nthat large learning rates compare favorably to other hyperparameters and\nregularization methods, in consistently satisfying these properties in tandem.\nIn addition to demonstrating the positive effect of large learning rates across\ndiverse spurious correlation datasets, models, and optimizers, we also present\nstrong evidence that the previously documented success of large learning rates\nin standard classification tasks is likely due to its effect on addressing\nhidden/rare spurious correlations in the training dataset.", "comment": "Accepted at ICCV 2025, 23 pages", "pdf_url": "http://arxiv.org/pdf/2507.17748v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大学习率同时实现对虚假相关性的鲁棒性和可压缩性", "tldr": "本文提出大学习率能够同时提升模型对虚假相关性的鲁棒性和网络的压缩性，并揭示其在标准分类任务中成功的潜在原因。", "motivation": "现代机器学习模型面临鲁棒性和资源效率（可压缩性）难以同时实现的问题。", "method": "通过实验证明大学习率能够产生理想的表示特性，如不变特征利用、类别分离和激活稀疏性。并在多种虚假相关数据集、模型和优化器上验证其积极效果。", "result": "大学习率能够同时实现对虚假相关性的鲁棒性和网络可压缩性。此外，大学习率还能产生不变特征利用、类别分离和激活稀疏性等理想的表示特性。研究发现大学习率在同时满足这些特性方面优于其他超参数和正则化方法。", "conclusion": "大学习率不仅能同时提升模型对虚假相关性的鲁棒性和网络可压缩性，而且其在标准分类任务中的成功可能归因于其解决了训练数据集中隐藏/罕见的虚假相关性。", "translation": "鲁棒性和资源效率是现代机器学习模型非常理想的两个特性。然而，同时实现它们仍然是一个挑战。在本文中，我们将高学习率定位为同时实现对虚假相关性的鲁棒性和网络可压缩性的促进因素。我们证明了大学习率也能产生理想的表示特性，例如不变特征利用、类别分离和激活稀疏性。重要的是，我们的发现表明，大学习率在持续同时满足这些特性方面优于其他超参数和正则化方法。除了证明大学习率在各种虚假相关数据集、模型和优化器上的积极作用外，我们还提供了有力的证据，表明大学习率在标准分类任务中先前记录的成功很可能归因于其在解决训练数据集中隐藏/罕见的虚假相关性方面的作用。", "summary": "本研究提出大学习率能够同时提升机器学习模型对虚假相关性的鲁棒性和网络的压缩性，解决了鲁棒性与资源效率难以兼得的挑战。论文通过实验证明大学习率能带来不变特征利用、类别分离和激活稀疏性等理想的表示特性，并优于其他超参数和正则化方法。此外，研究还指出大学习率在标准分类任务中的成功很可能源于其处理隐藏/罕见虚假相关性的能力。", "keywords": "大学习率, 鲁棒性, 可压缩性, 虚假相关性, 表示学习", "comments": "这篇论文提出了一种新颖的视角，将大学习率不仅仅视为优化过程中的一个超参数，而是作为一种能够同时提升模型鲁棒性和资源效率的机制。其创新点在于揭示了大学习率对模型内部表示特性（如不变性、分离性、稀疏性）的积极影响，并进一步提出其在标准分类任务中的成功可能与处理虚假相关性有关，这为理解深度学习的泛化能力提供了新的线索。该发现对于模型设计和训练策略具有重要的指导意义。"}}
{"id": "2507.17171", "title": "Ontological Definition of Seamless Digital Engineering Based on ISO/IEC 25000-Series SQuaRE Product Quality Model", "authors": ["James S. Wheaton", "Daniel R. Herber"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      to be published in INCOSE IS 2025 conference proceedings; 18 pages, 9 listings, 3 figures, 2 tables", "url": "http://arxiv.org/abs/2507.17171v1", "summary": "Since the introduction of Digital Engineering (DE) as a well-defined concept\nin 2018, organizations and industry groups have been working to interpret the\nDE concepts to establish consistent meta-models of those interrelated concepts\nfor integration into their DE processes and tools. To reach the breadth and\ndepth of DE concept definitions, the interpretation of international standard\nsources is necessary, including ISO/IEC/IEEE 15288, 24765, 42000-series, 15408,\n15206, 27000-series, and 25000-series, to effectively model the knowledge\ndomain where digital engineering applies. The harmonization of the concepts\nused in these international standards continues to improve with each revision,\nbut it may be more effectively accomplished by relying on the descriptive logic\nformalized in the Web Ontology Language (OWL 2 DL). This paper presents a\nverified and consistent ontology based on the Basic Formal Ontology (BFO) and\nCommon Core Ontologies (CCO) that defines Seamless Digital Engineering as a\ndigital tooling paradigm that relies on formal verification of digital\ninterfaces to provide a system-level qualification of the assured integrity of\na Digital Engineering Environment. The present work defines classes and\nequivalence axioms, while using only the BFO- and CCO-defined object properties\nthat relate them, to provide a baseline analysis that may inform future\nDE-related ontology development, using a case study to formally define the\n`seamless' quality in relation to the updated ISO 25010 SQuaRE product quality\nmodel. We identified ISO meta-model inconsistencies that are resolvable using\nthe BFO/CCO ontological framework, and define `seamless' as both a system\nintegration quality and a Human-Computer Interface quality-in-use, working to\ndisambiguate this concept in the context of DE.", "comment": "to be published in INCOSE IS 2025 conference proceedings; 18 pages, 9\n  listings, 3 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.17171v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于ISO/IEC 25000系列SQuaRE产品质量模型的无缝数字工程本体定义", "tldr": "本文基于BFO和CCO本体论，为无缝数字工程提供了本体定义，解决了ISO元模型的不一致性，并正式阐明了“无缝”质量。", "motivation": "为了更有效地解释和统一数字工程（DE）概念（尤其是“无缝”质量）在各种国际标准中的应用，本研究旨在利用OWL 2 DL等形式化描述逻辑，改进现有标准协调方法的不足。", "method": "本文基于基础形式本体论（BFO）和通用核心本体论（CCO）构建了一个经过验证且一致的本体，以定义无缝数字工程。它定义了类和等价公理，仅使用BFO和CCO定义的对象属性，并通过一个案例研究，结合ISO 25010 SQuaRE产品质量模型，正式定义了“无缝”质量。", "result": "研究识别了ISO元模型中可利用BFO/CCO本体框架解决的不一致性。它将“无缝”正式定义为系统集成质量和人机界面使用质量，从而在DE背景下消除了该概念的歧义。", "conclusion": "基于BFO/CCO的本体框架提供了一种正式且一致的方法来定义诸如“无缝数字工程”等复杂概念，解决了现有国际标准（如ISO元模型）中的不一致性，并为未来数字工程本体开发澄清了“无缝性”等质量。", "translation": "自2018年数字工程（DE）作为一个明确的概念被引入以来，各组织和行业团体一直致力于解释DE概念，以建立这些相互关联概念的一致元模型，并将其整合到其DE过程和工具中。为了达到DE概念定义的广度和深度，有必要解释国际标准来源，包括ISO/IEC/IEEE 15288、24765、42000系列、15408、15206、27000系列和25000系列，以有效地建模数字工程适用的知识领域。这些国际标准中所用概念的协调性随着每次修订而不断提高，但通过依赖Web本体语言（OWL 2 DL）中形式化的描述逻辑，可以更有效地实现这一点。本文提出了一个基于基础形式本体论（BFO）和通用核心本体论（CCO）的经过验证且一致的本体，它将无缝数字工程定义为一种数字工具范式，该范式依赖于数字接口的正式验证，以提供数字工程环境的完整性系统级认证。当前工作定义了类和等价公理，同时仅使用BFO和CCO定义的对象属性来关联它们，从而提供一个基线分析，该分析可以为未来与DE相关的本体开发提供信息，并使用案例研究来正式定义与更新的ISO 25010 SQuaRE产品质量模型相关的“无缝”质量。我们识别了可以使用BFO/CCO本体框架解决的ISO元模型不一致性，并将“无缝”定义为系统集成质量和人机界面使用质量，致力于在DE背景下消除该概念的歧义。", "summary": "本文提出了一种无缝数字工程的本体定义，利用基础形式本体论（BFO）和通用核心本体论（CCO）提供了一个形式化且一致的元模型。它解决了数字工程（DE）概念（尤其是“无缝”质量）在不同国际标准中协调的挑战。通过与ISO 25010 SQuaRE产品质量模型相关的案例研究，该工作识别了可通过BFO/CCO框架解决的ISO元模型不一致性，并将“无缝”正式定义为系统集成质量和人机界面使用质量，从而在DE背景下消除了其歧义。该方法旨在为未来DE相关的本体开发提供信息。", "keywords": "数字工程, 本体论, 无缝, ISO/IEC 25000, SQuaRE", "comments": "本文通过将形式本体论（BFO/CCO）应用于数字工程这一复杂领域，特别是解决了“无缝性”这一模糊概念，做出了重要贡献。其创新之处在于使用描述逻辑（OWL 2 DL）来解决国际标准元模型中的不一致性，这对于实现DE环境中的真正互操作性和完整性至关重要。这项工作提供了一个基础分析，可以指导未来DE中的标准化定义和工具开发。"}}
{"id": "2507.16995", "title": "Qubit-Efficient Quantum Algorithm for Linear Differential Equations", "authors": ["Di Fang", "David Lloyd George", "Yu Tong"], "categories": ["quant-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16995v1", "summary": "As quantum hardware rapidly advances toward the early fault-tolerant era, a\nkey challenge is to develop quantum algorithms that are not only theoretically\nsound but also hardware-friendly on near-term devices. In this work, we propose\na quantum algorithm for solving linear ordinary differential equations (ODEs)\nwith a provable runtime guarantee. Our algorithm uses only a single ancilla\nqubit, and is locality preserving, i.e., when the coefficient matrix of the ODE\nis $k$-local, the algorithm only needs to implement the time evolution of\n$(k+1)$-local Hamiltonians. We also discuss the connection between our proposed\nalgorithm and Lindbladian simulation as well as its application to the\ninteracting Hatano-Nelson model, a widely studied non-Hermitian model with rich\nphenomenology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16995v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "量子比特高效的线性微分方程量子算法", "tldr": "提出了一种单辅助量子比特且局部性保持的量子算法，用于在近期量子设备上高效解决线性常微分方程。", "motivation": "随着量子硬件向早期容错时代快速发展，开发不仅理论上可靠而且在近期设备上对硬件友好的量子算法是一个关键挑战。", "method": "提出了一种量子算法，用于解决线性常微分方程（ODEs），并具有可证明的运行时保证。该算法仅使用一个辅助量子比特，并且具有局部性保持特性，即当ODE的系数矩阵是k局部的，算法只需要实现(k+1)局部哈密顿量的时演化。", "result": "成功提出了一种解决线性常微分方程的量子算法，该算法具有可证明的运行时保证，并且在量子比特效率和局部性方面表现出色。", "conclusion": "该研究提出了一种适用于近期量子设备的线性常微分方程量子算法，通过其量子比特效率和局部性保持特性，解决了当前量子计算硬件的实际挑战。", "translation": "随着量子硬件迅速发展进入早期容错时代，一个关键挑战是开发不仅理论上可靠而且在近期设备上对硬件友好的量子算法。在这项工作中，我们提出了一种用于求解线性常微分方程（ODEs）的量子算法，并具有可证明的运行时保证。我们的算法仅使用一个辅助量子比特，并且具有局部性保持特性，即当ODE的系数矩阵是k局部的，算法只需要实现(k+1)局部哈密顿量的时演化。我们还讨论了我们提出的算法与林德布拉德模拟之间的联系，以及其在相互作用的Hatano-Nelson模型（一个广泛研究的具有丰富现象学的非厄米模型）中的应用。", "summary": "这项工作提出了一种用于解决线性常微分方程的量子算法，旨在解决近期量子硬件的实际限制。该算法仅需一个辅助量子比特，并保持局部性，意味着对于k-局部的系数矩阵，仅需模拟(k+1)-局部哈密顿量。该研究还探讨了算法与Lindbladian模拟的联系及其在Hatano-Nelson模型中的应用。", "keywords": "量子算法, 线性微分方程, 量子比特高效, 局部性保持, 近期量子设备", "comments": "这篇论文的创新点在于提出了一个量子比特高效且局部性保持的线性微分方程量子算法，这对于在当前受限的近期量子设备上实现实际应用至关重要。其对单辅助量子比特的使用和局部性保持特性显著降低了硬件要求，使其更具可行性。"}}
{"id": "2507.17226", "title": "A \"watch your replay videos\" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection", "authors": ["Sarah \"Magz\" Fernandez", "Greg L Nelson"], "categories": ["cs.HC", "K.3"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17226v1", "summary": "Generative AI is disrupting computing education. Most interventions focus on\nteaching GenAI use rather than helping students understand how AI changes their\nprogramming process. We designed and deployed a novel comparative video\nreflection assignment adapting the Describe, Examine, then Articulate Learning\n(DEAL) framework. In an introductory software engineering course, students\nrecorded themselves programming during their team project two times: first\nwithout, then with using generative AI. Students then analyzed their own videos\nusing a scaffolded set of reflection questions, including on their programming\nprocess and human, internet, and AI help-seeking. We conducted a qualitative\nthematic analysis of the reflections, finding students developed insights about\nplanning, debugging, and help-seeking behaviors that transcended AI use.\nStudents reported learning to slow down and understand before writing or\ngenerating code, recognized patterns in their problem-solving approaches, and\narticulated specific process improvements. Students also learned and reflected\non AI limits and downsides, and strategies to use AI more critically, including\nbetter prompting but also to benefit their learning instead of just completing\ntasks. Unexpectedly, the comparative reflection also scaffolded reflection on\nprogramming not involving AI use, and even led to students spontaneously\nsetting future goals to adopt video and other regular reflection. This work\ndemonstrates structured reflection on programming session videos can develop\nmetacognitive skills essential for programming with and without generative AI\nand also lifelong learning in our evolving field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17226v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一项关于编程时使用与不使用生成式AI的“观看回放视频”反思作业：学习编程、批判性AI使用与局限性以及反思", "tldr": "学生通过观看自己编程过程的回放视频，比较使用与不使用生成式AI的编程体验，从而加深了对编程、批判性AI使用和自我反思的理解。", "motivation": "大多数关于生成式AI的干预措施侧重于教授其使用方法，而非帮助学生理解AI如何改变他们的编程过程，因此本研究旨在通过一项新颖的比较性视频反思作业来解决这一问题。", "method": "研究设计并部署了一项新颖的比较性视频反思作业，该作业改编自“描述、检查、然后阐明学习（DEAL）”框架。在入门级软件工程课程中，学生们录制了两次编程过程：一次不使用生成式AI，另一次使用。随后，学生利用一套有支架的反思问题（包括关于他们的编程过程和寻求人类、互联网、AI帮助的问题）分析自己的视频。研究对反思内容进行了定性主题分析。", "result": "学生对规划、调试和寻求帮助的行为形成了超越AI使用的洞察；报告学会了在编写或生成代码前放慢速度并理解；认识到解决问题方法中的模式；阐明了具体的流程改进。学生还学习并反思了AI的局限性和缺点，以及更批判性地使用AI的策略。意外的是，这种比较性反思还促进了对不涉及AI使用的编程的反思，甚至促使学生自发设定未来目标，以采用视频和其他定期反思方法。", "conclusion": "本研究表明，对编程会话视频进行结构化反思可以培养元认知技能，这对于无论是否使用生成式AI的编程以及在不断发展的领域中的终身学习都至关重要。", "translation": "生成式AI正在颠覆计算机教育。大多数干预措施侧重于教授生成式AI的使用，而不是帮助学生理解AI如何改变他们的编程过程。我们设计并部署了一项新颖的比较性视频反思作业，该作业改编自“描述、检查、然后阐明学习（DEAL）”框架。在入门级软件工程课程中，学生们在团队项目中录制了两次编程过程：第一次没有使用生成式AI，第二次使用了。然后，学生利用一套有支架的反思问题来分析自己的视频，包括关于他们的编程过程以及寻求人类、互联网和AI帮助的问题。我们对反思内容进行了定性主题分析，发现学生对规划、调试和寻求帮助的行为形成了超越AI使用的洞察。学生报告说，他们在编写或生成代码之前学会了放慢速度并理解，认识到他们解决问题方法中的模式，并阐明了具体的流程改进。学生还学习并反思了AI的局限性和缺点，以及更批判性地使用AI的策略，包括更好地提示，以及如何使AI有利于他们的学习而不仅仅是完成任务。出乎意料的是，这种比较性反思也促进了对不涉及AI使用的编程的反思，甚至促使学生自发设定未来目标，以采用视频和其他定期反思方法。这项工作表明，对编程会话视频进行结构化反思可以培养元认知技能，这对于无论是否使用生成式AI的编程以及在我们不断发展的领域中的终身学习都至关重要。", "summary": "本研究提出了一种新颖的比较性视频反思作业，旨在帮助学生理解生成式AI如何影响其编程过程。学生在不使用和使用AI的情况下录制编程视频，并基于DEAL框架进行反思分析。结果显示，学生不仅深化了对编程、调试和寻求帮助的理解，还学会了批判性使用AI，并意外地促进了元认知和终身学习的意识。这项工作强调了结构化视频反思在培养编程和学习元认知技能方面的重要性。", "keywords": "生成式AI, 编程教育, 视频反思, 元认知, 批判性AI使用", "comments": "这项研究的创新之处在于其独特的比较性视频反思方法，它超越了单纯教授AI工具使用的层面，深入探讨了AI对学生编程思维和行为模式的深层影响。通过让学生自我观察和分析，有效地培养了他们的元认知能力和批判性思维，这对于在AI时代进行编程和终身学习至关重要。其发现学生能够将反思能力泛化到非AI相关编程，并自发设定未来学习目标，进一步凸显了该教学干预的有效性和深远意义。"}}
{"id": "2507.17744", "title": "Yume: An Interactive World Generation Model", "authors": ["Xiaofeng Mao", "Shaoheng Lin", "Zhen Li", "Chuanhao Li", "Wenshuo Peng", "Tong He", "Jiangmiao Pang", "Mingmin Chi", "Yu Qiao", "Kaipeng Zhang"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17744v1", "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17744v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Yume：一个交互式世界生成模型", "tldr": "Yume是一个交互式世界生成模型，能从图像创建动态世界，支持键盘探索，并引入MVDT、AAM、TTS-SDE等技术提升生成质量和控制。", "motivation": "创建并允许探索和控制一个交互式、真实且动态的世界，其输入可以来源于图像、文本或视频，并能通过外设或神经信号进行交互。", "method": "论文提出了Yume的预览版，该版本能从输入图像生成动态世界并支持键盘探索。其框架包含四个主要组件：1) 摄像机运动量化，用于稳定的训练和用户友好的键盘输入交互；2) 带有记忆模块的Masked Video Diffusion Transformer (MVDT)，用于自回归无限视频生成；3) 采样器中引入免训练的Anti-Artifact Mechanism (AAM) 和基于随机微分方程的时间旅行采样 (TTS-SDE)，以提升视觉质量和精确控制；4) 通过对抗蒸馏和缓存机制的协同优化实现模型加速。模型使用高质量的世界探索数据集\\sekai进行训练。", "result": "Yume在多样化的场景和应用中取得了显著成果。", "conclusion": "论文展示了Yume预览版在从输入图像生成动态世界并支持键盘探索方面的能力，并通过多组件框架实现了高保真和交互式视频世界生成。", "translation": "Yume旨在利用图像、文本或视频来创建一个交互式、真实且动态的世界，允许使用外围设备或神经信号进行探索和控制。在本报告中，我们介绍了Yume的预览版，它能从输入图像创建动态世界，并允许使用键盘操作进行世界探索。为了实现这种高保真和交互式视频世界生成，我们引入了一个精心设计的框架，该框架由四个主要组件组成，包括摄像机运动量化、视频生成架构、高级采样器和模型加速。首先，我们量化摄像机运动，以实现稳定的训练和用户友好的键盘输入交互。然后，我们引入了带有记忆模块的Masked Video Diffusion Transformer (MVDT)，以自回归方式进行无限视频生成。之后，在采样器中引入了免训练的抗伪影机制 (AAM) 和基于随机微分方程的时间旅行采样 (TTS-SDE)，以获得更好的视觉质量和更精确的控制。此外，我们通过对抗蒸馏和缓存机制的协同优化来研究模型加速。我们使用高质量的世界探索数据集\\sekai来训练Yume，它在多样化的场景和应用中取得了显著成果。所有数据、代码库和模型权重都可在https://github.com/stdstu12/YUME 上获取。Yume将每月更新以实现其最初目标。项目页面：https://stdstu12.github.io/YUME-Project/。", "summary": "Yume是一个旨在通过图像、文本或视频创建交互式、真实、动态世界并支持探索和控制的模型。本论文介绍了Yume的预览版，该版本可从单一图像生成动态世界并支持键盘探索。为实现高保真和交互性，模型提出一个由摄像机运动量化、带有记忆模块的MVDT视频生成架构、包含AAM和TTS-SDE的高级采样器以及模型加速机制组成的框架。Yume在高质量数据集上训练后，在多样化场景和应用中展示了显著的生成效果。", "keywords": "交互式世界生成, 视频扩散模型, MVDT, 动态世界, 虚拟现实", "comments": "这篇论文展示了在交互式世界生成领域的创新性尝试，通过结合视频扩散模型、记忆模块、高级采样策略和优化加速技术，实现了从静态图像到动态可探索世界的转换。其模块化设计有助于未来功能的扩展和性能提升。项目计划每月更新，表明其是一个持续发展且有潜力的研究方向。"}}
{"id": "2507.17196", "title": "Hybrid Semantic-Complementary Transmission for High-Fidelity Image Reconstruction", "authors": ["Hyelin Nam", "Jihong Park", "Jinho Choi", "Seong-Lyun Kim"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17196v1", "summary": "Recent advances in semantic communication (SC) have introduced neural network\n(NN)-based transceivers that convey semantic representation (SR) of signals\nsuch as images. However, these NNs are trained over diverse image distributions\nand thus often fail to reconstruct fine-grained image-specific details. To\novercome this limited reconstruction fidelity, we propose an extended SC\nframework, hybrid semantic communication (HSC), which supplements SR with\ncomplementary representation (CR) capturing residual image-specific\ninformation. The CR is constructed at the transmitter, and is combined with the\nactual SC outcome at the receiver to yield a high-fidelity recomposed image.\nWhile the transmission load of SR is fixed due to its NN-based structure, the\nload of CR can be flexibly adjusted to achieve a desirable fidelity. This\ncontrollability directly influences the final reconstruction error, for which\nwe derive a closed-form expression and the corresponding optimal CR. Simulation\nresults demonstrate that HSC substantially reduces MSE compared to the baseline\nSC without CR transmission across various channels and NN architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17196v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于高保真图像重建的混合语义-互补传输", "tldr": "本文提出了一种混合语义通信（HSC）框架，通过补充互补表示（CR）来解决现有语义通信（SC）在图像重建中细节丢失的问题，显著提高了图像重建的保真度。", "motivation": "现有的基于神经网络的语义通信（SC）系统在处理多样图像分布时，往往无法重建图像的细粒度细节，导致重建保真度有限。", "method": "本文提出了一种混合语义通信（HSC）框架，通过用互补表示（CR）补充语义表示（SR）来捕获剩余的图像特定信息。CR在发送端构建，并在接收端与实际的SC结果结合以获得高保真重构图像。CR的传输负载可以灵活调整，并且推导了最终重建误差的闭式表达式和相应的最优CR。", "result": "仿真结果表明，与没有CR传输的基线SC相比，HSC在各种信道和神经网络架构下显著降低了均方误差（MSE）。", "conclusion": "通过引入可灵活调整负载的互补表示，混合语义通信（HSC）框架能够有效克服传统语义通信在图像细节重建上的不足，显著提高图像重建的保真度。", "translation": "语义通信（SC）的最新进展引入了基于神经网络（NN）的收发器，可以传输图像等信号的语义表示（SR）。然而，这些神经网络在多样化的图像分布上进行训练，因此常常无法重建细粒度的图像特定细节。为了克服这种有限的重建保真度，我们提出了一种扩展的SC框架——混合语义通信（HSC），它通过补充捕获剩余图像特定信息的互补表示（CR）来增强SR。CR在发送端构建，并在接收端与实际的SC结果结合，以产生高保真重构图像。虽然SR的传输负载由于其基于NN的结构而固定，但CR的负载可以灵活调整以实现所需的保真度。这种可控性直接影响最终的重建误差，我们为此推导了闭式表达式和相应的最优CR。仿真结果表明，与没有CR传输的基线SC相比，HSC在各种信道和NN架构下显著降低了MSE。", "summary": "本文提出了一种混合语义通信（HSC）框架，旨在解决现有语义通信（SC）在图像重建中细节丢失导致保真度有限的问题。HSC通过在语义表示（SR）之外引入互补表示（CR）来捕获图像特有的残余信息，并在接收端结合两者进行高保真图像重建。该方法允许灵活调整CR的传输负载以控制重建保真度，并推导了相应的最优CR。仿真结果验证了HSC在降低均方误差（MSE）方面的显著优势。", "keywords": "语义通信, 混合传输, 图像重建, 互补表示, 保真度", "comments": "该论文提出了一种创新的混合方法，通过结合语义表示（SR）和可灵活调整负载的互补表示（CR），有效地解决了传统语义通信在图像细节重建方面的不足。其主要创新点在于引入CR来捕获细粒度信息，并推导了最优CR的理论表达式，为实现高保真图像重建提供了新的途径。该方法对于提升语义通信在图像传输领域的实用性具有重要意义，但目前仅基于仿真结果，未来可能需要实际系统验证。"}}
{"id": "2507.17479", "title": "SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving", "authors": ["Chuang Chen", "Xiaolin Qin", "Jing Hu", "Wenyi Ge"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17479v1", "summary": "Upsampling LiDAR point clouds in autonomous driving scenarios remains a\nsignificant challenge due to the inherent sparsity and complex 3D structures of\nthe data. Recent studies have attempted to address this problem by converting\nthe complex 3D spatial scenes into 2D image super-resolution tasks. However,\ndue to the sparse and blurry feature representation of range images, accurately\nreconstructing detailed and complex spatial topologies remains a major\ndifficulty. To tackle this, we propose a novel sparse point cloud upsampling\nmethod named SRMambaV2, which enhances the upsampling accuracy in long-range\nsparse regions while preserving the overall geometric reconstruction quality.\nSpecifically, inspired by human driver visual perception, we design a\nbiomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the\nfeature distribution in distant sparse areas. Meanwhile, we introduce a\ndual-branch network architecture to enhance the representation of sparse\nfeatures. In addition, we introduce a progressive adaptive loss (PAL) function\nto further refine the reconstruction of fine-grained details during the\nupsampling process. Experimental results demonstrate that SRMambaV2 achieves\nsuperior performance in both qualitative and quantitative evaluations,\nhighlighting its effectiveness and practical value in automotive sparse point\ncloud upsampling tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17479v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SRMambaV2：自动驾驶中稀疏点云上采样的仿生注意力机制", "tldr": "SRMambaV2提出了一种新的稀疏点云上采样方法，通过仿生2D选择性扫描自注意力机制、双分支网络架构和渐进式自适应损失函数，提高了自动驾驶中长距离稀疏区域的上采样精度和整体几何重建质量。", "motivation": "自动驾驶场景中LiDAR点云的上采样面临挑战，因为数据固有的稀疏性和复杂的3D结构。现有方法将3D场景转换为2D图像超分辨率任务，但由于深度图特征表示的稀疏和模糊，准确重建详细复杂的空间拓扑仍然是主要难题。", "method": "本文提出了SRMambaV2，一种新的稀疏点云上采样方法。它设计了一种仿生2D选择性扫描自注意力（2DSSA）机制来建模远处稀疏区域的特征分布，灵感来源于人类驾驶员的视觉感知。同时，引入了双分支网络架构以增强稀疏特征的表示。此外，还引入了渐进式自适应损失（PAL）函数，以在采样过程中进一步细化细粒度细节的重建。", "result": "实验结果表明，SRMambaV2在定性和定量评估中均取得了优越的性能，突出了其在汽车稀疏点云上采样任务中的有效性和实用价值。", "conclusion": "SRMambaV2通过其创新的仿生注意力机制、双分支网络和渐进式自适应损失函数，成功解决了自动驾驶中稀疏点云上采样的挑战，显著提高了上采样精度和几何重建质量。", "translation": "自动驾驶场景中的LiDAR点云上采样仍然是一个重大挑战，这归因于数据的固有稀疏性和复杂的3D结构。最近的研究试图通过将复杂的3D空间场景转换为2D图像超分辨率任务来解决这个问题。然而，由于深度图特征表示的稀疏和模糊，准确重建详细复杂的空间拓扑仍然是一个主要难题。为了解决这个问题，我们提出了一种名为SRMambaV2的新型稀疏点云上采样方法，该方法在保持整体几何重建质量的同时，提高了长距离稀疏区域的上采样精度。具体来说，受人类驾驶员视觉感知的启发，我们设计了一种仿生2D选择性扫描自注意力（2DSSA）机制来建模远处稀疏区域的特征分布。同时，我们引入了一个双分支网络架构来增强稀疏特征的表示。此外，我们引入了一个渐进式自适应损失（PAL）函数，以在上采样过程中进一步细化细粒度细节的重建。实验结果表明，SRMambaV2在定性和定量评估中均取得了优越的性能，突出了其在汽车稀疏点云上采样任务中的有效性和实用价值。", "summary": "SRMambaV2是一种针对自动驾驶中稀疏点云上采样的新方法，旨在解决现有方法在处理稀疏模糊深度图时重建细节困难的问题。该方法引入了受人类视觉感知启发的仿生2D选择性扫描自注意力机制，用于建模远距离稀疏区域特征；采用双分支网络架构增强稀疏特征表示；并利用渐进式自适应损失函数精细化细节重建。实验证明，SRMambaV2在性能上优于现有方法，提升了长距离稀疏区域的上采样精度和整体几何重建质量。", "keywords": "稀疏点云上采样, 仿生注意力, 自动驾驶, LiDAR, SRMambaV2", "comments": "该论文的创新点在于引入了受人类视觉感知启发的仿生2D选择性扫描自注意力机制，这为处理稀疏点云数据提供了一个新颖的视角。双分支网络架构和渐进式自适应损失函数也进一步提升了模型的性能。该方法在自动驾驶领域的应用潜力巨大，有助于提高LiDAR点云数据的利用效率和自动驾驶系统的感知能力。"}}
{"id": "2507.17094", "title": "PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search", "authors": ["Sukjin Kim", "Seongyeon Park", "Si Ung Noh", "Junguk Hong", "Taehee Kwon", "Hunseong Lim", "Jinho Lee"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      ATC 2025", "url": "http://arxiv.org/abs/2507.17094v1", "summary": "Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in\nnumerous applications, such as recommendation systems, natural language\nprocessing, and computer vision. While recent works on GPU-based acceleration\nhave significantly advanced ANNS performance, the ever-growing scale of\ndatasets now demands efficient multi-GPU solutions. However, the design of\nexisting works overlooks multi-GPU scalability, resulting in naive approaches\nthat treat additional GPUs as a means to extend memory capacity for large\ndatasets. This inefficiency arises from partitioning the dataset and\nindependently searching for data points similar to the queries in each GPU. We\ntherefore propose PathWeaver, a novel multi-GPU framework designed to scale and\naccelerate ANNS for large datasets. First, we propose pipelining-based path\nextension, a GPU-aware pipelining mechanism that reduces prior work's redundant\nsearch iterations by leveraging GPU-to-GPU communication. Second, we design\nghost staging that leverages a representative dataset to identify optimal query\nstarting points, reducing the search space for challenging queries. Finally, we\nintroduce direction-guided selection, a data selection technique that filters\nirrelevant points early in the search process, minimizing unnecessary memory\naccesses and distance computations. Comprehensive evaluations across diverse\ndatasets demonstrate that PathWeaver achieves 3.24$\\times$ geomean speedup and\nup to 5.30$\\times$ speedup on 95% recall rate over state-of-the-art\nmulti-GPU-based ANNS frameworks.", "comment": "ATC 2025", "pdf_url": "http://arxiv.org/pdf/2507.17094v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PathWeaver：一种用于基于图的近似最近邻搜索的高吞吐量多GPU系统", "tldr": "PathWeaver是一个新的多GPU框架，通过流水线、幽灵暂存和方向引导选择等技术，显著加速了大规模数据集的图基近似最近邻搜索，性能比现有技术提升高达5.30倍。", "motivation": "图基近似最近邻搜索（ANNS）在推荐系统、自然语言处理和计算机视觉等众多应用中广泛采用。尽管基于GPU的加速显著提升了ANNS性能，但不断增长的数据集规模要求高效的多GPU解决方案。现有工作在多GPU可扩展性设计上存在不足，简单地将额外的GPU视为扩展内存容量的手段，导致效率低下，因为它们对数据集进行分区并在每个GPU中独立搜索。", "method": "本文提出了PathWeaver，一个新颖的多GPU框架，旨在扩展和加速大规模数据集的ANNS。它包含三项主要技术：1) 基于流水线的路径扩展，一种GPU感知的流水线机制，利用GPU间通信减少冗余搜索迭代；2) 幽灵暂存，利用代表性数据集识别最佳查询起始点，从而减少复杂查询的搜索空间；3) 方向引导选择，一种数据选择技术，在搜索过程早期过滤掉不相关的点，从而最大程度地减少不必要的内存访问和距离计算。", "result": "PathWeaver在各种数据集上的综合评估表明，相对于最先进的多GPU ANNS框架，它实现了3.24倍的几何平均加速，并在95%召回率下实现了高达5.30倍的加速。", "conclusion": "PathWeaver通过其创新的多GPU框架和三项核心技术（基于流水线的路径扩展、幽灵暂存、方向引导选择），有效解决了大规模数据集下基于图的近似最近邻搜索的多GPU可扩展性问题，显著超越了现有最先进的解决方案。", "translation": "基于图的近似最近邻搜索（ANNS）在推荐系统、自然语言处理和计算机视觉等众多应用中广泛采用。尽管最近基于GPU的加速工作显著提升了ANNS性能，但不断增长的数据集规模现在要求高效的多GPU解决方案。然而，现有工作的设计忽视了多GPU的可扩展性，导致天真的方法将额外的GPU视为扩展大型数据集内存容量的手段。这种低效率源于对数据集进行分区并在每个GPU中独立搜索与查询相似的数据点。因此，我们提出了PathWeaver，一个新颖的多GPU框架，旨在扩展和加速大型数据集的ANNS。首先，我们提出了基于流水线的路径扩展，这是一种GPU感知的流水线机制，通过利用GPU到GPU的通信减少了先前工作的冗余搜索迭代。其次，我们设计了幽灵暂存，它利用代表性数据集来识别最佳查询起始点，从而减少了复杂查询的搜索空间。最后，我们引入了方向引导选择，这是一种数据选择技术，在搜索过程早期过滤掉不相关的点，从而最大限度地减少不必要的内存访问和距离计算。在各种数据集上的综合评估表明，PathWeaver相对于最先进的多GPU ANNS框架，实现了3.24倍的几何平均加速，并在95%召回率下实现了高达5.30倍的加速。", "summary": "本文提出了PathWeaver，一个针对大规模数据集的图基近似最近邻搜索（ANNS）的高吞吐量多GPU系统。针对现有多GPU ANNS方案在可扩展性上的不足，PathWeaver引入了三大创新技术：基于流水线的路径扩展，利用GPU间通信减少冗余搜索；幽灵暂存，通过代表性数据集优化查询起始点；以及方向引导选择，在搜索早期过滤不相关数据。实验结果显示，PathWeaver在95%召回率下，比现有最先进的多GPU ANNS框架实现了3.24倍的几何平均加速和高达5.30倍的加速。", "keywords": "近似最近邻搜索, 多GPU系统, 图基搜索, 高吞吐量, PathWeaver", "comments": "PathWeaver的创新之处在于其对多GPU ANNS系统设计的全面优化，解决了现有方案将多GPU简单视为内存扩展的低效问题。通过引入GPU间通信优化、查询起始点优化和早期数据过滤等机制，该系统显著提升了大规模数据集下ANNS的吞吐量和效率，为相关领域提供了重要的性能突破。"}}
{"id": "2507.16851", "title": "Coarse-to-fine crack cue for robust crack detection", "authors": ["Zelong Liu", "Yuliang Gu", "Zhichao Sun", "Huachao Zhu", "Xin Xiao", "Bo Du", "Laurent Najman", "Yongchao Xu"], "categories": ["cs.CV", "cs.NE", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16851v1", "summary": "Crack detection is an important task in computer vision. Despite impressive\nin-dataset performance, deep learning-based methods still struggle in\ngeneralizing to unseen domains. The thin structure property of cracks is\nusually overlooked by previous methods. In this work, we introduce CrackCue, a\nnovel method for robust crack detection based on coarse-to-fine crack cue\ngeneration. The core concept lies on leveraging the thin structure property to\ngenerate a robust crack cue, guiding the crack detection. Specifically, we\nfirst employ a simple max-pooling and upsampling operation on the crack image.\nThis results in a coarse crack-free background, based on which a fine\ncrack-free background can be obtained via a reconstruction network. The\ndifference between the original image and fine crack-free background provides a\nfine crack cue. This fine cue embeds robust crack prior information which is\nunaffected by complex backgrounds, shadow, and varied lighting. As a\nplug-and-play method, we incorporate the proposed CrackCue into three advanced\ncrack detection networks. Extensive experimental results demonstrate that the\nproposed CrackCue significantly improves the generalization ability and\nrobustness of the baseline methods. The source code will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16851v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "从粗到细的裂缝线索用于鲁棒的裂缝检测", "tldr": "本文提出了一种名为CrackCue的新方法，通过生成从粗到细的裂缝线索来提高深度学习模型在裂缝检测任务中的泛化能力和鲁棒性。", "motivation": "尽管深度学习方法在裂缝检测方面表现出色，但它们在泛化到未见领域时仍面临挑战，并且通常忽视了裂缝的细微结构特性。", "method": "本文引入了CrackCue，一种基于从粗到细裂缝线索生成的新型鲁棒裂缝检测方法。其核心思想是利用裂缝的细微结构特性来生成鲁棒的裂缝线索。具体来说，首先对裂缝图像进行简单的最大池化和上采样操作，得到粗略的无裂缝背景；然后通过重建网络获得精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异提供了精细的裂缝线索。这种线索嵌入了不受复杂背景、阴影和光照变化影响的鲁棒裂缝先验信息。作为一种即插即用的方法，CrackCue被集成到三种先进的裂缝检测网络中。", "result": "实验结果表明，所提出的CrackCue显著提高了基线方法的泛化能力和鲁棒性。", "conclusion": "CrackCue通过利用裂缝的细微结构特性生成鲁棒的裂缝线索，有效解决了深度学习方法在裂缝检测中泛化能力差的问题，显著提升了模型的鲁棒性。", "translation": "裂缝检测是计算机视觉中的一项重要任务。尽管在数据集内表现出色，但基于深度学习的方法在泛化到未见领域时仍然面临挑战。裂缝的细微结构特性通常被以往的方法所忽视。在这项工作中，我们引入了CrackCue，一种基于从粗到细裂缝线索生成的鲁棒裂缝检测新方法。其核心概念在于利用细微结构特性生成鲁棒的裂缝线索，以指导裂缝检测。具体来说，我们首先对裂缝图像进行简单的最大池化和上采样操作。这会产生一个粗略的无裂缝背景，在此基础上可以通过重建网络获得一个精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异提供了精细的裂缝线索。这种精细线索嵌入了不受复杂背景、阴影和光照变化影响的鲁棒裂缝先验信息。作为一种即插即用的方法，我们将所提出的CrackCue集成到三种先进的裂缝检测网络中。大量的实验结果表明，所提出的CrackCue显著提高了基线方法的泛化能力和鲁棒性。源代码将公开发布。", "summary": "本文提出了一种名为CrackCue的新型裂缝检测方法，旨在解决深度学习模型在裂缝检测中泛化能力差的问题。CrackCue利用裂缝的细微结构特性，通过从粗到细的策略生成鲁棒的裂缝线索。具体而言，它通过最大池化和重建网络生成无裂缝背景，并以此与原始图像的差异提取出不受复杂环境影响的裂缝先验信息。作为一种即插即用的模块，CrackCue能显著提升现有裂缝检测网络的泛化能力和鲁棒性。", "keywords": "裂缝检测, 深度学习, 泛化能力, 鲁棒性, 裂缝线索", "comments": "CrackCue的创新之处在于其利用裂缝的细微结构特性，通过生成从粗到细的裂缝线索来增强裂缝检测的鲁棒性和泛化能力。其“即插即用”的特性使其具有很高的实用价值和可扩展性，能够方便地集成到现有的深度学习框架中，有效解决实际应用中复杂背景、光照变化等问题。"}}
{"id": "2507.17079", "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey", "authors": ["Md Meftahul Ferdaus", "Kendall N. Niles", "Joe Tom", "Mahdi Abdelguerfi", "Elias Ioup"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review in ACM Computing Surveys", "url": "http://arxiv.org/abs/2507.17079v1", "summary": "Few-shot learning (FSL) enables object detection models to recognize novel\nclasses given only a few annotated examples, thereby reducing expensive manual\ndata labeling. This survey examines recent FSL advances for video and 3D object\ndetection. For video, FSL is especially valuable since annotating objects\nacross frames is more laborious than for static images. By propagating\ninformation across frames, techniques like tube proposals and temporal matching\nnetworks can detect new classes from a couple examples, efficiently leveraging\nspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces\nchallenges like sparsity and lack of texture. Solutions integrate FSL with\nspecialized point cloud networks and losses tailored for class imbalance.\nFew-shot 3D detection enables practical autonomous driving deployment by\nminimizing costly 3D annotation needs. Core issues in both domains include\nbalancing generalization and overfitting, integrating prototype matching, and\nhandling data modality properties. In summary, FSL shows promise for reducing\nannotation requirements and enabling real-world video, 3D, and other\napplications by efficiently leveraging information across feature, temporal,\nand data modalities. By comprehensively surveying recent advancements, this\npaper illuminates FSL's potential to minimize supervision needs and enable\ndeployment across video, 3D, and other real-world applications.", "comment": "Under review in ACM Computing Surveys", "pdf_url": "http://arxiv.org/pdf/2507.17079v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "视频和三维物体检测中的少样本学习：一项综述", "tldr": "本综述探讨了少样本学习在视频和三维物体检测中的最新进展，旨在通过少量标注数据实现新类别的识别，从而减少昂贵的数据标注工作。", "motivation": "少样本学习（FSL）能够让物体检测模型通过少量标注样本识别新类别，从而减少昂贵的人工数据标注需求。对于视频而言，跨帧标注物体比静态图像更费力，FSL显得尤为重要。对于三维检测，FSL通过最小化成本高昂的三维标注需求，有助于实际的自动驾驶部署。", "method": "本综述审视了视频和三维物体检测中少样本学习的最新进展。对于视频，它探讨了通过跨帧信息传播（如管状提议和时间匹配网络）来检测新类别。对于三维检测，它讨论了将FSL与专门的点云网络和针对类别不平衡的损失函数相结合的解决方案。", "result": "本综述全面地审视了少样本学习在视频和三维物体检测中的最新进展，揭示了其在减少标注需求和实现真实世界应用（如视频、三维及其他应用）方面的潜力。", "conclusion": "少样本学习在减少标注需求和通过有效利用特征、时间及数据模态信息方面显示出巨大潜力，能够赋能真实世界的视频、三维及其他应用。核心问题包括平衡泛化与过拟合、整合原型匹配以及处理数据模态特性。", "translation": "少样本学习（FSL）使物体检测模型能够仅凭少量标注示例识别新类别，从而减少昂贵的人工数据标注。本综述审视了视频和三维物体检测中少样本学习的最新进展。对于视频，FSL尤其有价值，因为跨帧标注物体比静态图像更费力。通过跨帧传播信息，管状提议和时间匹配网络等技术可以从少数几个示例中检测新类别，有效利用时空结构。用于LiDAR或深度数据三维检测的FSL面临稀疏性和缺乏纹理等挑战。解决方案将FSL与专门的点云网络和针对类别不平衡量身定制的损失函数相结合。少样本三维检测通过最小化成本高昂的三维标注需求，实现了实际的自动驾驶部署。这两个领域的核心问题包括平衡泛化与过拟合、整合原型匹配以及处理数据模态特性。总而言之，FSL通过有效利用特征、时间及数据模态信息，在减少标注需求和赋能真实世界视频、三维及其他应用方面显示出巨大潜力。通过全面综述最新进展，本文阐明了FSL在最小化监督需求和赋能视频、三维及其他真实世界应用部署方面的潜力。", "summary": "本综述全面探讨了少样本学习（FSL）在视频和三维物体检测领域的最新进展。FSL通过少量标注样本实现新类别识别，显著降低了数据标注成本，尤其对于视频和3D数据这种标注复杂且昂贵的场景。文章分析了视频FSL中利用时空结构的方法（如管状提议和时间匹配网络），以及3D FSL中应对稀疏性和纹理缺乏的解决方案。核心挑战包括泛化与过拟合的平衡、原型匹配的整合以及多模态数据处理。总结指出，FSL在减少标注需求和推动真实世界应用部署方面具有巨大潜力。", "keywords": "少样本学习, 视频物体检测, 三维物体检测, 综述, 数据标注", "comments": "这是一篇有价值的综述性论文，它系统地梳理了少样本学习在视频和三维物体检测这两个重要且具有挑战性领域中的最新进展。其创新点在于将FSL的应用扩展到高成本标注的视频和3D数据，并指出了这些特定模态下的挑战和解决方案。对于研究人员和工程师而言，这篇综述提供了一个全面的视角，有助于理解当前的技术现状和未来的研究方向。其重要性体现在它强调了FSL在降低实际应用（如自动驾驶）部署成本方面的潜力。"}}
{"id": "2412.07493", "title": "Onto-LLM-TAMP: Knowledge-oriented Task and Motion Planning using Large Language Models", "authors": ["Muhayy Ud Din", "Jan Rosell", "Waseem Akram", "Isiah Zaplana", "Maximo A Roa", "Irfan Hussain"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to knowledge based systems", "url": "http://arxiv.org/abs/2412.07493v2", "summary": "Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans.", "comment": "Submitted to knowledge based systems", "pdf_url": "http://arxiv.org/pdf/2412.07493v2", "cate": "cs.RO", "date": "2024-12-10", "updated": "2025-07-23", "AI": {"title_translation": "Onto-LLM-TAMP：面向知识的大语言模型任务与运动规划", "tldr": "Onto-LLM-TAMP通过知识驱动的提示增强，解决了现有大语言模型任务与运动规划在动态环境适应性差和语义错误问题，提高了规划的准确性和鲁棒性。", "motivation": "现有基于大型语言模型（LLM）的任务与运动规划（TAMP）方法受限于静态和基于模板的提示，导致其在动态环境和复杂任务上下文中的适应性有限，且易产生语义错误。", "method": "本文提出了一种新颖的Onto-LLM-TAMP框架。该框架采用基于知识的推理，通过任务上下文推理和基于知识的环境状态描述来细化和扩展用户提示。通过将领域特定知识整合到提示中，确保生成语义准确和上下文感知的任务计划。", "result": "所提出的框架有效解决了符号计划生成中的语义错误，例如在分层对象放置场景中保持逻辑时间目标顺序。通过仿真和真实世界场景验证，该框架在对动态环境的适应性和生成语义正确的任务计划方面，比基线方法有显著改进。", "conclusion": "Onto-LLM-TAMP框架通过集成知识驱动的推理，有效克服了当前基于LLM的TAMP方法的局限性，在动态环境中实现了更高的适应性和语义正确的任务计划。", "translation": "在动态环境中执行复杂的操控任务需要高效的任务与运动规划（TAMP）方法，该方法将高级符号计划与低级运动控制相结合。GPT-4 等大型语言模型（LLM）的进步正在通过提供自然语言作为描述任务、生成符号计划和推理的直观灵活方式来改变任务规划。然而，基于 LLM 的 TAMP 方法的有效性受到静态和基于模板的提示的限制，这限制了其对动态环境和复杂任务上下文的适应性。为了解决这些限制，本工作提出了一种新颖的 Onto-LLM-TAMP 框架，该框架采用基于知识的推理来通过任务上下文推理和基于知识的环境状态描述来细化和扩展用户提示。将领域特定知识整合到提示中可确保语义准确和上下文感知的任务计划。所提出的框架通过解决符号计划生成中的语义错误（例如在涉及分层对象放置的场景中保持逻辑时间目标顺序）展示了其有效性。所提出的框架通过仿真和真实世界场景进行了验证，在对动态环境的适应性和生成语义正确的任务计划方面，与基线方法相比显示出显著改进。", "summary": "本文提出Onto-LLM-TAMP框架，旨在克服现有基于大型语言模型（LLM）的任务与运动规划（TAMP）方法因静态提示而导致的适应性差和语义错误问题。该框架通过整合知识驱动的推理，细化并扩展用户提示，使其包含任务上下文和环境状态描述，从而确保生成语义准确且上下文感知的任务计划。实验结果表明，该框架能有效解决符号计划生成中的语义错误，并在仿真和真实世界场景中，相较于基线方法，显著提升了对动态环境的适应性以及生成语义正确任务计划的能力。", "keywords": "任务与运动规划, 大型语言模型, 知识推理, 提示工程, 机器人学", "comments": "该论文的创新之处在于将基于知识的推理和领域特定知识融入到大型语言模型（LLM）的提示工程中，以改进任务与运动规划（TAMP）。这种方法超越了传统的静态模板提示，显著增强了LLM生成计划的适应性和语义正确性，这对于在动态环境中进行机器人操作至关重要。该工作成功解决了LLM在机器人领域应用中的一个关键局限性。"}}
{"id": "2507.15489", "title": "Constrained Control Allocation With Continuous-Time Rate Constraints: Three-Dimensional Case", "authors": ["Süleyman Özkurt", "Adrian Grimm", "Walter Fichter"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Will be submitted as Engineering Note to Journal of Guidance, Control, and Dynamics", "url": "http://arxiv.org/abs/2507.15489v2", "summary": "This paper presents a novel quadratic programming (QP) approach for\nconstrained control allocation that directly incorporates continuous-time\nactuator rate constraints without requiring slack variables. Over-actuated\naircraft configurations, particularly prevalent in eVTOL and military\napplications, require control allocation algorithms to distribute commanded\ncontrol moments among available actuators while respecting position and rate\nconstraints. Existing methods such as direct allocation, pseudo-inverse,\ncascaded generalized inverse, and exact redistributed pseudo-inverse either\ncannot handle rate constraints in continuous time or require discretization\napproaches that compromise performance. Current QP methods that incorporate\nrate constraints rely on slack variables to ensure feasibility, which prevents\nfull utilization of the attainable moment set and degrades allocation\nperformance. The proposed methodology addresses this limitation by calculating\nthe attainable moment set from both position and rate constraints through\nconvex hull operations, then ensuring feasibility by scaling unattainable\ncommanded moments to the boundary of the attainable moment set while preserving\ntheir direction. This approach guarantees the feasibility of the optimization\nproblem without slack variables. The method is validated through simulation on\nan F-18 fighter aircraft control allocation problem, demonstrating equivalent\nperformance to the established exact redistributed pseudo-inverse method while\nproviding smoother actuator behavior and enhanced constraint satisfaction.\nResults show that incorporating continuous-time rate constraints leads to\nimproved actuator tracking, reduced overshoot, and more precise adherence to\nposition limits, which is essential for aircraft safety, ride comfort, and\nactuator longevity.", "comment": "Will be submitted as Engineering Note to Journal of Guidance,\n  Control, and Dynamics", "pdf_url": "http://arxiv.org/pdf/2507.15489v2", "cate": "math.OC", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "具有连续时间速率约束的约束控制分配：三维情况", "tldr": "提出一种新的QP方法，直接处理连续时间执行器速率约束，无需松弛变量，提升过驱动飞行器控制分配性能。", "motivation": "现有控制分配方法（如直接分配、伪逆、级联广义逆、精确重分配伪逆）无法在连续时间处理速率约束或需离散化，影响性能。当前纳入速率约束的QP方法依赖松弛变量，限制可达力矩集的充分利用并降低分配性能。", "method": "提出一种新颖的二次规划（QP）方法，直接整合连续时间执行器速率约束，无需松弛变量。通过凸包运算计算位置和速率约束下的可达力矩集，然后通过将不可达的指令力矩按方向缩放到可达力矩集的边界来确保可行性。", "result": "在F-18战斗机控制分配问题上的仿真验证表明，该方法性能与现有精确重分配伪逆方法相当，同时提供了更平滑的执行器行为和增强的约束满足能力。结果显示，整合连续时间速率约束可改善执行器跟踪、减少过冲、更精确地遵守位置限制。", "conclusion": "该方法通过直接整合连续时间速率约束且无需松弛变量，解决了现有QP方法的局限性，提升了控制分配的性能、安全性和执行器寿命。", "translation": "这篇论文提出了一种新颖的二次规划（QP）方法，用于约束控制分配，该方法直接纳入连续时间执行器速率约束，无需松弛变量。过驱动的飞行器配置，特别是在eVTOL和军事应用中普遍存在，需要控制分配算法在尊重位置和速率约束的同时，将指令控制力矩分配给可用的执行器。现有的方法，如直接分配、伪逆、级联广义逆和精确重分配伪逆，要么无法在连续时间处理速率约束，要么需要离散化方法，从而影响性能。当前纳入速率约束的QP方法依赖于松弛变量来确保可行性，这阻碍了可达力矩集的充分利用并降低了分配性能。所提出的方法通过凸包运算从位置和速率约束中计算可达力矩集，然后通过将不可达的指令力矩按方向缩放到可达力矩集的边界来确保可行性，从而解决了这一限制。这种方法保证了优化问题的可行性，而无需松弛变量。该方法通过在F-18战斗机控制分配问题上的仿真得到验证，结果表明其性能与已建立的精确重分配伪逆方法相当，同时提供了更平滑的执行器行为和增强的约束满足能力。结果显示，纳入连续时间速率约束可以改善执行器跟踪、减少过冲，以及更精确地遵守位置限制，这对于飞行器安全、乘坐舒适性和执行器寿命至关重要。", "summary": "本文提出一种新颖的二次规划（QP）方法，用于过驱动飞行器的约束控制分配。该方法创新性地直接整合连续时间执行器速率约束，无需松弛变量，解决了现有方法在处理速率约束或保证可行性方面的不足。通过计算可达力矩集并对不可达力矩进行缩放，确保了优化问题的可行性。仿真结果表明，该方法在保持性能的同时，能提供更平滑的执行器行为和更强的约束满足能力，对飞行器安全和执行器寿命有益。", "keywords": "控制分配, 速率约束, 二次规划, 凸包, 过驱动飞行器", "comments": "这篇论文的创新点在于提出了一个无需松弛变量的QP方法来直接处理连续时间速率约束，这解决了现有QP方法在可达力矩集利用率和分配性能上的局限。通过凸包运算和力矩缩放确保可行性的策略是其关键贡献，对于需要高精度和实时性的过驱动系统控制分配具有重要意义，尤其是在eVTOL和军事应用中。"}}
{"id": "2501.12296", "title": "RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning", "authors": ["Jiacheng Zuo", "Haibo Hu", "Zikang Zhou", "Yufei Cui", "Ziquan Liu", "Jianping Wang", "Nan Guan", "Jin Wang", "Chun Jason Xue"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.12296v3", "summary": "In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.12296v3", "cate": "cs.CV", "date": "2025-01-21", "updated": "2025-07-23", "AI": {"title_translation": "RALAD：通过检索增强学习弥合自动驾驶中的真实到模拟领域差距", "tldr": "RALAD通过低成本的检索增强学习，有效弥合了自动驾驶中真实世界与模拟环境的领域差距，显著提升了模型在模拟环境中的性能并降低了再训练成本。", "motivation": "在追求鲁棒的自动驾驶系统时，在真实世界数据集上训练的模型通常难以适应新环境，尤其是在面对极端天气条件等边缘情况时。在真实世界中收集这些边缘情况并非易事，这使得使用模拟器进行验证成为必要。然而，高昂的计算成本和数据分布的领域差距阻碍了真实和模拟驾驶场景之间的无缝过渡。", "method": "本文提出了RALAD（Retrieval-Augmented Learning for Autonomous Driving），一个新颖的低成本框架，旨在弥合真实到模拟的领域差距。RALAD具有三个主要设计：1) 通过增强型最优传输（OT）方法进行领域自适应，该方法考虑了单个和分组图像距离；2) 一个简单统一的框架，可应用于各种模型；3) 高效的微调技术，冻结计算成本高昂的层，同时保持鲁棒性。", "result": "实验结果表明，RALAD在三种不同模型上弥补了模拟环境中的性能下降，同时保持了真实世界场景的准确性。以Cross View为例，真实世界场景中的mIOU和mAP指标在RALAD微调前后保持稳定，而在模拟环境中，mIOU和mAP分别提高了10.30%和12.29%。此外，该方法的再训练成本降低了约88.1%。", "conclusion": "RALAD成功地以低成本弥合了自动驾驶中的真实到模拟领域差距，显著提高了模型在模拟环境中的性能，同时保持了真实世界性能，并大幅降低了再训练成本。", "translation": "在追求鲁棒的自动驾驶系统时，在真实世界数据集上训练的模型通常难以适应新环境，尤其是在面对极端天气条件等边缘情况时。在真实世界中收集这些边缘情况并非易事，这使得使用模拟器进行验证成为必要。然而，高昂的计算成本和数据分布的领域差距阻碍了真实和模拟驾驶场景之间的无缝过渡。为了解决这一挑战，我们提出了RALAD（Retrieval-Augmented Learning for Autonomous Driving），一个旨在以低成本弥合真实到模拟差距的新颖框架。RALAD具有三个主要设计，包括（1）通过增强型最优传输（OT）方法进行领域自适应，该方法考虑了单个和分组图像距离，（2）一个简单统一的框架，可应用于各种模型，以及（3）高效的微调技术，冻结计算成本高昂的层，同时保持鲁棒性。实验结果表明，RALAD在三种不同模型上弥补了模拟环境中的性能下降，同时保持了真实世界场景的准确性。以Cross View为例，真实世界场景中的mIOU和mAP指标在RALAD微调前后保持稳定，而在模拟环境中，mIOU和mAP指标分别提高了10.30%和12.29%。此外，我们的方法的再训练成本降低了约88.1%。我们的代码可在https://github.com/JiachengZuo/RALAD.git获取。", "summary": "本文提出了RALAD，一个用于自动驾驶的新型检索增强学习框架，旨在低成本地弥合真实世界与模拟环境之间的领域差距。该框架通过增强型最优传输进行领域自适应、提供统一的应用框架以及高效微调技术，解决了现有模型在模拟环境中性能下降及高昂再训练成本的问题。实验证明，RALAD显著提升了模型在模拟环境下的性能（mIOU和mAP分别提高10.30%和12.29%），同时保持了真实世界性能，并大幅降低了再训练成本约88.1%。", "keywords": "自动驾驶, 领域差距, 检索增强学习, 模拟器, 领域自适应", "comments": "RALAD的创新之处在于其结合了检索增强学习和优化的领域自适应方法，以低成本高效地解决了自动驾驶领域中真实到模拟的领域差距问题。其提出的高效微调策略，通过冻结高计算成本层，显著降低了再训练成本，这对于实际应用具有重要意义。该框架的通用性也使其能够应用于多种模型，增加了其实用价值。"}}
{"id": "2502.01670", "title": "Hardware-Efficient Photonic Tensor Core: Accelerating Deep Neural Networks with Structured Compression", "authors": ["Shupeng Ning", "Hanqing Zhu", "Chenghao Feng", "Jiaqi Gu", "David Z. Pan", "Ray T. Chen"], "categories": ["cs.AR", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.01670v2", "summary": "The rapid growth in computing demands, particularly driven by artificial\nintelligence applications, has begun to exceed the capabilities of traditional\nelectronic hardware. Optical computing offers a promising alternative due to\nits parallelism, high computational speed, and low power consumption. However,\nexisting photonic integrated circuits are constrained by large footprints,\ncostly electro-optical interfaces, and complex control mechanisms, limiting the\npractical scalability of optical neural networks (ONNs). To address these\nlimitations, we introduce a block-circulant photonic tensor core for a\nstructure-compressed optical neural network (StrC-ONN) architecture. The\nstructured compression technique substantially reduces both model complexity\nand hardware resources without sacrificing the versatility of neural networks,\nand achieves accuracy comparable to uncompressed models. Additionally, we\npropose a hardware-aware training framework to compensate for on-chip\nnonidealities to improve model robustness and accuracy. Experimental validation\nthrough image processing and classification tasks demonstrates that our\nStrC-ONN achieves a reduction in trainable parameters of up to 74.91%,while\nstill maintaining competitive accuracy levels. Performance analyses further\nindicate that this hardware-software co-design approach is expected to yield a\n3.56 times improvement in power efficiency. By reducing both hardware\nrequirements and control complexity across multiple dimensions, this work\nexplores a new pathway toward practical and scalable ONNs, highlighting a\npromising route to address future computational efficiency challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.01670v2", "cate": "cs.AR", "date": "2025-02-01", "updated": "2025-07-23", "AI": {"title_translation": "硬件高效光子张量核：利用结构化压缩加速深度神经网络", "tldr": "该研究提出了一种硬件高效的光子张量核，用于结构化压缩光神经网络（StrC-ONN），通过结构化压缩和硬件感知训练显著减少硬件资源和功耗，同时保持高精度，为可扩展光学计算提供了新途径。", "motivation": "传统电子硬件已无法满足人工智能应用快速增长的计算需求。光学计算虽具潜力，但现有光子集成电路受限于尺寸大、接口昂贵、控制复杂，阻碍了光神经网络（ONN）的实际可扩展性。", "method": "引入了一种块循环光子张量核，用于结构化压缩光神经网络（StrC-ONN）架构。采用结构化压缩技术，在不牺牲神经网络通用性的前提下，大幅减少模型复杂度和硬件资源。同时，提出了一种硬件感知训练框架，以补偿芯片上的非理想性，提高模型鲁棒性和精度。", "result": "通过图像处理和分类任务的实验验证表明，StrC-ONN可将可训练参数减少高达74.91%，同时保持具有竞争力的精度。性能分析进一步表明，这种软硬件协同设计方法预计可将能效提高3.56倍。", "conclusion": "通过多维度降低硬件需求和控制复杂性，这项工作探索了一条实现实用且可扩展ONN的新途径，为解决未来的计算效率挑战指明了有前景的方向。", "translation": "计算需求的快速增长，特别是受人工智能应用的驱动，已经开始超越传统电子硬件的能力。光学计算因其并行性、高计算速度和低功耗而提供了一个有前景的替代方案。然而，现有的光子集成电路受限于大尺寸、昂贵的电光接口和复杂的控制机制，限制了光神经网络（ONN）的实际可扩展性。为了解决这些限制，我们引入了一种块循环光子张量核，用于结构化压缩光神经网络（StrC-ONN）架构。结构化压缩技术在不牺牲神经网络通用性的前提下，大幅减少了模型复杂度和硬件资源，并实现了与未压缩模型相当的精度。此外，我们提出了一种硬件感知训练框架，以补偿芯片上的非理想性，从而提高模型的鲁棒性和精度。通过图像处理和分类任务的实验验证表明，我们的StrC-ONN在保持具有竞争力的精度水平的同时，将可训练参数减少了高达74.91%。性能分析进一步表明，这种软硬件协同设计方法有望将能效提高3.56倍。通过多维度降低硬件需求和控制复杂性，这项工作探索了一条实现实用且可扩展ONN的新途径，为解决未来的计算效率挑战指明了有前景的方向。", "summary": "本论文提出了一种硬件高效的光子张量核，用于构建结构化压缩光神经网络（StrC-ONN），以应对传统电子硬件在AI计算需求下遇到的瓶颈以及现有光子集成电路的局限性。通过引入块循环光子张量核和结构化压缩技术，该方法显著减少了模型复杂性和硬件资源，同时保持了与未压缩模型相当的精度。此外，还提出了硬件感知训练框架以提高模型鲁棒性。实验结果显示，StrC-ONN可减少高达74.91%的可训练参数，并实现3.56倍的能效提升，为实现实用和可扩展的光神经网络开辟了新途径。", "keywords": "光子张量核, 结构化压缩, 光神经网络, 硬件效率, 深度学习", "comments": "这项工作在光学计算领域具有重要创新性。它通过引入“块循环光子张量核”和“结构化压缩”技术，巧妙地解决了现有光子集成电路在尺寸、成本和控制复杂性方面的瓶颈。结合“硬件感知训练框架”，进一步提升了模型的鲁棒性和实用性。其在参数减少和能效提升方面的显著成果，为未来构建高效、可扩展的光学神经网络提供了有前景的解决方案，对于推动人工智能硬件发展具有重要意义。"}}
{"id": "2507.17453", "title": "Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees", "authors": ["Guanqin Zhang", "Kota Fukuda", "Zhenya Zhang", "H. M. N. Dilum Bandara", "Shiping Chen", "Jianjun Zhao", "Yulei Sui"], "categories": ["cs.LG", "cs.PL", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is an extended version of the ECOOP 2025 paper, with a comparison with DATE 2025 (Figure 7 of RQ1 in Section 5.2), as well as an in-depth discussion of OOPSLA 2025 in the related work (Section 6)", "url": "http://arxiv.org/abs/2507.17453v1", "summary": "The vulnerability of neural networks to adversarial perturbations has\nnecessitated formal verification techniques that can rigorously certify the\nquality of neural networks. As the state-of-the-art, branch and bound (BaB) is\na \"divide-and-conquer\" strategy that applies off-the-shelf verifiers to\nsub-problems for which they perform better. While BaB can identify the\nsub-problems that are necessary to be split, it explores the space of these\nsub-problems in a naive \"first-come-first-serve\" manner, thereby suffering from\nan issue of inefficiency to reach a verification conclusion. To bridge this\ngap, we introduce an order over different sub-problems produced by BaB,\nconcerning with their different likelihoods of containing counterexamples.\nBased on this order, we propose a novel verification framework Oliva that\nexplores the sub-problem space by prioritizing those sub-problems that are more\nlikely to find counterexamples, in order to efficiently reach the conclusion of\nthe verification. Even if no counterexample can be found in any sub-problem, it\nonly changes the order of visiting different sub-problem and so will not lead\nto a performance degradation. Specifically, Oliva has two variants, including\n$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that\nare more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy\ninspired by simulated annealing that gradually shifts from exploration to\nexploitation to locate the globally optimal sub-problems. We experimentally\nevaluate the performance of Oliva on 690 verification problems spanning over 5\nmodels with datasets MNIST and CIFAR10. Compared to the state-of-the-art\napproaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up\nto 80X in CIFAR10.", "comment": "This is an extended version of the ECOOP 2025 paper, with a\n  comparison with DATE 2025 (Figure 7 of RQ1 in Section 5.2), as well as an\n  in-depth discussion of OOPSLA 2025 in the related work (Section 6)", "pdf_url": "http://arxiv.org/pdf/2507.17453v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过有序引导分支界定树探索实现高效神经网络验证", "tldr": "引入了一种名为Oliva的新型验证框架，通过优先探索更有可能找到反例的子问题，显著提高了神经网络验证的效率。", "motivation": "神经网络易受对抗性扰动攻击，需要形式化验证技术来严格认证其质量。现有的分支界定（BaB）方法在探索子问题空间时效率低下，采用“先来先服务”的朴素方式，导致验证结论效率低。", "method": "提出了一种名为Oliva的新型验证框架。该框架基于对BaB产生的不同子问题进行排序，考虑它们包含反例的不同可能性。Oliva通过优先处理那些更有可能找到反例的子问题来探索子问题空间。Oliva有两个变体：$Oliva^{GR}$ (贪婪策略，始终优先处理更有可能找到反例的子问题) 和 $Oliva^{SA}$ (受模拟退火启发，逐渐从探索转向利用以定位全局最优子问题的平衡策略)。", "result": "在MNIST数据集上验证速度提高了25倍，在CIFAR10数据集上验证速度提高了80倍，性能优于现有最先进的方法。", "conclusion": "Oliva通过引入对分支界定树中子问题的有序探索，显著提高了神经网络验证的效率，即使没有找到反例也不会导致性能下降。", "translation": "神经网络对对抗性扰动的脆弱性使得形式化验证技术变得必要，这些技术可以严格认证神经网络的质量。作为最先进的方法，分支界定（BaB）是一种“分而治之”的策略，它将现成的验证器应用于它们表现更好的子问题。虽然BaB可以识别需要分割的子问题，但它以朴素的“先来先服务”方式探索这些子问题的空间，因此存在达到验证结论效率低下的问题。为了弥补这一差距，我们引入了BaB产生的不同子问题之间的顺序，这涉及到它们包含反例的不同可能性。基于这个顺序，我们提出了一种新颖的验证框架Oliva，该框架通过优先处理那些更有可能找到反例的子问题来探索子问题空间，以高效地达到验证结论。即使在任何子问题中都找不到反例，它也只是改变访问不同子问题的顺序，因此不会导致性能下降。具体而言，Oliva有两个变体，包括$Oliva^{GR}$，一种始终优先处理更有可能找到反例的子问题的贪婪策略，以及$Oliva^{SA}$，一种受模拟退火启发，逐渐从探索转向利用以定位全局最优子问题的平衡策略。我们在690个验证问题上实验评估了Oliva的性能，这些问题涵盖了使用MNIST和CIFAR10数据集的5个模型。与最先进的方法相比，我们证明了Oliva在MNIST中速度提高了25倍，在CIFAR10中速度提高了80倍。", "summary": "本文提出了一种名为Oliva的新型神经网络验证框架，旨在解决现有分支界定（BaB）方法在探索子问题时效率低下的问题。Oliva通过根据子问题包含反例的可能性对其进行排序，并优先探索那些更有可能找到反例的子问题，从而显著加速验证过程。该框架包含两种策略：贪婪策略$Oliva^{GR}$和平衡策略$Oliva^{SA}$。实验结果表明，Oliva在MNIST和CIFAR10数据集上的验证速度分别比现有技术快25倍和80倍。", "keywords": "神经网络验证, 分支界定, 对抗性扰动, Oliva, 效率", "comments": "该论文的创新点在于引入了对分支界定树中子问题进行有序探索的策略，解决了传统BaB方法“先来先服务”的低效问题。通过预测反例存在的可能性来优化探索顺序，显著提高了神经网络验证的效率。其重要性体现在为神经网络的安全性认证提供了一种更快速、更实用的工具，对于推动对抗鲁棒性研究具有积极意义。"}}
{"id": "2507.14184", "title": "NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment", "authors": ["ZhengXiao He", "Jinghao Wen", "Huayu Li", "Siyuan Tian", "Ao Li"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14184v3", "summary": "We present a novel and interpretable framework for electrocardiogram\n(ECG)-based disease detection that combines hyperdimensional computing (HDC)\nwith learnable neural encoding. Unlike conventional HDC approaches that rely on\nstatic, random projections, our method introduces a rhythm-aware and trainable\nencoding pipeline based on RR intervals, a physiological signal segmentation\nstrategy that aligns with cardiac cycles. The core of our design is a\nneural-distilled HDC architecture, featuring a learnable RR-block encoder and a\nBinaryLinear hyperdimensional projection layer, optimized jointly with\ncross-entropy and proxy-based metric loss. This hybrid framework preserves the\nsymbolic interpretability of HDC while enabling task-adaptive representation\nlearning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model\nsignificantly outperforms traditional HDC and classical ML baselines, achieving\n73.09\\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable\nrobustness on PTB-XL. Our framework offers an efficient and scalable solution\nfor edge-compatible ECG classification, with strong potential for interpretable\nand personalized health monitoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14184v3", "cate": "eess.SP", "date": "2025-07-12", "updated": "2025-07-23", "AI": {"title_translation": "NeuroHD-RA：神经蒸馏超维度模型与节律对齐", "tldr": "NeuroHD-RA是一个结合了超维度计算（HDC）和可学习神经编码的新型可解释框架，用于基于心电图（ECG）的疾病检测，通过节律感知和可训练的RR间隔编码，显著优于传统HDC和经典机器学习基线。", "motivation": "传统的超维度计算（HDC）方法依赖于静态、随机的投影，限制了其在心电图（ECG）疾病检测中的性能和适应性。本文旨在开发一个既能保留HDC的可解释性，又能实现任务自适应表示学习的新框架。", "method": "本文提出了NeuroHD-RA框架，该框架结合了超维度计算（HDC）和可学习的神经编码。其核心是一个神经蒸馏的HDC架构，包含一个可学习的RR块编码器和一个二值线性超维度投影层。该方法引入了基于RR间隔的节律感知和可训练的编码流程，与心动周期对齐。模型通过交叉熵和基于代理的度量损失联合优化。", "result": "在Apnea-ECG和PTB-XL数据集上的实验表明，NeuroHD-RA模型显著优于传统HDC和经典机器学习基线。在Apnea-ECG数据集上，模型实现了73.09%的准确率和0.626的F1分数，并在PTB-XL数据集上表现出相当的鲁棒性。", "conclusion": "NeuroHD-RA框架为边缘兼容的心电图分类提供了一个高效且可扩展的解决方案，在可解释和个性化健康监测方面具有巨大潜力。", "translation": "我们提出了一种新颖且可解释的基于心电图（ECG）疾病检测框架，该框架结合了超维度计算（HDC）与可学习的神经编码。与依赖静态、随机投影的传统HDC方法不同，我们的方法引入了一种节律感知且可训练的编码流程，该流程基于RR间隔——一种与心动周期对齐的生理信号分割策略。我们设计的核心是一个神经蒸馏的HDC架构，其特点是包含一个可学习的RR块编码器和一个二值线性超维度投影层，并通过交叉熵和基于代理的度量损失进行联合优化。这种混合框架保留了HDC的符号可解释性，同时实现了任务自适应的表示学习。在Apnea-ECG和PTB-XL数据集上的实验表明，我们的模型显著优于传统HDC和经典机器学习基线，在Apnea-ECG上实现了73.09%的精度和0.626的F1分数，并在PTB-XL上表现出相当的鲁棒性。我们的框架为边缘兼容的心电图分类提供了一个高效且可扩展的解决方案，在可解释和个性化健康监测方面具有巨大潜力。", "summary": "NeuroHD-RA是一种新颖的可解释框架，用于基于心电图（ECG）的疾病检测。它将超维度计算（HDC）与可学习的神经编码相结合，引入了基于RR间隔的节律感知和可训练编码流程，以实现与心动周期对齐。该框架的核心是神经蒸馏的HDC架构，通过联合优化实现任务自适应表示学习，同时保留HDC的可解释性。实验证明，NeuroHD-RA在Apnea-ECG和PTB-XL数据集上显著优于传统HDC和经典机器学习基线，为边缘兼容的心电图分类提供了高效、可扩展且可解释的解决方案。", "keywords": "超维度计算, 心电图, 神经编码, 疾病检测, 节律对齐", "comments": "该论文的创新之处在于将可学习的神经编码引入超维度计算（HDC），特别是通过引入节律感知（基于RR间隔）的编码方式，克服了传统HDC依赖静态随机投影的局限性。这种混合框架成功地结合了深度学习的表示学习能力和HDC固有的可解释性，为ECG分析提供了一个高效且适用于边缘设备的解决方案。其在可解释性、个性化健康监测方面的潜力值得关注。"}}
{"id": "2507.17303", "title": "A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model", "authors": ["Zhe Xu", "Ziyi Liu", "Junlin Hou", "Jiabo Ma", "Cheng Jin", "Yihui Wang", "Zhixuan Chen", "Zhengyu Zhang", "Zhengrui Guo", "Fengtao Zhou", "Yingxue Xu", "Xi Wang", "Ronald Cheong Kin Chan", "Li Liang", "Hao Chen"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17303v1", "summary": "Multimodal large language models (MLLMs) have emerged as powerful tools for\ncomputational pathology, offering unprecedented opportunities to integrate\npathological images with language context for comprehensive diagnostic\nanalysis. These models hold particular promise for automating complex tasks\nthat traditionally require expert interpretation of pathologists. However,\ncurrent MLLM approaches in pathology demonstrate significantly constrained\nreasoning capabilities, primarily due to their reliance on expensive\nchain-of-thought annotations. Additionally, existing methods remain limited to\nsimplex application of visual question answering (VQA) at region-of-interest\n(ROI) level, failing to address the full spectrum of diagnostic needs such as\nROI classification, detection, segmentation, whole-slide-image (WSI)\nclassification and VQA in clinical practice. In this study, we present\nSmartPath-R1, a versatile MLLM capable of simultaneously addressing both\nROI-level and WSI-level tasks while demonstrating robust pathological reasoning\ncapability. Our framework combines scale-dependent supervised fine-tuning and\ntask-aware reinforcement fine-tuning, which circumvents the requirement for\nchain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.\nFurthermore, SmartPath-R1 integrates multiscale and multitask analysis through\na mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.\nWe curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI\nsamples for training and evaluation. Extensive experiments across 72 tasks\nvalidate the effectiveness and superiority of the proposed approach. This work\nrepresents a significant step toward developing versatile, reasoning-enhanced\nAI systems for precision pathology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17303v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种通过推理增强多模态大语言模型实现的通用病理学副驾驶", "tldr": "提出SmartPath-R1，一个通用的多模态大语言模型，通过结合尺度依赖监督微调和任务感知强化微调，解决了现有病理学MLLM推理能力受限和任务范围狭窄的问题，并在ROI和WSI级别任务上表现出色。", "motivation": "现有的病理学多模态大语言模型（MLLMs）推理能力受限，主要因依赖昂贵的思维链标注。此外，现有方法仅限于感兴趣区域（ROI）级别的视觉问答（VQA）的单一应用，未能解决临床实践中如ROI分类、检测、分割、全玻片图像（WSI）分类及VQA等全面的诊断需求。", "method": "本研究提出了SmartPath-R1，一种通用的多模态大语言模型。该框架结合了尺度依赖监督微调和任务感知强化微调，通过利用MLLM的内在知识避免了对思维链监督的需求。此外，SmartPath-R1通过专家混合机制整合了多尺度和多任务分析，实现了对不同任务的动态处理。研究还策劃了一个包含230万ROI样本和18.8万WSI样本的大规模数据集用于训练和评估。", "result": "跨72项任务的广泛实验验证了所提方法的有效性和优越性。SmartPath-R1能够同时处理ROI级别和WSI级别任务，并展现出强大的病理学推理能力。", "conclusion": "这项工作代表着在开发通用、推理增强型人工智能系统以实现精准病理学方面迈出了重要一步。", "translation": "多模态大语言模型（MLLMs）已成为计算病理学的强大工具，为整合病理图像与语言上下文进行全面诊断分析提供了前所未有的机会。这些模型在自动化传统上需要病理学家专业解读的复杂任务方面具有特殊前景。然而，当前病理学中的MLLM方法表现出显著受限的推理能力，这主要归因于它们对昂贵的思维链标注的依赖。此外，现有方法仍局限于感兴趣区域（ROI）级别的视觉问答（VQA）的单一应用，未能解决临床实践中如ROI分类、检测、分割、全玻片图像（WSI）分类和VQA等全面的诊断需求。在本研究中，我们提出了SmartPath-R1，一个通用的MLLM，能够同时处理ROI级别和WSI级别任务，并展示出强大的病理学推理能力。我们的框架结合了尺度依赖监督微调和任务感知强化微调，通过利用MLLM的内在知识，规避了对思维链监督的要求。此外，SmartPath-R1通过专家混合机制整合了多尺度和多任务分析，从而实现了对不同任务的动态处理。我们策劃了一个包含230万ROI样本和18.8万WSI样本的大规模数据集用于训练和评估。跨72项任务的广泛实验验证了所提方法的有效性和优越性。这项工作代表着在开发通用、推理增强型人工智能系统以实现精准病理学方面迈出了重要一步。", "summary": "本文介绍了SmartPath-R1，一个通用的多模态大语言模型，旨在克服现有病理学MLLM在推理能力和任务范围上的局限性。SmartPath-R1通过结合尺度依赖监督微调和任务感知强化微调，以及利用专家混合机制进行多尺度和多任务分析，实现了强大的病理学推理能力，并能同时处理ROI和WSI级别的多种诊断任务。在一项包含230万ROI和18.8万WSI样本的大规模数据集上，SmartPath-R1在72项任务中均表现出卓越的性能，标志着精准病理学AI系统发展的重要进展。", "keywords": "多模态大语言模型, 计算病理学, 推理增强, ROI, WSI, SmartPath-R1", "comments": "这项研究通过提出SmartPath-R1，显著提升了多模态大语言模型在计算病理学领域的应用潜力。其创新点在于通过结合新的微调策略（尺度依赖监督微调和任务感知强化微调）来规避对昂贵思维链标注的依赖，并利用专家混合机制实现多尺度和多任务处理，从而解决了现有模型推理能力受限和任务覆盖不全的问题。该模型能够同时处理ROI和WSI级别任务，并在一系列广泛任务上验证了其有效性，为开发更通用、更智能的病理学AI辅助诊断系统奠定了基础。"}}
{"id": "2406.13166", "title": "Enhancing supply chain security with automated machine learning", "authors": ["Haibo Wang", "Lutfu S. Sua", "Bahram Alidaee"], "categories": ["cs.LG", "econ.GN", "math.OC", "q-fin.EC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      36 pages", "url": "http://arxiv.org/abs/2406.13166v3", "summary": "The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency.", "comment": "36 pages", "pdf_url": "http://arxiv.org/pdf/2406.13166v3", "cate": "cs.LG", "date": "2024-06-19", "updated": "2025-07-22", "AI": {"title_translation": "利用自动化机器学习增强供应链安全", "tldr": "本研究提出了一个自动化机器学习框架，用于增强供应链安全，通过检测欺诈活动、预测维护需求和预测物料积压，并取得了高精度。", "motivation": "全球供应链规模和复杂性日益增加，导致了港口排队、材料短缺和通货膨胀等挑战。鉴于供应链的庞大规模和大量可用数据，应用机器学习方法来解决这些挑战的兴趣日益增长，因为机器学习技术能更快地进行预测并近似最优解。", "method": "本文提出了一个自动化机器学习框架来增强供应链安全，具体通过检测欺诈活动、预测维护需求和预测物料积压。该框架利用了随机森林、XGBoost、LightGBM和神经网络等机器学习技术，并整合了数据预处理、特征选择、模型优化和推理部署。", "result": "使用不同大小的数据集，欺诈检测的准确率为88%（使用采样方法），机器故障预测的准确率为93.4%，物料积压预测的准确率为89.3%。超参数调优显著提高了这些模型的性能，其中XGBoost和LightGBB等某些监督技术达到了100%的精度。", "conclusion": "本研究通过简化数据预处理、特征选择、模型优化和推理部署，为供应链安全做出了贡献，解决了关键挑战并提高了运营效率。", "translation": "全球供应链日益增长的规模和复杂性带来了各种领域的新挑战，例如港口长时间排队导致的供应链中断、材料短缺和通货膨胀。伴随着供应链的规模和大量数据的可用性，解决这些挑战的努力使得人们对在供应链的许多方面应用机器学习方法产生了越来越浓厚的兴趣。与其他解决方案不同，包括随机森林、XGBoost、LightGBM和神经网络在内的机器学习技术能够更快地进行预测并近似最优解。本文提出了一个自动化机器学习框架，通过检测欺诈活动、预测维护需求和预测材料积压来增强供应链安全。使用不同大小的数据集，结果显示，欺诈检测使用采样方法实现了88%的准确率，机器故障预测达到了93.4%的准确率，材料积压预测达到了89.3%的准确率。超参数调优显著提高了这些模型的性能，某些监督技术如XGBoost和LightGBM达到了100%的精度。这项研究通过简化数据预处理、特征选择、模型优化和推理部署，为供应链安全做出了贡献，解决了关键挑战并提高了运营效率。", "summary": "本研究提出了一个自动化机器学习框架，旨在提升供应链安全，通过有效检测欺诈、预测设备维护需求以及预报物料积压。该框架整合了数据预处理、特征选择、模型优化和部署，并利用了多种机器学习算法。实验结果显示，在欺诈检测、机器故障预测和物料积压预测方面均取得了高准确率，特别是通过超参数调优，部分模型如XGBoost和LightGBM的精度可达100%。该工作通过优化数据处理流程，显著提升了供应链的运营效率和安全性。", "keywords": "供应链安全, 自动化机器学习, 欺诈检测, 预测维护, 物料积压", "comments": "该论文创新性地将自动化机器学习应用于供应链安全领域，解决了传统供应链管理中面临的复杂性和数据量大的挑战。其重要性在于提供了一个端到端的解决方案，从数据预处理到模型部署，提升了预测准确性和运营效率。特别是提到某些模型在超参数调优后达到100%的精度，这在实际应用中可能需要进一步验证其泛化能力，但无疑展示了该方法的巨大潜力。"}}
{"id": "2507.17691", "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17691v1", "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17691v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CASCADE：谷歌的LLM驱动JavaScript反混淆器", "tldr": "CASCADE是一个谷歌开发的LLM驱动的混合方法，用于反混淆JavaScript代码，通过结合Gemini和JSIR来提高效率并减少逆向工程工作。", "motivation": "JavaScript中的软件混淆阻碍了代码理解和分析，对软件测试、静态分析和恶意软件检测构成了重大挑战。", "method": "CASCADE采用一种新颖的混合方法，将Gemini的先进编码能力与编译器中间表示（IR），特别是JavaScript IR（JSIR）的确定性转换能力相结合。它利用Gemini识别关键的序言函数，然后利用JSIR进行后续的代码转换，以恢复语义元素并揭示原始程序行为。", "result": "CASCADE能够有效恢复原始字符串和API名称等语义元素，并揭示原始程序行为。它克服了现有静态和动态反混淆技术的局限性，消除了数百到数千条硬编码规则，同时实现了可靠性和灵活性。CASCADE已部署在谷歌的生产环境中，显著提高了JavaScript反混淆效率并减少了逆向工程工作。", "conclusion": "CASCADE通过结合LLM和IR的混合方法，为JavaScript反混淆提供了一个高效、可靠且灵活的解决方案，并已在实际生产环境中验证了其价值。", "translation": "软件混淆，尤其是在JavaScript中普遍存在，阻碍了代码理解和分析，对软件测试、静态分析和恶意软件检测构成了重大挑战。本文介绍了CASCADE，这是一种新颖的混合方法，它将Gemini的先进编码能力与编译器中间表示（IR），特别是JavaScript IR（JSIR）的确定性转换能力相结合。通过利用Gemini识别关键的序言函数（这是最普遍的混淆技术的基础组成部分），并利用JSIR进行后续的代码转换，CASCADE有效地恢复了原始字符串和API名称等语义元素，并揭示了原始程序行为。这种方法克服了现有静态和动态反混淆技术的局限性，消除了数百到数千条硬编码规则，同时实现了可靠性和灵活性。CASCADE已部署在谷歌的生产环境中，证明了JavaScript反混淆效率的显著提高，并减少了逆向工程工作。", "summary": "本文介绍了CASCADE，一个谷歌开发的LLM驱动JavaScript反混淆器。它采用一种结合Gemini语言模型和JavaScript IR的混合方法，旨在克服现有反混淆技术的局限性。CASCADE通过识别混淆中的关键序言函数并进行代码转换，有效恢复原始语义信息和程序行为。该系统已在谷歌生产环境中部署，显著提高了反混淆效率并减少了逆向工程工作。", "keywords": "JavaScript反混淆, LLM, Gemini, JSIR, 软件混淆", "comments": "CASCADE的创新之处在于其LLM（Gemini）与编译器IR的混合方法，这使其能够摆脱传统硬编码规则的限制，提高了反混淆的灵活性和可靠性。其在谷歌生产环境的部署证明了其实用价值和重要性。"}}
{"id": "2409.14659", "title": "Image memorability predicts social media virality and externally-associated commenting", "authors": ["Shikang Peng", "Wilma A. Bainbridge"], "categories": ["cs.HC", "cs.CE", "cs.SI", "J.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      47 pages, 5 figures", "url": "http://arxiv.org/abs/2409.14659v2", "summary": "Visual content on social media plays a key role in entertainment and\ninformation sharing, yet some images gain more engagement than others. We\npropose that image memorability - the ability to be remembered - may predict\nviral potential. Using 1,247 Reddit image posts across three timepoints, we\nassessed memorability with neural network ResMem and correlated the predicted\nmemorability scores with virality metrics. Memorable images are consistently\nassociated with more comments, even after controlling for image categories with\nResNet-152. Semantic analysis revealed that memorable images relate to more\nneutral-affect comments, suggesting a distinct pathway to virality from\nemotional contents. Additionally, visual consistency analysis showed that\nmemorable posts inspired diverse, externally-associated comments. By analyzing\nResMem's layers, we found that semantic distinctiveness was key to both\nmemorability and virality even after accounting for image category effects.\nThis study highlights memorability as a unique correlate of social media\nvirality, offering insights into how visual features and human cognitive\nbehavioral interactions are associated with online engagement.", "comment": "47 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2409.14659v2", "cate": "cs.HC", "date": "2024-09-23", "updated": "2025-07-22", "AI": {"title_translation": "图像记忆度预测社交媒体病毒式传播和外部相关评论", "tldr": "本研究提出图像记忆度可以预测社交媒体上的病毒式传播。通过分析Reddit上的图片帖子，发现可记忆的图片与更多评论相关，且这些评论多为中性情感，并具有语义独特性。", "motivation": "社交媒体上的视觉内容在娱乐和信息共享中扮演着关键角色，然而有些图片比其他图片获得更多的参与度。本研究提出图像记忆度（被记住的能力）可能预测病毒式传播潜力。", "method": "使用1,247个Reddit图片帖子，在三个时间点评估了记忆度，使用神经网络ResMem评估记忆度，并将预测的记忆度分数与病毒式传播指标相关联。通过ResNet-152控制图像类别后，进行了语义分析和视觉一致性分析，并分析了ResMem的层。", "result": "可记忆的图片始终与更多评论相关，即使在控制了图像类别之后。语义分析显示，可记忆的图片与更多中性情感的评论相关，这表明与情感内容不同的病毒式传播途径。此外，视觉一致性分析表明，可记忆的帖子激发了多样化的、外部相关的评论。通过分析ResMem的层，发现语义独特性是记忆度和病毒式传播的关键，即使在考虑了图像类别效应之后。", "conclusion": "本研究强调记忆度是社交媒体病毒式传播的一个独特关联因素，为视觉特征和人类认知行为互动如何与在线参与相关联提供了见解。", "translation": "社交媒体上的视觉内容在娱乐和信息共享中扮演着关键角色，然而有些图片比其他图片获得更多的参与度。我们提出图像记忆度——被记住的能力——可能预测病毒式传播潜力。通过使用跨越三个时间点的1,247个Reddit图片帖子，我们使用神经网络ResMem评估了记忆度，并将预测的记忆度分数与病毒式传播指标相关联。即使在控制了图像类别（使用ResNet-152）之后，可记忆的图片始终与更多评论相关。语义分析显示，可记忆的图片与更多中性情感的评论相关，这表明与情感内容不同的病毒式传播途径。此外，视觉一致性分析表明，可记忆的帖子激发了多样化的、外部相关的评论。通过分析ResMem的层，我们发现语义独特性是记忆度和病毒式传播的关键，即使在考虑了图像类别效应之后。这项研究强调记忆度是社交媒体病毒式传播的一个独特关联因素，为视觉特征和人类认知行为互动如何与在线参与相关联提供了见解。", "summary": "本研究探讨了图像记忆度与社交媒体病毒式传播之间的关系。研究人员使用神经网络ResMem分析了Reddit上的1,247张图片，发现图像记忆度可以预测其在社交媒体上的受欢迎程度。结果显示，可记忆的图片能吸引更多评论，且这些评论往往是中性情感而非强烈情感，这表明了不同的传播机制。此外，研究发现语义独特性是图像记忆度和病毒式传播的关键因素。本研究强调了图像记忆度在理解在线参与中的重要性。", "keywords": "图像记忆度, 社交媒体, 病毒式传播, ResMem, 语义独特性", "comments": "这项研究创新性地将图像记忆度这一认知心理学概念引入社交媒体病毒式传播的研究中，并利用了先进的神经网络模型进行分析。其发现记忆度与病毒式传播的独特关联，特别是中性情感评论和语义独特性的作用，为内容创作者和平台提供了新的视角。该研究的局限性可能在于其数据来源仅限于Reddit，未来可以扩展到其他社交媒体平台以验证其普遍性。"}}
{"id": "2507.17368", "title": "ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning", "authors": ["Hao Dai", "Chong Tang", "Jagmohan Chauhan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures", "url": "http://arxiv.org/abs/2507.17368v1", "summary": "Continual learning (CL) with long-tailed data distributions remains a\ncritical challenge for real-world AI systems, where models must sequentially\nadapt to new classes while retaining knowledge of old ones, despite severe\nclass imbalance. Existing methods struggle to balance stability and plasticity,\noften collapsing under extreme sample scarcity. To address this, we propose\nViRN, a novel CL framework that integrates variational inference (VI) with\ndistributional trilateration for robust long-tailed learning. First, we model\nclass-conditional distributions via a Variational Autoencoder to mitigate bias\ntoward head classes. Second, we reconstruct tail-class distributions via\nWasserstein distance-based neighborhood retrieval and geometric fusion,\nenabling sample-efficient alignment of tail-class representations. Evaluated on\nsix long-tailed classification benchmarks, including speech (e.g., rare\nacoustic events, accents) and image tasks, ViRN achieves a 10.24% average\naccuracy gain over state-of-the-art methods.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.17368v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "ViRN：用于长尾持续表征学习的变分推断与分布三边测量", "tldr": "ViRN是一个新的持续学习框架，它结合了变分推断和分布三边测量，旨在解决长尾数据分布下的持续学习挑战，并在多个基准测试中取得了显著的准确性提升。", "motivation": "在真实世界的人工智能系统中，长尾数据分布下的持续学习仍然是一个关键挑战。模型必须顺序适应新类别，同时保留旧知识，尽管存在严重的类别不平衡。现有方法难以平衡稳定性和可塑性，在样本极端稀缺时经常崩溃。", "method": "我们提出了ViRN，一个新颖的持续学习框架，它将变分推断（VI）与分布三边测量相结合，以实现鲁棒的长尾学习。首先，我们通过变分自编码器建模类别条件分布，以减轻对头部类别的偏见。其次，我们通过基于Wasserstein距离的邻域检索和几何融合重建尾部类别分布，从而实现尾部类别表征的样本高效对齐。", "result": "ViRN在包括语音（例如，罕见声学事件、口音）和图像任务在内的六个长尾分类基准测试中进行了评估，比最先进的方法平均准确率提高了10.24%。", "conclusion": "ViRN通过结合变分推断和分布三边测量，有效解决了长尾持续学习中的挑战，并在多个基准测试中取得了显著的性能提升，证明了其在处理类别不平衡和知识保留方面的有效性。", "translation": "持续学习（CL）与长尾数据分布仍然是真实世界AI系统的一个关键挑战，其中模型必须顺序适应新类别，同时保留旧知识，尽管存在严重的类别不平衡。现有方法难以平衡稳定性和可塑性，在样本极端稀缺时经常崩溃。为了解决这个问题，我们提出了ViRN，一个新颖的CL框架，它将变分推断（VI）与分布三边测量相结合，以实现鲁棒的长尾学习。首先，我们通过变分自编码器建模类别条件分布，以减轻对头部类别的偏见。其次，我们通过基于Wasserstein距离的邻域检索和几何融合重建尾部类别分布，从而实现尾部类别表征的样本高效对齐。ViRN在包括语音（例如，罕见声学事件、口音）和图像任务在内的六个长尾分类基准测试中进行了评估，比最先进的方法平均准确率提高了10.24%。", "summary": "本文提出了ViRN，一个针对长尾持续学习的新颖框架，旨在解决现有方法在类别不平衡和知识保留上的不足。ViRN结合了变分推断（通过变分自编码器处理头部类别偏差）和分布三边测量（通过Wasserstein距离和几何融合重建尾部类别分布）。实验结果表明，ViRN在六个长尾分类基准测试中，相较于现有最佳方法，平均准确率提升了10.24%，证明了其在持续学习场景下处理长尾数据分布的有效性。", "keywords": "持续学习, 长尾数据, 变分推断, 分布三边测量, 表征学习", "comments": "ViRN的创新点在于将变分推断与分布三边测量结合，以有效处理长尾数据分布下的持续学习挑战。特别是其通过变分自编码器减轻头部类别偏差和通过Wasserstein距离重建尾部类别分布的方法，为解决样本稀缺问题提供了新颖的视角，显著提升了模型在严峻类别不平衡条件下的稳定性和性能。"}}
{"id": "2507.15292", "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "authors": ["An Wang", "Rulin Zhou", "Mengya Xu", "Yiru Ye", "Longfei Gou", "Yiting Chang", "Hao Chen", "Chwee Ming Lim", "Jiankun Wang", "Hongliang Ren"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15292v3", "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15292v3", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "EndoControlMag：基于周期性参考重置和分层组织感知双掩模控制的鲁棒内窥镜血管运动放大", "tldr": "EndoControlMag是一种无需训练的内窥镜血管运动放大框架，通过周期性参考重置和分层组织感知双掩模控制，显著提高了复杂手术场景下的放大精度和视觉质量。", "motivation": "在内窥镜手术中，观察微小的血管运动对于手术精度和决策至关重要，但由于手术场景的复杂性和动态性，这仍然具有挑战性。", "method": "论文提出了EndoControlMag，一个无需训练的、基于拉格朗日的框架，用于内窥镜环境下的掩模条件血管运动放大。它包含两个关键模块：1) 周期性参考重置（PRR）方案，将视频分割成短的重叠片段，并动态更新参考帧，以防止误差累积并保持时间连贯性。2) 分层组织感知放大（HTM）框架，具有双模式掩模膨胀，首先使用预训练的视觉跟踪模型跟踪血管核心，然后对周围组织应用两种自适应软化策略之一：基于运动的软化（根据组织位移调整放大强度）或基于距离的指数衰减（模拟生物力学衰减）。", "result": "EndoControlMag在EndoVMM24数据集上进行了评估，该数据集涵盖四种不同手术类型和各种挑战性场景。定量指标、视觉评估和专家外科医生评估表明，EndoControlMag在放大精度和视觉质量方面显著优于现有方法，同时在挑战性手术条件下保持鲁棒性。", "conclusion": "EndoControlMag显著提高了内窥镜血管运动放大的精度和视觉质量，并在复杂的手术条件下表现出鲁棒性，为外科医生提供了关键的视觉信息。", "translation": "在内窥镜手术中，可视化细微的血管运动对于手术精度和决策至关重要，但由于手术场景的复杂性和动态性，这仍然具有挑战性。为了解决这个问题，我们引入了EndoControlMag，一个无需训练的、基于拉格朗日的框架，具有针对内窥镜环境量身定制的掩模条件血管运动放大功能。我们的方法具有两个关键模块：一个周期性参考重置（PRR）方案，它将视频分成短的重叠片段，并动态更新参考帧，以防止误差累积，同时保持时间连贯性；以及一个分层组织感知放大（HTM）框架，具有双模式掩模膨胀。HTM首先使用预训练的视觉跟踪模型跟踪血管核心，以在遮挡和视图变化下保持准确的定位。然后，它对周围组织应用两种自适应软化策略之一：基于运动的软化，根据观察到的组织位移按比例调节放大强度；或基于距离的指数衰减，模拟生物力学力衰减。这种双模式方法适应了不同的手术场景——基于运动的软化在复杂组织变形方面表现出色，而基于距离的软化在不可靠的光流条件下提供稳定性。我们在EndoVMM24数据集上评估了EndoControlMag，该数据集涵盖四种不同手术类型和各种挑战性场景，包括遮挡、器械干扰、视图变化和血管变形。定量指标、视觉评估和专家外科医生评估表明，EndoControlMag在放大精度和视觉质量方面显著优于现有方法，同时在挑战性手术条件下保持鲁棒性。代码、数据集和视频结果可在https://szupc.github.io/EndoControlMag/获取。", "summary": "本文提出了EndoControlMag，一个用于内窥镜血管运动放大的无需训练的拉格朗日框架。该框架通过周期性参考重置（PRR）解决误差累积问题，并引入分层组织感知放大（HTM）模块，利用双模式掩模膨胀（基于运动和基于距离的软化）来适应复杂的手术场景。在EndoVMM24数据集上的评估显示，EndoControlMag在放大精度、视觉质量和鲁棒性方面均显著优于现有方法。", "keywords": "内窥镜手术, 运动放大, 血管可视化, 周期性参考重置, 组织感知", "comments": "EndoControlMag的创新之处在于其训练无关的拉格朗日框架，特别是在内窥镜手术复杂环境中的应用。PRR和HTM模块的设计，尤其是HTM中针对不同组织变形模式的双模式软化策略，展现了对实际手术场景的深刻理解和有效应对。该方法在解决内窥镜下微小血管运动可视化难题上具有重要意义，有助于提高手术精度和安全性。"}}
{"id": "2411.16619", "title": "Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric", "authors": ["Zhichao Zhang", "Wei Sun", "Xinyue Li", "Yunhao Li", "Qihang Ge", "Jun Jia", "Zicheng Zhang", "Zhongpeng Ji", "Fengyu Sun", "Shangling Jui", "Xiongkuo Min", "Guangtao Zhai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM 2025", "url": "http://arxiv.org/abs/2411.16619v3", "summary": "AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n6,000 AGVs derived from 15 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released at\nhttps://github.com/zczhang-sjtu/GHVQ.git.", "comment": "Accepted by ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2411.16619v3", "cate": "cs.CV", "date": "2024-11-25", "updated": "2025-07-23", "AI": {"title_translation": "人类活动AGV质量评估：一个基准数据集和客观评估指标", "tldr": "本文构建了一个人类活动AGV质量评估数据集（Human-AGVQA）并提出了一个客观评估指标（GHVQ），用于解决AI生成视频中人类活动存在的视觉和语义失真问题，实验证明GHVQ优于现有指标。", "motivation": "AI生成的视频（AGVs）中涉及人类活动的视频常出现严重的视觉和语义失真，这阻碍了视频生成技术在现实世界场景中的实际应用。", "method": "本文首先构建了AI生成人类活动视频质量评估（Human-AGVQA）数据集，包含6,000个AGV，这些AGV由15个流行的文本到视频（T2V）模型使用400个描述多样人类活动的文本提示生成。研究人员进行了主观研究，评估了AGV的人体外观质量、动作连续性质量和整体视频质量，并识别了人体部位的语义问题。其次，开发了一个名为AI生成人类活动视频质量指标（GHVQ）的客观评估指标，用于自动分析人类活动AGV的质量。GHVQ系统地提取了以人为中心的质量特征、AI生成内容感知质量特征和时间连续性特征。", "result": "GHVQ在Human-AGVQA数据集上的表现大大优于现有质量指标，证明了其在评估人类活动AGV质量方面的有效性。", "conclusion": "本文证明了Human-AGVQA数据集和GHVQ指标在解决AI生成人类活动视频的质量评估问题上的有效性，并计划公开发布这些资源以促进未来研究。", "translation": "AI驱动的视频生成技术近年来取得了显著进展。然而，涉及人类活动的AI生成视频（AGV）通常表现出严重的视觉和语义失真，阻碍了视频生成技术在现实世界场景中的实际应用。为了解决这一挑战，我们对人类活动AGV质量评估进行了开创性研究，重点关注视觉质量评估和语义失真的识别。首先，我们构建了AI生成人类活动视频质量评估（Human-AGVQA）数据集，该数据集包含6,000个AGV，这些AGV来自15个流行的文本到视频（T2V）模型，使用了400个描述多样人类活动的文本提示。我们进行了一项主观研究，评估了AGV的人体外观质量、动作连续性质量和整体视频质量，并识别了人体部位的语义问题。基于Human-AGVQA，我们对T2V模型的性能进行了基准测试，并分析了它们在生成不同类别人类活动方面的优势和劣势。其次，我们开发了一个名为AI生成人类活动视频质量指标（GHVQ）的客观评估指标，以自动分析人类活动AGV的质量。GHVQ系统地提取了以人为中心的质量特征、AI生成内容感知质量特征和时间连续性特征，使其成为一个全面且可解释的人类活动AGV质量指标。广泛的实验结果表明，GHVQ在Human-AGVQA数据集上的表现大大优于现有质量指标，证明了其在评估人类活动AGV质量方面的有效性。Human-AGVQA数据集和GHVQ指标将在https://github.com/zczhang-sjtu/GHVQ.git发布。", "summary": "本文针对AI生成人类活动视频（AGV）存在的视觉和语义失真问题，提出了一个开创性的人类活动AGV质量评估框架。该框架包括构建了包含6000个AGV的Human-AGVQA基准数据集，并基于主观评估分析了T2V模型的优缺点。同时，开发了一个名为GHVQ的客观评估指标，该指标通过提取以人为中心的、内容感知的和时间连续性特征来综合评估AGV质量。实验证明，GHVQ在Human-AGVQA数据集上显著优于现有指标，有效解决了人类活动AGV的质量评估难题。", "keywords": "AI生成视频, 质量评估, 数据集, 客观指标, 人类活动", "comments": "该论文的创新之处在于首次系统地关注人类活动AGV的质量评估，并提供了两个关键贡献：一个大规模、多样化的人类活动AGV基准数据集Human-AGVQA，以及一个新颖、有效的客观评估指标GHVQ。GHVQ通过结合多维度特征，使其在评估人类活动AGV方面更全面和可解释，这对于推动AI视频生成技术在实际应用中的发展具有重要意义。"}}
{"id": "2507.17130", "title": "MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments", "authors": ["Seokhwan Jeong", "Hogyun Kim", "Younggun Cho"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 9 figures", "url": "http://arxiv.org/abs/2507.17130v1", "summary": "This paper presents a novel spherical target-based LiDAR-camera extrinsic\ncalibration method designed for outdoor environments with multi-robot systems,\nconsidering both target and sensor corruption. The method extracts the 2D\nellipse center from the image and the 3D sphere center from the pointcloud,\nwhich are then paired to compute the transformation matrix. Specifically, the\nimage is first decomposed using the Segment Anything Model (SAM). Then, a novel\nalgorithm extracts an ellipse from a potentially corrupted sphere, and the\nextracted center of ellipse is corrected for errors caused by the perspective\nprojection model. For the LiDAR pointcloud, points on the sphere tend to be\nhighly noisy due to the absence of flat regions. To accurately extract the\nsphere from these noisy measurements, we apply a hierarchical weighted sum to\nthe accumulated pointcloud. Through experiments, we demonstrated that the\nsphere can be robustly detected even under both types of corruption,\noutperforming other targets. We evaluated our method using three different\ntypes of LiDARs (spinning, solid-state, and non-repetitive) with cameras\npositioned in three different locations. Furthermore, we validated the\nrobustness of our method to target corruption by experimenting with spheres\nsubjected to various types of degradation. These experiments were conducted in\nboth a planetary test and a field environment. Our code is available at\nhttps://github.com/sparolab/MARSCalib.", "comment": "8 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.17130v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MARSCalib：多机器人、自动、鲁棒、基于球形目标的野外和地外环境外参标定", "tldr": "本文提出了一种针对多机器人系统在野外和地外环境中，基于球形目标、鲁棒的激光雷达-相机外参标定方法。", "motivation": "现有方法在多机器人系统、户外环境以及目标和传感器存在损坏的情况下进行激光雷达-相机外参标定时面临挑战。", "method": "本文提出了一种新颖的基于球形目标的激光雷达-相机外参标定方法。该方法从图像中提取2D椭圆中心，从点云中提取3D球体中心，然后进行配对以计算变换矩阵。具体地，图像首先使用Segment Anything Model (SAM) 进行分解。然后，一种新颖的算法从可能损坏的球体中提取椭圆，并对透视投影模型引起的误差进行校正。对于激光雷达点云，为从噪声测量中准确提取球体，对累积点云应用分层加权和。", "result": "实验证明，即使在两种类型的损坏下，球体也能被鲁棒地检测到，并且优于其他目标。该方法使用三种不同类型的激光雷达（旋转式、固态和非重复式）以及位于三个不同位置的相机进行了评估。此外，通过对遭受各种类型降解的球体进行实验，验证了该方法对目标损坏的鲁棒性。这些实验在行星测试环境和野外环境进行。", "conclusion": "该方法在野外和地外环境中，即使面对目标和传感器损坏，也能为多机器人系统提供鲁棒的激光雷达-相机外参标定。", "translation": "本文提出了一种新颖的基于球形目标的激光雷达-相机外参标定方法，专为户外多机器人系统设计，并考虑了目标和传感器损坏的情况。该方法从图像中提取2D椭圆中心，从点云中提取3D球体中心，然后进行配对以计算变换矩阵。具体来说，首先使用Segment Anything Model (SAM) 对图像进行分解。然后，一种新颖的算法从可能损坏的球体中提取椭圆，并对透视投影模型引起的误差进行校正。对于激光雷达点云，由于缺乏平面区域，球体上的点往往噪声很大。为了从这些嘈杂的测量中准确提取球体，我们对累积点云应用了分层加权和。通过实验，我们证明了即使在两种类型的损坏下，球体也能被鲁棒地检测到，并且优于其他目标。我们使用三种不同类型的激光雷达（旋转式、固态和非重复式）以及位于三个不同位置的相机对我们的方法进行了评估。此外，我们通过对遭受各种类型降解的球体进行实验，验证了我们方法对目标损坏的鲁棒性。这些实验在行星测试环境和野外环境进行。我们的代码可在https://github.com/sparolab/MARSCalib 获取。", "summary": "本文介绍了一种名为MARSCalib的新型激光雷达-相机外参标定方法，该方法基于球形目标，专为户外多机器人系统设计，并能应对目标和传感器损坏。该方法通过从图像中提取2D椭圆中心和从点云中提取3D球体中心并进行配对来计算变换矩阵。它利用SAM进行图像分解，并提出了一种新的椭圆提取和误差校正算法。对于噪声大的点云，采用分层加权和进行球体提取。实验证明，该方法在存在损坏的情况下也能鲁棒地检测球体，并优于其他目标，且对多种激光雷达和相机配置以及目标降解均表现出良好的鲁棒性，适用于野外和地外环境。", "keywords": "激光雷达-相机标定, 外参标定, 球形目标, 多机器人, 鲁棒性", "comments": "创新点：提出了新颖的基于球形目标的激光雷达-相机外参标定方法，特别是在处理受损目标和噪声传感器数据方面的鲁棒性。重要性：解决了多机器人系统在复杂户外（包括地外）环境中高精度外参标定的难题，尤其是在目标和传感器可能受损的实际应用场景中具有重要意义。方法特点：结合了先进的图像分割模型（SAM）和针对噪声点云的独特处理方法（分层加权和），提升了标定精度和鲁棒性。通用性：在多种激光雷达类型和相机配置下进行了验证，增加了方法的通用性和适用范围。"}}
{"id": "2507.17571", "title": "Bounds and Equivalence of Skew Polycyclic Codes over Finite Fields", "authors": ["Hassan Ou-azzou", "Anna-Lena Horlemann", "Nuh Aydin"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17571v1", "summary": "We study skew polycyclic codes over a finite field $\\mathbb{F}_q$, associated\nwith a skew polynomial $f(x) \\in \\mathbb{F}_q[x;\\sigma]$, where $\\sigma$ is an\nautomorphism of $\\mathbb{F}_q$. We start by proving the Roos-like bound for\nboth the Hamming and the rank metric for this class of codes. Next, we focus on\nthe Hamming and rank equivalence between two classes of polycyclic codes by\nintroducing an equivalence relation and describing its equivalence classes.\nFinally, we present examples that illustrate applications of the theory\ndeveloped in this paper.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17571v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "斜多项式循环码在有限域上的界和等价性", "tldr": "本文研究了有限域上斜多项式循环码的Roos类界及其在汉明度量和秩度量下的等价性。", "motivation": "研究有限域上与斜多项式相关的斜多项式循环码的性质，特别是其界和等价性。", "method": "首先，证明了斜多项式循环码在汉明度量和秩度量下的Roos类界。其次，通过引入等价关系并描述其等价类，研究了两类多项式循环码之间的汉明和秩等价性。最后，提供了应用实例。", "result": "证明了斜多项式循环码在汉明度量和秩度量下的Roos类界。引入并描述了等价关系及其等价类，从而研究了汉明和秩等价性。", "conclusion": "本文发展了关于斜多项式循环码的理论，并提供了应用实例。", "translation": "我们研究了有限域$\\mathbb{F}_q$上的斜多项式循环码，这些码与斜多项式$f(x) \\in \\mathbb{F}_q[x;\\sigma]$相关联，其中$\\sigma$是$\\mathbb{F}_q$的一个自同构。我们首先证明了这类码在汉明度量和秩度量下的Roos类界。接下来，我们通过引入等价关系并描述其等价类，研究了两类多项式循环码之间的汉明和秩等价性。最后，我们给出了说明本文所发展理论应用的例子。", "summary": "本文研究了有限域上与斜多项式相关的斜多项式循环码。研究内容包括证明了这些码在汉明度量和秩度量下的Roos类界，并通过引入等价关系探讨了不同类多项式循环码间的汉明和秩等价性，并提供了理论应用实例。", "keywords": "斜多项式循环码, Roos类界, 汉明度量, 秩度量, 等价性", "comments": "本文将Roos类界和等价性的研究扩展到了斜多项式循环码这一特定类型的编码上，这对于理解这类码的结构和性能具有重要意义。通过引入新的等价关系，为密码学和编码理论中的进一步研究提供了基础。"}}
{"id": "2507.17312", "title": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance", "authors": ["Peiqi Chen", "Lei Yu", "Yi Wan", "Yingying Pei", "Xinyi Liu", "Yongxiang Yao", "Yingying Zhang", "Lixiang Ru", "Liheng Zhong", "Jingdong Chen", "Ming Yang", "Yongjun Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.17312v1", "summary": "Semi-dense feature matching methods have shown strong performance in\nchallenging scenarios. However, the existing pipeline relies on a global search\nacross the entire feature map to establish coarse matches, limiting further\nimprovements in accuracy and efficiency. Motivated by this limitation, we\npropose a novel pipeline, CasP, which leverages cascaded correspondence priors\nfor guidance. Specifically, the matching stage is decomposed into two\nprogressive phases, bridged by a region-based selective cross-attention\nmechanism designed to enhance feature discriminability. In the second phase,\none-to-one matches are determined by restricting the search range to the\none-to-many prior areas identified in the first phase. Additionally, this\npipeline benefits from incorporating high-level features, which helps reduce\nthe computational costs of low-level feature extraction. The acceleration gains\nof CasP increase with higher resolution, and our lite model achieves a speedup\nof $\\sim2.2\\times$ at a resolution of 1152 compared to the most efficient\nmethod, ELoFTR. Furthermore, extensive experiments demonstrate its superiority\nin geometric estimation, particularly with impressive cross-domain\ngeneralization. These advantages highlight its potential for latency-sensitive\nand high-robustness applications, such as SLAM and UAV systems. Code is\navailable at https://github.com/pq-chen/CasP.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17312v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CasP：利用级联对应先验指导改进半稠密特征匹配流程", "tldr": "CasP通过利用级联对应先验，改进了半稠密特征匹配流程，显著提升了精度和效率，尤其在高分辨率和跨域场景下表现出色。", "motivation": "现有半稠密特征匹配方法依赖于对整个特征图进行全局搜索以建立粗略匹配，这限制了其在精度和效率上的进一步提升。", "method": "本文提出了一种名为CasP的新型流程，它利用级联对应先验进行指导。具体来说，匹配阶段被分解为两个渐进的阶段，通过一个旨在增强特征判别性的基于区域的选择性交叉注意力机制连接。在第二阶段，通过将搜索范围限制在第一阶段识别出的一对多先验区域内来确定一对一匹配。此外，该流程通过整合高级特征来降低低级特征提取的计算成本。", "result": "CasP的加速增益随分辨率的提高而增加，其lite模型在1152分辨率下比最有效的ELoFTR方法实现了约2.2倍的加速。此外，广泛的实验证明了CasP在几何估计方面的优越性，尤其在跨域泛化方面表现出色。", "conclusion": "CasP在精度和效率上的优势，特别是其出色的跨域泛化能力，凸显了其在延迟敏感和高鲁棒性应用（如SLAM和UAV系统）中的巨大潜力。", "translation": "半稠密特征匹配方法在挑战性场景中表现出强大的性能。然而，现有流程依赖于对整个特征图进行全局搜索以建立粗略匹配，这限制了精度和效率的进一步提升。受此限制的启发，我们提出了一种名为CasP的新型流程，它利用级联对应先验进行指导。具体来说，匹配阶段被分解为两个渐进的阶段，通过一个旨在增强特征判别性的基于区域的选择性交叉注意力机制连接。在第二阶段，通过将搜索范围限制在第一阶段识别出的一对多先验区域内来确定一对一匹配。此外，该流程通过整合高级特征来降低低级特征提取的计算成本。CasP的加速增益随分辨率的提高而增加，我们的lite模型在1152分辨率下比最有效的ELoFTR方法实现了约2.2倍的加速。此外，广泛的实验证明了其在几何估计方面的优越性，特别是在出色的跨域泛化方面。这些优势凸显了其在延迟敏感和高鲁棒性应用（如SLAM和UAV系统）中的潜力。代码可在 https://github.com/pq-chen/CasP 获取。", "summary": "本文提出了一种名为CasP的新型半稠密特征匹配流程，旨在解决现有方法中全局搜索导致的效率和精度瓶颈。CasP通过将匹配分解为两个渐进阶段，并利用级联对应先验指导搜索，同时引入区域选择性交叉注意力机制和整合高级特征以优化计算。实验结果表明，CasP在高分辨率下能显著提升匹配速度（最高2.2倍加速），并在几何估计和跨域泛化能力方面表现出优越性，使其非常适用于需要高鲁棒性和低延迟的SLAM和UAV等应用。", "keywords": "半稠密特征匹配, 级联对应先验, 几何估计, 效率, 跨域泛化", "comments": "CasP的创新之处在于其分阶段的级联匹配策略，通过限制搜索范围和引入选择性交叉注意力机制，有效解决了传统全局搜索的效率问题。同时，整合高级特征进一步优化了计算成本。这使其在实际应用中，尤其是在SLAM和UAV等对实时性和鲁棒性要求高的领域具有重要意义和实用价值。"}}
{"id": "2507.17682", "title": "Audio-Vision Contrastive Learning for Phonological Class Recognition", "authors": ["Daiqi Liu", "Tomás Arias-Vergara", "Jana Hutter", "Andreas Maier", "Paula Andrea Pérez-Toro"], "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      conference to TSD 2025", "url": "http://arxiv.org/abs/2507.17682v1", "summary": "Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.", "comment": "conference to TSD 2025", "pdf_url": "http://arxiv.org/pdf/2507.17682v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "音视频对比学习用于音系类别识别", "tldr": "该研究提出一种结合实时磁共振成像（rtMRI）和语音信号的多模态深度学习框架，利用对比学习进行音系类别识别，并在USC-TIMIT数据集上取得了最先进的性能。", "motivation": "准确分类发音-音系特征对于理解人类语音生成和开发强大的语音技术至关重要，尤其是在临床环境中，针对性的音素分析和治疗可以提高疾病诊断准确性和个性化康复效果。", "method": "本文提出一个多模态深度学习框架，结合实时磁共振成像（rtMRI）和语音信号，对三种关键的发音维度（发音方式、发音部位和发音清浊）进行分类。研究人员对源自上述发音维度的15个音系类别进行分类，并使用四种音/视频配置评估系统：单模态rtMRI、单模态音频信号、多模态中间融合和基于对比学习的音视频融合。", "result": "在USC-TIMIT数据集上的实验结果表明，基于对比学习的方法实现了最先进的性能，平均F1分数达到0.81，比单模态基线绝对提高了0.23。结果证实了对比表征学习在多模态发音分析中的有效性。", "conclusion": "研究结果证实了对比表征学习在多模态发音分析中的有效性。", "translation": "准确分类发音-音系特征对于理解人类语音生成和开发强大的语音技术至关重要，尤其是在临床环境中，针对性的音素分析和治疗可以提高疾病诊断准确性和个性化康复效果。在这项工作中，我们提出一个多模态深度学习框架，结合实时磁共振成像（rtMRI）和语音信号，对三种关键的发音维度进行分类：发音方式、发音部位和发音清浊。我们对源自上述发音维度的15个音系类别进行分类，并使用四种音/视频配置评估系统：单模态rtMRI、单模态音频信号、多模态中间融合以及基于对比学习的音视频融合。在USC-TIMIT数据集上的实验结果表明，我们基于对比学习的方法实现了最先进的性能，平均F1分数达到0.81，比单模态基线绝对提高了0.23。结果证实了对比表征学习在多模态发音分析中的有效性。我们的代码和处理后的数据集将在https://github.com/DaE-plz/AC_Contrastive_Phonology 公开，以支持未来的研究。", "summary": "本研究提出了一种结合实时磁共振成像（rtMRI）和语音信号的多模态深度学习框架，用于音系类别识别，特别是对发音方式、发音部位和发音清浊等发音维度进行分类。通过对比学习进行音视频融合，该方法在USC-TIMIT数据集上取得了0.81的平均F1分数，比单模态基线提高了0.23，达到了最先进的性能，验证了对比表征学习在多模态发音分析中的有效性。", "keywords": "对比学习, 音系类别识别, 多模态深度学习, rtMRI, 语音技术", "comments": "该论文的创新点在于将对比学习应用于音视频融合以进行音系类别识别，显著提高了性能并达到了最先进水平。这对于理解人类语音生成和开发临床语音技术具有重要意义。"}}
{"id": "2503.03460", "title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models", "authors": ["Alessio Galatolo", "Zhenbang Dai", "Katie Winkle", "Meriem Beloucif"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL25 Findings", "url": "http://arxiv.org/abs/2503.03460v2", "summary": "Fine-tuning Large Language Models (LLMs) with first-order methods like\nback-propagation is computationally intensive. Zeroth-Order (ZO) optimisation\nuses function evaluations instead of gradients, reducing memory usage, but\nsuffers from slow convergence in high-dimensional models. As a result, ZO\nresearch in LLMs has mostly focused on classification, overlooking more complex\ngenerative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm\ndesigned for Preference Optimisation in LLMs. We begin by analysing the\ninterplay between policy and reward models during traditional (first-order)\nPreference Optimisation, uncovering patterns in their relative updates. Guided\nby these insights, we adapt Simultaneous Perturbation Stochastic Approximation\n(SPSA) with a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO", "comment": "ACL25 Findings", "pdf_url": "http://arxiv.org/pdf/2503.03460v2", "cate": "cs.CL", "date": "2025-03-05", "updated": "2025-07-23", "AI": {"title_translation": "可视化策略-奖励相互作用以指导大型语言模型的零阶偏好优化", "tldr": "本文介绍了ZOPrO，一种用于大型语言模型偏好优化的新型零阶算法，它通过分析策略-奖励相互作用并调整SPSA来加速收敛，并在生成任务上实现了与一阶方法相当的收敛时间。", "motivation": "大型语言模型（LLMs）的一阶微调计算量大。零阶（ZO）优化虽然减少了内存使用，但在高维模型中收敛速度慢，导致其在LLMs研究中主要集中于分类任务，而忽略了更复杂的生成任务。", "method": "我们引入了ZOPrO，一种专为LLMs偏好优化设计的零阶算法。首先分析了传统（一阶）偏好优化中策略模型和奖励模型之间的相互作用，揭示了它们相对更新的模式。在此基础上，我们通过有针对性的采样策略改进了同步扰动随机逼近（SPSA）以加速收敛。", "result": "在摘要、机器翻译和对话助手等任务上的实验表明，我们的方法持续增强了奖励信号，并实现了与一阶方法相当的收敛时间。尽管它未能超越一些最先进的方法，但我们的工作是首次将零阶方法应用于LLMs的偏好优化，超越了分类任务，为这一领域开辟了新的研究方向。", "conclusion": "ZOPrO是首个将零阶方法应用于大型语言模型偏好优化的工作，超越了传统的分类任务，为该领域开辟了新的研究方向。", "translation": "通过反向传播等一阶方法对大型语言模型（LLMs）进行微调计算量巨大。零阶（ZO）优化使用函数评估而非梯度，从而减少内存使用，但在高维模型中收敛速度慢。因此，LLMs中的零阶研究主要集中于分类，而忽略了更复杂的生成任务。在本文中，我们引入了ZOPrO，一种专为LLMs偏好优化设计的新型零阶算法。我们首先分析了传统（一阶）偏好优化中策略模型和奖励模型之间的相互作用，揭示了它们相对更新的模式。在这些见解的指导下，我们通过有针对性的采样策略改进了同步扰动随机逼近（SPSA）以加速收敛。通过在摘要、机器翻译和对话助手等任务上的实验，我们证明了我们的方法持续增强了奖励信号，并实现了与一阶方法相当的收敛时间。尽管它未能超越一些最先进的方法，但我们的工作是首次将零阶方法应用于LLMs的偏好优化，超越了分类任务，为这一领域开辟了新的研究方向。代码和可视化可在https://github.com/alessioGalatolo/VisZOPrO 获取。", "summary": "本文提出了一种名为ZOPrO的新型零阶（ZO）优化算法，用于大型语言模型（LLMs）的偏好优化。针对一阶微调计算成本高以及现有零阶方法在LLMs中主要局限于分类任务的问题，ZOPrO通过分析策略与奖励模型的相互作用，并采用带有目标采样策略的同步扰动随机逼近（SPSA）来加速收敛。实验证明，该方法在摘要、机器翻译和对话等生成任务上能有效提升奖励信号，并达到与一阶方法相近的收敛速度，为LLMs的零阶偏好优化开启了新的研究方向。", "keywords": "零阶优化, 大型语言模型, 偏好优化, SPSA, 策略-奖励相互作用", "comments": "这项工作的创新之处在于首次将零阶优化方法应用于大型语言模型的偏好优化（生成任务），打破了以往零阶方法仅限于分类任务的局限性。它为解决LLMs微调的计算密集性问题提供了一个新颖的视角，尽管其性能尚未超越所有最先进的一阶方法，但其开辟了一个此前未被充分探索的研究方向，具有重要的奠基意义。"}}
{"id": "2507.17418", "title": "Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning", "authors": ["Joobin Jin", "Seokjun Hong", "Gyeongseon Baek", "Yeeun Kim", "Byeongjoon Noh"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17418v1", "summary": "Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17418v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Ctx2TrajGen：基于生成对抗模仿学习的交通上下文感知微观车辆轨迹生成", "tldr": "Ctx2TrajGen是一个利用GAIL生成真实城市车辆轨迹的框架，通过感知周围车辆和道路几何形状，解决了微观轨迹建模中的数据稀缺和域偏移问题，并在DRIFT数据集上表现出色。", "motivation": "精确建模微观车辆轨迹对于交通行为分析和自动驾驶系统至关重要。", "method": "本文提出了Ctx2TrajGen，一个上下文感知的轨迹生成框架，利用生成对抗模仿学习（GAIL）来合成真实的城市驾驶行为。该模型利用PPO和WGAN-GP来解决微观设置中固有的非线性相互依赖性和训练不稳定性。通过明确地以周围车辆和道路几何形状为条件，Ctx2TrajGen生成与现实世界上下文对齐的交互感知轨迹。", "result": "在无人机捕获的DRIFT数据集上进行的实验表明，Ctx2TrajGen在真实性、行为多样性和上下文保真度方面优于现有方法，为数据稀缺和域偏移问题提供了一个鲁棒的解决方案，且无需仿真。", "conclusion": "Ctx2TrajGen提供了一个强大的解决方案，通过生成上下文感知的真实微观车辆轨迹，克服了数据稀缺和域偏移的挑战，对交通行为分析和自动驾驶系统具有重要意义。", "translation": "精确建模微观车辆轨迹对于交通行为分析和自动驾驶系统至关重要。我们提出了Ctx2TrajGen，一个上下文感知的轨迹生成框架，它利用GAIL合成真实的城市驾驶行为。Ctx2TrajGen利用PPO和WGAN-GP，解决了微观环境中固有的非线性相互依赖性和训练不稳定性。通过明确地以周围车辆和道路几何形状为条件，Ctx2TrajGen生成与现实世界上下文对齐的交互感知轨迹。在无人机捕获的DRIFT数据集上进行的实验表明，Ctx2TrajGen在真实性、行为多样性和上下文保真度方面优于现有方法，为数据稀缺和域偏移问题提供了一个鲁棒的解决方案，且无需仿真。", "summary": "Ctx2TrajGen是一个基于GAIL的上下文感知轨迹生成框架，用于合成真实的城市微观车辆轨迹。它结合PPO和WGAN-GP，通过显式条件化周围车辆和道路几何形状，解决了轨迹建模的非线性依赖和训练不稳定性问题。在DRIFT数据集上的实验证明，该模型在真实性、多样性和上下文保真度上优于现有方法，有效缓解了数据稀缺和域偏移的挑战。", "keywords": "车辆轨迹生成, 生成对抗模仿学习, 上下文感知, 微观交通建模, 自动驾驶", "comments": "Ctx2TrajGen的创新之处在于其将生成对抗模仿学习与上下文感知相结合，通过显式地利用周围环境信息，显著提升了生成轨迹的真实性和多样性。该方法无需仿真即可解决数据稀缺和域偏移问题，这对于自动驾驶和交通研究领域具有重要意义。"}}
{"id": "2507.17383", "title": "Confidence Calibration in Vision-Language-Action Models", "authors": ["Thomas P Zollo", "Richard Zemel"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      34 pages, 19 figures", "url": "http://arxiv.org/abs/2507.17383v1", "summary": "Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.", "comment": "34 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2507.17383v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "视觉-语言-动作模型中的置信度校准", "tldr": "本文首次系统研究了视觉-语言-动作（VLA）基础模型中的置信度校准问题，提出并验证了多种方法以提高机器人行为的可靠性和可信度，包括提示集成和动作维度普拉特标定。", "motivation": "为了实现可信赖的机器人行为，机器人不仅需要具备高水平的任务成功率，还需要能够可靠地量化其成功的可能性。", "method": "本文首先对视觉-语言-动作（VLA）基础模型进行了广泛的基准测试，以理解任务成功与校准误差之间的关系。接着，引入了受贝叶斯启发、轻量级的VLA提示集成算法，该算法通过平均不同释义指令的置信度来提高校准。此外，还分析了任务时间范围内的校准情况。最后，提出了动作维度普拉特标定方法，独立地重新校准每个动作维度以获得更好的置信度估计。", "result": "研究发现任务性能和校准之间并不存在冲突。提示集成算法能够持续改进校准。置信度在任务进行到一定阶段后通常最为可靠，这为风险感知干预提供了自然的时间点。同时，揭示了不同动作维度之间的差异化误校准，并且动作维度普拉特标定方法能够产生更好的置信度估计。", "conclusion": "本研究旨在开发必要的工具和概念理解，通过可靠的不确定性量化，使视觉-语言-动作模型既能实现高性能又具有高度可信赖性。", "translation": "值得信赖的机器人行为不仅需要高水平的任务成功率，还需要机器人能够可靠地量化其成功的可能性。为此，我们首次系统研究了视觉-语言-动作（VLA）基础模型中的置信度校准问题，这类模型将视觉观察和自然语言指令映射到低级机器人运动指令。我们首先进行了广泛的基准测试，以理解跨多个数据集和VLA变体中任务成功与校准误差之间的关键关系，发现任务性能和校准之间并不矛盾。接下来，我们为VLA引入了提示集成方法，这是一种轻量级的、受贝叶斯启发的算法，它对不同释义指令的置信度进行平均，并持续改进校准。我们进一步分析了任务时间范围内的校准情况，表明置信度通常在取得一定进展后最为可靠，这为风险感知干预提供了自然的切入点。最后，我们揭示了动作维度上的差异化误校准，并提出了动作维度普拉特标定（action-wise Platt scaling）方法，该方法可以独立地重新校准每个动作维度，以产生更好的置信度估计。本研究旨在开始开发必要的工具和概念理解，通过可靠的不确定性量化，使VLA模型既能实现高性能又具有高度可信赖性。", "summary": "本文首次对视觉-语言-动作（VLA）基础模型中的置信度校准进行了系统研究，旨在提升机器人行为的可信赖性。研究发现任务性能与校准之间无冲突。为此，提出了基于贝叶斯思想的VLA提示集成算法，通过平均不同指令的置信度显著提升校准效果。同时，分析了任务执行过程中置信度的可靠性变化，并针对不同动作维度的误校准问题，引入了动作维度普拉特标定方法以优化置信度估计。这项工作致力于为构建高性能且高度可信赖的VLA模型奠定基础。", "keywords": "置信度校准, 视觉-语言-动作模型, 机器人, 提示集成, 不确定性量化", "comments": "本文首次系统性地探讨了VLA模型中的置信度校准问题，具有重要的创新性。其提出的提示集成和动作维度普拉特标定方法为提升机器人决策的透明度和可靠性提供了实用工具。研究发现任务性能与校准不矛盾，这为未来VLA模型的设计提供了重要指导。其对时间序列校准的分析也为风险管理提供了新的视角。"}}
{"id": "2507.17222", "title": "On the Construction of Barrier Certificate: A Dynamic Programming Perspective", "authors": ["Yu Chen", "Shaoyuan Li", "Xiang Yin"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17222v1", "summary": "In this paper, we revisit the formal verification problem for stochastic\ndynamical systems over finite horizon using barrier certificates. Most existing\nwork on this topic focuses on safety properties by constructing barrier\ncertificates based on the notion of $c$-martingales. In this work, we first\nprovide a new insight into the conditions of existing martingale-based barrier\ncertificates from the perspective of dynamic programming operators.\nSpecifically, we show that the existing conditions essentially provide a bound\non the dynamic programming solution, which exactly characterizes the safety\nprobability. Based on this new perspective, we demonstrate that the barrier\nconditions in existing approaches are unnecessarily conservative over unsafe\nstates. To address this, we propose a new set of safety barrier certificate\nconditions that are strictly less conservative than existing ones, thereby\nproviding tighter probability bounds for safety verification. We further extend\nour approach to the case of reach-avoid specifications by providing a set of\nnew barrier certificate conditions. We also illustrate how to search for these\nnew barrier certificates using sum-of-squares (SOS) programming. Finally, we\nuse two numerical examples to demonstrate the advantages of our method compared\nto existing approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17222v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "势函数证书的构建：一个动态规划的视角", "tldr": "本文从动态规划角度重新审视势函数证书，提出一套新的、更不保守的势函数条件，用于随机动力系统的形式化验证，并提供了更紧密的安全性概率界限。", "motivation": "现有的基于鞅的势函数证书在不安全状态上过于保守，导致安全性验证的概率界限不够紧密。", "method": "作者首先从动态规划算子的角度，提供了对现有鞅基势函数证书条件的新见解，指出它们本质上是对动态规划解的界限。在此基础上，提出了一套新的、严格不那么保守的安全势函数证书条件，并将其扩展到可达-规避规范。此外，还展示了如何使用平方和（SOS）规划来寻找这些新的势函数证书。", "result": "新提出的势函数证书条件比现有方法严格不那么保守，从而为安全验证提供了更紧密的概率界限。通过两个数值例子证明了新方法的优势。", "conclusion": "本文成功地从动态规划角度改进了势函数证书的构建，提出了更精确和不保守的条件，显著提升了随机动力系统形式化验证的效率和准确性。", "translation": "在本文中，我们利用势函数证书重新探讨了有限时间范围内随机动力系统的形式化验证问题。关于这个主题的大多数现有工作都集中在通过基于c-鞅概念构建势函数证书来处理安全属性。在这项工作中，我们首先从动态规划算子的角度，对现有基于鞅的势函数证书的条件提供了新的见解。具体来说，我们表明现有条件本质上提供了动态规划解的界限，该解精确地表征了安全概率。基于这一新视角，我们证明了现有方法中的势函数条件在不安全状态上是不必要的保守的。为了解决这个问题，我们提出了一套新的安全势函数证书条件，这些条件比现有条件严格不那么保守，从而为安全验证提供了更紧密的概率界限。我们进一步将我们的方法扩展到可达-规避规范的情况，提供了一套新的势函数证书条件。我们还说明了如何使用平方和（SOS）规划来寻找这些新的势函数证书。最后，我们使用两个数值例子来证明我们的方法与现有方法相比的优势。", "summary": "本文从动态规划的角度重新审视了随机动力系统形式化验证中的势函数证书。研究发现现有基于鞅的势函数条件在不安全状态上过于保守，因为它们是动态规划解的界限。为此，作者提出了一套新的、严格不那么保守的势函数条件，这些条件能提供更紧密的安全性概率界限，并将其扩展到可达-规避问题。论文还介绍了如何利用平方和规划来构造这些新的证书，并通过数值例子验证了其优越性。", "keywords": "势函数证书, 动态规划, 随机系统, 形式化验证, 保守性", "comments": "这项工作通过引入动态规划的视角，对现有势函数证书的保守性问题提供了深刻的理论解释，并提出了实用的改进方案。其创新之处在于揭示了现有条件与动态规划解的内在联系，并在此基础上设计出更不保守的条件，这对于随机系统安全验证的精度和效率具有重要意义。将SOS编程引入证书搜索也增强了方法的实用性。"}}
{"id": "2507.17009", "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17009v1", "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17009v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "医疗保健领域中生成式AI模型的多标签分类：自杀倾向和风险因素的案例研究", "tldr": "本研究探讨了使用生成式大型语言模型（GPT-3.5和GPT-4.5）对精神病学电子健康记录中的自杀相关因素进行多标签分类的可行性，并取得了良好性能。", "motivation": "自杀是一个严峻的全球健康危机，每年导致超过72万人死亡。早期识别自杀倾向（SI）、自杀未遂（SA）、自杀暴露（ES）和非自杀性自伤（NSSI）等自杀相关因素（SrFs）对于及时干预至关重要。尽管现有研究已将AI应用于临床笔记中的SrFs检测，但大多数将其视为二元分类任务，忽略了共存风险因素的复杂性。", "method": "本研究探索了生成式大型语言模型（LLMs），特别是GPT-3.5和GPT-4.5，在精神病学电子健康记录（EHRs）中对自杀相关因素（SrFs）进行多标签分类（MLC）的应用。我们提出了一个新颖的端到端生成式MLC流程，并引入了高级评估方法，包括标签集层面的指标和多标签混淆矩阵用于错误分析。", "result": "微调后的GPT-3.5取得了0.94的部分匹配准确率和0.91的F1分数，表现最佳。GPT-4.5在引导提示下，在包括稀有或少数标签集在内的所有标签集上都表现出卓越性能，表明其更平衡和稳健。研究结果揭示了系统性错误模式，例如自杀倾向（SI）和自杀未遂（SA）的混淆，并突出了模型倾向于谨慎过度标注。", "conclusion": "这项工作不仅证明了使用生成式AI处理复杂临床分类任务的可行性，而且为构建非结构化电子健康记录（EHR）数据以支持大规模临床研究和循证医学提供了蓝图。", "translation": "自杀仍然是一个紧迫的全球健康危机，每年有超过72万人死亡，另有数百万人受到自杀意念（SI）和自杀未遂（SA）的影响。早期识别自杀相关因素（SrFs），包括自杀意念（SI）、自杀未遂（SA）、自杀暴露（ES）和非自杀性自伤（NSSI），对于及时干预至关重要。虽然之前的研究已将人工智能应用于临床笔记中SrFs的检测，但大多数将自杀性视为二元分类任务，忽略了共存风险因素的复杂性。本研究探索了生成式大型语言模型（LLMs），特别是GPT-3.5和GPT-4.5，在精神病学电子健康记录（EHRs）中对SrFs进行多标签分类（MLC）的应用。我们提出了一个新颖的端到端生成式MLC流程，并引入了高级评估方法，包括标签集层面的指标和多标签混淆矩阵用于错误分析。微调后的GPT-3.5取得了0.94的部分匹配准确率和0.91的F1分数，表现最佳，而GPT-4.5在引导提示下在所有标签集上都表现出卓越性能，包括稀有或少数标签集，表明其更平衡和稳健的性能。我们的研究结果揭示了系统性错误模式，例如自杀意念（SI）和自杀未遂（SA）的混淆，并突出了模型倾向于谨慎过度标注。这项工作不仅证明了使用生成式AI处理复杂临床分类任务的可行性，而且为构建非结构化EHR数据以支持大规模临床研究和循证医学提供了蓝图。", "summary": "本研究旨在解决现有AI模型在自杀相关因素识别中将复杂共存风险视为二元分类的局限性。论文探索了使用生成式大型语言模型（LLMs），特别是GPT-3.5和GPT-4.5，对精神病学电子健康记录（EHRs）中的自杀相关因素（SrFs）进行多标签分类（MLC）。研究提出了一个新颖的端到端生成式MLC流程和先进的评估方法。结果显示，微调后的GPT-3.5和使用引导提示的GPT-4.5均表现出色，尤其GPT-4.5在处理稀有标签集时展现出更平衡和稳健的性能。研究还识别了系统性错误模式和模型倾向。这项工作不仅证明了生成式AI在复杂临床分类任务中的可行性，还为结构化非结构化EHR数据以支持大规模临床研究和循证医学提供了实践蓝图。", "keywords": "多标签分类, 生成式AI, 自杀倾向, 风险因素, 电子健康记录", "comments": "本研究的创新之处在于首次将生成式AI模型应用于医疗领域中复杂的多标签分类任务，特别是针对自杀相关因素的识别，这对于早期干预具有重要意义。它通过引入新颖的端到端生成式MLC流程和高级评估方法，超越了传统二元分类的局限性。该工作的重要性体现在为结构化非结构化电子健康记录数据提供了新的范式，有望推动大规模临床研究和循证医学。然而，研究也指出模型存在系统性错误模式（如SI和SA的混淆）和过度标注的倾向，这提示未来研究需进一步优化模型以提高精确度和减少偏差。"}}
{"id": "2507.17069", "title": "The Generalized Matrix Separation Problem: Algorithms", "authors": ["Xuemei Chen", "Owen Deen"], "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      24 pages", "url": "http://arxiv.org/abs/2507.17069v1", "summary": "When given a generalized matrix separation problem, which aims to recover a\nlow rank matrix $L_0$ and a sparse matrix $S_0$ from $M_0=L_0+HS_0$, the work\n\\cite{CW25} proposes a novel convex optimization problem whose objective\nfunction is the sum of the $\\ell_1$-norm and nuclear norm. In this paper we\ndetail the iterative algorithms and its associated computations for solving\nthis convex optimization problem. We present various efficient implementation\nstrategies, with attention to practical cases where $H$ is circulant,\nseparable, or block structured. Notably, we propose a preconditioning technique\nthat drastically improved the performance of our algorithms in terms of\nefficiency, accuracy, and robustness. While this paper serves as an\nillustrative algorithm implementation manual, we also provide theoretical\nguarantee for our preconditioning strategy. Numerical results illustrate the\neffectiveness of the proposed approach.", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2507.17069v1", "cate": "math.OC", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "广义矩阵分离问题：算法", "tldr": "本文详细阐述了解决广义矩阵分离问题的迭代算法，并提出了一种预处理技术，显著提升了算法的性能，并提供了理论保证。", "motivation": "针对广义矩阵分离问题，前人工作提出了一个凸优化问题。本文旨在详细阐述解决该凸优化问题的迭代算法和相关计算方法。", "method": "本文详细介绍了解决广义矩阵分离问题所提出的凸优化问题的迭代算法和相关计算方法。提出了多种高效的实现策略，并特别关注H为循环、可分离或块结构的情况。此外，还提出了一种预处理技术，显著提高了算法在效率、准确性和鲁棒性方面的性能。", "result": "提出的预处理技术显著提高了算法的效率、准确性和鲁棒性。数值结果表明所提出方法的有效性。此外，还提供了预处理策略的理论保证。", "conclusion": "本文作为算法实现手册，详细介绍了解决广义矩阵分离问题的迭代算法和实现策略，并为提出的预处理策略提供了理论保证。", "translation": "当给定一个广义矩阵分离问题时，其目标是从 $M_0=L_0+HS_0$ 中恢复一个低秩矩阵 $L_0$ 和一个稀疏矩阵 $S_0$，工作 \\cite{CW25} 提出了一种新颖的凸优化问题，其目标函数是 $\\ell_1$-范数和核范数之和。在本文中，我们详细介绍了解决此凸优化问题的迭代算法及其相关计算。我们提出了各种高效的实现策略，并关注 $H$ 是循环、可分离或块结构等实际情况。值得注意的是，我们提出了一种预处理技术，该技术在效率、准确性和鲁棒性方面极大地提高了我们算法的性能。虽然本文作为说明性的算法实现手册，但我们也为我们的预处理策略提供了理论保证。数值结果说明了所提出方法的有效性。", "summary": "本文详细阐述了解决广义矩阵分离问题（旨在从 $M_0=L_0+HS_0$ 中恢复低秩矩阵 $L_0$ 和稀疏矩阵 $S_0$）的凸优化问题的迭代算法。论文提出了多种高效实现策略，并引入了一种创新的预处理技术，该技术显著提升了算法的效率、准确性和鲁棒性。此外，本文还提供了该预处理策略的理论保证，并通过数值结果验证了其有效性。", "keywords": "广义矩阵分离, 迭代算法, 预处理, 凸优化, 低秩稀疏恢复", "comments": "本文的创新点在于提出了针对广义矩阵分离问题凸优化解法的详细迭代算法和高效实现策略，特别是引入了一种能够显著提升算法性能（效率、准确性、鲁棒性）的预处理技术，并提供了相应的理论保证，这对于实际应用具有重要意义。"}}
{"id": "2507.16881", "title": "Confidence Optimization for Probabilistic Encoding", "authors": ["Pengjiu Xia", "Yidian Huang", "Wenchao Wei", "Yuwen Tan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16881v1", "summary": "Probabilistic encoding introduces Gaussian noise into neural networks,\nenabling a smooth transition from deterministic to uncertain states and\nenhancing generalization ability. However, the randomness of Gaussian noise\ndistorts point-based distance measurements in classification tasks. To mitigate\nthis issue, we propose a confidence optimization probabilistic encoding (CPE)\nmethod that improves distance reliability and enhances representation learning.\nSpecifically, we refine probabilistic encoding with two key strategies: First,\nwe introduce a confidence-aware mechanism to adjust distance calculations,\nensuring consistency and reliability in probabilistic encoding classification\ntasks. Second, we replace the conventional KL divergence-based variance\nregularization, which relies on unreliable prior assumptions, with a simpler L2\nregularization term to directly constrain variance. The method we proposed is\nmodel-agnostic, and extensive experiments on natural language classification\ntasks demonstrate that our method significantly improves performance and\ngeneralization on both the BERT and the RoBERTa model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16881v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "概率编码的置信度优化", "tldr": "本文提出了一种置信度优化概率编码（CPE）方法，通过引入置信度感知机制和使用L2方差正则化来解决概率编码中高斯噪声对距离测量的扭曲问题，从而提高分类性能和泛化能力。", "motivation": "概率编码虽然通过引入高斯噪声增强了神经网络的泛化能力，但高斯噪声的随机性扭曲了分类任务中基于点的距离测量。", "method": "本文提出了一种置信度优化概率编码（CPE）方法。具体策略包括：1. 引入置信度感知机制来调整距离计算，以确保概率编码分类任务中的一致性和可靠性。2. 用更简单的L2正则化项替代传统的基于KL散度的方差正则化，直接约束方差，避免对不可靠先验假设的依赖。该方法是模型无关的。", "result": "在自然语言分类任务中，该方法在BERT和RoBERTa模型上显著提高了性能和泛化能力。", "conclusion": "通过引入置信度感知机制和使用L2方差正则化，本文提出的置信度优化概率编码（CPE）方法有效解决了概率编码中高斯噪声对距离测量的扭曲问题，从而提升了分类任务的性能和泛化能力。", "translation": "概率编码在神经网络中引入高斯噪声，实现了从确定性状态到不确定性状态的平滑过渡，并增强了泛化能力。然而，高斯噪声的随机性扭曲了分类任务中基于点的距离测量。为了缓解这个问题，我们提出了一种置信度优化概率编码（CPE）方法，该方法提高了距离可靠性并增强了表示学习。具体来说，我们通过两个关键策略改进了概率编码：首先，我们引入了一种置信度感知机制来调整距离计算，确保概率编码分类任务中的一致性和可靠性。其次，我们用更简单的L2正则化项取代了传统的基于KL散度的方差正则化（该正则化依赖于不可靠的先验假设），以直接约束方差。我们提出的方法是模型无关的，在自然语言分类任务上的广泛实验表明，我们的方法显著提高了BERT和RoBERTa模型的性能和泛化能力。", "summary": "本文提出了一种名为置信度优化概率编码（CPE）的新方法，旨在解决概率编码中高斯噪声导致的距离测量失真问题。CPE通过引入置信度感知机制来优化距离计算的可靠性，并用L2正则化取代传统的KL散度方差正则化以直接约束方差。该模型无关的方法在自然语言分类任务上，显著提升了BERT和RoBERTa模型的性能和泛化能力。", "keywords": "概率编码, 置信度优化, 高斯噪声, 泛化能力, L2正则化", "comments": "该论文的创新点在于提出了置信度优化概率编码（CPE），通过引入置信度感知机制和L2方差正则化，有效解决了概率编码中高斯噪声对距离测量的负面影响。其模型无关的特性和在BERT、RoBERTa模型上的显著性能提升显示了该方法的普适性和有效性，对提高深度学习模型在不确定性条件下的分类性能和泛化能力具有重要意义。"}}
{"id": "2507.16838", "title": "Segmentation-free Goodness of Pronunciation", "authors": ["Xinwei Cao", "Zijian Fan", "Torbjørn Svendsen", "Giampiero Salvi"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.16838v1", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.16838v1", "cate": "eess.AS", "date": "2025-07-18", "updated": "2025-07-18", "AI": {"title_translation": "无需分割的发音质量评估", "tldr": "提出无需分割的发音质量评估（GOP）方法，解决了传统GOP对预分割的依赖，并实现了音素级发音评估的最新SOTA性能。", "motivation": "现有的发音错误检测与诊断（MDD）系统，特别是音素级发音评估，大多基于需要语音预分割的GOP（Goodness of Pronunciation）方法。这种预分割限制了评估的准确性，并阻碍了使用现代基于CTC的声学模型。因此，需要开发一种无需预分割的GOP方法来克服这些限制。", "method": "本研究首先提出了自对齐GOP（GOP-SA），它允许使用CTC训练的ASR模型进行发音错误检测与诊断（MDD）。接着，定义了一种更通用的无对齐方法（GOP-AF），该方法考虑了目标音素所有可能的对齐。论文提供了GOP-AF定义的理论描述、解决潜在数值问题的实现以及适当的归一化处理，使其能适用于不同时间峰度的声学模型。", "result": "在CMU Kids和Speechocean762数据集上进行了广泛的实验，比较了不同方法的定义，并评估了GOP-AF对声学模型峰度和目标音素周围上下文的依赖性。结果表明，从所提出的方法导出的特征向量在音素级发音评估中取得了最先进（state-of-the-art）的结果。", "conclusion": "本文提出的无需分割的GOP方法（GOP-SA和GOP-AF）有效解决了传统GOP方法对语音预分割的依赖性，并成功地将现代CTC训练的声学模型应用于发音评估。实验证明，这些方法在音素级发音评估中达到了最先进的性能，显著提升了计算机辅助语言学习系统的实用性。", "translation": "发音错误检测与诊断（MDD）是现代计算机辅助语言学习（CALL）系统的重要组成部分。在MDD中，音素级发音评估是帮助二语学习者提高发音的关键。然而，大多数系统都基于某种形式的发音质量评估（GOP），这需要将语音预分割成语音单元。这限制了这些方法的准确性以及使用基于CTC的现代声学模型进行评估的可能性。在这项研究中，我们首先提出了自对齐GOP（GOP-SA），它使得CTC训练的ASR模型能够用于MDD。接下来，我们定义了一种更通用的无对齐方法，该方法考虑了目标音素所有可能的对齐（GOP-AF）。我们提供了GOP-AF定义的理论解释，解决了潜在数值问题的实现，以及适当的归一化，这使得该方法适用于不同时间峰度的声学模型。我们在CMU Kids和Speechocean762数据集上提供了广泛的实验结果，比较了我们方法的不同定义，评估了GOP-AF对声学模型峰度和目标音素周围上下文的依赖性。最后，我们将我们的方法与Speechocean762数据上的最新研究进行了比较，结果表明，从所提出的方法导出的特征向量在音素级发音评估中取得了最先进的结果。", "summary": "本文提出两种无需预分割的发音质量评估（GOP）方法：自对齐GOP（GOP-SA）和更通用的无对齐GOP（GOP-AF）。传统GOP依赖语音预分割，限制了准确性并阻碍了CTC模型应用。GOP-SA允许使用CTC训练的ASR模型，而GOP-AF则考虑所有可能的音素对齐，并解决了数值和归一化问题。实验结果表明，这些方法在音素级发音评估上取得了最先进的性能。", "keywords": "发音质量评估, 无需分割, CTC, 音素级评估, 计算机辅助语言学习", "comments": "这项研究的创新之处在于提出了无需预分割的GOP方法，这解决了传统GOP方法的关键局限性。通过引入GOP-SA和GOP-AF，该工作不仅使现代CTC训练的声学模型能够应用于发音评估，还显著提升了音素级发音评估的准确性，达到了当前SOTA水平。其贡献在于提高了计算机辅助语言学习系统的实用性和性能。"}}
{"id": "2507.17440", "title": "Parametric Integration with Neural Integral Operators", "authors": ["Christoph Schied", "Alexander Keller"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17440v1", "summary": "Real-time rendering imposes strict limitations on the sampling budget for\nlight transport simulation, often resulting in noisy images. However, denoisers\nhave demonstrated that it is possible to produce noise-free images through\nfiltering. We enhance image quality by removing noise before material shading,\nrather than filtering already shaded noisy images. This approach allows for\nmaterial-agnostic denoising (MAD) and leverages machine learning by\napproximating the light transport integral operator with a neural network,\neffectively performing parametric integration with neural operators. Our method\noperates in real-time, requires data from only a single frame, seamlessly\nintegrates with existing denoisers and temporal anti-aliasing techniques, and\nis efficient to train. Additionally, it is straightforward to incorporate with\nphysically based rendering algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17440v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "神经积分算子参数化积分", "tldr": "一种新的实时去噪方法，通过在材质着色前使用神经网络进行光传输积分来去除噪声，实现与材质无关的去噪。", "motivation": "实时渲染中采样预算限制导致图像噪声，现有去噪器在材质着色后进行过滤，限制了去噪效果。", "method": "在材质着色前去除噪声，通过使用神经网络近似光传输积分算子，实现材质无关的去噪（MAD）和参数化积分。", "result": "该方法实时运行，仅需单帧数据，可与现有去噪器和时间抗锯齿技术无缝集成，训练高效，且易于与基于物理的渲染算法结合。", "conclusion": "通过在材质着色前利用神经网络进行光传输积分，可以有效去除噪声，提高图像质量，并实现实时、高效且兼容性强的去噪。", "translation": "实时渲染对光传输模拟的采样预算有严格限制，这通常会导致图像出现噪声。然而，去噪器已经证明可以通过滤波生成无噪声图像。我们通过在材质着色之前去除噪声，而不是在已经着色的噪声图像上进行滤波来提高图像质量。这种方法实现了与材质无关的去噪（MAD），并通过使用神经网络近似光传输积分算子来利用机器学习，有效地执行了神经算子的参数化积分。我们的方法实时运行，仅需要单帧数据，可与现有去噪器和时间抗锯齿技术无缝集成，并且训练高效。此外，它很容易与基于物理的渲染算法结合使用。", "summary": "这篇论文提出了一种新的实时去噪方法，通过在材质着色之前而非之后去除噪声，从而提高图像质量。该方法通过使用神经网络近似光传输积分算子，实现了材质无关的去噪（MAD）和参数化积分。该方法实时、高效，仅需单帧数据，并能与现有渲染管线无缝集成。", "keywords": "实时渲染, 去噪, 神经网络, 光传输, 参数化积分", "comments": "这项工作的创新之处在于将去噪步骤提前到材质着色之前，通过引入“材质无关的去噪”概念，并利用神经网络近似光传输积分算子，提供了一种新颖的实时图像去噪解决方案。其优点在于提高了去噪效率和兼容性，特别是在实时渲染场景中具有重要意义。"}}
{"id": "2507.17261", "title": "Joint Resource Optimization Over Licensed and Unlicensed Spectrum in Spectrum Sharing UAV Networks Against Jamming Attacks", "authors": ["Rui Ding", "Fuhui Zhou", "Yuhang Wu", "Qihui Wu", "Tony Q. S. Quek"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17261v1", "summary": "Unmanned aerial vehicle (UAV) communication is of crucial importance in\nrealizing heterogeneous practical wireless application scenarios. However, the\ndensely populated users and diverse services with high data rate demands has\ntriggered an increasing scarcity of UAV spectrum utilization. To tackle this\nproblem, it is promising to incorporate the underutilized unlicensed spectrum\nwith the licensed spectrum to boost network capacity. However, the openness of\nunlicensed spectrum makes UAVs susceptible to security threats from potential\njammers. Therefore, a spectrum sharing UAV network coexisting with licensed\ncellular network and unlicensed Wi-Fi network is considered with the\nanti-jamming technique in this paper. The sum rate maximization of the\nsecondary network is studied by jointly optimizing the transmit power,\nsubchannel allocation, and UAV trajectory. We first decompose the challenging\nnon-convex problem into two subproblems, 1) the joint power and subchannel\nallocation and 2) UAV trajectory design subproblems. A low-complexity iterative\nalgorithm is proposed in a alternating optimization manner over these two\nsubproblems to solve the formulated problem. Specifically, the Lagrange dual\ndecomposition is exploited to jointly optimize the transmit power and\nsubchannel allocation iteratively. Then, an efficient iterative algorithm\ncapitalizing on successive convex approximation is designed to get a suboptimal\nsolution for UAV trajectory. Simulation results demonstrate that our proposed\nalgorithm can significantly improve the sum transmission rate compared with the\nbenchmark schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17261v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "联合资源优化：许可与非许可频谱在频谱共享无人机网络中对抗干扰攻击", "tldr": "针对频谱共享无人机网络中许可与非许可频谱的资源优化问题，本文提出了一种联合优化发射功率、子信道分配和无人机轨迹的算法，以在存在干扰攻击的情况下最大化总速率。", "motivation": "无人机通信面临频谱稀缺问题，结合未充分利用的非许可频谱与许可频谱可提升网络容量。然而，非许可频谱的开放性使得无人机易受干扰攻击。", "method": "本文考虑了一种抗干扰的频谱共享无人机网络，并研究了次级网络总速率最大化问题。通过联合优化发射功率、子信道分配和无人机轨迹来解决此非凸问题。该问题被分解为两个子问题：联合功率与子信道分配和无人机轨迹设计。提出了一种低复杂度的交替优化迭代算法，利用拉格朗日对偶分解优化功率和子信道分配，并利用连续凸近似设计无人机轨迹的次优解。", "result": "仿真结果表明，所提出的算法能显著提高总传输速率，优于基准方案。", "conclusion": "本文提出的联合资源优化算法能够有效提升频谱共享无人机网络在存在干扰攻击时的传输性能。", "translation": "无人机（UAV）通信在实现异构实际无线应用场景中至关重要。然而，密集的用户和高数据速率需求的各种服务引发了无人机频谱利用日益稀缺的问题。为了解决这个问题，将未充分利用的非许可频谱与许可频谱结合起来以提高网络容量是很有前景的。然而，非许可频谱的开放性使得无人机容易受到来自潜在干扰源的安全威胁。因此，本文考虑了一种与许可蜂窝网络和非许可Wi-Fi网络共存的频谱共享无人机网络，并采用了抗干扰技术。通过联合优化发射功率、子信道分配和无人机轨迹，研究了次级网络的总速率最大化问题。我们首先将具有挑战性的非凸问题分解为两个子问题：1）联合功率和子信道分配子问题和2）无人机轨迹设计子问题。提出了一种低复杂度的迭代算法，以交替优化的方式解决这两个子问题，从而解决所提出的问题。具体而言，利用拉格朗日对偶分解迭代地联合优化发射功率和子信道分配。然后，设计了一种利用连续凸近似的高效迭代算法，以获得无人机轨迹的次优解。仿真结果表明，我们提出的算法与基准方案相比，可以显著提高总传输速率。", "summary": "本文针对频谱共享无人机网络在存在干扰攻击下的资源优化问题，提出了一种联合优化许可与非许可频谱使用的策略。通过联合优化发射功率、子信道分配和无人机轨迹，旨在最大化次级网络总速率。研究将复杂的非凸问题分解为两个子问题，并采用交替优化迭代算法，结合拉格朗日对偶分解和连续凸近似技术进行求解。仿真结果验证了所提算法能够显著提升网络总传输速率，有效应对频谱稀缺和干扰威胁。", "keywords": "无人机通信, 频谱共享, 抗干扰, 资源优化, 联合优化", "comments": "该研究创新性地将许可与非许可频谱结合应用于无人机网络，并同时考虑了抗干扰问题，这在实际部署中具有重要意义。提出的问题分解和交替优化算法有效降低了问题复杂度，使其更具可行性。"}}
{"id": "2507.17128", "title": "Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy", "authors": ["Minxian Xu", "Linfeng Wen", "Junhan Liao", "Huaming Wu", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.17128v1", "summary": "The interactions within cloud-native applications are complex, with a\nconstantly changing number of services and loads, posing higher demands on\nauto-scaling approach. This mainly involves several challenges such as\nmicroservices dependency analysis, performance profiling, anomaly detection,\nworkload characterization and task co-location. Therefore, some advanced\nalgorithms have been investigated into auto-scaling cloud-native applications\nto optimize system and application performance. These algorithms can learn from\nhistorical data and appropriately adjust resource allocation based on the\ncurrent environment and load conditions to optimize resource utilization and\nsystem performance. In this paper, we systematically review the literature on\nstate-of-the-art auto-scaling approaches for cloud-native applications from\n2020, and further explore the technological evolution. Additionally, we propose\na detailed taxonomy to categorize current research from five perspectives,\nincluding infrastructure, architecture, scaling methods, optimization\nobjectives, and behavior modeling. Then, we provide a comprehensive comparison\nand in-depth discussion of the key features, advantages, limitations, and\napplication scenarios of each approach, considering their performance in\ndiverse environments and under various conditions. Finally, we summarize the\ncurrent state of research in this field, identify the gaps and unresolved\nchallenges, and emphasize promising directions for future exploration,\nparticularly in areas such as the application of large models, microservice\ndependency management, and the use of meta-learning techniques to enhance model\napplicability and adaptability across different environments.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.17128v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "云原生应用的自动扩缩容方法：一项调查与分类", "tldr": "本文对2020年以来云原生应用的自动扩缩容方法进行了调查，提出了一个分类法，比较了现有方法，并指出了未来的研究方向。", "motivation": "云原生应用的复杂交互、服务数量和负载的不断变化，以及微服务依赖分析、性能分析、异常检测等挑战，对自动扩缩容提出了更高要求，需要先进算法来优化资源利用和系统性能。", "method": "本文系统回顾了自2020年以来云原生应用最先进的自动扩缩容方法文献，提出了一个详细的分类法，从基础设施、架构、扩缩容方法、优化目标和行为建模五个角度对当前研究进行分类。同时，对每种方法的关键特性、优势、局限性和应用场景进行了全面比较和深入讨论。", "result": "本文提供了一个系统的自动扩缩容方法综述，一个详细的分类法，以及对现有方法在不同环境下的性能的全面比较。", "conclusion": "论文总结了该领域的研究现状，指出了存在的空白和未解决的挑战，并强调了未来有前景的探索方向，特别是在大型模型应用、微服务依赖管理以及使用元学习技术增强模型适用性和适应性等方面。", "translation": "云原生应用内部的交互复杂，服务数量和负载不断变化，对自动扩缩容方法提出了更高的要求。这主要涉及微服务依赖分析、性能分析、异常检测、工作负载特性化和任务协同部署等多个挑战。因此，一些先进的算法已被研究用于云原生应用的自动扩缩容，以优化系统和应用性能。这些算法可以从历史数据中学习，并根据当前环境和负载条件适当调整资源分配，以优化资源利用率和系统性能。在本文中，我们系统地回顾了自2020年以来关于云原生应用最先进自动扩缩容方法的文献，并进一步探讨了技术演进。此外，我们提出了一个详细的分类法，从基础设施、架构、扩缩容方法、优化目标和行为建模五个角度对当前研究进行分类。然后，我们对每种方法的关键特性、优势、局限性和应用场景进行了全面比较和深入讨论，同时考虑了它们在不同环境和各种条件下的性能。最后，我们总结了该领域的研究现状，指出了存在的空白和未解决的挑战，并强调了未来有前景的探索方向，特别是在大型模型应用、微服务依赖管理以及使用元学习技术增强模型在不同环境下的适用性和适应性等方面。", "summary": "本文系统综述了自2020年以来云原生应用的自动扩缩容方法，以应对复杂云原生环境带来的挑战。论文提出了一个基于基础设施、架构、扩缩容方法、优化目标和行为建模的详细分类法，并全面比较了现有方法的特点、优缺点和应用场景。最后，文章总结了研究现状，指出了研究空白，并展望了未来方向，包括大型模型应用、微服务依赖管理和元学习技术。", "keywords": "自动扩缩容, 云原生应用, 调查, 分类, 微服务", "comments": "本文为云原生应用自动扩缩容领域提供了及时且结构化的概述。提出的分类法为理解和归类现有研究提供了一个有价值的框架。其对未来方向的识别，特别是关于大型模型和元学习的应用，指出了关键的创新领域。"}}
{"id": "2507.17230", "title": "Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series", "authors": ["Clara Scalzer", "Saurav Pokhrel", "Sara Hunt", "Greg L Nelson"], "categories": ["cs.HC", "K.3"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17230v1", "summary": "Students continue their education when they feel their learning is meaningful\nand relevant for their future careers. Computing educators now face the\nchallenge of preparing students for careers increasingly shaped by generative\nAI (GenAI) with the goals of supporting their learning, motivation, ethics, and\ncareer development. Our longitudinal qualitative study of students in a\nGenAI-integrated creative media course shows how this is a \"wicked\" problem:\nprogress on one goal can then impede progress on other goals. Students\ndeveloped concerning patterns despite extensive instruction in critical and\nethical GenAI use including prompt engineering, ethics and bias, and industry\npanels on GenAI's career impact. We present an analysis of two students'\nexperiences to showcase this complexity. Increasing GenAI use skills can lower\nethics; for example, Pat started from purposefully avoiding GenAI use, to\ndependency. He described himself as a \"notorious cheater\" who now uses GenAi to\n\"get all the right answers\" while acknowledging he's learning less. Increasing\nethical awareness can lower the learning of GenAI use skills; for example,\nJay's newfound environmental concerns led to self-imposed usage limits that\nimpeded skill development, and new serious fears that GenAI would eliminate\ncreative careers they had been passionate about. Increased GenAI proficiency, a\npotential career skill, did not improve their career confidence. These findings\nsuggest that supporting student development in the GenAI era is a \"wicked\"\nproblem requiring multi-dimensional evaluation and design, rather than\noptimizing learning, GenAI skills, ethics, or career motivation individually.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17230v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "生成式人工智能学习设计是一个棘手问题：一项说明性纵向定性案例系列研究", "tldr": "在生成式AI时代，为学生设计学习是一个“棘手”的问题，因为在支持学习、动机、伦理和职业发展等目标上，一个目标的进步可能会阻碍另一个目标的实现。", "motivation": "计算教育者面临的挑战是，在生成式AI日益塑造职业的背景下，如何培养学生以支持他们的学习、动机、伦理和职业发展，并使他们的学习对未来职业有意义和相关。", "method": "本研究采用纵向定性研究方法，对一门整合了生成式AI的创意媒体课程中的学生进行了研究，并通过分析两名学生的经验来展示问题的复杂性。", "result": "研究发现，提高生成式AI使用技能可能会降低伦理意识（如学生Pat从避免使用到依赖AI作弊）；而提高伦理意识可能会阻碍生成式AI技能的学习（如学生Jay因环境担忧限制使用导致技能发展受阻，并对AI消除创意职业感到恐惧）。此外，生成式AI熟练度并未提高学生的职业信心。", "conclusion": "支持学生在生成式AI时代的发展是一个“棘手”的问题，需要多维度的评估和设计，而不是单独优化学习、AI技能、伦理或职业动机。", "translation": "当学生觉得他们的学习对未来的职业有意义和相关时，他们会继续接受教育。计算教育者现在面临的挑战是，如何培养学生以适应生成式人工智能（GenAI）日益塑造的职业，目标是支持他们的学习、动机、伦理和职业发展。我们对一门整合了GenAI的创意媒体课程中的学生进行的纵向定性研究表明，这是一个“棘手”的问题：一个目标的进步可能会阻碍其他目标的进步。尽管在批判性和伦理GenAI使用方面进行了广泛的指导，包括提示工程、伦理和偏见，以及关于GenAI职业影响的行业专题讨论，学生们仍然形成了令人担忧的模式。我们对两名学生的经验进行了分析，以展示这种复杂性。提高GenAI使用技能可能会降低伦理意识；例如，Pat从刻意避免使用GenAI，到变得依赖。他形容自己是一个“臭名昭著的作弊者”，现在使用GenAI来“获得所有正确答案”，同时承认自己学到的东西更少。提高伦理意识可能会降低GenAI使用技能的学习；例如，Jay新发现的环境担忧导致了自我设定的使用限制，阻碍了技能发展，并产生了GenAI会消除他们曾经热爱的创意职业的严重恐惧。GenAI熟练度的提高，作为一种潜在的职业技能，并未提高他们的职业信心。这些发现表明，在GenAI时代支持学生发展是一个“棘手”的问题，需要多维度的评估和设计，而不是单独优化学习、GenAI技能、伦理或职业动机。", "summary": "本研究通过一项纵向定性案例系列研究，探讨了在生成式AI时代为学生设计学习的挑战。研究发现，将生成式AI融入教育是一个“棘手”的问题，因为在支持学生学习、动机、伦理和职业发展等目标之间存在冲突。例如，提高AI技能可能导致伦理问题，而强调伦理则可能阻碍技能发展。研究呼吁采取多维度评估和设计方法，而非孤立地优化各个学习目标。", "keywords": "生成式AI, 学习设计, 棘手问题, 教育伦理, 职业发展", "comments": "这项研究创新性地将“棘手问题”框架应用于生成式AI时代的教育设计，揭示了在培养学生AI技能、伦理意识和职业发展之间存在的复杂权衡。其纵向定性案例研究方法提供了深入的洞察，强调了教育者在整合AI时面临的挑战，并建议采取整体而非单一优化的策略。这对于当前AI教育的实践和理论发展具有重要指导意义。"}}
{"id": "2507.17123", "title": "Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems", "authors": ["Jacob M. Delgado-López", "Ricardo A. Morell-Rodriguez", "Sebastián O. Espinosa-Del Rosario", "Wilfredo E. Lugo-Beauchamp"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17123v1", "summary": "The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for\neffective containment and treatment, particularly in resource-constrained\nenvironments. This study presents an AI-driven diagnostic tool developed for\ndeployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained\nMobileNetV2 architecture for binary classification. The model was trained on\nthe open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,\nwhich reflects a well-balanced performance in precision and recall. To optimize\nthe model, the TensorRT framework was used to accelerate inference for FP32 and\nto perform post-training quantization for FP16 and INT8 formats. TensorRT's\nmixed-precision capabilities enabled these optimizations, which reduced the\nmodel size, increased inference speed, and lowered power consumption by\napproximately a factor of two, all while maintaining the original accuracy.\nPower consumption analysis confirmed that the optimized models used\nsignificantly less energy during inference, reinforcing their suitability for\ndeployment in resource-constrained environments. The system was deployed with a\nWi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to\nupload and analyze images directly through connected devices such as mobile\nphones. This setup ensures simple access and seamless connectivity, making the\ntool practical for real-world applications. These advancements position the\ndiagnostic tool as an efficient, scalable, and energy-conscious solution to\naddress diagnosis challenges in underserved regions, paving the way for broader\nadoption in low-resource healthcare settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17123v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "嵌入式系统上用于实时猴痘诊断的计算机视觉", "tldr": "本研究开发了一种基于AI的猴痘诊断工具，可在NVIDIA Jetson Orin Nano上部署，通过TensorRT优化实现了高效、低功耗的实时诊断，尤其适用于资源受限环境。", "motivation": "快速诊断猴痘等传染病对于有效遏制和治疗至关重要，尤其是在资源受限的环境中。", "method": "该研究开发了一种基于AI的诊断工具，部署在NVIDIA Jetson Orin Nano上，利用预训练的MobileNetV2架构进行二分类。模型在开源猴痘皮肤病变数据集上进行训练。为优化模型，使用TensorRT框架加速FP32推理，并对FP16和INT8格式进行训练后量化。系统通过Wi-Fi热点和基于网络的界面部署，支持通过移动设备上传和分析图像。", "result": "模型在F1-Score上达到93.07%，表现出良好的精确度和召回率平衡。TensorRT优化显著减小了模型大小，提高了推理速度，并将功耗降低了大约一半，同时保持了原始精度。功耗分析证实优化后的模型在推理过程中能耗显著降低，适合资源受限环境。", "conclusion": "该诊断工具是一种高效、可扩展、节能的解决方案，旨在解决服务不足地区的诊断挑战，为在低资源医疗环境中更广泛的应用铺平道路。", "translation": "传染病的快速诊断，如猴痘，对于有效遏制和治疗至关重要，尤其是在资源受限的环境中。本研究提出了一种为部署在NVIDIA Jetson Orin Nano上而开发的AI驱动诊断工具，该工具利用预训练的MobileNetV2架构进行二分类。该模型在开源的猴痘皮肤病变数据集上进行了训练，F1-Score达到了93.07%，这反映了在精确度和召回率方面表现良好。为了优化模型，使用TensorRT框架加速FP32推理，并对FP16和INT8格式进行训练后量化。TensorRT的混合精度功能实现了这些优化，从而减小了模型大小，提高了推理速度，并将功耗降低了大约一半，同时保持了原始精度。功耗分析证实，优化后的模型在推理过程中能耗显著降低，进一步证实了它们适用于资源受限环境。该系统部署了Wi-Fi接入点（AP）热点和基于网络的界面，使用户能够通过手机等连接设备直接上传和分析图像。这种设置确保了简单的访问和无缝连接，使该工具在实际应用中具有实用性。这些进展使该诊断工具成为一种高效、可扩展且节能的解决方案，以应对服务不足地区的诊断挑战，为在低资源医疗环境中更广泛地采用铺平了道路。", "summary": "本研究开发了一种基于AI的猴痘诊断工具，旨在解决资源受限环境下的快速诊断需求。该工具部署在NVIDIA Jetson Orin Nano上，采用MobileNetV2模型，并在猴痘皮肤病变数据集上取得了93.07%的F1-Score。通过TensorRT框架进行优化（包括量化和混合精度），模型大小、推理速度和功耗均得到显著改善，同时保持了高精度。系统支持Wi-Fi和网页界面，方便移动设备访问。这使得该工具成为一种高效、节能且易于部署的解决方案，适用于低资源医疗场景。", "keywords": "猴痘, 计算机视觉, 嵌入式系统, 实时诊断, AI", "comments": "这项研究的创新之处在于成功地将AI驱动的猴痘诊断模型部署到嵌入式系统上，并进行了深度优化以适应资源受限环境。通过TensorRT进行模型量化和加速，显著降低了功耗并提高了推理速度，同时保持了高精度，这对于实际应用具有重要意义。该系统通过Wi-Fi和网页界面提供便捷访问，进一步提升了其在低资源医疗环境中的实用性和可扩展性。"}}
{"id": "2507.16955", "title": "A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion", "authors": ["Yalda Zafari", "Roaa Elalfy", "Mohamed Mabrok", "Somaya Al-Maadeed", "Tamer Khattab", "Essam A. Rashed"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16955v1", "summary": "Early and accurate interpretation of screening mammograms is essential for\neffective breast cancer detection, yet it remains a complex challenge due to\nsubtle imaging findings and diagnostic ambiguity. Many existing AI approaches\nfall short by focusing on single view inputs or single-task outputs, limiting\ntheir clinical utility. To address these limitations, we propose a novel\nmulti-view, multitask hybrid deep learning framework that processes all four\nstandard mammography views and jointly predicts diagnostic labels and BI-RADS\nscores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,\ncombining convolutional encoders for rich local feature extraction with Visual\nState Space Models (VSSMs) to capture global contextual dependencies. To\nimprove robustness and interpretability, we incorporate a gated attention-based\nfusion module that dynamically weights information across views, effectively\nhandling cases with missing data. We conduct extensive experiments across\ndiagnostic tasks of varying complexity, benchmarking our proposed hybrid models\nagainst baseline CNN architectures and VSSM models in both single task and\nmulti task learning settings. Across all tasks, the hybrid models consistently\noutperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,\nthe shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.\nFor the more challenging ternary classification, it attains an F1 score of\n0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.\nThese results highlight the effectiveness of the proposed hybrid framework and\nunderscore both the potential and limitations of multitask learning for\nimproving diagnostic performance and enabling clinically meaningful mammography\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16955v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "用于多视图、多任务乳腺X线摄影分析的混合CNN-VSSM模型：基于注意力的融合实现稳健诊断", "tldr": "本文提出了一种混合CNN-VSSM深度学习框架，用于多视图、多任务乳腺X线摄影分析，通过注意力机制融合信息，在诊断性能上优于基线模型。", "motivation": "早期准确解读筛查乳腺X线照片对于有效的乳腺癌检测至关重要，但由于细微的影像学发现和诊断模糊性，这仍然是一个复杂的挑战。现有许多AI方法侧重于单视图输入或单任务输出，限制了其临床实用性。", "method": "提出了一种新颖的多视图、多任务混合深度学习框架，处理所有四种标准乳腺X线摄影视图，并联合预测每侧乳房的诊断标签和BI-RADS评分。该架构集成了混合CNN-VSSM骨干网络，结合了用于丰富局部特征提取的卷积编码器和用于捕获全局上下文依赖关系的视觉状态空间模型（VSSM）。为了提高鲁棒性和可解释性，引入了一个门控注意力融合模块，动态加权跨视图信息，有效处理缺失数据的情况。", "result": "在所有任务中，混合模型始终优于基线模型。在二元BI-RADS 1 vs. 5分类任务中，共享混合模型实现了0.9967的AUC和0.9830的F1分数。对于更具挑战性的三元分类，F1分数达到0.7790，而在五类BI-RADS任务中，最佳F1分数达到0.4904。", "conclusion": "这些结果突出了所提出的混合框架的有效性，并强调了多任务学习在提高诊断性能和实现临床有意义的乳腺X线摄影分析方面的潜力和局限性。", "translation": "早期准确解读筛查乳腺X线照片对于有效的乳腺癌检测至关重要，但由于细微的影像学发现和诊断模糊性，这仍然是一个复杂的挑战。现有许多AI方法侧重于单视图输入或单任务输出，限制了其临床实用性。为了解决这些限制，我们提出了一种新颖的多视图、多任务混合深度学习框架，该框架处理所有四种标准乳腺X线摄影视图，并联合预测每侧乳房的诊断标签和BI-RADS评分。我们的架构集成了混合CNN-VSSM骨干网络，结合了用于丰富局部特征提取的卷积编码器和用于捕获全局上下文依赖关系的视觉状态空间模型（VSSM）。为了提高鲁棒性和可解释性，我们引入了一个门控注意力融合模块，该模块动态加权跨视图信息，有效处理缺失数据的情况。我们针对不同复杂度的诊断任务进行了广泛的实验，在单任务和多任务学习设置中，将我们提出的混合模型与基线CNN架构和VSSM模型进行了基准测试。在所有任务中，混合模型始终优于基线模型。在二元BI-RADS 1 vs. 5分类任务中，共享混合模型实现了0.9967的AUC和0.9830的F1分数。对于更具挑战性的三元分类，它获得了0.7790的F1分数，而在五类BI-RADS任务中，最佳F1分数达到0.4904。这些结果突出了所提出的混合框架的有效性，并强调了多任务学习在提高诊断性能和实现临床有意义的乳腺X线摄影分析方面的潜力和局限性。", "summary": "本文提出了一种新颖的混合CNN-VSSM深度学习框架，用于多视图、多任务乳腺X线摄影分析，以提高乳腺癌检测的准确性和实用性。该模型结合了CNN的局部特征提取能力和VSSM的全局上下文捕获能力，并通过注意力机制融合多视图信息，有效处理缺失数据。实验结果表明，该混合模型在多种诊断任务上均优于基线模型，尤其在二元BI-RADS分类中表现出色，验证了其在乳腺X线摄影分析中的潜力。", "keywords": "乳腺X线摄影, 混合模型, CNN, VSSM, 多任务学习, 注意力机制", "comments": "该论文的创新点在于提出了一个结合CNN和VSSM的混合架构，并引入了注意力机制进行多视图融合，有效解决了现有AI方法在乳腺X线摄影分析中单视图/单任务的局限性。其重要性在于提升了乳腺癌早期检测的准确性和鲁棒性，特别是处理多视图和缺失数据的能力。局限性在于，尽管在二元分类中表现优异，但在更精细的五类BI-RADS分类任务中F1分数仍有提升空间，表明多任务学习在该场景下仍有其固有的挑战。"}}
{"id": "2507.17747", "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "authors": ["Linbo Cao", "Jinman Zhao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 pages, 7 figures. Accepted to COLM 2025. Code available at: this http URL", "url": "http://arxiv.org/abs/2507.17747v1", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "pdf_url": "http://arxiv.org/pdf/2507.17747v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "预训练测试集不再是万能：一种辩论驱动的问答基准方法", "tldr": "提出了一种辩论驱动的评估范式，将现有问答数据集转化为对抗性辩论，以更可持续地衡量语言模型的真实推理能力，有效对抗数据污染和浅层记忆。", "motivation": "当前语言模型在标准问答基准上表现饱和，存在数据污染、记忆化以及数据集创建成本不断上升的问题。", "method": "提出了一种辩论驱动的评估范式，将现有问答数据集转换为结构化对抗性辩论。其中一个模型捍卫官方答案，另一个模型构建并捍卫替代答案，由一个对正确答案不知情的评判模型进行裁决。通过多轮论证，该方法显著增加了难度并惩罚了浅层记忆，同时通过重用问答项减少了策展开销。主要贡献包括：1) 一个将问答任务转换为辩论评估的评估流程；2) 一个在MMLU-Pro问题子集上展示其有效性的公共基准，包含标准化协议和参考模型。", "result": "实证结果验证了该方法的鲁棒性及其对抗数据污染的有效性：一个在测试问题上微调的Llama 3.1模型在准确性上显示出显著提升（50% -> 82%），但在辩论中的表现反而更差。结果还表明，即使是较弱的评判者也能可靠地区分出更强的辩论者，这表明基于辩论的评估可以扩展到未来更强大的系统，同时保持创建新基准成本的一小部分。", "conclusion": "该框架强调“预训练测试集不再是万能”，为衡量高级语言模型的真实推理能力提供了一条可持续的路径。", "translation": "随着前沿语言模型日益饱和标准问答基准，关于数据污染、记忆化和不断上升的数据集创建成本的担忧持续存在。我们提出了一种辩论驱动的评估范式，将任何现有问答数据集转化为结构化对抗性辩论——其中一个模型被赋予官方答案进行辩护，另一个模型构建并辩护一个替代答案——由一个对正确解决方案不知情的评判模型进行裁决。通过强制进行多轮论证，这种方法在惩罚浅层记忆的同时，大大增加了难度，并且通过重用问答项来减少策展开销。我们做出了两项主要贡献：(1) 一个系统地将问答任务转换为基于辩论评估的评估流程，以及 (2) 一个公共基准，展示了我们的范式在MMLU-Pro问题子集上的有效性，并附带标准化协议和参考模型。实证结果验证了该方法的鲁棒性及其对抗数据污染的有效性——一个在测试问题上进行微调的Llama 3.1模型显示出显著的准确性提升（50% -> 82%），但在辩论中的表现更差。结果还表明，即使是较弱的评判者也能可靠地区分出更强的辩论者，这突显了基于辩论的评估如何能够扩展到未来更强大的系统，同时保持创建新基准成本的一小部分。总的来说，我们的框架强调“预训练测试集不再是万能”，为衡量高级语言模型的真实推理能力提供了一条可持续的路径。", "summary": "该论文针对当前语言模型在标准问答基准上存在的饱和、数据污染和记忆化问题，提出了一种名为“辩论驱动评估范式”的新方法。该范式将现有问答数据集转化为模型间的结构化对抗性辩论，一个模型辩护官方答案，另一个模型辩护替代答案，并由一个独立的评判模型裁决。这种方法通过多轮论证增加了评估难度，有效惩罚了浅层记忆，并降低了数据集创建成本。作者贡献了一个评估流程和一个公共基准。实验结果表明，即使经过测试集微调的模型在传统评估中表现出色，但在辩论中表现下降，证明了新方法对抗数据污染的有效性。此外，即使是较弱的评判者也能有效区分辩论者，表明该方法具有良好的可扩展性和成本效益，为衡量语言模型的真实推理能力提供了一条可持续的途径。", "keywords": "语言模型, 问答基准, 辩论驱动, 数据污染, 推理能力", "comments": "这项工作具有重要的创新性，它通过引入辩论机制，将传统的静态问答评估转化为动态对抗性评估，有效解决了当前大型语言模型在基准测试中存在的“数据污染”和“浅层记忆”问题。其核心价值在于提供了一种更鲁棒、更具挑战性且成本效益更高的评估范范式，能够更好地衡量模型的真实推理能力而非简单的记忆能力。这对于推动语言模型研究向更深层次的理解和推理发展至关重要。"}}
{"id": "2501.13926", "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step", "authors": ["Ziyu Guo", "Renrui Zhang", "Chengzhuo Tong", "Zhizheng Zhao", "Rui Huang", "Haoquan Zhang", "Manyuan Zhang", "Jiaming Liu", "Shanghang Zhang", "Peng Gao", "Hongsheng Li", "Pheng-Ann Heng"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Journal Version. Code and models are released at this https URL", "url": "http://arxiv.org/abs/2501.13926v2", "summary": "Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image, which is the first to incorporate reflection in\nautoregressive image generation. Using our investigated reasoning strategies,\nwe enhance a baseline model, Show-o, to achieve superior results, with a\nsignificant +24% improvement on the GenEval benchmark, surpassing Stable\nDiffusion 3 by +15%. We hope our study provides unique insights and paves a new\npath for integrating CoT reasoning with autoregressive image generation. Code\nand models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "comment": "Journal Version. Code and models are released at\n  https://github.com/ZiyuGuo99/Image-Generation-CoT", "pdf_url": "http://arxiv.org/pdf/2501.13926v2", "cate": "cs.CV", "date": "2025-01-23", "updated": "2025-07-23", "AI": {"title_translation": "我们能用CoT生成图像吗？让我们逐步验证和强化图像生成", "tldr": "本文首次全面探讨了将思维链（CoT）推理应用于自回归图像生成，通过结合验证、DPO和新型奖励模型（PARM/PARM++）显著提升了图像生成性能，并在GenEval基准上实现了显著改进，超越了现有SOTA模型。", "motivation": "思维链（CoT）推理在大型模型的复杂理解任务中已被广泛探索，但其是否能应用于图像生成场景的验证和强化仍是一个悬而未决的问题。本研究旨在首次全面探究CoT推理在增强自回归图像生成方面的潜力。", "method": "本研究首次全面调查了思维链（CoT）推理在增强自回归图像生成中的潜力。方法包括：1. 扩展测试时计算以进行验证。2. 使用直接偏好优化（DPO）对齐模型偏好。3. 整合上述技术以产生互补效应。此外，鉴于奖励模型的重要性，论文提出了专为自回归图像生成设计的潜在评估奖励模型（PARM）和PARM++。PARM通过潜在评估方法自适应地评估每个生成步骤，PARM++进一步引入了反射机制来纠正不满意的生成图像。", "result": "研究结果表明，所提出的方法可以有效地适应和组合，显著提高图像生成性能。通过使用所研究的推理策略，一个基线模型Show-o得到了增强，在GenEval基准上实现了显著的+24%改进，并超越了Stable Diffusion 3达+15%。", "conclusion": "本研究首次全面探讨了思维链（CoT）推理在增强自回归图像生成方面的潜力，并提出了创新的奖励模型PARM和PARM++。研究结果为将CoT推理与自回归图像生成相结合提供了独特的见解，并开辟了一条新途径。", "translation": "思维链（CoT）推理已在大型模型中得到广泛探索，以解决复杂的理解任务。然而，此类策略是否可以应用于验证和强化图像生成场景仍是一个悬而未决的问题。在本文中，我们首次全面调查了CoT推理增强自回归图像生成的潜力。我们专注于三种技术：扩展测试时计算以进行验证，使用直接偏好优化（DPO）对齐模型偏好，以及整合这些技术以产生互补效应。我们的结果表明，这些方法可以有效地适应和组合，显著提高图像生成性能。此外，鉴于奖励模型在我们的发现中发挥的关键作用，我们提出了潜在评估奖励模型（PARM）和PARM++，它们专门用于自回归图像生成。PARM通过潜在评估方法自适应地评估每个生成步骤，融合了现有奖励模型的优势，而PARM++进一步引入了反射机制以自我纠正生成的 unsatisfactory 图像，这是首次将反射机制引入自回归图像生成。利用我们研究的推理策略，我们将一个基线模型Show-o增强，以实现卓越的结果，在GenEval基准上显著提高了+24%，超越Stable Diffusion 3达+15%。我们希望我们的研究能提供独特的见解，并为将CoT推理与自回归图像生成相结合铺平新道路。代码和模型已在 https://github.com/ZiyuGuo99/Image-Generation-CoT 发布。", "summary": "本文首次全面探索了将思维链（CoT）推理应用于自回归图像生成。研究提出了三种核心技术，包括测试时计算验证、DPO偏好对齐以及它们的集成。为进一步提升性能，论文还引入了创新的潜在评估奖励模型PARM及其增强版PARM++，其中PARM++首次在自回归图像生成中引入了反射机制。实验结果显示，通过这些CoT推理策略，基线模型Show-o在GenEval基准上实现了+24%的显著提升，并超越Stable Diffusion 3达+15%。本研究为CoT与自回归图像生成的融合提供了新思路。", "keywords": "CoT, 图像生成, 自回归, 奖励模型, DPO", "comments": "本论文的创新之处在于首次将思维链（CoT）推理全面应用于自回归图像生成领域，这为图像生成引入了更强的可控性和验证机制。特别是提出的PARM和PARM++奖励模型，通过潜在评估和反射机制，为生成质量的提升提供了新的范式，尤其是PARM++首次引入的反射机制，有望显著提高生成图像的自我纠错能力。研究成果在量化指标上表现出色，超越了现有SOTA模型，显示了其重要性和潜力。"}}
{"id": "2507.16854", "title": "CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis", "authors": ["Xiaoqiang He"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16854v1", "summary": "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect\nterms within paired image-text data and determine their fine grained sentiment\npolarities, representing a fundamental task for improving the effectiveness of\napplications such as product review systems and public opinion monitoring.\nExisting methods face challenges such as cross modal alignment noise and\ninsufficient consistency in fine-grained representations. While global modality\nalignment methods often overlook the connection between aspect terms and their\ncorresponding local visual regions, bridging the representation gap between\ntext and images remains a challenge. To address these limitations, this paper\nintroduces an end to end Contrastive Learning framework with Adaptive\nMulti-loss and Progressive Attention Fusion(CLAMP). The framework is composed\nof three novel modules: Progressive Attention Fusion network, Multi-task\nContrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive\nAttention Fusion network enhances fine-grained alignment between textual\nfeatures and image regions via hierarchical, multi-stage cross modal\ninteractions, effectively suppressing irrelevant visual noise. Secondly,\nmulti-task contrastive learning combines global modal contrast and local\ngranularity alignment to enhance cross modal representation consistency.\nAdaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting\nmechanism to calibrate loss contributions according to each task's uncertainty,\nthereby mitigating gradient interference. Evaluation on standard public\nbenchmarks demonstrates that CLAMP consistently outperforms the vast majority\nof existing state of the art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16854v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "CLAMP：面向多模态方面级情感分析的自适应多损失与渐进式融合对比学习", "tldr": "本文提出了一个名为 CLAMP 的端到端对比学习框架，用于解决多模态方面级情感分析中跨模态对齐噪声和细粒度表示一致性不足的问题，并在基准测试中表现出色。", "motivation": "现有多模态方面级情感分析(MABSA)方法面临跨模态对齐噪声和细粒度表示一致性不足的挑战，尤其是在全局模态对齐忽视方面词与其对应局部视觉区域连接的情况下，文本与图像之间的表示鸿沟难以弥合。", "method": "本文提出了一个端到端的对比学习框架 CLAMP，包含三个模块：1. 渐进式注意力融合网络，通过分层、多阶段跨模态交互增强文本特征与图像区域的细粒度对齐并抑制无关视觉噪声。2. 多任务对比学习，结合全局模态对比和局部粒度对齐以增强跨模态表示一致性。3. 自适应多损失聚合，采用基于动态不确定性的加权机制校准损失贡献，以减轻梯度干扰。", "result": "在标准公共基准测试中，CLAMP 持续优于绝大多数现有最先进方法。", "conclusion": "CLAMP 框架通过创新的渐进式注意力融合、多任务对比学习和自适应多损失聚合模块，有效解决了多模态方面级情感分析中的关键挑战，并取得了卓越的性能，证明了其在改善产品评论系统和舆情监控等应用方面的潜力。", "translation": "多模态方面级情感分析（MABSA）旨在识别配对图像-文本数据中的方面词并确定其细粒度情感极性，是提高产品评论系统和舆情监控等应用有效性的基础任务。现有方法面临跨模态对齐噪声和细粒度表示一致性不足等挑战。虽然全局模态对齐方法通常忽略方面词与其相应局部视觉区域之间的联系，但弥合文本和图像之间的表示鸿沟仍然是一个挑战。为了解决这些限制，本文引入了一个端到端的对比学习框架，该框架具有自适应多损失和渐进式注意力融合（CLAMP）。该框架由三个新颖的模块组成：渐进式注意力融合网络、多任务对比学习和自适应多损失聚合。渐进式注意力融合网络通过分层、多阶段的跨模态交互增强文本特征和图像区域之间的细粒度对齐，有效抑制无关视觉噪声。其次，多任务对比学习结合全局模态对比和局部粒度对齐，以增强跨模态表示一致性。自适应多损失聚合采用基于动态不确定性的加权机制，根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。在标准公共基准测试上的评估表明，CLAMP 持续优于绝大多数现有最先进方法。", "summary": "本文针对多模态方面级情感分析（MABSA）中存在的跨模态对齐噪声和细粒度表示一致性不足问题，提出了一个名为 CLAMP 的端到端对比学习框架。CLAMP 包含渐进式注意力融合网络、多任务对比学习和自适应多损失聚合三个核心模块，分别用于增强细粒度对齐、提升跨模态表示一致性以及优化损失贡献。实验结果表明，CLAMP 在公共基准测试上显著优于现有主流方法。", "keywords": "多模态方面级情感分析, 对比学习, 注意力融合, 多任务学习, 情感分析", "comments": "CLAMP 通过其独特的渐进式注意力融合、多任务对比学习和自适应多损失聚合机制，系统地解决了MABSA领域中细粒度对齐和跨模态一致性的核心难题。其创新点在于将多种高级技术有效整合到一个统一框架中，通过动态调整和分层交互，显著提升了模型性能，对于多模态情感分析领域具有重要的推动作用。"}}
{"id": "2408.08068", "title": "The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan P. Brumby", "Anna Cox"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2408.08068v2", "summary": "Informal Knowledge Sharing (KS) is vital for end-user programmers to gain\nexpertise. To better understand how personal (self-efficacy), social\n(reputational gains, trust between colleagues), and software-related\n(codification effort) variables influence spreadsheet KS intention, we\nconducted a multiple regressions analysis based on survey data from spreadsheet\nusers (n=100) in administrative and finance roles. We found that high levels of\nspreadsheet self-efficacy and a perception that sharing would result in\nreputational gains predicted higher KS intention, but individuals who found\nknowledge codification effortful showed lower KS intention. We also observed\nthat regardless of occupation, users tended to report a lower sense of\nself-efficacy in their general spreadsheet proficiency, despite also reporting\nhigh self-efficacy in spreadsheet use for job-related contexts. Our findings\nsuggest that acknowledging and designing for these social and personal\nvariables can help avoid situations where experienced individuals refrain\nunnecessarily from sharing, with implications for spreadsheet design.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2408.08068v2", "cate": "cs.HC", "date": "2024-08-15", "updated": "2025-07-23", "AI": {"title_translation": "电子表格自我效能悖论：终端用户编程中非正式知识共享的社会激励", "tldr": "研究发现，尽管电子表格用户在工作相关情境中自我效能感高，但在一般熟练度上自我效能感较低，且自我效能高和声誉收益预期会促进知识共享意愿，但知识编码困难则会抑制。", "motivation": "为了更好地理解个人（自我效能）、社会（声誉收益、同事信任）和软件相关（编码努力）变量如何影响电子表格知识共享意愿，因为非正式知识共享对终端用户程序员获取专业知识至关重要。", "method": "我们对行政和财务角色的电子表格用户（n=100）进行了调查，并基于调查数据进行了多元回归分析。", "result": "高水平的电子表格自我效能和认为分享会带来声誉收益的感知预示着更高的知识共享意愿；但认为知识编码困难的个体表现出较低的知识共享意愿。此外，无论职业如何，用户在一般电子表格熟练度上倾向于报告较低的自我效能感，尽管他们在工作相关的电子表格使用中报告了较高的自我效能感。", "conclusion": "研究结果表明，认识并设计考虑这些社会和个人变量有助于避免经验丰富的个体不必要地避免分享，这对电子表格设计具有启示意义。", "translation": "非正式知识共享对终端用户程序员获取专业知识至关重要。为了更好地理解个人（自我效能）、社会（声誉收益、同事信任）和软件相关（编码努力）变量如何影响电子表格知识共享意愿，我们基于对行政和财务角色的电子表格用户（n=100）的调查数据进行了多元回归分析。我们发现，高水平的电子表格自我效能和认为分享会带来声誉收益的感知预示着更高的知识共享意愿，但认为知识编码困难的个体表现出较低的知识共享意愿。我们还观察到，无论职业如何，用户倾向于报告他们在一般电子表格熟练度上的自我效能感较低，尽管他们在工作相关的电子表格使用中也报告了较高的自我效能感。我们的发现表明，认识并设计考虑这些社会和个人变量有助于避免经验丰富的个体不必要地避免分享，这对电子表格设计具有启示意义。", "summary": "本研究探讨了个人、社会和软件相关因素如何影响电子表格用户的非正式知识共享意愿。通过对100名电子表格用户的调查和多元回归分析，研究发现，高自我效能感和预期声誉收益能促进知识共享，而知识编码的困难则会抑制。论文还揭示了一个悖论：用户在工作相关情境中自我效能感高，但在普遍熟练度上自我效能感较低。研究强调了在电子表格设计中考虑这些社会和个人变量的重要性，以鼓励经验丰富的用户进行知识共享。", "keywords": "电子表格, 知识共享, 自我效能, 社会激励, 终端用户编程", "comments": "本研究揭示了电子表格用户知识共享行为中的一个重要悖论，即其自我效能感在不同情境下的差异。它强调了社会激励和编码努力在知识共享中的关键作用，为设计更有效的知识共享机制提供了实践启示。创新点在于将自我效能与社会激励因素相结合，深入分析了终端用户编程环境下的知识共享行为。"}}
{"id": "2502.08791", "title": "VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots", "authors": ["Yuxuan Zhang", "Adnan Abdullah", "Sanjeev J. Koppal", "Md Jahidul Islam"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      V2, includes suppl as appendix", "url": "http://arxiv.org/abs/2502.08791v2", "summary": "Vision-language navigation (VLN) has emerged as a promising paradigm,\nenabling mobile robots to perform zero-shot inference and execute tasks without\nspecific pre-programming. However, current systems often separate map\nexploration and path planning, with exploration relying on inefficient\nalgorithms due to limited (partially observed) environmental information. In\nthis paper, we present a novel navigation pipeline named \"VL-Explore\" for\nsimultaneous exploration and target discovery in unknown environments,\nleveraging the capabilities of a vision-language model named CLIP. Our approach\nrequires only monocular vision and operates without any prior map or knowledge\nabout the target. For comprehensive evaluations, we designed a functional\nprototype of a UGV (unmanned ground vehicle) system named \"Open Rover\", a\ncustomized platform for general-purpose VLN tasks. We integrated and deployed\nthe VL-Explore pipeline on Open Rover to evaluate its throughput, obstacle\navoidance capability, and trajectory performance across various real-world\nscenarios. Experimental results demonstrate that VL-Explore consistently\noutperforms traditional map-traversal algorithms and achieves performance\ncomparable to path-planning methods that depend on prior map and target\nknowledge. Notably, VL-Explore offers real-time active navigation without\nrequiring pre-captured candidate images or pre-built node graphs, addressing\nkey limitations of existing VLN pipelines.", "comment": "V2, includes suppl as appendix", "pdf_url": "http://arxiv.org/pdf/2502.08791v2", "cate": "cs.RO", "date": "2025-02-12", "updated": "2025-07-22", "AI": {"title_translation": "VL-Explore：移动机器人的零样本视觉语言探索与目标发现", "tldr": "VL-Explore是一个新的机器人导航管道，它利用视觉语言模型CLIP在未知环境中同时进行探索和目标发现，无需预先地图或目标知识，并优于传统方法。", "motivation": "现有的视觉语言导航(VLN)系统通常将地图探索和路径规划分开，且探索算法效率低下，受限于部分观测到的环境信息。此外，现有VLN管道需要预先捕获候选图像或预构建节点图。", "method": "本文提出了一个名为“VL-Explore”的新型导航管道，利用视觉语言模型CLIP的能力，在未知环境中同时进行探索和目标发现。该方法仅需要单目视觉，无需任何先验地图或目标知识。作者还设计了一个名为“Open Rover”的UGV系统原型，用于通用VLN任务，并将VL-Explore部署在其上进行评估。", "result": "VL-Explore在各种真实世界场景中，其吞吐量、避障能力和轨迹性能均得到评估。实验结果表明，VL-Explore始终优于传统的地图遍历算法，并且性能与依赖先验地图和目标知识的路径规划方法相当。VL-Explore能够提供实时主动导航，无需预先捕获候选图像或预构建节点图。", "conclusion": "VL-Explore成功解决了现有视觉语言导航系统在未知环境探索和目标发现方面的局限性，实现了高效且无需先验知识的零样本导航，并提供了与依赖先验知识的方法相当的性能。", "translation": "视觉语言导航（VLN）已成为一种有前景的范式，使移动机器人能够执行零样本推理并执行任务而无需特定的预编程。然而，当前的系统通常将地图探索和路径规划分开，探索由于有限（部分观察到）的环境信息而依赖于低效的算法。在本文中，我们提出了一种名为“VL-Explore”的新型导航管道，用于在未知环境中同时进行探索和目标发现，利用了视觉语言模型CLIP的能力。我们的方法仅需要单目视觉，并且在没有任何先验地图或目标知识的情况下运行。为了进行全面评估，我们设计了一个名为“Open Rover”的UGV（无人地面车辆）系统功能原型，这是一个用于通用VLN任务的定制平台。我们将VL-Explore管道集成并部署在Open Rover上，以评估其在各种真实世界场景中的吞吐量、避障能力和轨迹性能。实验结果表明，VL-Explore始终优于传统的地图遍历算法，并且性能与依赖先验地图和目标知识的路径规划方法相当。值得注意的是，VL-Explore提供实时主动导航，无需预先捕获候选图像或预构建节点图，解决了现有VLN管道的关键局限性。", "summary": "本文提出了一种名为VL-Explore的创新导航管道，旨在解决移动机器人在未知环境中进行零样本视觉语言探索和目标发现的挑战。该系统利用视觉语言模型CLIP，仅依赖单目视觉，无需任何预先地图或目标知识，即可同时实现探索和目标发现。作者开发了一个名为“Open Rover”的UGV原型来评估VL-Explore，实验结果表明其性能优于传统地图遍历算法，并与依赖先验知识的路径规划方法相媲美，同时提供了实时主动导航能力，克服了现有VLN系统的主要限制。", "keywords": "视觉语言导航, 零样本, 机器人探索, 目标发现, CLIP", "comments": "VL-Explore的创新之处在于其将探索和目标发现相结合，并利用CLIP模型实现零样本导航，无需预先地图或目标知识。这对于移动机器人在未知环境中自主操作具有重要意义，特别是在实际应用中减少了对人工预编程和环境建模的依赖。其在实时性和无需预捕获图像方面的优势，解决了现有VLN系统的一些关键局限性。"}}
{"id": "2402.04711", "title": "High-dimensional multidisciplinary design optimization for aircraft eco-design / Optimisation multi-disciplinaire en grande dimension pour l'éco-conception avion en avant-projet", "authors": ["Paul Saves"], "categories": ["math.OC", "cs.LG", "cs.MS", "stat.ML"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      PhD Thesis, Université de Toulouse, Toulouse, 2024 on Gaussian Process kernels for Bayesian optimization in high dimension with mixed and hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box optimization, Bayesian inference, Multidisciplinary design optimization, Mixed hierarchical and categorical inputs, Eco-friendly aircraft design", "url": "http://arxiv.org/abs/2402.04711v3", "summary": "The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an\nefficient approach for optimizing a multidisciplinary black-box model when the\noptimization problem is constrained and involves a large number of mixed\ninteger design variables (typically 100 variables). The targeted optimization\napproach, called EGO, is based on a sequential enrichment of an adaptive\nsurrogate model and, in this context, GP surrogate models are one of the most\nwidely used in engineering problems to approximate time-consuming high fidelity\nmodels. EGO is a heuristic BO method that performs well in terms of solution\nquality. However, like any other global optimization method, EGO suffers from\nthe curse of dimensionality, meaning that its performance is satisfactory on\nlower dimensional problems, but deteriorates as the dimensionality of the\noptimization search space increases. For realistic aircraft design problems,\nthe typical size of the design variables can even exceed 100 and, thus, trying\nto solve directly the problems using EGO is ruled out. The latter is especially\ntrue when the problems involve both continuous and categorical variables\nincreasing even more the size of the search space. In this Ph.D thesis,\neffective parameterization tools are investigated, including techniques like\npartial least squares regression, to significantly reduce the number of design\nvariables. Additionally, Bayesian optimization is adapted to handle discrete\nvariables and high-dimensional spaces in order to reduce the number of\nevaluations when optimizing innovative aircraft concepts such as the \"DRAGON\"\nhybrid airplane to reduce their climate impact.", "comment": "PhD Thesis, Universit\\'e de Toulouse, Toulouse, 2024 on Gaussian\n  Process kernels for Bayesian optimization in high dimension with mixed and\n  hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box\n  optimization, Bayesian inference, Multidisciplinary design optimization,\n  Mixed hierarchical and categorical inputs, Eco-friendly aircraft design", "pdf_url": "http://arxiv.org/pdf/2402.04711v3", "cate": "math.OC", "date": "2024-02-07", "updated": "2024-05-26", "AI": {"title_translation": "飞机生态设计中的高维多学科设计优化 / 飞机初步设计中的高维多学科优化用于生态设计", "tldr": "针对飞机生态设计中高维多学科优化问题，通过参数化工具和改进贝叶斯优化来克服维度灾难，减少评估次数。", "motivation": "现有优化方法（如EGO）在处理高维（变量数超过100，包含连续和分类变量）约束黑盒多学科优化问题时，会遇到“维度灾难”，性能显著下降，无法直接应用于实际飞机设计问题。", "method": "本论文研究了有效的参数化工具，包括偏最小二乘回归（PLS），以显著减少设计变量的数量。此外，调整了贝叶斯优化（BO）以处理离散变量和高维空间，从而在优化创新飞机概念时减少评估次数。该方法基于EGO，使用高斯过程（GP）代理模型。", "result": "通过参数化工具和适应的贝叶斯优化，能够有效处理高维和混合整数设计变量的优化问题，并成功减少评估次数。", "conclusion": "该博士论文提出并验证了一种有效的方法，通过结合参数化工具和适应的贝叶斯优化，成功解决了飞机生态设计中高维、混合整数约束黑盒模型的优化难题，尤其是在减少计算成本方面表现出色。", "translation": "这篇博士论文的目标是提出一种有效的方法，用于优化一个多学科黑盒模型，当优化问题受到约束并涉及大量混合整数设计变量（通常为100个变量）时。目标优化方法，称为EGO，基于自适应代理模型的顺序丰富，在这种情况下，高斯过程（GP）代理模型是工程问题中用于近似耗时高保真模型的最广泛使用的方法之一。EGO是一种启发式贝叶斯优化（BO）方法，在解决方案质量方面表现良好。然而，像任何其他全局优化方法一样，EGO也受到维度灾难的影响，这意味着其性能在低维问题上令人满意，但随着优化搜索空间维度的增加而恶化。对于实际的飞机设计问题，设计变量的典型大小甚至可以超过100个，因此，直接使用EGO解决这些问题是不可行的。当问题涉及连续和分类变量时，搜索空间的大小甚至会进一步增加，这一点尤其明显。在这篇博士论文中，研究了有效的参数化工具，包括偏最小二乘回归等技术，以显著减少设计变量的数量。此外，贝叶斯优化被调整以处理离散变量和高维空间，从而在优化创新飞机概念（如“DRAGON”混合飞机）以减少其气候影响时减少评估次数。", "summary": "这篇博士论文旨在解决飞机生态设计中高维、混合整数约束黑盒多学科优化问题。针对现有EGO等方法在高维问题中面临的“维度灾难”，论文提出了一种改进方案。该方案通过研究偏最小二乘回归等有效的参数化工具来显著减少设计变量数量，并调整贝叶斯优化以处理离散变量和高维空间，从而在优化如“DRAGON”混合飞机等创新概念时有效减少评估次数，提高优化效率。", "keywords": "高维优化, 多学科设计优化, 飞机生态设计, 贝叶斯优化, 参数化", "comments": "创新点在于结合参数化工具和改进的贝叶斯优化来克服高维优化中的“维度灾难”，特别是在处理混合整数变量和减少计算成本方面具有重要意义。该方法对于实际工程问题，尤其是复杂的飞机设计优化，具有很高的实用价值。"}}
{"id": "2407.05593", "title": "Unmasking Trees for Tabular Data", "authors": ["Calvin McCarter"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in Transactions on Machine Learning Research (7/2025)", "url": "http://arxiv.org/abs/2407.05593v5", "summary": "Despite much work on advanced deep learning and generative modeling\ntechniques for tabular data generation and imputation, traditional methods have\ncontinued to win on imputation benchmarks. We herein present UnmaskingTrees, a\nsimple method for tabular imputation (and generation) employing\ngradient-boosted decision trees which are used to incrementally unmask\nindividual features. On a benchmark for out-of-the-box performance on 27 small\ntabular datasets, UnmaskingTrees offers leading performance on imputation;\nstate-of-the-art performance on generation given data with missingness; and\ncompetitive performance on vanilla generation given data without missingness.\nTo solve the conditional generation subproblem, we propose a tabular\nprobabilistic prediction method, BaltoBot, which fits a balanced tree of\nboosted tree classifiers. Unlike older methods, it requires no parametric\nassumption on the conditional distribution, accommodating features with\nmultimodal distributions; unlike newer diffusion methods, it offers fast\nsampling, closed-form density estimation, and flexible handling of discrete\nvariables. We finally consider our two approaches as meta-algorithms,\ndemonstrating in-context learning-based generative modeling with TabPFN.", "comment": "Published in Transactions on Machine Learning Research (7/2025)", "pdf_url": "http://arxiv.org/pdf/2407.05593v5", "cate": "cs.LG", "date": "2024-07-08", "updated": "2025-07-23", "AI": {"title_translation": "用于表格数据的“揭示树”", "tldr": "本文提出了UnmaskingTrees，一种基于梯度提升决策树的表格数据插补和生成方法，在基准测试中表现出色。同时引入了BaltoBot用于条件生成，该方法无需参数假设，能有效处理多峰和离散数据。", "motivation": "尽管深度学习和生成模型在表格数据生成和插补方面进行了大量研究，但传统方法在插补基准测试中仍持续领先。", "method": "本文提出了UnmaskingTrees，一种用于表格数据插补（和生成）的简单方法，它采用梯度提升决策树逐步揭示单个特征。为了解决条件生成子问题，本文提出了表格概率预测方法BaltoBot，它拟合了一个平衡的提升树分类器树。BaltoBot不需要对条件分布进行参数假设，能适应多峰分布的特征，并提供快速采样、封闭形式的密度估计以及对离散变量的灵活处理。作者还将这两种方法视为元算法，并结合TabPFN展示了基于上下文学习的生成建模。", "result": "在27个小型表格数据集的开箱即用性能基准测试中，UnmaskingTrees在插补方面表现领先；在存在缺失数据的情况下，生成性能达到最先进水平；在没有缺失数据的普通生成方面，表现具有竞争力。", "conclusion": "UnmaskingTrees和BaltoBot是针对表格数据插补和生成提出的有效方法，它们在性能上超越或匹敌现有技术，并且BaltoBot在条件生成方面具有独特优势，无需参数假设并能处理多峰和离散数据。", "translation": "尽管在表格数据生成和插补方面，先进的深度学习和生成建模技术进行了大量研究，但传统方法在插补基准测试中仍持续领先。本文提出了UnmaskingTrees，一种用于表格插补（和生成）的简单方法，它采用梯度提升决策树逐步揭示单个特征。在27个小型表格数据集的开箱即用性能基准测试中，UnmaskingTrees在插补方面表现领先；在存在缺失数据的情况下，生成性能达到最先进水平；在没有缺失数据的普通生成方面，表现具有竞争力。为了解决条件生成子问题，我们提出了一种表格概率预测方法BaltoBot，它拟合了一个平衡的提升树分类器树。与旧方法不同，它不需要对条件分布进行参数假设，能适应多峰分布的特征；与新的扩散方法不同，它提供快速采样、封闭形式的密度估计以及对离散变量的灵活处理。我们最后将我们的两种方法视为元算法，演示了使用TabPFN进行基于上下文学习的生成建模。", "summary": "本文介绍了UnmaskingTrees，一种基于梯度提升决策树的表格数据插补和生成方法。该方法在多个表格数据集上展现了在插补方面的领先性能，在有缺失数据的生成方面达到最先进水平，并在无缺失数据的生成方面具有竞争力。此外，论文还提出了BaltoBot，一种用于条件生成的表格概率预测方法，它无需参数假设，能处理多峰分布和离散变量，并提供快速采样和封闭形式的密度估计。这两种方法被视为元算法，并与TabPFN结合展示了上下文学习的生成建模能力。", "keywords": "表格数据, 插补, 生成, 梯度提升树, UnmaskingTrees", "comments": "该论文创新性地将梯度提升决策树应用于表格数据的插补和生成，并提出了一个无需参数假设的条件生成方法BaltoBot，解决了现有方法的一些局限性。其在基准测试上的领先和竞争力表现突显了传统树模型在表格数据任务中的强大潜力，为该领域提供了新的视角和有效工具。"}}
{"id": "2507.17049", "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "categories": ["cs.SE", "cs.RO"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17049v1", "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17049v1", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "评估视觉语言动作机器人中的不确定性和质量", "tldr": "本文提出了针对视觉语言动作（VLA）机器人的不确定性和质量评估指标，并通过大规模实验证明其有效性，挑战了传统仅依赖成功率的评估方法。", "motivation": "现有的视觉语言动作（VLA）模型评估主要依赖任务成功率，但这无法捕捉任务执行的质量以及模型决策的置信度。因此，需要更全面的指标来评估VLA模型。", "method": "作者提出了8个不确定性指标和5个质量指标，专门针对VLA模型在机器人操作任务中的应用。他们通过一项大规模实证研究评估了这些指标的有效性，该研究涉及3个最先进的VLA模型在4个代表性机器人操作任务中的908次成功任务执行。人类领域专家手动标注了任务质量，以便分析所提出指标与专家判断之间的相关性。", "result": "结果显示，所提出的几个指标与人类评估之间存在中度到强度的相关性，突显了它们在评估任务质量和模型置信度方面的实用性。此外，研究发现某些指标能够区分未成功任务中高质量、中等质量和低质量的执行，这在测试预言机不可用时尤其有意义。", "conclusion": "该研究挑战了当前仅依赖二元成功率的评估实践的充分性，并为改进VLA驱动机器人系统的实时监控和自适应增强铺平了道路。", "translation": "视觉语言动作（VLA）模型是一类多模态人工智能（AI）系统，它整合了视觉感知、自然语言理解和动作规划，使智能体能够解释其环境、理解指令并自主执行具身任务。最近，该领域取得了显著进展。这类模型通常通过任务成功率进行评估，但这未能捕捉任务执行的质量和模型对其决策的置信度。在本文中，我们提出了8个不确定性指标和5个质量指标，专门为机器人操作任务中的VLA模型设计。我们通过一项大规模实证研究评估了它们的有效性，该研究涉及来自3个最先进VLA模型在4个代表性机器人操作任务中的908次成功任务执行。人类领域专家手动标注了任务质量，使我们能够分析所提出的指标与专家判断之间的相关性。结果显示，所提出的几个指标与人类评估之间存在中度到强度的相关性，突显了它们在评估任务质量和模型置信度方面的实用性。此外，我们发现某些指标能够区分未成功任务中高质量、中等质量和低质量的执行，这在测试预言机不可用时尤其有意义。我们的发现挑战了当前仅依赖二元成功率的评估实践的充分性，并为改进VLA驱动机器人系统的实时监控和自适应增强铺平了道路。", "summary": "本文针对视觉语言动作（VLA）机器人模型现有的评估方法（仅依赖成功率）无法捕捉执行质量和置信度的问题，提出了8个不确定性指标和5个质量指标。通过对大规模机器人操作任务的实证研究，并结合人类专家的质量标注，验证了这些新指标与人类评估的高度相关性，证明了它们在更全面评估VLA模型方面的有效性，并为未来的实时监控和系统改进提供了新的方向。", "keywords": "视觉语言动作模型, 机器人操作, 不确定性评估, 质量评估, 人工智能评估", "comments": "这篇论文的创新点在于提出了专门针对视觉语言动作（VLA）机器人模型的不确定性和质量评估指标，弥补了现有评估方法（仅依赖二元成功率）的不足。其重要性在于为VLA模型提供了一个更全面、更细致的评估框架，有助于更好地理解模型的性能、置信度和任务执行质量，从而推动VLA机器人系统的发展和实际应用。通过引入人类专家标注来验证指标的有效性，增加了研究结果的可信度。"}}
{"id": "2507.17083", "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction", "authors": ["Zaipeng Duan", "Chenxu Dang", "Xuzhong Hu", "Pei An", "Junfeng Ding", "Jie Zhan", "Yunbiao Xu", "Jie Ma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by CVPR2025", "url": "http://arxiv.org/abs/2507.17083v1", "summary": "Multimodal 3D occupancy prediction has garnered significant attention for its\npotential in autonomous driving. However, most existing approaches are\nsingle-modality: camera-based methods lack depth information, while LiDAR-based\nmethods struggle with occlusions. Current lightweight methods primarily rely on\nthe Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth\nestimation and fails to fully exploit the geometric and semantic information of\n3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction\nnetwork called SDG-OCC, which incorporates a joint semantic and depth-guided\nview transformation coupled with a fusion-to-occupancy-driven active\ndistillation. The enhanced view transformation constructs accurate depth\ndistributions by integrating pixel semantics and co-point depth through\ndiffusion and bilinear discretization. The fusion-to-occupancy-driven active\ndistillation extracts rich semantic information from multimodal data and\nselectively transfers knowledge to image features based on LiDAR-identified\nregions. Finally, for optimal performance, we introduce SDG-Fusion, which uses\nfusion alone, and SDG-KL, which integrates both fusion and distillation for\nfaster inference. Our method achieves state-of-the-art (SOTA) performance with\nreal-time processing on the Occ3D-nuScenes dataset and shows comparable\nperformance on the more challenging SurroundOcc-nuScenes dataset, demonstrating\nits effectiveness and robustness. The code will be released at\nhttps://github.com/DzpLab/SDGOCC.", "comment": "accepted by CVPR2025", "pdf_url": "http://arxiv.org/pdf/2507.17083v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "SDGOCC：用于3D多模态占用预测的语义和深度引导鸟瞰图变换", "tldr": "SDG-OCC提出了一种新的多模态3D占用预测网络，通过语义和深度引导的视图变换以及融合驱动的主动蒸馏，解决了现有方法在深度估计和信息利用上的不足，实现了最先进的性能。", "motivation": "现有的大多数多模态3D占用预测方法是单模态的：基于摄像头的方法缺乏深度信息，而基于激光雷达的方法存在遮挡问题。当前的轻量级方法主要依赖Lift-Splat-Shoot (LSS)管线，但其深度估计不准确，并且未能充分利用3D激光雷达点的几何和语义信息。", "method": "我们提出了SDG-OCC，一个新颖的多模态占用预测网络。它结合了语义和深度引导的视图变换，以及融合驱动的主动蒸馏。增强的视图变换通过扩散和双线性离散化整合像素语义和共点深度来构建准确的深度分布。融合驱动的主动蒸馏从多模态数据中提取丰富的语义信息，并根据激光雷达识别区域选择性地将知识传递给图像特征。为了获得最佳性能，我们还引入了仅使用融合的SDG-Fusion和结合融合与蒸馏以实现更快推理的SDG-KL。", "result": "我们的方法在Occ3D-nuScenes数据集上实现了最先进（SOTA）的实时处理性能，并在更具挑战性的SurroundOcc-nuScenes数据集上表现出可比的性能。", "conclusion": "SDG-OCC通过其创新的语义和深度引导视图变换以及融合驱动的主动蒸馏，有效解决了多模态3D占用预测中的挑战，并在多个基准测试中展现出卓越的有效性和鲁棒性。", "translation": "多模态3D占用预测因其在自动驾驶中的潜力而受到广泛关注。然而，大多数现有方法是单模态的：基于摄像头的方法缺乏深度信息，而基于激光雷达的方法则难以处理遮挡。当前的轻量级方法主要依赖Lift-Splat-Shoot (LSS)管线，该管线存在深度估计不准确的问题，并且未能充分利用3D激光雷达点的几何和语义信息。因此，我们提出了一种新颖的多模态占用预测网络，名为SDG-OCC，该网络结合了联合语义和深度引导的视图变换以及融合驱动的主动蒸馏。增强的视图变换通过扩散和双线性离散化整合像素语义和共点深度，从而构建准确的深度分布。融合驱动的主动蒸馏从多模态数据中提取丰富的语义信息，并根据激光雷达识别区域选择性地将知识传递给图像特征。最后，为了获得最佳性能，我们引入了SDG-Fusion（仅使用融合）和SDG-KL（结合融合与蒸馏以实现更快推理）。我们的方法在Occ3D-nuScenes数据集上实现了最先进（SOTA）的实时处理性能，并在更具挑战性的SurroundOcc-nuScenes数据集上表现出可比的性能，证明了其有效性和鲁棒性。代码将在https://github.com/DzpLab/SDGOCC发布。", "summary": "本文提出了一种名为SDG-OCC的新型多模态3D占用预测网络，旨在解决现有方法在深度信息缺乏和遮挡处理上的不足。SDG-OCC通过引入语义和深度引导的视图变换来构建准确的深度分布，并采用融合驱动的主动蒸馏从多模态数据中提取和传递语义知识。该方法在Occ3D-nuScenes和SurroundOcc-nuScenes数据集上均实现了最先进或可比的性能，证明了其有效性和实时处理能力。", "keywords": "多模态3D占用预测, 鸟瞰图变换, 语义引导, 深度引导, 主动蒸馏", "comments": "SDG-OCC的创新之处在于其结合了语义和深度信息进行视图变换，并引入了融合驱动的主动蒸馏机制，有效提升了多模态3D占用预测的精度和效率。该方法解决了LSS管线深度估计不准和信息利用不足的问题，对自动驾驶领域的环境感知具有重要意义。"}}
{"id": "2507.17334", "title": "Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection", "authors": ["Weihua Gao", "Chunxu Ren", "Wenlong Niu", "Xiaodong Peng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17334v1", "summary": "In low-altitude surveillance and early warning systems, detecting weak moving\ntargets remains a significant challenge due to low signal energy, small spatial\nextent, and complex background clutter. Existing methods struggle with\nextracting robust features and suffer from the lack of reliable annotations. To\naddress these limitations, we propose a novel Temporal Point-Supervised (TPS)\nframework that enables high-performance detection of weak targets without any\nmanual annotations.Instead of conventional frame-based detection, our framework\nreformulates the task as a pixel-wise temporal signal modeling problem, where\nweak targets manifest as short-duration pulse-like responses. A Temporal Signal\nReconstruction Network (TSRNet) is developed under the TPS paradigm to\nreconstruct these transient signals.TSRNet adopts an encoder-decoder\narchitecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)\nmodule to enhance its sensitivity to diverse temporal patterns. Additionally, a\ngraph-based trajectory mining strategy is employed to suppress false alarms and\nensure temporal consistency.Extensive experiments on a purpose-built low-SNR\ndataset demonstrate that our framework outperforms state-of-the-art methods\nwhile requiring no human annotations. It achieves strong detection performance\nand operates at over 1000 FPS, underscoring its potential for real-time\ndeployment in practical scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17334v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "瞬时点监督信号重建：一种无需人工标注的弱小运动目标检测框架", "tldr": "提出了一种无需人工标注的瞬时点监督框架（TPS）及其网络（TSRNet），用于实时高效地检测弱小运动目标。", "motivation": "在低空监视和预警系统中，由于信号能量低、空间范围小和背景杂波复杂，检测弱小运动目标是一个重大挑战。现有方法难以提取鲁棒特征且缺乏可靠的标注。", "method": "提出了一种新颖的瞬时点监督（TPS）框架，将任务重新表述为像素级时间信号建模问题。开发了瞬时信号重建网络（TSRNet），采用编码器-解码器架构并集成了动态多尺度注意力（DMSAttention）模块。此外，采用基于图的轨迹挖掘策略来抑制虚警并确保时间一致性。", "result": "在专门构建的低信噪比数据集上，该框架性能优于现有最先进方法，且无需人工标注。实现了强大的检测性能，并以超过1000 FPS的速度运行。", "conclusion": "该框架在实际场景中具有实时部署的巨大潜力。", "translation": "在低空监视和预警系统中，由于信号能量低、空间范围小和背景杂波复杂，检测弱小运动目标仍然是一个重大挑战。现有方法难以提取鲁棒特征并受制于缺乏可靠的标注。为了解决这些限制，我们提出了一种新颖的瞬时点监督（TPS）框架，该框架能够在没有任何人工标注的情况下实现高性能的弱目标检测。与传统的基于帧的检测不同，我们的框架将任务重新表述为像素级时间信号建模问题，其中弱目标表现为短时脉冲状响应。在TPS范式下，开发了一个瞬时信号重建网络（TSRNet）来重建这些瞬态信号。TSRNet采用编码器-解码器架构，并集成了动态多尺度注意力（DMSAttention）模块，以增强其对不同时间模式的敏感性。此外，采用基于图的轨迹挖掘策略来抑制虚警并确保时间一致性。在专门构建的低信噪比数据集上进行的广泛实验表明，我们的框架在无需任何人工标注的情况下优于现有最先进方法。它实现了强大的检测性能，并以超过1000 FPS的速度运行，这突显了其在实际场景中实时部署的潜力。", "summary": "本文提出了一种新颖的瞬时点监督（TPS）框架，用于无需人工标注的弱小运动目标检测。该框架将检测任务重新定义为像素级时间信号建模，并开发了瞬时信号重建网络（TSRNet），结合动态多尺度注意力模块和图基轨迹挖掘策略。实验证明，该方法在低信噪比数据集上表现优异，且能实现实时高速运行，展现了其在实际应用中的巨大潜力。", "keywords": "弱目标检测, 瞬时点监督, 信号重建, 无标注, 实时检测", "comments": "该论文的创新点在于提出了一个无需人工标注的弱目标检测框架，通过将问题转化为时间信号重建，并结合了点监督、动态多尺度注意力以及图基轨迹挖掘，有效解决了传统方法对标注的依赖和复杂背景下的检测难题。其高帧率也预示着在实时系统中的巨大应用价值。"}}
{"id": "2507.17508", "title": "Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation", "authors": ["Jorgen Cani", "Christos Diou", "Spyridon Evangelatos", "Vasileios Argyriou", "Panagiotis Radoglou-Grammatikis", "Panagiotis Sarigiannidis", "Iraklis Varlamis", "Georgios Th. Papadopoulos"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17508v1", "summary": "Automated X-ray inspection is crucial for efficient and unobtrusive security\nscreening in various public settings. However, challenges such as object\nocclusion, variations in the physical properties of items, diversity in X-ray\nscanning devices, and limited training data hinder accurate and reliable\ndetection of illicit items. Despite the large body of research in the field,\nreported experimental evaluations are often incomplete, with frequently\nconflicting outcomes. To shed light on the research landscape and facilitate\nfurther research, a systematic, detailed, and thorough comparative evaluation\nof recent Deep Learning (DL)-based methods for X-ray object detection is\nconducted. For this, a comprehensive evaluation framework is developed,\ncomposed of: a) Six recent, large-scale, and widely used public datasets for\nX-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and\nPIDray), b) Ten different state-of-the-art object detection schemes covering\nall main categories in the literature, including generic Convolutional Neural\nNetwork (CNN), custom CNN, generic transformer, and hybrid CNN-transformer\narchitectures, and c) Various detection (mAP50 and mAP50:95) and\ntime/computational-complexity (inference time (ms), parameter size (M), and\ncomputational load (GFLOPS)) metrics. A thorough analysis of the results leads\nto critical observations and insights, emphasizing key aspects such as: a)\nOverall behavior of the object detection schemes, b) Object-level detection\nperformance, c) Dataset-specific observations, and d) Time efficiency and\ncomputational complexity analysis. To support reproducibility of the reported\nexperimental results, the evaluation code and model weights are made publicly\navailable at https://github.com/jgenc/xray-comparative-evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17508v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "X射线成像中基于深度学习技术的违禁品检测：一项比较评估", "tldr": "本文对X射线成像中基于深度学习的违禁品检测方法进行了系统性的比较评估。研究使用了六个公共数据集和十种最先进的检测方案，分析了它们的性能、效率和计算复杂度，并公开了代码以支持可重复性。", "motivation": "自动化X射线安检面临物体遮挡、物理特性变化、设备多样性及训练数据有限等挑战，导致违禁品检测不准确且不可靠。现有研究的实验评估常不完整且结果冲突，因此需要进行系统、详细的比较评估，以阐明研究现状并促进未来研究。", "method": "本研究建立了一个综合评估框架，包括：a) 六个近期、大规模且广泛使用的X射线违禁品检测公共数据集（OPIXray、CLCXray、SIXray、EDS、HiXray和PIDray）；b) 十种涵盖文献中所有主要类别的最先进目标检测方案，包括通用CNN、自定义CNN、通用Transformer和混合CNN-Transformer架构；c) 各种检测指标（mAP50和mAP50:95）以及时间/计算复杂度指标（推理时间（ms）、参数大小（M）和计算负载（GFLOPS））。对结果进行了彻底分析。", "result": "通过对结果的彻底分析，得出了关键的观察和见解，强调了以下几个关键方面：a) 目标检测方案的整体行为；b) 对象级检测性能；c) 数据集特定观察；d) 时间效率和计算复杂度分析。为了支持实验结果的可重复性，评估代码和模型权重已公开可用。", "conclusion": "本研究通过系统、详细的比较评估，为X射线成像中基于深度学习的违禁品检测方法提供了关键的观察和见解，旨在阐明当前的研究现状并促进该领域的进一步发展。公开评估代码和模型权重也增强了研究的可重复性。", "translation": "自动化X射线检查对于各种公共场所中高效、不显眼的安检至关重要。然而，物体遮挡、物品物理特性变化、X射线扫描设备多样性以及训练数据有限等挑战阻碍了对违禁品的准确可靠检测。尽管该领域有大量研究，但报告的实验评估往往不完整，结果经常相互冲突。为了阐明研究现状并促进进一步研究，本文对近期基于深度学习（DL）的X射线物体检测方法进行了系统、详细和彻底的比较评估。为此，开发了一个综合评估框架，该框架包括：a) 六个近期、大规模且广泛使用的X射线违禁品检测公共数据集（OPIXray、CLCXray、SIXray、EDS、HiXray和PIDray），b) 文献中涵盖所有主要类别的十种不同的最先进目标检测方案，包括通用卷积神经网络（CNN）、自定义CNN、通用Transformer和混合CNN-Transformer架构，以及c) 各种检测指标（mAP50和mAP50:95）和时间/计算复杂度指标（推理时间（ms）、参数大小（M）和计算负载（GFLOPS））。对结果的彻底分析得出了关键的观察和见解，强调了以下几个关键方面：a) 目标检测方案的整体行为，b) 对象级检测性能，c) 数据集特定观察，以及d) 时间效率和计算复杂度分析。为了支持报告实验结果的可重复性，评估代码和模型权重已在https://github.com/jgenc/xray-comparative-evaluation 公开提供。", "summary": "本论文旨在解决自动化X射线违禁品检测中存在的挑战，如遮挡和数据限制，通过对基于深度学习的方法进行全面的比较评估。研究建立了一个稳健的评估框架，利用六个公共X射线数据集、十种最先进的目标检测架构（包括CNN、Transformer和混合架构）以及多种性能和复杂度指标。该研究对方案行为、对象级性能、数据集特性和计算效率等方面提供了关键见解，并公开了所有代码和模型权重，以确保可重复性并促进未来的研究。", "keywords": "X射线成像, 违禁品检测, 深度学习, 比较评估, 目标检测", "comments": "这篇论文非常重要，因为它在一个经常被不完整和相互冲突的实验结果困扰的领域中，提供了一个急需的系统化和全面的比较评估。通过使用广泛的最新模型和多个大型数据集，它为X射线违禁品检测中不同深度学习架构的优缺点提供了宝贵的见解。代码和权重的公开显著增强了可重复性，并支持了进一步的研究，使其成为一项有价值的基准研究。"}}
{"id": "2412.02198", "title": "Transformer-Based Auxiliary Loss for Face Recognition Across Age Variations", "authors": ["Pritesh Prakash", "S Umamaheswaran"], "categories": ["cs.CV", "I.5.2"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Face Recognition for Age-variant Datasets", "url": "http://arxiv.org/abs/2412.02198v3", "summary": "Aging presents a significant challenge in face recognition, as changes in\nskin texture and tone can alter facial features over time, making it\nparticularly difficult to compare images of the same individual taken years\napart, such as in long-term identification scenarios. Transformer networks have\nthe strength to preserve sequential spatial relationships caused by aging\neffect. This paper presents a technique for loss evaluation that uses a\ntransformer network as an additive loss in the face recognition domain. The\nstandard metric loss function typically takes the final embedding of the main\nCNN backbone as its input. Here, we employ a transformer-metric loss, a\ncombined approach that integrates both transformer-loss and metric-loss. This\nresearch intends to analyze the transformer behavior on the convolution output\nwhen the CNN outcome is arranged in a sequential vector. These sequential\nvectors have the potential to overcome the texture or regional structure\nreferred to as wrinkles or sagging skin affected by aging. The transformer\nencoder takes input from the contextual vectors obtained from the final\nconvolution layer of the network. The learned features can be more\nage-invariant, complementing the discriminative power of the standard metric\nloss embedding. With this technique, we use transformer loss with various base\nmetric-loss functions to evaluate the effect of the combined loss functions. We\nobserve that such a configuration allows the network to achieve SoTA results in\nLFW and age-variant datasets (CA-LFW and AgeDB). This research expands the role\nof transformers in the machine vision domain and opens new possibilities for\nexploring transformers as a loss function.", "comment": "Face Recognition for Age-variant Datasets", "pdf_url": "http://arxiv.org/pdf/2412.02198v3", "cate": "cs.CV", "date": "2024-12-03", "updated": "2025-07-23", "AI": {"title_translation": "基于Transformer的辅助损失用于跨年龄变化人脸识别", "tldr": "本文提出一种将Transformer网络作为辅助损失函数，与标准度量损失结合的“Transformer-度量损失”方法，以提高跨年龄变化人脸识别的性能，并在LFW、CA-LFW和AgeDB数据集上取得了SoTA结果。", "motivation": "人脸识别中的年龄变化是一个重大挑战，因为皮肤纹理和色调的变化会随时间改变面部特征，使得识别跨越多年拍摄的同一人的图像变得特别困难，尤其是在长期识别场景中。Transformer网络能够保留由老化效应引起的顺序空间关系。", "method": "本文提出了一种损失评估技术，即使用Transformer网络作为人脸识别领域的附加损失，称之为“Transformer-度量损失”。这种方法结合了Transformer损失和度量损失。与通常将主干CNN的最终嵌入作为输入的标准度量损失函数不同，Transformer编码器从网络最终卷积层获得的上下文向量中获取输入，这些向量被排列成顺序向量。通过这种技术，将Transformer损失与各种基础度量损失函数结合使用，以评估组合损失函数的效果。", "result": "观察到这种配置使得网络在LFW和年龄变化数据集（CA-LFW和AgeDB）上取得了最先进（SoTA）的结果。", "conclusion": "这项研究扩展了Transformer在机器视觉领域的作用，并为探索Transformer作为损失函数开辟了新的可能性。", "translation": "衰老对人脸识别提出了重大挑战，因为皮肤纹理和色调的变化会随着时间的推移改变面部特征，这使得比较多年后拍摄的同一人的图像变得特别困难，例如在长期识别场景中。Transformer网络具有保留由衰老效应引起的顺序空间关系的能力。本文提出了一种损失评估技术，该技术使用Transformer网络作为人脸识别领域的附加损失。标准度量损失函数通常将主干CNN的最终嵌入作为输入。在这里，我们采用了一种Transformer-度量损失，这是一种结合了Transformer损失和度量损失的组合方法。本研究旨在分析当CNN输出以顺序向量排列时，Transformer在卷积输出上的行为。这些顺序向量有可能克服被称为皱纹或皮肤松弛的纹理或区域结构，这些结构受到衰老的影响。Transformer编码器从网络最终卷积层获得的上下文向量中获取输入。学习到的特征可以更具年龄不变性，补充了标准度量损失嵌入的判别能力。通过这种技术，我们将Transformer损失与各种基础度量损失函数结合使用，以评估组合损失函数的效果。我们观察到，这种配置使得网络在LFW和年龄变化数据集（CA-LFW和AgeDB）上取得了最先进（SoTA）的结果。这项研究扩展了Transformer在机器视觉领域的作用，并为探索Transformer作为损失函数开辟了新的可能性。", "summary": "本文针对人脸识别中因年龄增长导致面部特征变化带来的挑战，提出了一种新颖的“Transformer-度量损失”方法。该方法将Transformer网络作为辅助损失项，与标准度量损失相结合，利用Transformer处理从CNN最终卷积层提取的顺序上下文向量，以捕获并克服由衰老引起的纹理和区域结构变化（如皱纹）。实验结果表明，该方法在LFW、CA-LFW和AgeDB等数据集上达到了最先进的性能，展示了Transformer作为损失函数在机器视觉领域的潜力。", "keywords": "人脸识别, Transformer, 辅助损失, 年龄变化, 度量学习", "comments": "该论文的创新点在于将Transformer网络作为一种辅助损失函数引入到人脸识别任务中，而非仅仅作为特征提取器。通过将Transformer应用于CNN的中间层输出，并将其作为附加损失，有效解决了年龄变化导致的面部特征模糊和形变问题。这一方法为Transformer在机器视觉领域的应用开辟了新思路，展示了其超越传统特征提取的潜力，尤其在处理时序或结构变化方面具有优势。"}}
{"id": "2507.17712", "title": "Quantum Software Security Challenges within Shared Quantum Computing Environments", "authors": ["Samuel Ovaskainen", "Majid Haghparast", "Tommi Mikkonen"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at the 2025 IEEE International Conference on Quantum Computing and Engineering (QCE)", "url": "http://arxiv.org/abs/2507.17712v1", "summary": "The number of qubits in quantum computers keeps growing, but most quantum\nprograms remain relatively small because of the noisy nature of the underlying\nquantum hardware. This might lead quantum cloud providers to explore increased\nhardware utilization, and thus profitability through means such as\nmulti-programming, which would allow the execution of multiple programs in\nparallel. The adoption of such technology would bring entirely new challenges\nto the field of quantum software security. This article explores and reports\nthe key challenges identified in quantum software security within shared\nquantum computing environments.", "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "pdf_url": "http://arxiv.org/pdf/2507.17712v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "量子软件安全在共享量子计算环境中的挑战", "tldr": "随着量子计算硬件利用率的提高，共享量子计算环境中的量子软件将面临新的安全挑战，本文探讨并报告了这些挑战。", "motivation": "量子计算机中量子比特数量的增长以及底层硬件的噪声特性，促使量子云提供商探索多程序并行执行以提高硬件利用率和盈利能力。这种并行执行模式将为量子软件安全带来全新的挑战，因此需要对这些挑战进行探索和识别。", "method": "本文通过探索和报告的方式，识别了共享量子计算环境中量子软件安全面临的关键挑战。", "result": "本文识别并报告了共享量子计算环境中量子软件安全面临的关键挑战。", "conclusion": "本文识别了在共享量子计算环境下量子软件安全领域中出现的关键挑战。", "translation": "量子计算机中的量子比特数量持续增长，但由于底层量子硬件的噪声特性，大多数量子程序仍然相对较小。这可能导致量子云提供商探索通过多程序并行执行等方式提高硬件利用率，从而增加盈利能力。这种技术的采用将给量子软件安全领域带来全新的挑战。本文探讨并报告了在共享量子计算环境中识别出的量子软件安全的关键挑战。", "summary": "尽管量子计算机的量子比特数量不断增加，但由于硬件噪声，量子程序仍较小。为提高硬件利用率，量子云提供商可能采用多程序并行执行，这将引入新的量子软件安全问题。本文旨在探讨并报告共享量子计算环境中量子软件面临的关键安全挑战。", "keywords": "量子软件安全, 共享量子计算, 多程序, 量子云, 安全挑战", "comments": "本文探讨了一个新兴且重要的领域，即共享量子计算环境下的软件安全问题。随着量子计算技术的成熟和商业化，多用户和多程序并行执行将成为趋势，因此预先识别并解决其潜在的安全挑战至关重要，具有前瞻性。"}}
{"id": "2507.10054", "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks", "authors": ["Emir Bosnak", "Sahand Moslemi", "Mayasah Lami", "Anil Koyuncu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to ICSME 2025", "url": "http://arxiv.org/abs/2507.10054v2", "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities,\nthis study examines a more direct threat: open-source LLMs generating\nvulnerable code when prompted. We propose a dual experimental design: (1)\nDynamic Prompting, which systematically varies vulnerability type, user\npersona, and prompt phrasing across structured templates; and (2) Reverse\nPrompting, which derives natural-language prompts from real vulnerable code\nsamples. We evaluate three open-source 7B-parameter models (Qwen2, Mistral,\nGemma) using static analysis to assess both the presence and correctness of\ngenerated vulnerabilities. Our results show that all models frequently generate\nthe requested vulnerabilities, though with significant performance differences.\nGemma achieves the highest correctness for memory vulnerabilities under Dynamic\nPrompting (e.g., 98.6% for buffer overflows), while Qwen2 demonstrates the most\nbalanced performance across all tasks. We find that professional personas\n(e.g., \"DevOps Engineer\") consistently elicit higher success rates than student\npersonas, and that the effectiveness of direct versus indirect phrasing is\ninverted depending on the prompting strategy. Vulnerability reproduction\naccuracy follows a non-linear pattern with code complexity, peaking in a\nmoderate range. Our findings expose how LLMs' reliance on pattern recall over\nsemantic reasoning creates significant blind spots in their safety alignments,\nparticularly for requests framed as plausible professional tasks.", "comment": "Accepted to ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2507.10054v2", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-23", "AI": {"title_translation": "显式漏洞生成与大型语言模型：超越对抗性攻击的调查", "tldr": "本研究探讨了大型语言模型（LLMs）在被明确要求生成不安全代码时的行为。结果显示，LLMs会频繁生成请求的漏洞，并且其安全对齐存在显著盲点，尤其是在处理看似合理的专业任务时。", "motivation": "之前的研究主要关注LLMs无意中产生的漏洞，而本研究旨在探究LLMs在被明确要求生成不安全代码时，是否会直接产生漏洞，这是一种更直接的威胁。", "method": "本研究采用双重实验设计：一是“动态提示”，系统性地改变漏洞类型、用户角色和提示措辞；二是“反向提示”，从真实漏洞代码样本中提取自然语言提示。研究评估了Qwen2、Mistral和Gemma三个7B参数的开源模型，并使用静态分析来评估生成漏洞的存在性和正确性。", "result": "所有模型都能频繁生成请求的漏洞，但性能存在显著差异。Gemma在动态提示下对内存漏洞的正确性最高（例如，缓冲区溢出达到98.6%），而Qwen2在所有任务中表现最均衡。专业角色（如“DevOps工程师”）比学生角色能带来更高的成功率。直接与间接措辞的有效性取决于提示策略，呈现反向关系。漏洞复现准确性与代码复杂性呈非线性关系，在中等复杂度范围内达到峰值。", "conclusion": "大型语言模型（LLMs）依赖模式回忆而非语义推理，这导致其在安全对齐方面存在显著盲点，尤其是在将请求框定为合理的专业任务时。", "translation": "大型语言模型（LLMs）正越来越多地被用作代码助手，然而，当明确要求它们生成不安全代码时，它们的行为仍知之甚少。虽然之前的研究侧重于无意中产生的漏洞，但本研究审查了一个更直接的威胁：开源LLMs在被提示时生成易受攻击的代码。我们提出了一个双重实验设计：(1) 动态提示，系统地改变漏洞类型、用户角色和结构化模板中的提示措辞；以及 (2) 反向提示，从真实的易受攻击代码样本中导出自然语言提示。我们使用静态分析评估了三个开源的7B参数模型（Qwen2、Mistral、Gemma），以评估生成漏洞的存在性和正确性。我们的结果表明，所有模型都频繁生成请求的漏洞，尽管性能存在显著差异。Gemma在动态提示下对内存漏洞的正确性最高（例如，缓冲区溢出达到98.6%），而Qwen2在所有任务中表现最均衡。我们发现，专业角色（例如，“DevOps工程师”）比学生角色能持续带来更高的成功率，并且直接与间接措辞的有效性根据提示策略而颠倒。漏洞复现准确性与代码复杂性呈非线性模式，在中等范围内达到峰值。我们的发现揭示了LLMs对模式回忆而非语义推理的依赖，这在其安全对齐中产生了显著的盲点，特别是对于被框定为合理专业任务的请求。", "summary": "本研究深入探讨了大型语言模型（LLMs）在被明确要求生成不安全代码时的行为。通过采用动态提示和反向提示的双重实验设计，并评估了Qwen2、Mistral和Gemma三个开源7B模型，发现所有模型都能频繁生成请求的漏洞。研究揭示了不同模型在漏洞生成正确性上的差异，以及用户角色和提示措辞对成功率的影响。结果表明，LLMs在安全对齐方面存在盲点，尤其是在处理看似合理的专业任务时，这源于它们对模式回忆的依赖而非语义理解。", "keywords": "大型语言模型, 漏洞生成, 安全对齐, 动态提示, 反向提示", "comments": "这项研究创新性地将LLM的漏洞生成能力从无意行为转向了“有意”生成，揭示了一个新的且直接的安全威胁。其采用双重提示策略和对用户角色、提示措辞等因素的细致分析，为理解LLM在安全领域的潜在风险提供了宝贵的洞察。研究结果强调了LLM在安全对齐上的不足，特别是其对模式匹配的过度依赖，这对于LLM的开发者和使用者都具有重要警示意义。"}}
{"id": "2507.17382", "title": "Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective", "authors": ["Hao Dai", "Jagmohan Chauhan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 6 figures. Forty-second International Conference on Machine Learning. 2025", "url": "http://arxiv.org/abs/2507.17382v1", "summary": "Continual Generalized Category Discovery (C-GCD) faces a critical challenge:\nincrementally learning new classes from unlabeled data streams while preserving\nknowledge of old classes. Existing methods struggle with catastrophic\nforgetting, especially when unlabeled data mixes known and novel categories. We\naddress this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,\nrevealing that covariance misalignment between old and new classes drives\nperformance degradation. Building on this insight, we propose Variational Bayes\nC-GCD (VB-CGCD), a novel framework that integrates variational inference with\ncovariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns\nclass distributions while suppressing pseudo-label noise via stochastic\nvariational updates. Experiments show VB-CGCD surpasses prior art by +15.21%\nwith the overall accuracy in the final session on standard benchmarks. We also\nintroduce a new challenging benchmark with only 10% labeled data and extended\nonline phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher\nthan state-of-the-art (38.55%), demonstrating its robust applicability across\ndiverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD", "comment": "20 pages, 6 figures. Forty-second International Conference on Machine\n  Learning. 2025", "pdf_url": "http://arxiv.org/pdf/2507.17382v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "持续泛化类别发现：从贝叶斯视角学习与遗忘", "tldr": "本文提出VB-CGCD，一个基于变分贝叶斯的新框架，通过解决协方差失配和抑制伪标签噪声来应对持续泛化类别发现中的灾难性遗忘问题，在标准基准和新挑战性基准上均显著优于现有方法。", "motivation": "持续泛化类别发现（C-GCD）面临的核心挑战是在从无标签数据流中增量学习新类别的同时，保留对旧类别的知识，现有方法在面对包含已知和新类别的无标签数据时，存在严重的灾难性遗忘问题。", "method": "通过贝叶斯视角分析C-GCD的遗忘动态，发现新旧类别间的协方差失配导致性能下降。基于此，提出了变分贝叶斯C-GCD (VB-CGCD) 框架，该框架将变分推断与协方差感知最近类均值分类相结合，并通过随机变分更新自适应地对齐类别分布，同时抑制伪标签噪声。", "result": "VB-CGCD在标准基准测试中，最终会话的整体准确率超越现有技术15.21%。在一个仅有10%标注数据且在线阶段更长的新挑战性基准上，VB-CGCD达到了67.86%的最终准确率，显著高于现有最佳方法（38.55%）。", "conclusion": "VB-CGCD通过解决协方差失配和伪标签噪声问题，有效应对了持续泛化类别发现中的灾难性遗忘，并在多个基准测试中展现出卓越的性能和鲁棒性。", "translation": "持续泛化类别发现（C-GCD）面临一个关键挑战：从无标签数据流中增量学习新类别，同时保留旧类别的知识。现有方法存在灾难性遗忘问题，尤其当无标签数据混合了已知和新类别时。我们通过贝叶斯视角分析C-GCD的遗忘动态来解决这个问题，揭示了新旧类别之间的协方差失配导致性能下降。基于此洞察，我们提出了变分贝叶斯C-GCD（VB-CGCD），一个新颖的框架，它将变分推断与协方差感知最近类均值分类相结合。VB-CGCD通过随机变分更新自适应地对齐类别分布，同时抑制伪标签噪声。实验表明，VB-CGCD在标准基准测试中，最终会话的整体准确率超越现有技术15.21%。我们还引入了一个新的挑战性基准，该基准仅有10%的标注数据且在线阶段更长，VB-CGCD达到了67.86%的最终准确率，显著高于现有最佳方法（38.55%），证明了其在不同场景下的鲁棒适用性。代码可在：https://github.com/daihao42/VB-CGCD 获取。", "summary": "该论文提出了持续泛化类别发现（C-GCD）中的一个关键挑战：在学习新类别的同时防止旧知识的遗忘。作者从贝叶斯视角分析发现协方差失配是性能下降的原因。为此，论文提出了VB-CGCD框架，该框架结合变分推断和协方差感知分类，通过自适应分布对齐和伪标签噪声抑制来解决问题。实验结果表明，VB-CGCD在标准和新的挑战性基准上均显著优于现有方法，展示了其在持续学习场景下的强大性能和鲁棒性。", "keywords": "持续泛化类别发现, 灾难性遗忘, 贝叶斯推断, 变分推断, 协方差失配", "comments": "本文通过引入贝叶斯视角深入分析了持续泛化类别发现中的灾难性遗忘问题，并创新性地提出了变分贝叶斯框架VB-CGCD，以解决新旧类别间的协方差失配和伪标签噪声。其在多个基准测试上的显著性能提升，特别是对新挑战性基准的优异表现，证明了该方法的有效性和普适性，对持续学习领域具有重要意义。"}}
{"id": "2505.01231", "title": "Correct Estimation of Higher-Order Spectra: From Theoretical Challenges to Practical Multi-Channel Implementation in SignalSnap", "authors": ["Markus Sifft", "Armin Ghorbanietemad", "Fabian Wagner", "Daniel Hägele"], "categories": ["physics.data-an", "eess.SP", "quant-ph"], "primary_category": "Subjects:       Data Analysis, Statistics and Probability (physics.data-an)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01231v2", "summary": "Higher-order spectra (Brillinger's polyspectra) offer powerful methods for\nsolving critical problems in signal processing and data analysis. Despite their\nsignificant potential, their practical use has remained limited due to\nunresolved mathematical issues in spectral estimation, including the absence of\nunbiased and consistent estimators and the high computational cost associated\nwith evaluating multidimensional spectra. Consequently, existing tools\nfrequently produce artifacts, no existing software library correctly implements\nBrillinger's cumulant-based trispectrum, or fail to scale effectively to\nreal-world data volumes, leaving crucial applications like multi-detector\nspectral analysis largely unexplored. In this paper, we revisit higher-order\nspectra from a modern perspective, addressing the root causes of their\nhistorical underuse. We reformulate higher-order spectral estimation using\nrecently derived multivariate k-statistics, yielding unbiased and consistent\nestimators that eliminate spurious artifacts and precisely align with\nBrillinger's theoretical definitions. Our methodology covers single- and\nmulti-channel spectral analysis up to the bispectrum (third order) and\ntrispectrum (fourth order), enabling robust investigations of inter-frequency\ncoupling, non-Gaussian behavior, and time-reversal symmetry breaking.\nAdditionally, we introduce quasi-polyspectra to uncover non-stationary,\ntime-dependent higher-order features. We implement these new estimators in\nSignalSnap, an open-source GPU-accelerated library capable of efficiently\nanalyzing datasets exceeding hundreds of gigabytes within minutes. In\napplications such as continuous quantum measurements, SignalSnap's rigorous\nestimators enable precise quantitative matching between experimental data and\ntheoretical models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01231v2", "cate": "physics.data-an", "date": "2025-05-02", "updated": "2025-07-23", "AI": {"title_translation": "高阶谱的正确估计：从理论挑战到SignalSnap中的实际多通道实现", "tldr": "提出了一种纠正高阶谱估计的新方法，解决了现有工具的偏差和计算问题，并在GPU加速的SignalSnap库中实现，可高效处理大数据。", "motivation": "高阶谱在信号处理和数据分析中具有巨大潜力，但由于缺乏无偏和一致的估计器、高计算成本以及现有工具产生伪影或无法有效扩展等未解决的数学问题，其实际应用受到了限制。", "method": "论文从现代视角重新审视了高阶谱，利用最近推导的多元k统计量重新构建了高阶谱估计，获得了无偏且一致的估计器，并消除了伪影。该方法涵盖了单通道和多通道分析，最高可达双谱和三谱。此外，还引入了准多谱以揭示非平稳、时变的高阶特征。这些新估计器已在开源、GPU加速的SignalSnap库中实现。", "result": "本研究获得了无偏、一致的估计器，消除了虚假伪影，并与Brillinger的理论定义精确对齐。该方法能够对频率间耦合、非高斯行为和时间反转对称性破缺进行稳健研究。SignalSnap库能够高效分析数百GB以上的数据集，并在连续量子测量等应用中实现了实验数据与理论模型之间的精确定量匹配。", "conclusion": "该研究通过提供无偏、一致且计算高效的高阶谱估计方法，解决了长期存在的理论和实践挑战，极大地扩展了高阶谱在信号处理和数据分析中的应用潜力。", "translation": "高阶谱（Brillinger的多谱）为解决信号处理和数据分析中的关键问题提供了强大的方法。尽管其潜力巨大，但由于谱估计中未解决的数学问题，其实际应用仍然有限，这些问题包括缺乏无偏和一致的估计器以及评估多维谱所需的高计算成本。因此，现有工具经常产生伪影，没有现有软件库能正确实现Brillinger基于累积量的三谱，或者无法有效扩展到真实世界的数据量，导致多探测器谱分析等关键应用在很大程度上未被探索。在本文中，我们从现代视角重新审视了高阶谱，解决了其长期未被充分利用的根本原因。我们利用最近推导的多元k统计量重新构建了高阶谱估计，得到了无偏且一致的估计器，消除了虚假伪影，并与Brillinger的理论定义精确对齐。我们的方法涵盖了最高达双谱（三阶）和三谱（四阶）的单通道和多通道谱分析，从而能够对频率间耦合、非高斯行为和时间反转对称性破缺进行稳健研究。此外，我们引入了准多谱以揭示非平稳、时变的高阶特征。我们在SignalSnap中实现了这些新估计器，SignalSnap是一个开源的GPU加速库，能够在几分钟内高效分析超过数百GB的数据集。在连续量子测量等应用中，SignalSnap的严谨估计器能够实现实验数据与理论模型之间的精确定量匹配。", "summary": "本论文解决了高阶谱估计中长期存在的数学和实践问题，这些问题导致现有方法产生伪影、缺乏无偏性且计算成本高昂。作者利用多元k统计量重新构建了高阶谱估计，提出了无偏、一致且能消除伪影的新方法，并与Brillinger的理论定义精确对齐。该方法支持单通道和多通道分析，并引入了准多谱以处理非平稳特征。所有新估计器均已在开源、GPU加速的SignalSnap库中实现，该库能够高效处理大规模数据集，并在实际应用中实现了实验数据与理论模型之间的高精度匹配。", "keywords": "高阶谱, k统计量, SignalSnap, 谱估计, 多通道分析", "comments": "这篇论文的创新点在于利用多元k统计量重新构建了高阶谱估计，解决了长期存在的无偏性和一致性问题，并消除了伪影。其重要性体现在提供了一个理论上更严谨、实践中更高效的解决方案，特别是通过SignalSnap的GPU加速实现，使得高阶谱分析能够应用于大规模真实世界数据，从而推动了信号处理和数据分析领域的发展。"}}
{"id": "2503.13844", "title": "Towards Detecting Persuasion on Social Media: From Model Development to Insights on Persuasion Strategies", "authors": ["Elyas Meguellati", "Stefano Civelli", "Pietro Bernardelle", "Shazia Sadiq", "Irwin King", "Gianluca Demartini"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.13844v2", "summary": "Political advertising plays a pivotal role in shaping public opinion and\ninfluencing electoral outcomes, often through subtle persuasive techniques\nembedded in broader propaganda strategies. Detecting these persuasive elements\nis crucial for enhancing voter awareness and ensuring transparency in\ndemocratic processes. This paper presents an integrated approach that bridges\nmodel development and real-world application through two interconnected\nstudies. First, we introduce a lightweight model for persuasive text detection\nthat achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3\nwhile requiring significantly fewer computational resources and training data\nthan existing methods. Second, we demonstrate the model's practical utility by\ncollecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,\npartially annotating a subset for persuasion, and fine-tuning the model to\nadapt from mainstream news to social media content. We then apply the\nfine-tuned model to label the remainder of the APA22 dataset, revealing\ndistinct patterns in how political campaigns leverage persuasion through\ndifferent funding strategies, word choices, demographic targeting, and temporal\nshifts in persuasion intensity as election day approaches. Our findings not\nonly underscore the necessity of domain-specific modeling for analyzing\npersuasion on social media but also show how uncovering these strategies can\nenhance transparency, inform voters, and promote accountability in digital\ncampaigns.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.13844v2", "cate": "cs.CL", "date": "2025-03-18", "updated": "2025-07-23", "AI": {"title_translation": "社交媒体劝说检测：从模型开发到劝说策略洞察", "tldr": "本文提出了一种轻量级劝说文本检测模型，在SemEval 2023任务中表现出色，并将其应用于澳大利亚联邦选举广告数据，揭示了政治竞选中的劝说策略模式。", "motivation": "政治广告在塑造公众舆论和影响选举结果方面发挥着关键作用，通常通过融入宣传策略中的微妙劝说技巧。检测这些劝说元素对于提高选民意识和确保民主过程的透明度至关重要。", "method": "本文提出了一个集成的两阶段方法：首先，开发了一个轻量级的劝说文本检测模型，该模型在SemEval 2023任务3的子任务3中达到了最先进的性能，同时需要更少的计算资源和训练数据。其次，通过收集澳大利亚联邦选举2022年Facebook广告（APA22）数据集，部分标注劝说内容，并对模型进行微调以适应社交媒体内容，然后应用微调后的模型标注剩余数据以揭示劝说模式。", "result": "所提出的轻量级模型在SemEval 2023任务3的子任务3中实现了最先进的性能，且计算资源和训练数据需求显著减少。模型应用于APA22数据集后，揭示了政治竞选活动如何通过不同的资金策略、词语选择、人口统计学定位以及临近选举日时劝说强度的时序变化来利用劝说的独特模式。", "conclusion": "研究结果不仅强调了对社交媒体上的劝说进行领域特定建模的必要性，而且表明揭示这些策略可以增强透明度，告知选民，并促进数字竞选的问责制。", "translation": "政治广告在塑造公众舆论和影响选举结果方面发挥着关键作用，通常通过嵌入在更广泛宣传策略中的微妙劝说技巧来实现。检测这些劝说元素对于提高选民意识和确保民主过程的透明度至关重要。本文提出了一个集成的两阶段方法，通过两个相互关联的研究，将模型开发与实际应用相结合。首先，我们引入了一个轻量级的劝说文本检测模型，该模型在SemEval 2023任务3的子任务3中达到了最先进的性能，同时比现有方法需要显著更少的计算资源和训练数据。其次，我们通过收集澳大利亚联邦选举2022年Facebook广告（APA22）数据集，部分标注了其中的劝说内容，并对模型进行了微调，使其能够从主流新闻内容适应到社交媒体内容。随后，我们将微调后的模型应用于标注APA22数据集的其余部分，揭示了政治竞选活动如何通过不同的资金策略、词语选择、人口统计学定位以及临近选举日时劝说强度的时序变化来利用劝说的独特模式。我们的研究结果不仅强调了对社交媒体上的劝说进行领域特定建模的必要性，而且表明揭示这些策略可以增强透明度，告知选民，并促进数字竞选的问责制。", "summary": "本文旨在检测社交媒体上的劝说，提出了一个轻量级劝说文本检测模型，该模型在SemEval 2023挑战赛中表现出色，并显著减少了资源需求。研究通过收集并分析澳大利亚联邦选举2022年Facebook广告数据集，将该模型应用于实际场景，揭示了政治竞选活动中劝说策略的独特模式，包括资金策略、词语选择、人口定位和时间变化。研究强调了领域特定建模的重要性，并展示了揭示这些策略如何提高数字竞选的透明度、选民知情度和问责制。", "keywords": "劝说检测, 社交媒体, 政治广告, 轻量级模型, SemEval 2023", "comments": "本文通过提出一个高效的轻量级模型来解决社交媒体劝说检测的挑战，并在实际数据集上进行了应用，不仅验证了模型的有效性，更深入地揭示了政治竞选中的劝说策略，具有重要的理论和实践意义。其创新之处在于结合了模型开发和实际应用，并强调了领域特定建模的必要性。"}}
{"id": "2507.17132", "title": "Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for Construction Robot", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17132v1", "summary": "With the rapid development of the construction industry, issues such as harsh\nworking environments, high-intensity and high-risk tasks, and labor shortages\nhave become increasingly prominent. This drives higher demands for construction\nrobots in terms of low energy consumption, high mobility, and high load\ncapacity. This paper focuses on the design and optimization of leg structures\nfor construction robots, aiming to improve their dynamic performance, reduce\nenergy consumption, and enhance load-bearing capabilities. Firstly, based on\nthe leg configuration of ants in nature, we design a structure for the robot's\nleg. Secondly, we propose a novel structural optimization method. Using the\nLagrangian approach, a dynamic model of the leg was established. Combining the\ndynamic model with the leg's motion trajectory, we formulated multiple dynamic\nevaluation metrics and conducted a comprehensive optimization study on the\ngeometric parameters of each leg segment. The results show that the optimized\nleg structure reduces peak joint torques and energy consumption by over 20%.\nFinally, dynamic simulation experiments were conducted using ADAMS. The results\ndemonstrate a significant reduction in the driving power of each joint after\noptimization, validating the effectiveness and rationality of the proposed\nstrategy. This study provides a theoretical foundation and technical support\nfor the design of heavy-load, high-performance construction robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17132v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "建筑机器人腿部机构的动力学建模与尺寸优化", "tldr": "本研究针对建筑机器人腿部结构进行设计与优化，通过拉格朗日动力学建模和几何参数优化，成功将关节峰值扭矩和能耗降低超20%，并通过ADAMS仿真验证了其有效性，为高负载、高性能建筑机器人设计提供了支持。", "motivation": "随着建筑行业的快速发展，恶劣的工作环境、高强度高风险任务以及劳动力短缺等问题日益突出，这促使对建筑机器人提出了更高的要求，包括低能耗、高机动性和高负载能力。", "method": "首先，借鉴自然界蚂蚁腿部构型，设计了机器人腿部结构。其次，提出了一种新颖的结构优化方法，利用拉格朗日方法建立腿部动力学模型，并结合运动轨迹，制定了多项动力学评价指标，对腿部各节段的几何参数进行了综合优化研究。最后，通过ADAMS进行动力学仿真实验验证。", "result": "优化后的腿部结构使关节峰值扭矩和能耗降低了20%以上。ADAMS动力学仿真实验结果表明，优化后各关节的驱动功率显著降低，验证了所提出策略的有效性和合理性。", "conclusion": "本研究为重载、高性能建筑机器人的设计提供了理论基础和技术支持。通过对腿部结构的优化，显著提升了机器人的动态性能，降低了能耗。", "translation": "随着建筑行业的快速发展，恶劣的工作环境、高强度高风险任务以及劳动力短缺等问题日益突出。这促使对建筑机器人提出了更高的要求，包括低能耗、高机动性和高负载能力。本文主要关注建筑机器人腿部结构的设计与优化，旨在提升其动力学性能，降低能耗，增强承载能力。首先，基于自然界蚂蚁腿部构型，设计了机器人腿部结构。其次，提出了一种新颖的结构优化方法。利用拉格朗日方法建立了腿部的动力学模型。结合动力学模型与腿部运动轨迹，制定了多项动力学评价指标，并对腿部各节段的几何参数进行了综合优化研究。结果表明，优化后的腿部结构使关节峰值扭矩和能耗降低了20%以上。最后，通过ADAMS进行了动力学仿真实验。结果表明，优化后各关节的驱动功率显著降低，验证了所提出策略的有效性和合理性。本研究为重载、高性能建筑机器人的设计提供了理论基础和技术支持。", "summary": "本研究针对建筑行业对高性能机器人的需求，提出了一种仿生腿部结构设计，并开发了一套基于拉格朗日动力学模型和多指标评价的结构优化方法。通过对腿部几何参数的优化，成功将机器人关节峰值扭矩和能耗降低了20%以上，并通过ADAMS仿真验证了优化策略的有效性，为重载、高性能建筑机器人的设计提供了关键技术支持。", "keywords": "建筑机器人, 腿部机构, 动力学建模, 尺寸优化, 能耗降低", "comments": "该论文创新性地借鉴了蚂蚁腿部构型进行仿生设计，并结合拉格朗日动力学建模和多指标优化，实现了机器人腿部结构在能耗和扭矩方面的显著优化。其方法严谨，并通过仿真验证了有效性，为解决建筑机器人面临的实际挑战提供了有价值的解决方案。未来可以考虑在实际样机上进行验证，以进一步评估其在复杂工况下的表现。"}}
{"id": "2507.17654", "title": "On Function-Correcting Codes in the Lee Metric", "authors": ["Gyanendra K. Verma", "Abhay Kumar Singh"], "categories": ["cs.IT", "cs.DM", "cs.IR", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17654v1", "summary": "Function-correcting codes are a coding framework designed to minimize\nredundancy while ensuring that specific functions or computations of encoded\ndata can be reliably recovered, even in the presence of errors. The choice of\nmetric is crucial in designing such codes, as it determines which computations\nmust be protected and how errors are measured and corrected. Previous work by\nLiu and Liu [6] studied function-correcting codes over $\\mathbb{Z}_{2^l},\\\nl\\geq 2$ using the homogeneous metric, which coincides with the Lee metric over\n$\\mathbb{Z}_4$. In this paper, we extend the study to codes over\n$\\mathbb{Z}_m,$ for any positive integer $m\\geq 2$ under the Lee metric and aim\nto determine their optimal redundancy. To achieve this, we introduce irregular\nLee distance codes and derive upper and lower bounds on the optimal redundancy\nby characterizing the shortest possible length of such codes. These general\nbounds are then simplified and applied to specific classes of functions,\nincluding Lee-local functions, Lee weight functions, and Lee weight\ndistribution functions, leading to improved some bounds compared to those of\nLiu and Liu [6] over $\\mathbb{Z}_4$ and generalize the other bounds over\n$\\mathbb{Z}_m$ in the Lee metric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17654v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "李度量下的函数纠错码", "tldr": "本文研究李度量下$\\mathbb{Z}_m$上的函数纠错码，引入不规则李距离码并推导其最优冗余的上下界，改进并推广了现有结果。", "motivation": "旨在扩展Liu和Liu[6]在齐次度量（与$\\mathbb{Z}_4$上的李度量一致）下对函数纠错码的研究，并确定在李度量下$\\mathbb{Z}_m$上函数纠错码的最优冗余。", "method": "引入了不规则李距离码，并通过刻画这类码的最短可能长度来推导了最优冗余的上下界。然后将这些通用界限简化并应用于特定的函数类别。", "result": "导出的通用界限被简化并应用于李局部函数、李权重函数和李权重分布函数等特定函数类别，与Liu和Liu[6]在$\\mathbb{Z}_4$上的结果相比，改进了一些界限，并推广了在李度量下$\\mathbb{Z}_m$上的其他界限。", "conclusion": "本文成功地将李度量下函数纠错码的研究扩展到$\\mathbb{Z}_m$上，并通过引入不规则李距离码，为这类码的最优冗余提供了新的上下界，并对特定函数类别的界限进行了改进和推广。", "translation": "函数纠错码是一种编码框架，旨在最大程度地减少冗余，同时确保即使存在错误，编码数据的特定函数或计算也能可靠地恢复。度量的选择在此类码的设计中至关重要，因为它决定了必须保护哪些计算以及如何测量和纠正错误。Liu和Liu[6]先前的研究在齐次度量（与$\\mathbb{Z}_4$上的李度量一致）下研究了$\\mathbb{Z}_{2^l}, l\\geq 2$上的函数纠错码。在本文中，我们将研究扩展到在李度量下$\\mathbb{Z}_m$（对于任何正整数$m\\geq 2$）上的码，旨在确定它们的最优冗余。为了实现这一目标，我们引入了不规则李距离码，并通过刻画这类码的最短可能长度来推导了最优冗余的上下界。然后，这些通用界限被简化并应用于特定的函数类别，包括李局部函数、李权重函数和李权重分布函数，与Liu和Liu[6]在$\\mathbb{Z}_4$上的结果相比，改进了一些界限，并推广了在李度量下$\\mathbb{Z}_m$上的其他界限。", "summary": "本文在李度量下对$\\mathbb{Z}_m$上的函数纠错码进行了深入研究，旨在确定其最优冗余。通过引入不规则李距离码，作者推导了最优冗余的上下界，并通过分析其最短可能长度来建立这些界限。这些通用界限随后被应用于李局部函数、李权重函数和李权重分布函数等特定函数类别，从而在$\\mathbb{Z}_4$上改进了现有的一些界限，并在$\\mathbb{Z}_m$上推广了李度量下的其他界限。", "keywords": "函数纠错码, 李度量, 最优冗余, 不规则李距离码, 错误纠正", "comments": "本文的创新点在于将李度量下的函数纠错码的研究范围从特定的$\\mathbb{Z}_4$扩展到更一般的$\\mathbb{Z}_m$，并通过引入“不规则李距离码”这一新概念，为确定最优冗余提供了新的理论工具。其重要性在于为函数纠错码的设计提供了更普适的理论基础和更紧密的冗余界限，这对于在存在错误的情况下可靠地恢复计算至关重要。"}}
{"id": "2507.17327", "title": "CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits", "authors": ["Chao He", "Jianqiang Ren", "Jianjing Xiang", "Xiejie Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17327v1", "summary": "With the rapid advancement of large foundation models, AIGC, cloud rendering,\nand real-time motion capture technologies, digital humans are now capable of\nachieving synchronized facial expressions and body movements, engaging in\nintelligent dialogues driven by natural language, and enabling the fast\ncreation of personalized avatars. While current mainstream approaches to\ndigital humans primarily focus on 3D models and 2D video-based representations,\ninteractive 2D cartoon-style digital humans have received relatively less\nattention. Compared to 3D digital humans that require complex modeling and high\nrendering costs, and 2D video-based solutions that lack flexibility and\nreal-time interactivity, 2D cartoon-style Live2D models offer a more efficient\nand expressive alternative. By simulating 3D-like motion through layered\nsegmentation without the need for traditional 3D modeling, Live2D enables\ndynamic and real-time manipulation. In this technical report, we present\nCartoonAlive, an innovative method for generating high-quality Live2D digital\nhumans from a single input portrait image. CartoonAlive leverages the shape\nbasis concept commonly used in 3D face modeling to construct facial blendshapes\nsuitable for Live2D. It then infers the corresponding blendshape weights based\non facial keypoints detected from the input image. This approach allows for the\nrapid generation of a highly expressive and visually accurate Live2D model that\nclosely resembles the input portrait, within less than half a minute. Our work\nprovides a practical and scalable solution for creating interactive 2D cartoon\ncharacters, opening new possibilities in digital content creation and virtual\ncharacter animation. The project homepage is\nhttps://human3daigc.github.io/CartoonAlive_webpage/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17327v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CartoonAlive：从单张肖像画走向富有表现力的Live2D建模", "tldr": "CartoonAlive是一种创新方法，可以从单张肖像图像在不到半分钟内生成高质量且富有表现力的Live2D数字人物。", "motivation": "尽管当前主流的数字人方法主要集中在3D模型和基于2D视频的表示上，但交互式2D卡通风格的数字人受到的关注相对较少。与需要复杂建模和高渲染成本的3D数字人以及缺乏灵活性和实时交互性的基于2D视频的解决方案相比，2D卡通风格的Live2D模型提供了一种更高效和富有表现力的替代方案。", "method": "CartoonAlive利用3D人脸建模中常用的形状基概念来构建适用于Live2D的面部混合形状，然后根据输入图像中检测到的面部关键点推断相应的混合形状权重。", "result": "CartoonAlive可以在不到半分钟内快速生成一个高度表现力且视觉准确的Live2D模型，该模型与输入肖像非常相似。", "conclusion": "这项工作为创建交互式2D卡通角色提供了一个实用且可扩展的解决方案，为数字内容创作和虚拟角色动画开辟了新的可能性。", "translation": "随着大型基础模型、AIGC、云渲染和实时运动捕捉技术的迅速发展，数字人现在能够实现同步的面部表情和身体动作，进行由自然语言驱动的智能对话，并实现个性化头像的快速创建。尽管当前主流的数字人方法主要集中在3D模型和基于2D视频的表示上，但交互式2D卡通风格的数字人受到的关注相对较少。与需要复杂建模和高渲染成本的3D数字人以及缺乏灵活性和实时交互性的基于2D视频的解决方案相比，2D卡通风格的Live2D模型提供了一种更高效和富有表现力的替代方案。通过分层分割模拟3D般运动而无需传统的3D建模，Live2D实现了动态和实时操作。在本技术报告中，我们介绍了CartoonAlive，这是一种从单张输入肖像图像生成高质量Live2D数字人物的创新方法。CartoonAlive利用3D人脸建模中常用的形状基概念来构建适用于Live2D的面部混合形状。然后，它根据输入图像中检测到的面部关键点推断相应的混合形状权重。这种方法可以在不到半分钟内快速生成一个高度表现力且视觉准确的Live2D模型，该模型与输入肖像非常相似。我们的工作为创建交互式2D卡通角色提供了一个实用且可扩展的解决方案，为数字内容创作和虚拟角色动画开辟了新的可能性。项目主页是https://human3daigc.github.io/CartoonAlive_webpage/。", "summary": "本文介绍了CartoonAlive，一种从单张肖像图像快速生成高质量、富有表现力2D卡通风格Live2D数字人物的创新方法。该方法利用3D人脸建模中的形状基概念构建Live2D面部混合形状，并通过检测面部关键点推断混合形状权重，从而实现高效且视觉准确的模型生成。这为交互式2D卡通角色的创建提供了实用且可扩展的解决方案。", "keywords": "Live2D建模, 2D卡通数字人, 单张肖像, 形状基, 实时生成", "comments": "CartoonAlive的创新点在于它解决了2D卡通风格数字人创建中效率和表现力的痛点，通过结合3D人脸建模的形状基概念与2D Live2D技术，实现了从单张图像快速生成高质量模型。其重要性在于为数字内容创作和虚拟角色动画提供了新的可能性，降低了2D数字人制作的门槛。"}}
{"id": "2507.16267", "title": "SFNet: A Spatial-Frequency Domain Deep Learning Network for Efficient Alzheimer's Disease Diagnosis", "authors": ["Xinyue Yang", "Meiliang Liu", "Yunfang Xu", "Xiaoxiao Yang", "Zhengye Si", "Zijin Li", "Zhiwen Zhao"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16267v2", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\npredominantly affects the elderly population and currently has no cure.\nMagnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is\nessential for the early diagnosis of AD. MRI inherently contains both spatial\nand frequency information, as raw signals are acquired in the frequency domain\nand reconstructed into spatial images via the Fourier transform. However, most\nexisting AD diagnostic models extract features from a single domain, limiting\ntheir capacity to fully capture the complex neuroimaging characteristics of the\ndisease. While some studies have combined spatial and frequency information,\nthey are mostly confined to 2D MRI, leaving the potential of dual-domain\nanalysis in 3D MRI unexplored. To overcome this limitation, we propose\nSpatio-Frequency Network (SFNet), the first end-to-end deep learning framework\nthat simultaneously leverages spatial and frequency domain information to\nenhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense\nconvolutional network to extract local spatial features and a global frequency\nmodule to capture global frequency-domain representations. Additionally, a\nnovel multi-scale attention module is proposed to further refine spatial\nfeature extraction. Experiments on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that SFNet outperforms existing baselines\nand reduces computational overhead in classifying cognitively normal (CN) and\nAD, achieving an accuracy of 95.1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16267v2", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "SFNet：一种用于高效阿尔茨海默病诊断的时空-频率域深度学习网络", "tldr": "SFNet是一种新的深度学习网络，它同时利用MRI的时空和频率信息，实现了高效的3D阿尔茨海默病诊断，准确率达到95.1%。", "motivation": "阿尔茨海默病（AD）的早期诊断至关重要，磁共振成像（MRI）是一种关键的非侵入性成像技术。然而，大多数现有AD诊断模型从单一域（空间或频率）提取特征，未能充分利用MRI固有的时空和频率信息，且在3D MRI双域分析方面探索不足，限制了它们捕捉复杂神经影像学特征的能力。", "method": "本文提出了SFNet（Spatio-Frequency Network），这是一个端到端深度学习框架，首次同时利用3D MRI的空间和频率域信息来增强AD诊断。SFNet集成了增强型密集卷积网络以提取局部空间特征，以及一个全局频率模块以捕获全局频率域表示。此外，还引入了一个新颖的多尺度注意力模块以进一步优化空间特征提取。", "result": "在阿尔茨海默病神经影像学倡议（ADNI）数据集上的实验表明，SFNet在认知正常（CN）和AD分类任务中优于现有基线模型，并且降低了计算开销，实现了95.1%的准确率。", "conclusion": "SFNet通过同时利用3D MRI的时空和频率域信息，显著提高了阿尔茨海默病诊断的效率和准确性，证明了其在早期诊断和临床应用中的潜力。", "translation": "阿尔茨海默病（AD）是一种进行性神经退行性疾病，主要影响老年人群，目前尚无治愈方法。磁共振成像（MRI）作为一种非侵入性成像技术，对AD的早期诊断至关重要。MRI本身包含空间和频率信息，因为原始信号在频率域中采集，并通过傅里叶变换重建为空间图像。然而，大多数现有AD诊断模型从单一域提取特征，限制了它们充分捕捉疾病复杂神经影像学特征的能力。虽然一些研究结合了空间和频率信息，但它们大多局限于2D MRI，导致3D MRI中双域分析的潜力尚未被探索。为了克服这一限制，我们提出了时空-频率网络（SFNet），这是第一个端到端的深度学习框架，它同时利用空间和频率域信息来增强基于3D MRI的AD诊断。SFNet集成了增强型密集卷积网络以提取局部空间特征，以及一个全局频率模块以捕获全局频率域表示。此外，还提出了一个新颖的多尺度注意力模块以进一步细化空间特征提取。在阿尔茨海默病神经影像学倡议（ADNI）数据集上的实验表明，SFNet在认知正常（CN）和AD分类方面优于现有基线模型并降低了计算开销，实现了95.1%的准确率。", "summary": "本文提出了SFNet，一个创新的端到端深度学习框架，专为基于3D MRI的阿尔茨海默病诊断设计。SFNet首次同时利用MRI固有的空间和频率域信息，通过结合增强型密集卷积网络、全局频率模块和多尺度注意力模块来全面捕捉疾病特征。实验结果显示，SFNet在ADNI数据集上对AD和认知正常病例的分类准确率达到95.1%，优于现有方法并降低了计算成本，展现了其在AD早期诊断中的显著优势。", "keywords": "阿尔茨海默病诊断, 深度学习, 空间-频率域, 3D MRI, SFNet", "comments": "本文的创新点在于首次提出了一个端到端深度学习框架SFNet，能够同时利用3D MRI的空间和频率域信息进行阿尔茨海默病诊断，有效解决了现有模型单一域或2D局限性的问题。其结合局部空间特征提取和全局频率域表示的方法，以及引入多尺度注意力模块，提升了特征捕捉的全面性和准确性。这项工作为AD的早期、高效诊断提供了新的视角和强大的工具，具有重要的临床应用潜力。"}}
{"id": "2108.03776", "title": "An immersed $CR$-$P_0$ element for Stokes interface problems and the optimal convergence analysis", "authors": ["Haifeng Ji", "Feng Wang", "Jinru Chen", "Zhilin Li"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2108.03776v2", "summary": "This paper presents and analyzes an immersed finite element (IFE) method for\nsolving Stokes interface problems with a piecewise constant viscosity\ncoefficient that has a jump across the interface. In the method, the\ntriangulation does not need to fit the interface and the IFE spaces are\nconstructed from the traditional $CR$-$P_0$ element with modifications near the\ninterface according to the interface jump conditions. We prove that the IFE\nbasis functions are unisolvent on arbitrary interface elements and the IFE\nspaces have the optimal approximation capabilities, although the proof is\nchallenging due to the coupling of the velocity and the pressure. The stability\nand the optimal error estimates of the proposed IFE method are also derived\nrigorously. The constants in the error estimates are shown to be independent of\nthe interface location relative to the triangulation. Numerical examples are\nprovided to verify the theoretical results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2108.03776v2", "cate": "math.NA", "date": "2021-08-09", "updated": "2025-07-23", "AI": {"title_translation": "用于Stokes界面问题的浸入式$CR$-$P_0$单元及其最优收敛性分析", "tldr": "本文提出并分析了一种用于解决具有跳跃粘度系数的Stokes界面问题的浸入式有限元(IFE)方法，并对其稳定性、最优逼近能力和误差估计进行了严格证明和数值验证。", "motivation": "解决具有分段常数粘度系数且在界面处存在跳跃的Stokes界面问题。", "method": "提出了一种浸入式有限元(IFE)方法。该方法基于传统的$CR$-$P_0$单元，并在界面附近根据界面跳跃条件进行修改，其三角剖分无需与界面吻合。", "result": "证明了IFE基函数在任意界面单元上是唯一可解的，IFE空间具有最优逼近能力。所提出的IFE方法的稳定性和最优误差估计被严格推导，且误差估计中的常数与界面相对于三角剖分的位置无关。数值例子验证了理论结果。", "conclusion": "所提出的浸入式有限元方法对于Stokes界面问题是稳定且最优收敛的，并且其性能不受界面位置的影响。", "translation": "本文提出并分析了一种用于求解具有分段常数粘度系数且在界面处存在跳跃的Stokes界面问题的浸入式有限元(IFE)方法。在该方法中，三角剖分无需与界面吻合，IFE空间由传统的$CR$-$P_0$单元根据界面跳跃条件在界面附近进行修改而构建。我们证明了IFE基函数在任意界面单元上是唯一可解的，并且IFE空间具有最优逼近能力，尽管由于速度和压力的耦合使得证明具有挑战性。所提出的IFE方法的稳定性和最优误差估计也得到了严格推导。误差估计中的常数被证明与界面相对于三角剖分的位置无关。提供了数值例子来验证理论结果。", "summary": "本文提出了一种基于修改后的$CR$-$P_0$单元的浸入式有限元(IFE)方法，用于解决具有跳跃粘度系数的Stokes界面问题。该方法允许三角剖分不拟合界面。研究严格证明了该IFE基函数的唯一可解性、IFE空间的最优逼近能力以及方法的稳定性与最优误差估计，并指出误差常数与界面位置无关。数值实验验证了理论结果。", "keywords": "浸入式有限元, Stokes界面问题, $CR$-$P_0$单元, 最优收敛性, 误差估计", "comments": "本文的创新点在于提出了一个基于修改版$CR$-$P_0$单元的浸入式有限元方法来处理Stokes界面问题，特别是在处理界面跳跃条件和无需网格拟合界面方面的灵活性。其重要性体现在对IFE方法严格的理论分析，包括基函数的唯一可解性、空间的最优逼近能力以及误差估计，并克服了速度和压力耦合带来的证明挑战。这为解决复杂流体界面问题提供了一个鲁棒且高效的数值工具。"}}
{"id": "2507.17477", "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17477v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17477v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大型语言模型的不确定性驱动自适应自对齐框架", "tldr": "本文提出了一个名为不确定性驱动的自适应自对齐（UDASA）框架，旨在无需人工标注的情况下，通过量化和利用模型输出的不确定性，实现大型语言模型与人类意图和安全规范的高质量对齐，并在多项任务中表现优于现有方法。", "motivation": "在缺乏人工标注的情况下，实现大型语言模型（LLM）与人类意图和安全规范的高质量对齐仍然是一个基本挑战。", "method": "UDASA框架首先为每个输入生成多个响应，并在语义、事实性和价值对齐三个维度上量化输出不确定性。基于这些不确定性分数，框架构建偏好对，并根据不确定性差异将训练样本分为保守、中等和探索三个阶段。模型随后在这些阶段中逐步优化。", "result": "实验结果表明，UDASA在多个对齐任务（包括无害性、有用性、真实性和受控情感生成）上优于现有对齐方法，显著提高了模型性能。", "conclusion": "本文提出的UDASA框架能够通过不确定性驱动的自适应自对齐方法，在没有人工标注的情况下有效提高大型语言模型与人类意图和安全规范的对齐质量。", "translation": "大型语言模型（LLM）在指令遵循和通用推理方面取得了显著进展。然而，在没有人为标注的情况下实现与人类意图和安全规范的高质量对齐仍然是一个基本挑战。在这项工作中，我们提出了一个不确定性驱动的自适应自对齐（UDASA）框架，旨在以全自动方式改进LLM对齐。UDASA首先为每个输入生成多个响应，并在语义、事实性和价值对齐三个维度上量化输出不确定性。基于这些不确定性分数，该框架构建偏好对，并根据其不确定性差异将训练样本分为保守、中等和探索三个阶段。然后模型在这些阶段中逐步优化。此外，我们进行了一系列初步研究，以验证核心设计假设，并为所提出的框架提供了强有力的经验动机。实验结果表明，UDASA在多个任务（包括无害性、有用性、真实性和受控情感生成）上优于现有对齐方法，显著提高了模型性能。", "summary": "本文提出了一个名为不确定性驱动的自适应自对齐（UDASA）框架，旨在解决大型语言模型在无人标注下实现与人类意图和安全规范高质量对齐的挑战。UDASA通过生成多响应并量化语义、事实性和价值对齐的不确定性来构建偏好对，并分阶段（保守、中等、探索）优化模型。实验证明，UDASA在多种对齐任务上显著优于现有方法，提升了模型性能。", "keywords": "大型语言模型, 自对齐, 不确定性, 人类对齐, 无标注学习", "comments": "UDASA的创新点在于其完全自动化的自对齐机制，通过量化不确定性来指导模型优化，避免了对人工标注的依赖。这种方法在提高LLM对齐质量方面具有重要意义，尤其是在数据标注成本高昂的背景下。其分阶段优化的策略也值得关注。"}}
{"id": "2507.17445", "title": "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17445v1", "summary": "Detecting diverse objects within complex indoor 3D point clouds presents\nsignificant challenges for robotic perception, particularly with varied object\nshapes, clutter, and the co-existence of static and dynamic elements where\ntraditional bounding box methods falter. To address these limitations, we\npropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor\nmobile robots.\n  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles\nnaturally occlusions and provides a consistent top-down view aiding to\ndistinguish static obstacles from dynamic agents. The obtained 2D BEV results\nis directly usable to downstream robotic tasks like navigation, motion\nprediction, and planning. Our architecture utilizes an axis compact encoder and\na window-based backbone to extract rich spatial features from this BEV map. A\nquery-based decoder head then employs learned object queries to concurrently\npredict object classes and instance masks in the BEV space. This mask-centric\nformulation effectively captures the footprint of both static and dynamic\nobjects regardless of their shape, offering a robust alternative to bounding\nbox regression. We demonstrate the effectiveness of IndoorBEV on a custom\nindoor dataset featuring diverse object classes including static objects\n  and dynamic elements like robots and miscellaneous items, showcasing its\npotential for robust indoor scene understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17445v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "IndoorBEV: 室内场景中基于掩码预测的联合目标检测与足迹补全，用于鸟瞰图感知", "tldr": "IndoorBEV是一种新的基于掩码的鸟瞰图方法，用于在室内场景中同时检测静态和动态物体并完成其足迹，克服了传统边界框方法的局限性。", "motivation": "传统边界框方法在复杂的室内3D点云中检测多样化对象时面临挑战，尤其是在对象形状多变、杂乱以及静态和动态元素并存的情况下。", "method": "提出IndoorBEV，一种新颖的基于掩码的鸟瞰图（BEV）方法。它将3D场景投影到2D BEV网格中，利用轴紧凑编码器和基于窗口的骨干网络提取空间特征。查询式解码器头使用学习到的对象查询来同时预测BEV空间中的对象类别和实例掩码，以捕捉物体的足迹。", "result": "在包含静态物体和机器人、杂项等动态元素的自定义室内数据集上，IndoorBEV展示了其有效性，证明了其在鲁棒室内场景理解方面的潜力。", "conclusion": "IndoorBEV通过其掩码中心的公式，提供了一种鲁棒的替代方案来解决传统边界框方法的局限性，有效捕捉了室内场景中静态和动态物体的足迹，对下游机器人任务有直接用处。", "translation": "在复杂的室内3D点云中检测各种物体对机器人感知提出了重大挑战，特别是当物体形状多样、环境杂乱以及静态和动态元素并存时，传统边界框方法在此类情况下表现不佳。为了解决这些局限性，我们提出了一种名为IndoorBEV的新型基于掩码的鸟瞰图（BEV）方法，专为室内移动机器人设计。\n在BEV方法中，3D场景被投影到2D BEV网格中，这自然地处理了遮挡问题，并提供了一致的俯视图，有助于区分静态障碍物和动态代理。获得的2D BEV结果可以直接用于导航、运动预测和规划等下游机器人任务。我们的架构利用一个轴紧凑编码器和一个基于窗口的骨干网络从BEV图中提取丰富的空间特征。然后，一个基于查询的解码器头部使用学习到的对象查询来同时预测BEV空间中的对象类别和实例掩码。这种以掩码为中心的公式有效地捕捉了静态和动态物体的足迹，无论其形状如何，为边界框回归提供了一种鲁棒的替代方案。我们在一个自定义的室内数据集上展示了IndoorBEV的有效性，该数据集包含多样化的对象类别，包括静态物体和机器人、杂项等动态元素，展示了其在鲁棒室内场景理解方面的潜力。", "summary": "IndoorBEV是一种针对室内移动机器人提出的新型基于掩码的鸟瞰图感知方法。它通过将3D点云投影到2D BEV网格，并利用编码器-解码器架构（包含轴紧凑编码器、窗口骨干网络和查询式解码器头）来同时预测对象类别和实例掩码，从而有效地捕捉静态和动态对象的足迹，克服了传统边界框在复杂室内场景中的局限性。该方法可直接用于机器人导航等下游任务，并在自定义室内数据集上验证了其在鲁棒场景理解方面的有效性。", "keywords": "室内感知, 鸟瞰图, 掩码预测, 物体检测, 足迹补全", "comments": "本文提出了一种创新的基于掩码的BEV方法IndoorBEV，有效地解决了传统边界框在复杂室内环境中对异形物体和动静态元素检测的不足。其核心创新在于采用掩码预测来捕捉物体足迹，这对于需要精确空间理解的机器人下游任务（如导航、规划）具有重要意义。该方法提高了室内场景感知的鲁棒性。"}}
{"id": "2507.17325", "title": "Integrating Grid impedance estimation method into Advanced Angle Estimation Kalman Filter in GFL inverter", "authors": ["Phuoc Sang Nguyen", "Ghavameddin Nourbakhsh", "Gerard Ledwich"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17325v1", "summary": "The growing integration of power electronic converter-interfaced distributed\nenergy resources into modern power systems presents significant challenges for\nsystem monitoring, protection, and control. Grid impedance plays a critical\nrole in the operation and stability assessment of grid-connected inverter\nsystems. This study presents a real-time grid impedance estimation method based\non the Discrete Fourier Transform. The proposed method is integrated with the\nAdvanced Angle Estimation Kalman Filter using a Linear Quadratic Regulator\ncurrent controller (AAEKF-LQR), assisting the use of impedance information for\naccurate instantaneous phase angle estimation. Simulation results confirm that\nthe proposed impedance estimation method interacts effectively with the\nAAEKF-LQR controller, maintaining stable system performance under weak grid\nconditions. The approach also demonstrates the ability to deliver fast and\naccurate impedance estimation during operational variations in grid conditions,\nthereby supporting stable inverter operation.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17325v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "将电网阻抗估计方法集成到GFL逆变器的高级角度估计卡尔曼滤波器中", "tldr": "本文提出了一种基于离散傅里叶变换的实时电网阻抗估计方法，并将其与高级角度估计卡尔曼滤波器（AAEKF-LQR）结合，以在弱电网条件下实现稳定的逆变器运行和准确的瞬时相角估计。", "motivation": "现代电力系统中，电力电子变换器接口的分布式能源资源日益增多，给系统监测、保护和控制带来了重大挑战。电网阻抗在并网逆变器系统的运行和稳定性评估中起着关键作用，因此需要准确的实时电网阻抗估计。", "method": "本研究提出了一种基于离散傅里叶变换（DFT）的实时电网阻抗估计方法。该方法与使用线性二次调节器电流控制器（LQR）的高级角度估计卡尔曼滤波器（AAEKF-LQR）相结合，以利用阻抗信息进行准确的瞬时相角估计。", "result": "仿真结果证实，所提出的阻抗估计方法与AAEKF-LQR控制器有效互动，在弱电网条件下保持了稳定的系统性能。该方法还展示了在电网条件操作变化期间提供快速准确的阻抗估计的能力，从而支持逆变器的稳定运行。", "conclusion": "所提出的电网阻抗估计方法与AAEKF-LQR控制器相结合，能够有效支持并网逆变器在弱电网条件下的稳定运行，并提供快速准确的阻抗估计。", "translation": "电力电子变换器接口的分布式能源日益融入现代电力系统，对系统监测、保护和控制提出了重大挑战。电网阻抗在并网逆变器系统的运行和稳定性评估中扮演着关键角色。本研究提出了一种基于离散傅里叶变换的实时电网阻抗估计方法。所提出的方法与使用线性二次调节器电流控制器（LQR）的高级角度估计卡尔曼滤波器（AAEKF-LQR）相结合，辅助利用阻抗信息进行准确的瞬时相角估计。仿真结果证实，所提出的阻抗估计方法与AAEKF-LQR控制器有效互动，在弱电网条件下保持了稳定的系统性能。该方法还展示了在电网条件操作变化期间提供快速准确的阻抗估计的能力，从而支持逆变器的稳定运行。", "summary": "本研究提出了一种将基于离散傅里叶变换的实时电网阻抗估计方法与高级角度估计卡尔曼滤波器（AAEKF-LQR）相结合的新型方案，旨在解决并网逆变器在弱电网环境下的稳定运行和精确相角估计问题。仿真结果表明，该集成方法能够有效协同工作，在弱电网条件下维持系统稳定，并快速准确地估计电网阻抗，从而支持逆变器的稳定运行。", "keywords": "电网阻抗估计, 卡尔曼滤波器, GFL逆变器, 弱电网, 相角估计", "comments": "本文的创新点在于将实时电网阻抗估计与高级角度估计卡尔曼滤波器（AAEKF-LQR）相结合，以应对弱电网条件下并网逆变器稳定运行的挑战。这种集成方法提高了瞬时相角估计的准确性，并增强了系统在复杂电网条件下的鲁棒性。其重要性体现在为日益增长的分布式能源并网提供了更可靠的控制策略。"}}
{"id": "2504.10352", "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "authors": ["Yifan Yang", "Shujie Liu", "Jinyu Li", "Yuxuan Hu", "Haibin Wu", "Hui Wang", "Jianwei Yu", "Lingwei Meng", "Haiyang Sun", "Yanqing Liu", "Yan Lu", "Kai Yu", "Xie Chen"], "categories": ["eess.AS", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted in ACM MM 2025", "url": "http://arxiv.org/abs/2504.10352v2", "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information.Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://microsoft.com/research/project/vall-e-x/palle.", "comment": "Accepted in ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2504.10352v2", "cate": "eess.AS", "date": "2025-04-14", "updated": "2025-07-23", "AI": {"title_translation": "伪自回归神经编解码语言模型用于高效零样本文本到语音合成", "tldr": "提出了一种伪自回归（PAR）编解码语言模型PALLE，它结合了自回归和非自回归模型的优点，实现了高质量、高相似度、高可懂度且推理速度快十倍的零样本文本到语音合成。", "motivation": "现有的零样本文本到语音（TTS）系统面临困境：自回归（AR）模型生成速度慢且缺乏持续时间控制，而非自回归（NAR）模型缺乏时间建模且设计复杂。", "method": "本文提出了一种新颖的伪自回归（PAR）编解码语言建模方法，它统一了AR和NAR建模，结合了AR的显式时间建模和NAR的并行生成，在固定时间步生成动态长度的跨度。在此基础上，提出了PALLE，一个两阶段TTS系统：第一阶段PAR沿着时间维度逐步生成语音标记，每一步并行预测所有位置但只保留最左侧的跨度；第二阶段，低置信度标记被并行迭代细化，利用全局上下文信息。", "result": "PALLE在LibriTTS上训练，在LibriSpeech test-clean数据集上，在语音质量、说话人相似性和可懂度方面优于包括F5-TTS、E2-TTS和MaskGCT在内的大规模数据训练的最新系统，同时实现了高达十倍的推理速度。", "conclusion": "伪自回归（PAR）编解码语言模型PALLE有效解决了零样本TTS系统中自回归和非自回归模型的缺点，实现了在语音质量、说话人相似性、可懂度方面超越现有SOTA系统，并大幅提升了推理速度。", "translation": "最近的零样本文本到语音（TTS）系统面临一个共同的困境：自回归（AR）模型生成速度慢且缺乏持续时间可控性，而非自回归（NAR）模型缺乏时间建模并且通常需要复杂的设计。在本文中，我们引入了一种新颖的伪自回归（PAR）编解码语言建模方法，它统一了AR和NAR建模。PAR结合了AR的显式时间建模和NAR的并行生成，在固定时间步生成动态长度的跨度。基于PAR，我们提出了PALLE，一个两阶段TTS系统，利用PAR进行初始生成，然后进行NAR细化。在第一阶段，PAR沿着时间维度逐步生成语音标记，每一步并行预测所有位置但只保留最左侧的跨度。在第二阶段，低置信度标记被并行迭代细化，利用全局上下文信息。实验表明，PALLE在LibriTTS上训练，在LibriSpeech test-clean数据集上，在语音质量、说话人相似性和可懂度方面优于包括F5-TTS、E2-TTS和MaskGCT在内的大规模数据训练的最新系统，同时实现了高达十倍的推理速度。音频样本可在https://microsoft.com/research/project/vall-e-x/palle获取。", "summary": "本文提出了一种新颖的伪自回归（PAR）编解码语言建模方法，旨在解决零样本文本到语音（TTS）系统中自回归（AR）和非自回归（NAR）模型的缺点。PAR结合了AR的显式时间建模和NAR的并行生成，并在固定时间步生成动态长度的跨度。基于PAR，作者提出了PALLE，一个两阶段TTS系统，通过PAR进行初始生成，再通过NAR进行细化。实验证明，PALLE在语音质量、说话人相似性和可懂度方面超越了现有的SOTA系统，并实现了高达十倍的推理速度。", "keywords": "零样本文本到语音, 伪自回归, 编解码语言模型, PALLE, 语音合成", "comments": "该论文的创新点在于提出了伪自回归（PAR）编解码语言模型，有效地融合了自回归和非自回归模型的优点，解决了零样本TTS领域长期存在的生成速度与建模能力之间的矛盾。PALLE系统通过两阶段设计，不仅提升了语音合成的质量和相似度，还大幅提高了推理速度，这对于实际应用具有重要意义。"}}
{"id": "2507.17133", "title": "BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs", "authors": ["Jianmin Hu", "Minxian Xu", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.17133v1", "summary": "In recent years, the Mixture-of-Experts (MoE) architecture has been widely\napplied to large language models (LLMs), providing a promising solution that\nactivates only a subset of the model's parameters during computation, thereby\nreducing overall memory requirements and allowing for faster inference compared\nto dense models. Despite these advantages, existing systems still face issues\nof low efficiency due to static model placement and lack of dynamic workloads\nadaptation. This leads to suboptimal resource utilization and increased\nlatency, especially during bursty requests periods.\n  To address these challenges, this paper introduces BrownoutServe, a novel\nserving framework designed to optimize inference efficiency and maintain\nservice reliability for MoE-based LLMs under dynamic computational demands and\ntraffic conditions. BrownoutServe introduces \"united experts\" that integrate\nknowledge from multiple experts, reducing the times of expert access and\ninference latency. Additionally, it proposes a dynamic brownout mechanism to\nadaptively adjust the processing of certain tokens, optimizing inference\nperformance while guaranteeing service level objectives (SLOs) are met. Our\nevaluations show the effectiveness of BrownoutServe under various workloads: it\nachieves up to 2.07x throughput improvement compared to vLLM and reduces SLO\nviolations by 90.28%, showcasing its robustness under bursty traffic while\nmaintaining acceptable inference accuracy.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.17133v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "BrownoutServe: 面向MoE大模型的突发工作负载下SLO感知推理服务", "tldr": "BrownoutServe是一种新型服务框架，通过引入“联合专家”和动态降级机制，解决了MoE大模型在突发工作负载下推理效率低下和SLO违规的问题，显著提升了吞吐量并降低了SLO违规率。", "motivation": "现有系统由于静态模型部署和缺乏动态工作负载适应性，导致MoE大模型在突发请求期间资源利用率低下和推理延迟增加，效率低下。", "method": "本文提出了BrownoutServe框架，引入了“联合专家”以集成多专家知识，减少专家访问次数和推理延迟。同时，它提出了动态降级机制，自适应调整特定令牌的处理，以优化推理性能并保证服务水平目标（SLO）。", "result": "BrownoutServe在各种工作负载下表现出有效性：与vLLM相比，吞吐量提高了2.07倍，SLO违规率降低了90.28%，同时保持了可接受的推理精度。", "conclusion": "BrownoutServe在突发流量下表现出强大的鲁棒性，有效优化了MoE大模型的推理效率并维持了服务可靠性。", "translation": "近年来，专家混合（MoE）架构已广泛应用于大型语言模型（LLMs），提供了一种有前景的解决方案，它在计算过程中仅激活模型参数的一个子集，从而降低了总体内存需求并实现了比密集模型更快的推理速度。尽管有这些优势，现有系统仍然面临由于静态模型部署和缺乏动态工作负载适应性导致的效率低下问题。这导致资源利用率次优和延迟增加，尤其是在突发请求期间。\n为了解决这些挑战，本文引入了BrownoutServe，一种新颖的服务框架，旨在优化MoE-based LLMs在动态计算需求和流量条件下的推理效率并保持服务可靠性。BrownoutServe引入了“联合专家”，它整合了多个专家的知识，减少了专家访问次数和推理延迟。此外，它提出了一种动态降级机制，自适应调整特定令牌的处理，优化推理性能，同时保证满足服务水平目标（SLOs）。我们的评估显示了BrownoutServe在各种工作负载下的有效性：与vLLM相比，它实现了高达2.07倍的吞吐量提升，并将SLO违规率降低了90.28%，展示了其在突发流量下的鲁棒性，同时保持了可接受的推理精度。", "summary": "本文提出BrownoutServe，一个针对MoE大模型的新型推理服务框架，旨在解决现有系统在突发工作负载下效率低、资源利用率差和延迟高的问题。BrownoutServe通过引入“联合专家”来减少专家访问和推理延迟，并采用动态降级机制自适应调整令牌处理以满足SLO。实验结果表明，BrownoutServe在吞吐量方面比vLLM提升高达2.07倍，并减少了90.28%的SLO违规，同时保持了推理精度，验证了其在动态和突发流量下的鲁棒性和有效性。", "keywords": "MoE, LLMs, 推理服务, SLO感知, 突发工作负载", "comments": "本文针对MoE大模型在突发工作负载下的推理效率和SLO保证问题，提出了创新的BrownoutServe框架。其核心创新在于“联合专家”概念和动态降级机制，有效解决了资源利用率和延迟问题。特别是，在保证可接受精度的前提下，显著提升吞吐量并降低SLO违规率，体现了该方法的实用性和重要性。"}}
{"id": "2507.16884", "title": "SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling", "authors": ["Yi Guo", "Wei Wang", "Zhihang Yuan", "Rong Cao", "Kuan Chen", "Zhengyang Chen", "Yuanyuan Huo", "Yang Zhang", "Yuping Wang", "Shouda Liu", "Yuxuan Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Tech Report", "url": "http://arxiv.org/abs/2507.16884v1", "summary": "Generative models like Flow Matching have achieved state-of-the-art\nperformance but are often hindered by a computationally expensive iterative\nsampling process. To address this, recent work has focused on few-step or\none-step generation by learning the average velocity field, which directly maps\nnoise to data. MeanFlow, a leading method in this area, learns this field by\nenforcing a differential identity that connects the average and instantaneous\nvelocities. In this work, we argue that this differential formulation is a\nlimiting special case of a more fundamental principle. We return to the first\nprinciples of average velocity and leverage the additivity property of definite\nintegrals. This leads us to derive a novel, purely algebraic identity we term\nInterval Splitting Consistency. This identity establishes a self-referential\nrelationship for the average velocity field across different time intervals\nwithout resorting to any differential operators. Based on this principle, we\nintroduce SplitMeanFlow, a new training framework that enforces this algebraic\nconsistency directly as a learning objective. We formally prove that the\ndifferential identity at the core of MeanFlow is recovered by taking the limit\nof our algebraic consistency as the interval split becomes infinitesimal. This\nestablishes SplitMeanFlow as a direct and more general foundation for learning\naverage velocity fields. From a practical standpoint, our algebraic approach is\nsignificantly more efficient, as it eliminates the need for JVP computations,\nresulting in simpler implementation, more stable training, and broader hardware\ncompatibility. One-step and two-step SplitMeanFlow models have been\nsuccessfully deployed in large-scale speech synthesis products (such as\nDoubao), achieving speedups of 20x.", "comment": "Tech Report", "pdf_url": "http://arxiv.org/pdf/2507.16884v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "SplitMeanFlow：少步生成模型中的区间分裂一致性", "tldr": "SplitMeanFlow 引入了一种代数“区间分裂一致性”来高效地进行少步生成建模，比先前的微分方法更具通用性，并在语音合成中实现了 20 倍的速度提升。", "motivation": "传统的生成模型因迭代采样过程而计算成本高昂。现有的少步生成方法（如 MeanFlow）依赖于一种有限制的微分恒等式。", "method": "本文提出了 SplitMeanFlow，这是一个基于一种新颖的、纯代数恒等式——“区间分裂一致性”的训练框架。该恒等式利用定积分的加性特性，为不同时间间隔的平均速度场建立了自引用关系，避免了微分算子和 JVP 计算的需求。", "result": "SplitMeanFlow 显著提高了效率，实现了更简单的实现、更稳定的训练和更广泛的硬件兼容性。一步和两步的 SplitMeanFlow 模型已成功部署在大型语音合成产品（如豆包）中，实现了 20 倍的速度提升。", "conclusion": "SplitMeanFlow 通过引入一种代数一致性，为学习平均速度场提供了一个更直接和更通用的基础，该代数一致性在极限情况下可恢复先前的微分恒等式，从而带来了效率和部署方面的实际优势。", "translation": "生成模型如流匹配（Flow Matching）已达到最先进的性能，但通常受限于计算成本高昂的迭代采样过程。为了解决这个问题，最近的工作侧重于通过学习平均速度场实现少步或一步生成，该速度场直接将噪声映射到数据。MeanFlow 是该领域的领先方法，通过强制执行连接平均速度和瞬时速度的微分恒等式来学习该场。在这项工作中，我们认为这种微分公式是一个更基本原理的限制性特例。我们回归到平均速度的第一性原理，并利用定积分的加性属性。这使我们能够推导出一个新颖的、纯代数恒等式，我们称之为“区间分裂一致性”（Interval Splitting Consistency）。该恒等式为不同时间间隔的平均速度场建立了自引用关系，而无需借助任何微分算子。基于此原理，我们引入了 SplitMeanFlow，一个将这种代数一致性直接作为学习目标的全新训练框架。我们正式证明，当区间分裂变得无穷小时，MeanFlow 核心的微分恒等式可以通过我们的代数一致性取极限来恢复。这确立了 SplitMeanFlow 作为学习平均速度场更直接和更通用的基础。从实践角度来看，我们的代数方法效率显著更高，因为它消除了对 JVP 计算的需求，从而实现了更简单的实现、更稳定的训练和更广泛的硬件兼容性。一步和两步的 SplitMeanFlow 模型已成功部署在大型语音合成产品（如豆包）中，实现了 20 倍的速度提升。", "summary": "本文针对生成模型中迭代采样计算成本高的问题，提出了 SplitMeanFlow 框架。该框架基于一种新颖的、纯代数恒等式——“区间分裂一致性”，该恒等式利用定积分的加性特性，为平均速度场建立了跨时间区间的自引用关系，避免了微分操作和 JVP 计算。研究证明，SplitMeanFlow 比现有方法（如 MeanFlow）更具通用性，且在实际应用中更高效，训练更稳定，兼容性更广。该方法已成功应用于大型语音合成产品（如豆bao），实现了 20 倍的速度提升。", "keywords": "生成模型, 少步生成, 平均速度场, 区间分裂一致性, SplitMeanFlow", "comments": "该论文的创新之处在于，它将学习平均速度场的方法从微分公式提升到更基础的代数恒等式，这不仅简化了计算，还提高了实际应用性。其在真实世界产品中实现显著速度提升的部署案例，进一步凸显了这项工作的重要性和实用价值。"}}
{"id": "2507.16843", "title": "Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems", "authors": ["Zhongsheng Wang", "Sijie Wang", "Jia Wang", "Yung-I Liang", "Yuxi Zhang", "Jiamou Liu"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ICONIP 2024", "url": "http://arxiv.org/abs/2507.16843v1", "summary": "In the design of customer relationship management (CRM) systems, accurately\nidentifying customer types and offering personalized services are key to\nenhancing customer satisfaction and loyalty. However, this process faces the\nchallenge of discerning customer voices and intentions, and general pre-trained\nautomatic speech recognition (ASR) models make it difficult to effectively\naddress industry-specific speech recognition tasks. To address this issue, we\ninnovatively proposed a solution for fine-tuning industry-specific ASR models,\nwhich significantly improved the performance of the fine-tuned ASR models in\nindustry applications. Experimental results show that our method substantially\nimproves the crucial auxiliary role of the ASR model in industry CRM systems,\nand this approach has also been adopted in actual industrial applications.", "comment": "Accepted by ICONIP 2024", "pdf_url": "http://arxiv.org/pdf/2507.16843v1", "cate": "cs.SD", "date": "2025-07-20", "updated": "2025-07-20", "AI": {"title_translation": "弱监督技术助力工业级CRM系统增强ASR模型", "tldr": "本文提出了一种微调行业特定ASR模型的方法，显著提升了其在工业级CRM系统中的性能，并已在实际应用中采纳。", "motivation": "在客户关系管理（CRM）系统中，准确识别客户类型并提供个性化服务对提升客户满意度和忠诚度至关重要。然而，现有通用预训练ASR模型难以有效处理行业特定的语音识别任务，因此需要一种方法来增强ASR模型以应对这一挑战。", "method": "本文提出了一种创新性的解决方案，用于微调行业特定的ASR模型。", "result": "实验结果表明，所提出的方法显著提升了ASR模型在行业CRM系统中的关键辅助作用。该方法已被实际工业应用采纳。", "conclusion": "通过微调行业特定的ASR模型，可以显著提高其在工业级CRM系统中的性能，从而有效应对行业特定的语音识别挑战。", "translation": "在客户关系管理（CRM）系统的设计中，准确识别客户类型并提供个性化服务是提升客户满意度和忠诚度的关键。然而，这一过程面临着辨别客户声音和意图的挑战，且通用预训练的自动语音识别（ASR）模型难以有效处理行业特定的语音识别任务。为解决这一问题，我们创新性地提出了一种微调行业特定ASR模型的解决方案，该方案显著提高了微调后ASR模型在行业应用中的性能。实验结果表明，我们的方法大大提升了ASR模型在行业CRM系统中的关键辅助作用，并且这种方法也已在实际工业应用中采纳。", "summary": "本文针对CRM系统中通用ASR模型难以处理行业特定语音识别任务的问题，提出了一种创新性的微调行业特定ASR模型的方法。实验证明，该方法显著提升了ASR模型在工业级CRM系统中的性能，并已在实际工业应用中得到采纳，有效增强了ASR在CRM中的辅助作用。", "keywords": "ASR模型, CRM系统, 语音识别, 模型微调, 行业应用", "comments": "该论文的创新点在于提出了针对行业特定ASR模型的微调解决方案，而非仅仅依赖弱监督技术（标题提及但抽象中未详述）。其重要性体现在解决了工业级CRM系统中语音识别的实际痛点，并通过实际应用验证了其有效性。抽象中未详细说明所使用的“弱监督技术”的具体细节，这可能是其局限性之一。"}}
{"id": "2507.17284", "title": "State Estimation with 1-Bit Observations and Imperfect Models: Bussgang Meets Kalman in Neural Networks", "authors": ["Chaehyun Jung", "TaeJun Ha", "Hyeonuk Kim", "Jeonghun Park"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17284v1", "summary": "State estimation from noisy observations is a fundamental problem in many\napplications of signal processing. Traditional methods, such as the extended\nKalman filter, work well under fully-known Gaussian models, while recent hybrid\ndeep learning frameworks, combining model-based and data-driven approaches, can\nalso handle partially known models and non-Gaussian noise. However, existing\nstudies commonly assume the absence of quantization distortion, which is\ninevitable, especially with non-ideal analog-to-digital converters. In this\nwork, we consider a state estimation problem with 1-bit quantization. 1-bit\nquantization causes significant quantization distortion and severe information\nloss, rendering conventional state estimation strategies unsuitable. To address\nthis, inspired by the Bussgang decomposition technique, we first develop the\nBussgang-aided Kalman filter by assuming perfectly known models. The proposed\nmethod suitably captures quantization distortion into the state estimation\nprocess. In addition, we propose a computationally efficient variant, referred\nto as the reduced Bussgang-aided Kalman filter and, building upon it, introduce\na deep learning-based approach for handling partially known models, termed the\nBussgang-aided KalmanNet. In particular, the Bussgang-aided KalmanNet jointly\nuses a dithering technique and a gated recurrent unit (GRU) architecture to\neffectively mitigate the effects of 1-bit quantization and model mismatch.\nThrough simulations on the Lorenz-Attractor model and the Michigan NCLT\ndataset, we demonstrate that our proposed methods achieve accurate state\nestimation performance even under highly nonlinear, mismatched models and 1-bit\nobservations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17284v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于1比特观测和不完善模型的状态估计：Bussgang在神经网络中与卡尔曼相遇", "tldr": "本文提出了一系列基于Bussgang分解的卡尔曼滤波器变体，包括Bussgang辅助卡尔曼滤波器、简化版以及结合深度学习的Bussgang辅助卡尔曼网络，以解决在存在1比特量化失真和模型不匹配情况下的状态估计问题。", "motivation": "传统状态估计算法（如扩展卡尔曼滤波器）在完全已知高斯模型下表现良好，而混合深度学习框架能处理部分已知模型和非高斯噪声。然而，现有研究普遍忽略了量化失真（尤其是在非理想模数转换器中不可避免），特别是1比特量化会导致严重的信息损失，使传统策略失效。", "method": "1. 提出Bussgang辅助卡尔曼滤波器（Bussgang-aided Kalman filter），假设模型完全已知，将量化失真纳入状态估计过程。2. 提出计算效率更高的简化版Bussgang辅助卡尔曼滤波器（reduced Bussgang-aided Kalman filter）。3. 引入基于深度学习的Bussgang辅助卡尔曼网络（Bussgang-aided KalmanNet），用于处理部分已知模型，该网络结合了抖动技术和门控循环单元（GRU）架构以减轻1比特量化和模型不匹配的影响。", "result": "通过在Lorenz-Attractor模型和Michigan NCLT数据集上的仿真，证明所提出的方法即使在高度非线性、不匹配的模型和1比特观测条件下，也能实现准确的状态估计性能。", "conclusion": "本文提出的Bussgang辅助卡尔曼滤波器及其深度学习变体Bussgang辅助卡尔曼网络，有效解决了在1比特量化和模型不完善情况下的状态估计难题，展现了良好的性能。", "translation": "从噪声观测中进行状态估计是信号处理中许多应用的基础问题。传统的方​​法，例如扩展卡尔曼滤波器，在完全已知的高斯模型下表现良好，而最近的混合深度学习框架结合了基于模型和数据驱动的方法，也可以处理部分已知模型和非高斯噪声。然而，现有研究通常假设不存在量化失真，而这在非理想模数转换器中是不可避免的。在这项工作中，我们考虑了1比特量化下的状态估计问题。1比特量化会导致显著的量化失真和严重的信息损失，使得传统的​​状态估计策略不适用。为了解决这个问题，受Bussgang分解技术的启发，我们首先假设模型完全已知，开发了Bussgang辅助卡尔曼滤波器。所提出的方法将量化失真适当地捕获到状态估计过程中。此外，我们提出了一种计算效率高的变体，称为简化Bussgang辅助卡尔曼滤波器，并在此基础上引入了一种基于深度学习的方法来处理部分已知模型，称为Bussgang辅助卡尔曼网络。特别是，Bussgang辅助卡尔曼网络联合使用抖动技术和门控循环单元（GRU）架构，以有效减轻1比特量化和模型不匹配的影响。通过在Lorenz-Attractor模型和Michigan NCLT数据集上的仿真，我们证明了我们提出的方法即使在高度非线性、不匹配的模型和1比特观测条件下也能实现准确的状态估计性能。", "summary": "本文针对1比特量化和模型不完善条件下的状态估计问题，提出了一系列创新方法。首先，引入了Bussgang辅助卡尔曼滤波器来处理1比特观测下的量化失真。在此基础上，开发了计算效率更高的简化版本，并进一步提出了结合深度学习的Bussgang辅助卡尔曼网络（Bussgang-aided KalmanNet），该网络利用抖动技术和GRU架构来应对部分已知模型和量化效应。实验证明，这些方法在复杂非线性系统和1比特观测下仍能实现准确的状态估计。", "keywords": "状态估计, 1比特观测, 量化失真, Bussgang分解, 卡尔曼滤波器, 深度学习, 神经网络", "comments": "该论文的创新点在于将Bussgang分解技术引入到1比特量化条件下的状态估计问题中，并成功地将其与经典的卡尔曼滤波和现代的深度学习（GRU）相结合。这种混合方法不仅考虑了量化失真，还兼顾了模型不完善的情况，具有很强的实用价值和鲁棒性。尤其是在资源受限或需要低功耗传感器的应用中，1比特观测是常见且具有挑战性的，该研究为此提供了一个有效的解决方案。"}}
{"id": "2507.17147", "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17147v1", "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17147v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CogDual：通过基于隐式规则奖励的强化学习增强大型语言模型的双重认知", "tldr": "CogDual是一个新的角色扮演语言代理（RPLA），它采用“认知-响应”推理范式，通过结合外部情境意识和内部自我意识来提高角色扮演的一致性和上下文对齐。它使用强化学习进行优化，并在多个基准测试中表现优于现有基线。", "motivation": "现有的角色扮演语言代理（RPLAs）方法通常依赖于提示工程或监督微调来模仿角色行为，但往往忽视了驱动这些行为的潜在认知机制。", "method": "本文引入了CogDual，一个新颖的RPLA，采用“认知-响应”推理范式。它通过联合建模外部情境意识和内部自我意识来生成响应。为了进一步优化性能，CogDual采用了强化学习，并设计了两种通用的奖励机制，适用于开放域文本生成。", "result": "在CoSER基准测试以及Cross-MR和LifeChoice上的广泛实验表明，CogDual始终优于现有基线，并且能够有效地泛化到各种角色扮演任务中。", "conclusion": "CogDual通过其独特的双重认知和强化学习方法，显著提高了大型语言模型在角色扮演任务中的表现和泛化能力。", "translation": "角色扮演语言代理（RPLAs）已成为大型语言模型（LLMs）的一个重要应用方向。现有方法通常依赖于提示工程或监督微调来使模型在特定场景中模仿角色行为，但往往忽视了驱动这些行为的潜在认知机制。受认知心理学启发，我们引入了CogDual，一种采用“认知-响应”推理范式的新颖RPLA。通过联合建模外部情境意识和内部自我意识，CogDual生成的响应具有更高的角色一致性和上下文对齐性。为了进一步优化性能，我们采用了强化学习，并设计了两种通用的奖励机制，适用于开放域文本生成。在CoSER基准测试以及Cross-MR和LifeChoice上的广泛实验表明，CogDual始终优于现有基线，并且能够有效地泛化到各种角色扮演任务中。", "summary": "本文提出了CogDual，一个基于认知心理学启发的角色扮演语言代理（RPLA），旨在解决现有LLMs在角色扮演中忽视认知机制的问题。CogDual采用“认知-响应”推理范式，结合外部情境和内部自我意识来生成更一致和上下文对齐的响应。通过强化学习和通用奖励机制进行优化，CogDual在多项角色扮演任务中表现出优于基线的性能和良好的泛化能力。", "keywords": "角色扮演语言代理, 大型语言模型, 强化学习, 认知心理学, 文本生成", "comments": "CogDual的创新之处在于其引入了“认知-响应”推理范式，并结合了外部情境意识和内部自我意识，这为LLMs在角色扮演中提供了更深层次的认知基础。通过强化学习优化，使其在开放域文本生成中具有更好的泛化能力，为RPLA的发展开辟了新方向。"}}
{"id": "2507.17020", "title": "Ethics through the Facets of Artificial Intelligence", "authors": ["Flavio Soares Correa da Silva"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17020v1", "summary": "Artificial Intelligence (AI) has received unprecedented attention in recent\nyears, raising ethical concerns about the development and use of AI technology.\nIn the present article, we advocate that these concerns stem from a blurred\nunderstanding of AI, how it can be used, and how it has been interpreted in\nsociety. We explore the concept of AI based on three descriptive facets and\nconsider ethical issues related to each facet. Finally, we propose a framework\nfor the ethical assessment of the use of AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17020v1", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "人工智能多维度伦理", "tldr": "本文认为AI伦理问题源于对AI的模糊理解，提出了基于AI三个描述性方面的伦理探讨，并提出了一个AI使用伦理评估框架。", "motivation": "近年来人工智能受到前所未有的关注，随之而来的是对其开发和使用中的伦理担忧。本文认为这些担忧源于对AI、其使用方式及其在社会中如何被解释的模糊理解。", "method": "本文基于AI的三个描述性方面来探讨AI概念，并考虑与每个方面相关的伦理问题。", "result": "本文提出了一个用于AI使用伦理评估的框架。", "conclusion": "本文认为对AI的伦理担忧源于对其的模糊理解，并通过探讨AI的不同方面及其相关伦理问题，最终提出了一个AI使用伦理评估框架，旨在促进对AI伦理的清晰理解和评估。", "translation": "人工智能（AI）近年来受到了前所未有的关注，引发了对AI技术开发和使用的伦理担忧。在本文中，我们主张这些担忧源于对AI、其如何被使用以及其在社会中如何被解释的模糊理解。我们基于三个描述性方面探讨AI的概念，并考虑与每个方面相关的伦理问题。最后，我们提出了一个用于AI使用伦理评估的框架。", "summary": "本文探讨了人工智能的伦理问题，指出这些问题源于对AI的模糊理解。文章通过分析AI的三个描述性方面来审视相关的伦理困境，并最终提出了一个用于评估AI使用的伦理框架，旨在促进更清晰的AI伦理认识。", "keywords": "人工智能, 伦理, 评估框架, 模糊理解", "comments": "本文创新性地将AI伦理问题归因于对AI概念的模糊理解，并通过多维度分析和提出评估框架来解决这一问题，对AI伦理研究具有重要意义。"}}
{"id": "2507.17242", "title": "High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces", "authors": ["Gege Ming", "Weihua Pei", "Sen Tian", "Xiaogang Chen", "Xiaorong Gao", "Yijun Wang"], "categories": ["cs.HC", "eess.SP", "q-bio.NC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17242v1", "summary": "Brain-computer interface (BCI) technology establishes a direct communication\npathway between the brain and external devices. Current visual BCI systems\nsuffer from insufficient information transfer rates (ITRs) for practical use.\nSpatial information, a critical component of visual perception, remains\nunderexploited in existing systems because the limited spatial resolution of\nrecording methods hinders the capture of the rich spatiotemporal dynamics of\nbrain signals. This study proposed a frequency-phase-space fusion encoding\nmethod, integrated with 256-channel high-density electroencephalogram (EEG)\nrecordings, to develop high-speed BCI systems. In the classical frequency-phase\nencoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode\nconfigurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%\nover the traditional 64-9 setup. In the proposed frequency-phase-space encoding\n200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and\n103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.\nThis study demonstrates the essential role and immense potential of\nhigh-density EEG in decoding the spatiotemporal information of visual stimuli.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17242v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "高密度脑电图实现最快速的视觉脑机接口", "tldr": "本研究提出了一种结合高密度脑电图（EEG）的频率-相位-空间融合编码方法，显著提高了视觉脑机接口（BCI）的信息传输率（ITR），实现了迄今为止最快的视觉BCI系统。", "motivation": "当前的视觉脑机接口系统信息传输率（ITR）不足以满足实际应用需求。现有系统未充分利用空间信息，因为有限的记录方法空间分辨率阻碍了对大脑信号丰富时空动态的捕捉。", "method": "本研究提出了一种频率-相位-空间融合编码方法，并将其与256通道高密度脑电图（EEG）记录相结合，以开发高速脑机接口系统。研究在经典的频率-相位编码40目标BCI范式和提出的频率-相位-空间编码200目标BCI范式中进行了测试。", "result": "在经典的频率-相位编码40目标BCI范式中，256-66、128-32和64-21电极配置的理论ITR相比传统64-9设置分别增加了83.66%、79.99%和55.50%。在提出的频率-相位-空间编码200目标BCI范式中，这些增幅分别达到了195.56%、153.08%和103.07%。在线BCI系统实现了平均472.7 bpm的实际ITR。", "conclusion": "本研究证明了高密度脑电图在解码视觉刺激时空信息方面的重要作用和巨大潜力。", "translation": "脑机接口（BCI）技术建立了大脑与外部设备之间的直接通信路径。当前的视觉BCI系统信息传输率（ITR）不足以满足实际应用需求。空间信息作为视觉感知的关键组成部分，在现有系统中仍未得到充分利用，因为有限的记录方法空间分辨率阻碍了对大脑信号丰富时空动态的捕捉。本研究提出了一种频率-相位-空间融合编码方法，并将其与256通道高密度脑电图（EEG）记录相结合，以开发高速脑机接口系统。在经典的频率-相位编码40目标BCI范式中，256-66、128-32和64-21电极配置相比传统64-9设置带来了83.66%、79.99%和55.50%的理论ITR增加。在提出的频率-相位-空间编码200目标BCI范式中，这些增幅分别达到了195.56%、153.08%和103.07%。在线BCI系统实现了平均472.7 bpm的实际ITR。本研究证明了高密度脑电图在解码视觉刺激时空信息方面的重要作用和巨大潜力。", "summary": "本研究旨在解决当前视觉脑机接口（BCI）系统信息传输率（ITR）不足的问题，通过提出一种结合256通道高密度脑电图（EEG）的频率-相位-空间融合编码方法，显著提高了BCI的性能。实验结果显示，在不同电极配置下，该方法在理论ITR上实现了大幅提升，最高增幅达195.56%，并在在线系统中实现了平均472.7 bpm的实际ITR，证明了高密度EEG在解码视觉时空信息和加速BCI方面的巨大潜力。", "keywords": "脑机接口, 高密度脑电图, 信息传输率, 频率-相位-空间编码, 视觉感知", "comments": "该论文通过引入高密度脑电图和创新的频率-相位-空间融合编码方法，显著提升了视觉脑机接口的信息传输率，为BCI的实际应用迈出了重要一步。其强调空间信息利用的创新点，对未来高性能BCI系统的发展具有重要指导意义。"}}
{"id": "2507.17125", "title": "Model Compression Engine for Wearable Devices Skin Cancer Diagnosis", "authors": ["Jacob M. Delgado-López", "Andrea P. Seda-Hernandez", "Juan D. Guadalupe-Rosado", "Luis E. Fernandez Ramirez", "Miguel Giboyeaux-Camilo", "Wilfredo E. Lugo-Beauchamp"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17125v1", "summary": "Skin cancer is one of the most prevalent and preventable types of cancer, yet\nits early detection remains a challenge, particularly in resource-limited\nsettings where access to specialized healthcare is scarce. This study proposes\nan AI-driven diagnostic tool optimized for embedded systems to address this\ngap. Using transfer learning with the MobileNetV2 architecture, the model was\nadapted for binary classification of skin lesions into \"Skin Cancer\" and\n\"Other.\" The TensorRT framework was employed to compress and optimize the model\nfor deployment on the NVIDIA Jetson Orin Nano, balancing performance with\nenergy efficiency. Comprehensive evaluations were conducted across multiple\nbenchmarks, including model size, inference speed, throughput, and power\nconsumption. The optimized models maintained their performance, achieving an\nF1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.\nPost-compression results showed reductions in model size of up to 0.41, along\nwith improvements in inference speed and throughput, and a decrease in energy\nconsumption of up to 0.93 in INT8 precision. These findings validate the\nfeasibility of deploying high-performing, energy-efficient diagnostic tools on\nresource-constrained edge devices. Beyond skin cancer detection, the\nmethodologies applied in this research have broader applications in other\nmedical diagnostics and domains requiring accessible, efficient AI solutions.\nThis study underscores the potential of optimized AI systems to revolutionize\nhealthcare diagnostics, thereby bridging the divide between advanced technology\nand underserved regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17125v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于可穿戴设备皮肤癌诊断的模型压缩引擎", "tldr": "本研究提出了一种针对资源受限边缘设备优化的AI驱动诊断工具，用于皮肤癌早期检测，通过模型压缩和优化在保持高性能的同时实现低能耗。", "motivation": "皮肤癌是常见且可预防的癌症之一，但其早期检测仍面临挑战，尤其是在医疗资源匮乏的地区。本研究旨在解决这一问题，通过开发一种针对嵌入式系统优化的AI驱动诊断工具。", "method": "本研究使用MobileNetV2架构进行迁移学习，将模型调整为皮肤病变的二分类（“皮肤癌”和“其他”）。采用TensorRT框架对模型进行压缩和优化，以部署在NVIDIA Jetson Orin Nano上，并在模型大小、推理速度、吞吐量和功耗等多个基准上进行了综合评估。", "result": "优化后的模型保持了性能，F1-Score达到87.18%，精确率为93.18%，召回率为81.91%。压缩后，模型大小减少了0.41，推理速度和吞吐量得到改善，INT8精度下的能耗降低了0.93。", "conclusion": "这些发现验证了在资源受限的边缘设备上部署高性能、节能诊断工具的可行性。本研究应用的方法学在其他医疗诊断和需要可访问、高效AI解决方案的领域具有更广泛的应用前景，并强调了优化AI系统在革新医疗诊断方面的潜力。", "translation": "皮肤癌是最常见且可预防的癌症类型之一，但其早期检测仍然是一个挑战，特别是在医疗资源稀缺的受限环境中。本研究提出了一种针对嵌入式系统优化的AI驱动诊断工具，以弥补这一空白。该研究使用MobileNetV2架构进行迁移学习，将模型调整为皮肤病变的二分类（“皮肤癌”和“其他”）。采用TensorRT框架对模型进行压缩和优化，以部署在NVIDIA Jetson Orin Nano上，平衡了性能和能效。在模型大小、推理速度、吞吐量和功耗等多个基准上进行了综合评估。优化后的模型保持了性能，F1-Score达到87.18%，精确率为93.18%，召回率为81.91%。压缩后结果显示，模型大小最多减少了0.41，推理速度和吞吐量得到改善，INT8精度下能耗最多降低了0.93。这些发现验证了在资源受限的边缘设备上部署高性能、节能诊断工具的可行性。除了皮肤癌检测，本研究应用的方法学在其他医疗诊断和需要可访问、高效AI解决方案的领域具有更广泛的应用。这项研究强调了优化AI系统在革新医疗诊断方面的潜力，从而弥合了先进技术与服务不足地区之间的鸿沟。", "summary": "本研究开发了一种针对可穿戴设备优化的AI诊断工具，用于皮肤癌早期检测，旨在解决资源受限地区医疗资源不足的问题。通过在MobileNetV2模型上应用TensorRT框架进行压缩和优化，成功将高性能AI模型部署到NVIDIA Jetson Orin Nano等边缘设备上。实验结果表明，该方法在显著减小模型大小、提高推理速度和降低能耗的同时，保持了较高的诊断准确性，验证了在边缘设备上部署高效AI诊断工具的可行性，并具有广泛的应用前景。", "keywords": "皮肤癌诊断, 模型压缩, 边缘计算, 可穿戴设备, 医疗AI", "comments": "该研究的创新之处在于其将先进的AI模型（MobileNetV2）与模型压缩技术（TensorRT）相结合，实现了在资源受限的可穿戴设备上的高效部署。这对于解决医疗资源不均的问题具有重要意义，尤其是在早期癌症诊断方面。其方法学不仅限于皮肤癌，还可推广到其他医疗诊断领域，显示出巨大的应用潜力。"}}
{"id": "2507.16962", "title": "Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition, Image-level, and Feature-level Methods", "authors": ["Qinqin Yang", "Firoozeh Shomal-Zadeh", "Ali Gholipour"], "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      20 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2507.16962v1", "summary": "Modern medical imaging technologies have greatly advanced neuroscience\nresearch and clinical diagnostics. However, imaging data collected across\ndifferent scanners, acquisition protocols, or imaging sites often exhibit\nsubstantial heterogeneity, known as \"batch effects\" or \"site effects\". These\nnon-biological sources of variability can obscure true biological signals,\nreduce reproducibility and statistical power, and severely impair the\ngeneralizability of learning-based models across datasets. Image harmonization\naims to eliminate or mitigate such site-related biases while preserving\nmeaningful biological information, thereby improving data comparability and\nconsistency. This review provides a comprehensive overview of key concepts,\nmethodological advances, publicly available datasets, current challenges, and\nfuture directions in the field of medical image harmonization, with a focus on\nmagnetic resonance imaging (MRI). We systematically cover the full imaging\npipeline, and categorize harmonization approaches into prospective acquisition\nand reconstruction strategies, retrospective image-level and feature-level\nmethods, and traveling-subject-based techniques. Rather than providing an\nexhaustive survey, we focus on representative methods, with particular emphasis\non deep learning-based approaches. Finally, we summarize the major challenges\nthat remain and outline promising avenues for future research.", "comment": "20 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.16962v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "磁共振成像中的标准化：采集、图像层面和特征层面方法的综述", "tldr": "本文综述了磁共振成像（MRI）中消除“批次效应”的图像标准化方法，涵盖采集、图像层面和特征层面策略，并强调了深度学习的应用和未来挑战。", "motivation": "现代医学成像技术在神经科学研究和临床诊断中取得了巨大进展，但不同扫描仪、采集协议或成像站点收集的数据存在显著异质性（“批次效应”），这会掩盖真实的生物信号，降低数据可重复性和统计功效，并损害基于学习的模型在不同数据集间的泛化能力。图像标准化旨在消除或减轻这些站点相关的偏差，同时保留有意义的生物信息，从而提高数据可比性和一致性。", "method": "本文是一篇综述，全面概述了医学图像标准化领域的关键概念、方法进展、公开数据集、当前挑战和未来方向，重点关注磁共振成像（MRI）。它系统地涵盖了完整的成像流程，并将标准化方法分为前瞻性采集和重建策略、回顾性图像层面和特征层面方法以及基于旅行受试者的技术。综述侧重于代表性方法，特别强调基于深度学习的方法。", "result": "本文提供了磁共振成像图像标准化的全面概述，分类了不同的标准化方法，并指出了该领域当前面临的主要挑战以及未来研究的有前景的方向。", "conclusion": "本文总结了磁共振成像图像标准化领域仍存在的主要挑战，并概述了未来研究的有前景的途径。", "translation": "现代医学成像技术极大地推动了神经科学研究和临床诊断。然而，在不同扫描仪、采集协议或成像站点收集的成像数据通常表现出显著的异质性，即“批次效应”或“站点效应”。这些非生物变异源会掩盖真实的生物信号，降低可重复性和统计功效，并严重损害基于学习的模型在数据集之间的泛化能力。图像标准化旨在消除或减轻此类与站点相关的偏差，同时保留有意义的生物信息，从而提高数据可比性和一致性。本综述全面概述了医学图像标准化领域的关键概念、方法进展、公开数据集、当前挑战和未来方向，重点关注磁共振成像（MRI）。我们系统地涵盖了完整的成像流程，并将标准化方法分为前瞻性采集和重建策略、回顾性图像层面和特征层面方法以及基于旅行受试者的技术。我们没有提供详尽的调查，而是侧重于代表性方法，特别强调基于深度学习的方法。最后，我们总结了仍然存在的主要挑战，并概述了未来研究的有前景的途径。", "summary": "本文综述了磁共振成像（MRI）中的图像标准化方法，旨在解决因不同扫描仪和协议导致的“批次效应”问题，以提高数据可比性和基于学习模型的泛化能力。综述系统地分类了标准化策略，包括采集、图像层面和特征层面的方法，并特别强调了深度学习的应用。文章还讨论了该领域当前面临的挑战并展望了未来的研究方向。", "keywords": "图像标准化, 磁共振成像, 批次效应, 深度学习, 综述", "comments": "这篇综述非常及时且重要，因为它解决了医学图像分析中普遍存在的“批次效应”问题，该问题严重影响了数据的可靠性和AI模型的泛化能力。其创新之处在于系统地分类了不同层面的标准化方法，并特别强调了深度学习的应用，为研究人员提供了全面的参考。作为一篇综述，它不提供新的实验结果，但其对未来研究方向的展望具有重要的指导意义。"}}
{"id": "2507.03256", "title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation", "authors": ["Xinyang Li", "Gen Li", "Zhihui Lin", "Yichen Qian", "GongXin Yao", "Weinan Jia", "Aowen Wang", "Weihua Chen", "Fan Wang"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      12 pages, 7 figures", "url": "http://arxiv.org/abs/2507.03256v2", "summary": "Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of the virtual metaverse. Recently, diffusion\nmodels have become a popular generative technique in this field with their\nstrong generation capabilities. However, several challenges remain for\ndiffusion-based methods: 1) inefficient inference and visual artifacts caused\nby the implicit latent space of Variational Auto-Encoders (VAE), which\ncomplicates the diffusion process; 2) a lack of authentic facial expressions\nand head movements due to inadequate multi-modal information fusion. In this\npaper, MoDA handles these challenges by: 1) defining a joint parameter space\nthat bridges motion generation and neural rendering, and leveraging flow\nmatching to simplify diffusion learning; 2) introducing a multi-modal diffusion\narchitecture to model the interaction among noisy motion, audio, and auxiliary\nconditions, enhancing overall facial expressiveness. In addition, a\ncoarse-to-fine fusion strategy is employed to progressively integrate different\nmodalities, ensuring effective feature fusion. Experimental results demonstrate\nthat MoDA improves video diversity, realism, and efficiency, making it suitable\nfor real-world applications. Project Page:\nhttps://lixinyyang.github.io/MoDA.github.io/", "comment": "12 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.03256v2", "cate": "cs.GR", "date": "2025-07-04", "updated": "2025-07-23", "AI": {"title_translation": "MoDA：多模态扩散架构用于说话人脸生成", "tldr": "MoDA提出一种多模态扩散架构，通过联合参数空间和粗到细融合策略，解决了扩散模型在说话人脸生成中效率低、伪影多和表情不自然的问题，提高了生成视频的多样性、真实性和效率。", "motivation": "在虚拟元宇宙中，生成具有任意身份和语音音频的说话人脸是一个关键问题。现有基于扩散模型的方法面临挑战：1) 变分自编码器（VAE）的隐式潜在空间导致推理效率低下和视觉伪影；2) 多模态信息融合不足导致面部表情和头部动作不真实。", "method": "MoDA通过以下方法解决挑战：1) 定义一个连接运动生成和神经渲染的联合参数空间，并利用流匹配简化扩散学习；2) 引入多模态扩散架构来建模噪声运动、音频和辅助条件之间的交互，增强整体面部表现力；3) 采用粗到细的融合策略逐步整合不同模态，确保有效特征融合。", "result": "实验结果表明，MoDA提高了视频多样性、真实性和效率。", "conclusion": "MoDA在说话人脸生成方面表现出色，其改进的视频多样性、真实性和效率使其适用于现实世界应用。", "translation": "标题：MoDA：多模态扩散架构用于说话人脸生成\n\n摘要：\n生成具有任意身份和语音音频的说话人脸仍然是虚拟元宇宙领域的一个关键问题。最近，扩散模型凭借其强大的生成能力，已成为该领域流行的生成技术。然而，基于扩散的方法仍面临一些挑战：1) 变分自编码器（VAE）的隐式潜在空间导致推理效率低下和视觉伪影，这使得扩散过程复杂化；2) 由于多模态信息融合不足，缺乏真实的面部表情和头部动作。在本文中，MoDA通过以下方式处理这些挑战：1) 定义一个连接运动生成和神经渲染的联合参数空间，并利用流匹配简化扩散学习；2) 引入多模态扩散架构来建模噪声运动、音频和辅助条件之间的交互，增强整体面部表现力。此外，采用了一种从粗到细的融合策略来逐步整合不同的模态，确保有效的特征融合。实验结果表明，MoDA提高了视频的多样性、真实性和效率，使其适用于现实世界应用。项目页面：https://lixinyyang.github.io/MoDA.github.io/", "summary": "本文提出MoDA，一种用于说话人脸生成的多模态扩散架构，旨在解决现有扩散模型在效率、视觉伪影和表情真实性方面的挑战。MoDA通过定义联合参数空间、利用流匹配简化扩散过程，并引入多模态扩散架构和粗到细融合策略来增强模态间交互和面部表现力。实验证明，MoDA有效提升了生成视频的多样性、真实性和效率，使其适用于实际应用。", "keywords": "说话人脸生成, 扩散模型, 多模态融合, 虚拟元宇宙, 流匹配", "comments": "MoDA的创新点在于其结合了联合参数空间、流匹配和多模态扩散架构，并辅以粗到细的融合策略，系统性地解决了扩散模型在说话人脸生成中面临的多个关键问题。特别是在处理VAE隐式潜在空间导致的低效和伪影，以及多模态融合不足导致的表情不自然方面，提出了有效的解决方案。这对于虚拟元宇宙等需要高质量人脸生成的应用具有重要意义。"}}
{"id": "2502.02371", "title": "RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic Docking", "authors": ["Yaroslav Balytskyi", "Inna Hubenko", "Alina Balytska", "Christopher V. Kelly"], "categories": ["q-bio.BM", "cs.AI", "cs.LG", "physics.bio-ph", "physics.med-ph"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02371v2", "summary": "Accurate identification of druggable pockets and their features is essential\nfor structure-based drug design and effective downstream docking. Here, we\npresent RAPID-Net, a deep learning-based algorithm designed for the accurate\nprediction of binding pockets and seamless integration with docking pipelines.\nOn the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of\nTop-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity\ncriterion, compared to 49.1% for DiffBindFR. On the most challenging time split\nof PoseBusters aiming to assess generalization ability (structures submitted\nafter September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of\nTop-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3.\nNotably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose\nwith RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather\nthan sampling, is the primary accuracy bottleneck. The lightweight inference,\nscalability, and competitive accuracy of RAPID-Net position it as a viable\noption for large-scale virtual screening campaigns. Across diverse benchmark\ndatasets, RAPID-Net outperforms other pocket prediction tools, including\nPUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection\nrates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the\ndevelopment of novel therapeutics by highlighting its performance on\npharmacologically relevant targets. RAPID-Net accurately identifies distal\nfunctional sites, offering new opportunities for allosteric inhibitor design.\nIn the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net\nuncovers a wider array of potential binding pockets than existing predictors,\nwhich typically annotate only the orthosteric pocket and overlook secondary\ncavities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02371v2", "cate": "q-bio.BM", "date": "2025-02-04", "updated": "2025-07-23", "AI": {"title_translation": "RAPID-Net: 结合位点无关对接的准确口袋识别", "tldr": "RAPID-Net是一种基于深度学习的算法，用于准确识别药物口袋，显著提升了对接性能，并能发现新的结合位点，有望加速新药开发。", "motivation": "在结构引导的药物设计中，准确识别可成药口袋及其特征对于有效的下游对接至关重要。", "method": "本文提出了RAPID-Net，一种基于深度学习的算法，旨在准确预测结合口袋并与对接流程无缝集成。", "result": "在PoseBusters基准测试中，RAPID-Net引导的AutoDock Vina的Top-1姿态RMSD < 2 Å且满足化学有效性标准的比例为54.9%，优于DiffBindFR的49.1%。在评估泛化能力的PoseBusters时间分割测试中，RAPID-Net引导的AutoDock Vina的Top-1姿态RMSD < 2 Å且PB有效比例为53.1%。在92.2%的情况下，RAPID-Net引导的Vina至少采样到一个RMSD < 2 Å的姿态，表明姿态排序而非采样是准确性的主要瓶颈。RAPID-Net在对接准确性和口袋-配体交叉率方面优于PUResNet和Kalasanty等其他口袋预测工具。它还能准确识别远端功能位点，并在SARS-CoV-2的RNA依赖性RNA聚合酶案例中揭示了比现有预测器更广泛的潜在结合口袋。", "conclusion": "RAPID-Net的轻量级推理、可扩展性和竞争性准确性使其成为大规模虚拟筛选活动的有效选择。它有望加速新型疗法的开发，特别是通过发现新的变构结合位点。", "translation": "准确识别可成药口袋及其特征对于结构引导药物设计和有效的下游对接至关重要。本文提出了RAPID-Net，一种基于深度学习的算法，旨在准确预测结合口袋并与对接流程无缝集成。在PoseBusters基准测试中，RAPID-Net引导的AutoDock Vina的Top-1姿态RMSD < 2 Å且满足PoseBusters化学有效性标准的比例为54.9%，而DiffBindFR为49.1%。在PoseBusters最具挑战性的时间分割测试（旨在评估泛化能力，即2021年9月30日之后提交的结构）中，RAPID-Net引导的AutoDock Vina的Top-1姿态RMSD < 2 Å且PB有效比例为53.1%，而AlphaFold 3为59.5%。值得注意的是，在92.2%的情况下，RAPID-Net引导的Vina至少采样到一个RMSD < 2 Å的姿态（无论其排名如何），这表明姿态排序而非采样是主要的准确性瓶颈。RAPID-Net的轻量级推理、可扩展性和竞争性准确性使其成为大规模虚拟筛选活动的有效选择。在不同的基准数据集上，RAPID-Net在对接准确性和口袋-配体交叉率方面均优于包括PUResNet和Kalasanty在内的其他口袋预测工具。此外，我们通过突出其在药理学相关靶点上的表现，证明了RAPID-Net加速新型疗法开发的潜力。RAPID-Net能准确识别远端功能位点，为变构抑制剂设计提供了新机会。在SARS-CoV-2的RNA依赖性RNA聚合酶案例中，RAPID-Net揭示了比现有预测器更广泛的潜在结合口袋，而现有预测器通常只注释正构口袋并忽略次级腔体。", "summary": "RAPID-Net是一种深度学习驱动的药物口袋识别工具，旨在提高结构引导药物设计的对接精度。它能准确识别药物口袋并与对接流程有效整合，在PoseBusters基准测试中表现出色，性能超越了现有工具。RAPID-Net的轻量级和可扩展性使其适用于大规模虚拟筛选，并且能够发现常规方法难以发现的远端功能位点，为药物发现，特别是变构抑制剂的设计开辟了新途径。", "keywords": "药物口袋识别, 深度学习, 结构引导药物设计, 对接, 虚拟筛选", "comments": "RAPID-Net的创新之处在于其深度学习方法能够准确识别药物口袋，并解决了传统对接中口袋识别的难题。其在泛化能力和发现新结合位点（如变构位点）上的表现尤为重要，这对于药物发现具有重大意义。论文指出姿态排序是主要瓶颈，这为未来的研究提供了明确的方向。"}}
{"id": "2507.16856", "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models", "authors": ["Youngjin Na", "Sangheon Jeong", "Youngwan Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages, 6 figures", "url": "http://arxiv.org/abs/2507.16856v1", "summary": "As vision-language models (VLMs) are increasingly deployed in real-world\napplications, new safety risks arise from the subtle interplay between images\nand text. In particular, seemingly innocuous inputs can combine to reveal\nharmful intent, leading to unsafe model responses. Despite increasing attention\nto multimodal safety, previous approaches based on post hoc filtering or static\nrefusal prompts struggle to detect such latent risks, especially when\nharmfulness emerges only from the combination of inputs. We propose SIA (Safety\nvia Intent Awareness), a training-free prompt engineering framework that\nproactively detects and mitigates harmful intent in multimodal inputs. SIA\nemploys a three-stage reasoning process: (1) visual abstraction via captioning,\n(2) intent inference through few-shot chain-of-thought prompting, and (3)\nintent-conditioned response refinement. Rather than relying on predefined rules\nor classifiers, SIA dynamically adapts to the implicit intent inferred from the\nimage-text pair. Through extensive experiments on safety-critical benchmarks\nincluding SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves\nsubstantial safety improvements, outperforming prior methods. Although SIA\nshows a minor reduction in general reasoning accuracy on MMStar, the\ncorresponding safety gains highlight the value of intent-aware reasoning in\naligning VLMs with human-centric values.", "comment": "5 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.16856v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "SIA：通过意图感知增强视觉-语言模型的安全性", "tldr": "SIA是一个无需训练的提示工程框架，通过三阶段推理（视觉抽象、意图推断、响应细化）主动检测并减轻多模态输入中的有害意图，显著提高了视觉-语言模型的安全性。", "motivation": "视觉-语言模型（VLMs）在实际应用中面临新的安全风险，特别是当图像和文本的微妙交互（看似无害的输入组合）揭示有害意图时，可能导致不安全的模型响应。现有的方法（如事后过滤或静态拒绝提示）难以检测这些潜在风险，尤其当危害仅从输入组合中出现时。", "method": "本文提出了SIA（Safety via Intent Awareness），一个无需训练的提示工程框架，用于主动检测和减轻多模态输入中的有害意图。SIA采用三阶段推理过程：1) 通过字幕进行视觉抽象；2) 通过少样本思维链提示进行意图推断；3) 基于意图的响应细化。SIA不依赖预定义规则或分类器，而是动态适应从图像-文本对推断出的隐式意图。", "result": "通过在SIUO、MM-SafetyBench和HoliSafe等安全关键基准上的广泛实验，SIA展示了显著的安全改进，优于现有方法。尽管SIA在MMStar上的通用推理准确性略有下降，但相应的安全增益突出了意图感知推理在使VLM与以人为中心价值观对齐方面的价值。", "conclusion": "意图感知推理对于使视觉-语言模型与以人为中心价值观对齐具有重要价值，并且能够显著提高模型的安全性。", "translation": "随着视觉-语言模型（VLMs）越来越多地部署在实际应用中，图像和文本之间微妙的相互作用带来了新的安全风险。特别是，看似无害的输入组合可能揭示有害意图，导致不安全的模型响应。尽管多模态安全性受到越来越多的关注，但以往基于事后过滤或静态拒绝提示的方法难以检测此类潜在风险，尤其当危害仅从输入组合中出现时。我们提出了SIA（Safety via Intent Awareness），一个无需训练的提示工程框架，用于主动检测和减轻多模态输入中的有害意图。SIA采用三阶段推理过程：(1) 通过字幕进行视觉抽象，(2) 通过少样本思维链提示进行意图推断，以及 (3) 基于意图的响应细化。SIA不依赖预定义规则或分类器，而是动态适应从图像-文本对推断出的隐式意图。通过在包括SIUO、MM-SafetyBench和HoliSafe在内的安全关键基准上的广泛实验，我们证明SIA实现了显著的安全改进，优于现有方法。尽管SIA在MMStar上的通用推理准确性略有下降，但相应的安全增益突出了意图感知推理在使VLM与以人为中心价值观对齐方面的价值。", "summary": "本文提出SIA（Safety via Intent Awareness），一个针对视觉-语言模型（VLMs）的无需训练的提示工程框架，旨在主动检测并缓解多模态输入中由图像和文本微妙交互产生的有害意图。SIA通过视觉抽象、意图推断和意图条件响应细化三阶段推理过程，动态适应隐式意图。实验结果表明，SIA在多个安全基准上显著提升了安全性，优于现有方法，并强调了意图感知推理在使VLMs与人类价值观对齐方面的价值，尽管通用推理准确性略有下降。", "keywords": "视觉-语言模型, 安全性, 意图感知, 提示工程, 多模态安全", "comments": "SIA的创新之处在于其无需训练的提示工程框架和三阶段推理过程，能够主动检测并缓解多模态输入中潜在的有害意图，解决了现有方法难以处理的“输入组合”风险。其重要性在于显著提升了视觉-语言模型的安全性，使其更好地与人类价值观对齐。一个限制是，虽然安全性大幅提升，但通用推理准确性略有下降，这可能需要在实际部署中进行权衡。"}}
{"id": "2501.06348", "title": "Why Automate This? Exploring Correlations between Desire for Robotic Automation, Invested Time and Well-Being", "authors": ["Ruchira Ray", "Leona Pang", "Sanjana Srivastava", "Li Fei-Fei", "Samantha Shorey", "Roberto Martín-Martín"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      20 pages, 14 figures", "url": "http://arxiv.org/abs/2501.06348v3", "summary": "Understanding the motivations underlying the human inclination to automate\ntasks is vital to developing truly helpful robots integrated into daily life.\nAccordingly, we ask: are individuals more inclined to automate chores based on\nthe time they consume or the feelings experienced while performing them? This\nstudy explores these preferences and whether they vary across different social\ngroups (i.e., gender category and income level). Leveraging data from the\nBEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use\nSurvey Well-Being Module, we investigate the relationship between the desire\nfor automation, time spent on daily activities, and their associated feelings -\nHappiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.\nOur key findings show that, despite common assumptions, time spent does not\nstrongly relate to the desire for automation for the general population. For\nthe feelings analyzed, only happiness and pain are key indicators. Significant\ndifferences by gender and economic level also emerged: Women prefer to automate\nstressful activities, whereas men prefer to automate those that make them\nunhappy; mid-income individuals prioritize automating less enjoyable and\nmeaningful activities, while low and high-income show no significant\ncorrelations. We hope our research helps motivate technologies to develop\nrobots that match the priorities of potential users, moving domestic robotics\ntoward more socially relevant solutions. We open-source all the data, including\nan online tool that enables the community to replicate our analysis and explore\nadditional trends at https://robin-lab.cs.utexas.edu/why-automate-this/.", "comment": "20 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2501.06348v3", "cate": "cs.HC", "date": "2025-01-10", "updated": "2025-07-22", "AI": {"title_translation": "为什么自动化这项任务？探索机器人自动化意愿、投入时间和幸福感之间的相关性", "tldr": "研究发现，人们对自动化家务的意愿与花费的时间关系不大，但与幸福感和痛苦感有关，且存在性别和收入差异。", "motivation": "了解人类自动化任务的内在动机对于开发真正有用的、融入日常生活的机器人至关重要。", "method": "利用BEHAVIOR-1K数据集、美国时间使用调查和美国时间使用调查幸福感模块的数据，研究了自动化意愿、日常活动花费的时间及其相关感受（幸福感、意义感、悲伤、痛苦、压力或疲惫）之间的关系。同时，探讨了这些偏好是否在不同社会群体（性别和收入水平）中存在差异。", "result": "主要发现表明，尽管存在普遍假设，但对于普通人群而言，花费的时间与自动化意愿没有强关联。在分析的感受中，只有幸福感和痛苦是关键指标。性别和经济水平也存在显著差异：女性倾向于自动化有压力的活动，而男性倾向于自动化让他们不快乐的活动；中等收入人群优先自动化不那么愉快和有意义的活动，而低收入和高收入人群没有显示出显著相关性。", "conclusion": "本研究旨在帮助激励技术开发与潜在用户优先事项相匹配的机器人，使家用机器人朝着更具社会相关性的解决方案发展。研究还开源了所有数据和一个在线工具。", "translation": "了解人类自动化任务的内在动机对于开发真正有用的、融入日常生活的机器人至关重要。因此，我们提出问题：个人更倾向于根据消耗的时间还是执行任务时的感受来自动化家务？本研究探讨了这些偏好以及它们是否因不同的社会群体（即性别类别和收入水平）而异。我们利用BEHAVIOR-1K数据集、美国时间使用调查和美国时间使用调查幸福感模块的数据，调查了自动化意愿、日常活动花费的时间及其相关感受——幸福感、意义感、悲伤、痛苦、压力或疲惫——之间的关系。我们的主要发现表明，尽管存在普遍假设，但对于普通人群而言，花费的时间与自动化意愿没有强关联。在分析的感受中，只有幸福感和痛苦是关键指标。性别和经济水平也存在显著差异：女性倾向于自动化有压力的活动，而男性倾向于自动化让他们不快乐的活动；中等收入人群优先自动化不那么愉快和有意义的活动，而低收入和高收入人群没有显示出显著相关性。我们希望我们的研究有助于激励技术开发与潜在用户优先事项相匹配的机器人，使家用机器人朝着更具社会相关性的解决方案发展。我们开源了所有数据，包括一个在线工具，使社区能够复制我们的分析并探索更多趋势，网址为 https://robin-lab.cs.utexas.edu/why-automate-this/。", "summary": "本研究旨在理解人们自动化家务的动机，以促进开发更实用的家用机器人。研究利用多项数据集，分析了自动化意愿与任务花费时间、任务感受（如幸福感、痛苦、压力）以及不同社会群体（性别、收入）间的关系。结果显示，自动化意愿与任务耗时无强关联，但与幸福感和痛苦感相关。此外，研究揭示了显著的性别和收入差异：女性倾向于自动化压力大的活动，男性则倾向于自动化使其不快乐的活动；中等收入者优先自动化不愉快或无意义的活动，而低高收入者无显著相关性。本研究强调，未来机器人应根据用户优先级进行设计，并开源了数据和分析工具。", "keywords": "机器人自动化, 自动化意愿, 时间使用, 幸福感, 社会群体差异", "comments": "这项研究具有重要的社会相关性，它挑战了关于自动化意愿的普遍假设（即时间是主要驱动因素），并揭示了情感因素和社会经济差异在其中扮演的关键角色。其创新之处在于将自动化意愿与具体情感和群体差异联系起来，这为未来家用机器人的设计提供了更人性化和个性化的指导。开源数据和工具的做法也值得称赞，有助于促进该领域的研究。"}}
{"id": "2503.12122", "title": "ICCO: Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control", "authors": ["Yoshiki Yano", "Kazuki Shibata", "Maarten Kokshoorn", "Takamitsu Matsubara"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 9 figures, to be published in the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems", "url": "http://arxiv.org/abs/2503.12122v2", "summary": "Recent advances in Large Language Models (LLMs) have permitted the\ndevelopment of language-guided multi-robot systems, which allow robots to\nexecute tasks based on natural language instructions. However, achieving\neffective coordination in distributed multi-agent environments remains\nchallenging due to (1) misalignment between instructions and task requirements\nand (2) inconsistency in robot behaviors when they independently interpret\nambiguous instructions. To address these challenges, we propose\nInstruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement\nLearning (MARL) framework designed to enhance coordination in language-guided\nmulti-robot systems. ICCO consists of a Coordinator agent and multiple Local\nAgents, where the Coordinator generates Task-Aligned and Consistent\nInstructions (TACI) by integrating language instructions with environmental\nstates, ensuring task alignment and behavioral consistency. The Coordinator and\nLocal Agents are jointly trained to optimize a reward function that balances\ntask efficiency and instruction following. A Consistency Enhancement Term is\nadded to the learning objective to maximize mutual information between\ninstructions and robot behaviors, further improving coordination. Simulation\nand real-world experiments validate the effectiveness of ICCO in achieving\nlanguage-guided task-aligned multi-robot control. The demonstration can be\nfound at https://yanoyoshiki.github.io/ICCO/.", "comment": "8 pages, 9 figures, to be published in the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems", "pdf_url": "http://arxiv.org/pdf/2503.12122v2", "cate": "cs.RO", "date": "2025-03-15", "updated": "2025-07-23", "AI": {"title_translation": "ICCO：学习指令条件协调器以实现语言引导的任务对齐多机器人控制", "tldr": "ICCO是一个多智能体强化学习框架，通过一个协调器生成任务对齐且一致的指令，解决了语言引导多机器人系统中的协调挑战。", "motivation": "在分布式多智能体环境中，语言引导的多机器人系统难以有效协调，原因在于指令与任务要求不一致，以及机器人独立解释模糊指令时行为不一致。", "method": "提出指令条件协调器（ICCO），一个多智能体强化学习（MARL）框架。ICCO包含一个协调器智能体和多个局部智能体。协调器通过整合语言指令和环境状态生成任务对齐且一致的指令（TACI），确保任务对齐和行为一致性。协调器和局部智能体联合训练以优化平衡任务效率和指令遵循的奖励函数。学习目标中加入了“一致性增强项”以最大化指令与机器人行为之间的互信息，进一步提升协调性。", "result": "仿真和真实世界实验验证了ICCO在实现语言引导的任务对齐多机器人控制方面的有效性。", "conclusion": "ICCO被证明在实现语言引导的任务对齐多机器人控制方面是有效的。", "translation": "大型语言模型（LLMs）的最新进展使得语言引导的多机器人系统得以发展，这些系统允许机器人根据自然语言指令执行任务。然而，在分布式多智能体环境中实现有效协调仍然具有挑战性，原因在于 (1) 指令与任务要求之间的不匹配，以及 (2) 机器人独立解释模糊指令时行为的不一致性。为了解决这些挑战，我们提出了指令条件协调器（ICCO），这是一个多智能体强化学习（MARL）框架，旨在增强语言引导多机器人系统中的协调性。ICCO由一个协调器智能体和多个局部智能体组成，其中协调器通过整合语言指令和环境状态来生成任务对齐且一致的指令（TACI），从而确保任务对齐和行为一致性。协调器和局部智能体共同训练，以优化一个平衡任务效率和指令遵循的奖励函数。学习目标中添加了一个“一致性增强项”，以最大化指令与机器人行为之间的互信息，进一步改善协调性。仿真和真实世界实验验证了ICCO在实现语言引导的任务对齐多机器人控制方面的有效性。演示可在 https://yanoyoshiki.github.io/ICCO/ 上找到。", "summary": "该论文提出了一种名为ICCO（指令条件协调器）的多智能体强化学习框架，旨在解决语言引导多机器人系统中指令与任务不匹配及行为不一致的协调挑战。ICCO包含一个协调器智能体和多个局部智能体，协调器负责生成任务对齐且一致的指令。通过联合训练和引入一致性增强项，ICCO有效提升了多机器人系统的协调性，并在仿真和真实世界实验中得到验证。", "keywords": "指令条件协调器, 多机器人控制, 语言引导, 任务对齐, 多智能体强化学习", "comments": "ICCO通过引入指令条件协调器和一致性增强项，有效解决了语言引导多机器人系统中指令与任务不对齐以及行为不一致的协调难题，为提升多机器人系统的鲁棒性和效率提供了新途径。其将语言指令与环境状态结合生成一致指令的机制具有创新性。"}}
{"id": "2312.11053", "title": "Conflict Detection for Temporal Knowledge Graphs:A Fast Constraint Mining Algorithm and New Benchmarks", "authors": ["Jianhao Chen", "Junyang Ren", "Wentao Ding", "Haoyuan Ouyang", "Wei Hu", "Yuzhong Qu"], "categories": ["cs.AI", "cs.DB"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.11053v2", "summary": "Temporal facts, which are used to describe events that occur during specific\ntime periods, have become a topic of increased interest in the field of\nknowledge graph (KG) research. In terms of quality management, the introduction\nof time restrictions brings new challenges to maintaining the temporal\nconsistency of KGs. Previous studies rely on manually enumerated temporal\nconstraints to detect conflicts, which are labor-intensive and may have\ngranularity issues. To address this problem, we start from the common pattern\nof temporal facts and propose a pattern-based temporal constraint mining\nmethod, PaTeCon. Unlike previous studies, PaTeCon uses graph patterns and\nstatistical information relevant to the given KG to automatically generate\ntemporal constraints, without the need for human experts. In this paper, we\nillustrate how this method can be optimized to achieve significant speed\nimprovement. We also annotate Wikidata and Freebase to build two new benchmarks\nfor conflict detection. Extensive experiments demonstrate that our\npattern-based automatic constraint mining approach is highly effective in\ngenerating valuable temporal constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.11053v2", "cate": "cs.AI", "date": "2023-12-18", "updated": "2025-07-23", "AI": {"title_translation": "临时知识图谱的冲突检测：一种快速约束挖掘算法和新基准", "tldr": "提出PaTeCon，一种自动挖掘时序知识图谱时间约束的方法，解决了手动约束的痛点，并构建了新基准，实验证明其有效。", "motivation": "知识图谱中时间事实的引入给维护时间一致性带来了新挑战。现有方法依赖手动枚举时间约束，耗费大量人力且可能存在粒度问题。", "method": "提出基于模式的时间约束挖掘方法PaTeCon。PaTeCon利用图模式和统计信息自动生成时间约束，无需人工干预。该方法还进行了速度优化，并构建了两个新的冲突检测基准（基于Wikidata和Freebase）。", "result": "大量实验表明，所提出的基于模式的自动约束挖掘方法在生成有价值的时间约束方面非常有效。", "conclusion": "本文提出了一种有效的自动时间约束挖掘方法PaTeCon，解决了现有方法的局限性，并为时间知识图谱的冲突检测提供了新的基准，有效提高了时间约束的生成效率和质量。", "translation": "临时事实，用于描述在特定时间段内发生的事件，已成为知识图谱（KG）研究领域日益关注的话题。在质量管理方面，时间限制的引入为维护知识图谱的时间一致性带来了新的挑战。以往的研究依赖于手动枚举时间约束来检测冲突，这耗费大量人力且可能存在粒度问题。为了解决这个问题，我们从时间事实的常见模式出发，提出了一种基于模式的时间约束挖掘方法PaTeCon。与以往的研究不同，PaTeCon利用图模式和与给定知识图谱相关的统计信息自动生成时间约束，无需人类专家。在本文中，我们阐述了如何优化此方法以实现显著的速度提升。我们还标注了Wikidata和Freebase，以构建两个新的冲突检测基准。大量实验表明，我们基于模式的自动约束挖掘方法在生成有价值的时间约束方面非常有效。", "summary": "本文针对时间知识图谱中时间一致性维护的挑战，提出了一种名为PaTeCon的模式化时间约束自动挖掘方法。该方法利用图模式和统计信息自动生成时间约束，避免了传统手动枚举方法的耗时和粒度问题。研究还对该方法进行了速度优化，并构建了两个新的冲突检测基准。实验结果验证了PaTeCon在生成有效时间约束方面的显著效果。", "keywords": "时间知识图谱, 冲突检测, 约束挖掘, PaTeCon, 基准", "comments": "这篇论文的创新点在于提出了一个无需人工干预的自动时间约束挖掘方法PaTeCon，解决了传统方法效率低下和粒度问题。同时，构建了新的基准数据集，对该领域的研究具有重要推动作用。其速度优化也提升了方法的实用性。"}}
{"id": "2410.04571", "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      superalignment, weak-to-strong generalization on unseen OOD task; formerly appeared as arXiv:2505.21959v1 which was uploaded as a new submission in error", "url": "http://arxiv.org/abs/2410.04571v3", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called **EnsemW2S**, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4%, and 3.2%\nimprovements on ID datasets and, upto 6% and 2.28% on OOD datasets for experts\nand student models respectively, underscoring the effectiveness of our proposed\nmethod in advancing W2S generalization.", "comment": "superalignment, weak-to-strong generalization on unseen OOD task;\n  formerly appeared as arXiv:2505.21959v1 which was uploaded as a new\n  submission in error", "pdf_url": "http://arxiv.org/pdf/2410.04571v3", "cate": "cs.LG", "date": "2024-10-06", "updated": "2025-07-23", "AI": {"title_translation": "EnsemW2S：通过大型语言模型集成增强弱到强泛化能力", "tldr": "EnsemW2S通过迭代集成弱专家模型来增强大型语言模型的弱到强泛化能力，从而显著提升在分布内和分布外数据集上的性能。", "motivation": "随着大型语言模型(LLMs)性能接近并可能超越人类水平，开发有效方法来使用较小的、仅暴露于人类水平数据的模型来监督和增强这些强大的LLMs变得至关重要。本文旨在解决弱到强(W2S)泛化挑战。", "method": "本文提出了一种名为EnsemW2S的新方法，旨在改进弱专家模型。该方法采用令牌级集成策略，迭代结合多个弱专家模型，系统地解决前一迭代中发现的不足。通过不断完善这些弱模型，显著增强了它们集体监督更强学生模型的能力。", "result": "EnsemW2S在分布内(ID)数据集上使专家模型和学生模型分别取得了4%和3.2%的提升；在分布外(OOD)数据集上，专家模型和学生模型分别取得了高达6%和2.28%的提升。", "conclusion": "本文提出的EnsemW2S方法在推进弱到强泛化方面表现出显著的有效性，通过集成弱专家模型显著提升了LLMs的泛化性能。", "translation": "随着大型语言模型（LLMs）迅速接近并可能超越人类水平的性能，开发能够有效监督和增强这些强大模型的方法变得势在必行，这些方法使用较小的、仅暴露于人类水平数据的模型。我们通过提出一种旨在改进弱专家模型的新方法来解决这一关键的弱到强（W2S）泛化挑战，通过在相同的有限人类水平数据上进行训练，使其能够泛化到复杂的、超人类水平的任务。我们的方法名为 EnsemW2S，采用令牌级集成策略，迭代结合多个弱专家模型，系统地解决前一迭代中发现的不足。通过不断完善这些弱模型，我们显著增强了它们集体监督更强学生模型的能力。我们广泛评估了弱专家集成模型和后续的强学生模型在分布内（ID）和分布外（OOD）数据集上的泛化性能。对于 OOD，我们特别引入了问题难度作为定义分布偏移的额外维度。我们的实证结果表明，在 ID 数据集上，专家模型和学生模型分别取得了 4% 和 3.2% 的显著改进，在 OOD 数据集上，专家模型和学生模型分别取得了高达 6% 和 2.28% 的改进，这强调了我们提出的方法在推进 W2S 泛化方面的有效性。", "summary": "本文提出了EnsemW2S，一种旨在解决大型语言模型(LLMs)弱到强(W2S)泛化挑战的新方法。EnsemW2S通过令牌级集成策略，迭代结合并精炼多个在有限人类水平数据上训练的弱专家模型，以使其能够监督和泛化到更复杂的超人类任务。实验结果表明，该方法显著提升了集成专家模型和学生模型在分布内和分布外数据集上的性能，证明了其在增强LLMs泛化能力方面的有效性。", "keywords": "大型语言模型, 弱到强泛化, 模型集成, 监督学习, 分布外泛化", "comments": "EnsemW2S的创新点在于其迭代的令牌级集成策略，有效地利用了多个弱专家模型来协同提升对强学生模型的监督能力。这对于在有限人类水平数据下训练出能处理超人类任务的LLMs具有重要意义。该方法通过解决弱到强泛化这一关键挑战，为LLMs的实际应用和性能提升开辟了新的途径。"}}
{"id": "2507.17093", "title": "Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing", "authors": ["Danushka Liyanage", "Nelum Attanayake", "Zijian Luo", "Rahul Gopinath"], "categories": ["cs.SE", "68N30", "D.2.4; D.2.5; D.2.8"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      ICSME'25 Registered Report", "url": "http://arxiv.org/abs/2507.17093v1", "summary": "Background: Fuzzers are often guided by coverage, making the estimation of\nmaximum achievable coverage a key concern in fuzzing. However, achieving 100%\ncoverage is infeasible for most real-world software systems, regardless of\neffort. While static reachability analysis can provide an upper bound, it is\noften highly inaccurate. Recently, statistical estimation methods based on\nspecies richness estimators from biostatistics have been proposed as a\npotential solution. Yet, the lack of reliable benchmarks with labeled ground\ntruth has limited rigorous evaluation of their accuracy.\n  Objective: This work examines the reliability of reachability estimators from\ntwo axes: addressing the lack of labeled ground truth and evaluating their\nreliability on real-world programs.\n  Methods: (1) To address the challenge of labeled ground truth, we propose an\nevaluation framework that synthetically generates large programs with complex\ncontrol flows, ensuring well-defined reachability and providing ground truth\nfor evaluation. (2) To address the criticism from use of synthetic benchmarks,\nwe adapt a reliability check for reachability estimators on real-world\nbenchmarks without labeled ground truth -- by varying the size of sampling\nunits, which, in theory, should not affect the estimate.\n  Results: These two studies together will help answer the question of whether\ncurrent reachability estimators are reliable, and defines a protocol to\nevaluate future improvements in reachability estimation.", "comment": "ICSME'25 Registered Report", "pdf_url": "http://arxiv.org/pdf/2507.17093v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "评估模糊测试中统计最大覆盖估计器的可靠性", "tldr": "本文提出了一种评估模糊测试中统计最大覆盖估计器可靠性的方法，通过构建带有真实值的合成基准和在无真实值真实程序上进行可靠性检查，旨在解决现有评估方法的局限性。", "motivation": "模糊测试中覆盖率引导至关重要，但实现100%覆盖率几乎不可能，且静态分析不准确。尽管统计估计方法被提出，但由于缺乏带有标记真实值的可靠基准，其准确性评估受限。", "method": "1. 提出一个评估框架，合成生成具有复杂控制流的大型程序，以提供可达性的真实值。2. 针对真实世界基准，在没有标记真实值的情况下，通过改变采样单元大小来检查可达性估计器的可靠性。", "result": "这两项研究将共同帮助回答当前可达性估计器是否可靠的问题，并定义一个评估未来可达性估计改进的协议。", "conclusion": "本研究通过提出一个合成程序生成框架和一种针对真实世界程序的可靠性检查方法，旨在评估统计最大覆盖估计器的可靠性，并为未来的可达性估计改进提供了评估协议。", "translation": "背景：模糊测试器通常由覆盖率引导，这使得最大可实现覆盖率的估计成为模糊测试中的一个关键问题。然而，对于大多数实际软件系统而言，无论付出多少努力，实现100%覆盖率都是不可行的。虽然静态可达性分析可以提供上限，但其通常高度不准确。最近，基于生物统计学中物种丰富度估计器的统计估计方法被提出作为一种潜在的解决方案。然而，缺乏带有标记真实值的可靠基准限制了对其准确性的严格评估。\n目标：这项工作从两个方面审视可达性估计器的可靠性：解决标记真实值的缺乏，并评估它们在真实世界程序上的可靠性。\n方法：（1）为了解决标记真实值的挑战，我们提出了一个评估框架，该框架综合生成具有复杂控制流的大型程序，确保明确的可达性并为评估提供真实值。（2）为了解决使用合成基准的批评，我们对没有标记真实值的真实世界基准的可达性估计器进行了可靠性检查——通过改变采样单元的大小，这在理论上不应影响估计。\n结果：这两项研究将共同帮助回答当前可达性估计器是否可靠的问题，并定义一个评估未来可达性估计改进的协议。", "summary": "本文旨在评估模糊测试中统计最大覆盖估计器的可靠性。为了解决缺乏标记真实值的挑战，作者提出了一个评估框架，通过合成生成具有复杂控制流的程序来提供真实值。同时，为了在没有标记真实值的真实世界程序上进行评估，该工作通过改变采样单元的大小来适应可靠性检查。这项研究旨在确定当前可达性估计器的可靠性，并为未来的改进定义评估协议。", "keywords": "模糊测试, 覆盖率估计, 可靠性, 统计方法, 基准测试", "comments": "本文解决了模糊测试中一个关键且实际的问题：如何准确可靠地评估代码覆盖率。其创新之处在于采用了双重方法：既创建了带有明确真实值的合成基准用于受控评估，又开发了一种在没有显式真实值的情况下评估真实世界系统可靠性的方法。这种综合性方法为覆盖率估计器提供了一个更健壮和实用的评估框架，对于提升模糊测试器的效率至关重要。"}}
{"id": "2507.17088", "title": "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning", "authors": ["Arkajyoti Mitra", "Afia Anjum", "Paul Agbaje", "Mert Pesé", "Habeeb Olufowobi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17088v1", "summary": "Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot\nlearning capabilities, making them essential for several downstream tasks.\nHowever, fine-tuning these models at scale remains challenging, particularly in\nfederated environments where data is decentralized and non-iid across clients.\nExisting parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)\nreduce computational overhead but struggle with heterogeneous client data,\nleading to suboptimal generalization. To address these challenges, we propose\nFedVLM, a federated LoRA fine-tuning framework that enables decentralized\nadaptation of VLMs while preserving model privacy and reducing reliance on\ncentralized training. To further tackle data heterogeneity, we introduce\npersonalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each\nclient's unique data distribution, significantly improving local adaptation\nwhile maintaining global model aggregation. Experiments on the RLAIF-V dataset\nshow that pLoRA improves client-specific performance by 24.5% over standard\nLoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a\nscalable and efficient solution for fine-tuning VLMs in federated settings,\nadvancing personalized adaptation in distributed learning scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17088v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "FedVLM：通过联邦学习实现可扩展的个性化视觉-语言模型", "tldr": "FedVLM提出了一种联邦LoRA微调框架，通过引入个性化LoRA（pLoRA）来解决联邦环境中视觉-语言模型（VLM）微调的数据异质性问题，显著提高了非独立同分布（non-iid）数据下的客户端性能。", "motivation": "大规模微调视觉-语言模型（VLM）面临挑战，尤其是在数据分散且非独立同分布（non-iid）的联邦环境中。现有的参数高效微调方法（如LoRA）难以处理异构客户端数据，导致泛化能力不佳。", "method": "本文提出了FedVLM，一个联邦LoRA微调框架，用于在保护模型隐私的同时，去中心化地适应VLM并减少对集中训练的依赖。为进一步解决数据异质性问题，引入了个性化LoRA（pLoRA），它能根据每个客户端独特的数据分布动态调整LoRA参数，在保持全局模型聚合的同时显著改善局部适应性。", "result": "在RLAIF-V数据集上的实验表明，pLoRA相比标准LoRA，将客户端特定性能提高了24.5%，证明了其在非独立同分布（non-iid）设置下卓越的适应能力。", "conclusion": "FedVLM为联邦环境中的VLM微调提供了一个可扩展且高效的解决方案，推动了分布式学习场景中的个性化适应。", "translation": "视觉-语言模型（VLM）展示了令人印象深刻的零样本和少样本学习能力，这使得它们对多个下游任务至关重要。然而，大规模微调这些模型仍然具有挑战性，特别是在数据去中心化且客户端之间非独立同分布（non-iid）的联邦环境中。现有的参数高效微调方法，如LoRA（低秩适应），虽然降低了计算开销，但难以处理异构客户端数据，导致次优的泛化能力。为了解决这些挑战，我们提出了FedVLM，一个联邦LoRA微调框架，它能够在保护模型隐私和减少对集中训练依赖的同时，实现VLM的去中心化适应。为了进一步解决数据异质性问题，我们引入了个性化LoRA（pLoRA），它能够根据每个客户端独特的数据分布动态调整LoRA参数，显著改善局部适应性，同时保持全局模型聚合。在RLAIF-V数据集上的实验表明，pLoRA相比标准LoRA，将客户端特定性能提高了24.5%，证明了其在非独立同分布（non-iid）设置下卓越的适应能力。FedVLM为联邦环境中的VLM微调提供了一个可扩展且高效的解决方案，推动了分布式学习场景中的个性化适应。", "summary": "本文提出了FedVLM，一个针对联邦环境中视觉-语言模型（VLM）的LoRA微调框架，旨在解决数据去中心化和非独立同分布（non-iid）带来的挑战。核心创新是个性化LoRA（pLoRA），它能根据每个客户端独特的数据分布动态调整LoRA参数，显著提升了局部适应性和客户端性能。实验证明，pLoRA在非独立同分布设置下，相比标准LoRA将客户端性能提高了24.5%。FedVLM为联邦学习中VLM的个性化适应提供了一个可扩展且高效的解决方案。", "keywords": "联邦学习, 视觉-语言模型, LoRA, 个性化, 数据异质性", "comments": "该论文通过引入个性化LoRA（pLoRA）在联邦学习框架中解决了VLM微调的数据异质性问题，这是一个重要的创新点。它使得模型能够更好地适应每个客户端的独特数据分布，同时保持联邦学习的隐私和去中心化优势。该方法对于在真实世界复杂场景中部署VLM具有重要意义。"}}
{"id": "2507.17347", "title": "Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation", "authors": ["Haotian Chen", "Zhiyong Xiao"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17347v1", "summary": "In the field of food image processing, efficient semantic segmentation\ntechniques are crucial for industrial applications. However, existing\nlarge-scale Transformer-based models (such as FoodSAM) face challenges in\nmeeting practical deploymentrequirements due to their massive parameter counts\nand high computational resource demands. This paper introduces TUNable Adapter\nmodule (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that\nintegrates multiscale trainable adapters into the Swin Transformer\narchitecture, achieving high-performance food image segmentation by updating\nonly 4% of the parameters. The core innovation of Swin-TUNA lies in its\nhierarchical feature adaptation mechanism: it designs separable convolutions in\ndepth and dimensional mappings of varying scales to address the differences in\nfeatures between shallow and deep networks, combined with a dynamic balancing\nstrategy for tasks-agnostic and task-specific features. Experiments demonstrate\nthat this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and\nUECFoodPix Complete datasets, respectively, surpassing the fully parameterized\nFoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).\nFurthermore, Swin-TUNA exhibits faster convergence and stronger generalization\ncapabilities in low-data scenarios, providing an efficient solution for\nassembling lightweight food image.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17347v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Swin-TUNA：一种用于精确食物图像分割的新型PEFT方法", "tldr": "Swin-TUNA是一种参数高效微调（PEFT）方法，通过引入多尺度可训练适配器到Swin Transformer架构中，以极少的参数（4%）实现了高性能食物图像分割，解决了现有大型模型参数量大、计算资源需求高的问题。", "motivation": "现有大型Transformer模型（如FoodSAM）参数量巨大，计算资源需求高，难以满足实际部署要求，而高效的语义分割技术对于食物图像处理的工业应用至关重要。", "method": "Swin-TUNA是一种参数高效微调（PEFT）方法，它将多尺度可训练适配器集成到Swin Transformer架构中。其核心创新在于分层特征适应机制，通过设计深度可分离卷积和不同尺度的维度映射来解决浅层和深层网络之间的特征差异，并结合了任务无关和任务特定特征的动态平衡策略。", "result": "Swin-TUNA在FoodSeg103和UECFoodPix Complete数据集上分别达到了50.56%和74.94%的mIoU，超越了全参数化的FoodSAM模型，同时将参数量减少了98.7%（仅8.13M）。此外，Swin-TUNA在低数据量场景下表现出更快的收敛速度和更强的泛化能力。", "conclusion": "Swin-TUNA为构建轻量级食物图像分割模型提供了一种高效的解决方案，通过参数高效微调方法，在保证高性能的同时大幅降低了模型复杂度和资源需求。", "translation": "在食物图像处理领域，高效的语义分割技术对于工业应用至关重要。然而，现有的大规模基于Transformer的模型（如FoodSAM）由于其庞大的参数量和高计算资源需求，在满足实际部署要求方面面临挑战。本文介绍了一种可调适配器模块（Swin-TUNA），这是一种参数高效微调（PEFT）方法，它将多尺度可训练适配器集成到Swin Transformer架构中，通过仅更新4%的参数实现了高性能的食物图像分割。Swin-TUNA的核心创新在于其分层特征适应机制：它设计了深度可分离卷积和不同尺度的维度映射，以解决浅层和深层网络之间的特征差异，并结合了任务无关和任务特定特征的动态平衡策略。实验表明，该方法在FoodSeg103和UECFoodPix Complete数据集上分别达到了50.56%和74.94%的mIoU，超越了全参数化的FoodSAM模型，同时将参数量减少了98.7%（仅8.13M）。此外，Swin-TUNA在低数据量场景下表现出更快的收敛速度和更强的泛化能力，为组装轻量级食物图像提供了高效解决方案。", "summary": "Swin-TUNA提出了一种新颖的参数高效微调（PEFT）方法，通过在Swin Transformer中集成多尺度可训练适配器，实现高精度食物图像分割，同时大幅减少模型参数和计算需求。该方法通过分层特征适应机制，包括深度可分离卷积和动态平衡策略，有效处理不同网络层的特征差异。实验证明，Swin-TUNA在保持甚至超越现有大型模型性能的同时，显著降低了参数量，并在低数据量场景下展现出优越的收敛性和泛化能力，为轻量级食物图像分割提供了高效方案。", "keywords": "食物图像分割, PEFT, Swin Transformer, 参数高效微调, 语义分割", "comments": "Swin-TUNA的创新性在于其独特的分层特征适应机制，通过引入多尺度可训练适配器和动态平衡策略，在PEFT框架下有效解决了大型模型在食物图像分割领域的部署难题。其在极少参数量下达到甚至超越全参数模型的性能，对于资源受限的工业应用具有重要意义。"}}
{"id": "2507.16915", "title": "Avoiding spectral pollution for transfer operators using residuals", "authors": ["April Herwig", "Matthew J. Colbrook", "Oliver Junge", "Péter Koltai", "Julia Slipantschuk"], "categories": ["math.DS", "cs.LG", "cs.NA", "math.NA", "math.SP", "stat.ML"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16915v1", "summary": "Koopman operator theory enables linear analysis of nonlinear dynamical\nsystems by lifting their evolution to infinite-dimensional function spaces.\nHowever, finite-dimensional approximations of Koopman and transfer\n(Frobenius--Perron) operators are prone to spectral pollution, introducing\nspurious eigenvalues that can compromise spectral computations. While recent\nadvances have yielded provably convergent methods for Koopman operators,\nanalogous tools for general transfer operators remain limited. In this paper,\nwe present algorithms for computing spectral properties of transfer operators\nwithout spectral pollution, including extensions to the Hardy-Hilbert space.\nCase studies--ranging from families of Blaschke maps with known spectrum to a\nmolecular dynamics model of protein folding--demonstrate the accuracy and\nflexibility of our approach. Notably, we demonstrate that spectral features can\narise even when the corresponding eigenfunctions lie outside the chosen space,\nhighlighting the functional-analytic subtleties in defining the \"true\" Koopman\nspectrum. Our methods offer robust tools for spectral estimation across a broad\nrange of applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16915v1", "cate": "math.DS", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "使用残差避免传递算子的谱污染", "tldr": "本文提出了一种计算传递算子谱性质的算法，有效避免了谱污染问题，为非线性动力系统的谱分析提供了稳健工具。", "motivation": "有限维近似Koopman算子和传递算子容易产生谱污染，引入虚假特征值，从而影响谱计算。尽管Koopman算子已有收敛方法，但通用传递算子的类似工具仍然有限。", "method": "本文提出了计算传递算子谱性质的算法，能够避免谱污染，并扩展到Hardy-Hilbert空间。该方法利用残差（根据标题推断）来避免谱污染。", "result": "通过Blaschke映射族和蛋白质折叠分子动力学模型等案例研究，证明了该方法的准确性和灵活性。值得注意的是，研究表明即使相应的特征函数位于所选空间之外，谱特征也可能出现。", "conclusion": "本文提出的方法为广泛应用中的谱估计提供了稳健的工具，并强调了在定义“真实”Koopman谱时功能分析上的微妙之处。", "translation": "Koopman算子理论通过将非线性动力系统的演化提升到无限维函数空间，从而实现对其的线性分析。然而，Koopman算子和传递（Frobenius-Perron）算子的有限维近似容易受到谱污染的影响，引入虚假特征值，这可能会损害谱计算。尽管最近的进展为Koopman算子提供了可证明收敛的方法，但对于通用传递算子的类似工具仍然有限。在本文中，我们提出了在没有谱污染的情况下计算传递算子谱性质的算法，包括对Hardy-Hilbert空间的扩展。从具有已知谱的Blaschke映射族到蛋白质折叠的分子动力学模型等案例研究，都证明了我们方法的准确性和灵活性。值得注意的是，我们证明了即使相应的特征函数位于所选空间之外，谱特征也可能出现，这突出了在定义“真实”Koopman谱时功能分析上的微妙之处。我们的方法为广泛应用中的谱估计提供了稳健的工具。", "summary": "本文针对Koopman算子和传递算子有限维近似中存在的谱污染问题，提出了一系列计算传递算子谱性质的算法，有效避免了虚假特征值的引入。这些算法，包括对Hardy-Hilbert空间的扩展，通过案例研究（如Blaschke映射和蛋白质折叠模型）展示了其准确性和灵活性。研究还发现，谱特征即便在特征函数超出所选空间时也能出现，这揭示了Koopman谱定义的复杂性。该方法为多种应用场景下的谱估计提供了稳健的解决方案。", "keywords": "谱污染, 传递算子, Koopman算子, 残差, 谱估计", "comments": "该论文通过引入避免谱污染的算法，为非线性动力系统分析中的Koopman和传递算子理论提供了重要的进步。其创新点在于解决了现有方法在有限维近似中谱污染的局限性，特别是在通用传递算子方面。通过扩展到Hardy-Hilbert空间并展示在蛋白质折叠等复杂系统中的应用，该工作显著提升了谱估计的鲁棒性和准确性，对于理解非线性系统的内在动力学具有重要意义。"}}
{"id": "2412.02808", "title": "Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation", "authors": ["Raphael Ruschel", "Md Awsafur Rahman", "Hardik Prajapati", "Suya You", "B. S. Manjuanth"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.02808v2", "summary": "Understanding video content is pivotal for advancing real-world applications\nlike activity recognition, autonomous systems, and human-computer interaction.\nWhile scene graphs are adept at capturing spatial relationships between objects\nin individual frames, extending these representations to capture dynamic\ninteractions across video sequences remains a significant challenge. To address\nthis, we present TCDSG, Temporally Consistent Dynamic Scene Graphs, an\ninnovative end-to-end framework that detects, tracks, and links subject-object\nrelationships across time, generating action tracklets, temporally consistent\nsequences of entities and their interactions. Our approach leverages a novel\nbipartite matching mechanism, enhanced by adaptive decoder queries and feedback\nloops, ensuring temporal coherence and robust tracking over extended sequences.\nThis method not only establishes a new benchmark by achieving over 60%\nimprovement in temporal recall@k on the Action Genome, OpenPVSG, and MEVA\ndatasets but also pioneers the augmentation of MEVA with persistent object ID\nannotations for comprehensive tracklet generation. By seamlessly integrating\nspatial and temporal dynamics, our work sets a new standard in multi-frame\nvideo analysis, opening new avenues for high-impact applications in\nsurveillance, autonomous navigation, and beyond.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.02808v2", "cate": "cs.CV", "date": "2024-12-03", "updated": "2025-07-22", "AI": {"title_translation": "时间一致的动态场景图：一种用于动作轨迹生成端到端方法", "tldr": "本文提出TCDSG，一个端到端框架，用于在视频序列中检测、跟踪和链接主体-客体关系，生成时间一致的动作轨迹，并在多个数据集上显著提高了时间召回率。", "motivation": "理解视频内容对于活动识别、自主系统和人机交互等现实应用至关重要。虽然场景图擅长捕捉单个帧中对象的空间关系，但将这些表示扩展到捕捉视频序列中的动态交互仍然是一个重大挑战。", "method": "本文提出了TCDSG（时间一致的动态场景图），一个创新的端到端框架，它在时间上检测、跟踪和链接主体-客体关系，生成动作轨迹。该方法利用了一种新颖的二分匹配机制，并通过自适应解码器查询和反馈循环进行增强，确保了长期序列的时间连贯性和鲁棒跟踪。", "result": "该方法在Action Genome、OpenPVSG和MEVA数据集上的时间召回率@k方面实现了超过60%的改进，并首次通过持久对象ID注释增强了MEVA数据集，以实现全面的轨迹生成。", "conclusion": "通过无缝整合空间和时间动态，本文工作为多帧视频分析树立了新标准，为监视、自主导航等高影响力应用开辟了新途径。", "translation": "理解视频内容对于推进活动识别、自主系统和人机交互等现实世界应用至关重要。虽然场景图擅长捕捉单个帧中对象的空间关系，但将这些表示扩展到捕捉视频序列中的动态交互仍然是一个重大挑战。为了解决这个问题，我们提出了TCDSG，即时间一致的动态场景图，这是一个创新的端到端框架，它在时间上检测、跟踪和链接主体-客体关系，生成动作轨迹，即实体及其交互的时间一致序列。我们的方法利用了一种新颖的二分匹配机制，并通过自适应解码器查询和反馈循环进行增强，确保了长期序列的时间连贯性和鲁棒跟踪。该方法不仅通过在Action Genome、OpenPVSG和MEVA数据集上实现超过60%的时间召回率@k改进，建立了新基准，而且还率先通过持久对象ID注释增强了MEVA，以实现全面的轨迹生成。通过无缝整合空间和时间动态，我们的工作为多帧视频分析树立了新标准，为监视、自主导航等高影响力应用开辟了新途径。", "summary": "本文提出了一种名为TCDSG（时间一致的动态场景图）的端到端框架，旨在解决视频中动态交互建模的挑战。TCDSG能够检测、跟踪并关联视频序列中的主体-客体关系，生成时间一致的动作轨迹。该方法采用新颖的二分匹配机制，结合自适应解码器查询和反馈循环，显著提升了在Action Genome、OpenPVSG和MEVA数据集上的时间召回率，并为MEVA数据集新增了持久对象ID注释。这项工作为多帧视频分析设定了新标准，并在监控和自主导航等领域具有广泛应用前景。", "keywords": "动态场景图, 动作轨迹, 视频理解, 时间一致性, 二分匹配", "comments": "该论文的创新点在于提出了TCDSG，一个端到端的框架，能够有效地将场景图的空间关系扩展到时间维度，以捕捉视频中的动态交互。其引入的二分匹配机制和对MEVA数据集的增强，是实现时间一致性跟踪的关键。这项工作对视频理解、特别是动作轨迹生成领域具有重要意义，为实际应用如监控和自主导航提供了新的解决方案。"}}
{"id": "2507.16820", "title": "Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature", "authors": ["Ngan Tran", "Haihua Chen", "Ana Cleveland", "Yuhan Zhou"], "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.DL"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      36 pages, 14 figures, 5 tables", "url": "http://arxiv.org/abs/2507.16820v1", "summary": "This study presents a comprehensive bibliometric and topic analysis of the\ndisaster informatics literature published between January 2020 to September\n2022. Leveraging a large-scale corpus and advanced techniques such as\npre-trained language models and generative AI, we identify the most active\ncountries, institutions, authors, collaboration networks, emergent topics,\npatterns among the most significant topics, and shifts in research priorities\nspurred by the COVID-19 pandemic. Our findings highlight (1) countries that\nwere most impacted by the COVID-19 pandemic were also among the most active,\nwith each country having specific research interests, (2) countries and\ninstitutions within the same region or share a common language tend to\ncollaborate, (3) top active authors tend to form close partnerships with one or\ntwo key partners, (4) authors typically specialized in one or two specific\ntopics, while institutions had more diverse interests across several topics,\nand (5) the COVID-19 pandemic has influenced research priorities in disaster\ninformatics, placing greater emphasis on public health. We further demonstrate\nthat the field is converging on multidimensional resilience strategies and\ncross-sectoral data-sharing collaborations or projects, reflecting a heightened\nawareness of global vulnerability and interdependency. Collecting and quality\nassurance strategies, data analytic practices, LLM-based topic extraction and\nsummarization approaches, and result visualization tools can be applied to\ncomparable datasets or solve similar analytic problems. By mapping out the\ntrends in disaster informatics, our analysis offers strategic insights for\npolicymakers, practitioners, and scholars aiming to enhance disaster\ninformatics capacities in an increasingly uncertain and complex risk landscape.", "comment": "36 pages, 14 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.16820v1", "cate": "cs.SI", "date": "2025-06-28", "updated": "2025-06-28", "AI": {"title_translation": "COVID-19大流行后的灾害信息学：基于大规模学术文献的文献计量与主题分析", "tldr": "本研究对COVID-19大流行后灾害信息学领域的文献进行了全面的文献计量和主题分析，揭示了研究趋势和重点变化。", "motivation": "本研究旨在对COVID-19大流行后发表的灾害信息学文献进行分析，识别活跃的国家、机构、作者、合作网络、新兴主题以及由大流行引起的科研重点转变。", "method": "本研究采用了全面的文献计量和主题分析方法，利用大规模语料库和预训练语言模型、生成式AI等先进技术，并应用了基于LLM的主题提取、摘要方法和结果可视化工具。", "result": "研究结果表明：1) 受COVID-19大流行影响最严重的国家也是最活跃的，且各有特定研究兴趣；2) 同一地区或共享语言的国家和机构倾向于合作；3) 顶尖活跃作者与一两个主要合作伙伴形成紧密关系；4) 作者通常专注于一两个特定主题，而机构兴趣更广泛；5) COVID-19大流行影响了灾害信息学的研究重点，更强调公共卫生；6) 该领域正趋向多维度韧性策略和跨部门数据共享合作。", "conclusion": "本研究通过描绘灾害信息学的发展趋势，为政策制定者、从业者和学者提供了战略性见解，以增强在日益不确定和复杂风险环境中的灾害信息学能力。", "translation": "本研究对2020年1月至2022年9月期间发表的灾害信息学文献进行了全面的文献计量和主题分析。我们利用大规模语料库和预训练语言模型、生成式AI等先进技术，识别了最活跃的国家、机构、作者、合作网络、新兴主题、最重要主题之间的模式，以及COVID-19大流行引起的科研重点转变。我们的研究结果强调：（1）受COVID-19大流行影响最严重的国家也是最活跃的国家之一，每个国家都有特定的研究兴趣；（2）同一地区或共享共同语言的国家和机构倾向于合作；（3）顶尖活跃作者倾向于与一两个主要合作伙伴建立紧密关系；（4）作者通常专注于一两个特定主题，而机构则对多个主题有更广泛的兴趣；（5）COVID-19大流行影响了灾害信息学的研究重点，更加强调公共卫生。我们进一步表明，该领域正在趋向多维度韧性策略和跨部门数据共享合作或项目，这反映了对全球脆弱性和相互依赖性的高度认识。数据收集和质量保证策略、数据分析实践、基于LLM的主题提取和摘要方法以及结果可视化工具可以应用于类似数据集或解决类似的分析问题。通过描绘灾害信息学的发展趋势，我们的分析为旨在增强日益不确定和复杂风险环境中灾害信息学能力的政策制定者、从业者和学者提供了战略性见解。", "summary": "本研究利用大规模学术文献和先进的AI技术（如预训练语言模型和生成式AI），对2020年至2022年间灾害信息学领域的文献进行了全面的文献计量和主题分析。研究识别了活跃的国家、机构、作者及合作网络，并揭示了COVID-19大流行如何影响了研究重点，使其更侧重于公共卫生、多维度韧性策略和跨部门数据共享。本分析为提升灾害信息学能力提供了战略性见解。", "keywords": "灾害信息学, COVID-19, 文献计量分析, 主题建模, 研究趋势", "comments": "该论文及时且切合实际地分析了COVID-19疫情后的灾害信息学领域，利用预训练语言模型和生成式AI等先进人工智能方法进行大规模的文献计量和主题分析。其创新之处在于应用这些现代技术来绘制研究趋势和转变，为这一关键领域的利益相关者提供了宝贵的战略见解。对大流行影响以及韧性和数据共享新兴主题的关注尤为重要。"}}
{"id": "2507.14687", "title": "An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions", "authors": ["Robin Lee", "Youngho Nam"], "categories": ["cs.SE", "68Q60, 03B70", "D.2.5"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures", "url": "http://arxiv.org/abs/2507.14687v2", "summary": "Modified Condition/Decision Coverage (MC/DC) is a mandatory structural\ncoverage criterion for ensuring the reliability and safety of critical systems.\nWhile its strictest form, Unique-Cause MC/DC, offers the highest assurance,\nresearch on its efficient test generation has been lacking. This gap is\nparticularly significant, as an analysis of large-scale avionics systems shows\nthat 99.7% of all conditional decisions are, in fact, Singular Boolean\nExpressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This\npaper proposes 'Robin's Rule', a deterministic algorithm that directly\nconstructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause\nMC/DC for SBEs with N conditions, without generating a full truth table. To\nvalidate our approach, we constructed a benchmark by reformulating the TCAS-II\nspecifications into SBEs and verified the results using an industry-standard,\ncertified commercial tool. The results confirm that our method consistently\nachieves 100% coverage with the theoretical minimum number of tests and is more\nefficient than the commercial tool. This work provides a practical and provably\noptimal solution for verifying safety-critical systems, ensuring both rigor and\nefficiency.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.14687v2", "cate": "cs.SE", "date": "2025-07-19", "updated": "2025-07-22", "AI": {"title_translation": "一种为奇异布尔表达式生成最小独特原因MC/DC测试用例的高效算法", "tldr": "提出“Robin's Rule”算法，高效生成奇异布尔表达式的最小独特原因MC/DC测试用例，比商业工具更优。", "motivation": "Unique-Cause MC/DC是关键系统强制性覆盖标准，但其高效测试生成方法缺乏研究，尤其对于航空电子系统中99.7%的奇异布尔表达式。", "method": "提出“Robin's Rule”确定性算法，直接为N个条件的SBE构造N+1个最小测试集，无需生成完整真值表。通过将TCAS-II规范重构为SBEs并使用商业工具验证。", "result": "该方法始终以理论最小测试数量实现100%覆盖，且比商业工具更高效。", "conclusion": "该工作为验证安全关键系统提供了一个实用且可证明最优的解决方案，确保了严谨性和效率。", "translation": "修正条件/判定覆盖 (MC/DC) 是确保关键系统可靠性和安全性的强制性结构覆盖准则。虽然其最严格形式——独特原因MC/DC——提供了最高保证，但关于其高效测试生成的研究一直缺乏。这一空白尤为重要，因为对大规模航空电子系统的分析表明，99.7%的所有条件判定实际上是奇异布尔表达式 (SBEs)，这是应用独特原因MC/DC的理想结构。本文提出了“Robin's Rule”，这是一种确定性算法，它直接构建一个包含N+1个用例的最小测试集，以保证对N个条件的SBE实现100%独特原因MC/DC，而无需生成完整的真值表。为了验证我们的方法，我们通过将TCAS-II规范重构为SBEs来构建了一个基准，并使用行业标准、经过认证的商业工具验证了结果。结果证实，我们的方法始终以理论最小测试数量实现100%覆盖，并且比商业工具更高效。这项工作为验证安全关键系统提供了一个实用且可证明最优的解决方案，确保了严谨性和效率。", "summary": "本文提出“Robin's Rule”算法，旨在高效生成奇异布尔表达式（SBEs）的独特原因MC/DC测试用例。该算法为N个条件的SBE直接构建N+1个最小测试集，无需完整真值表。通过与TCAS-II规范的基准测试和商业工具的对比，验证了该方法在实现100%覆盖的同时，具有更高的效率和理论最优性，为安全关键系统验证提供了实用方案。", "keywords": "MC/DC, 独特原因MC/DC, 奇异布尔表达式, 测试生成, Robin's Rule", "comments": "本文的创新点在于提出了“Robin's Rule”算法，专门针对航空电子系统中常见的奇异布尔表达式，解决了独特原因MC/DC测试用例生成效率低的痛点。其重要性在于提供了一个理论上最小且实践中高效的解决方案，提高了安全关键系统验证的严谨性和效率。"}}
{"id": "2507.17417", "title": "A Comprehensive Evaluation on Quantization Techniques for Large Language Models", "authors": ["Yutong Liu", "Cairong Zhao", "Guosheng Hu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17417v1", "summary": "For large language models (LLMs), post-training quantization (PTQ) can\nsignificantly reduce memory footprint and computational overhead. Model\nquantization is a rapidly evolving research field. Though many papers have\nreported breakthrough performance, they may not conduct experiments on the same\nground since one quantization method usually contains multiple components. In\naddition, analyzing the theoretical connections among existing methods is\ncrucial for in-depth understanding. To bridge these gaps, we conduct an\nextensive review of state-of-the-art methods and perform comprehensive\nevaluations on the same ground to ensure fair comparisons. To our knowledge,\nthis fair and extensive investigation remains critically important yet\nunderexplored. To better understand the theoretical connections, we decouple\nthe published quantization methods into two steps: pre-quantization\ntransformation and quantization error mitigation. We define the former as a\npreprocessing step applied before quantization to reduce the impact of\noutliers, making the data distribution flatter and more suitable for\nquantization. Quantization error mitigation involves techniques that offset the\nerrors introduced during quantization, thereby enhancing model performance. We\nevaluate and analyze the impact of different components of quantization\nmethods. Additionally, we analyze and evaluate the latest MXFP4 data format and\nits performance. Our experimental results demonstrate that optimized rotation\nand scaling yield the best performance for pre-quantization transformation, and\ncombining low-rank compensation with GPTQ occasionally outperforms using GPTQ\nalone for quantization error mitigation. Furthermore, we explore the potential\nof the latest MXFP4 quantization and reveal that the optimal pre-quantization\ntransformation strategy for INT4 does not generalize well to MXFP4, inspiring\nfurther investigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17417v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大型语言模型量化技术的综合评估", "tldr": "本论文对大型语言模型（LLMs）的最新训练后量化（PTQ）技术进行了全面评估，通过将量化方法解耦为预量化转换和量化误差缓解两个步骤，确保了公平比较，并分析了不同组件的影响，同时探索了MXFP4格式。", "motivation": "大型语言模型（LLMs）的训练后量化（PTQ）能显著减少内存占用和计算开销。然而，量化研究领域进展迅速，现有研究在实验设置上缺乏统一标准，导致难以进行公平比较。此外，深入理解现有方法之间的理论联系也至关重要。因此，本研究旨在弥合这些差距，进行全面且公平的评估，并分析理论联系。", "method": "研究首先对最先进的量化方法进行了广泛回顾，并在统一的实验环境下进行了全面评估，以确保公平比较。为了更好地理解理论联系，研究将已发表的量化方法解耦为两个步骤：预量化转换（量化前应用于减少异常值影响的预处理步骤）和量化误差缓解（抵消量化过程中引入误差的技术）。研究评估并分析了量化方法不同组件的影响，并分析和评估了最新的MXFP4数据格式及其性能。", "result": "实验结果表明，优化的旋转和缩放对于预量化转换能产生最佳性能；在量化误差缓解方面，将低秩补偿与GPTQ结合使用，在某些情况下会优于单独使用GPTQ。此外，研究发现针对INT4的最佳预量化转换策略不适用于MXFP4。", "conclusion": "本研究的全面评估揭示了量化方法中不同组件的关键作用，并指出INT4的最佳预量化转换策略不适用于MXFP4，这启发了对MXFP4的进一步深入研究。研究结果为大型语言模型的量化技术提供了更清晰的理解和指导。", "translation": "对于大型语言模型（LLMs）而言，训练后量化（PTQ）可以显著减少内存占用和计算开销。模型量化是一个快速发展的研究领域。尽管许多论文报告了突破性性能，但由于一种量化方法通常包含多个组件，它们可能未在相同的实验基础上进行实验。此外，分析现有方法之间的理论联系对于深入理解至关重要。为了弥合这些差距，我们对最先进的方法进行了广泛回顾，并在相同的实验基础上进行了全面评估，以确保公平比较。据我们所知，这种公平而广泛的调查仍然至关重要但未被充分探索。为了更好地理解理论联系，我们将已发表的量化方法解耦为两个步骤：预量化转换和量化误差缓解。我们将前者定义为量化前应用的预处理步骤，以减少异常值的影响，使数据分布更平坦，更适合量化。量化误差缓解涉及抵消量化过程中引入误差的技术，从而提高模型性能。我们评估并分析了量化方法不同组件的影响。此外，我们分析和评估了最新的MXFP4数据格式及其性能。我们的实验结果表明，优化的旋转和缩放对于预量化转换能产生最佳性能，并且将低秩补偿与GPTQ结合使用，在某些情况下会优于单独使用GPTQ，用于量化误差缓解。此外，我们探索了最新MXFP4量化的潜力，并揭示了INT4的最佳预量化转换策略不适用于MXFP4，这启发了进一步的调查。", "summary": "本研究旨在弥合大型语言模型（LLMs）训练后量化（PTQ）领域中缺乏公平比较和理论联系分析的现状。通过对现有最先进方法进行全面回顾和在统一基准上的公平评估，研究将量化方法解耦为预量化转换和量化误差缓解两个核心步骤。实验结果表明，优化的旋转和缩放是最佳的预量化转换策略，而低秩补偿与GPTQ结合有时优于单独使用GPTQ进行误差缓解。研究还发现，INT4的最佳预量化转换策略不适用于MXFP4，这为未来的研究方向提供了重要启示。", "keywords": "量化, 大型语言模型, 训练后量化, 评估, MXFP4", "comments": "本论文的创新之处在于其对大型语言模型量化技术进行了首次全面且公平的基准评估，通过解耦量化过程为预量化转换和误差缓解，深入分析了不同组件的影响。这对于理解和改进LLM量化方法具有重要意义，为该领域的未来研究提供了坚实的实验和理论基础。其揭示的MXFP4特性也为新数据格式的优化指明了方向。"}}
{"id": "2507.17511", "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression", "authors": ["Jiajun Luo", "Yicheng Xiao", "Jianru Xu", "Yangxiu You", "Rongwei Lu", "Chen Tang", "Jingyan Jiang", "Zhi Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17511v1", "summary": "Diffusion models produce realistic images and videos but require substantial\ncomputational resources, necessitating multi-accelerator parallelism for\nreal-time deployment. However, parallel inference introduces significant\ncommunication overhead from exchanging large activations between devices,\nlimiting efficiency and scalability. We present CompactFusion, a compression\nframework that significantly reduces communication while preserving generation\nquality. Our key observation is that diffusion activations exhibit strong\ntemporal redundancy-adjacent steps produce highly similar activations,\nsaturating bandwidth with near-duplicate data carrying little new information.\nTo address this inefficiency, we seek a more compact representation that\nencodes only the essential information. CompactFusion achieves this via\nResidual Compression that transmits only compressed residuals (step-wise\nactivation differences). Based on empirical analysis and theoretical\njustification, we show that it effectively removes redundant data, enabling\nsubstantial data reduction while maintaining high fidelity. We also integrate\nlightweight error feedback to prevent error accumulation. CompactFusion\nestablishes a new paradigm for parallel diffusion inference, delivering lower\nlatency and significantly higher generation quality than prior methods. On\n4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also\nuniquely supports communication-heavy strategies like sequence parallelism on\nslow networks, achieving 6.7x speedup over prior overlap-based method.\nCompactFusion applies broadly across diffusion models and parallel settings,\nand integrates easily without requiring pipeline rework. Portable\nimplementation demonstrated on xDiT is publicly available at\nhttps://github.com/Cobalt-27/CompactFusion", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17511v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用残差压缩加速并行扩散模型服务", "tldr": "CompactFusion通过残差压缩显著减少并行扩散模型推理中的通信开销，从而实现更低的延迟和更高的生成质量。", "motivation": "扩散模型在实时部署时需要多加速器并行，但并行推理中设备间交换大量激活会产生显著的通信开销，限制了效率和可扩展性。", "method": "本文提出了CompactFusion框架，通过残差压缩（只传输压缩后的步进激活差异）来减少通信。其核心是利用扩散激活的强时间冗余性，并结合轻量级误差反馈以防止误差累积。", "result": "CompactFusion在4xL20上实现了3.0倍的加速，并显著提高了保真度。它还能在慢速网络上支持通信密集型策略（如序列并行），比现有基于重叠的方法快6.7倍。它适用于各种扩散模型和并行设置。", "conclusion": "CompactFusion为并行扩散推理建立了一个新范式，通过有效去除冗余数据，显著降低了延迟并提高了生成质量，并且易于集成。", "translation": "扩散模型能生成逼真的图像和视频，但需要大量的计算资源，因此实时部署需要多加速器并行。然而，并行推理引入了设备间交换大量激活的显著通信开销，限制了效率和可扩展性。我们提出了CompactFusion，一个显著减少通信同时保持生成质量的压缩框架。我们的主要观察是，扩散激活表现出很强的时间冗余性——相邻步骤会产生高度相似的激活，用几乎重复的、携带很少新信息的数据饱和带宽。为了解决这种低效率问题，我们寻求一种更紧凑的表示，只编码必要的信息。CompactFusion通过残差压缩实现了这一点，只传输压缩后的残差（步进激活差异）。基于经验分析和理论证明，我们表明它能有效去除冗余数据，从而在保持高保真度的同时实现大量数据缩减。我们还集成了轻量级误差反馈以防止误差累积。CompactFusion为并行扩散推理建立了一个新范式，比现有方法提供更低的延迟和显著更高的生成质量。在4xL20上，它实现了3.0倍的加速，同时大大提高了保真度。它还独特地支持慢速网络上的通信密集型策略，如序列并行，比现有基于重叠的方法快6.7倍。CompactFusion广泛适用于各种扩散模型和并行设置，并且无需重新设计管道即可轻松集成。在xDiT上演示的可移植实现已公开发布在https://github.com/Cobalt-27/CompactFusion", "summary": "CompactFusion是一个新的压缩框架，旨在解决并行扩散模型推理中由设备间大量激活交换引起的通信开销问题。该框架利用扩散激活的时间冗余性，通过残差压缩（仅传输压缩后的步进激活差异）来显著减少通信量，同时保持生成质量。它还集成了轻量级误差反馈以防止误差积累。CompactFusion在实验中展示了显著的加速和更高的保真度，例如在4xL20上实现3.0倍加速，并在慢速网络上通过支持序列并行实现6.7倍加速。该方法易于集成，适用于各种扩散模型和并行设置。", "keywords": "扩散模型, 并行推理, 残差压缩, 通信优化, CompactFusion", "comments": "CompactFusion的创新点在于发现了扩散激活的强时间冗余性，并利用残差压缩这一巧妙方法来去除冗余数据，从而解决了并行推理中的通信瓶颈。其在实际部署中的显著性能提升和广泛适用性使其具有重要价值。"}}
{"id": "2503.22913", "title": "Resona: Improving Context Copying in Linear Recurrence Models with Retrieval", "authors": ["Xinyu Wang", "Linrui Ma", "Jerry Huang", "Peng Lu", "Prasanna Parthasarathi", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the Second Conference on Language Modeling", "url": "http://arxiv.org/abs/2503.22913v3", "summary": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.", "comment": "Accepted at the Second Conference on Language Modeling", "pdf_url": "http://arxiv.org/pdf/2503.22913v3", "cate": "cs.CL", "date": "2025-03-28", "updated": "2025-07-23", "AI": {"title_translation": "Resona：通过检索改进线性循环模型中的上下文复制", "tldr": "线性循环模型在上下文学习方面与Transformer有差距。本文引入Resona框架，通过检索增强线性循环模型，显著提升了其在多种任务上的表现，特别是上下文学习和语言建模能力。", "motivation": "线性循环模型虽然计算高效，但在上下文学习及需要记忆上下文信息的任务上与Transformer模型存在显著差距。", "method": "引入Resona框架，一个简单且可扩展的检索增强方法，使线性循环模型能够整合从输入上下文中检索到的信息，从而适应不同的任务需求。", "result": "实验表明，经过Resona增强的线性循环模型在多种合成和真实世界的自然语言任务上均观察到显著的性能提升。", "conclusion": "Resona是一种通用的方法，能够有效提升线性循环大型语言模型的上下文学习和语言建模能力。", "translation": "大型语言模型（LLM）研究领域的最新转变表明，人们越来越关注新颖的架构，以与长期主导该领域的原型Transformer模型竞争。线性循环模型因其计算效率已被证明是一个可行的竞争者。然而，这类模型在上下文学习以及其他需要从上下文中回忆信息的任务方面，与Transformer模型相比仍然存在相当大的差距。在这项工作中，我们引入了Resona，一个简单且可扩展的框架，用于通过检索来增强线性循环模型。Resona使模型能够整合从提供的输入上下文中检索到的信息，从而能够根据不同的任务要求定制行为。对各种线性循环模型的实验表明，Resona增强的模型在各种合成以及真实世界的自然语言任务上都观察到显著的性能提升，突出了其作为一种通用方法来改进线性循环LLM的上下文学习和语言建模能力。", "summary": "本文提出了Resona，一个通过检索增强线性循环大型语言模型（LLLM）的框架。尽管线性循环模型计算高效，但在上下文学习方面逊于Transformer模型。Resona通过整合检索到的上下文信息，显著提升了线性循环模型在合成和真实世界自然语言任务上的表现，证明其能有效提高模型的上下文学习和语言建模能力。", "keywords": "线性循环模型, 上下文学习, 检索增强, 大语言模型, Resona", "comments": "Resona的创新之处在于其通过检索机制弥补了线性循环模型在上下文学习方面的不足，同时保持了其计算效率。这对于开发更高效、更强大的LLM架构具有重要意义，尤其是在资源受限或需要长上下文处理的场景下。"}}
{"id": "2507.17736", "title": "Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems", "authors": ["Shreya Meel", "Sennur Ulukus"], "categories": ["cs.IT", "cs.CR", "cs.DB", "cs.NI", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17736v1", "summary": "We introduce the problem of symmetric private information retrieval (SPIR) on\nreplicated databases modeled by a simple graph. In this model, each vertex\ncorresponds to a server, and a message is replicated on two servers if and only\nif there is an edge between them. We consider the setting where the server-side\ncommon randomness necessary to accomplish SPIR is also replicated at the\nservers according to the graph, and we call this as message-specific common\nrandomness. In this setting, we establish a lower bound on the SPIR capacity,\ni.e., the maximum download rate, for general graphs, by proposing an achievable\nSPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the\nminimum size of message-specific randomness should be equal to the size of a\nmessage. Finally, by providing matching upper bounds, we derive the exact SPIR\ncapacity for the class of path and regular graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17736v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "对称私有信息检索 (SPIR) 在基于图的复制系统上", "tldr": "本文在图模型下引入了对称私有信息检索（SPIR）问题，建立了SPIR容量的下界，证明了实现SPIR所需随机性的最小大小，并推导了路径图和正则图的精确SPIR容量。", "motivation": "在由简单图建模的复制数据库上引入对称私有信息检索（SPIR）问题，并考虑服务器端公共随机性也根据图进行复制的设置。", "method": "提出了一种可实现的SPIR方案以建立SPIR容量的下界；证明了实现SPIR所需的最小消息特定随机性大小；通过提供匹配的上界，推导出了路径图和正则图的精确SPIR容量。", "result": "为通用图建立了SPIR容量的下界；证明了任何SPIR方案要可行，消息特定随机性的最小大小应等于消息的大小；推导出了路径图和正则图的精确SPIR容量。", "conclusion": "本文为图模型下的SPIR问题建立了理论基础，包括容量下界、随机性要求以及特定图类的精确容量。", "translation": "我们介绍了在由简单图建模的复制数据库上进行对称私有信息检索（SPIR）的问题。在这个模型中，每个顶点对应一个服务器，当且仅当它们之间存在一条边时，消息才在两个服务器上复制。我们考虑了这样的设置：实现SPIR所需的服务器端公共随机性也根据图在服务器上复制，我们称之为消息特定公共随机性。在此设置下，我们通过提出一种可实现的SPIR方案，为通用图建立了SPIR容量（即最大下载速率）的下界。接下来，我们证明，对于任何SPIR方案要可行，消息特定随机性的最小大小应等于消息的大小。最后，通过提供匹配的上界，我们推导出了路径图和正则图的精确SPIR容量。", "summary": "本文引入了在图模型下复制数据库上的对称私有信息检索（SPIR）问题，其中服务器端的公共随机性也按照图结构复制。研究建立了通用图的SPIR容量下界，证明了实现SPIR所需的最小消息特定随机性大小等于消息大小，并为路径图和正则图推导出了精确的SPIR容量。", "keywords": "对称私有信息检索, 图, 复制系统, 容量, 公共随机性", "comments": "本文创新性地将SPIR问题与图论中的复制系统结合，并考虑了公共随机性的图结构复制，为理解和设计图结构下的隐私信息检索系统提供了理论基础和容量界限。"}}
{"id": "2507.14982", "title": "How Many Simultaneous Beamformers are Needed for Integrated Sensing and Communications?", "authors": ["Kareem M. Attiah", "Wei Yu"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      26 pages, 7 figures", "url": "http://arxiv.org/abs/2507.14982v2", "summary": "Consider a downlink integrated sensing and communications (ISAC) system in\nwhich a base station employs linear beamforming to communicate to $K$ users,\nwhile simultaneously uses sensing beams to perform a sensing task of estimating\n$L$ real parameters. How many beamformers are needed to achieve the best\nperformance for both sensing and communications? This paper establishes bounds\non the minimum number of downlink beamformers, in which sensing performance is\nmeasured in terms of the Cram\\'{e}r-Rao bound for parameter estimation and\ncommunications performance is measured in terms of the\nsignal-to-interference-and-noise ratios. We show that an ISAC system requires\nat most $K + \\sqrt{\\frac{L(L+1)}{2}}$ beamformers if the remote users have the\nability to cancel the interference caused by the sensing beams. If cancelling\ninterference due to the sensing beams is not possible, the bound becomes\n$\\sqrt{K^2 + \\frac{L(L+1)}{2}}$. Interestingly, in the latter case, the bound\non the number of beamformers is less than the sum of the bounds for each task\nindividually. These results can be extended to sensing tasks for which the\nperformance is measured as a function of $d$ quadratic terms in the\nbeamformers. In this case, the bound becomes $K + \\sqrt{d}$ and $\\sqrt{K^2 +\nd}$, respectively. Specifically, for estimating complex path losses and\nangles-of-arrival of $N_\\text{tr}$ targets while communicating to $K$ users,\nthe bound on the minimum number of beamformers scales linearly in $K$ and in\n$N_\\text{tr}$, assuming interference from sensing can be cancelled. When\ninterference cancellation is not possible, the following exact characterization\nfor the case of $N_\\text{tr} = 1$ can be obtained: when $K=0$ or $1$, two\nbeamformers should be used; when $K \\ge 2$, exactly $K$ beamformers should be\nused, i.e., communication beamformers alone are already sufficient.", "comment": "26 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.14982v2", "cate": "eess.SP", "date": "2025-07-20", "updated": "2025-07-22", "AI": {"title_translation": "集成传感与通信需要多少个同步波束赋形器？", "tldr": "本文建立了集成传感与通信（ISAC）系统所需最少下行波束赋形器数量的界限，考虑了用户是否能消除感知干扰两种情况，并给出了具体的数学表达式和特殊情况下的分析。", "motivation": "在下行集成传感与通信（ISAC）系统中，基站使用线性波束赋形与K个用户通信，同时使用感知波束执行L个实参数的估计任务。本文旨在回答，需要多少个波束赋形器才能实现传感和通信的最佳性能？", "method": "本文通过建立下行波束赋形器最小数量的界限来解决问题。传感性能通过参数估计的Cramér-Rao下界衡量，通信性能通过信干噪比（SINR）衡量。", "result": "1. 如果远程用户能够消除感知波束引起的干扰，ISAC系统最多需要 $K + \\sqrt{\\frac{L(L+1)}{2}}$ 个波束赋形器。2. 如果无法消除感知波束引起的干扰，则界限变为 $\\sqrt{K^2 + \\frac{L(L+1)}{2}}$。在这种情况下，波束赋形器数量的界限小于每个任务单独界限的总和。3. 对于性能衡量为波束赋形器中d个二次项的感知任务，界限分别变为 $K + \\sqrt{d}$ 和 $\\sqrt{K^2 + d}$。4. 具体来说，对于估计 $N_\\text{tr}$ 个目标的复路径损耗和到达角，同时与K个用户通信，如果感知干扰可以消除，所需波束赋形器的数量与K和 $N_\\text{tr}$ 呈线性关系。5. 当无法消除干扰时，对于 $N_\\text{tr} = 1$ 的情况：当 $K=0$ 或 $1$ 时，应使用两个波束赋形器；当 $K \\ge 2$ 时，只需使用K个波束赋形器，即仅通信波束赋形器就已足够。", "conclusion": "本文得出的一个有趣结论是，当无法消除感知干扰时，所需波束赋形器数量的界限小于单独执行每个任务所需界限的总和。此外，在特定条件下（如 $N_\\text{tr}=1$ 且 $K \\ge 2$ 时），仅通信波束赋形器就足以满足需求。", "translation": "考虑一个下行集成传感与通信（ISAC）系统，其中基站采用线性波束赋形与K个用户通信，同时利用感知波束执行估计L个实参数的感知任务。需要多少个波束赋形器才能实现传感和通信的最佳性能？本文建立了下行波束赋形器最小数量的界限，其中传感性能通过参数估计的Cramér-Rao下界衡量，通信性能通过信干噪比衡量。我们表明，如果远程用户能够消除感知波束引起的干扰，ISAC系统最多需要 $K + \\sqrt{\\frac{L(L+1)}{2}}$ 个波束赋形器。如果无法消除感知波束引起的干扰，则界限变为 $\\sqrt{K^2 + \\frac{L(L+1)}{2}}$。有趣的是，在后一种情况下，波束赋形器数量的界限小于每个任务单独界限的总和。这些结果可以扩展到感知性能衡量为波束赋形器中d个二次项函数的情况。在这种情况下，界限分别变为 $K + \\sqrt{d}$ 和 $\\sqrt{K^2 + d}$。具体来说，对于估计 $N_\\text{tr}$ 个目标的复路径损耗和到达角，同时与K个用户通信，假设感知干扰可以消除，所需波束赋形器的数量与K和 $N_\\text{tr}$ 呈线性关系。当无法消除感知干扰时，可以得到 $N_\\text{tr} = 1$ 情况的精确表征：当 $K=0$ 或 $1$ 时，应使用两个波束赋形器；当 $K \\ge 2$ 时，正好使用K个波束赋形器，即仅通信波束赋形器就已足够。", "summary": "本文研究了集成传感与通信（ISAC）系统所需的最小下行波束赋形器数量，以实现传感和通信的最佳性能。通过建立基于Cramér-Rao下界和信干噪比的界限，作者分析了用户能够或不能消除感知干扰的两种情况。研究发现，在用户能够消除感知干扰时，所需波束赋形器数量为 $K + \\sqrt{\\frac{L(L+1)}{2}}$；而在无法消除干扰时，数量为 $\\sqrt{K^2 + \\frac{L(L+1)}{2}}$，且后者小于单独任务的界限之和。文章还推广了结果到更一般的感知任务，并对特定场景（如 $N_\\text{tr}=1$）给出了精确的波束赋形器数量。", "keywords": "集成传感与通信, 波束赋形, Cramér-Rao下界, 干扰消除, 资源分配", "comments": "本文创新性地量化了集成传感与通信系统中波束赋形器的需求数量，并考虑了干扰消除能力对系统性能的影响。其发现，在某些情况下，集成系统的波束赋形器数量需求低于单独执行传感和通信任务的总和，这突显了ISAC在资源效率方面的潜力。特别是对 $N_\\text{tr}=1$ 且 $K \\ge 2$ 时通信波束赋形器足以满足需求的结论，具有重要的指导意义。"}}
{"id": "2507.17332", "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image", "authors": ["Hyeongjin Nam", "Donghwan Kim", "Gyeongsik Moon", "Kyoung Mu Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published at ICCV 2025, 22 pages including the supplementary material", "url": "http://arxiv.org/abs/2507.17332v1", "summary": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.", "comment": "Published at ICCV 2025, 22 pages including the supplementary material", "pdf_url": "http://arxiv.org/pdf/2507.17332v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PARTE：单图三维人体重建中的部件引导纹理化", "tldr": "PARTE提出了一种利用3D人体部件信息来解决现有3D人体重建方法中纹理错位问题的框架。", "motivation": "现有3D人体重建方法的主要限制之一是不同人体部件之间的纹理错位，导致每个部件（如夹克或裤子）无法保持独立的纹理。大多数现有方法没有明确利用部件分割先验，从而导致重建中纹理错位。", "method": "我们提出了PARTE框架，它利用3D人体部件信息作为关键指导来重建3D人体纹理。该框架包含两个核心组件：1. 三维部件分割模块（PartSegmenter）：从单张图像中推断3D人体部件信息，首先重建无纹理人体表面，并基于此预测人体部件标签。2. 部件引导纹理化模块（PartTexturer）：将部件信息整合到纹理重建中，该模块从预训练图像生成网络中获取关于人体部件纹理对齐的先验知识。", "result": "广泛的实验表明，我们的框架在3D人体重建方面达到了最先进的质量。", "conclusion": "通过明确利用3D人体部件信息和引入部件引导的纹理化机制，PARTE成功解决了现有3D人体重建方法中纹理错位的问题，并达到了SOTA性能。", "translation": "现有三维人体重建方法的主要限制之一是不同人体部件之间的纹理错位。每个身体部位，例如夹克或裤子，都应该保持独特的纹理，而不会相互混淆。人体部件的结构连贯性是推断单张图像中不可见区域人体纹理的关键线索。然而，大多数现有三维人体重建方法并未明确利用这种部件分割先验，导致其重建中的纹理错位。为此，我们提出了PARTE，它利用三维人体部件信息作为重建三维人体纹理的关键指导。我们的框架包含两个核心组件。首先，为了从单张图像中推断三维人体部件信息，我们提出了一个三维部件分割模块（PartSegmenter），它首先重建一个无纹理的人体表面，并基于该无纹理表面预测人体部件标签。其次，为了将部件信息整合到纹理重建中，我们引入了一个部件引导纹理化模块（PartTexturer），该模块从预训练的图像生成网络中获取关于人体部件纹理对齐的先验知识。广泛的实验表明，我们的框架在三维人体重建方面达到了最先进的质量。项目页面可在https://hygenie1228.github.io/PARTE/访问。", "summary": "PARTE是一种用于单图三维人体重建的新框架，旨在解决现有方法中常见的纹理错位问题。它通过引入一个三维部件分割模块来推断人体部件信息，并结合一个部件引导纹理化模块，利用部件先验知识进行纹理重建，从而显著提升了重建质量。", "keywords": "三维人体重建, 纹理化, 部件引导, 单图重建, 纹理对齐", "comments": "PARTE的创新点在于明确地将3D人体部件信息作为纹理重建的关键指导，解决了现有方法中纹理错位这一核心限制。通过引入专门的部件分割和部件引导纹理化模块，它能够更好地保持不同身体部件纹理的独立性和结构一致性，从而实现了更真实的三维人体重建效果。"}}
{"id": "2507.16450", "title": "RIS-aided Latent Space Alignment for Semantic Channel Equalization", "authors": ["Tomás Hüttebräucker", "Mario Edoardo Pandolfo", "Simone Fiorellino", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "categories": ["cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16450v2", "summary": "Semantic communication systems introduce a new paradigm in wireless\ncommunications, focusing on transmitting the intended meaning rather than\nensuring strict bit-level accuracy. These systems often rely on Deep Neural\nNetworks (DNNs) to learn and encode meaning directly from data, enabling more\nefficient communication. However, in multi-user settings where interacting\nagents are trained independently-without shared context or joint\noptimization-divergent latent representations across AI-native devices can lead\nto semantic mismatches, impeding mutual understanding even in the absence of\ntraditional transmission errors. In this work, we address semantic mismatch in\nMultiple-Input Multiple-Output (MIMO) channels by proposing a joint physical\nand semantic channel equalization framework that leverages the presence of\nReconfigurable Intelligent Surfaces (RIS). The semantic equalization is\nimplemented as a sequence of transformations: (i) a pre-equalization stage at\nthe transmitter; (ii) propagation through the RIS-aided channel; and (iii) a\npost-equalization stage at the receiver. We formulate the problem as a\nconstrained Minimum Mean Squared Error (MMSE) optimization and propose two\nsolutions: (i) a linear semantic equalization chain, and (ii) a non-linear\nDNN-based semantic equalizer. Both methods are designed to operate under\nsemantic compression in the latent space and adhere to transmit power\nconstraints. Through extensive evaluations, we show that the proposed joint\nequalization strategies consistently outperform conventional, disjoint\napproaches to physical and semantic channel equalization across a broad range\nof scenarios and wireless channel conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16450v2", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "RIS辅助的语义信道均衡潜在空间对齐", "tldr": "本文提出了一种RIS辅助的联合物理和语义信道均衡框架，以解决多用户语义通信系统中的语义不匹配问题，并证明其优于传统方法。", "motivation": "在多用户语义通信系统中，独立训练的AI原生设备可能导致潜在表示发散，从而引起语义不匹配，即使没有传统的传输错误，也会阻碍相互理解。", "method": "本文提出了一种联合物理和语义信道均衡框架，利用可重构智能表面（RIS）来解决多输入多输出（MIMO）信道中的语义不匹配问题。语义均衡通过一系列变换实现：(i)发射机的预均衡阶段；(ii)通过RIS辅助信道传播；(iii)接收机的后均衡阶段。问题被表述为受约束的最小均方误差（MMSE）优化问题，并提出了两种解决方案：(i)线性语义均衡链；(ii)基于DNN的非线性语义均衡器。两种方法都在潜在空间中进行语义压缩并遵守发射功率约束。", "result": "所提出的联合均衡策略在广泛的场景和无线信道条件下，始终优于传统的、分离的物理和语义信道均衡方法。", "conclusion": "通过引入RIS辅助的联合物理和语义信道均衡框架，可以有效解决多用户语义通信系统中的语义不匹配问题，显著提升系统性能。", "translation": "语义通信系统在无线通信中引入了一种新的范式，专注于传输预期含义而不是确保严格的比特级精度。这些系统通常依赖深度神经网络（DNNs）直接从数据中学习和编码含义，从而实现更高效的通信。然而，在多用户环境中，当交互代理独立训练——没有共享上下文或联合优化——AI原生设备之间发散的潜在表示可能导致语义不匹配，即使没有传统的传输错误，也会阻碍相互理解。在这项工作中，我们通过提出一种利用可重构智能表面（RIS）的联合物理和语义信道均衡框架来解决多输入多输出（MIMO）信道中的语义不匹配问题。语义均衡被实现为一系列变换：(i)发射机的预均衡阶段；(ii)通过RIS辅助信道传播；(iii)接收机的后均衡阶段。我们将问题表述为受约束的最小均方误差（MMSE）优化问题，并提出了两种解决方案：(i)线性语义均衡链；(ii)基于DNN的非线性语义均衡器。两种方法都设计为在潜在空间中进行语义压缩并遵守发射功率约束。通过广泛的评估，我们表明所提出的联合均衡策略在广泛的场景和无线信道条件下，始终优于传统的、分离的物理和语义信道均衡方法。", "summary": "该研究旨在解决多用户语义通信系统中由独立训练的AI设备导致的语义不匹配问题。通过引入一种RIS辅助的联合物理和语义信道均衡框架，该框架包括发射机预均衡、RIS辅助信道传播和接收机后均衡三个阶段。作者将该问题建模为受约束的MMSE优化，并提出了线性和DNN基的两种解决方案。实验结果表明，该联合均衡策略在多种场景下均优于传统的分离方法。", "keywords": "语义通信, 信道均衡, 可重构智能表面, 潜在空间对齐, 深度学习", "comments": "本文的创新点在于将RIS引入语义通信系统，并提出了一个联合物理和语义信道均衡框架，有效解决了多用户环境中潜在表示发散导致的语义不匹配问题。这种结合物理层和语义层优化的方法，为未来AI原生通信系统的设计提供了新的思路，具有重要的理论和实际意义。"}}
{"id": "2305.13613", "title": "A reduced-order model for segregated fluid-structure interaction solvers based on an ALE approach", "authors": ["Valentin Nkana Ngan", "Giovanni Stabile", "Andrea Mola", "Gianluigi Rozza"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2305.13613v3", "summary": "This article presents a Galerkin projection-based reduced-order modelling\n(ROM) approach for segregated fluid-structure interaction (FSI) problems,\nformulated within an Arbitrary Lagrangian Eulerian (ALE) framework at low\nReynolds numbers using the Finite Volume Method (FVM). The ROM is constructed\nusing Proper Orthogonal Decomposition (POD) and incorporates a data-driven\ntechnique that combines classical Galerkin projection with radial basis\nfunction (RBF) networks. The results demonstrate the numerical stability and\naccuracy of the proposed method relative to the high-fidelity model. The ROM\nsuccessfully captures transient flow fields and, importantly, the forces acting\non the moving structure without exhibiting unphysical growth or divergence over\ntime. This is further supported by the bounded evolution of error metrics and\nphysical observables, which remain consistent with the full-order simulations\nthroughout the prediction horizon. The method's effectiveness is validated\nthrough a benchmark vortex-induced vibration (VIV) case involving a circular\ncylinder at Reynolds number Re=200. The hybrid ROM approach yields an accurate\nand efficient tool for solving FSI problems involving mesh motion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2305.13613v3", "cate": "math.NA", "date": "2023-05-23", "updated": "2025-07-23", "AI": {"title_translation": "基于ALE方法的隔离流固耦合求解器的降阶模型", "tldr": "本文提出了一种基于ALE框架下隔离流固耦合问题的降阶模型（ROM），结合了Galerkin投影和RBF网络，在低雷诺数下表现出良好的稳定性、准确性和效率。", "motivation": "针对隔离流固耦合（FSI）问题，尤其是在任意拉格朗日欧拉（ALE）框架和低雷诺数下，需要一个能够有效处理网格运动并保持数值稳定性和准确性的高效求解工具。", "method": "提出了一种基于Galerkin投影的降阶建模（ROM）方法，用于基于ALE框架和有限体积法（FVM）的隔离流固耦合问题。该ROM利用 प्रॉपर正交分解（POD）构建，并结合了一种数据驱动技术，该技术将经典Galerkin投影与径向基函数（RBF）网络相结合。", "result": "该方法相对于高保真模型表现出数值稳定性和准确性。ROM成功捕获了瞬态流场以及作用在运动结构上的力，且未出现非物理增长或随时间发散。误差度量和物理可观测量的演变有界，在整个预测范围内与全阶模拟保持一致。通过雷诺数Re=200的圆形气缸涡激振动（VIV）基准案例验证了其有效性。", "conclusion": "所提出的混合ROM方法为解决涉及网格运动的流固耦合问题提供了一种准确高效的工具。", "translation": "本文提出了一种基于Galerkin投影的降阶建模（ROM）方法，用于在任意拉格朗日欧拉（ALE）框架下，采用有限体积法（FVM）在低雷诺数下解决隔离流固耦合（FSI）问题。该ROM是利用 प्रॉper正交分解（POD）构建的，并结合了一种数据驱动技术，该技术将经典Galerkin投影与径向基函数（RBF）网络相结合。结果表明，所提出的方法相对于高保真模型具有数值稳定性和准确性。该ROM成功捕获了瞬态流场，更重要的是，捕获了作用在运动结构上的力，而没有随时间表现出非物理增长或发散。误差度量和物理可观测量的有界演变进一步支持了这一点，它们在整个预测范围内与全阶模拟保持一致。该方法的有效性通过雷诺数Re=200的圆形气缸涡激振动（VIV）基准案例得到了验证。这种混合ROM方法为解决涉及网格运动的FSI问题提供了一种准确高效的工具。", "summary": "本文提出了一种基于Galerkin投影的降阶模型（ROM），用于处理基于任意拉格朗日欧拉（ALE）框架的隔离流固耦合（FSI）问题。该模型结合了 प्रॉper正交分解（POD）和一种将经典Galerkin投影与径向基函数（RBF）网络结合的数据驱动技术。实验结果表明，该混合ROM方法在捕获瞬态流场和结构受力方面具有良好的数值稳定性、准确性和效率，且避免了非物理发散，为涉及网格运动的FSI问题提供了一个有效工具。", "keywords": "降阶模型, 流固耦合, 任意拉格朗日欧拉, 伽辽金投影, 径向基函数", "comments": "这篇论文的创新点在于提出了一种混合降阶建模方法，将经典的Galerkin投影与数据驱动的径向基函数（RBF）网络相结合，以解决基于ALE框架的隔离流固耦合（FSI）问题。其重要性在于提供了一种计算效率高且能保持高精度的FSI求解工具，尤其适用于涉及复杂网格运动的场景，这对于工程仿真具有实际意义。"}}
{"id": "2507.17136", "title": "Dynamic Parameter Identification of a Curtain Wall Installation Robotic Arm", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17136v1", "summary": "In the construction industry, traditional methods fail to meet the modern\ndemands for efficiency and quality. The curtain wall installation is a critical\ncomponent of construction projects. We design a hydraulically driven robotic\narm for curtain wall installation and a dynamic parameter identification\nmethod. We establish a Denavit-Hartenberg (D-H) model based on measured robotic\narm structural parameters and integrate hydraulic cylinder dynamics to\nconstruct a composite parametric system driven by a Stribeck friction model. By\ndesigning high-signal-to-noise ratio displacement excitation signals for\nhydraulic cylinders and combining Fourier series to construct optimal\nexcitation trajectories that satisfy joint constraints, this method effectively\nexcites the characteristics of each parameter in the minimal parameter set of\nthe dynamic model of the robotic arm. On this basis, a hierarchical progressive\nparameter identification strategy is proposed: least squares estimation is\nemployed to separately identify and jointly calibrate the dynamic parameters of\nboth the hydraulic cylinder and the robotic arm, yielding Stribeck model curves\nfor each joint. Experimental validation on a robotic arm platform demonstrates\nresidual standard deviations below 0.4 Nm between theoretical and measured\njoint torques, confirming high-precision dynamic parameter identification for\nthe hydraulic-driven curtain wall installation robotic arm. This significantly\ncontributes to enhancing the intelligence level of curtain wall installation\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17136v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "幕墙安装机械臂动力学参数辨识", "tldr": "本文提出了一种高精度液压驱动幕墙安装机械臂的动力学参数辨识方法，旨在提升幕墙安装作业的智能化水平。", "motivation": "在建筑行业中，传统方法无法满足现代对效率和质量的需求，尤其在关键的幕墙安装环节。", "method": "设计了一种液压驱动的幕墙安装机械臂，并提出了一种动力学参数辨识方法。该方法建立了基于D-H模型、集成液压缸动力学和Stribeck摩擦模型的复合参数系统。通过设计高信噪比的位移激励信号和基于傅里叶级数的最佳激励轨迹，有效激发了动力学模型参数特性。进而，提出了一种分层渐进参数辨识策略，利用最小二乘估计分别辨识并联合校准液压缸和机械臂的动力学参数，并获得Stribeck模型曲线。", "result": "在机械臂平台上的实验验证显示，理论与测量关节扭矩之间的残差标准偏差低于0.4 Nm，证实了液压驱动幕墙安装机械臂的高精度动力学参数辨识。", "conclusion": "该方法实现了液压驱动幕墙安装机械臂的高精度动力学参数辨识，显著有助于提升幕墙安装作业的智能化水平。", "translation": "在建筑行业中，传统方法未能满足现代对效率和质量的需求。幕墙安装是建筑项目中的关键组成部分。我们设计了一种液压驱动的幕墙安装机械臂和一种动力学参数辨识方法。我们基于测量的机械臂结构参数建立了Denavit-Hartenberg (D-H) 模型，并结合液压缸动力学构建了一个由Stribeck摩擦模型驱动的复合参数系统。通过为液压缸设计高信噪比的位移激励信号，并结合傅里叶级数构建满足关节约束的最佳激励轨迹，该方法有效地激发了机械臂动力学模型最小参数集中每个参数的特性。在此基础上，提出了一种分层渐进参数辨识策略：采用最小二乘估计分别辨识并联合校准液压缸和机械臂的动力学参数，从而得到每个关节的Stribeck模型曲线。在机械臂平台上的实验验证表明，理论和测量关节扭矩之间的残差标准偏差低于0.4 Nm，证实了液压驱动幕墙安装机械臂的高精度动力学参数辨识。这极大地有助于提升幕墙安装操作的智能化水平。", "summary": "本论文针对传统建筑方法在幕墙安装中效率和质量不足的问题，设计了一种液压驱动的幕墙安装机械臂。提出了一种动力学参数辨识方法，该方法通过建立D-H模型、集成液压缸动力学和Stribeck摩擦的复合系统，并设计优化激励轨迹，采用分层最小二乘辨识策略。实验结果表明，该方法实现了高精度的参数辨识，有效提升了幕墙安装的智能化水平。", "keywords": "幕墙安装机械臂, 动力学参数辨识, 液压驱动, Stribeck摩擦, 最小二乘估计", "comments": "该论文为幕墙安装这一关键且劳动密集型建筑任务的自动化和精度提升提供了一个切实可行的解决方案。将液压动力学和Stribeck摩擦进行集成建模，并结合优化的激励和分层辨识策略，代表了一种实现高精度动力学参数辨识的稳健方法。这项工作对于通过机器人技术提高建筑效率和质量具有重要意义。"}}
{"id": "2507.17519", "title": "Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners", "authors": ["Kostas Karakontis", "Thanos Petsanis", "Athanasios Ch. Kapoutsis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17519v1", "summary": "Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17519v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "适用于二维无人机路径规划器的地形感知自适应", "tldr": "本文提出了一种模块化算法，能够扩展现有的二维无人机路径规划器，使其具备地形感知能力，通过调整高度和相机方向来改善三维重建效果。", "motivation": "现有的商业软件中的多无人机覆盖路径规划（mCPP）算法通常将感兴趣区域（RoI）视为二维平面，忽略了重要的三维结构特征。这导致了不完整的三维重建，尤其是在遮挡或垂直表面周围。", "method": "本文提出了一种模块化算法，通过调整高度和相机方向，将商业二维路径规划器扩展为地形感知规划。为证明其有效性，作者将DARP算法扩展为DARP-3D。", "result": "与基线相比，所提出的方法始终能捕获到改进的三维重建效果，尤其是在具有显著垂直特征的区域。在多个三维环境进行了仿真，并使用大疆硬件进行了真实飞行测试。", "conclusion": "通过将二维路径规划器扩展为地形感知规划，可以显著改善无人机在复杂三维环境中的三维重建质量。", "translation": "流行的商业软件中的多无人机覆盖路径规划（mCPP）算法通常将感兴趣区域（RoI）仅视为二维平面，忽略了重要的三维结构特征。这导致了不完整的三维重建，尤其是在遮挡或垂直表面周围。在本文中，我们提出了一种模块化算法，可以扩展商业二维路径规划器，通过调整高度和相机方向来促进地形感知规划。为了证明它，我们扩展了著名的DARP（优化多机器人覆盖路径规划的分区）算法，并生成了DARP-3D。我们展示了在多个三维环境中的仿真结果以及使用大疆硬件进行的真实飞行测试。与基线相比，我们的方法始终捕获到改进的三维重建，特别是在具有显著垂直特征的区域。该算法的开源实现可在以下网址获取：https://github.com/konskara/TerraPlan", "summary": "本文针对现有二维多无人机覆盖路径规划算法忽略三维地形导致重建不完整的问题，提出了一种模块化算法。该算法通过调整无人机高度和相机方向，将现有二维规划器扩展为地形感知规划。通过将DARP算法扩展为DARP-3D进行验证，仿真和真实飞行测试结果表明，该方法在复杂三维环境中能显著改善三维重建质量，尤其是在垂直特征区域。", "keywords": "无人机路径规划, 地形感知, 三维重建, 覆盖路径规划, DARP-3D", "comments": "该论文的创新点在于提出了一种模块化且通用的方法，能够将现有的二维无人机路径规划器升级为地形感知能力，而无需从头开发复杂的3D规划算法。通过调整高度和相机方向的简单策略，有效解决了传统2D规划在3D重建中的不足，尤其是在垂直结构区域。其开源实现也提升了研究成果的可用性和影响力。"}}
{"id": "2507.17422", "title": "Optimizing Car Resequencing on Mixed-Model Assembly Lines: Algorithm Development and Deployment", "authors": ["Andreas Karrenbauer", "Bernd Kuhn", "Kurt Mehlhorn", "Paolo Luigi Rinaldi"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17422v1", "summary": "The mixed-model assembly line (MMAL) is a production system used in the\nautomobile industry to manufacture different car models on the same conveyor,\noffering a high degree of product customization and flexibility. However, the\nMMAL also poses challenges, such as finding optimal sequences of models\nsatisfying multiple constraints and objectives related to production\nperformance, quality, and delivery -- including minimizing the number of color\nchangeovers in the Paint Shop, balancing the workload and setup times on the\nassembly line, and meeting customer demand and delivery deadlines. We propose a\nmulti-objective algorithm to solve the MMAL resequencing problem under\nconsideration of all these aspects simultaneously. We also present empirical\nresults obtained from recorded event data of the production process over $4$\nweeks following the deployment of our algorithm in the Saarlouis plant of\nFord-Werke GmbH. We achieved an improvement of the average batch size of about\n$30\\%$ over the old control software translating to a $23\\%$ reduction of color\nchangeovers. Moreover, we reduced the spread of cars planned for a specific\ndate by $10\\%$, reducing the risk of delays in delivery. We discuss\neffectiveness and robustness of our algorithm in improving production\nperformance and quality as well as trade-offs and limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17422v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "混合模型装配线汽车重新排序优化：算法开发与部署", "tldr": "该研究提出了一种多目标算法，用于优化混合模型装配线上的汽车重新排序问题，并在福特工厂部署后实现了显著的生产性能提升，例如颜色转换减少和交货延迟风险降低。", "motivation": "混合模型装配线（MMAL）在汽车工业中用于生产不同车型，提供高度定制和灵活性，但也面临挑战，如寻找满足生产性能、质量和交付等多个约束和目标的最佳模型序列，包括最小化喷漆车间的颜色转换、平衡装配线工作负载和设置时间以及满足客户需求和交货期限。", "method": "提出了一种多目标算法来同时解决考虑所有这些方面的MMAL重新排序问题。", "result": "在福特-Werke GmbH的Saarlouis工厂部署算法后，与旧控制软件相比，平均批次大小提高了约30%，颜色转换减少了23%。此外，计划在特定日期交付的汽车分散度降低了10%，从而降低了交付延迟的风险。", "conclusion": "该算法有效且鲁棒地提高了生产性能和质量，并减少了交付延迟的风险，但仍需讨论其权衡和局限性。", "translation": "混合模型装配线（MMAL）是汽车行业中用于在同一传送带上生产不同车型的生产系统，提供高度的产品定制和灵活性。然而，MMAL也带来了挑战，例如寻找满足与生产性能、质量和交付相关的多重约束和目标的最佳模型序列——包括最小化喷漆车间的颜色转换次数、平衡装配线上的工作负载和设置时间，以及满足客户需求和交货期限。我们提出了一种多目标算法来同时解决MMAL重新排序问题，并考虑所有这些方面。我们还展示了在福特-Werke GmbH萨尔路易斯工厂部署我们的算法后，从生产过程的4周记录事件数据中获得的实证结果。与旧的控制软件相比，我们将平均批次大小提高了约30%，这意味着颜色转换减少了23%。此外，我们将计划在特定日期交付的汽车分散度降低了10%，从而降低了交付延迟的风险。我们讨论了我们的算法在提高生产性能和质量方面的有效性和鲁棒性，以及其权衡和局限性。", "summary": "该论文提出了一种多目标算法，旨在解决混合模型装配线（MMAL）上的汽车重新排序问题，该问题涉及同时优化生产性能、质量和交付目标，如最小化颜色转换和平衡工作负载。该算法已在福特工厂部署，并取得了显著成效，包括平均批次大小增加30%（颜色转换减少23%）和计划交付汽车分散度降低10%，从而有效降低了交付延迟风险。论文还讨论了算法的有效性、鲁棒性及其局限性。", "keywords": "混合模型装配线, 汽车重新排序, 多目标算法, 生产优化, 颜色转换", "comments": "该论文的创新之处在于提出了一个多目标算法来同时解决MMAL的复杂重新排序问题，并考虑了多个相互关联的生产约束。其重要性体现在算法在实际工业环境（福特工厂）中的成功部署和显著的性能提升，这证明了其在优化生产效率和降低成本方面的实际应用价值。"}}
{"id": "2507.04591", "title": "Emerging Frameworks for Objective Task-based Evaluation of Quantitative Medical Imaging Methods", "authors": ["Yan Liu", "Huitian Xia", "Nancy A. Obuchowski", "Richard Laforest", "Arman Rahmim", "Barry A. Siegel", "Abhinav K. Jha"], "categories": ["physics.med-ph", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "Comments:      19 pages, 7 figures", "url": "http://arxiv.org/abs/2507.04591v2", "summary": "Quantitative imaging (QI) is demonstrating strong promise across multiple\nclinical applications. For clinical translation of QI methods, objective\nevaluation on clinically relevant tasks is essential. To address this need,\nmultiple evaluation strategies are being developed. In this paper, based on\nprevious literature, we outline four emerging frameworks to perform evaluation\nstudies of QI methods. We first discuss the use of virtual imaging trials\n(VITs) to evaluate QI methods. Next, we outline a no-gold-standard evaluation\nframework to clinically evaluate QI methods without ground truth. Third, a\nframework to evaluate QI methods for joint detection and quantification tasks\nis outlined. Finally, we outline a framework to evaluate QI methods that output\nmulti-dimensional parameters, such as radiomic features. We review these\nframeworks, discussing their utilities and limitations. Further, we examine\nfuture research areas in evaluation of QI methods. Given the recent\nadvancements in PET, including long axial field-of-view scanners and the\ndevelopment of artificial-intelligence algorithms, we present these frameworks\nin the context of PET.", "comment": "19 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.04591v2", "cate": "physics.med-ph", "date": "2025-07-07", "updated": "2025-07-22", "AI": {"title_translation": "定量医学影像方法客观任务评估的新兴框架", "tldr": "本文概述了四种新兴的定量医学影像（QI）方法评估框架，涵盖了虚拟成像试验、无金标准评估、联合检测与量化任务以及多维参数输出的评估，并将其应用于PET领域。", "motivation": "为了促进定量医学影像（QI）方法在临床中的转化，对其进行基于临床相关任务的客观评估至关重要。本文旨在概述应对这一需求的新兴评估策略。", "method": "本文基于现有文献，概述了四种新兴的定量医学影像（QI）方法评估框架，包括：1) 使用虚拟成像试验（VITs）进行评估；2) 适用于无金标准情况的临床评估框架；3) 针对联合检测和量化任务的评估框架；4) 针对输出多维参数（如影像组学特征）的QI方法的评估框架。文章回顾了这些框架的效用和局限性，并探讨了未来的研究方向。", "result": "本文概述了四种用于评估定量医学影像（QI）方法的新兴框架：虚拟成像试验（VITs）、无金标准评估、联合检测和量化任务评估、以及多维参数输出评估。这些框架的效用和局限性得到了讨论，并且在PET的背景下进行了呈现。", "conclusion": "本文提出的评估框架对于定量医学影像方法的临床转化至关重要，特别是在PET等快速发展的领域，这些框架有助于客观评估其性能，并指导未来的研究方向。", "translation": "定量影像（QI）在多个临床应用中展现出强大的前景。对于QI方法的临床转化，在临床相关任务上进行客观评估至关重要。为了满足这一需求，多种评估策略正在开发中。本文基于现有文献，概述了四种新兴的QI方法评估框架。我们首先讨论了使用虚拟成像试验（VITs）来评估QI方法。接下来，我们概述了一个无金标准评估框架，用于在没有真实标准的情况下临床评估QI方法。第三，概述了一个评估QI方法用于联合检测和量化任务的框架。最后，我们概述了一个评估输出多维参数（如影像组学特征）的QI方法的框架。我们回顾了这些框架，讨论了它们的效用和局限性。此外，我们还探讨了QI方法评估的未来研究领域。鉴于PET的最新进展，包括长轴视野扫描仪和人工智能算法的开发，我们在PET的背景下介绍了这些框架。", "summary": "本文基于现有文献，概述了四种用于客观评估定量医学影像（QI）方法的新兴框架，以促进其临床转化。这些框架包括虚拟成像试验（VITs）、无金标准评估、针对联合检测和量化任务的评估，以及对输出多维参数（如影像组学特征）的QI方法的评估。文章详细探讨了这些框架的实用性和局限性，并讨论了未来的研究方向，尤其是在PET技术快速发展的背景下。", "keywords": "定量影像, 评估框架, 医学影像, PET, 虚拟成像试验", "comments": "本文系统地总结了定量医学影像（QI）评估的最新框架，为QI方法的临床转化提供了重要的指导。其创新之处在于提出了多种适应不同评估场景的框架，特别是考虑了无金标准和多维参数输出等复杂情况。将这些框架置于PET的背景下，也体现了对当前医学影像领域前沿技术发展的关注。对于推动QI研究的标准化和临床应用具有重要意义。"}}
{"id": "2507.17301", "title": "Efficient Column-Wise N:M Pruning on RISC-V CPU", "authors": ["Chi-Wei Chu", "Ding-Yong Hong", "Jan-Jan Wu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17301v1", "summary": "In deep learning frameworks, weight pruning is a widely used technique for\nimproving computational efficiency by reducing the size of large models. This\nis especially critical for convolutional operators, which often act as\nperformance bottlenecks in convolutional neural networks (CNNs). However, the\neffectiveness of pruning heavily depends on how it is implemented, as different\nmethods can significantly impact both computational performance and memory\nfootprint. In this work, we propose a column-wise N:M pruning strategy applied\nat the tile level and modify XNNPACK to enable efficient execution of pruned\nmodels on the RISC-V vector architecture. Additionally, we propose fusing the\noperations of im2col and data packing to minimize redundant memory accesses and\nmemory overhead. To further optimize performance, we incorporate AITemplate's\nprofiling technique to identify the optimal implementation for each\nconvolutional operator. Our proposed approach effectively increases ResNet\ninference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy\nwithin 2.1\\% of the dense baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17301v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "RISC-V CPU上的高效列式N:M剪枝", "tldr": "本文提出了一种高效的列式N:M剪枝策略，并对XNNPACK进行了修改，以在RISC-V向量架构上加速深度学习模型的推理，实现了4.0倍的吞吐量提升。", "motivation": "在深度学习框架中，权重剪枝是提高计算效率和减小模型大小的常用技术，尤其对于卷积操作来说至关重要，因为它们常常是卷积神经网络（CNN）的性能瓶颈。然而，剪枝的有效性严重依赖于其实现方式，不同的方法会显著影响计算性能和内存占用。因此，需要一种高效的剪枝实现方案，特别是在RISC-V等资源受限的平台上。", "method": "本文提出了一种应用于瓦片级别的列式N:M剪枝策略，并修改了XNNPACK以在RISC-V向量架构上高效执行剪枝模型。此外，还提出了融合im2col和数据打包操作，以最小化冗余内存访问和内存开销。为进一步优化性能，引入了AITemplate的分析技术来识别每个卷积操作的最佳实现。", "result": "所提出的方法将ResNet推理吞吐量提高了4.0倍，并且ImageNet top-1准确率保持在密集基线2.1%以内。", "conclusion": "本文提出的列式N:M剪枝策略结合多项优化，显著提高了RISC-V向量架构上深度学习模型的推理吞吐量，同时保持了高精度。", "translation": "在深度学习框架中，权重剪枝是一种广泛使用的技术，通过减小大型模型的尺寸来提高计算效率。这对于卷积运算符尤为关键，它们通常是卷积神经网络（CNN）中的性能瓶颈。然而，剪枝的有效性在很大程度上取决于其实现方式，因为不同的方法会显著影响计算性能和内存占用。在这项工作中，我们提出了一种应用于瓦片级别的列式N:M剪枝策略，并修改了XNNPACK，以在RISC-V向量架构上实现剪枝模型的高效执行。此外，我们提出融合im2col和数据打包操作，以最小化冗余内存访问和内存开销。为了进一步优化性能，我们结合了AITemplate的分析技术来识别每个卷积操作的最佳实现。我们提出的方法有效地将ResNet推理吞吐量提高了4.0倍，并将ImageNet top-1准确率保持在密集基线2.1%以内。", "summary": "本文提出了一种面向RISC-V向量架构的高效列式N:M剪枝策略，旨在优化深度学习模型的计算效率和内存占用。通过在瓦片级别应用剪枝、修改XNNPACK、融合im2col和数据打包操作，并利用AITemplate进行性能分析，该方法显著提升了ResNet推理吞吐量达4.0倍，同时保持了与密集模型相近的ImageNet准确率。", "keywords": "权重剪枝, RISC-V, 卷积神经网络, 性能优化, XNNPACK", "comments": "本文的创新点在于将列式N:M剪枝策略与针对RISC-V向量架构的特定优化相结合，包括对XNNPACK的修改、im2col和数据打包的融合以及AITemplate的性能分析。这对于在资源受限的边缘设备上部署高效的深度学习模型具有重要意义，尤其是在CNNs性能优化方面展现了显著的提升效果。"}}
{"id": "2507.17482", "title": "LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning", "authors": ["Luca Salvatore Lorello", "Nikolaos Manginas", "Marco Lippi", "Stefano Melacci"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17482v1", "summary": "Neuro-symbolic artificial intelligence aims to combine neural architectures\nwith symbolic approaches that can represent knowledge in a human-interpretable\nformalism. Continual learning concerns with agents that expand their knowledge\nover time, improving their skills while avoiding to forget previously learned\nconcepts. Most of the existing approaches for neuro-symbolic artificial\nintelligence are applied to static scenarios only, and the challenging setting\nwhere reasoning along the temporal dimension is necessary has been seldom\nexplored. In this work we introduce LTLZinc, a benchmarking framework that can\nbe used to generate datasets covering a variety of different problems, against\nwhich neuro-symbolic and continual learning methods can be evaluated along the\ntemporal and constraint-driven dimensions. Our framework generates expressive\ntemporal reasoning and continual learning tasks from a linear temporal logic\nspecification over MiniZinc constraints, and arbitrary image classification\ndatasets. Fine-grained annotations allow multiple neural and neuro-symbolic\ntraining settings on the same generated datasets. Experiments on six\nneuro-symbolic sequence classification and four class-continual learning tasks\ngenerated by LTLZinc, demonstrate the challenging nature of temporal learning\nand reasoning, and highlight limitations of current state-of-the-art methods.\nWe release the LTLZinc generator and ten ready-to-use tasks to the\nneuro-symbolic and continual learning communities, in the hope of fostering\nresearch towards unified temporal learning and reasoning frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17482v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "LTLZinc：一个用于持续学习和神经符号时间推理的基准测试框架", "tldr": "LTLZinc是一个基准测试框架，用于评估神经符号和持续学习方法在时间推理和约束驱动任务中的表现，并揭示了现有方法的局限性。", "motivation": "现有的神经符号人工智能方法大多应用于静态场景，很少探索需要沿时间维度进行推理的挑战性设置。", "method": "本文引入了LTLZinc，一个基准测试框架，能够从线性时间逻辑规范（基于MiniZinc约束）和任意图像分类数据集生成多样化的时间推理和持续学习任务数据集。该框架提供细粒度注解，允许在相同生成的数据集上进行多种神经和神经符号训练设置。", "result": "在LTLZinc生成的六个神经符号序列分类任务和四个类持续学习任务上的实验表明，时间学习和推理具有挑战性，并突出了当前最先进方法的局限性。", "conclusion": "LTLZinc生成器和十个即用型任务的发布旨在促进神经符号和持续学习社区在统一时间学习和推理框架方面的研究。", "translation": "神经符号人工智能旨在将神经网络架构与符号方法相结合，后者能够以人类可解释的形式表示知识。持续学习关注的是智能体随时间扩展知识，在提升技能的同时避免遗忘先前学习的概念。大多数现有的神经符号人工智能方法仅应用于静态场景，而需要沿时间维度进行推理的挑战性设置则鲜有探索。在这项工作中，我们引入了LTLZinc，一个基准测试框架，可用于生成涵盖各种不同问题的数据集，并据此评估神经符号和持续学习方法在时间维度和约束驱动维度上的表现。我们的框架通过MiniZinc约束上的线性时间逻辑规范和任意图像分类数据集，生成富有表达力的时间推理和持续学习任务。细粒度标注允许在相同生成的数据集上进行多种神经和神经符号训练设置。在LTLZinc生成的六个神经符号序列分类任务和四个类持续学习任务上的实验，证明了时间学习和推理的挑战性，并突出了当前最先进方法的局限性。我们发布了LTLZinc生成器和十个即用型任务给神经符号和持续学习社区，希望能促进对统一时间学习和推理框架的研究。", "summary": "本文介绍了LTLZinc，一个新颖的基准测试框架，旨在解决神经符号AI和持续学习在时间推理方面研究不足的问题。LTLZinc能够生成复杂的、时间依赖的数据集，用于评估和比较不同方法在时间维度和约束驱动任务中的性能。实验结果揭示了现有先进方法在处理时间学习和推理时的局限性。该框架及其预设任务的发布旨在推动相关领域的研究进展。", "keywords": "持续学习, 神经符号AI, 时间推理, 基准测试, 线性时间逻辑", "comments": "LTLZinc的创新之处在于它提供了一个急需的基准测试平台，专注于神经符号AI和持续学习中的时间推理，这是现有研究的薄弱环节。其能够生成多样化、带有细粒度标注的数据集，对于推动该领域的发展具有重要意义。通过揭示现有方法的局限性，它为未来的研究指明了方向。"}}
{"id": "2507.16876", "title": "Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review", "authors": ["Charlotte Jennings", "Andrew Broad", "Lucy Godson", "Emily Clarke", "David Westhead", "Darren Treanor"], "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Main article (50 pages, inc 3 tables, 4 figures). Supplementary material included with additional methodological information and data", "url": "http://arxiv.org/abs/2507.16876v1", "summary": "Multimodal machine learning integrating histopathology and molecular data\nshows promise for cancer prognostication. We systematically reviewed studies\ncombining whole slide images (WSIs) and high-throughput omics to predict\noverall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL\n(12/08/2024), plus citation screening, identified eligible studies. Data\nextraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed\nSWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).\n  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all\nused The Cancer Genome Atlas. Approaches included regularised Cox regression\n(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged\n0.550-0.857; multimodal models typically outperformed unimodal ones. However,\nall studies showed unclear/high bias, limited external validation, and little\nfocus on clinical utility.\n  Multimodal WSI-omics survival prediction is a fast-growing field with\npromising results but needs improved methodological rigor, broader datasets,\nand clinical evaluation.\n  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),\nsupported by UKRI Industrial Strategy Challenge Fund.", "comment": "Main article (50 pages, inc 3 tables, 4 figures). Supplementary\n  material included with additional methodological information and data", "pdf_url": "http://arxiv.org/pdf/2507.16876v1", "cate": "q-bio.QM", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "机器学习驱动的多模态预后模型整合病理图像和高通量组学数据用于癌症总生存期预测：一项系统综述", "tldr": "一项系统综述发现，结合病理图像和组学数据的多模态机器学习模型在癌症预后方面有前景，但方法学严谨性、数据集和临床评估仍需改进。", "motivation": "多模态机器学习整合组织病理学和分子数据在癌症预后方面显示出前景，但需要系统地审查结合全玻片图像（WSIs）和高通量组学数据以预测总生存期的研究。", "method": "研究者对EMBASE、PubMed和Cochrane CENTRAL数据库进行了系统检索（截至2024年8月12日），并进行引文筛选。数据提取使用CHARMS工具，偏倚评估采用PROBAST+AI，综合分析遵循SWiM和PRISMA 2020指南。研究方案已在PROSPERO注册（CRD42024594745）。", "result": "识别出48项研究（均自2017年以来），涵盖19种癌症类型，所有研究均使用了The Cancer Genome Atlas数据。研究方法包括正则化Cox回归（4项）、经典机器学习（13项）和深度学习（31项）。报告的c-指数范围为0.550-0.857；多模态模型通常优于单模态模型。然而，所有研究都显示出不明确/高偏倚，外部验证有限，且很少关注临床实用性。", "conclusion": "多模态WSI-组学生存预测是一个快速发展的领域，结果有前景，但需要改进方法学严谨性、更广泛的数据集和临床评估。", "translation": "多模态机器学习整合组织病理学和分子数据在癌症预后方面显示出前景。我们系统地回顾了结合全玻片图像（WSIs）和高通量组学数据以预测总生存期的研究。通过检索EMBASE、PubMed和Cochrane CENTRAL（截至2024年8月12日）以及引文筛选，确定了符合条件的研究。数据提取使用CHARMS工具；偏倚评估采用PROBAST+AI；综合分析遵循SWiM和PRISMA 2020指南。研究方案已在PROSPERO注册（CRD42024594745）。\n共有48项研究（均自2017年以来），涵盖19种癌症类型，符合标准；所有研究均使用了The Cancer Genome Atlas数据。研究方法包括正则化Cox回归（n=4）、经典机器学习（n=13）和深度学习（n=31）。报告的c-指数范围为0.550-0.857；多模态模型通常优于单模态模型。然而，所有研究都显示出不明确/高偏倚，外部验证有限，且很少关注临床实用性。\n多模态WSI-组学生存预测是一个快速发展的领域，结果有前景，但需要改进方法学严谨性、更广泛的数据集和临床评估。\n由英国利兹教学医院NHS信托基金NPIC资助（项目104687），并得到英国研究与创新产业战略挑战基金的支持。", "summary": "本系统综述旨在评估结合病理图像和高通量组学数据的机器学习模型在癌症总生存期预测中的应用。研究回顾了48项相关文献，发现多模态模型通常优于单模态模型，但普遍存在方法学偏倚高、外部验证不足及临床实用性关注度低的问题。尽管该领域发展迅速且结果有前景，但仍需提高研究严谨性，扩展数据集并加强临床评估。", "keywords": "机器学习, 多模态, 癌症预后, 系统综述, 组学数据", "comments": "这篇系统综述揭示了将机器学习应用于多模态数据（病理图像和组学数据）进行癌症预后预测的巨大潜力，同时也明确指出了当前研究存在的普遍局限性，如方法学严谨性不足、外部验证缺乏和临床实用性关注度低。其重要性在于为未来该领域的研究指明了方向，强调了需要改进的方面，以确保研究结果的可靠性和临床转化价值。"}}
{"id": "2507.17248", "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th Annual ACM Symposium on User Interface Software and Technology), Busan, Republic of Korea, 28 Sep - 1 Oct 2025", "url": "http://arxiv.org/abs/2507.17248v1", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.", "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "pdf_url": "http://arxiv.org/pdf/2507.17248v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "现实代理：通过抽象表示在MR中与现实世界对象进行流畅交互", "tldr": "现实代理系统通过抽象表示（代理）解决了混合现实中与拥挤、遥远或被遮挡的现实对象交互的难题，实现了更流畅、更丰富的交互，无需新手势或菜单。", "motivation": "在混合现实（MR）中，当现实世界对象拥挤、遥远或部分被遮挡时，直接选择和操作它们非常困难，因为输入与物理约束紧密耦合。", "method": "引入代理（现实世界对象的抽象表示）来解耦交互与物理约束。开发了“现实代理”（Reality Proxy）系统，在选择过程中将交互目标从物理对象无缝转移到其代理。该系统利用AI为代理丰富语义属性和分层空间关系，从而实现新颖的交互。", "result": "现实代理系统能够促进基本选择，并实现以前笨拙的MR交互，如浏览、基于属性的过滤、导航嵌套组和复杂的多对象选择，且无需新的手势或菜单系统。系统在办公室信息检索、大规模空间导航和多无人机控制等多种场景中展示了其多功能性。专家评估表明该系统具有实用性和可用性。", "conclusion": "基于代理的抽象为未来的MR系统提供了一种强大且可泛化的交互范式。", "translation": "在混合现实（MR）中，当现实世界对象拥挤、遥远或部分被遮挡时，与它们进行交互通常会变得困难，阻碍了直接的选择和操作。我们观察到这些困难源于直接在物理对象上执行交互，其中输入与其物理约束紧密耦合。我们的关键洞察是通过引入代理——现实世界对象的抽象表示——来将交互与这些约束解耦。我们将这一概念体现在“现实代理”（Reality Proxy）系统中，该系统在选择过程中将交互目标从物理对象无缝转移到其代理。除了促进基本选择，“现实代理”系统还利用AI为代理丰富其对应物理对象的语义属性和分层空间关系，从而在MR中实现新颖且以前笨拙的交互——例如浏览、基于属性的过滤、导航嵌套组和复杂的多对象选择——所有这些都无需新的手势或菜单系统。我们展示了“现实代理”在多种场景中的多功能性，包括办公室信息检索、大规模空间导航和多无人机控制。专家评估表明该系统具有实用性和可用性，这表明基于代理的抽象为未来的MR系统提供了一种强大且可泛化的交互范式。", "summary": "本文提出了“现实代理”（Reality Proxy）系统，旨在解决混合现实（MR）中与拥挤、遥远或被遮挡的现实世界对象交互的难题。通过引入现实对象的抽象表示（代理），该系统将交互从物理约束中解耦。现实代理利用AI为代理添加语义和空间信息，从而实现高级交互，如过滤和分组导航，而无需新的交互方式。系统在多个场景中得到验证，并被专家认为具有实用性和通用性，为未来的MR交互范式提供了新思路。", "keywords": "混合现实, 现实代理, 抽象表示, 人机交互, AI增强交互", "comments": "这篇论文的创新点在于提出了“现实代理”这一概念，通过引入现实世界对象的抽象表示来解耦交互与物理约束，有效解决了MR中复杂场景下的对象选择和操作难题。利用AI增强代理的语义和空间属性，进一步扩展了交互的可能性，且无需用户学习新的手势或菜单系统，大大提升了用户体验和系统的通用性。这种基于抽象代理的交互范式为未来MR系统的设计提供了强大的新思路。"}}
{"id": "2507.17292", "title": "Non-Orthogonal AFDM: A Promising Spectrum-Efficient Waveform for 6G High-Mobility Communications", "authors": ["Yu Zhang", "Qin Yi", "Leila Musavian", "Tongyang Xu", "Zilong Liu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      This work has been accepted by IEEE PIMRC 2025", "url": "http://arxiv.org/abs/2507.17292v1", "summary": "This paper proposes a spectrum-efficient nonorthogonal affine frequency\ndivision multiplexing (AFDM) waveform for reliable high-mobility communications\nin the upcoming sixth-generation (6G) mobile systems. Our core idea is to\nintroduce a compression factor to enable controllable subcarrier overlapping in\nchirp-based AFDM modulation. To mitigate intercarrier interference (ICI), we\nintroduce linear precoding at the transmitter and an iterative detection scheme\nat the receiver. Simulation results demonstrate that these techniques can\neffectively reduce interference and maintain robust bit error rate (BER)\nperformance even under aggressive compression factors and high-mobility channel\nconditions. The proposed non-orthogonal AFDM waveform offers a promising\nsolution for next-generation wireless networks, balancing spectrum efficiency\nand Doppler resilience in highly dynamic environments.", "comment": "This work has been accepted by IEEE PIMRC 2025", "pdf_url": "http://arxiv.org/pdf/2507.17292v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "非正交AFDM：一种有前景的6G高移动性通信频谱高效波形", "tldr": "本文提出了一种非正交AFDM波形，通过引入压缩因子、线性预编码和迭代检测，在高移动性6G通信中实现了频谱效率和多普勒抗性的平衡。", "motivation": "为即将到来的第六代（6G）移动系统中的高移动性通信提供一种可靠且频谱高效的波形。", "method": "提出了一种频谱高效的非正交仿射频分复用（AFDM）波形。核心思想是引入一个压缩因子，以实现基于线性调频的AFDM调制中可控的子载波重叠。为减轻载波间干扰（ICI），在发送端引入线性预编码，并在接收端引入迭代检测方案。", "result": "仿真结果表明，所提出的技术即使在激进的压缩因子和高移动性信道条件下，也能有效减少干扰并保持鲁棒的误码率（BER）性能。", "conclusion": "所提出的非正交AFDM波形为下一代无线网络提供了一个有前景的解决方案，在高动态环境中平衡了频谱效率和多普勒抗性。", "translation": "本文提出了一种频谱高效的非正交仿射频分复用（AFDM）波形，用于即将到来的第六代（6G）移动系统中的可靠高移动性通信。我们的核心思想是引入一个压缩因子，以实现基于线性调频的AFDM调制中可控的子载波重叠。为了减轻载波间干扰（ICI），我们在发送端引入线性预编码，并在接收端引入迭代检测方案。仿真结果表明，即使在激进的压缩因子和高移动性信道条件下，这些技术也能有效减少干扰并保持鲁棒的误码率（BER）性能。所提出的非正交AFDM波形为下一代无线网络提供了一个有前景的解决方案，在高动态环境中平衡了频谱效率和多普勒抗性。", "summary": "本文提出了一种针对6G高移动性通信的频谱高效非正交AFDM波形。通过引入压缩因子实现子载波重叠，并结合发送端的线性预编码和接收端的迭代检测方案来抑制载波间干扰。仿真结果验证了该方案在激进压缩和高移动性环境下能有效降低干扰并保持良好的误码率性能，为未来无线网络提供了频谱效率和多普勒抗性的平衡解决方案。", "keywords": "非正交AFDM, 6G通信, 高移动性, 频谱效率, 载波间干扰", "comments": "该论文创新性地将压缩因子引入AFDM调制，以实现非正交传输，从而提高频谱效率。同时，通过结合线性预编码和迭代检测来有效管理由此带来的载波间干扰，展现了在6G高移动性场景下的潜力。其主要贡献在于提出了一种能够平衡频谱效率和多普勒抗性的新波形。"}}
{"id": "2501.08518", "title": "Alleviating Seasickness through Brain-Computer Interface-based Attention Shift", "authors": ["Xiaoyu Bao", "Kailin Xu", "Jiawei Zhu", "Haiyun Huang", "Kangning Li", "Qiyun Huang", "Yuanqing Li"], "categories": ["cs.HC", "cs.AI", "eess.SP", "q-bio.QM"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.08518v2", "summary": "Seasickness poses a widespread problem that adversely impacts both passenger\ncomfort and the operational efficiency of maritime crews. Although attention\nshift has been proposed as a potential method to alleviate symptoms of motion\nsickness, its efficacy remains to be rigorously validated, especially in\nmaritime environments. In this study, we develop an AI-driven brain-computer\ninterface (BCI) to realize sustained and practical attention shift by\nincorporating tasks such as breath counting. Forty-three participants completed\na real-world nautical experiment consisting of a real-feedback session, a\nresting session, and a pseudo-feedback session. Notably, 81.39\\% of the\nparticipants reported that the BCI intervention was effective. EEG analysis\nrevealed that the proposed system can effectively regulate motion sickness EEG\nsignatures, such as an decrease in total band power, along with an increase in\ntheta relative power and a decrease in beta relative power. Furthermore, an\nindicator of attentional focus, the theta/beta ratio, exhibited a significant\nreduction during the real-feedback session, providing further evidence to\nsupport the effectiveness of the BCI in shifting attention. Collectively, this\nstudy presents a novel nonpharmacological, portable, and effective approach for\nseasickness intervention, which has the potential to open up a brand-new\napplication domain for BCIs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.08518v2", "cate": "cs.HC", "date": "2025-01-15", "updated": "2025-07-23", "AI": {"title_translation": "通过脑机接口实现注意力转移以缓解晕船", "tldr": "研究开发了一种基于脑机接口的注意力转移系统，有效缓解了晕船症状。", "motivation": "晕船是一个普遍存在的问题，影响乘客舒适度和海员操作效率。虽然注意力转移被认为是缓解动晕症的潜在方法，但其在海洋环境中的疗效尚未得到严格验证。", "method": "本研究开发了一种AI驱动的脑机接口（BCI），通过结合数呼吸等任务实现持续有效的注意力转移。43名参与者完成了一项真实航海实验，包括真实反馈会话、休息会话和伪反馈会话。", "result": "81.39%的参与者报告BCI干预有效。脑电图（EEG）分析显示，该系统能有效调节动晕症的脑电特征，如总频带功率下降、theta相对功率增加和beta相对功率下降。此外，注意力集中指标theta/beta比在真实反馈会话中显著降低。", "conclusion": "这项研究提出了一种新颖的、非药物的、便携且有效的晕船干预方法，有望为脑机接口开辟全新的应用领域。", "translation": "晕船是一个普遍存在的问题，对乘客舒适度和海员的操作效率都产生不利影响。尽管注意力转移被提出作为缓解动晕症症状的潜在方法，但其疗效仍有待严格验证，尤其是在海洋环境中。在这项研究中，我们开发了一种人工智能驱动的脑机接口（BCI），通过结合数呼吸等任务来实现持续和实用的注意力转移。43名参与者完成了一项真实世界的航海实验，包括真实反馈会话、休息会话和伪反馈会话。值得注意的是，81.39%的参与者报告BCI干预有效。脑电图（EEG）分析显示，所提出的系统可以有效调节动晕症的脑电图特征，例如总频带功率的降低，以及theta相对功率的增加和beta相对功率的降低。此外，注意力集中的一个指标，theta/beta比，在真实反馈会话期间表现出显著降低，为支持BCI在转移注意力方面的有效性提供了进一步的证据。总的来说，这项研究提出了一种新颖的、非药物的、便携且有效的晕船干预方法，这有潜力为脑机接口开辟一个全新的应用领域。", "summary": "本研究开发了一种AI驱动的脑机接口（BCI），通过注意力转移来缓解晕船。在真实航海实验中，81.39%的参与者认为BCI有效，且EEG分析显示其能调节动晕症相关的脑电特征并显著降低注意力集中指标theta/beta比。这表明该方法是一种新颖、非药物且有效的晕船干预手段。", "keywords": "晕船, 脑机接口, 注意力转移, 脑电图, 动晕症", "comments": "本研究创新性地将AI驱动的脑机接口应用于晕船缓解，提供了一种非药物、便携且有效的新方法，有望拓展BCI的应用领域。实验结果通过参与者反馈和EEG数据提供了有力证据。其创新点在于将注意力转移与BCI技术结合，并进行了真实航海实验验证。然而，抽象中未提及与其他现有方法的比较，且长期效果有待进一步研究。"}}
{"id": "2504.20630", "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Tao Jin", "Zhou Zhao"], "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2504.20630v5", "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos are\navailable at https://aaronz345.github.io/ISDramaDemo. We provide the dataset\nand the evaluation code at https://huggingface.co/datasets/AaronZ345/MRSDrama\nand https://github.com/AaronZ345/ISDrama.", "comment": "Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2504.20630v5", "cate": "eess.AS", "date": "2025-04-29", "updated": "2025-07-23", "AI": {"title_translation": "ISDrama：通过多模态提示生成沉浸式空间戏剧", "tldr": "本文首次提出了ISDrama模型和MRSDrama数据集，旨在解决基于多模态提示的沉浸式空间戏剧生成任务中的挑战。", "motivation": "多模态沉浸式空间戏剧生成任务面临同时建模空间信息和戏剧韵律的挑战，且数据收集成本高昂，目前尚无有效解决方案。", "method": "本文构建了首个多模态录制空间戏剧数据集MRSDrama。在此基础上，提出了首个通过多模态提示生成沉浸式空间戏剧的模型ISDrama。ISDrama包含多模态姿态编码器（基于对比学习，考虑多普态效应）和沉浸式戏剧Transformer（基于流的mamba-transformer，结合Drama-MOE进行韵律和姿态控制）。此外，还设计了上下文一致的无分类器指导策略以连贯生成完整戏剧。", "result": "实验结果表明，ISDrama在客观和主观指标上均优于基线模型。", "conclusion": "本文通过构建首个多模态录制空间戏剧数据集MRSDrama并提出ISDrama模型，首次成功解决了基于多模态提示的沉浸式空间戏剧生成中的挑战，并在性能上超越了基线模型。", "translation": "多模态沉浸式空间戏剧生成旨在基于多模态提示创建具有戏剧性韵律的连续多说话人双耳语音，在AR、VR等领域具有潜在应用。该任务需要基于多模态输入同时建模空间信息和戏剧性韵律，且数据收集成本高昂。据我们所知，我们的工作是首次尝试解决这些挑战。我们构建了MRSDrama，首个多模态录制空间戏剧数据集，其中包含双耳戏剧音频、剧本、视频、几何姿态和文本提示。然后，我们提出了ISDrama，首个通过多模态提示生成沉浸式空间戏剧的模型。ISDrama包含以下主要组件：1）多模态姿态编码器，基于对比学习，考虑移动说话人引起的多普勒效应，从多模态提示中提取统一的姿态信息。2）沉浸式戏剧Transformer，一个基于流的mamba-transformer模型，用于生成高质量戏剧，并结合Drama-MOE来选择合适的专家以增强韵律和姿态控制。我们还设计了一种上下文一致的无分类器指导策略，以连贯地生成完整戏剧。实验结果表明，ISDrama在客观和主观指标上均优于基线模型。演示可在https://aaronz345.github.io/ISDramaDemo获取。我们提供了数据集和评估代码，分别位于https://huggingface.co/datasets/AaronZ345/MRSDrama和https://github.com/AaronZ345/ISDrama。", "summary": "本文针对多模态沉浸式空间戏剧生成中同时建模空间信息和戏剧韵律的挑战及高昂的数据收集成本，首次提出了解决方案。研究者构建了首个多模态录制空间戏剧数据集MRSDrama，并提出了首个沉浸式空间戏剧生成模型ISDrama。ISDrama模型包含多模态姿态编码器和沉浸式戏剧Transformer，并设计了上下文一致的生成策略。实验结果验证了ISDrama在各项指标上均优于基线模型。", "keywords": "沉浸式空间戏剧生成, 多模态提示, 双耳语音, 戏剧韵律, 数据集", "comments": "该论文在沉浸式空间戏剧生成领域具有开创性意义。它不仅首次提出了解决该任务的模型ISDrama，还构建了首个多模态数据集MRSDrama，解决了数据稀缺的痛点。ISDrama模型结合了对比学习、mamba-transformer和MOE等先进技术，旨在有效处理多模态输入、空间信息和戏剧韵律。其在AR/VR等领域的应用潜力巨大，为未来沉浸式媒体内容创作奠定了基础。"}}
{"id": "2504.03038", "title": "How to Adapt Control Barrier Functions? A Learning-Based Approach with Applications to a VTOL Quadplane", "authors": ["Taekyung Kim", "Randal W. Beard", "Dimitra Panagou"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      2025 IEEE Conference on Decision and Control (CDC). Project page: this https URL", "url": "http://arxiv.org/abs/2504.03038v3", "summary": "In this paper, we present a novel theoretical framework for online adaptation\nof Control Barrier Function (CBF) parameters, i.e., of the class K functions\nincluded in the CBF condition, under input constraints. We introduce the\nconcept of locally validated CBF parameters, which are adapted online to\nguarantee finite-horizon safety, based on conditions derived from Nagumo's\ntheorem and tangent cone analysis. To identify these parameters online, we\nintegrate a learning-based approach with an uncertainty-aware verification\nprocess that account for both epistemic and aleatoric uncertainties inherent in\nneural network predictions. Our method is demonstrated on a VTOL quadplane\nmodel during challenging transition and landing maneuvers, showcasing enhanced\nperformance while maintaining safety.", "comment": "2025 IEEE Conference on Decision and Control (CDC). Project page:\n  https://www.taekyung.me/how-to-adapt-cbf", "pdf_url": "http://arxiv.org/pdf/2504.03038v3", "cate": "cs.RO", "date": "2025-04-03", "updated": "2025-07-23", "AI": {"title_translation": "如何自适应控制障碍函数？一种基于学习的方法及其在垂直起降四旋翼飞行器上的应用", "tldr": "本文提出一种基于学习的在线自适应控制障碍函数（CBF）参数的方法，以确保垂直起降四旋翼飞行器在复杂机动中的有限时间安全。", "motivation": "论文旨在解决在存在输入约束的情况下，如何在线自适应控制障碍函数（CBF）参数，以确保有限时间内的安全性。", "method": "本文提出了一种新颖的理论框架，用于在线自适应控制障碍函数（CBF）参数。具体方法包括：引入局部验证的CBF参数概念，基于Nagumo定理和切线锥分析的条件，实现在线自适应以保证有限时间安全；集成基于学习的方法，并结合考虑神经网络预测中认知不确定性和偶然不确定性的不确定性感知验证过程。", "result": "该方法在垂直起降四旋翼飞行器模型上，于具有挑战性的过渡和着陆机动中进行了验证，结果显示在保持安全性的同时，性能得到了提升。", "conclusion": "论文成功开发并验证了一种基于学习的在线自适应控制障碍函数（CBF）框架，该框架能够确保安全性并提升性能，即使在存在不确定性的情况下也是如此。", "translation": "在本文中，我们提出了一种新颖的理论框架，用于在输入约束下在线自适应控制障碍函数（CBF）参数，即CBF条件中包含的K类函数。我们引入了局部验证的CBF参数概念，这些参数基于Nagumo定理和切线锥分析导出的条件在线自适应，以保证有限时间内的安全性。为了在线识别这些参数，我们集成了一种基于学习的方法与一种不确定性感知验证过程，该过程考虑了神经网络预测中固有的认知不确定性和偶然不确定性。我们的方法在垂直起降四旋翼飞行器模型上，于具有挑战性的过渡和着陆机动中进行了演示，结果显示在保持安全性的同时，性能得到了提升。", "summary": "本文提出了一种新颖的理论框架，用于在线自适应控制障碍函数（CBF）参数（即CBF条件中包含的K类函数），并在输入约束下进行。该方法引入了局部验证的CBF参数概念，这些参数基于Nagumo定理和切线锥分析导出的条件在线自适应，以保证有限时间内的安全性。为了在线识别这些参数，该框架将一种基于学习的方法与一种不确定性感知验证过程相结合，该过程考虑了神经网络预测中固有的认知不确定性和偶然不确定性。该方法在垂直起降四旋翼飞行器模型上，于具有挑战性的过渡和着陆机动中进行了演示，结果表明在保持安全性的同时，性能得到了提升。", "keywords": "控制障碍函数, 在线自适应, 基于学习的控制, 垂直起降四旋翼飞行器, 安全性", "comments": "本文的创新之处在于实现了控制障碍函数（CBF）参数的在线自适应，并将其与考虑不确定性的学习验证过程相结合，这对于存在固有不确定性的实际机器人应用至关重要。将其应用于垂直起降四旋翼飞行器，突显了其在复杂航空机动中的实际相关性。"}}
{"id": "2409.12967", "title": "How Consistent Are Humans When Grading Programming Assignments?", "authors": ["Marcus Messer", "Neil C. C. Brown", "Michael Kölling", "Miaojing Shi"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.12967v3", "summary": "Providing consistent summative assessment to students is important, as the\ngrades they are awarded affect their progression through university and future\ncareer prospects. While small cohorts are typically assessed by a single\nassessor, such as the module/class leader, larger cohorts are often assessed by\nmultiple assessors, typically teaching assistants, which increases the risk of\ninconsistent grading.\n  To investigate the consistency of human grading of programming assignments,\nwe asked 28 participants to each grade 40 CS1 introductory Java assignments,\nproviding grades and feedback for correctness, code elegance, readability and\ndocumentation; the 40 assignments were split into two batches of 20. The 28\nparticipants were divided into seven groups of four (where each group graded\nthe same 40 assignments) to allow us to investigate the consistency of a group\nof assessors. In the second batch of 20, we duplicated one assignment from the\nfirst to analyse the internal consistency of individual assessors.\n  Our results show that human graders in our study can not agree on the grade\nto give a piece of student work and are often individually inconsistent,\nsuggesting that the idea of a ``gold standard'' of human grading might be\nflawed. This highlights that a shared rubric alone is not enough to ensure\nconsistency, and other aspects such as assessor training and alternative\ngrading practices should be explored to improve the consistency of human\ngrading further when grading programming assignments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.12967v3", "cate": "cs.CY", "date": "2024-09-02", "updated": "2025-07-23", "AI": {"title_translation": "人类在批改编程作业时的一致性如何？", "tldr": "本研究发现人类在批改编程作业时，不同评分者之间难以达成一致，且个体评分者也常出现不一致，表明仅靠共享评分标准不足以保证评分一致性。", "motivation": "为学生提供一致的总结性评估至关重要，因为成绩会影响他们的学业进展和未来职业前景。然而，大型班级通常由多名评估者批改，这增加了评分不一致的风险。", "method": "研究招募了28名参与者，每人批改40份CS1入门Java编程作业，涵盖正确性、代码优雅性、可读性和文档。这些作业分为两批，每批20份。28名参与者被分成7组，每组4人，每组批改相同的40份作业，以调查评估者群体的一致性。在第二批20份作业中，研究复制了第一批中的一份作业，以分析个体评估者的内部一致性。", "result": "研究结果表明，人类评分者在对学生作品打分时无法达成一致，并且个体评分者也常常前后不一致。这表明“黄金标准”的人工评分理念可能存在缺陷。", "conclusion": "仅靠共享的评分标准不足以确保评分一致性。为了进一步提高编程作业人工评分的一致性，应探索评估者培训和替代评分实践等其他方面。", "translation": "为学生提供一致的总结性评估至关重要，因为他们获得的成绩会影响他们在大学的学业进展和未来的职业前景。虽然小班级通常由单一评估者（如模块/班级负责人）进行评估，但大班级通常由多名评估者（通常是助教）进行评估，这增加了评分不一致的风险。\n为了调查人类在编程作业评分方面的一致性，我们邀请了28名参与者，每人批改40份CS1入门Java作业，并对正确性、代码优雅性、可读性和文档提供分数和反馈；这40份作业被分为两批，每批20份。28名参与者被分为七组，每组四人（每组批改相同的40份作业），以便我们调查评估者群体的一致性。在第二批20份作业中，我们复制了第一批中的一份作业，以分析个体评估者的内部一致性。\n我们的结果表明，本研究中的人类评分者无法就学生作品的评分达成一致，并且个体评分者也常常前后不一致，这表明“黄金标准”的人工评分理念可能存在缺陷。这突出表明，仅靠共享的评分标准不足以确保一致性，应探索评估者培训和替代评分实践等其他方面，以进一步提高编程作业人工评分的一致性。", "summary": "本研究旨在调查人类在批改编程作业时的一致性。28名参与者批改了40份Java作业，并对正确性、代码优雅性、可读性和文档进行评分和反馈。研究通过分组评估和复制作业的方式，分析了评估者群体和个体评估者的一致性。结果显示，人类评分者之间难以达成一致，且个体评分者也常出现内部不一致。这表明仅依靠共享评分标准不足以保证一致性，强调了评估者培训和替代评分实践的重要性。", "keywords": "编程作业评分, 评分一致性, 人工评估, CS1, 评估者培训", "comments": "这项研究揭示了编程作业人工评分中普遍存在的不一致性，挑战了“黄金标准”评分的假设。其创新之处在于通过实验设计，同时考察了群体间和个体内部的评分一致性。研究结果具有重要的实践意义，为教育机构改进评分流程提供了明确的方向，即除了共享评分标准外，还需加强评估者培训和探索新的评分方法，以提高评估的公平性和可靠性。"}}
{"id": "2507.16969", "title": "LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models", "authors": ["Shilong Zhao", "Fei Sun", "Kaike Zhang", "Shaoling Jing", "Du Su", "Zhichao Shi", "Zhiyi Yin", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16969v1", "summary": "Recent studies have demonstrated the vulnerability of sequential recommender\nsystems to Model Extraction Attacks (MEAs). MEAs collect responses from\nrecommender systems to replicate their functionality, enabling unauthorized\ndeployments and posing critical privacy and security risks. Black-box attacks\nin prior MEAs are ineffective at exposing recommender system vulnerabilities\ndue to random sampling in data selection, which leads to misaligned synthetic\nand real-world distributions. To overcome this limitation, we propose LLM4MEA,\na novel model extraction method that leverages Large Language Models (LLMs) as\nhuman-like rankers to generate data. It generates data through interactions\nbetween the LLM ranker and target recommender system. In each interaction, the\nLLM ranker analyzes historical interactions to understand user behavior, and\nselects items from recommendations with consistent preferences to extend the\ninteraction history, which serves as training data for MEA. Extensive\nexperiments demonstrate that LLM4MEA significantly outperforms existing\napproaches in data quality and attack performance, reducing the divergence\nbetween synthetic and real-world data by up to 64.98% and improving MEA\nperformance by 44.82% on average. From a defensive perspective, we propose a\nsimple yet effective defense strategy and identify key hyperparameters of\nrecommender systems that can mitigate the risk of MEAs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16969v1", "cate": "cs.IR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "LLM4MEA：基于大型语言模型对序列推荐系统进行无数据模型提取攻击", "tldr": "LLM4MEA提出了一种利用大型语言模型作为人类排名器来生成数据的新型模型提取攻击方法，显著提高了对序列推荐系统的攻击性能和数据质量，并提出了防御策略。", "motivation": "现有模型提取攻击（MEA）在黑盒场景下对序列推荐系统无效，因为它们采用随机采样导致生成数据与真实世界数据分布不匹配，从而无法有效暴露系统漏洞。", "method": "LLM4MEA利用大型语言模型（LLM）作为类人排名器来生成数据。LLM排名器与目标推荐系统交互，分析历史交互以理解用户行为，并从推荐中选择具有一致偏好的项目来扩展交互历史，这些扩展的交互历史作为MEA的训练数据。", "result": "LLM4MEA在数据质量和攻击性能上显著优于现有方法，将合成数据与真实世界数据之间的差异减少了高达64.98%，平均将MEA性能提高了44.82%。此外，还提出了一种简单有效的防御策略。", "conclusion": "LLM4MEA通过利用大型语言模型生成高质量数据，成功克服了现有模型提取攻击的局限性，显著提高了攻击效果，并为防御此类攻击提供了新的思路。", "translation": "最近的研究表明，序列推荐系统容易受到模型提取攻击（MEA）的影响。MEA通过收集推荐系统的响应来复制其功能，从而实现未经授权的部署，并带来严重隐私和安全风险。先前MEA中的黑盒攻击由于数据选择中的随机采样，导致合成数据与真实世界分布不匹配，因此在暴露推荐系统漏洞方面效果不佳。为了克服这一限制，我们提出了LLM4MEA，这是一种新颖的模型提取方法，它利用大型语言模型（LLM）作为类人排名器来生成数据。它通过LLM排名器与目标推荐系统之间的交互来生成数据。在每次交互中，LLM排名器分析历史交互以理解用户行为，并从具有一致偏好的推荐中选择项目以扩展交互历史，这作为MEA的训练数据。大量实验表明，LLM4MEA在数据质量和攻击性能方面显著优于现有方法，将合成数据与真实世界数据之间的差异减少了高达64.98%，平均将MEA性能提高了44.82%。从防御角度来看，我们提出了一种简单而有效的防御策略，并确定了可以减轻MEA风险的推荐系统关键超参数。", "summary": "LLM4MEA是一种新颖的无数据模型提取攻击方法，专为序列推荐系统设计。它利用大型语言模型（LLM）作为智能数据生成器，通过模拟用户行为与目标推荐系统交互，以克服现有黑盒攻击中数据分布不匹配的问题。实验证明，LLM4MEA在数据质量和攻击效率上均显著优于现有方法，并能有效减少合成数据与真实数据之间的差异。此外，该研究还提出了一种防御策略及关键超参数以减轻此类攻击的风险。", "keywords": "模型提取攻击, 序列推荐系统, 大型语言模型, 无数据攻击, 黑盒攻击", "comments": "本文提出了一种创新的模型提取攻击方法，利用LLM的强大理解和生成能力来克服传统攻击中数据采样和分布对齐的挑战。其“无数据”的特性尤其引人注目，因为它绕过了对真实训练数据的依赖。这项工作不仅揭示了序列推荐系统在模型安全方面的潜在脆弱性，也为未来防御机制的设计提供了宝贵的见解，特别是通过识别可缓解风险的推荐系统超参数。"}}
{"id": "2507.16933", "title": "SiLQ: Simple Large Language Model Quantization-Aware Training", "authors": ["Steven K. Esser", "Jeffrey L. McKinstry", "Deepika Bablani", "Rathinakumar Appuswamy", "Dharmendra S. Modha"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures", "url": "http://arxiv.org/abs/2507.16933v1", "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.", "comment": "12 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.16933v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "SiLQ: 简单大型语言模型量化感知训练", "tldr": "SiLQ是一种简单的量化感知训练方法，能够以极小的额外训练成本显著提升大型语言模型量化后的性能。", "motivation": "大型语言模型（LLMs）可以通过量化来减少推理延迟、模型大小和能耗，从而以更低的成本提供更好的用户体验。然而，挑战在于如何在合理的时间内提供精度损失最小的量化模型，特别是避免使用与专用推理加速器不兼容的机制。", "method": "本文提出了一种简单、端到端的量化感知训练（QAT）方法，其在总模型训练预算增加不到0.1%的情况下，在多个现代基准测试中，无论是基础模型还是指令模型变体，都大幅超越了领先的已发表量化方法。该方法易于推广到不同的模型架构，可应用于激活、缓存和权重，并且除了量化本身之外，不需要向模型引入额外的操作。", "result": "SiLQ方法在总模型训练预算增加不到0.1%的情况下，在多个现代基准测试中，无论是基础模型还是指令模型变体，都大幅超越了领先的已发表量化方法。该方法能够轻松推广到不同的模型架构，并可应用于激活、缓存和权重。", "conclusion": "SiLQ提供了一种高效且高性能的量化感知训练方法，它以极低的额外成本显著提高了大型语言模型量化后的性能，并具有良好的通用性和兼容性，解决了现有量化方法的挑战。", "translation": "大型语言模型可以通过量化来减少推理时间延迟、模型大小和能耗，从而以更低的成本提供更好的用户体验。一个挑战是如何在合理的时间内提供精度损失最小的量化模型，特别是如何在不要求与专用推理加速器不兼容的机制的情况下做到这一点。在此，我们展示了一种简单、端到端的量化感知训练方法，该方法在总模型训练预算增加不到0.1%的情况下，在多个现代基准测试中，无论是基础模型还是指令模型变体，都大幅超越了领先的已发表量化方法。该方法易于推广到不同的模型架构，可应用于激活、缓存和权重，并且除了量化本身之外，不需要向模型引入额外的操作。", "summary": "本文提出了一种名为SiLQ的简单量化感知训练（QAT）方法，旨在解决大型语言模型（LLMs）量化后精度损失的问题。SiLQ以极低的额外训练成本（小于0.1%）在多个基准测试中显著优于现有领先的量化方法。该方法具有良好的通用性，适用于不同模型架构，并能应用于激活、缓存和权重，且无需引入额外的操作，使其与专用推理加速器兼容。", "keywords": "大型语言模型, 量化感知训练, SiLQ, 模型量化, 性能优化", "comments": "SiLQ的创新之处在于其“简单”和“高效”，以极小的训练成本实现了大幅度的性能提升，解决了LLM量化中的精度损失痛点。其通用性和对硬件兼容性的强调也增加了其实用价值。"}}
{"id": "2507.17131", "title": "Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance", "authors": ["Yufei He", "Ruoyu Li", "Alex Chen", "Yue Liu", "Yulin Chen", "Yuan Sui", "Cheng Chen", "Yi Zhu", "Luca Luo", "Frank Yang", "Bryan Hooi"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17131v1", "summary": "Large language model (LLM) agents often struggle in environments where rules\nand required domain knowledge frequently change, such as regulatory compliance\nand user risk screening. Current approaches, like offline fine-tuning and\nstandard prompting, are insufficient because they cannot effectively adapt to\nnew knowledge during actual operation. To address this limitation, we propose\nthe Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework\ndesigned specifically to continuously learn updated domain knowledge at test\ntime. ARIA assesses its own uncertainty through structured self-dialogue,\nproactively identifying knowledge gaps and requesting targeted explanations or\ncorrections from human experts. It then systematically updates an internal,\ntimestamped knowledge repository with provided human guidance, detecting and\nresolving conflicting or outdated knowledge through comparisons and\nclarification queries. We evaluate ARIA on the realistic customer due diligence\nname screening task on TikTok Pay, alongside publicly available dynamic\nknowledge tasks. Results demonstrate significant improvements in adaptability\nand accuracy compared to baselines using standard offline fine-tuning and\nexisting self-improving agents. ARIA is deployed within TikTok Pay serving over\n150 million monthly active users, confirming its practicality and effectiveness\nfor operational use in rapidly evolving environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17131v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使自改进智能体能够在测试时通过人机交互指导进行学习", "tldr": "ARIA是一种LLM智能体框架，通过自我对话和人类专家指导，在测试时持续学习动态知识，显著提高了在快速变化环境中的适应性和准确性。", "motivation": "大型语言模型(LLM)智能体在规则和所需领域知识频繁变化的环境中表现不佳，现有方法无法在实际操作中有效适应新知识。", "method": "提出自适应反射交互式智能体(ARIA)框架，通过结构化自我对话评估不确定性，识别知识差距，并向人类专家请求解释或纠正。ARIA系统地更新内部时间戳知识库，并通过比较和澄清查询检测并解决冲突或过时的知识。", "result": "ARIA在TikTok Pay的客户尽职调查姓名筛选任务以及公开动态知识任务上进行了评估。结果表明，与使用标准离线微调和现有自改进智能体作为基线相比，ARIA在适应性和准确性方面有显著提升。ARIA已部署在TikTok Pay，服务超过1.5亿月活跃用户。", "conclusion": "ARIA框架通过在测试时整合人类指导和自我修正机制，有效解决了LLM智能体在动态环境中持续学习和适应的挑战，并在实际应用中展现出实用性和有效性。", "translation": "大型语言模型（LLM）智能体在规则和所需领域知识频繁变化的环境中常常表现不佳，例如监管合规和用户风险筛查。当前的方法，如离线微调和标准提示，不足以应对，因为它们无法在实际操作中有效地适应新知识。为了解决这一限制，我们提出了自适应反射交互式智能体（ARIA），这是一个专门设计用于在测试时持续学习更新领域知识的LLM智能体框架。ARIA通过结构化自我对话评估自身的不确定性，主动识别知识差距并请求人类专家提供有针对性的解释或纠正。然后，它系统地使用提供的人类指导更新内部带时间戳的知识库，通过比较和澄清查询检测并解决冲突或过时的知识。我们在TikTok Pay上真实的客户尽职调查姓名筛选任务以及公开可用的动态知识任务上评估了ARIA。结果表明，与使用标准离线微调和现有自改进智能体作为基线相比，ARIA在适应性和准确性方面有显著改进。ARIA已部署在TikTok Pay，服务超过1.5亿月活跃用户，证实了其在快速演变环境中操作使用的实用性和有效性。", "summary": "本文提出了自适应反射交互式智能体（ARIA），一个旨在解决大型语言模型（LLM）智能体在动态环境中适应性不足问题的框架。ARIA通过自我对话识别知识空白，并主动向人类专家请求指导，然后更新其内部知识库。实验结果显示，ARIA在适应性和准确性上显著优于现有方法，并在TikTok Pay的实际部署中验证了其在快速变化环境中的有效性和实用性。", "keywords": "LLM智能体, 自改进, 测试时学习, 人机交互, 知识更新", "comments": "ARIA的创新之处在于其结合了自我反思和人机交互的持续学习机制，使得LLM智能体能够实时适应不断变化的外部知识。其在TikTok Pay的实际部署证明了其在工业级应用中的鲁棒性和有效性，为解决LLM在动态环境中的知识更新问题提供了有力的解决方案。该方法对于需要高适应性和准确性的领域具有重要意义。"}}
{"id": "2412.16209", "title": "Challenges learning from imbalanced data using tree-based models: Prevalence estimates systematically depend on hyperparameters and can be upwardly biased", "authors": ["Nathan Phelps", "Daniel J. Lizotte", "Douglas G. Woolford"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.16209v3", "summary": "Imbalanced binary classification problems arise in many fields of study. When\nusing machine learning models for these problems, it is common to subsample the\nmajority class (i.e., undersampling) to create a (more) balanced dataset for\nmodel training. This biases the model's predictions because the model learns\nfrom a dataset that does not follow the same data generating process as new\ndata. One way of accounting for this bias is to analytically map the resulting\npredictions to new values based on the sampling rate for the majority class,\nwhich was used to create the training dataset. While this approach may work\nwell for some machine learning models, we show that calibrating a random forest\nthis way has unintended negative consequences, including prevalence estimates\nthat can be upwardly biased. These prevalence estimates depend on both i) the\nnumber of predictors considered at each split in the random forest; and ii) the\nsampling rate used. We explain the former using known properties of random\nforests and analytical calibration. However, in investigating the latter issue,\nwe made a surprising discovery - contrary to the widespread belief that\ndecision trees are biased towards the majority class, they actually can be\nbiased towards the minority class.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.16209v3", "cate": "cs.LG", "date": "2024-12-17", "updated": "2025-07-23", "AI": {"title_translation": "使用基于树的模型从不平衡数据中学习的挑战：流行率估计系统地依赖于超参数并可能向上偏倚", "tldr": "在使用树模型处理不平衡数据时，通过下采样校准会导致流行率估计向上偏倚，且决策树可能偏向少数类。", "motivation": "针对不平衡二元分类问题，常见的多数类下采样方法会导致模型预测偏差。尽管有分析校准方法来纠正此偏差，但本文指出该方法对随机森林等模型可能产生负面影响。", "method": "本文研究了使用下采样和分析校准方法处理不平衡数据时，随机森林模型的表现。具体分析了流行率估计如何受到随机森林中分割时考虑的预测器数量和采样率的影响。", "result": "研究发现，以这种方式校准随机森林会导致意想不到的负面后果，包括流行率估计可能向上偏倚。这些估计值依赖于随机森林的超参数（每次分割考虑的预测器数量）和采样率。此外，研究发现决策树实际上可能偏向少数类。", "conclusion": "使用下采样和分析校准方法处理不平衡数据时，基于树的模型（特别是随机森林）的流行率估计可能不准确，且受超参数和采样率影响。决策树的偏差方向也可能与传统认知不同。", "translation": "不平衡二元分类问题出现在许多研究领域。当使用机器学习模型处理这些问题时，通常对多数类进行下采样以创建（更）平衡的数据集用于模型训练。这会使模型的预测产生偏差，因为模型从不遵循与新数据相同的数据生成过程的数据集中学习。解决这种偏差的一种方法是，根据用于创建训练数据集的多数类采样率，将结果预测分析性地映射到新值。虽然这种方法可能适用于某些机器学习模型，但我们表明，以这种方式校准随机森林会产生意想不到的负面后果，包括流行率估计可能向上偏倚。这些流行率估计取决于：i）随机森林中每次分割考虑的预测器数量；和 ii）使用的采样率。我们使用随机森林和分析校准的已知特性来解释前者。然而，在调查后一个问题时，我们有了一个惊人的发现——与决策树偏向多数类的普遍看法相反，它们实际上可能偏向少数类。", "summary": "本研究探讨了在使用基于树的模型处理不平衡二元分类问题时，通过对多数类进行下采样并进行分析校准所带来的挑战。研究发现，这种方法可能导致随机森林的流行率估计向上偏倚，并且这些估计值系统地依赖于模型超参数和采样率。此外，研究还揭示了一个出人意料的发现：与普遍认知相反，决策树可能偏向少数类而非多数类。", "keywords": "不平衡数据, 基于树的模型, 随机森林, 下采样, 流行率估计, 偏差", "comments": "这篇论文揭示了处理不平衡数据时一种常见方法（下采样与校准）的潜在缺陷，特别是对于基于树的模型。关于决策树可能偏向少数类的发现尤其具有洞察力，挑战了现有假设，有助于更深入地理解这些模型。"}}
{"id": "2507.16861", "title": "Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection", "authors": ["Xiang Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16861v1", "summary": "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)\nrepresentation is crucial for enhancing 3D perception capabilities of\nautonomous vehicles. However, current methods are often affected by\nmisalignment between camera and LiDAR features. This misalignment leads to\ninaccurate depth supervision in camera branch and erroneous fusion during\ncross-modal feature aggregation. The root cause of this misalignment lies in\nprojection errors, stemming from minor extrinsic calibration inaccuracies and\nrolling shutter effect of LiDAR during vehicle motion. In this work, our key\ninsight is that these projection errors are predominantly concentrated at\nobject-background boundaries, which are readily identified by 2D detectors.\nBased on this, our main motivation is to utilize 2D object priors to pre-align\ncross-modal features before fusion. To address local misalignment, we propose\nPrior Guided Depth Calibration (PGDC), which leverages 2D priors to correct\nlocal misalignment and preserve correct cross-modal feature pairs. To resolve\nglobal misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)\nto process calibrated results from PGDC, suppressing noise and explicitly\nenhancing sharp transitions at object-background boundaries. To effectively\nutilize these transition-aware depth representations, we incorporate Structural\nGuidance Depth Modulator (SGDM), using a gated attention mechanism to\nefficiently fuse aligned depth and image features. Our proposed method achieves\nstate-of-the-art performance on nuScenes validation dataset, with its mAP and\nNDS reaching 71.5% and 73.6% respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16861v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "融合前瞻：2D引导的跨模态对齐实现鲁棒3D检测", "tldr": "该论文提出了一种2D引导的跨模态对齐方法，通过解决LiDAR和相机特征之间的局部和全局错位问题，提高了3D检测的性能，并在nuScenes数据集上取得了最先进的结果。", "motivation": "当前将LiDAR和相机输入整合到统一的鸟瞰图（BEV）表示中的方法，常受到相机和LiDAR特征之间错位的影响。这种错位导致相机分支中深度监督不准确和跨模态特征聚合过程中融合错误。错位的根本原因在于投影误差，源于轻微的外部校准不准确和车辆运动期间LiDAR的滚动快门效应，这些误差主要集中在物体-背景边界处。", "method": "该方法利用2D物体先验知识在融合前预对齐跨模态特征。为解决局部错位，提出了先验引导深度校准（PGDC），利用2D先验来纠正局部错位并保留正确的跨模态特征对。为解决全局错位，引入了不连续感知几何融合（DAGF），用于处理PGDC校准后的结果，抑制噪声并明确增强物体-背景边界处的急剧过渡。为了有效利用这些过渡感知深度表示，结合了结构引导深度调制器（SGDM），使用门控注意力机制高效融合对齐的深度和图像特征。", "result": "该方法在nuScenes验证数据集上取得了最先进的性能，其mAP和NDS分别达到71.5%和73.6%。", "conclusion": "Not mentioned in abstract", "translation": "将LiDAR和相机输入整合到统一的鸟瞰图（BEV）表示中，对于增强自动驾驶汽车的3D感知能力至关重要。然而，当前方法经常受到相机和LiDAR特征之间错位的影响。这种错位导致相机分支中深度监督不准确和跨模态特征聚合过程中融合错误。这种错位的根本原因在于投影误差，源于轻微的外部校准不准确和车辆运动期间LiDAR的滚动快门效应。在这项工作中，我们的关键见解是这些投影误差主要集中在物体-背景边界，这些边界很容易被2D检测器识别。基于此，我们的主要动机是利用2D物体先验在融合前预对齐跨模态特征。为解决局部错位，我们提出了先验引导深度校准（PGDC），它利用2D先验来纠正局部错位并保留正确的跨模态特征对。为解决全局错位，我们引入了不连续感知几何融合（DAGF）来处理PGDC校准后的结果，抑制噪声并明确增强物体-背景边界处的急剧过渡。为了有效利用这些过渡感知深度表示，我们结合了结构引导深度调制器（SGDM），使用门控注意力机制高效融合对齐的深度和图像特征。我们提出的方法在nuScenes验证数据集上取得了最先进的性能，其mAP和NDS分别达到71.5%和73.6%。", "summary": "该论文针对自动驾驶中LiDAR和相机融合时存在的特征错位问题，提出了一种2D引导的跨模态对齐方法。研究发现错位主要源于投影误差且集中在物体-背景边界。为此，作者提出了PGDC来纠正局部错位，DAGF来处理全局错位并增强边界，以及SGDM来有效融合校准后的深度和图像特征。实验结果表明，该方法在nuScenes数据集上实现了最先进的3D检测性能。", "keywords": "3D检测, 跨模态融合, 特征对齐, LiDAR-Camera, 深度校准", "comments": "该论文的创新点在于其“融合前瞻”的理念，即在特征融合之前进行2D引导的跨模态对齐，有效地解决了LiDAR和相机融合中常见的特征错位问题。通过利用2D检测器识别出的物体-背景边界作为关键区域来校正投影误差，具有很强的实用性和针对性。提出的PGDC、DAGF和SGDM模块协同工作，分别解决了局部和全局的对齐问题，并优化了特征融合过程，展现了精巧的设计。在nuScenes数据集上取得的SOTA结果也证明了其方法的有效性。"}}
{"id": "2507.17269", "title": "MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate Cancer Lesion Region Segmentation", "authors": ["Zhengcheng Lin", "Zuobin Ying", "Zhenyu Li", "Zhenyu Liu", "Jian Lu", "Weiping Ding"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17269v1", "summary": "Early diagnosis and accurate identification of lesion location and\nprogression in prostate cancer (PCa) are critical for assisting clinicians in\nformulating effective treatment strategies. However, due to the high semantic\nhomogeneity between lesion and non-lesion areas, existing medical image\nsegmentation methods often struggle to accurately comprehend lesion semantics,\nresulting in the problem of semantic confusion. To address this challenge, we\npropose a novel Pixel Anchor Module, which guides the model to discover a\nsparse set of feature anchors that serve to capture and interpret global\ncontextual information. This mechanism enhances the model's nonlinear\nrepresentation capacity and improves segmentation accuracy within lesion\nregions. Moreover, we design a self-attention-based Top_k selection strategy to\nfurther refine the identification of these feature anchors, and incorporate a\nfocal loss function to mitigate class imbalance, thereby facilitating more\nprecise semantic interpretation across diverse regions. Our method achieves\nstate-of-the-art performance on the PI-CAI dataset, demonstrating 69.73% IoU\nand 74.32% Dice scores, and significantly improving prostate cancer lesion\ndetection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17269v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MyGO：明确你的目标，避免前列腺癌病灶区域分割中的语义混淆", "tldr": "MyGO是一种新的图像分割方法，通过像素锚模块和自注意力机制解决前列腺癌病灶分割中的语义混淆问题，并在PI-CAI数据集上实现了SOTA性能。", "motivation": "前列腺癌的早期诊断和病灶定位对治疗至关重要。然而，现有医学图像分割方法由于病灶和非病灶区域之间的高度语义同质性，难以准确理解病灶语义，导致语义混淆问题。", "method": "提出了一种新颖的像素锚模块（Pixel Anchor Module），引导模型发现稀疏的特征锚点来捕获和解释全局上下文信息。此外，设计了一种基于自注意力的Top_k选择策略来进一步细化特征锚点的识别，并引入焦点损失函数以减轻类别不平衡问题。", "result": "该方法在PI-CAI数据集上实现了最先进的性能，IoU达到69.73%，Dice分数达到74.32%，显著提高了前列腺癌病灶检测。", "conclusion": "通过引入像素锚模块和改进的选择策略，MyGO有效解决了前列腺癌病灶分割中的语义混淆问题，并显著提升了分割精度。", "translation": "前列腺癌（PCa）的早期诊断和病灶位置及进展的准确识别对于协助临床医生制定有效的治疗策略至关重要。然而，由于病灶和非病灶区域之间的高度语义同质性，现有的医学图像分割方法往往难以准确理解病灶语义，导致语义混淆问题。为了应对这一挑战，我们提出了一种新颖的像素锚模块（Pixel Anchor Module），它引导模型发现一组稀疏的特征锚点，用于捕获和解释全局上下文信息。这种机制增强了模型的非线性表示能力，并提高了病灶区域内的分割精度。此外，我们设计了一种基于自注意力的Top_k选择策略，以进一步细化这些特征锚点的识别，并结合焦点损失函数以减轻类别不平衡，从而促进跨不同区域更精确的语义解释。我们的方法在PI-CAI数据集上实现了最先进的性能，IoU达到69.73%，Dice分数达到74.32%，并显著改善了前列腺癌病灶检测。", "summary": "本文提出MyGO方法，旨在解决前列腺癌病灶区域分割中因语义同质性导致的语义混淆问题。该方法引入像素锚模块以捕获全局上下文信息，并通过基于自注意力的Top_k选择策略和焦点损失函数进一步优化特征锚点识别和类别平衡。MyGO在PI-CAI数据集上取得了领先的分割性能，有效提升了前列腺癌病灶的检测准确性。", "keywords": "前列腺癌, 图像分割, 语义混淆, 像素锚模块, 自注意力", "comments": "MyGO的创新点在于提出了像素锚模块，这是一种新颖的机制，通过稀疏特征锚点来解决医学图像分割中常见的语义混淆问题，特别是在病灶和非病灶区域高度相似的情况下。结合自注意力机制和焦点损失函数，该方法在实际应用中具有显著的潜力，能够提高前列腺癌诊断的准确性。"}}
{"id": "2507.17089", "title": "IONext: Unlocking the Next Era of Inertial Odometry", "authors": ["Shanshan Zhang", "Siyue Wang", "Tianshui Wen", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17089v1", "summary": "Researchers have increasingly adopted Transformer-based models for inertial\nodometry. While Transformers excel at modeling long-range dependencies, their\nlimited sensitivity to local, fine-grained motion variations and lack of\ninherent inductive biases often hinder localization accuracy and\ngeneralization. Recent studies have shown that incorporating large-kernel\nconvolutions and Transformer-inspired architectural designs into CNN can\neffectively expand the receptive field, thereby improving global motion\nperception. Motivated by these insights, we propose a novel CNN-based module\ncalled the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures\nboth global motion patterns and local, fine-grained motion features from\ndynamic inputs. This module dynamically generates selective weights based on\nthe input, enabling efficient multi-scale feature aggregation. To further\nimprove temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),\nwhich selectively extracts representative and task-relevant motion features in\nthe temporal domain. This unit addresses the limitations of temporal modeling\nobserved in existing CNN approaches. Built upon DADM and STGU, we present a new\nCNN-based inertial odometry backbone, named Next Era of Inertial Odometry\n(IONext). Extensive experiments on six public datasets demonstrate that IONext\nconsistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based\nmethods. For instance, on the RNIN dataset, IONext reduces the average ATE by\n10% and the average RTE by 12% compared to the representative model iMOT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17089v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "IONext：开启惯性里程计的新纪元", "tldr": "IONext提出了一种基于CNN的惯性里程计新模型，通过引入DADM和STGU模块，有效解决了现有Transformer模型对局部运动不敏感以及CNN模型在时序建模上的局限性，并在多个公开数据集上表现优于SOTA方法。", "motivation": "研究人员越来越多地采用基于Transformer的模型进行惯性里程计，但Transformer对局部精细运动变化的敏感性有限，并且缺乏固有的归纳偏置，这阻碍了定位精度和泛化能力。最近的研究表明，将大核卷积和受Transformer启发的架构设计融入CNN可以有效扩大感受野，从而改善全局运动感知。受这些启发，本文提出了一种新方法。", "method": "本文提出了一种名为双翼自适应动态混合器（DADM）的新型基于CNN的模块，它能自适应地从动态输入中捕获全局运动模式和局部精细运动特征，并通过动态生成选择性权重实现高效的多尺度特征聚合。为进一步改善时序建模，引入了时空门控单元（STGU），该单元在时域中选择性地提取具有代表性和任务相关的运动特征，解决了现有CNN方法在时序建模上的局限性。基于DADM和STGU，本文提出了一个名为“惯性里程计新纪元”（IONext）的新型基于CNN的惯性里程计骨干网络。", "result": "在六个公开数据集上的大量实验表明，IONext始终优于最先进的（SOTA）基于Transformer和CNN的方法。例如，在RNIN数据集上，与代表性模型iMOT相比，IONext将平均ATE（绝对轨迹误差）降低了10%，平均RTE（相对轨迹误差）降低了12%。", "conclusion": "IONext通过其创新的DADM和STGU模块，有效克服了当前惯性里程计领域中Transformer和CNN方法的局限性，并在多个数据集上取得了领先的性能，预示着惯性里程计进入了一个新时代。", "translation": "研究人员越来越多地采用基于Transformer的模型进行惯性里程计。尽管Transformer擅长建模长程依赖，但其对局部、精细运动变化的敏感性有限以及缺乏固有的归纳偏置，常常阻碍定位精度和泛化能力。最近的研究表明，将大核卷积和受Transformer启发的架构设计融入CNN可以有效扩大感受野，从而改善全局运动感知。受这些见解的启发，我们提出了一种名为双翼自适应动态混合器（DADM）的新型基于CNN的模块，它能自适应地从动态输入中捕获全局运动模式和局部、精细运动特征。该模块根据输入动态生成选择性权重，实现高效的多尺度特征聚合。为进一步改善时序建模，我们引入了时空门控单元（STGU），该单元在时域中选择性地提取具有代表性和任务相关的运动特征。该单元解决了现有CNN方法中观察到的时序建模局限性。基于DADM和STGU，我们提出了一个名为“惯性里程计新纪元”（IONext）的新型基于CNN的惯性里程计骨干网络。在六个公开数据集上的大量实验表明，IONext始终优于最先进的（SOTA）基于Transformer和CNN的方法。例如，在RNIN数据集上，IONext与代表性模型iMOT相比，平均ATE降低了10%，平均RTE降低了12%。", "summary": "本文提出了一种名为IONext的新型CNN惯性里程计骨干网络，旨在解决现有Transformer模型对局部运动不敏感以及CNN模型在时序建模上的不足。IONext的核心是双翼自适应动态混合器（DADM）和时空门控单元（STGU）。DADM自适应捕获全局和局部运动特征，而STGU则增强了时域特征提取。实验结果表明，IONext在六个公共数据集上均优于最先进的Transformer和CNN方法，例如在RNIN数据集上分别将ATE和RTE降低了10%和12%。", "keywords": "惯性里程计, CNN, Transformer, DADM, STGU", "comments": "IONext的创新之处在于其独特地结合了CNN的优势，并通过DADM和STGU模块解决了现有Transformer模型在局部运动感知上的不足以及传统CNN在时序建模上的局限性。这种方法不仅充分利用了CNN对局部特征的敏感性，还通过自适应混合器和门控单元实现了对全局和时序依赖性的有效建模，使其在惯性里程计领域取得了显著的性能提升，为未来研究提供了新的方向。"}}
{"id": "2507.17186", "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17186v1", "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17186v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "FinGAIA：一个用于评估金融领域AI智能体的端到端基准", "tldr": "本文介绍了FinGAIA，首个用于评估金融领域AI智能体的端到端基准。该基准包含407项任务，涵盖七个金融子领域。对10个主流AI智能体的评估显示，表现最佳的ChatGPT准确率为48.9%，远低于金融专家，且揭示了多种常见失败模式，为未来研究指明了方向。", "motivation": "尽管AI智能体在自动化复杂任务方面展现出巨大潜力，但它们在金融领域的多步骤、多工具协作能力尚未得到充分探索。", "method": "本文引入了FinGAIA，一个端到端基准，旨在评估AI智能体在金融领域的实际能力。FinGAIA包含407个精心设计的任务，涵盖证券、基金、银行、保险、期货、信托和资产管理七个主要金融子领域，并按三个层次的任务深度（基本业务分析、资产决策支持、战略风险管理）组织。研究在零样本设置下评估了10个主流AI智能体。", "result": "表现最佳的智能体ChatGPT实现了48.9%的整体准确率，虽然优于非专业人士，但仍比金融专家低35个百分点以上。错误分析揭示了五种常见的失败模式，包括跨模态对齐缺陷、金融术语偏差、操作流程意识障碍等。", "conclusion": "这些失败模式为未来的研究指明了关键方向。这项工作提供了首个与金融领域紧密相关的智能体基准，旨在客观评估并促进该关键领域智能体的发展。", "translation": "AI智能体的蓬勃发展为自动化各个领域的复杂任务带来了前所未有的机遇。然而，它们在金融领域的多步骤、多工具协作能力仍未得到充分探索。本文介绍了FinGAIA，一个旨在评估AI智能体在金融领域实际能力的端到端基准。FinGAIA包含407个精心设计的任务，涵盖证券、基金、银行、保险、期货、信托和资产管理七个主要金融子领域。这些任务被组织成三个层次的场景深度：基本业务分析、资产决策支持和战略风险管理。我们在零样本设置下评估了10个主流AI智能体。表现最佳的智能体ChatGPT实现了48.9%的整体准确率，虽然优于非专业人士，但仍比金融专家低35个百分点以上。错误分析揭示了五种常见的失败模式：跨模态对齐缺陷、金融术语偏差、操作流程意识障碍等。这些模式为未来的研究指明了关键方向。我们的工作提供了首个与金融领域紧密相关的智能体基准，旨在客观评估并促进该关键领域智能体的发展。部分数据可在https://github.com/SUFE-AIFLM-Lab/FinGAIA获取。", "summary": "本文提出了FinGAIA，一个针对金融领域AI智能体进行端到端评估的基准。FinGAIA包含407项任务，覆盖七个金融子领域和三个深度层次。对10个主流AI智能体的评估显示，最佳表现的ChatGPT准确率为48.9%，远低于金融专家。研究还识别出多种失败模式，为未来研究提供了方向。该工作是首个金融领域AI智能体基准，旨在推动该领域智能体的发展。", "keywords": "AI智能体, 金融领域, 基准测试, FinGAIA, 错误分析", "comments": "这项工作的创新之处在于首次为金融领域的AI智能体提供了一个全面的端到端评估基准。其重要性在于揭示了当前AI智能体在金融复杂任务处理上的不足，特别是与人类专家的巨大差距，并通过错误分析为未来研究指明了具体的改进方向，对推动金融AI发展具有指导意义。"}}
{"id": "2507.17373", "title": "SFUOD: Source-Free Unknown Object Detection", "authors": ["Keon-Hee Park", "Seun-An Choe", "Gyeong-Moon Park"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.17373v1", "summary": "Source-free object detection adapts a detector pre-trained on a source domain\nto an unlabeled target domain without requiring access to labeled source data.\nWhile this setting is practical as it eliminates the need for the source\ndataset during domain adaptation, it operates under the restrictive assumption\nthat only pre-defined objects from the source domain exist in the target\ndomain. This closed-set setting prevents the detector from detecting undefined\nobjects. To ease this assumption, we propose Source-Free Unknown Object\nDetection (SFUOD), a novel scenario which enables the detector to not only\nrecognize known objects but also detect undefined objects as unknown objects.\nTo this end, we propose CollaPAUL (Collaborative tuning and Principal\nAxis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning\nenhances knowledge adaptation by integrating target-dependent knowledge from\nthe auxiliary encoder with source-dependent knowledge from the pre-trained\ndetector through a cross-domain attention mechanism. Additionally, principal\naxes-based unknown labeling assigns pseudo-labels to unknown objects by\nestimating objectness via principal axes projection and confidence scores from\nmodel predictions. The proposed CollaPAUL achieves state-of-the-art\nperformances on SFUOD benchmarks, and extensive experiments validate its\neffectiveness.", "comment": "This paper has been accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17373v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SFUOD：无源未知目标检测", "tldr": "SFUOD提出了一个新颖的无源未知目标检测场景，并提出CollaPAUL框架，使其能够在不访问源数据的情况下，检测目标域中的已知和未知对象，并取得了最先进的性能。", "motivation": "现有的无源目标检测方法假设目标域中只存在源域中预定义的对象，这种封闭集设置限制了检测器无法检测未定义的对象。为了解决这一限制，本文提出了无源未知目标检测（SFUOD）的新场景，使检测器不仅能识别已知对象，还能检测未定义对象作为未知对象。", "method": "本文提出了名为CollaPAUL（协同调优和基于主轴的未知标签）的新框架用于SFUOD。CollaPAUL通过跨域注意力机制，将辅助编码器中的目标依赖知识与预训练检测器中的源依赖知识相结合，进行协同调优以增强知识适应性。此外，它通过主轴投影估计目标性以及模型预测的置信度分数，为未知对象分配伪标签。", "result": "所提出的CollaPAUL在SFUOD基准测试中取得了最先进的性能，并通过大量实验验证了其有效性。", "conclusion": "CollaPAUL框架能够有效地解决无源未知目标检测问题，使其在不访问源数据的情况下，能够同时识别已知目标并检测未知目标，并取得了优异的性能。", "translation": "无源目标检测旨在将预训练的检测器从源域适应到未标记的目标域，而无需访问标记的源数据。虽然这种设置是实用的，因为它在域适应过程中消除了对源数据集的需求，但它在限制性假设下运行，即目标域中只存在源域中预定义的对象。这种封闭集设置阻止了检测器检测未定义的对象。为了放宽这一假设，我们提出了无源未知目标检测（SFUOD），这是一种新颖的场景，它使检测器不仅能够识别已知对象，而且能够将未定义对象检测为未知对象。为此，我们提出了CollaPAUL（协同调优和基于主轴的未知标签），一个用于SFUOD的新颖框架。协同调优通过跨域注意力机制将来自辅助编码器的目标依赖知识与来自预训练检测器的源依赖知识相结合，从而增强知识适应性。此外，基于主轴的未知标签通过主轴投影估计目标性和模型预测的置信度分数，为未知对象分配伪标签。所提出的CollaPAUL在SFUOD基准测试中取得了最先进的性能，并且广泛的实验验证了其有效性。", "summary": "本文针对现有无源目标检测无法识别未知对象的限制，提出了SFUOD（无源未知目标检测）新场景。为实现这一目标，作者提出了CollaPAUL框架，该框架通过协同调优（结合源域和目标域知识）和基于主轴的未知标签（为未知对象分配伪标签）两大核心机制，使检测器能够在不访问源数据的情况下，有效识别已知对象并检测未知对象。实验结果表明，CollaPAUL在SFUOD基准上达到了最先进的性能。", "keywords": "无源目标检测, 未知目标检测, 域适应, 伪标签, 跨域注意力", "comments": "该论文的创新点在于提出了“无源未知目标检测”这一新颖且更具实用性的场景，解决了现有无源目标检测仅限于封闭集设定的局限性。CollaPAUL框架中的协同调优和基于主轴的未知标签方法，巧妙地在无源数据访问的约束下，实现了对未知对象的识别能力，对实际应用具有重要意义。"}}
{"id": "2503.06894", "title": "A Deep Learning Approach for Augmenting Perceptional Understanding of Histopathology Images", "authors": ["Xiaoqian Hu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by International Conference on Semantic & Natural Language Processing (SNLP 2025)", "url": "http://arxiv.org/abs/2503.06894v3", "summary": "In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.", "comment": "Accepted by International Conference on Semantic & Natural Language\n  Processing (SNLP 2025)", "pdf_url": "http://arxiv.org/pdf/2503.06894v3", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-23", "AI": {"title_translation": "一种用于增强组织病理学图像感知理解的深度学习方法", "tldr": "本文提出一种结合ViT和GPT-2的多模态模型，通过生成图像描述来增强医疗专业人员对组织病理学图像的感知和诊断能力。", "motivation": "数字化技术在增强人类健康、认知和感知方面取得了显著进展，尤其是在计算病理学领域。该研究旨在通过增强组织病理学图像的分析来提高诊断准确性，并辅助医疗专业人员。", "method": "提出了一种结合Vision Transformers (ViT) 和 GPT-2 的多模态模型，用于组织病理学图像描述。该模型在包含临床和学术资源密集图像描述的Arch-Dataset上进行微调，以捕捉病理图像的复杂性。", "result": "该模型能够生成准确、符合上下文的图像描述，增强医疗专业人员的认知能力，从而实现更高效的疾病分类、分割和检测。它还增强了对图像中可能被忽视的细微病理特征的感知，从而提高了诊断准确性。", "conclusion": "该方法展示了数字技术在医学图像分析中增强人类认知能力的潜力，为实现更个性化和准确的医疗结果迈出了重要一步。", "translation": "近年来，数字技术在增强人类健康、认知和感知方面取得了显著进展，尤其是在计算病理学领域。本文提出了一种新颖的方法，通过利用结合视觉Transformer (ViT) 和 GPT-2 的多模态模型进行图像描述，来增强组织病理学图像的分析。该模型在专门的Arch-Dataset上进行微调，该数据集包含来自临床和学术资源的密集图像描述，以捕捉病理图像的复杂性，例如组织形态、染色变异和病理状况。通过生成准确、符合上下文的描述，该模型增强了医疗专业人员的认知能力，从而实现更高效的疾病分类、分割和检测。该模型增强了对图像中可能被忽视的细微病理特征的感知，从而提高了诊断准确性。我们的方法展示了数字技术在医学图像分析中增强人类认知能力的潜力，为实现更个性化和准确的医疗结果迈出了重要一步。", "summary": "本文介绍了一种深度学习方法，通过结合Vision Transformers (ViT) 和 GPT-2 构建的多模态模型，对组织病理学图像进行图像描述。该模型在专门数据集上微调，能够生成准确的上下文描述，从而增强医疗专业人员的认知能力，提高疾病分类、分割和检测的效率，并提升对细微病理特征的感知及诊断准确性。", "keywords": "组织病理学图像, 深度学习, 图像描述, 多模态模型, Vision Transformer, GPT-2", "comments": "该研究的创新点在于将多模态深度学习（结合ViT和GPT-2）应用于组织病理学图像的自动描述，以辅助人类专家进行诊断。通过生成描述性文本，它不仅提供了图像内容，还增强了对复杂病理特征的理解，对于提高诊断效率和准确性具有重要意义。此方法为计算病理学领域提供了一个新颖的工具，有望推动更个性化的医疗实践。"}}
{"id": "2410.19384", "title": "Learning Neural Strategy-Proof Matching Mechanism from Examples", "authors": ["Ryota Maruo", "Koh Takeuchi", "Hisashi Kashima"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.19384v2", "summary": "Designing two-sided matching mechanisms is challenging when practical demands\nfor matching outcomes are difficult to formalize and the designed mechanism\nmust satisfy theoretical conditions. To address this, prior work has proposed a\nframework that learns a matching mechanism from examples, using a parameterized\nfamily that satisfies properties such as stability. However, despite its\nusefulness, this framework does not guarantee strategy-proofness (SP), and\ncannot handle varying numbers of agents or incorporate publicly available\ncontextual information about agents, both of which are crucial in real-world\napplications. In this paper, we propose a new parametrized family of matching\nmechanisms that always satisfy strategy-proofness, are applicable for an\narbitrary number of agents, and deal with public contextual information of\nagents, based on the serial dictatorship (SD). This family is represented by\nNeuralSD, a novel neural network architecture based on SD, where agent rankings\nin SD are treated as learnable parameters computed from agents' contexts using\nan attention-based sub-network. To enable learning, we introduce tensor serial\ndictatorship (TSD), a differentiable relaxation of SD using tensor operations.\nThis allows NeuralSD to be trained end-to-end from example matchings while\nsatisfying SP. We conducted experiments to learn a matching mechanism from\nmatching examples while satisfying SP. We demonstrated that our method\noutperformed baselines in predicting matchings and on several metrics for\ngoodness of matching outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.19384v2", "cate": "cs.AI", "date": "2024-10-25", "updated": "2025-07-23", "AI": {"title_translation": "从示例中学习神经策略证明匹配机制", "tldr": "本文提出了一种基于序列独裁（SD）的新型神经网络架构NeuralSD，通过引入可微分的张量序列独裁（TSD），实现了从示例中端到端学习策略证明的匹配机制，解决了现有方法在策略证明性、可变智能体数量和上下文信息处理方面的局限。", "motivation": "现有学习匹配机制的框架无法保证策略证明性（SP），不能处理可变数量的智能体，也无法整合公开的上下文信息，而这些在实际应用中至关重要。设计双边匹配机制本身就因实际需求难以形式化且需满足理论条件而充满挑战。", "method": "提出了一种基于序列独裁（SD）的新型参数化匹配机制家族NeuralSD，它是一种新颖的神经网络架构。该架构将SD中的智能体排名视为可学习参数，通过一个基于注意力的子网络从智能体上下文信息中计算得出。为了实现学习，引入了张量序列独裁（TSD），这是SD的一种可微分松弛形式，利用张量操作，从而使NeuralSD能够从示例匹配中进行端到端训练，同时满足策略证明性。", "result": "实验证明，所提出的方法在预测匹配结果方面优于基线，并在匹配结果的多个优劣度量上表现出色。", "conclusion": "本文成功提出了一种新型的、基于神经网络的策略证明匹配机制NeuralSD，它能够从示例中学习，同时保证策略证明性、处理可变数量的智能体并融入上下文信息，并在实验中取得了优越的性能。", "translation": "设计双边匹配机制具有挑战性，因为匹配结果的实际需求难以形式化，且所设计的机制必须满足理论条件。为了解决这个问题，先前的工作提出了一个从示例中学习匹配机制的框架，该框架使用满足稳定性等属性的参数化族。然而，尽管其有用，该框架不保证策略证明性（SP），也无法处理可变数量的智能体或整合关于智能体的公开上下文信息，这两者在实际应用中都至关重要。在本文中，我们提出了一种新的参数化匹配机制家族，它总是满足策略证明性，适用于任意数量的智能体，并处理智能体的公开上下文信息，该家族基于序列独裁（SD）。这个家族由NeuralSD表示，这是一种基于SD的新型神经网络架构，其中SD中的智能体排名被视为可学习参数，使用基于注意力的子网络从智能体上下文中计算得出。为了实现学习，我们引入了张量序列独裁（TSD），这是SD的一种可微分松弛形式，使用张量操作。这使得NeuralSD能够从示例匹配中进行端到端训练，同时满足SP。我们进行了实验，从匹配示例中学习匹配机制，同时满足SP。我们证明了我们的方法在预测匹配结果和匹配结果优劣的几个指标上优于基线。", "summary": "本文提出了一种名为NeuralSD的新型神经网络架构，用于从示例中学习策略证明的双边匹配机制。针对现有方法在策略证明性、可变智能体数量和上下文信息处理方面的不足，NeuralSD基于序列独裁（SD），通过注意力子网络从智能体上下文学习排名，并利用可微分的张量序列独裁（TSD）实现端到端训练，同时保证策略证明性。实验结果表明，该方法在匹配预测和匹配质量方面均优于基线。", "keywords": "策略证明性, 匹配机制, 神经网络, 序列独裁, 张量序列独裁", "comments": "这篇论文通过将经典的序列独裁机制与现代神经网络技术结合，提出了一种新颖且实用的方法来解决双边匹配机制设计中的关键挑战。其创新点在于引入了可微分的张量序列独裁（TSD）来使整个系统可端到端训练，同时保证了重要的策略证明性属性。这对于需要从数据中学习复杂偏好但又要求理论保证的实际应用具有重要意义。"}}
{"id": "2501.06438", "title": "Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning", "authors": ["Maomao Li", "Lijian Lin", "Yunfei Liu", "Ye Zhu", "Yu Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2501.06438v2", "summary": "This paper presents Qffusion, a dual-frame-guided framework for portrait\nvideo editing. Specifically, we consider a design principle of ``animation for\nediting'', and train Qffusion as a general animation framework from two still\nreference images while we can use it for portrait video editing easily by\napplying modified start and end frames as references during inference.\nLeveraging the powerful generative power of Stable Diffusion, we propose a\nQuadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which\narranges the latent codes of two reference images and that of four facial\nconditions into a four-grid fashion, separately. Then, we fuse features of\nthese two modalities and use self-attention for both appearance and temporal\nlearning, where representations at different times are jointly modeled under\nQGA. Our Qffusion can achieve stable video editing without additional networks\nor complex training stages, where only the input format of Stable Diffusion is\nmodified. Further, we propose a Quadrant-grid Propagation (QGP) inference\nstrategy, which enjoys a unique advantage on stable arbitrary-length video\ngeneration by processing reference and condition frames recursively. Through\nextensive experiments, Qffusion consistently outperforms state-of-the-art\ntechniques on portrait video editing. Project page:\nhttps://qffusion.github.io/page/.", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2501.06438v2", "cate": "cs.CV", "date": "2025-01-11", "updated": "2025-07-23", "AI": {"title_translation": "Qffusion：基于象限网格注意力学习的可控肖像视频编辑", "tldr": "Qffusion是一个双帧引导框架，利用Stable Diffusion通过象限网格注意力学习实现可控的肖像视频编辑，无需额外网络或复杂训练即可超越现有技术。", "motivation": "旨在解决肖像视频编辑中稳定性和可控性的挑战，并提出一种通用动画框架，可轻松应用于视频编辑。", "method": "本文提出了Qffusion框架，一个双帧引导的肖像视频编辑系统。它采用“动画用于编辑”的设计原则，从两张参考图像训练为通用动画框架。核心方法包括象限网格排列（QGA）方案，该方案将两张参考图像和四种面部条件的潜在代码分别排列成四格形式。然后，融合这两种模态的特征，并使用自注意力机制进行外观和时间学习。此外，还提出了象限网格传播（QGP）推理策略，通过递归处理参考帧和条件帧，实现稳定任意长度的视频生成，且仅需修改Stable Diffusion的输入格式。", "result": "Qffusion无需额外的网络或复杂的训练阶段，即可实现稳定的视频编辑。通过广泛的实验，Qffusion在肖像视频编辑方面始终优于现有最先进的技术。", "conclusion": "Qffusion通过其创新的象限网格注意力学习和传播策略，为可控肖像视频编辑提供了一个高效且性能优越的解决方案，显著超越了现有技术，且无需复杂的训练或额外网络。", "translation": "本文介绍了Qffusion，一个用于肖像视频编辑的双帧引导框架。具体来说，我们考虑了“动画用于编辑”的设计原则，将Qffusion训练为一个通用的动画框架，从两张静态参考图像出发，而在推理时通过应用修改后的起始和结束帧作为参考，可以轻松地将其用于肖像视频编辑。利用Stable Diffusion强大的生成能力，我们提出了一种象限网格排列（QGA）方案，用于潜在码的重新排列，该方案将两张参考图像和四种面部条件的潜在码分别排列成四格形式。然后，我们融合这两种模态的特征，并使用自注意力机制进行外观和时间学习，其中在QGA下共同建模不同时间点的表示。我们的Qffusion无需额外的网络或复杂的训练阶段，即可实现稳定的视频编辑，仅修改了Stable Diffusion的输入格式。此外，我们提出了一种象限网格传播（QGP）推理策略，该策略通过递归处理参考帧和条件帧，在稳定任意长度视频生成方面具有独特的优势。通过广泛的实验，Qffusion在肖像视频编辑方面始终优于现有最先进的技术。项目页面：https://qffusion.github.io/page/。", "summary": "Qffusion是一个创新的双帧引导框架，专为可控肖像视频编辑设计。它遵循“动画用于编辑”的原则，利用Stable Diffusion的强大能力，通过独特的象限网格排列（QGA）方案，将参考图像和面部条件的潜在代码进行四格化处理。该框架通过自注意力机制学习外观和时间特征，无需额外网络或复杂训练即可实现稳定编辑。此外，其象限网格传播（QGP）推理策略确保了任意长度视频的稳定生成。实验证明，Qffusion在肖像视频编辑性能上超越了现有最先进的技术。", "keywords": "肖像视频编辑, Qffusion, 象限网格注意力, Stable Diffusion, 视频生成", "comments": "Qffusion的创新之处在于其将“动画用于编辑”的设计理念与Stable Diffusion相结合，通过独特的象限网格排列（QGA）和传播（QGP）策略，解决了视频编辑中的稳定性和可控性难题。其无需额外网络或复杂训练即可实现SOTA性能，显示了其高效性和实用性，为未来基于扩散模型的视频编辑提供了新的思路。"}}
{"id": "2507.17165", "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations", "authors": ["Taher A. Ghaleb", "Dulina Rathnayake"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at the 41st IEEE International Conference on Software Maintenance and Evolution 2025 (ICSME'25)", "url": "http://arxiv.org/abs/2507.17165v1", "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support.", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "pdf_url": "http://arxiv.org/pdf/2507.17165v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "LLM能编写CI吗？一项关于GitHub Actions配置自动生成的研究", "tldr": "本研究初步评估了LLM在从自然语言描述生成GitHub Actions配置方面的能力，发现尽管相似度可达69%，但完美匹配度仅为3%，且存在结构和上下文错误，揭示了LLM在此任务中的局限性。", "motivation": "持续集成（CI）服务（如GitHub Actions）的YAML配置编写繁琐且易错。尽管大型语言模型（LLM）在软件工程任务自动化方面应用日益广泛，但其生成CI配置的能力尚未得到充分探索。", "method": "本研究进行了一项初步评估，测试了六种LLM（包括GPT-4o、Llama、Gemma等通用模型和GPT-4.1、Code Llama、CodeGemma等代码预训练模型），从自然语言描述生成GitHub Actions配置。研究引入了首个从GitHub Actions文档构建的、包含描述与对应YAML配置的标注数据集。评估采用零样本提示，并分析了生成结果的相似度和完美匹配度。", "result": "零样本提示的生成结果与真实值相似度最高达69%，但完美匹配度仅为3%。代码预训练模型在基于YAML的CI任务中表现略逊于通用模型。对GPT-4o输出的分析揭示了诸如步骤缺失或重命名、描述误解以及不必要添加等问题，这些问题可能影响结构和上下文的正确性。", "conclusion": "研究结果表明，LLM在CI配置生成方面存在局限性，其生成质量与可执行CI配置所需的精确度之间存在差距。本研究为改进LLM与配置语言的对齐提供了见解，并为CI自动化和工具支持的未来工作提供了指导。", "translation": "持续集成（CI）服务，例如GitHub Actions，要求开发者编写基于YAML的配置，这可能既繁琐又容易出错。尽管大型语言模型（LLM）在自动化软件工程任务方面的使用日益增多，但它们生成CI配置的能力仍未得到充分探索。本文提出了一项初步研究，评估了六种LLM从自然语言描述生成GitHub Actions配置的能力。我们评估了三种通用基础模型（GPT-4o、Llama和Gemma）和三种代码预训练模型（GPT-4.1、Code Llama和CodeGemma）。我们还引入了首个此类标注数据集，该数据集从GitHub Actions文档构建，将描述与相应的最佳实践YAML配置配对。零样本提示实现了与真实值高达69%的相似度，但完美匹配度仅为3%。代码预训练模型在基于YAML的CI任务中表现略逊于通用模型，揭示了LLM在CI配置生成方面的局限性。对GPT-4o输出的分析揭示了诸如步骤缺失或重命名、描述误解以及不必要添加等问题，这些问题可能影响结构和上下文的正确性，表明生成质量与可执行CI配置所需的精确度之间存在差距。我们的研究为改进LLM与配置语言的对齐提供了见解，并为CI自动化和工具支持的未来工作提供了指导。", "summary": "本研究评估了大型语言模型（LLM）从自然语言描述自动生成GitHub Actions配置的能力。研究测试了六种LLM（包括通用型和代码预训练型），并构建了一个新的标注数据集。结果显示，LLM在零样本提示下生成配置的相似度最高可达69%，但完美匹配度仅为3%，且存在结构和语义错误，表明LLM在此类精确配置生成任务中仍有显著局限性，为未来CI自动化工具的开发提供了方向。", "keywords": "LLMs, CI, GitHub Actions, 配置生成, 自然语言处理", "comments": "这项研究首次系统地评估了LLM在CI配置自动生成方面的能力，填补了该领域的研究空白。其创新之处在于构建了首个GitHub Actions配置的标注数据集，为后续研究提供了基础。研究结果清晰地指出了当前LLM在生成精确、可执行配置方面的局限性，特别是对结构和上下文正确性的要求。这对于理解LLM在特定领域（如配置语言）的应用挑战及其未来改进方向具有重要意义。"}}
{"id": "2507.17450", "title": "Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition", "authors": ["Arsha Niksa", "Hooman Zare", "Ali Shahrabi", "Hanieh Hatami", "Mohammadreza Razvan"], "categories": ["cs.LG", "55N31"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17450v1", "summary": "We present a topological pipeline for automated multiclass emotion\nrecognition from eye-tracking data. Delay embeddings of gaze trajectories are\nanalyzed using persistent homology. From the resulting persistence diagrams, we\nextract shape-based features such as mean persistence, maximum persistence, and\nentropy. A random forest classifier trained on these features achieves up to\n$75.6\\%$ accuracy on four emotion classes, which are the quadrants the\nCircumplex Model of Affect. The results demonstrate that persistence diagram\ngeometry effectively encodes discriminative gaze dynamics, suggesting a\npromising topological approach for affective computing and human behavior\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17450v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "眼动中的持久模式：一种拓扑学方法用于情绪识别", "tldr": "本文提出了一种基于拓扑学（持久同调）的方法，通过分析眼动轨迹来自动识别多类别情绪，并在四种情绪类别上取得了75.6%的准确率。", "motivation": "本研究旨在从眼动数据中实现自动化的多类别情绪识别。", "method": "研究采用拓扑学方法，对注视轨迹进行延迟嵌入，并使用持久同调进行分析。从得到的持久图谱中提取了形状特征（如平均持久度、最大持久度、熵），然后使用随机森林分类器对这些特征进行训练。", "result": "基于所提特征训练的随机森林分类器在四种情绪类别（情绪环状模型的四个象限）上达到了高达75.6%的准确率。", "conclusion": "结果表明，持久图谱的几何结构能有效编码判别性注视动态，这预示着一种有前景的拓扑学方法可用于情感计算和人类行为分析。", "translation": "我们提出了一种用于从眼动追踪数据中自动进行多类别情绪识别的拓扑学流程。通过持久同调分析注视轨迹的延迟嵌入。从得到的持久图谱中，我们提取了基于形状的特征，如平均持久度、最大持久度及熵。一个基于这些特征训练的随机森林分类器在四种情绪类别（情绪环状模型的四个象限）上取得了高达75.6%的准确率。结果表明，持久图谱的几何结构能有效编码判别性注视动态，这预示着一种有前景的拓扑学方法可用于情感计算和人类行为分析。", "summary": "本研究提出了一种创新的拓扑学方法，利用持久同调分析眼动追踪数据中的注视轨迹，以实现多类别情绪的自动识别。通过从持久图谱中提取形状特征，并使用随机森林分类器进行训练，该方法在四种情绪类别上实现了75.6%的准确率。研究结果强调了持久图谱在编码判别性注视动态方面的有效性，为情感计算和人类行为分析提供了一种有前景的拓扑学途径。", "keywords": "眼动, 情绪识别, 拓扑学, 持久同调, 注视动态", "comments": "这项研究的创新之处在于将拓扑数据分析（尤其是持久同调）应用于眼动数据，以识别情绪模式。这种方法能够捕捉眼动轨迹中更深层次的结构和模式，而非仅仅是简单的统计量。其重要性在于为情感计算和人类行为分析提供了一种新的、潜在更鲁棒的工具。该方法显示出在眼动数据中发现持久模式的潜力，为理解复杂的人类行为提供了新的视角。"}}
{"id": "2507.16044", "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16044v2", "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16044v2", "cate": "cs.SE", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "使REST API具备代理能力：从OpenAPI到用于工具增强型LLM的模型上下文协议服务器", "tldr": "本文提出AutoMCP，一个将OpenAPI规范自动编译为LLM工具集成所需的Model Context Protocol (MCP) 服务器的工具，显著提高了将现有REST API转化为LLM代理可用工具的效率和成功率。", "motivation": "大型语言模型（LLM）正从被动文本生成器发展为能够调用外部工具的活跃代理。Anthropic的Model Context Protocol (MCP) 是工具集成的重要标准，但手动构建MCP服务器过程繁琐、重复且成本高昂，导致其实际采用率较低，因此急需自动化解决方案来简化集成。", "method": "论文首先分析了MCP的采用趋势，揭示了手动构建服务器的低效性。为解决此问题，提出了AutoMCP，这是一个能从OpenAPI 2.0/3.0规范自动生成完整MCP服务器实现的编译器，包括Schema注册和认证处理。AutoMCP通过解析REST API定义来自动化这一过程。", "result": "AutoMCP在包含5066个端点的50个真实世界API上进行了评估。初始成功率为76.5%，主要归因于OpenAPI契约中的不一致或遗漏。经过平均每个API 19行的微小规范修改后，成功率显著提升至99.9%。研究还分析了MCP的采用情况，量化了手动服务器开发的成本，并贡献了一个包含5066个可调用工具的语料库以及修复常见规范缺陷的见解。", "conclusion": "尽管OpenAPI规范存在一些质量问题，但它能够实现近乎完整的Model Context Protocol (MCP) 服务器自动化，这显著降低了将现有REST API转换为大型语言模型（LLM）代理可用工具的集成成本。", "translation": "大型语言模型（LLM）正在从被动文本生成器演变为能够调用外部工具的活跃代理。为了支持这一转变，可扩展的工具集成协议至关重要。Anthropic于2024年推出的模型上下文协议（MCP）提供了一种模式驱动的标准，用于动态工具发现和调用。然而，构建MCP服务器仍然是手动且重复的，需要开发人员编写粘合代码、处理身份验证并手动配置模式，这复制了MCP旨在消除的大部分集成工作。\n本文研究了MCP服务器的构建是否可以有意义地自动化。我们首先分析了采用趋势：在发布后六个月内创建的22,000多个带有MCP标签的GitHub存储库中，不到5%包含服务器，这些项目通常很小，由单一维护者维护，并且以重复的脚手架代码为主。为了解决这一差距，我们提出了AutoMCP，一个从OpenAPI 2.0/3.0规范生成MCP服务器的编译器。AutoMCP解析REST API定义并生成完整的服务器实现，包括模式注册和身份验证处理。\n我们在涵盖10多个领域的50个真实世界API（跨越5,066个端点）上评估了AutoMCP。从1,023个工具调用的分层样本中，76.5%的调用在开箱即用状态下成功。手动故障分析揭示了五个常见问题，所有这些问题都归因于OpenAPI契约中的不一致或遗漏。经过平均每个API 19行的少量规范修改后，AutoMCP实现了99.9%的成功率。\n我们的发现（i）分析了MCP的采用情况并量化了手动服务器开发的成本，（ii）证明了OpenAPI规范尽管存在质量问题，但能够实现近乎完整的MCP服务器自动化，并且（iii）贡献了一个包含5,066个可调用工具的语料库以及修复常见规范缺陷的见解。", "summary": "本文提出了AutoMCP，一个将OpenAPI 2.0/3.0规范自动编译为Model Context Protocol (MCP) 服务器的工具，旨在解决当前手动构建MCP服务器的繁琐和低效率问题。通过对22,000多个GitHub仓库的分析，揭示了MCP服务器低采用率的原因在于其重复的手动集成工作。AutoMCP在50个真实API上进行了评估，最初实现76.5%的成功率，在对OpenAPI规范进行少量修复后，成功率提升至99.9%，证明了OpenAPI能有效支持MCP服务器的自动化生成，并显著降低了将REST API集成到LLM代理中的成本。", "keywords": "Model Context Protocol, OpenAPI, LLM, REST API, 工具自动化", "comments": "这篇论文的创新点在于提出了AutoMCP，一个将现有OpenAPI规范自动转换为LLM可用的MCP服务器的编译器，极大地简化了工具集成流程。其重要性体现在它解决了LLM作为代理调用外部工具时面临的效率瓶颈，通过自动化降低了开发成本和技术门槛，有望加速REST API向LLM工具的转化和应用。论文还通过实证分析量化了手动开发的成本，并提供了修复常见OpenAPI规范问题的实用见解，具有很高的实践价值。"}}
{"id": "2507.17026", "title": "The surprising strength of weak classifiers for validating neural posterior estimates", "authors": ["Vansh Bansal", "Tianyu Chen", "James G. Scott"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17026v1", "summary": "Neural Posterior Estimation (NPE) has emerged as a powerful approach for\namortized Bayesian inference when the true posterior $p(\\theta \\mid y)$ is\nintractable or difficult to sample. But evaluating the accuracy of neural\nposterior estimates remains challenging, with existing methods suffering from\nmajor limitations. One appealing and widely used method is the classifier\ntwo-sample test (C2ST), where a classifier is trained to distinguish samples\nfrom the true posterior $p(\\theta \\mid y)$ versus the learned NPE approximation\n$q(\\theta \\mid y)$. Yet despite the appealing simplicity of the C2ST, its\ntheoretical and practical reliability depend upon having access to a\nnear-Bayes-optimal classifier -- a requirement that is rarely met and, at best,\ndifficult to verify. Thus a major open question is: can a weak classifier still\nbe useful for neural posterior validation? We show that the answer is yes.\nBuilding on the work of Hu and Lei, we present several key results for a\nconformal variant of the C2ST, which converts any trained classifier's scores\n-- even those of weak or over-fitted models -- into exact finite-sample\np-values. We establish two key theoretical properties of the conformal C2ST:\n(i) finite-sample Type-I error control, and (ii) non-trivial power that\ndegrades gently in tandem with the error of the trained classifier. The upshot\nis that even weak, biased, or overfit classifiers can still yield powerful and\nreliable tests. Empirically, the Conformal C2ST outperforms classical\ndiscriminative tests across a wide range of benchmarks. These results reveal\nthe under appreciated strength of weak classifiers for validating neural\nposterior estimates, establishing the conformal C2ST as a practical,\ntheoretically grounded diagnostic for modern simulation-based inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17026v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "弱分类器在验证神经后验估计中的惊人强度", "tldr": "传统方法评估神经后验估计（NPE）的准确性存在挑战，特别是对分类器要求高。本文提出了一种共形C2ST方法，即使是弱分类器也能提供可靠的NPE验证，具有理论保证和经验优势。", "motivation": "评估神经后验估计（NPE）的准确性是挑战性的，现有方法（如分类器双样本检验C2ST）依赖于难以获得的近贝叶斯最优分类器，这限制了其实用性和可靠性。因此，存在一个主要开放问题：一个弱分类器是否仍然可用于神经后验验证？", "method": "本文基于Hu和Lei的工作，提出了一种共形变体分类器双样本检验（Conformal C2ST）。该方法能将任何训练好的分类器（包括弱分类器或过拟合模型）的得分转换为精确的有限样本p值，从而进行神经后验验证。", "result": "理论上，共形C2ST具有有限样本I型错误控制，并且具有非平凡的检验力，其检验力随训练分类器误差的增加而缓慢下降。经验上，共形C2ST在广泛的基准测试中优于经典的判别性测试。这些结果表明，即使是弱、有偏或过拟合的分类器也能产生强大且可靠的检验。", "conclusion": "共形C2ST是一种实用且有理论基础的诊断工具，用于现代基于仿真的推断，它揭示了弱分类器在验证神经后验估计方面被低估的强大能力。", "translation": "神经后验估计（NPE）已成为当真实后验$p(\\theta \\mid y)$难以处理或难以采样时进行摊销贝叶斯推断的强大方法。但评估神经后验估计的准确性仍然具有挑战性，现有方法存在重大局限。一种吸引人且广泛使用的方法是分类器双样本检验（C2ST），其中训练一个分类器来区分来自真实后验$p(\\theta \\mid y)$的样本与学习到的NPE近似$q(\\theta \\mid y)$的样本。然而，尽管C2ST具有吸引人的简单性，其理论和实践可靠性却取决于能否获得一个接近贝叶斯最优的分类器——这个要求很少能满足，而且充其量也难以验证。因此，一个主要的开放问题是：一个弱分类器是否仍然可用于神经后验验证？我们表明答案是肯定的。基于Hu和Lei的工作，我们为C2ST的一个共形变体提出了几个关键结果，它将任何训练好的分类器的得分——即使是弱模型或过拟合模型的得分——转换为精确的有限样本p值。我们建立了共形C2ST的两个关键理论性质：（i）有限样本I型错误控制，以及（ii）非平凡的检验力，其随训练分类器误差的增加而缓慢下降。结果是，即使是弱的、有偏的或过拟合的分类器仍然可以产生强大而可靠的检验。在经验上，共形C2ST在广泛的基准测试中优于经典的判别性测试。这些结果揭示了弱分类器在验证神经后验估计方面被低估的强大能力，将共形C2ST确立为一种实用、有理论基础的现代基于仿真推断的诊断工具。", "summary": "本文针对神经后验估计（NPE）准确性评估中对强分类器的依赖问题，提出了一种名为共形C2ST的新方法。该方法基于共形预测原理，能将任意分类器（包括弱分类器或过拟合模型）的得分转化为精确的p值。研究证明共形C2ST在理论上能控制I型错误并保持有效的检验力，且在实践中表现优于传统方法，表明弱分类器也能有效验证NPE。", "keywords": "神经后验估计, 分类器双样本检验, 共形预测, 弱分类器, 贝叶斯推断", "comments": "这项工作创新性地解决了神经后验估计验证中对强分类器的高度依赖问题，通过引入共形预测思想，极大地拓宽了C2ST的适用性。其理论保证（I型错误控制和稳健的检验力）以及经验上的优越性，使其成为模拟推理领域一个重要的、更实用的诊断工具，对于贝叶斯推断的可靠性评估具有重要意义。"}}
{"id": "2505.22334", "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22334v2", "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22334v2", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-07-23", "AI": {"title_translation": "通过冷启动强化学习推进多模态推理", "tldr": "本文提出了一种两阶段方法，结合监督微调（SFT）作为冷启动和强化学习（RL），以提升多模态大语言模型（MLLMs）的推理能力，并在多个基准测试中取得了最先进的性能。", "motivation": "尽管“顿悟时刻”（通过反思进行自我纠正）通常被认为是强化学习的涌现特性，但作者首先发现这些模式在RL训练前就存在于多模态大语言模型（MLLMs）中，但与推理性能的提高不一定相关。因此，本研究的动机是探索一种有效的方法来增强多模态推理能力。", "method": "本文提出了一种两阶段方法来增强多模态推理：1) 使用结构化思维链推理模式进行监督微调（SFT）作为冷启动；2) 随后通过GRPO进行强化学习，以进一步完善这些能力。", "result": "该结合方法在具有挑战性的多模态推理基准测试中始终优于单独的SFT和单独的RL方法。在3B和7B规模的开源MLLMs中，取得了最先进的性能，其中7B模型在MathVista上从66.3%提高到73.4%，在We-Math上从62.9%提高到70.4%。3B模型也达到了与多个7B模型相当的性能。", "conclusion": "这项工作为构建先进的多模态推理模型提供了实用指导。", "translation": "大型语言模型（LLMs）的最新进展展示了令人印象深刻的思维链推理能力，其中强化学习（RL）在这一进展中发挥了关键作用。虽然“顿悟时刻”模式——即模型通过反思表现出自我纠正——通常被归因于RL的涌现特性，但我们首先证明这些模式在RL训练之前就存在于多模态LLMs（MLLMs）中，但可能不一定与推理性能的提高相关。基于这些见解，我们提出了一项关于通过两阶段方法增强多模态推理的全面研究：(1) 以结构化思维链推理模式进行监督微调（SFT）作为冷启动，随后 (2) 通过GRPO进行强化学习以进一步完善这些能力。我们广泛的实验表明，这种组合方法在具有挑战性的多模态推理基准测试中始终优于单独的SFT和单独的RL方法。由此产生的模型在3B和7B规模的开源MLLMs中均达到了最先进的性能，我们的7B模型比基础模型有显著改进（例如，MathVista上从66.3%提高到73.4%，We-Math上从62.9%提高到70.4%），我们的3B模型也达到了与多个7B模型相当的性能。总的来说，这项工作为构建先进的多模态推理模型提供了实用指导。我们的代码可在https://github.com/waltonfuture/RL-with-Cold-Start获取。", "summary": "本文研究了如何通过结合监督微调（SFT）和强化学习（RL）来提升多模态大语言模型（MLLMs）的推理能力。作者首先指出，“顿悟时刻”等自我纠正模式在RL训练前就已存在于MLLMs中。在此基础上，他们提出了一种两阶段方法：首先使用结构化思维链模式进行SFT作为“冷启动”，然后通过GRPO进行RL以进一步优化。实验结果表明，这种结合方法在多模态推理基准测试中显著优于单独使用SFT或RL的方法，并在开源MLLMs中达到了最先进的性能，为构建高级多模态推理模型提供了实用指导。", "keywords": "多模态推理, 强化学习, 冷启动, 大语言模型, 思维链", "comments": "本文的创新之处在于其两阶段的训练范式，特别是将监督微调作为“冷启动”阶段，为后续的强化学习奠定基础。此外，作者指出“顿悟时刻”在RL训练前就已存在于MLLMs中，这一发现对于理解模型行为和设计更有效的训练策略具有重要意义。该研究为提升多模态推理能力提供了实用的指导，其开源代码也促进了社区的进一步研究。"}}
{"id": "2507.16847", "title": "EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media", "authors": ["Ismail Hossain", "Sai Puppala", "Md Jahangir Alam", "Sajedul Talukder"], "categories": ["cs.SI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      We are submitting this paper to ICWSM 2026 conference on September 15th, 2025", "url": "http://arxiv.org/abs/2507.16847v1", "summary": "Social media platforms serve as a significant medium for sharing personal\nemotions, daily activities, and various life events, ensuring individuals stay\ninformed about the latest developments. From the initiation of an account,\nusers progressively expand their circle of friends or followers, engaging\nactively by posting, commenting, and sharing content. Over time, user behavior\non these platforms evolves, influenced by demographic attributes and the\nnetworks they form. In this study, we present a novel approach that leverages\nopen-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through\nprompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint\nembedding technique, to analyze and predict the evolution of user behavior on\nsocial media over their lifetime. Our experiments demonstrate the potential of\nthese models to forecast future stages of a user's social evolution, including\nnetwork changes, future connections, and shifts in user activities.\nExperimental results highlight the effectiveness of our approach, with GPT-2\nachieving the lowest perplexity (8.21) in a Cross-modal configuration,\noutperforming RoBERTa (9.11) and BERT, and underscoring the importance of\nleveraging Cross-modal configurations for superior performance. This approach\naddresses critical challenges in social media, such as friend recommendations\nand activity predictions, offering insights into the trajectory of user\nbehavior. By anticipating future interactions and activities, this research\naims to provide early warnings about potential negative outcomes, enabling\nusers to make informed decisions and mitigate risks in the long term.", "comment": "We are submitting this paper to ICWSM 2026 conference on September\n  15th, 2025", "pdf_url": "http://arxiv.org/pdf/2507.16847v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "EVOLVE-X：嵌入融合与语言提示用于社交媒体用户演变预测", "tldr": "EVOLVE-X提出了一种结合大型语言模型（Llama-3、Mistral-7B、Gemma-7B）的提示工程与联合嵌入技术（GPT-2、BERT、RoBERTa）的新方法，用于预测社交媒体用户行为的长期演变，并在实验中展现出有效性。", "motivation": "社交媒体用户行为随时间演变，受人口属性和社交网络影响。预测用户演变对于朋友推荐、活动预测以及提供潜在负面结果的早期预警至关重要。", "method": "本研究提出了一种新颖的方法，结合了通过提示工程使用的开源模型Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT，并与GPT-2、BERT和RoBERTa通过联合嵌入技术相结合。该方法旨在分析和预测社交媒体用户行为在生命周期内的演变。", "result": "实验证明了该模型预测用户社交演变未来阶段的潜力，包括网络变化、未来连接和用户活动转变。实验结果突出显示了该方法的有效性，其中GPT-2在跨模态配置中实现了最低的困惑度（8.21），优于RoBERTa（9.11）和BERT，并强调了利用跨模态配置获得卓越性能的重要性。", "conclusion": "EVOLVE-X通过结合语言提示和嵌入融合，成功预测了社交媒体用户的长期行为演变，为朋友推荐、活动预测以及风险缓解提供了有价值的见解。", "translation": "社交媒体平台是分享个人情感、日常活动和各种生活事件的重要媒介，确保个人随时了解最新动态。从账户创建开始，用户逐渐扩大他们的朋友圈或关注者，通过发布、评论和分享内容积极参与。随着时间的推移，这些平台上的用户行为会演变，受人口属性和他们形成的网络影响。在这项研究中，我们提出了一种新颖的方法，该方法利用开源模型Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT通过提示工程，并结合GPT-2、BERT和RoBERTa使用联合嵌入技术，来分析和预测社交媒体用户行为在其生命周期中的演变。我们的实验证明了这些模型预测用户社交演变未来阶段的潜力，包括网络变化、未来连接和用户活动转变。实验结果突出显示了我们方法的有效性，其中GPT-2在跨模态配置中实现了最低的困惑度（8.21），优于RoBERTa（9.11）和BERT，并强调了利用跨模态配置获得卓越性能的重要性。这种方法解决了社交媒体中的关键挑战，例如朋友推荐和活动预测，提供了对用户行为轨迹的深入见解。通过预测未来的互动和活动，这项研究旨在提供潜在负面结果的早期预警，使用户能够做出明智的决策并长期降低风险。", "summary": "EVOLVE-X提出了一种创新的方法，结合了大型语言模型（如Llama-3、Mistral-7B、Gemma-7B）的提示工程与预训练语言模型（GPT-2、BERT、RoBERTa）的联合嵌入技术，以预测社交媒体用户行为的长期演变。该研究旨在通过分析用户在社交媒体上的网络变化、未来连接和活动转变来提供早期预警和见解。实验结果表明，该方法能够有效预测用户社交演变的未来阶段，特别是GPT-2在跨模态配置中表现出卓越性能，强调了其在朋友推荐和活动预测等方面的应用潜力。", "keywords": "用户演变预测, 社交媒体, 嵌入融合, 语言提示, 跨模态", "comments": "该论文的创新点在于结合了大型语言模型的提示工程能力与传统预训练模型的联合嵌入技术，形成了一种新颖的跨模态方法来预测用户演变。其重要性在于能够为社交媒体平台提供用户行为的长期预测能力，从而在朋友推荐、活动预测以及潜在风险预警方面提供有价值的见解。"}}
{"id": "2507.16953", "title": "Fundamental limits of distributed covariance matrix estimation via a conditional strong data processing inequality", "authors": ["Mohammad Reza Rahmani", "Mohammad Hossein Yassaee", "Mohammad Reza Aref"], "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16953v1", "summary": "Estimating high-dimensional covariance matrices is a key task across many\nfields. This paper explores the theoretical limits of distributed covariance\nestimation in a feature-split setting, where communication between agents is\nconstrained. Specifically, we study a scenario in which multiple agents each\nobserve different components of i.i.d. samples drawn from a sub-Gaussian random\nvector. A central server seeks to estimate the complete covariance matrix using\na limited number of bits communicated by each agent. We obtain a nearly tight\nminimax lower bound for covariance matrix estimation under operator norm and\nFrobenius norm. Our main technical tool is a novel generalization of the strong\ndata processing inequality (SDPI), termed the Conditional Strong Data\nProcessing Inequality (C-SDPI) coefficient, introduced in this work. The C-SDPI\ncoefficient shares key properties such as tensorization with the conventional\nSDPI. Crucially, it quantifies the average contraction in a state-dependent\nchannel and can be significantly lower than the worst-case SDPI coefficient\nover the state input.\n  Utilizing the doubling trick of Geng-Nair and an operator Jensen inequality,\nwe compute this coefficient for Gaussian mixture channels. We then employ it to\nestablish minimax lower bounds on estimation error, capturing the trade-offs\namong sample size, communication cost, and data dimensionality. Building on\nthis, we present a nearly optimal estimation protocol whose sample and\ncommunication requirements match the lower bounds up to logarithmic factors.\nUnlike much of the existing literature, our framework does not assume infinite\nsamples or Gaussian distributions, making it broadly applicable. Finally, we\nextend our analysis to interactive protocols, showing interaction can\nsignificantly reduce communication requirements compared to non-interactive\nschemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16953v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "分布式协方差矩阵估计的基本极限：基于条件强数据处理不等式", "tldr": "本文通过引入条件强数据处理不等式（C-SDPI），探讨了通信受限的分布式协方差矩阵估计的理论极限，并提出了一个接近最优的估计协议。", "motivation": "在许多领域中，高维协方差矩阵的估计是一项关键任务。本文旨在探索在特征分裂设置下，通信受限的多智能体分布式协方差估计的理论极限。", "method": "本文引入了一种新型的强数据处理不等式（SDPI）的推广，称为条件强数据处理不等式（C-SDPI）系数。该系数通过Geng-Nair的加倍技巧和算子Jensen不等式计算，并用于建立估计误差的极小极大下界。在此基础上，提出了一个接近最优的估计协议，并将其分析扩展到交互式协议。", "result": "获得了在算子范数和Frobenius范数下协方差矩阵估计的接近紧致的极小极大下界。C-SDPI系数能够量化状态依赖信道中的平均收缩，并且显著低于最坏情况下的SDPI系数。提出的估计协议的样本和通信要求与下界匹配（至对数因子）。研究还表明，在交互式协议中，通信需求可以显著降低。", "conclusion": "本文成功地确定了分布式协方差矩阵估计的理论极限，并提供了一个在非高斯或有限样本设置下仍能保持广适性的接近最优的估计协议，同时揭示了交互式协议在降低通信需求方面的潜力。", "translation": "高维协方差矩阵的估计是许多领域的关键任务。本文探讨了在特征分裂设置下分布式协方差估计的理论极限，其中代理之间的通信受到限制。具体来说，我们研究了一个场景，其中多个代理各自观察来自次高斯随机向量的独立同分布样本的不同分量。中央服务器旨在利用每个代理通信的有限比特数来估计完整的协方差矩阵。我们获得了在算子范数和Frobenius范数下协方差矩阵估计的接近紧致的极小极大下界。我们的主要技术工具是本文引入的强数据处理不等式（SDPI）的一种新颖推广，称为条件强数据处理不等式（C-SDPI）系数。C-SDPI系数与传统SDPI共享关键属性，例如张量积特性。至关重要的是，它量化了状态依赖信道中的平均收缩，并且可以显著低于状态输入上最坏情况下的SDPI系数。\n利用Geng-Nair的加倍技巧和算子Jensen不等式，我们计算了高斯混合信道的该系数。然后我们利用它来建立估计误差的极小极大下界，捕获了样本大小、通信成本和数据维度之间的权衡。在此基础上，我们提出了一个接近最优的估计协议，其样本和通信要求与下界匹配（至对数因子）。与现有文献中的大部分工作不同，我们的框架不假设无限样本或高斯分布，这使其具有广泛的适用性。最后，我们将分析扩展到交互式协议，表明与非交互式方案相比，交互可以显著减少通信需求。", "summary": "本文研究了通信受限的分布式高维协方差矩阵估计的理论极限。通过引入一种新的技术工具——条件强数据处理不等式（C-SDPI）系数，作者推导出了协方差矩阵估计的极小极大下界，并分析了样本量、通信成本和数据维度之间的权衡。在此基础上，提出了一种接近最优的估计协议，其性能接近理论下限。该框架不依赖于无限样本或高斯分布假设，并且还探讨了交互式协议对通信需求的显著降低作用。", "keywords": "分布式协方差估计, 条件强数据处理不等式, 极小极大下界, 通信限制, 高维数据", "comments": "本文的创新点在于引入了条件强数据处理不等式（C-SDPI）作为分析分布式估计理论极限的关键工具，它能够更精确地量化状态依赖信道中的信息收缩。其重要性体现在为通信受限的分布式协方差矩阵估计提供了严格的理论下界，并设计了接近最优的协议。此外，该框架不依赖于无限样本或高斯分布的假设，使其在实际应用中具有更广泛的适用性。对交互式协议的分析也揭示了其在降低通信成本方面的潜力。"}}
{"id": "2507.17335", "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition", "authors": ["Guangzhu Xu", "Zhi Ke", "Pengcheng Zuo", "Bangjun Lei"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17335v1", "summary": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17335v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "TransLPRNet：用于单/双行中文车牌识别的轻量级视觉-语言网络", "tldr": "提出TransLPRNet，一个轻量级视觉-语言网络，用于单/双行中文车牌识别，通过合成数据和透视校正网络，实现高精度和高速度。", "motivation": "开放环境下车牌识别面临车牌类型多样性和成像条件复杂性挑战；CNN和CRNN方法存在局限性。", "method": "提出TransLPRNet，一个统一的轻量级视觉编码器与文本解码器集成的预训练框架，针对单/双行中文车牌。构建了合成单/双行车牌数据集以缓解数据稀缺。引入透视校正网络（PTN），通过车牌角点坐标回归和视图分类信息监督。", "result": "在粗定位扰动下，校正后的CCPD测试集平均识别准确率达99.34%；在精细定位扰动下，准确率提高到99.58%。在双行车牌测试集上，平均识别准确率达98.70%。处理速度达到167帧/秒。", "conclusion": "TransLPRNet通过其轻量级设计、数据增强策略和透视校正机制，显著提升了单/双行中文车牌识别的准确性和实用性。", "translation": "开放环境下的车牌识别在各个领域都有广泛应用；然而，车牌类型的多样性和成像条件带来了巨大的挑战。为了解决基于CNN和CRNN的车牌识别方法所遇到的局限性，本文提出了一种统一的解决方案，将轻量级视觉编码器与文本解码器集成在一个专门针对单行和双行中文车牌的预训练框架中。为了缓解双行车牌数据集的稀缺性，我们通过合成图像、将纹理映射到真实场景并与真实车牌图像混合，构建了一个单/双行车牌数据集。此外，为了提高系统的识别精度，我们引入了一个透视校正网络（PTN），该网络采用车牌角点坐标回归作为隐变量，并由车牌视图分类信息进行监督。该网络具有更好的稳定性、可解释性和低标注成本。所提出的算法在粗定位扰动下，在校正后的CCPD测试集上实现了99.34%的平均识别准确率。在精细定位扰动下进行评估时，准确率进一步提高到99.58%。在双行车牌测试集上，它实现了98.70%的平均识别准确率，处理速度高达每秒167帧，表明其具有强大的实际适用性。", "summary": "本文针对开放环境下中文车牌识别的挑战，提出TransLPRNet，一个整合轻量级视觉编码器和文本解码器的统一框架，用于单/双行车牌识别。为解决数据稀缺，通过图像合成构建了单/双行车牌数据集。此外，引入透视校正网络（PTN）以提高识别精度。实验结果表明，TransLPRNet在不同定位扰动下的单/双行车牌识别上均达到高准确率（99.34%-99.58%），且处理速度快（167帧/秒），具有很强的实用性。", "keywords": "车牌识别, 视觉-语言网络, 透视校正, 数据合成, 中文车牌", "comments": "这篇论文的创新点在于提出了一个轻量级的视觉-语言网络TransLPRNet，并结合了有效的数据合成策略和独特的透视校正网络（PTN），解决了中文单/双行车牌识别中的数据稀缺和角度偏差问题。其高精度和高处理速度显示了强大的实用价值和潜力。"}}
{"id": "2507.17515", "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models", "authors": ["Songshuo Lu", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17515v1", "summary": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17515v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "URPO：一种大型语言模型的统一奖励与策略优化框架", "tldr": "URPO是一种统一奖励与策略优化框架，它将大型语言模型的指令遵循和奖励建模整合到一个模型和训练阶段，显著提升了性能并简化了对齐流程。", "motivation": "现有的大型语言模型对齐流程通常将策略模型与单独训练的奖励模型配对，导致流程复杂、资源密集，且由于静态奖励信号而存在性能瓶颈。", "method": "URPO框架将指令遵循（“玩家”）和奖励建模（“裁判”）统一到单个模型和单个训练阶段中。它将所有对齐数据（包括偏好对、可验证推理和开放式指令）重构为统一的生成格式，并通过单一的组相对策略优化（GRPO）循环进行优化。这使得模型能够从真实偏好和可验证逻辑中学习，同时为开放式任务生成自己的奖励。", "result": "在Qwen2.5-7B模型上的实验表明，URPO显著优于使用单独生成奖励模型的强大基线。它将AlpacaEval上的指令遵循得分从42.24提升到44.84，复合推理平均值从32.66提升到35.66。此外，URPO在训练过程中培养出卓越的内部评估器，RewardBench得分达到85.15，超过了其取代的专用奖励模型（83.55）。", "conclusion": "URPO通过消除对单独奖励模型的需求，并促进生成和评估之间的协同演化动态，为实现稳健对齐的语言模型提供了一条更简单、更高效、更有效的路径。", "translation": "大规模对齐流程通常将策略模型与单独训练的奖励模型配对，其参数在强化学习（RL）期间保持冻结。这种分离导致了复杂、资源密集型管道，并由于静态奖励信号而受到性能上限的限制。我们提出了一种新颖的框架，统一奖励与策略优化（URPO），它在一个模型和单个训练阶段内统一了指令遵循（“玩家”）和奖励建模（“裁判”）。我们的方法将所有对齐数据——包括偏好对、可验证推理和开放式指令——重铸为统一的生成格式，并通过单一的组相对策略优化（GRPO）循环进行优化。这使得模型能够从真实偏好和可验证逻辑中学习，同时为开放式任务生成自己的奖励。在Qwen2.5-7B模型上的实验证明了URPO的优越性。我们的统一模型显著优于使用单独生成奖励模型的强大基线，将AlpacaEval上的指令遵循得分从42.24提高到44.84，复合推理平均值从32.66提高到35.66。此外，URPO在训练过程中培养出卓越的内部评估器，其RewardBench得分达到85.15，并超越了其取代的专用奖励模型（83.55）。通过消除对单独奖励模型的需求并促进生成和评估之间的协同演化动态，URPO为实现稳健对齐的语言模型提供了一条更简单、更高效、更有效的路径。", "summary": "URPO是一个新颖的框架，旨在解决大型语言模型对齐中策略模型与独立奖励模型分离所带来的复杂性、资源消耗和性能瓶颈。它通过将指令遵循和奖励建模统一到单个模型和训练阶段来简化流程。URPO将所有对齐数据转化为统一的生成格式，并使用GRPO循环进行优化，使模型能够从真实数据中学习并自我生成奖励。实验证明，URPO在指令遵循和推理能力上显著优于基线模型，并能培养出更优秀的内部评估器，从而提供了一种更简单、高效且有效的LLM对齐方法。", "keywords": "URPO, 大型语言模型, 统一优化, 奖励模型, 策略优化", "comments": "URPO的创新点在于其统一的奖励与策略优化框架，打破了传统LLM对齐中策略模型和奖励模型分离的范式。这种一体化设计不仅简化了训练流程，降低了资源消耗，更重要的是通过生成和评估的协同演化动态，克服了静态奖励信号带来的性能限制。其在内部评估能力上的提升也证明了该框架的有效性。"}}
{"id": "2311.12451", "title": "A frame approach for equations involving the fractional Laplacian", "authors": ["Ioannis P. A. Papadopoulos", "Timon S. Gutleb", "José A. Carrillo", "Sheehan Olver"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2311.12451v4", "summary": "Exceptionally elegant formulae exist for the fractional Laplacian operator\napplied to weighted classical orthogonal polynomials. We utilize these results\nto construct a solver, based on frame properties, for equations involving the\nfractional Laplacian of any power, $s \\in (0,1)$, on an unbounded domain in one\nor two dimensions. The numerical method represents solutions in an expansion of\nweighted classical orthogonal polynomials as well as their unweighted\ncounterparts with a specific extension to $\\mathbb{R}^d$, $d \\in \\{1,2\\}$. We\nexamine the frame properties of this family of functions for the solution\nexpansion and, under standard frame conditions, derive an a priori estimate for\nthe stationary equation. Moreover, we prove one achieves the expected order of\nconvergence when considering an implicit Euler discretization in time for the\nfractional heat equation. We apply our solver to numerous examples including\nthe fractional heat equation (utilizing up to a $6^\\text{th}$-order\nRunge--Kutta time discretization), a fractional heat equation with a\ntime-dependent exponent $s(t)$, and a two-dimensional problem, observing\nspectral convergence in the spatial dimension for sufficiently smooth data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2311.12451v4", "cate": "math.NA", "date": "2023-11-21", "updated": "2025-07-23", "AI": {"title_translation": "分数阶拉普拉斯方程的框架方法", "tldr": "基于框架性质，利用正交多项式，开发了一种用于无界域分数阶拉普拉斯方程的数值求解器，并展示了谱收敛性。", "motivation": "利用分数阶拉普拉斯算子应用于加权经典正交多项式的现有优雅公式，构建一个用于解决涉及分数阶拉普拉斯方程的求解器。", "method": "该方法基于框架性质构建了一个求解器，通过加权经典正交多项式及其非加权对应项的展开式来表示解。研究了这族函数的框架性质，推导了平稳方程的先验估计，并证明了当对分数阶热方程进行时间隐式欧拉离散化时能达到预期的收敛阶数。将该求解器应用于多个示例。", "result": "推导了平稳方程的先验估计。证明了分数阶热方程时间隐式欧拉离散化可以达到预期的收敛阶数。在应用于多个示例（包括分数阶热方程和二维问题）时，对于足够光滑的数据，观察到空间维度上的谱收敛。", "conclusion": "所开发的基于框架的分数阶拉普拉斯方程求解器是有效的，并展示了良好的收敛特性，包括对光滑数据的谱收敛。", "translation": "对于应用于加权经典正交多项式上的分数阶拉普拉斯算子，存在极其优雅的公式。我们利用这些结果，基于框架性质，构建了一个求解器，用于解决一维或二维无界域上涉及任意阶数 $s \\in (0,1)$ 分数阶拉普拉斯算子的方程。该数值方法通过加权经典正交多项式及其在 $\\mathbb{R}^d$, $d \\in \\{1,2\\}$ 上特定扩展的非加权对应项的展开式来表示解。我们检验了这族函数用于解展开的框架性质，并在标准框架条件下，推导了平稳方程的先验估计。此外，我们证明了当考虑分数阶热方程的时间隐式欧拉离散化时，可以达到预期的收敛阶数。我们将所开发的求解器应用于众多示例，包括分数阶热方程（利用高达6阶的龙格-库塔时间离散化）、具有时变指数 $s(t)$ 的分数阶热方程以及二维问题，观察到对于足够光滑的数据，在空间维度上实现了谱收敛。", "summary": "本文介绍了一种针对一维或二维无界域上分数阶拉普拉斯方程（$s \\in (0,1)$）的新型数值求解器。该方法利用了分数阶拉普拉斯算子在加权经典正交多项式上的现有优雅公式，并基于框架性质构建。解被表示为加权和非加权正交多项式的展开式。作者推导了平稳方程的先验估计，并证明了使用隐式欧拉法对分数阶热方程进行时间离散化时能达到预期的收敛率。该求解器应用于各种示例，包括时变指数和二维问题，结果显示对于光滑数据，在空间维度上实现了谱收敛。", "keywords": "分数阶拉普拉斯, 框架方法, 正交多项式, 谱收敛, 数值求解器", "comments": "该研究的创新之处在于利用“极其优雅的公式”和“框架性质”来构建一个针对无界域分数阶拉普拉斯方程的鲁棒求解器，这在处理上具有挑战性。在空间维度上展示的谱收敛性和在时间维度上预期的收敛阶数突出了该方法的效率和准确性，使其成为分数阶偏微分方程数值方法领域的重要贡献。"}}
{"id": "2411.04030", "title": "Quantum-Safe Hybrid Key Exchanges with KEM-Based Authentication", "authors": ["Christopher Battarbee", "Christoph Striecks", "Ludovic Perret", "Sebastian Ramacher", "Kevin Verhaeghe"], "categories": ["cs.CR", "quant-ph", "94A60, 81P94, 94A62", "E.3"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.04030v2", "summary": "Authenticated Key Exchange (AKE) between any two entities is one of the most\nimportant security protocols available for securing our digital networks and\ninfrastructures. In PQCrypto 2023, Bruckner, Ramacher and Striecks proposed a\nnovel hybrid AKE (HAKE) protocol, dubbed Muckle+, that is particularly useful\nin large quantum-safe networks consisting of a large number of nodes. Their\nprotocol is hybrid in the sense that it allows key material from conventional\nand post-quantum primitives, as well as from quantum key distribution, to be\nincorporated into a single end-to-end shared key.\n  To achieve the desired authentication properties, Muckle+ utilizes\npost-quantum digital signatures. However, available instantiations of such\nsignatures schemes are not yet efficient enough compared to their post-quantum\nkey-encapsulation mechanism (KEM) counterparts, particularly in large networks\nwith potentially several connections in a short period of time.\n  To mitigate this gap, we propose Muckle# that pushes the efficiency\nboundaries of currently known HAKE constructions. Muckle# uses post-quantum\nkey-encapsulating mechanisms for implicit authentication inspired by recent\nworks done in the area of Transport Layer Security (TLS) protocols,\nparticularly, in KEMTLS (CCS'20).\n  We port those ideas to the HAKE framework and develop novel proof techniques\non the way. Due to our novel KEM-based approach, the resulting protocol has a\nslightly different message flow compared to prior work that we carefully align\nwith the HAKE framework and which makes our changes to the Muckle+ non-trivial.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.04030v2", "cate": "cs.CR", "date": "2024-11-06", "updated": "2025-07-23", "AI": {"title_translation": "量子安全混合密钥交换与基于KEM的认证", "tldr": "Muckle#协议通过使用基于KEM的隐式认证，提高了混合认证密钥交换（HAKE）协议的效率，解决了现有后量子数字签名效率不足的问题。", "motivation": "现有的后量子数字签名方案效率不高，尤其是在大型网络中，而PQCrypto 2023提出的Muckle+协议依赖于这些签名。因此，需要提出一种更高效的混合认证密钥交换协议以弥补这一效率差距。", "method": "本文提出了Muckle#协议，该协议借鉴了传输层安全（TLS）协议（特别是KEMTLS）中基于KEM的隐式认证思想，并将其应用于混合认证密钥交换（HAKE）框架。在此过程中，开发了新的证明技术，并调整了协议的消息流以适应HAKE框架。", "result": "Muckle#协议通过使用后量子密钥封装机制（KEM）进行隐式认证，成功突破了当前已知HAKE结构的效率界限，提供了更高效的量子安全混合密钥交换方案。", "conclusion": "Muckle#协议通过其新颖的基于KEM的隐式认证方法，为量子安全混合密钥交换提供了一种更高效的解决方案，有效缓解了后量子数字签名效率不足的挑战，对于构建大型量子安全网络具有重要意义。", "translation": "任何两个实体之间的认证密钥交换（AKE）是保护我们数字网络和基础设施最重要的安全协议之一。在PQCrypto 2023上，Bruckner、Ramacher和Striecks提出了一种新颖的混合AKE（HAKE）协议，名为Muckle+，该协议在由大量节点组成的大型量子安全网络中特别有用。他们的协议之所以是混合的，是因为它允许将来自传统和后量子原语以及量子密钥分发的密钥材料整合到一个单一的端到端共享密钥中。为了实现所需的认证特性，Muckle+利用了后量子数字签名。然而，与后量子密钥封装机制（KEM）对应物相比，此类签名方案的现有实例化效率尚不足，尤其是在短时间内可能存在多个连接的大型网络中。为了弥补这一差距，我们提出了Muckle#，它突破了当前已知HAKE结构的效率界限。Muckle#使用后量子密钥封装机制进行隐式认证，其灵感来源于传输层安全（TLS）协议领域的最新工作，特别是KEMTLS（CCS'20）。我们将这些思想移植到HAKE框架中，并在此过程中开发了新颖的证明技术。由于我们新颖的基于KEM的方法，所得协议与先前的工作相比具有略微不同的消息流，我们仔细地将其与HAKE框架对齐，这使得我们对Muckle+的更改并非微不足道。", "summary": "本文提出了一种名为Muckle#的新型混合认证密钥交换（HAKE）协议，旨在提高量子安全网络中密钥交换的效率。针对现有HAKE协议（如Muckle+）中后量子数字签名效率不足的问题，Muckle#借鉴传输层安全（TLS）协议中基于KEM的隐式认证思想，利用后量子密钥封装机制实现认证。通过将这些思想移植到HAKE框架并开发新的证明技术，Muckle#在保持量子安全性的同时，显著提升了协议的效率。", "keywords": "量子安全, 混合密钥交换, KEM, 认证, 后量子密码", "comments": "本文的创新点在于将基于KEM的隐式认证思想引入到混合认证密钥交换（HAKE）协议中，以解决后量子数字签名效率低下的问题。这对于构建高效且量子安全的数字网络至关重要。该方法是对现有Muckle+协议的非平凡改进，并开发了新的证明技术，显示了其理论贡献。"}}
{"id": "2507.17520", "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation", "authors": ["Shuai Yang", "Hao Li", "Yilun Chen", "Bin Wang", "Yang Tian", "Tai Wang", "Hanqing Wang", "Feng Zhao", "Yiyi Liao", "Jiangmiao Pang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      38 pages", "url": "http://arxiv.org/abs/2507.17520v1", "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.", "comment": "38 pages", "pdf_url": "http://arxiv.org/pdf/2507.17520v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "InstructVLA：从理解到操作的视觉-语言-动作指令微调", "tldr": "InstructVLA是一个端到端的视觉-语言-动作（VLA）模型，通过新颖的VLA指令微调范式，在保留大型视觉-语言模型（VLM）灵活推理能力的同时，显著提升了机器人的操作性能，并在模拟和真实世界中展现了强大的泛化能力。", "motivation": "现有视觉-语言-动作（VLA）模型在多模态推理和精确动作生成之间存在权衡，通常局限于特定任务的操作数据，并遭受预训练视觉-语言能力灾难性遗忘的问题。", "method": "引入InstructVLA，一个端到端的VLA模型，通过视觉-语言-动作指令微调（VLA-IT）这一新颖的训练范式。该范式采用多模态训练和混合专家适应，在标准VLM语料库和精选的65万样本VLA-IT数据集上共同优化文本推理和动作生成。", "result": "在域内SimplerEnv任务上，InstructVLA比SpatialVLA提高了30.5%。在SimplerEnv-Instruct（一个需要闭环控制和高级指令理解的80任务基准）上，它比微调后的OpenVLA高出92%，比由GPT-4o辅助的动作专家高出29%。此外，InstructVLA在多模态任务上超越了基线VLM，并通过利用文本推理在模拟和真实世界环境中提升操作性能，展现了推理时缩放能力。", "conclusion": "这些结果表明InstructVLA有潜力弥合直观可控的人机交互与高效策略学习之间的鸿沟。", "translation": "为了在现实世界中有效运作，机器人必须将多模态推理与精确的动作生成相结合。然而，现有的视觉-语言-动作（VLA）模型往往顾此失彼，将其能力局限于特定任务的操作数据，并遭受预训练视觉-语言能力灾难性遗忘的问题。为了弥合这一差距，我们引入了InstructVLA，一个端到端的VLA模型，它在保持大型视觉-语言模型（VLM）灵活推理能力的同时，提供了领先的操作性能。InstructVLA引入了一种新颖的训练范式，即视觉-语言-动作指令微调（VLA-IT），该范式采用多模态训练和混合专家适应，在标准VLM语料库和精选的65万样本VLA-IT数据集上共同优化文本推理和动作生成。在域内SimplerEnv任务上，InstructVLA比SpatialVLA提高了30.5%。为了评估泛化能力，我们引入了SimplerEnv-Instruct，一个需要闭环控制和高级指令理解的80任务基准，InstructVLA在该基准上比微调后的OpenVLA高出92%，比由GPT-4o辅助的动作专家高出29%。此外，InstructVLA在多模态任务上超越了基线VLM，并通过利用文本推理在模拟和真实世界环境中提升操作性能，展现了推理时缩放能力。这些结果表明InstructVLA有潜力弥合直观可控的人机交互与高效策略学习之间的鸿沟。", "summary": "InstructVLA是一个新颖的端到端视觉-语言-动作（VLA）模型，旨在解决现有VLA模型在多模态推理、动作生成和泛化能力方面的局限性。该模型引入了视觉-语言-动作指令微调（VLA-IT）范式，通过结合多模态训练和混合专家适应，在大型VLM语料库和专门构建的VLA-IT数据集上共同优化文本推理和动作生成。实验证明，InstructVLA在特定任务和泛化基准上均显著超越现有模型，并在模拟和真实环境中展现了通过文本推理提升操作性能的能力，预示其在实现直观人机交互方面的巨大潜力。", "keywords": "视觉-语言-动作, 指令微调, 机器人操作, 多模态推理, InstructVLA", "comments": "InstructVLA的创新之处在于其VLA-IT训练范式，该范式成功地将大型视觉-语言模型的灵活推理能力与精确的机器人操作相结合，克服了现有VLA模型常见的灾难性遗忘和任务局限性。其在多个基准测试上的显著性能提升，特别是引入了新的泛化基准SimplerEnv-Instruct，证明了其强大的泛化能力和在现实世界应用中的潜力。这是一个重要的进展，有望推动机器人领域更智能、更通用的人机交互。"}}
{"id": "2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14534v2", "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14534v2", "cate": "eess.AS", "date": "2025-07-19", "updated": "2025-07-23", "AI": {"title_translation": "Conan：一种用于零样本自适应语音转换的分块在线网络", "tldr": "Conan是一种分块在线零样本语音转换模型，解决了实时语音转换中语义保真度、自然度和对未知说话人适应性的挑战。", "motivation": "当前的语音转换模型在实时约束下难以保持语义保真度、产生听起来自然的转换，并且难以有效适应未见的说话人特征。", "method": "Conan模型包含三个核心组件：1) 利用Emformer进行低延迟流内容编码的流内容提取器；2) 提取参考语音细粒度风格特征以增强风格适应性的自适应风格编码器；3) 使用像素混洗机制实现全因果HiFiGAN的因果混洗声码器。", "result": "实验评估表明，Conan在主观和客观指标上均优于基线模型。", "conclusion": "Conan在主观和客观指标上均优于基线模型，有效解决了零样本在线语音转换中的挑战。", "translation": "零样本在线语音转换（VC）在实时通信和娱乐方面具有巨大前景。然而，当前的VC模型在实时约束下难以保持语义保真度、提供自然听起来的转换，并且难以有效适应未见的说话人特征。为了解决这些挑战，我们引入了Conan，一个分块在线零样本语音转换模型，它在匹配参考语音的音色和风格的同时保留了源语音的内容。Conan包含三个核心组件：1）一个流内容提取器，利用Emformer进行低延迟流内容编码；2）一个自适应风格编码器，从参考语音中提取细粒度风格特征以增强风格适应性；3）一个因果混洗声码器，它使用像素混洗机制实现了全因果HiFiGAN。实验评估表明，Conan在主观和客观指标上均优于基线模型。音频样本可在https://aaronz345.github.io/ConanDemo找到。", "summary": "本论文介绍了Conan，一个分块在线零样本语音转换模型，旨在解决现有模型在实时约束下语义保真度、自然度以及对未知说话人适应性方面的不足。Conan通过其独特的三大组件——流内容提取器、自适应风格编码器和因果混洗声码器——实现了在保留源内容的同时匹配参考语音风格和音色的能力。实验结果表明，Conan在多项主客观指标上均优于现有基线模型。", "keywords": "零样本语音转换, 在线语音转换, 分块网络, 实时通信, 语音合成", "comments": "Conan的创新之处在于其分块在线处理方式和专门设计的三个核心组件，特别是利用Emformer进行低延迟流内容编码和像素混洗机制实现全因果HiFiGAN，这对于实现实时、高质量的零样本语音转换至关重要。该研究对于实时通信和娱乐领域的应用具有重要意义。"}}
{"id": "2507.17475", "title": "Output Feedback Design for Parameter Varying Systems subject to Persistent Disturbances and Control Rate Constraints", "authors": ["Jackson G. Ernesto", "Eugenio B. Castelan", "Walter Lucia"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Preprint of the manuscript submitted to the International Journal of Robust and Nonlinear Control (IJRNC)", "url": "http://arxiv.org/abs/2507.17475v1", "summary": "This paper presents a technique for designing output feedback controllers for\nconstrained linear parameter-varying systems that are subject to persistent\ndisturbances. Specifically, we develop an incremental parameter-varying output\nfeedback control law to address control rate constraints, as well as state and\ncontrol amplitude constraints. The proposal is based on the concept of robust\npositively invariant sets and applies the extended Farkas' lemma to derive a\nset of algebraic conditions that define both the control gains and a robust\npositively invariant polyhedron that satisfies the control and state\nconstraints. These algebraic conditions are formulated into a bilinear\noptimization problem aimed at determining the output feedback gains and the\nassociated polyedral robust positively invariant region. The obtained\ncontroller ensures that any closed-loop trajectory originating from the\npolyhedron converges to another smaller inner polyhedral set around the origin\nin finite time, where the trajectory remains ultimately bounded regardless of\nthe persistent disturbances and variations in system parameters. Furthermore,\nby including the sizes of the two polyhedral sets inside the objective\nfunction, the proposed optimization can also jointly enlarge the outer set\nwhile minimizing the inner one. Numerical examples are presented to demonstrate\nthe effectiveness of our proposal in managing the specified constraints,\ndisturbances, and parameter variations.", "comment": "Preprint of the manuscript submitted to the International Journal of\n  Robust and Nonlinear Control (IJRNC)", "pdf_url": "http://arxiv.org/pdf/2507.17475v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "针对受持续扰动和控制速率约束的参数变化系统的输出反馈设计", "tldr": "本文提出了一种针对受持续扰动和控制速率约束的参数变化系统，基于鲁棒正不变集和Farkas引理的输出反馈控制器设计方法，通过双线性优化确保了闭环轨迹的有界性和收敛性。", "motivation": "针对受持续扰动和控制速率、状态及控制幅度约束的线性参数变化系统，设计输出反馈控制器。", "method": "提出了一种增量式参数变化输出反馈控制律。该方法基于鲁棒正不变集概念，并应用扩展的Farkas引理推导出一系列代数条件，这些条件定义了控制增益和满足约束的鲁棒正不变多面体。这些代数条件被表述为一个双线性优化问题，用于确定输出反馈增益和相关的多面体鲁棒正不变区域。", "result": "所获得控制器确保任何源自多面体的闭环轨迹在有限时间内收敛到原点周围的一个更小的内部多面体集合，并且无论持续扰动和系统参数变化如何，轨迹最终都保持有界。通过在目标函数中包含两个多面体集合的大小，所提出的优化方法还可以联合增大外部集合同时最小化内部集合。数值例子证明了该方法在管理指定约束、扰动和参数变化方面的有效性。", "conclusion": "本文成功开发了一种针对受持续扰动和多种约束的线性参数变化系统的输出反馈控制器设计技术，通过鲁棒正不变集和双线性优化，实现了闭环系统的收敛性和有界性，并展示了其有效性。", "translation": "本文提出了一种为受持续扰动的受约束线性参数变化系统设计输出反馈控制器的技术。具体来说，我们开发了一种增量式参数变化输出反馈控制律，以解决控制速率约束以及状态和控制幅度约束。该方案基于鲁棒正不变集的概念，并应用扩展的Farkas引理推导出一系列代数条件，这些条件定义了控制增益和满足控制与状态约束的鲁棒正不变多面体。这些代数条件被表述为一个双线性优化问题，旨在确定输出反馈增益和相关的多面体鲁棒正不变区域。所获得的控制器确保任何源自该多面体的闭环轨迹在有限时间内收敛到原点周围的另一个更小的内部多面体集合，并且无论持续扰动和系统参数如何变化，轨迹最终都保持有界。此外，通过将两个多面体集合的大小包含在目标函数中，所提出的优化方法还可以联合增大外部集合同时最小化内部集合。文中提供了数值例子来证明我们提出的方法在管理指定约束、扰动和参数变化方面的有效性。", "summary": "本文提出了一种针对受持续扰动和多种约束（控制速率、状态、控制幅度）的线性参数变化系统设计输出反馈控制器的方法。该方法基于鲁棒正不变集和扩展Farkas引理，将控制器设计问题转化为一个双线性优化问题，以确定控制增益和鲁棒正不变多面体区域。所设计的控制器能确保闭环系统轨迹在有限时间内收敛到原点附近的一个更小区域，并最终保持有界，不受扰动和参数变化影响。该优化还允许同时扩大外部不变集并缩小内部收敛集。数值例子验证了其有效性。", "keywords": "参数变化系统, 输出反馈, 鲁棒正不变集, 控制速率约束, 持续扰动", "comments": "这项研究的创新之处在于其将增量式参数变化输出反馈控制律与鲁棒正不变集理论和扩展Farkas引理相结合，以解决受多种约束和持续扰动的线性参数变化系统的控制器设计问题。通过将其表述为双线性优化问题，并能够同时优化内外不变集的大小，提高了实际应用中的灵活性和性能。其重要性在于为复杂受限系统提供了一种有效的控制策略。"}}
{"id": "2507.17411", "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions", "authors": ["Pál András Papp", "Toni Böhnlein", "A. N. Yzelman"], "categories": ["cs.DC", "90B35, 90C10, 68Q10, 68W10", "C.1.4"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Published in the 54th International Conference on Parallel Processing (ICPP 2025)", "url": "http://arxiv.org/abs/2507.17411v1", "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.", "comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17411v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多处理器调度与内存约束：基本特性和寻找最优解", "tldr": "研究了多处理器在两级内存层次结构下的计算DAG调度问题，分析了其理论特性和计算复杂性，并提出了一种基于整数线性规划（ILP）的整体调度算法，实验证明其性能优于传统方法。", "motivation": "该研究旨在解决在两级内存层次结构中，多处理器调度通用计算DAG的问题。这个问题是现有多个著名模型的自然推广，同时涵盖了工作负载平衡、通信以及缓存大小限制引起的数据移动。作者还指出，分别优化并行化和内存管理会导致与最优解存在线性因子差距。", "method": "本文从理论角度分析了该问题的基本特性，包括其计算复杂性。在算法方面，提出了一种将问题表示为整数线性规划（ILP）并求解的自然技术，并基于此开发了一种整体调度算法。最后，通过在小型计算任务基准上进行实验，研究了该算法的性能和特性。", "result": "理论分析表明，分别优化并行化和内存管理的方法会导致与最优解存在线性因子差距。实验结果证实，基于ILP的方法比结合经典调度算法和内存管理策略的基线方法能够找到明显更好的解决方案。", "conclusion": "论文证明了在两级内存层次结构下，多处理器调度问题中分别优化并行化和内存管理的局限性，并成功开发了一种基于ILP的整体调度算法，该算法在实际应用中能够获得比传统组合方法更好的性能。", "translation": "我们研究了在两级内存层次结构中，多处理器上通用计算DAG的调度问题。这种设置是文献中几个著名模型的自然推广，它同时捕获了工作负载平衡、通信以及由于缓存大小限制导致的数据移动。我们首先从理论角度分析了该问题的基本特性，例如其计算复杂性。我们还证明，像许多应用程序中那样，分别优化并行化和内存管理，可能导致解决方案与最优解相差一个线性因子。在算法方面，我们讨论了一种将问题表示为整数线性规划（ILP）并求解的自然技术。我们基于这种方法开发了一种整体调度算法，并在一个小型计算任务基准上实验研究了其性能和特性。我们的结果证实，基于ILP的方法确实比结合了经典调度算法和内存管理策略的基线方法能够找到明显更好的解决方案。", "summary": "本文研究了在两级内存层次结构下多处理器调度计算DAG的问题，该问题整合了工作负载平衡、通信和数据移动。研究首先从理论上分析了问题的基本性质和计算复杂性，并指出单独优化并行化和内存管理会导致次优解。随后，论文提出了一种基于整数线性规划（ILP）的整体调度算法，并通过实验证明其性能显著优于传统组合方法。", "keywords": "多处理器调度, 内存约束, 计算DAG, 整数线性规划, 整体调度", "comments": "这篇论文的创新点在于将多处理器调度与内存约束相结合，并将其视为一个整体问题进行研究，而不是将并行化和内存管理分开优化。通过提出基于ILP的整体调度算法，解决了传统方法次优的问题，为实际应用中更高效的计算资源管理提供了新的思路和方法。"}}
{"id": "2507.17140", "title": "Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall Installation", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17140v1", "summary": "In the context of labor shortages and rising costs, construction robots are\nregarded as the key to revolutionizing traditional construction methods and\nimproving efficiency and quality in the construction industry. In order to\nensure that construction robots can perform tasks efficiently and accurately in\ncomplex construction environments, traditional single-objective trajectory\noptimization methods are difficult to meet the complex requirements of the\nchanging construction environment. Therefore, we propose a multi-objective\ntrajectory optimization for the robotic arm used in the curtain wall\ninstallation. First, we design a robotic arm for curtain wall installation,\nintegrating serial, parallel, and folding arm elements, while considering its\nphysical properties and motion characteristics. In addition, this paper\nproposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)\nthat incorporates a focus operator screening mechanism to accelerate the\nconvergence of the algorithm towards the Pareto front, thereby effectively\nbalancing the multi-objective constraints of construction robots. The proposed\nalgorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive\ntrials on the DTLZ3 and WFG3 test functions, showing significantly better\nconvergence efficiency than the other algorithms. Finally, we conduct two sets\nof experiments on the designed robotic arm platform, which confirm the\nefficiency and practicality of the NSGA-III-FO algorithm in solving\nmulti-objective trajectory planning problems for curtain wall installation\ntasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17140v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "幕墙安装中机械臂的多目标轨迹规划", "tldr": "针对幕墙安装机械臂，提出并验证了一种基于NSGA-III-FO的多目标轨迹优化算法，以提高施工效率和解决复杂环境下的轨迹规划问题。", "motivation": "在劳动力短缺和成本上升的背景下，建筑机器人是革新传统施工方法、提高效率和质量的关键。然而，传统的单目标轨迹优化方法难以满足复杂多变的施工环境需求。", "method": "首先，设计了一种集成串联、并联和折叠臂元素的幕墙安装机械臂，并考虑其物理特性和运动特性。其次，提出了一种NSGA-III-FO算法（NSGA-III with Focused Operator），该算法通过引入聚焦操作符筛选机制，加速算法向Pareto前沿的收敛，以平衡建筑机器人的多目标约束。最后，在DTLZ3和WFG3测试函数上与NSGA-III、MOEA/D、MSOPS-II进行了对比测试，并在设计的机械臂平台上进行了两组实验。", "result": "NSGA-III-FO算法在DTLZ3和WFG3测试函数上显示出比NSGA-III、MOEA/D和MSOPS-II显著更好的收敛效率。在设计的机械臂平台上进行的实验证实了NSGA-III-FO算法在解决幕墙安装任务多目标轨迹规划问题中的效率和实用性。", "conclusion": "本文提出的NSGA-III-FO算法能有效解决幕墙安装机械臂的多目标轨迹规划问题，证实了其在复杂施工环境下的高效性和实用性，有助于提升建筑机器人的性能。", "translation": "标题：幕墙安装中机械臂的多目标轨迹规划\n\n摘要：在劳动力短缺和成本上升的背景下，建筑机器人被认为是彻底改变传统施工方法、提高建筑行业效率和质量的关键。为了确保建筑机器人在复杂的施工环境中高效准确地执行任务，传统的单目标轨迹优化方法难以满足多变的施工环境的复杂要求。因此，我们提出了一种用于幕墙安装机械臂的多目标轨迹优化方法。首先，我们设计了一种用于幕墙安装的机械臂，它集成了串联、并联和折叠臂元件，同时考虑了其物理特性和运动特性。此外，本文提出了一种NSGA-III-FO算法（NSGA-III with Focused Operator，即带有聚焦操作符的NSGA-III），该算法引入了聚焦操作符筛选机制，以加速算法向Pareto前沿的收敛，从而有效平衡建筑机器人的多目标约束。所提出的算法在DTLZ3和WFG3测试函数上与NSGA-III、MOEA/D和MSOPS-II进行了十次连续试验对比，结果显示其收敛效率显著优于其他算法。最后，我们在设计的机械臂平台上进行了两组实验，证实了NSGA-III-FO算法在解决幕墙安装任务的多目标轨迹规划问题中的效率和实用性。", "summary": "本文针对建筑行业劳动力短缺和传统单目标优化在复杂施工环境下的局限性，提出了一种用于幕墙安装机械臂的多目标轨迹优化方法。研究设计了一款融合串联、并联和折叠臂元素的机械臂，并开发了基于NSGA-III的改进算法NSGA-III-FO，通过引入聚焦操作符筛选机制加速算法收敛。实验结果表明，NSGA-III-FO算法在测试函数上表现出更优的收敛效率，并在实际机械臂平台上验证了其在幕墙安装任务中解决多目标轨迹规划问题的效率和实用性。", "keywords": "机械臂, 多目标轨迹规划, 幕墙安装, NSGA-III-FO, 建筑机器人", "comments": "本文创新性地将多目标优化应用于建筑机械臂的轨迹规划，特别是在幕墙安装这一具体场景，具有较强的工程应用价值。提出的NSGA-III-FO算法通过引入聚焦操作符，有效提升了多目标优化算法的收敛效率，解决了传统单目标优化在复杂施工环境中面临的挑战。这项研究为提升建筑机器人的自动化水平和适应复杂环境的能力提供了新的思路和方法。"}}
{"id": "2507.16971", "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "authors": ["Aleksandr Perevalov", "Andreas Both"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      During the final evaluation on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants", "url": "http://arxiv.org/abs/2507.16971v1", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "pdf_url": "http://arxiv.org/pdf/2507.16971v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "Text-to-SPARQL超越英语：通过类人推理实现知识图谱上的多语言问答", "tldr": "本文介绍了mKGQAgent，一个受人类启发的多语言知识图谱问答框架，它将自然语言问题转换为SPARQL查询，并在Text2SPARQL挑战赛中获得第一名。", "motivation": "通过多语言自然语言接口访问知识是信息检索领域的新兴挑战之一。现有方法多集中于组合组件来解决下游任务并给出答案，但缺乏对多语言知识图谱问答的有效处理。", "method": "本文引入了mKGQAgent，一个受人类启发的框架，它将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务。通过利用协调的LLM代理工作流进行规划、实体链接和查询优化，并由经验池引导进行上下文学习，mKGQAgent有效地处理多语言KGQA。", "result": "在Text2SPARQL挑战赛2025中，mKGQAgent在基于DBpedia和Corporate的KGQA基准测试中，表现优于其他参与者，获得了第一名。", "conclusion": "这项工作为开发多语言语义解析中的类人推理系统开辟了新途径。", "translation": "通过多语言自然语言接口访问知识是信息检索及其相关领域的新兴挑战之一。存储在知识图谱中的结构化知识可以通过特定的查询语言（例如SPARQL）进行查询。因此，需要将自然语言输入转换为查询以满足信息需求。以前的方法主要侧重于组合解决下游任务并最终给出答案的组件（例如基于规则或基于神经网络的）。我们引入了mKGQAgent，一个受人类启发的框架，它将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务。通过利用协调的LLM代理工作流进行规划、实体链接和查询优化——由经验池指导进行上下文学习——mKGQAgent有效地处理多语言KGQA。在Text2SPARQL挑战赛2025中基于DBpedia和Corporate的KGQA基准测试中进行评估，我们的方法在其他参与者中获得了第一名。这项工作为开发多语言语义解析中的类人推理系统开辟了新途径。", "summary": "mKGQAgent是一个受人类启发的多语言知识图谱问答（KGQA）框架，旨在将多语言自然语言问题有效地转换为SPARQL查询。该框架通过将任务分解为规划、实体链接和查询优化等模块化子任务，并利用协调的LLM代理工作流和经验池进行上下文学习。在Text2SPARQL挑战赛2025的DBpedia和Corporate KGQA基准测试中，mKGQAgent取得了第一名的成绩，为多语言语义解析中开发类人推理系统提供了新方向。", "keywords": "Text-to-SPARQL, 多语言问答, 知识图谱, LLM代理, 类人推理", "comments": "本文的创新之处在于提出了一个受人类启发、模块化且可解释的框架mKGQAgent，用于多语言Text-to-SPARQL转换。它通过LLM代理工作流和经验池的结合，有效解决了多语言知识图谱问答的挑战，并在比赛中取得了优异成绩，为未来类人推理系统的发展奠定了基础。"}}
{"id": "2507.17352", "title": "LightCom: A Generative AI-Augmented Framework for QoE-Oriented Communications", "authors": ["Chunmei Xu", "Siqi Zhang", "Yi Ma", "Rahim Tafazolli"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17352v1", "summary": "Data-intensive and immersive applications, such as virtual reality, impose\nstringent quality of experience (QoE) requirements that challenge traditional\nquality of service (QoS)-driven communication systems. This paper presents\nLightCom, a lightweight encoding and generative AI (GenAI)-augmented decoding\nframework, designed for QoE-oriented communications under low signal-to-noise\nratio (SNR) conditions. LightCom simplifies transmitter design by applying\nbasic low-pass filtering for source coding and minimal channel coding,\nsignificantly reducing processing complexity and energy consumption. At the\nreceiver, GenAI models reconstruct high-fidelity content from highly compressed\nand degraded signals by leveraging generative priors to infer semantic and\nstructural information beyond traditional decoding capabilities. The key design\nprinciples are analyzed, along with the sufficiency and error-resilience of the\nsource representation. We also develop importance-aware power allocation\nstrategies to enhance QoE and extend perceived coverage. Simulation results\ndemonstrate that LightCom achieves up to a $14$ dB improvement in robustness\nand a $9$ dB gain in perceived coverage, outperforming traditional QoS-driven\nsystems relying on sophisticated source and channel coding. This paradigm shift\nmoves communication systems towards human-centric QoE metrics rather than\nbit-level fidelity, paving the way for more efficient and resilient wireless\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17352v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "LightCom：一种面向QoE的生成式AI增强通信框架", "tldr": "LightCom提出了一种轻量级编码和生成式AI解码框架，旨在低信噪比下实现以QoE为中心的通信，显著提升鲁棒性和感知覆盖。", "motivation": "数据密集型和沉浸式应用对体验质量（QoE）提出了严格要求，传统以服务质量（QoS）为中心的通信系统难以满足。", "method": "LightCom框架在发送端采用轻量级编码（基本低通滤波和最小信道编码）以降低复杂度和能耗。在接收端，利用生成式AI模型从高度压缩和降级的信号中重建高保真内容，并结合重要性感知功率分配策略来增强QoE和扩展感知覆盖。", "result": "仿真结果表明，LightCom在鲁棒性方面提升了高达14 dB，在感知覆盖方面获得了9 dB的增益，优于传统的QoS驱动系统。", "conclusion": "LightCom将通信系统从比特级保真度转向以人为中心的QoE指标，为更高效和弹性强的无线网络铺平了道路。", "translation": "数据密集型和沉浸式应用，如虚拟现实，对体验质量（QoE）提出了严格要求，这给传统的以服务质量（QoS）驱动的通信系统带来了挑战。本文提出了LightCom，一个轻量级编码和生成式AI（GenAI）增强解码框架，专为低信噪比（SNR）条件下的面向QoE的通信而设计。LightCom通过应用基本的低通滤波进行信源编码和最小的信道编码来简化发送器设计，显著降低了处理复杂性和能耗。在接收端，GenAI模型通过利用生成式先验知识推断超越传统解码能力的语义和结构信息，从高度压缩和降级的信号中重建高保真内容。文中分析了关键设计原则，以及信源表示的充分性和抗错误能力。我们还开发了重要性感知功率分配策略，以增强QoE并扩展感知覆盖。仿真结果表明，LightCom在鲁棒性方面实现了高达14 dB的改进，在感知覆盖方面获得了9 dB的增益，优于依赖复杂信源和信道编码的传统QoS驱动系统。这种范式转变将通信系统推向以人为中心的QoE指标，而非比特级保真度，为更高效和弹性的无线网络铺平了道路。", "summary": "LightCom是一种创新的通信框架，旨在解决传统QoS系统在应对现代数据密集型应用QoE需求时的挑战。它通过在发送端采用轻量级编码和在接收端利用生成式AI进行高保真内容重建，显著降低了系统复杂性，并在低信噪比条件下实现了卓越的性能提升，包括更高的鲁棒性和更广的感知覆盖。该框架代表了通信范式从比特级保真度向以人为中心的QoE指标的转变。", "keywords": "生成式AI, QoE, 低信噪比, 无线通信, LightCom", "comments": "LightCom的创新之处在于其将生成式AI引入通信系统的解码端，以在低信噪比下重建高质量内容，从而将重点从传统的比特级QoS转移到用户体验（QoE）。这种方法不仅简化了发送端设计，降低了复杂性和能耗，而且在性能上取得了显著提升，为未来高效且弹性的无线网络提供了新的思路。其核心贡献在于证明了通过语义和结构信息推断，即使在严重降级的信号下也能实现高质量通信。"}}
{"id": "2507.17234", "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17234v1", "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17234v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CLARIFID：通过强化临床准确印象和强制详细发现来改进放射报告生成", "tldr": "CLARIFID通过模拟专家工作流程和多视图融合，提高了放射报告生成的临床准确性。", "motivation": "现有的放射报告自动生成方法难以提供临床可靠的结论，尤其是在确保报告事实正确性方面不足，且常依赖单视图图像，限制了诊断全面性。", "method": "本文提出了CLARIFID框架，直接优化诊断正确性，模仿专家两步工作流程。具体包括：1. 通过段落感知预训练学习从“发现”到“印象”的逻辑流；2. 使用近端策略优化进行微调，以印象部分的CheXbert F1分数作为奖励；3. 强制执行推理感知解码，先完成“发现”部分再合成“印象”；4. 通过基于Vision-Transformer的多视图编码器融合多胸部X射线视图。推理时，应用推理感知下一词强制策略和报告级重排序，确保模型首先生成全面的“发现”部分，然后合成“印象”，从而保持连贯的临床推理。", "result": "在MIMIC-CXR数据集上的实验结果表明，该方法在临床疗效方面表现优越，并且在标准NLG指标和临床感知分数上均优于现有基线。", "conclusion": "CLARIFID通过其创新的框架和推理感知策略，显著提高了放射报告生成的临床准确性和质量。", "translation": "放射报告的自动生成有潜力减轻放射科医生繁重的工作量，然而目前的方法难以提供临床可靠的结论。特别是，大多数现有方法侧重于生成流畅的文本，但未能有效确保报告的事实正确性，并且常常依赖单视图图像，限制了诊断的全面性。我们提出了CLARIFID，一个新颖的框架，通过模仿专家两步工作流程直接优化诊断正确性。具体而言，CLARIFID（1）通过段落感知预训练学习从“发现”（Findings）到“印象”（Impression）的逻辑流，（2）使用近端策略优化（Proximal Policy Optimization）进行微调，其中印象部分的CheXbert F1分数作为奖励，（3）强制执行推理感知解码，在合成“印象”之前完成“发现”部分，以及（4）通过基于视觉Transformer的多视图编码器融合多幅胸部X射线图像。在推理阶段，我们应用了一种推理感知下一词强制策略，随后进行报告级重排序，确保模型首先生成全面的“发现”部分，然后合成“印象”，从而保持连贯的临床推理。在MIMIC-CXR数据集上的实验结果表明，我们的方法在临床疗效方面取得了卓越表现，并且在标准自然语言生成（NLG）指标和临床感知分数上均优于现有基线。", "summary": "CLARIFID是一个新颖的放射报告自动生成框架，旨在解决现有方法在临床可靠性和事实正确性方面的不足。它通过模拟放射科专家的两步工作流程（先发现后印象）、引入段落感知预训练、基于强化学习的微调、推理感知解码以及多视图图像融合来直接优化诊断的正确性。在MIMIC-CXR数据集上的实验证明，CLARIFID在临床疗效和各项评估指标上均优于现有方法，显著提升了生成报告的临床准确性和全面性。", "keywords": "放射报告生成, 临床准确性, 多视图融合, 强化学习, 医疗AI", "comments": "该论文的创新点在于其模仿专家诊断流程的两步法（先发现后印象），并通过强化学习和推理感知解码来确保临床准确性和逻辑连贯性。此外，多视图图像融合也提升了诊断的全面性。这对于提高AI在医疗领域的可靠性具有重要意义，解决了现有方法在事实正确性方面的痛点。"}}
{"id": "2501.10601", "title": "Understanding Computational Science and Engineering (CSE) and Domain Science Skills Development in National Laboratory Postgraduate Internships", "authors": ["Morgan M. Fong", "Hilary Egan", "Marc Day", "Kristin Potter", "Michael J. Martin"], "categories": ["cs.CY", "97", "K.3"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      In review at Journal of Engineering Education", "url": "http://arxiv.org/abs/2501.10601v2", "summary": "Background: Harnessing advanced computing for scientific discovery and\ntechnological innovation demands scientists and engineers well-versed in both\ndomain science and computational science and engineering (CSE). However, few\nuniversities provide access to both integrated domain science/CSE\ncross-training and Top-500 High-Performance Computing (HPC) facilities.\nNational laboratories offer internship opportunities capable of developing\nthese skills. Purpose: This student presents an evaluation of federally-funded\npostgraduate internship outcomes at a national laboratory. This study seeks to\nanswer three questions: 1) What computational skills, research skills, and\nprofessional skills do students improve through internships at the selected\nnational laboratory. 2) Do students gain knowledge in domain science topics\nthrough their internships. 3) Do students' career interests change after these\ninternships? Design/Method: We developed a survey and collected responses from\npast participants of five federally-funded internship programs and compare\nparticipant ratings of their prior experience to their internship experience.\nFindings: Our results indicate that participants improve CSE skills and domain\nscience knowledge, and are more interested in working at national labs.\nParticipants go on to degree programs and positions in relevant domain science\ntopics after their internships. Conclusions: We show that national laboratory\ninternships are an opportunity for students to build CSE skills that may not be\navailable at all institutions. We also show a growth in domain science skills\nduring their internships through direct exposure to research topics. The survey\ninstrument and approach used may be adapted to other studies to measure the\nimpact of postgraduate internships in multiple disciplines and internship\nsettings.", "comment": "In review at Journal of Engineering Education", "pdf_url": "http://arxiv.org/pdf/2501.10601v2", "cate": "cs.CY", "date": "2025-01-17", "updated": "2025-07-23", "AI": {"title_translation": "理解国家实验室研究生实习中计算科学与工程（CSE）和领域科学技能的发展", "tldr": "国家实验室实习帮助研究生提升计算科学与工程（CSE）和领域科学技能，并增加在国家实验室工作的兴趣，弥补了大学培训的不足。", "motivation": "当前科学发现和技术创新要求科学家和工程师精通领域科学和计算科学与工程（CSE），但大学很少能提供这方面的综合交叉培训及高性能计算设施。国家实验室的实习机会可能弥补这一技能培养的空白。", "method": "研究人员开发了一项调查问卷，并收集了五个联邦资助实习项目往期参与者的回复，对比了参与者在实习前后的经验评价。", "result": "研究结果表明，参与者提升了计算科学与工程（CSE）技能和领域科学知识，并对在国家实验室工作表现出更强的兴趣。实习结束后，参与者继续攻读相关领域科学的学位课程或从事相关职位。", "conclusion": "国家实验室实习为学生提供了在其他机构可能无法获得的计算科学与工程（CSE）技能培养机会。同时，通过直接接触研究课题，实习期间领域科学技能也得到了提升。研究中使用的调查工具和方法可适用于其他研究，以衡量研究生实习在不同学科和环境中的影响。", "translation": "背景：利用先进计算进行科学发现和技术创新需要科学家和工程师精通领域科学和计算科学与工程（CSE）。然而，很少有大学提供集成的领域科学/CSE交叉培训以及顶级500强高性能计算（HPC）设施。国家实验室提供能够培养这些技能的实习机会。目的：本研究评估了国家实验室联邦资助的研究生实习成果。本研究旨在回答三个问题：1）学生通过在选定的国家实验室实习，提高了哪些计算技能、研究技能和专业技能？2）学生通过实习是否获得了领域科学知识？3）学生在这些实习后职业兴趣是否发生变化？设计/方法：我们开发了一项调查，并收集了五个联邦资助实习项目的往期参与者的回复，比较了参与者对其先前经验和实习经验的评价。发现：我们的结果表明，参与者提高了CSE技能和领域科学知识，并且对在国家实验室工作更感兴趣。参与者在实习后继续攻读相关领域科学的学位课程和职位。结论：我们表明，国家实验室实习为学生提供了培养可能在所有机构都无法获得的CSE技能的机会。我们还通过直接接触研究课题，展示了他们在实习期间领域科学技能的增长。所使用的调查工具和方法可以适用于其他研究，以衡量研究生实习在多个学科和实习环境中的影响。", "summary": "本研究评估了国家实验室联邦资助的研究生实习项目，旨在探究其对学生计算、研究、专业技能、领域科学知识以及职业兴趣的影响。通过对往期参与者的调查，结果显示实习显著提升了学生的计算科学与工程（CSE）技能和领域科学知识，并增强了他们对国家实验室职业的兴趣，许多参与者随后进入相关领域的深造或就业。研究总结认为，国家实验室实习有效弥补了大学教育在CSE和领域科学培训方面的不足，提供了独特的技能发展机会。", "keywords": "计算科学与工程, 领域科学, 国家实验室, 实习, 技能发展", "comments": "该论文强调了大学教育在集成计算科学与工程（CSE）和领域科学培训方面存在的关键空白。它为国家实验室实习在解决这一空白方面的价值提供了实证证据，这对于科学计算领域的人才培养至关重要。基于参与者调查的方法学为评估此类项目提供了一种实用方法，其可移植性是一个优势。"}}
{"id": "2507.17487", "title": "CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)", "authors": ["Lorenzo Marconi", "Flavia Ricci", "Riccardo Rosati"], "categories": ["cs.AI", "cs.DB"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Extended version of paper accepted at the 24th International Semantic Web Conference (ISWC 2025)", "url": "http://arxiv.org/abs/2507.17487v1", "summary": "We investigate Controlled Query Evaluation (CQE) over ontologies, where\ninformation disclosure is regulated by epistemic dependencies (EDs), a family\nof logical rules recently proposed for the CQE framework. In particular, we\ncombine EDs with the notion of optimal GA censors, i.e. maximal sets of ground\natoms that are entailed by the ontology and can be safely revealed. We focus on\nanswering Boolean unions of conjunctive queries (BUCQs) with respect to the\nintersection of all optimal GA censors - an approach that has been shown in\nother contexts to ensure strong security guarantees with favorable\ncomputational behavior. First, we characterize the security of this\nintersection-based approach and identify a class of EDs (namely, full EDs) for\nwhich it remains safe. Then, for a subclass of EDs and for DL-Lite_R\nontologies, we show that answering BUCQs in the above CQE semantics is in AC^0\nin data complexity by presenting a suitable, detailed first-order rewriting\nalgorithm. Finally, we report on experiments conducted in two different\nevaluation scenarios, showing the practical feasibility of our rewriting\nfunction.", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17487v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "认知依赖下的受控查询评估：算法与实验（扩展版）", "tldr": "本文研究了在本体论中，结合认知依赖（EDs）和最优GA审查器进行受控查询评估（CQE），证明了其安全性、计算复杂性，并通过实验验证了算法的实际可行性。", "motivation": "在本体论中，信息披露需要通过认知依赖（EDs）进行规范。本文旨在将EDs与最优GA审查器结合，以实现具有强大安全保障和良好计算性能的受控查询评估（CQE）。", "method": "研究了本体论中受认知依赖（EDs）规范的信息披露的受控查询评估（CQE）。具体结合了EDs和最优GA审查器的概念。重点关注回答布尔合取查询（BUCQs）关于所有最优GA审查器的交集。首先，表征了这种基于交集的方法的安全性，并确定了其对一类EDs（即完全EDs）保持安全。然后，对于EDs的一个子类和DL-Lite_R本体，通过详细的一阶重写算法证明了上述CQE语义下回答BUCQs的数据复杂性为AC^0。", "result": "基于交集的方法对于一类EDs（即完全EDs）保持安全。对于EDs的一个子类和DL-Lite_R本体，回答BUCQs的数据复杂性为AC^0。实验结果表明，所提出的重写函数在实际评估场景中是可行的。", "conclusion": "本文研究了认知依赖下受控查询评估的安全性、计算复杂性及其算法的实际可行性，为本体论中的信息披露提供了一种有效的方法。", "translation": "我们研究了本体论中的受控查询评估（CQE），其中信息披露受到认知依赖（EDs）的规范，EDs是最近为CQE框架提出的一系列逻辑规则。特别是，我们将EDs与最优GA审查器的概念结合起来，即本体论所蕴含的、可以安全披露的最大原子集。我们专注于回答布尔合取查询（BUCQs）关于所有最优GA审查器的交集——这种方法在其他语境中已被证明能确保强大的安全保障和良好的计算行为。首先，我们表征了这种基于交集的方法的安全性，并确定了一类EDs（即完全EDs），对于它们，该方法仍然是安全的。然后，对于EDs的一个子类和DL-Lite_R本体，我们通过提出一个合适的、详细的一阶重写算法，表明在上述CQE语义下回答BUCQs的数据复杂性为AC^0。最后，我们报告了在两种不同评估场景中进行的实验，显示了我们重写函数的实际可行性。", "summary": "本研究探讨了在本体论中，受控查询评估（CQE）如何通过认知依赖（EDs）来规范信息披露。论文将EDs与最优GA审查器结合，专注于回答布尔合取查询（BUCQs）关于所有最优GA审查器的交集，这种方法在其他语境中已被证明能提供强大的安全保障和良好的计算性能。研究首先明确了该方法的安全性，并指出其对完全EDs类仍保持安全。接着，针对EDs的一个子类和DL-Lite_R本体，论文提出了一种详细的一阶重写算法，证明了在该CQE语义下回答BUCQs的数据复杂性为AC^0。最终，通过实验验证了所提重写函数在实践中的可行性。", "keywords": "受控查询评估, 认知依赖, 本体论, 数据复杂性, 一阶重写", "comments": "本文的创新点在于将认知依赖（EDs）与最优GA审查器相结合，为本体论中的受控查询评估（CQE）提供了一种新的方法。其重要性体现在理论上证明了该方法在特定条件下的安全性和较低的数据复杂性（AC^0），并通过实验验证了其在实际应用中的可行性，这对于需要严格信息披露控制的系统具有重要意义。"}}
{"id": "2507.17135", "title": "SADA: Stability-guided Adaptive Diffusion Acceleration", "authors": ["Ting Jiang", "Yixiao Wang", "Hancheng Ye", "Zishan Shao", "Jingwei Sun", "Jingyang Zhang", "Zekai Chen", "Jianyi Zhang", "Yiran Chen", "Hai Li"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted and published by ICML 2025. Code is available at: this https URL", "url": "http://arxiv.org/abs/2507.17135v1", "summary": "Diffusion models have achieved remarkable success in generative tasks but\nsuffer from high computational costs due to their iterative sampling process\nand quadratic attention costs. Existing training-free acceleration strategies\nthat reduce per-step computation cost, while effectively reducing sampling\ntime, demonstrate low faithfulness compared to the original baseline. We\nhypothesize that this fidelity gap arises because (a) different prompts\ncorrespond to varying denoising trajectory, and (b) such methods do not\nconsider the underlying ODE formulation and its numerical solution. In this\npaper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a\nnovel paradigm that unifies step-wise and token-wise sparsity decisions via a\nsingle stability criterion to accelerate sampling of ODE-based generative\nmodels (Diffusion and Flow-matching). For (a), SADA adaptively allocates\nsparsity based on the sampling trajectory. For (b), SADA introduces principled\napproximation schemes that leverage the precise gradient information from the\nnumerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using\nboth EDM and DPM++ solvers reveal consistent $\\ge 1.8\\times$ speedups with\nminimal fidelity degradation (LPIPS $\\leq 0.10$ and FID $\\leq 4.5$) compared to\nunmodified baselines, significantly outperforming prior methods. Moreover, SADA\nadapts seamlessly to other pipelines and modalities: It accelerates ControlNet\nwithout any modifications and speeds up MusicLDM by $1.8\\times$ with $\\sim\n0.01$ spectrogram LPIPS.", "comment": "Accepted and published by ICML 2025. Code is available at:\n  https://github.com/Ting-Justin-Jiang/sada-icml", "pdf_url": "http://arxiv.org/pdf/2507.17135v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SADA：稳定性引导的自适应扩散加速", "tldr": "SADA通过统一的稳定性标准来自适应地加速扩散模型的采样，显著提高了速度，同时保持了高保真度。", "motivation": "扩散模型虽然在生成任务上表现出色，但由于迭代采样过程和二次注意力成本，计算成本高昂。现有的无训练加速策略虽然能减少采样时间，但与原始基线相比，保真度较低。作者推测这是因为不同提示对应不同的去噪轨迹，并且现有方法未考虑底层的ODE公式及其数值解。", "method": "本文提出了稳定性引导的自适应扩散加速（SADA），这是一种新范式，通过单一的稳定性标准统一了步长和令牌稀疏性决策，以加速基于ODE的生成模型（扩散和流匹配）的采样。SADA根据采样轨迹自适应地分配稀疏性，并引入了利用数值ODE求解器精确梯度信息的原理性近似方案。", "result": "在SD-2、SDXL和Flux上使用EDM和DPM++求解器进行综合评估显示，与未修改的基线相比，SADA实现了持续的≥1.8倍加速，且保真度下降极小（LPIPS≤0.10，FID≤4.5），显著优于现有方法。此外，SADA能无缝适应其他管道和模态，例如无需修改即可加速ControlNet，并将MusicLDM加速1.8倍，频谱LPIPS约为0.01。", "conclusion": "SADA通过引入稳定性引导的自适应稀疏性分配和原理性近似方案，成功解决了扩散模型采样的计算成本和保真度权衡问题，实现了显著的加速，同时保持了高质量的生成效果。", "translation": "扩散模型在生成任务中取得了显著成功，但由于其迭代采样过程和二次注意力成本而面临高计算成本。现有的无训练加速策略虽然有效减少了采样时间，但与原始基线相比，保真度较低。我们假设这种保真度差距产生的原因是：(a) 不同的提示对应不同的去噪轨迹，以及 (b) 这些方法没有考虑底层的ODE公式及其数值解。在本文中，我们提出了稳定性引导的自适应扩散加速（SADA），这是一种新颖的范式，通过单一的稳定性标准统一了步长和令牌稀疏性决策，以加速基于ODE的生成模型（扩散和流匹配）的采样。针对 (a)，SADA 根据采样轨迹自适应地分配稀疏性。针对 (b)，SADA 引入了利用数值ODE求解器精确梯度信息的原理性近似方案。在SD-2、SDXL和Flux上使用EDM和DPM++求解器进行的综合评估显示，与未修改的基线相比，SADA 实现了持续的≥1.8倍加速，且保真度下降极小（LPIPS≤0.10和FID≤4.5），显著优于现有方法。此外，SADA 无缝适应其他管道和模态：它无需任何修改即可加速ControlNet，并将MusicLDM加速1.8倍，频谱LPIPS约为0.01。", "summary": "SADA提出了一种新的扩散模型加速范式，通过统一的稳定性标准自适应地分配步长和令牌稀疏性，以解决现有加速方法在计算成本和生成质量之间的权衡问题。该方法考虑了去噪轨迹的差异和ODE数值解的精确梯度信息，在多个模型和模态上实现了显著的采样加速（≥1.8倍），同时保持了极低的保真度损失，性能优于现有技术。", "keywords": "扩散模型, 加速采样, 稳定性引导, 自适应稀疏性, ODE求解器", "comments": "SADA的创新之处在于其统一的稳定性标准，能够同时进行步长和令牌的稀疏性决策，并且考虑了ODE公式的数值解。这使得它在加速扩散模型的同时，能够有效保持生成内容的保真度，解决了长期存在的速度与质量的权衡问题。其在不同模型和模态上的普适性也显示了其重要性。"}}
{"id": "2507.17416", "title": "Efficient and Robust Semantic Image Communication via Stable Cascade", "authors": ["Bilal Khalid", "Pedro Freire", "Sergei K. Turitsyn", "Jaroslaw E. Prilepsky"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 Workshop on Machine Learning for Wireless Communication and Networks (ML4Wireless)", "url": "http://arxiv.org/abs/2507.17416v1", "summary": "Diffusion Model (DM) based Semantic Image Communication (SIC) systems face\nsignificant challenges, such as slow inference speed and generation randomness,\nthat limit their reliability and practicality. To overcome these issues, we\npropose a novel SIC framework inspired by Stable Cascade, where extremely\ncompact latent image embeddings are used as conditioning to the diffusion\nprocess. Our approach drastically reduces the data transmission overhead,\ncompressing the transmitted embedding to just 0.29% of the original image size.\nIt outperforms three benchmark approaches - the diffusion SIC model conditioned\non segmentation maps (GESCO), the recent Stable Diffusion (SD)-based SIC\nframework (Img2Img-SC), and the conventional JPEG2000 + LDPC coding - by\nachieving superior reconstruction quality under noisy channel conditions, as\nvalidated across multiple metrics. Notably, it also delivers significant\ncomputational efficiency, enabling over 3x faster reconstruction for 512 x 512\nimages and more than 16x faster for 1024 x 1024 images as compared to the\napproach adopted in Img2Img-SC.", "comment": "Accepted at ICML 2025 Workshop on Machine Learning for Wireless\n  Communication and Networks (ML4Wireless)", "pdf_url": "http://arxiv.org/pdf/2507.17416v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过Stable Cascade实现高效鲁棒的语义图像通信", "tldr": "本文提出一种基于Stable Cascade的新型语义图像通信（SIC）框架，通过使用极紧凑的潜在图像嵌入作为扩散过程的条件，有效解决了现有扩散模型在SIC中面临的推理速度慢和生成随机性问题，实现了高压缩率、高重建质量和更快的速度。", "motivation": "扩散模型（DM）在语义图像通信（SIC）系统中面临推理速度慢和生成随机性大的挑战，这些问题限制了其可靠性和实用性。", "method": "提出一种受Stable Cascade启发的SIC框架，其中极紧凑的潜在图像嵌入被用作扩散过程的条件，以大幅减少数据传输开销。", "result": "将传输的嵌入数据压缩到原始图像大小的0.29%。在噪声信道条件下，重建质量优于GESCO、Img2Img-SC和JPEG2000 + LDPC等三种基准方法。计算效率显著提升，512x512图像的重建速度比Img2Img-SC快3倍以上，1024x1024图像快16倍以上。", "conclusion": "该方法在语义图像通信中实现了高效、鲁棒的图像传输和重建，有效解决了现有扩散模型面临的速度和可靠性问题。", "translation": "扩散模型（DM）在语义图像通信（SIC）系统中面临显著挑战，例如推理速度慢和生成随机性，这些问题限制了它们的可靠性和实用性。为了克服这些问题，我们提出了一种受Stable Cascade启发的新型SIC框架，其中极紧凑的潜在图像嵌入被用作扩散过程的条件。我们的方法大幅减少了数据传输开销，将传输的嵌入数据压缩到原始图像大小的0.29%。通过在多个指标上进行验证，结果显示在噪声信道条件下，它在重建质量方面优于三种基准方法——以分割图为条件的扩散SIC模型（GESCO）、最近基于Stable Diffusion的SIC框架（Img2Img-SC）以及传统的JPEG2000 + LDPC编码。值得注意的是，它还提供了显著的计算效率，与Img2Img-SC中采用的方法相比，512 x 512图像的重建速度提高了3倍以上，1024 x 1024图像的重建速度提高了16倍以上。", "summary": "本文提出一种基于Stable Cascade的新型语义图像通信（SIC）框架，通过使用极紧凑的潜在图像嵌入作为扩散过程的条件，有效解决了现有扩散模型在SIC中面临的推理速度慢和生成随机性问题。该方法显著降低了数据传输开销（仅为原始图像的0.29%），并在噪声信道条件下实现了优于现有基准方法的重建质量和计算效率，尤其在图像重建速度上有显著提升。", "keywords": "语义图像通信, 扩散模型, Stable Cascade, 图像压缩, 鲁棒性", "comments": "该论文创新性地将Stable Cascade的思想引入语义图像通信，通过优化数据表示和利用扩散模型特性，有效解决了传统DM在SIC应用中的效率和鲁棒性瓶颈。其在压缩率、重建质量和计算速度上的显著提升，展示了该方法在实际应用中的巨大潜力。"}}
{"id": "2507.17112", "title": "Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement", "authors": ["Yuhan Wang", "Qing Xie", "Zhifeng Bao", "Mengzi Tang", "Lin Li", "Yongjian Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17112v1", "summary": "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by\ntransferring knowledge across domains. Disentangled representation learning\nprovides an effective solution to model complex user preferences by separating\nintra-domain features (domain-shared and domain-specific features), thereby\nenhancing robustness and interpretability. However, disentanglement-based CDR\nmethods employing generative modeling or GNNs with contrastive objectives face\ntwo key challenges: (i) pre-separation strategies decouple features before\nextracting collaborative signals, disrupting intra-domain interactions and\nintroducing noise; (ii) unsupervised disentanglement objectives lack explicit\ntask-specific guidance, resulting in limited consistency and suboptimal\nalignment. To address these challenges, we propose DGCDR, a GNN-enhanced\nencoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to\nextract high-order collaborative signals, providing enriched representations as\na robust foundation for disentanglement. The encoder then dynamically\ndisentangles features into domain-shared and -specific spaces, preserving\ncollaborative information during the separation process. To handle challenge\n(ii), the decoder introduces an anchor-based supervision that leverages\nhierarchical feature relationships to enhance intra-domain consistency and\ncross-domain alignment. Extensive experiments on real-world datasets\ndemonstrate that DGCDR achieves state-of-the-art performance, with improvements\nof up to 11.59% across key metrics. Qualitative analyses further validate its\nsuperior disentanglement quality and transferability. Our source code and\ndatasets are available on GitHub for further comparison.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17112v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过监督解耦增强跨域推荐的可迁移性和一致性", "tldr": "DGCDR提出了一种基于GNN的编码器-解码器框架，通过先提取协作信号和引入锚点监督来解决现有跨域推荐中解耦表示学习的挑战，实现了最先进的性能。", "motivation": "现有的基于解耦的跨域推荐方法面临两大挑战：一是预分离策略在提取协作信号前解耦特征，破坏了域内交互并引入噪声；二是无监督解耦目标缺乏明确的任务特定指导，导致一致性有限和次优对齐。", "method": "本文提出了DGCDR，一个GNN增强的编码器-解码器框架。为解决挑战(i)，DGCDR首先应用GNN提取高阶协作信号，为解耦提供鲁棒基础；编码器动态解耦特征到域共享和域特定空间，在分离过程中保留协作信息。为解决挑战(ii)，解码器引入基于锚点的监督，利用分层特征关系增强域内一致性和跨域对齐。", "result": "在真实世界数据集上的大量实验表明，DGCDR实现了最先进的性能，关键指标提升高达11.59%。定性分析进一步验证了其卓越的解耦质量和可迁移性。", "conclusion": "DGCDR通过其创新的GNN增强编码器-解码器框架，有效解决了跨域推荐中解耦表示学习的挑战，显著提升了推荐系统的性能、解耦质量和可迁移性。", "translation": "跨域推荐（CDR）旨在通过跨域知识迁移来缓解数据稀疏性问题。解耦表示学习通过分离域内特征（域共享和域特定特征）提供了一种有效建模复杂用户偏好的解决方案，从而增强了鲁棒性和可解释性。然而，采用生成模型或结合对比目标的GNN的基于解耦的CDR方法面临两个关键挑战：(i) 预分离策略在提取协作信号之前解耦特征，破坏了域内交互并引入噪声；(ii) 无监督解耦目标缺乏明确的任务特定指导，导致一致性有限和次优对齐。为了解决这些挑战，我们提出了DGCDR，一个GNN增强的编码器-解码器框架。为了处理挑战(i)，DGCDR首先应用GNN提取高阶协作信号，提供丰富的表示作为解耦的稳健基础。编码器随后将特征动态解耦到域共享和域特定空间，在分离过程中保留协作信息。为了处理挑战(ii)，解码器引入了一种基于锚点的监督，利用分层特征关系来增强域内一致性和跨域对齐。在真实世界数据集上的大量实验表明，DGCDR实现了最先进的性能，关键指标提升高达11.59%。定性分析进一步验证了其卓越的解耦质量和可迁移性。我们的源代码和数据集已在GitHub上提供，以供进一步比较。", "summary": "本文提出DGCDR，一个GNN增强的编码器-解码器框架，用于解决跨域推荐中解耦表示学习的挑战。针对现有方法在特征预分离和无监督解耦方面的不足，DGCDR首先利用GNN提取高阶协作信号，然后动态解耦特征；同时，引入锚点监督来增强域内一致性和跨域对齐。实验证明DGCDR在性能、解耦质量和可迁移性方面均达到最先进水平。", "keywords": "跨域推荐, 解耦表示学习, 图神经网络, 监督解耦, 数据稀疏性", "comments": "该论文的创新点在于提出了DGCDR框架，通过结合GNN提取协作信号和引入锚点监督，有效解决了现有解耦方法在跨域推荐中遇到的两大难题。其方法论逻辑清晰，通过先提取信息再解耦，并辅以监督指导，显著提升了模型的性能和可解释性。实验结果也验证了其优越性。"}}
{"id": "2503.11937", "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV'25, The project page is available at this https URL", "url": "http://arxiv.org/abs/2503.11937v3", "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.", "comment": "ICCV'25, The project page is available at\n  https://tri-mac.github.io/att-adapter/", "pdf_url": "http://arxiv.org/pdf/2503.11937v3", "cate": "cs.CV", "date": "2025-03-15", "updated": "2025-07-23", "AI": {"title_translation": "Att-Adapter：一种通过条件变分自编码器实现的鲁棒精确领域特定多属性T2I扩散适配器", "tldr": "Att-Adapter是一种新的即插即用模块，通过条件变分自编码器，使预训练的T2I扩散模型能够精确控制连续多属性，且无需配对数据。", "motivation": "现有文本到图像（T2I）扩散模型在生成高质量图像方面表现出色，但在新领域中（例如，眼睛睁开程度或汽车宽度等数值）通过纯文本指导精确控制连续属性，尤其是同时控制多个属性，仍然是一个重大挑战。", "method": "本文引入了Att-Adapter，一个新颖的即插即用模块，旨在实现预训练扩散模型中的细粒度、多属性控制。该方法从一组非配对且包含多个视觉属性的样本图像中学习单个控制适配器。Att-Adapter利用解耦的交叉注意力模块自然地将多个领域属性与文本条件相结合。此外，为了减轻过拟合并匹配视觉世界的多样性，Att-Adapter中引入了条件变分自编码器（CVAE）。", "result": "在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线方法。此外，该方法实现了更广泛的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。Att-Adapter训练时不需要配对合成数据，并且易于扩展到单个模型中的多个属性。", "conclusion": "Att-Adapter有效解决了T2I扩散模型中多属性精确控制的挑战，提供了一种鲁棒、灵活且无需配对数据训练的解决方案，并在控制范围和属性解耦方面超越了现有方法。", "translation": "文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著的性能。然而，在新领域（例如，眼睛睁开程度或汽车宽度等数值）中，通过纯文本指导实现对连续属性，特别是同时对多个属性的精确控制，仍然是一个重大挑战。为了解决这个问题，我们引入了属性（Att）适配器，这是一种新颖的即插即用模块，旨在使预训练的扩散模型能够进行细粒度的多属性控制。我们的方法从一组可以是非配对且包含多个视觉属性的样本图像中学习单个控制适配器。Att-Adapter利用解耦的交叉注意力模块，自然地将多个领域属性与文本条件相结合。我们进一步在Att-Adapter中引入了条件变分自编码器（CVAE），以减轻过拟合，匹配视觉世界的不同性质。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线方法。此外，我们的方法实现了更广泛的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter非常灵活，训练时不需要配对合成数据，并且易于在单个模型中扩展到多个属性。", "summary": "Att-Adapter是一种新颖的即插即用模块，旨在解决文本到图像（T2I）扩散模型在多属性精确控制方面的挑战。它通过学习单个控制适配器，利用解耦的交叉注意力模块整合多领域属性与文本条件，并引入条件变分自编码器（CVAE）以提高鲁棒性。该方法在不需要配对合成数据的情况下，实现了对连续多属性的细粒度控制，并在控制范围和属性解耦方面超越了现有的LoRA和StyleGAN基线方法。", "keywords": "T2I扩散模型, 多属性控制, Att-Adapter, 条件变分自编码器, 领域特定", "comments": "Att-Adapter的创新性在于其即插即用设计和对条件变分自编码器（CVAE）的引入，有效提升了T2I模型对连续多属性的精确控制能力。其无需配对数据训练的特点极大地降低了数据准备的门槛，使其在实际应用中更具吸引力。此外，其在控制范围和属性解耦方面的优势，使其成为多属性T2I生成领域的一个重要进展。"}}
{"id": "2507.16983", "title": "Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals", "authors": ["Sonny T. Jones", "Grange M. Simpson", "Patrick M. Pilarski", "Ashley N. Dalrymple"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures, accepted at the 6th Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM2025), June 11-14, 2025", "url": "http://arxiv.org/abs/2507.16983v1", "summary": "Rehabilitation technology is a natural setting to study the shared learning\nand decision-making of human and machine agents. In this work, we explore the\nuse of Hierarchical Reinforcement Learning (HRL) to develop adaptive control\nstrategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy\nfor individuals with motor impairments. Inspired by prominent models of\nbiological sensorimotor processing, our investigated HRL approach breaks down\nthe complex task of exoskeleton control adaptation into a higher-level\nframework for terrain strategy adaptation and a lower-level framework for\nproviding predictive information; this latter element is implemented via the\ncontinual learning of general value functions (GVFs). GVFs generated temporal\nabstractions of future signal values from multiple wearable lower-limb sensors,\nincluding electromyography, pressure insoles, and goniometers. We investigated\ntwo methods for incorporating actual and predicted sensor signals into a policy\nnetwork with the intent to improve the decision-making capacity of the control\nsystem of a lower-limb exoskeleton during ambulation across varied terrains. As\na key result, we found that the addition of predictions made from GVFs\nincreased overall network accuracy. Terrain-specific performance increases were\nseen while walking on even ground, uneven ground, up and down ramps, and turns,\nterrains that are often misclassified without predictive information. This\nsuggests that predictive information can aid decision-making during\nuncertainty, e.g., on terrains that have a high chance of being misclassified.\nThis work, therefore, contributes new insights into the nuances of HRL and the\nfuture development of exoskeletons to facilitate safe transitioning and\ntraversing across different walking environments.", "comment": "5 pages, 3 figures, accepted at the 6th Multi-disciplinary Conference\n  on Reinforcement Learning and Decision Making (RLDM2025), June 11-14, 2025", "pdf_url": "http://arxiv.org/pdf/2507.16983v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "用于自适应行走控制的分层强化学习框架，利用下肢传感器信号的广义价值函数", "tldr": "本研究探索了利用分层强化学习（HRL）和广义价值函数（GVFs）为下肢外骨骼开发自适应控制策略，以增强运动障碍患者的移动能力，并发现GVFs生成的预测信息能显著提高在复杂地形上的决策准确性。", "motivation": "康复技术是研究人机共享学习和决策的天然场景。本工作旨在探索使用分层强化学习（HRL）为下肢外骨骼开发自适应控制策略，以增强运动障碍患者的移动能力和自主性。", "method": "受生物感觉运动处理模型的启发，本研究采用了一种分层强化学习（HRL）方法。该方法将复杂的外骨骼控制适应任务分解为用于地形策略适应的高层框架和用于提供预测信息的低层框架。低层框架通过持续学习广义价值函数（GVFs）来实现，GVFs从多个可穿戴下肢传感器（包括肌电图、压力鞋垫和测角计）生成未来信号值的时序抽象。研究还探讨了两种将实际和预测传感器信号整合到策略网络中的方法，以提高下肢外骨骼在不同地形行走时的控制系统决策能力。", "result": "关键结果显示，加入由GVFs生成的预测信息显著提高了整体网络准确性。在平坦地面、不平坦地面、上下坡以及转弯等地形上，都观察到了特定地形的性能提升，这些地形在没有预测信息的情况下经常被错误分类。", "conclusion": "预测信息可以在不确定性条件下（例如，在容易被错误分类的地形上）辅助决策。这项工作为分层强化学习的细微之处以及未来外骨骼的发展提供了新见解，有助于实现在不同行走环境中的安全过渡和穿越。", "translation": "康复技术是研究人机共享学习和决策的天然场景。在这项工作中，我们探索了使用分层强化学习（HRL）为下肢外骨骼开发自适应控制策略，旨在增强运动障碍患者的移动能力和自主性。受生物感觉运动处理突出模型的启发，我们研究的HRL方法将复杂的外骨骼控制适应任务分解为用于地形策略适应的高层框架和用于提供预测信息的低层框架；后一个元素通过广义价值函数（GVFs）的持续学习来实现。GVFs从多个可穿戴下肢传感器，包括肌电图、压力鞋垫和测角计，生成未来信号值的时序抽象。我们研究了两种将实际和预测传感器信号整合到策略网络中的方法，旨在提高下肢外骨骼在不同地形行走时控制系统的决策能力。作为一个关键结果，我们发现添加由GVFs生成的预测信息提高了整体网络准确性。在平坦地面、不平坦地面、上下坡以及转弯等地形上都观察到了特定地形的性能提升，这些地形在没有预测信息的情况下经常被错误分类。这表明预测信息可以在不确定性条件下辅助决策，例如在有很高几率被错误分类的地形上。因此，这项工作为HRL的细微之处以及未来外骨骼的发展贡献了新见解，以促进在不同行走环境中的安全过渡和穿越。", "summary": "本研究提出了一种分层强化学习（HRL）框架，用于下肢外骨骼的自适应行走控制。该框架将控制任务分解为地形策略适应和基于广义价值函数（GVFs）的预测信息提供。GVFs从多传感器数据生成未来信号预测，并通过将实际和预测信号整合到策略网络中。实验结果表明，GVFs提供的预测信息显著提高了外骨骼在各种复杂地形（如不平坦地面、坡道和转弯）上的决策准确性和性能，证明了预测信息在外骨骼控制中辅助不确定性决策的潜力。", "keywords": "分层强化学习, 广义价值函数, 外骨骼, 自适应控制, 传感器信号", "comments": "该论文的创新点在于将分层强化学习与广义价值函数（GVFs）相结合，用于外骨骼的自适应控制。通过利用GVFs生成预测性的传感器信号信息，系统能够更好地应对复杂和不确定地形，显著提高了决策准确性。这对于康复技术和辅助设备的发展具有重要意义，尤其是在提高运动障碍患者的自主性和安全性方面。未来的工作可能会探索更复杂的环境适应性以及与用户意图的更深层次融合。"}}
{"id": "2507.14792", "title": "SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors", "authors": ["Kaixin Ji", "Danula Hettiachchi", "Falk Scholer", "Flora D. Salim", "Damiano Spina"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted in Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), September 2025", "url": "http://arxiv.org/abs/2507.14792v2", "summary": "Information processing tasks involve complex cognitive mechanisms that are\nshaped by various factors, including individual goals, prior experience, and\nsystem environments. Understanding such behaviors requires a sophisticated and\npersonalized data capture of how one interacts with modern information systems\n(e.g., web search engines). Passive sensors, such as wearables, capturing\nphysiological and behavioral data, have the potential to provide solutions in\nthis context. This paper presents a novel dataset, SenseSeek, designed to\nevaluate the effectiveness of consumer-grade sensors in a complex information\nprocessing scenario: searching via systems (e.g., search engines), one of the\ncommon strategies users employ for information seeking. The SenseSeek dataset\ncomprises data collected from 20 participants, 235 trials of the stimulated\nsearch process, 940 phases of stages in the search process, including the\nrealization of Information Need (IN), Query Formulation (QF), Query Submission\nby Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)\nor Listening (RJ-L). The data includes Electrodermal Activities (EDA),\nElectroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured\nusing consumer-grade sensors. It also contains 258 features extracted from the\nsensor data, the gaze-annotated screen recordings, and task responses. We\nvalidate the usefulness of the dataset by providing baseline analysis on the\nimpacts of different cognitive intents and interaction modalities on the sensor\ndata, and effectiveness of the data in discriminating the search stages. To our\nknowledge, SenseSeek is the first dataset that characterizes the multiple\nstages involved in information seeking with physiological signals collected\nfrom multiple sensors. We hope this dataset can serve as a reference for future\nresearch on information-seeking behaviors.", "comment": "Accepted in Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (IMWUT), September 2025", "pdf_url": "http://arxiv.org/pdf/2507.14792v2", "cate": "cs.HC", "date": "2025-07-20", "updated": "2025-07-23", "AI": {"title_translation": "SenseSeek数据集：多模态感知研究信息搜寻行为", "tldr": "SenseSeek是一个新的多模态数据集，利用消费级传感器研究信息搜寻行为的复杂阶段，包含生理和行为数据，并通过基线分析验证了其在区分搜索阶段方面的有效性。", "motivation": "信息处理任务涉及复杂的认知机制，受多种因素影响。理解这些行为需要对用户如何与现代信息系统（如网络搜索引擎）互动进行复杂且个性化的数据捕获。被动传感器（如可穿戴设备）捕获的生理和行为数据有望提供解决方案。", "method": "本文提出了一个名为SenseSeek的新型数据集，旨在评估消费级传感器在复杂信息处理场景（通过系统搜索）中的有效性。该数据集包含从20名参与者、235次模拟搜索过程试验中收集的数据，涵盖信息需求（IN）、查询制定（QF）、通过打字（QS-T）或说话（QS-S）提交查询以及通过阅读（RJ-R）或听力（RJ-L）判断相关性等940个搜索阶段。数据包括通过消费级传感器捕获的皮肤电活动（EDA）、脑电图（EEG）、瞳孔（PUPIL）、注视（GAZE）和运动（MOTION）数据，以及从传感器数据、注视标注的屏幕录像和任务响应中提取的258个特征。", "result": "通过对不同认知意图和交互模式对传感器数据的影响，以及数据在区分搜索阶段方面的有效性进行基线分析，验证了该数据集的实用性。", "conclusion": "SenseSeek是首个利用多传感器收集的生理信号来表征信息搜寻所涉及的多个阶段的数据集。该数据集有望为未来信息搜寻行为研究提供参考。", "translation": "信息处理任务涉及复杂的认知机制，这些机制受到各种因素的影响，包括个人目标、先验经验和系统环境。理解此类行为需要对人们如何与现代信息系统（例如网络搜索引擎）互动进行复杂且个性化的数据捕获。被动传感器，例如可穿戴设备，捕获生理和行为数据，有潜力在此背景下提供解决方案。本文提出了一个新颖的数据集SenseSeek，旨在评估消费级传感器在复杂信息处理场景中的有效性：通过系统（例如搜索引擎）进行搜索，这是用户采用的常见信息搜寻策略之一。SenseSeek数据集包含从20名参与者、235次模拟搜索过程试验中收集的数据，涵盖搜索过程中的940个阶段，包括信息需求（IN）的实现、查询制定（QF）、通过打字（QS-T）或说话（QS-S）提交查询，以及通过阅读（RJ-R）或听力（RJ-L）判断相关性。数据包括皮肤电活动（EDA）、脑电图（EEG）、瞳孔、注视和运动数据，这些数据均使用消费级传感器捕获。它还包含从传感器数据、注视标注的屏幕录像和任务响应中提取的258个特征。我们通过对不同认知意图和交互模式对传感器数据的影响，以及数据在区分搜索阶段方面的有效性进行基线分析，验证了数据集的实用性。据我们所知，SenseSeek是首个利用从多个传感器收集的生理信号来表征信息搜寻所涉及的多个阶段的数据集。我们希望该数据集能为未来信息搜寻行为的研究提供参考。", "summary": "SenseSeek数据集是一个新颖的多模态数据集，旨在通过消费级传感器收集生理和行为数据来研究复杂的信息搜寻行为。该数据集包含20名参与者在235次模拟搜索试验中，涵盖信息需求、查询制定和相关性判断等940个阶段的数据，包括EDA、EEG、PUPIL、GAZE和MOTION数据，并提取了258个特征。通过基线分析，该数据集被验证在理解认知意图、交互模式以及区分搜索阶段方面具有实用性。SenseSeek是首个利用多传感器生理信号表征信息搜寻多阶段的数据集，有望促进未来相关研究。", "keywords": "信息搜寻, 多模态感知, SenseSeek数据集, 生理信号, 认知行为", "comments": "SenseSeek数据集的创新之处在于其首次利用多传感器收集的生理信号来表征信息搜寻的多个阶段，并且使用了消费级传感器，这降低了研究门槛，使其更具普适性。该数据集为理解信息搜寻行为背后的复杂认知机制提供了宝贵的资源，对人机交互、信息科学和认知神经科学等领域的研究具有重要意义。"}}
{"id": "2505.01709", "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation", "authors": ["Kaidong Zhang", "Rongtao Xu", "Pengzhen Ren", "Junfan Lin", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      project page: this https URL", "url": "http://arxiv.org/abs/2505.01709v3", "summary": "Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.", "comment": "project page: https://abliao.github.io/RoBridge/", "pdf_url": "http://arxiv.org/pdf/2505.01709v3", "cate": "cs.RO", "date": "2025-05-03", "updated": "2025-07-23", "AI": {"title_translation": "RoBridge：连接认知与执行的通用机器人操作分层架构", "tldr": "RoBridge是一个分层架构，通过结合大型视觉语言模型和强化学习，解决了通用机器人操作中认知与执行之间的鸿沟，并在新任务和虚实迁移中表现出色。", "motivation": "在开放式场景中操作机器人面临“程序技能困境”和“声明性技能困境”，现有方法通常在认知和执行能力之间做出妥协。", "method": "提出RoBridge，一个分层智能架构，包含：1. 基于大型预训练视觉语言模型（VLM）的高级认知规划器（HCP）；2. 作为符号桥梁的不变可操作表示（IOR）；3. 通用具身代理（GEA）。它保持VLM的声明性技能并释放强化学习的程序技能。", "result": "RoBridge在新任务上实现了75%的成功率，在虚实迁移中平均成功率达到83%（每任务仅使用5个真实世界数据样本），表现优于现有基线。", "conclusion": "这项工作代表了在机器人系统中整合认知推理与物理执行的重要一步，为通用机器人操作提供了一种新范式。", "translation": "标题：RoBridge：连接认知与执行的通用机器人操作分层架构\n摘要：在开放式场景中执行多样化任务的机器人操作是机器人领域一个重要的研究和应用方向。尽管自然语言处理和大型多模态模型在增强机器人理解复杂指令方面取得了最新进展，但机器人操作在开放环境中仍面临程序技能困境和声明性技能困境。现有方法通常在认知和执行能力之间做出妥协。为了解决这些挑战，本文提出了RoBridge，一个用于通用机器人操作的分层智能架构。它由一个基于大型预训练视觉语言模型（VLM）的高级认知规划器（HCP）、一个作为符号桥梁的不变可操作表示（IOR）以及一个通用具身代理（GEA）组成。RoBridge保持了VLM的声明性技能，并释放了强化学习的程序技能，有效弥合了认知与执行之间的鸿沟。RoBridge在现有基线上表现出显著的性能提升，在新任务上实现了75%的成功率，在虚实迁移中平均成功率达到83%（每任务仅使用五个真实世界数据样本）。这项工作代表了在机器人系统中整合认知推理与物理执行的重要一步，为通用机器人操作提供了一种新范式。", "summary": "本文提出了RoBridge，一种分层智能架构，旨在解决通用机器人操作中认知与执行之间的鸿沟。RoBridge通过结合基于大型视觉语言模型的高级认知规划器、不变可操作表示和通用具身代理，实现了VLM的声明性技能与强化学习的程序技能的有效整合。实验结果表明，RoBridge在新任务上表现出色，并在虚实迁移中取得了显著的成功率提升。", "keywords": "机器人操作, 分层架构, 认知与执行, 视觉语言模型, 强化学习", "comments": "RoBridge的创新之处在于其分层架构，有效地结合了大型视觉语言模型的认知能力和强化学习的执行能力，弥合了机器人操作中的认知与执行鸿沟。其在数据效率（每任务仅需5个真实样本）和虚实迁移方面的表现尤为突出，为通用机器人操作提供了一个有前景的新范式。"}}
{"id": "2412.14382", "title": "Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem", "authors": ["Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "categories": ["cs.AI", "cs.LG", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.14382v3", "summary": "Mixed-integer programming (MIP) is a powerful paradigm for modeling and\nsolving various important combinatorial optimization problems. Recently,\nlearning-based approaches have shown a potential to speed up MIP solving via\noffline training that then guides important design decisions during the search.\nHowever, a significant drawback of these methods is their heavy reliance on\noffline training, which requires collecting training datasets and\ncomputationally costly training epochs yet offering only limited generalization\nto unseen (larger) instances. In this paper, we propose Balans, an adaptive\nmeta-solver for MIPs with online learning capability that does not require any\nsupervision or apriori training. At its core, Balans is based on adaptive\nlarge-neighborhood search, operating on top of an MIP solver by successive\napplications of destroy and repair neighborhood operators. During the search,\nthe selection among different neighborhood definitions is guided on the fly for\nthe instance at hand via multi-armed bandit algorithms. Our extensive\nexperiments on hard optimization instances show that Balans offers significant\nperformance gains over the default MIP solver, is better than committing to any\nsingle best neighborhood, and improves over the state-of-the-art\nlarge-neighborhood search for MIPs. Finally, we release Balans as a highly\nconfigurable, MIP solver agnostic, open-source software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.14382v3", "cate": "cs.AI", "date": "2024-12-18", "updated": "2025-07-23", "AI": {"title_translation": "Balans：基于多臂老虎机的混合整数规划问题自适应大邻域搜索", "tldr": "Balans是一种无需离线训练的自适应元求解器，它使用多臂老虎机算法在线学习，显著提升了混合整数规划（MIP）的求解性能，优于现有方法并已开源。", "motivation": "现有基于学习的混合整数规划（MIP）求解方法严重依赖离线训练，这需要收集训练数据集和计算成本高昂的训练周期，且对未见实例（尤其是更大规模的实例）的泛化能力有限。", "method": "本文提出了Balans，一种具有在线学习能力的混合整数规划（MIP）自适应元求解器，无需任何监督或预训练。Balans的核心是自适应大邻域搜索，通过连续应用破坏和修复邻域操作符在MIP求解器之上运行。在搜索过程中，不同邻域定义的选择由多臂老虎机算法根据当前实例实时指导。", "result": "在困难优化实例上的广泛实验表明，Balans比默认的MIP求解器提供了显著的性能提升，优于仅使用单一最佳邻域的方法，并改进了目前最先进的MIP大邻域搜索方法。Balans已作为一个高度可配置、与MIP求解器无关的开源软件发布。", "conclusion": "Balans是一种有效的基于在线学习的MIP自适应元求解器，它克服了离线训练的局限性，并实现了卓越的性能。", "translation": "混合整数规划（MIP）是一种强大的范式，用于建模和解决各种重要的组合优化问题。最近，基于学习的方法显示出通过离线训练加速MIP求解的潜力，从而在搜索过程中指导重要的设计决策。然而，这些方法的一个显著缺点是它们严重依赖离线训练，这需要收集训练数据集和计算成本高昂的训练周期，但对未见（更大规模的）实例的泛化能力有限。在本文中，我们提出了Balans，一种具有在线学习能力的MIP自适应元求解器，它不需要任何监督或先验训练。Balans的核心是自适应大邻域搜索，通过连续应用破坏和修复邻域操作符在MIP求解器之上运行。在搜索过程中，不同邻域定义的选择由多臂老虎机算法根据当前实例实时指导。我们在困难优化实例上的广泛实验表明，Balans比默认的MIP求解器提供了显著的性能提升，优于仅使用单一最佳邻域的方法，并改进了目前最先进的MIP大邻域搜索方法。最后，我们将Balans作为一个高度可配置、与MIP求解器无关的开源软件发布。", "summary": "本文提出了Balans，一个针对混合整数规划（MIP）的自适应元求解器，旨在解决现有基于学习方法中离线训练的局限性。Balans采用自适应大邻域搜索，并利用多臂老虎机算法进行在线学习，以实时指导邻域操作的选择。实验结果表明，Balans在求解困难优化问题时，相比默认MIP求解器和最先进的大邻域搜索方法，展现出显著的性能提升，且无需预训练即可实现更好的泛化能力。Balans已作为开源软件发布。", "keywords": "混合整数规划, 大邻域搜索, 多臂老虎机, 在线学习, 元求解器", "comments": "该论文的创新之处在于其采用多臂老虎机算法进行在线学习，从而避免了传统基于学习方法中离线训练的弊端，并显著提高了对未见实例的泛化能力。Balans作为一个与MIP求解器无关的开源软件发布，也大大增加了其实用性和可推广性。"}}
{"id": "2412.17252", "title": "A Coalition Game for On-demand Multi-modal 3D Automated Delivery System", "authors": ["Farzan Moosavi", "Bilal Farooq"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.17252v2", "summary": "We introduce a multi-modal autonomous delivery optimization framework as a\ncoalition game for a fleet of UAVs and ADRs operating in two overlaying\nnetworks to address last-mile delivery in urban environments, including\nhigh-density areas and time-critical applications. The problem is defined as\nmultiple depot pickup and delivery with time windows constrained over\noperational restrictions, such as vehicle battery limitation, precedence time\nwindow, and building obstruction. Utilizing the coalition game theory, we\ninvestigate cooperation structures among the modes to capture how strategic\ncollaboration can improve overall routing efficiency. To do so, a generalized\nreinforcement learning model is designed to evaluate the cost-sharing and\nallocation to different modes to learn the cooperative behaviour with respect\nto various realistic scenarios. Our methodology leverages an end-to-end deep\nmulti-agent policy gradient method augmented by a novel spatio-temporal\nadjacency neighbourhood graph attention network using a heterogeneous\nedge-enhanced attention model and transformer architecture. Several numerical\nexperiments on last-mile delivery applications have been conducted, showing the\nresults from the case study in the city of Mississauga, which shows that\ndespite the incorporation of an extensive network in the graph for two modes\nand a complex training structure, the model addresses realistic operational\nconstraints and achieves high-quality solutions compared with the existing\ntransformer-based and classical methods. It can perform well on non-homogeneous\ndata distribution, generalizes well on different scales and configurations, and\ndemonstrates a robust cooperative performance under stochastic scenarios across\nvarious tasks, which is effectively reflected by coalition analysis and cost\nallocation to signify the advantage of cooperation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.17252v2", "cate": "cs.LG", "date": "2024-12-23", "updated": "2025-07-22", "AI": {"title_translation": "按需多模式3D自动化配送系统的联盟博弈", "tldr": "论文提出一个基于联盟博弈的多模式自动配送优化框架，利用深度强化学习和图注意力网络解决城市最后一公里配送问题，并证明了合作的优势。", "motivation": "解决城市环境中（包括高密度区域和时间敏感应用）最后一公里配送问题，通过多模式合作提高整体路由效率。", "method": "引入一个多模式自主配送优化框架，将其定义为联盟博弈。利用广义强化学习模型评估成本分摊和分配，以学习合作行为。方法论采用端到端深度多智能体策略梯度方法，并辅以新颖的时空邻接邻域图注意力网络，该网络使用异构边缘增强注意力模型和Transformer架构。", "result": "该模型能够处理现实操作限制并获得高质量解决方案，优于现有基于Transformer和经典方法。它在非同质数据分布上表现良好，在不同规模和配置下泛化能力强，并在各种任务的随机场景下表现出鲁棒的合作性能，通过联盟分析和成本分配有效反映了合作的优势。在密西沙加市的案例研究中展示了结果。", "conclusion": "尽管模型网络和训练结构复杂，但它能有效处理现实操作约束并实现高质量解决方案，证明了多模式合作在按需自动配送系统中的显著优势。", "translation": "我们引入了一个多模式自主配送优化框架，将其作为无人机和自动驾驶机器人车队在两个重叠网络中运行的联盟博弈，以解决城市环境中的最后一公里配送问题，包括高密度区域和时间敏感应用。该问题被定义为多站点取送货，受时间窗约束以及操作限制，如车辆电池限制、先行时间窗和建筑物障碍。利用联盟博弈论，我们研究了模式间的合作结构，以捕捉战略协作如何提高整体路由效率。为此，设计了一个广义强化学习模型来评估成本分摊和分配给不同模式，以学习各种现实场景下的合作行为。我们的方法利用了端到端深度多智能体策略梯度方法，并辅以新颖的时空邻接邻域图注意力网络，该网络使用异构边缘增强注意力模型和Transformer架构。针对最后一公里配送应用进行了多项数值实验，展示了密西沙加市案例研究的结果，表明尽管在图中为两种模式整合了广泛的网络和复杂的训练结构，该模型仍能解决现实操作约束并实现高质量解决方案，优于现有基于Transformer和经典方法。它可以在非同质数据分布上表现良好，在不同规模和配置下泛化能力强，并在各种任务的随机场景下表现出鲁棒的合作性能，这通过联盟分析和成本分配有效反映出来，以表明合作的优势。", "summary": "本文提出了一个创新的多模式自主配送优化框架，该框架将无人机和自动驾驶机器人车队的协同操作建模为联盟博弈，旨在解决城市最后一公里配送问题。通过结合广义强化学习和新颖的时空图注意力网络，该方法有效地处理了复杂的现实操作约束，并在多个实验中展示了卓越的性能和泛化能力，证明了模式间合作在提高配送效率和应对随机性方面的显著优势。", "keywords": "联盟博弈, 多模式配送, 深度强化学习, 最后一公里配送, 图注意力网络", "comments": "这篇论文的创新点在于将多模式自动配送问题建模为联盟博弈，并结合了先进的深度强化学习和图注意力网络来解决复杂的现实约束。其提出的时空邻接邻域图注意力网络和异构边缘增强注意力模型是技术亮点。该研究对于未来城市智能物流系统的发展具有重要意义，尤其是在提高效率和应对复杂环境方面。"}}
{"id": "2507.17320", "title": "EventLines: Time Compression for Discrete Event Timelines", "authors": ["Yuet Ling Wong", "Niklas Elmqvist"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17320v1", "summary": "Discrete event sequences serve as models for numerous real-world datasets,\nincluding publications over time, project milestones, and medication dosing\nduring patient treatments. These event sequences typically exhibit bursty\nbehavior, where events cluster together in rapid succession, interspersed with\nperiods of inactivity. Standard timeline charts with linear time axes fail to\nadequately represent such data, resulting in cluttered regions during event\nbursts while leaving other areas unutilized. We introduce EventLines, a novel\ntechnique that dynamically adjusts the time scale to match the underlying event\ndistribution, enabling more efficient use of screen space. To address the\nchallenges of non-linear time scaling, EventLines employs the time axis's\nvisual representation itself to communicate the varying scale. We present\nfindings from a crowdsourced graphical perception study that examines how\ndifferent time scale representations influence temporal perception.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17320v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "事件线：离散事件时间线的时序压缩", "tldr": "EventLines是一种新的时间线技术，通过动态调整时间尺度来有效显示具有爆发性行为的离散事件序列，并解决了非线性时间尺度下的感知挑战。", "motivation": "离散事件序列数据通常表现出爆发性行为，即事件在短时间内密集出现，然后是长时间的非活跃期。标准时间线图使用线性时间轴，无法充分表示此类数据，导致事件爆发时区域混乱，而其他区域未被充分利用。", "method": "引入了EventLines技术，通过动态调整时间尺度以匹配底层事件分布，从而更有效地利用屏幕空间。为了解决非线性时间尺度带来的挑战，EventLines利用时间轴的视觉表示本身来传达不同的尺度。", "result": "进行了一项众包图形感知研究，考察了不同的时间尺度表示如何影响时间感知，并展示了研究结果。", "conclusion": "EventLines提供了一种有效解决离散事件时间线中爆发性数据可视化挑战的新方法，通过动态时间尺度调整和视觉表示来改善屏幕空间利用和时间感知。", "translation": "离散事件序列是许多真实世界数据集的模型，包括随时间变化的出版物、项目里程碑和患者治疗期间的药物剂量。这些事件序列通常表现出爆发性行为，即事件在短时间内迅速聚集，期间穿插着不活跃期。使用线性时间轴的标准时间线图无法充分表示此类数据，导致事件爆发时区域混乱，而其他区域未被充分利用。我们引入了EventLines，这是一种新颖的技术，可以动态调整时间尺度以匹配底层事件分布，从而更有效地利用屏幕空间。为了解决非线性时间尺度带来的挑战，EventLines利用时间轴的视觉表示本身来传达不同的尺度。我们展示了一项众包图形感知研究的结果，该研究考察了不同的时间尺度表示如何影响时间感知。", "summary": "EventLines是一种创新技术，旨在解决标准时间线图在表示具有爆发性行为的离散事件序列时遇到的问题。通过动态调整时间尺度以适应事件分布，EventLines能更有效地利用屏幕空间，避免了传统线性时间轴导致的拥挤和空白区域。该方法通过时间轴本身的视觉表示来传达尺度的变化，并通过一项众包研究验证了不同时间尺度表示对时间感知的影响。", "keywords": "离散事件序列, 时间线可视化, 时间压缩, 爆发性数据, 感知研究", "comments": "这篇论文提出了一种新颖的时间线可视化方法EventLines，解决了传统线性时间轴在处理爆发性离散事件数据时的局限性。其创新点在于动态调整时间尺度以优化屏幕空间利用，并利用时间轴本身的视觉特性来传达这种非线性变化。通过用户感知研究来验证其有效性，增加了研究的严谨性。"}}
{"id": "2507.17121", "title": "Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation", "authors": ["Faisal Ahmed", "Mohammad Alfrad Nobel Bhuiyan"], "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 1 Figure", "url": "http://arxiv.org/abs/2507.17121v1", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and\nearly diagnosis through automated retinal image analysis can significantly\nreduce the risk of blindness. This paper presents a robust deep learning\nframework for both binary and five-class DR classification, leveraging transfer\nlearning and extensive data augmentation to address the challenges of class\nimbalance and limited training data. We evaluate a range of pretrained\nconvolutional neural network architectures, including variants of ResNet and\nEfficientNet, on the APTOS 2019 dataset.\n  For binary classification, our proposed model achieves a state-of-the-art\naccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of\n98.9%, and an AUC of 99.4%. In the more challenging five-class severity\nclassification task, our model obtains a competitive accuracy of 84.6% and an\nAUC of 94.1%, outperforming several existing approaches. Our findings also\ndemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between\naccuracy and computational efficiency across both tasks.\n  These results underscore the effectiveness of combining class-balanced\naugmentation with transfer learning for high-performance DR diagnosis. The\nproposed framework provides a scalable and accurate solution for DR screening,\nwith potential for deployment in real-world clinical environments.", "comment": "9 pages, 1 Figure", "pdf_url": "http://arxiv.org/pdf/2507.17121v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用迁移学习和数据增强的鲁棒五类和二元糖尿病视网膜病变分类", "tldr": "本文提出了一种结合迁移学习和数据增强的深度学习框架，用于鲁棒的二元和五类糖尿病视网膜病变分类，在APTOS 2019数据集上取得了优异的性能，并具有实际临床部署潜力。", "motivation": "糖尿病视网膜病变（DR）是全球视力丧失的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明风险。", "method": "本文提出了一个鲁棒的深度学习框架，用于二元和五类DR分类。该框架利用迁移学习和大量数据增强来解决类别不平衡和训练数据有限的挑战。研究评估了包括ResNet和EfficientNet变体在内的多种预训练卷积神经网络架构，并在APTOS 2019数据集上进行了测试。", "result": "在二元分类中，所提出的模型取得了98.9%的准确率、98.6%的精确率、99.3%的召回率、98.9%的F1分数和99.4%的AUC，达到了最先进的水平。在更具挑战性的五类严重程度分类任务中，模型取得了84.6%的准确率和94.1%的AUC，优于现有的一些方法。研究还表明EfficientNet-B0和ResNet34在两项任务中都在准确性和计算效率之间提供了最佳权衡。", "conclusion": "这些结果强调了结合类别平衡增强和迁移学习对高性能DR诊断的有效性。所提出的框架为DR筛查提供了一个可扩展且准确的解决方案，具有在真实临床环境中部署的潜力。", "translation": "糖尿病视网膜病变（DR）是全球视力丧失的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明风险。本文提出了一个鲁棒的深度学习框架，用于二元和五类DR分类，利用迁移学习和大量数据增强来解决类别不平衡和训练数据有限的挑战。我们评估了一系列预训练卷积神经网络架构，包括ResNet和EfficientNet的变体，并在APTOS 2019数据集上进行了测试。\n对于二元分类，我们提出的模型取得了98.9%的最先进准确率，以及98.6%的精确率、99.3%的召回率、98.9%的F1分数和99.4%的AUC。在更具挑战性的五类严重程度分类任务中，我们的模型取得了84.6%的竞争性准确率和94.1%的AUC，优于现有的一些方法。我们的研究结果还表明，EfficientNet-B0和ResNet34在两项任务中都在准确性和计算效率之间提供了最佳权衡。\n这些结果强调了结合类别平衡增强和迁移学习对高性能DR诊断的有效性。所提出的框架为DR筛查提供了一个可扩展且准确的解决方案，具有在真实临床环境中部署的潜力。", "summary": "本文提出了一种基于深度学习的鲁棒框架，用于糖尿病视网膜病变的二元和五类分类。该框架利用迁移学习和数据增强技术来克服数据稀缺和类别不平衡问题。在APTOS 2019数据集上，模型在二元分类中实现了98.9%的准确率和99.4%的AUC，在五类分类中实现了84.6%的准确率和94.1%的AUC，性能优于现有方法，并发现EfficientNet-B0和ResNet34在准确性和效率之间达到最佳平衡。研究结果证明了结合类别平衡增强与迁移学习在DR诊断中的高效性，并展现了该框架在临床应用中的巨大潜力。", "keywords": "糖尿病视网膜病变, 深度学习, 迁移学习, 数据增强, 分类", "comments": "该论文的创新之处在于其对迁移学习和数据增强的有效结合，成功解决了糖尿病视网膜病变分类中常见的数据稀缺和类别不平衡问题。其在二元和五类分类任务中均取得了优异的性能，特别是二元分类达到了最先进的水平，这对于早期DR诊断具有重要意义。该框架的可扩展性和准确性使其在真实临床环境中具有很高的部署潜力，有望为全球视力保护做出贡献。"}}
{"id": "2507.17394", "title": "HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs", "authors": ["Zhaolin Cai", "Fan Li", "Ziwei Zheng", "Yanjun Qin"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.17394v1", "summary": "Video Anomaly Detection (VAD) aims to identify and locate deviations from\nnormal patterns in video sequences. Traditional methods often struggle with\nsubstantial computational demands and a reliance on extensive labeled datasets,\nthereby restricting their practical applicability. To address these\nconstraints, we propose HiProbe-VAD, a novel framework that leverages\npre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring\nfine-tuning. In this paper, we discover that the intermediate hidden states of\nMLLMs contain information-rich representations, exhibiting higher sensitivity\nand linear separability for anomalies compared to the output layer. To\ncapitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)\nmechanism that intelligently identifies and extracts the most informative\nhidden states from the optimal intermediate layer during the MLLMs reasoning.\nThen a lightweight anomaly scorer and temporal localization module efficiently\ndetects anomalies using these extracted hidden states and finally generate\nexplanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate\nthat HiProbe-VAD outperforms existing training-free and most traditional\napproaches. Furthermore, our framework exhibits remarkable cross-model\ngeneralization capabilities in different MLLMs without any tuning, unlocking\nthe potential of pre-trained MLLMs for video anomaly detection and paving the\nway for more practical and scalable solutions.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.17394v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HiProbe-VAD: 基于免调优多模态大语言模型隐藏状态探测的视频异常检测", "tldr": "HiProbe-VAD提出了一种利用多模态大语言模型隐藏状态进行免调优视频异常检测的新框架，解决了传统方法的计算和数据依赖问题，并在多个数据集上表现出色。", "motivation": "传统视频异常检测（VAD）方法面临计算量大、对大量标注数据集依赖性强的问题，这限制了它们的实际应用。", "method": "我们提出了HiProbe-VAD，一个无需微调即可利用预训练多模态大语言模型（MLLMs）进行VAD的新颖框架。研究发现MLLMs的中间隐藏状态包含信息丰富的表示，对异常具有更高的敏感性和线性可分性。为此，我们提出了动态层显著性探测（DLSP）机制，智能地从最佳中间层识别并提取最具信息量的隐藏状态。然后，一个轻量级异常评分器和时间定位模块利用这些提取的隐藏状态高效检测异常并生成解释。", "result": "HiProbe-VAD在UCF-Crime和XD-Violence数据集上的实验表明，它优于现有免训练方法和大多数传统方法。此外，我们的框架在不同MLLMs上无需任何调优就展现出卓越的跨模型泛化能力。", "conclusion": "HiProbe-VAD解锁了预训练多模态大语言模型在视频异常检测中的潜力，为更实用、可扩展的解决方案铺平了道路。", "translation": "视频异常检测（VAD）旨在识别和定位视频序列中偏离正常模式的异常。传统方法通常面临巨大的计算需求和对大量标注数据集的依赖，从而限制了它们的实际适用性。为了解决这些限制，我们提出了HiProbe-VAD，一个新颖的框架，它利用预训练的多模态大语言模型（MLLMs）进行VAD，而无需进行微调。在本文中，我们发现MLLMs的中间隐藏状态包含信息丰富的表示，与输出层相比，对异常表现出更高的敏感性和线性可分性。为了利用这一点，我们提出了一种动态层显著性探测（DLSP）机制，该机制在MLLMs推理过程中智能地识别并提取最佳中间层中最具信息量的隐藏状态。然后，一个轻量级异常评分器和时间定位模块利用这些提取的隐藏状态高效检测异常，并最终生成解释。在UCF-Crime和XD-Violence数据集上的实验表明，HiProbe-VAD优于现有免训练方法和大多数传统方法。此外，我们的框架在不同MLLMs上无需任何调优就展现出卓越的跨模型泛化能力，释放了预训练MLLMs在视频异常检测中的潜力，并为更实用和可扩展的解决方案铺平了道路。", "summary": "本文提出了HiProbe-VAD，一个创新的免调优视频异常检测框架，旨在克服传统方法对计算资源和大量标注数据的依赖。该框架利用预训练多模态大语言模型（MLLMs）的中间隐藏状态，发现这些状态对异常具有更高的敏感性和可分性。通过动态层显著性探测（DLSP）机制提取关键隐藏状态，并结合轻量级异常评分器进行检测和解释。实验证明，HiProbe-VAD在多个数据集上性能优越，并展现出卓越的跨模型泛化能力，为视频异常检测提供了实用且可扩展的解决方案。", "keywords": "视频异常检测, 多模态大语言模型, 隐藏状态探测, 免调优, 泛化能力", "comments": "HiProbe-VAD的创新点在于其免调优（tuning-free）方法，通过探测预训练多模态大语言模型的隐藏状态来进行视频异常检测，这显著降低了计算成本和对标注数据的需求。其提出的动态层显著性探测（DLSP）机制是核心亮点，有效利用了MLLMs的内在表示能力。该方法在实际应用中具有重要意义，因为它提高了VAD的实用性和可扩展性，尤其是在资源受限或数据稀缺的场景下。其出色的跨模型泛化能力也预示着该方法具有广阔的应用前景。"}}
{"id": "2507.17030", "title": "CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates", "authors": ["Tianyu Chen", "Vansh Bansal", "James G. Scott"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17030v1", "summary": "We consider the problem of validating whether a neural posterior estimate \\(\nq(\\theta \\mid x) \\) is an accurate approximation to the true, unknown true\nposterior \\( p(\\theta \\mid x) \\). Existing methods for evaluating the quality\nof an NPE estimate are largely derived from classifier-based tests or\ndivergence measures, but these suffer from several practical drawbacks. As an\nalternative, we introduce the \\emph{Conditional Localization Test} (CoLT), a\nprincipled method designed to detect discrepancies between \\( p(\\theta \\mid x)\n\\) and \\( q(\\theta \\mid x) \\) across the full range of conditioning inputs.\nRather than relying on exhaustive comparisons or density estimation at every \\(\nx \\), CoLT learns a localization function that adaptively selects points\n$\\theta_l(x)$ where the neural posterior $q$ deviates most strongly from the\ntrue posterior $p$ for that $x$. This approach is particularly advantageous in\ntypical simulation-based inference settings, where only a single draw \\( \\theta\n\\sim p(\\theta \\mid x) \\) from the true posterior is observed for each\nconditioning input, but where the neural posterior \\( q(\\theta \\mid x) \\) can\nbe sampled an arbitrary number of times. Our theoretical results establish\nnecessary and sufficient conditions for assessing distributional equality\nacross all \\( x \\), offering both rigorous guarantees and practical\nscalability. Empirically, we demonstrate that CoLT not only performs better\nthan existing methods at comparing $p$ and $q$, but also pinpoints regions of\nsignificant divergence, providing actionable insights for model refinement.\nThese properties position CoLT as a state-of-the-art solution for validating\nneural posterior estimates.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17030v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "CoLT: 评估神经后验估计准确性的条件局部化测试", "tldr": "CoLT是一种新的测试方法，用于评估神经后验估计的准确性，它能有效地发现偏差并优于现有方法。", "motivation": "现有评估神经后验估计（NPE）准确性的方法（如基于分类器或散度度量的方法）存在实际缺点。因此，需要一种能够检测在所有条件输入范围内神经后验估计与真实后验之间差异的原理性方法。", "method": "论文引入了条件局部化测试（CoLT）。CoLT不依赖于详尽的比较或在每个输入x处进行密度估计，而是学习一个局部化函数，该函数自适应地选择神经后验q与真实后验p偏差最大的点θ_l(x)。这种方法特别适用于模拟推理设置，其中真实后验只观察到一次抽样，而神经后验可以任意采样。", "result": "理论结果建立了评估所有x分布相等性的必要和充分条件，提供了严格的保证和实际可扩展性。在经验上，CoLT在比较真实后验p和神经后验q方面表现优于现有方法，并且能够精确定位显著差异的区域，为模型改进提供可操作的见解。", "conclusion": "CoLT是一种用于验证神经后验估计的先进解决方案，能够有效地检测并定位估计偏差，为模型优化提供关键洞察。", "translation": "我们考虑验证神经后验估计 q(θ|x) 是否是真实、未知后验 p(θ|x) 的准确近似的问题。现有评估NPE估计质量的方法主要来源于基于分类器的测试或散度度量，但这些方法存在一些实际缺点。作为替代，我们引入了“条件局部化测试”（CoLT），这是一种旨在检测在所有条件输入范围内 p(θ|x) 和 q(θ|x) 之间差异的原理性方法。CoLT不依赖于详尽的比较或在每个x处进行密度估计，而是学习一个局部化函数，该函数自适应地选择神经后验 q 对于该 x 与真实后验 p 偏差最强的点 θ_l(x)。这种方法在典型的基于模拟的推理设置中特别有利，在这种设置中，对于每个条件输入，只能观察到真实后验 p(θ|x) 的一次抽样 θ，而神经后验 q(θ|x) 可以任意次数地采样。我们的理论结果建立了评估所有 x 分布相等性的必要和充分条件，提供了严格的保证和实际可扩展性。在经验上，我们证明 CoLT 不仅在比较 p 和 q 方面优于现有方法，而且还能精确定位显著差异的区域，为模型改进提供可操作的见解。这些特性使 CoLT 成为验证神经后验估计的先进解决方案。", "summary": "CoLT是一种新颖的条件局部化测试方法，旨在解决现有神经后验估计（NPE）验证方法的局限性。它通过学习一个局部化函数来识别神经后验q与真实后验p之间偏差最大的区域，尤其适用于模拟推理场景，其中真实后验采样受限而神经后验可自由采样。CoLT在理论上提供了严格的分布相等性评估条件，并在实践中表现出优于现有方法的性能，能够精确定位差异区域，从而为模型优化提供有价值的指导。", "keywords": "神经后验估计, 条件局部化测试, 模拟推理, 模型验证, 分布比较", "comments": "CoLT的创新之处在于其自适应局部化函数，避免了详尽比较和密度估计的计算开销，尤其适用于真实后验采样受限的场景。其能够精确定位偏差区域的特性，为模型调试和改进提供了直接的“可操作见解”，这是现有方法难以提供的。这对于提高模拟推理等领域中神经后验估计的可靠性和实用性具有重要意义。"}}
{"id": "2502.03829", "title": "FE-UNet: Frequency Domain Enhanced U-Net for Low-Frequency Information-Rich Image Segmentation", "authors": ["Guohao Huo", "Ruiting Dai", "Ling Shao", "Jinliang Liu", "Hao Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03829v2", "summary": "In deep-sea exploration and surgical robotics scenarios, environmental\nlighting and device resolution limitations often cause high-frequency feature\nattenuation. Addressing the differences in frequency band sensitivity between\nCNNs and the human visual system (mid-frequency sensitivity with low-frequency\nsensitivity surpassing high-frequency), we experimentally quantified the CNN\ncontrast sensitivity function and proposed a wavelet adaptive spectrum fusion\n(WASF) method inspired by biological vision mechanisms to balance\ncross-frequency image features. Furthermore, we designed a perception frequency\nblock (PFB) that integrates WASF to enhance frequency-domain feature\nextraction. Based on this, we developed the FE-UNet model, which employs a SAM2\nbackbone network and incorporates fine-tuned Hiera-Large modules to ensure\nsegmentation accuracy while improving generalization capability. Experiments\ndemonstrate that FE-UNet achieves state-of-the-art performance in cross-domain\ntasks such as marine organism segmentation and polyp segmentation, showcasing\nrobust adaptability and significant application potential.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03829v2", "cate": "cs.CV", "date": "2025-02-06", "updated": "2025-07-23", "AI": {"title_translation": "FE-UNet：用于低频信息丰富图像分割的频域增强型U-Net", "tldr": "FE-UNet通过引入受生物视觉启发的频域增强模块（WASF和PFB），在低频信息丰富的图像分割任务中实现了SOTA性能，解决了高频特征衰减问题。", "motivation": "在深海探测和手术机器人等场景中，环境光照和设备分辨率限制常导致高频特征衰减。此外，卷积神经网络（CNN）与人类视觉系统在频带敏感性上存在差异，CNN对中频敏感，而人类视觉系统对低频敏感度超过高频。为解决这些问题并提升图像分割性能，特别是针对低频信息丰富的图像。", "method": "1. 实验量化了CNN的对比敏感度函数。2. 提出了受生物视觉机制启发的“小波自适应频谱融合”（WASF）方法，以平衡跨频段图像特征。3. 设计了集成WASF的“感知频率块”（PFB），以增强频域特征提取。4. 基于此，开发了FE-UNet模型，该模型采用SAM2骨干网络并结合了微调的Hiera-Large模块，以确保分割精度并提高泛化能力。", "result": "FE-UNet在海洋生物分割和息肉分割等跨域任务中实现了最先进的（SOTA）性能。", "conclusion": "FE-UNet展现出强大的适应性和显著的应用潜力，特别是在处理低频信息丰富的图像分割任务方面。", "translation": "在深海探测和手术机器人场景中，环境光照和设备分辨率限制常常导致高频特征衰减。针对卷积神经网络（CNN）与人类视觉系统之间在频带敏感性上的差异（人类视觉系统对中频敏感，低频敏感度超过高频），我们实验性地量化了CNN的对比敏感度函数，并提出了一种受生物视觉机制启发的小波自适应频谱融合（WASF）方法，以平衡跨频段图像特征。此外，我们设计了一个集成了WASF的感知频率块（PFB），用于增强频域特征提取。在此基础上，我们开发了FE-UNet模型，该模型采用SAM2骨干网络并结合了微调的Hiera-Large模块，以确保分割精度并提高泛化能力。实验表明，FE-UNet在海洋生物分割和息肉分割等跨域任务中取得了最先进的性能，展示了强大的适应性和显著的应用潜力。", "summary": "本研究提出了FE-UNet模型，旨在解决深海探测和手术机器人等场景中因高频特征衰减导致的图像分割挑战。模型通过量化CNN的对比敏感度函数，并引入受生物视觉启发的小波自适应频谱融合（WASF）方法和感知频率块（PFB）来增强频域特征提取，平衡跨频段图像特征。FE-UNet采用SAM2骨干网络并融合Hiera-Large模块，在海洋生物和息肉分割等跨域任务中达到了最先进的性能，展现出卓越的泛化能力和应用潜力。", "keywords": "频域增强, U-Net, 图像分割, 小波自适应频谱融合, 感知频率块", "comments": "这项研究的创新点在于将生物视觉机制（特别是对频率敏感度的理解）引入到深度学习模型设计中，通过WASF和PFB模块增强了模型对频域特征的处理能力，尤其是在低频信息丰富的图像分割场景中。其在跨域任务上的SOTA表现表明了该方法的有效性和泛化能力，对于解决特定场景下的图像质量问题具有重要意义。"}}
{"id": "2507.16863", "title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "authors": ["Hongcheng Gao", "Zihao Huang", "Lin Xu", "Jingyi Tang", "Xinhao Li", "Yue Liu", "Haoyang Li", "Taihang Hu", "Minhua Lin", "Xinlong Yang", "Ge Wu", "Balong Bi", "Hongyu Chen", "Wentao Zhang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16863v1", "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16863v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "像素、模式，但无诗意：像人类一样看世界", "tldr": "本文提出了Turing Eye Test (TET)基准测试，旨在评估多模态大语言模型（MLLMs）在感知任务上的表现，发现当前SOTA的MLLMs在人类直观处理的感知任务上表现出灾难性失败，表明视觉塔泛化能力是MLLMs与人类感知之间的关键差距。", "motivation": "实现多模态大语言模型（MLLMs）类人感知和推理是人工智能的核心挑战。尽管现有研究主要关注提升推理能力，但一个根本问题是：MLLMs能否真正像人类一样感知世界？本文将焦点从推理转向感知。", "method": "本文引入了Turing Eye Test (TET)，这是一个具有挑战性的、以感知为导向的基准测试，包含四个诊断任务，用于评估MLLMs在人类直观处理的合成图像上的表现。", "result": "研究发现，最先进的多模态大语言模型在人类看来微不足道的感知任务上表现出灾难性失败。上下文学习和基于语言骨干的训练（对之前的基准有效）未能改善这些任务的性能，而微调视觉塔能够实现快速适应。", "conclusion": "当前的多模态大语言模型在视觉塔泛化能力上存在关键差距，这与人类感知能力形成对比。本文提出的TET基准测试揭示了这一点，并指出未来的工作应致力于增强视觉泛化。", "translation": "在多模态大语言模型（MLLMs）中实现类人感知和推理仍然是人工智能的核心挑战。尽管最近的研究主要集中在增强MLLMs的推理能力上，但一个基本问题依然存在：多模态大语言模型能否真正像人类一样感知世界？本文将焦点从推理转向感知。我们没有专门为推理构建基准，而是引入了图灵眼测试（TET），这是一个具有挑战性的、以感知为导向的基准，包含四个诊断任务，评估MLLMs在人类直观处理的合成图像上的表现。我们的发现表明，最先进的MLLMs在我们的感知任务上表现出灾难性失败，而这些任务对人类来说是微不足道的。上下文学习和基于语言骨干的训练——对以前的基准有效——未能改善我们任务的性能，而微调视觉塔能够实现快速适应，这表明我们的基准对视觉塔的泛化能力提出了挑战，而非对语言骨干的知识和推理能力——这是当前MLLMs与人类感知之间的关键差距。我们在此版本中发布了TET任务的代表性子集，并将在未来的工作中引入更多样化的任务和方法来增强视觉泛化。", "summary": "本研究旨在探讨多模态大语言模型（MLLMs）是否能像人类一样感知世界，而非仅仅关注推理能力。为此，作者提出了图灵眼测试（TET），一个专门评估MLLMs感知能力的基准测试。结果显示，当前最先进的MLLMs在人类直观的感知任务上表现出严重不足，且传统的上下文学习和语言骨干训练方法无效。研究表明，视觉塔的泛化能力是MLLMs与人类感知之间存在的关键差距。", "keywords": "多模态大语言模型, 感知, 图灵眼测试, 视觉泛化, 人工智能", "comments": "该论文通过引入Turing Eye Test (TET)基准测试，创新性地将研究焦点从MLLMs的推理能力转向了其感知能力，揭示了当前MLLMs在视觉泛化方面存在的显著局限性。这一发现对于未来MLLMs的发展方向具有重要指导意义，强调了视觉处理能力提升的重要性。"}}
{"id": "2410.08385", "title": "Language model developers should report train-test overlap", "authors": ["Andy K Zhang", "Kevin Klyman", "Yifan Mai", "Yoav Levine", "Yian Zhang", "Rishi Bommasani", "Percy Liang"], "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 Spotlight; 23 pages", "url": "http://arxiv.org/abs/2410.08385v2", "summary": "Language models are extensively evaluated, but correctly interpreting\nevaluation results requires knowledge of train-test overlap which refers to the\nextent to which the language model is trained on the very data it is being\ntested on. The public currently lacks adequate information about train-test\noverlap: most models have no public train-test overlap statistics, and third\nparties cannot directly measure train-test overlap since they do not have\naccess to the training data. To make this clear, we document the practices of\n30 model developers, finding that just 9 developers report train-test overlap:\n4 developers release training data under open-source licenses, enabling the\ncommunity to directly measure train-test overlap, and 5 developers publish\ntheir train-test overlap methodology and statistics. By engaging with language\nmodel developers, we provide novel information about train-test overlap for\nthree additional developers. Overall, we take the position that language model\ndevelopers should publish train-test overlap statistics and/or training data\nwhenever they report evaluation results on public test sets. We hope our work\nincreases transparency into train-test overlap to increase the community-wide\ntrust in model evaluations.", "comment": "ICML 2025 Spotlight; 23 pages", "pdf_url": "http://arxiv.org/pdf/2410.08385v2", "cate": "cs.LG", "date": "2024-10-10", "updated": "2025-07-22", "AI": {"title_translation": "语言模型开发者应报告训练-测试重叠", "tldr": "为了正确解释语言模型的评估结果，开发者应公开训练-测试重叠统计数据和/或训练数据，因为目前大多数模型缺乏相关信息，导致社区信任度不足。", "motivation": "正确解释语言模型的评估结果需要了解训练-测试重叠，但目前公众缺乏关于训练-测试重叠的充分信息，大多数模型没有公开相关统计数据，第三方也无法直接测量，这导致了社区对模型评估的信任度不足。", "method": "本文档记录了30家模型开发者的实践，并与其中一些开发者进行了接触，以获取额外的训练-测试重叠信息。", "result": "在30家模型开发者中，只有9家报告了训练-测试重叠：其中4家以开源许可发布训练数据，5家公布了其训练-测试重叠方法和统计数据。此外，通过与开发者沟通，本文为另外三家开发者提供了新的训练-测试重叠信息。", "conclusion": "语言模型开发者在报告公共测试集上的评估结果时，应公布训练-测试重叠统计数据和/或训练数据，以增加社区对模型评估的信任度。", "translation": "语言模型被广泛评估，但正确解释评估结果需要了解训练-测试重叠，这指的是语言模型在多大程度上是在其被测试的相同数据上进行训练的。目前公众缺乏关于训练-测试重叠的充分信息：大多数模型没有公开训练-测试重叠统计数据，第三方也无法直接测量训练-测试重叠，因为他们无法访问训练数据。为了阐明这一点，我们记录了30家模型开发者的实践，发现只有9家开发者报告了训练-测试重叠：4家开发者在开源许可下发布训练数据，使社区能够直接测量训练-测试重叠，另有5家开发者公布了他们的训练-测试重叠方法和统计数据。通过与语言模型开发者的接触，我们为另外三家开发者提供了关于训练-测试重叠的新信息。总的来说，我们认为语言模型开发者在报告公共测试集上的评估结果时，应公布训练-测试重叠统计数据和/或训练数据。我们希望我们的工作能增加训练-测试重叠的透明度，从而提高社区对模型评估的整体信任度。", "summary": "本研究强调了语言模型训练-测试重叠信息公开的重要性。调查发现，在30家模型开发者中，仅有9家报告了相关数据（4家开源训练数据，5家公布统计方法和数据），且通过沟通为额外三家提供了信息。论文呼吁所有语言模型开发者在公布评估结果时，应同时公开训练-测试重叠统计数据或训练数据，以增强评估结果的透明度和社区信任。", "keywords": "语言模型, 训练-测试重叠, 透明度, 模型评估, 数据公开", "comments": "本文提出了一个关于语言模型评估透明度的重要问题，即训练-测试重叠。其创新之处在于通过调查实际开发者的实践并呼吁公开透明，为提高模型评估的可信度提供了具体建议。这对于促进AI社区的健康发展和建立对大型语言模型的信任至关重要。"}}
{"id": "2507.17454", "title": "C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning", "authors": ["Shusen Ma", "Yun-Bo Zhao", "Yu Kang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17454v1", "summary": "Multivariate time series forecasting has drawn increasing attention due to\nits practical importance. Existing approaches typically adopt either\nchannel-mixing (CM) or channel-independence (CI) strategies. CM strategy can\ncapture inter-variable dependencies but fails to discern variable-specific\ntemporal patterns. CI strategy improves this aspect but fails to fully exploit\ncross-variable dependencies like CM. Hybrid strategies based on feature fusion\noffer limited generalization and interpretability. To address these issues, we\npropose C3RL, a novel representation learning framework that jointly models\nboth CM and CI strategies. Motivated by contrastive learning in computer\nvision, C3RL treats the inputs of the two strategies as transposed views and\nbuilds a siamese network architecture: one strategy serves as the backbone,\nwhile the other complements it. By jointly optimizing contrastive and\nprediction losses with adaptive weighting, C3RL balances representation and\nforecasting performance. Extensive experiments on seven models show that C3RL\nboosts the best-case performance rate to 81.4\\% for models based on CI strategy\nand to 76.3\\% for models based on CM strategy, demonstrating strong\ngeneralization and effectiveness. The code will be available once the paper is\naccepted.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17454v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "C3RL: 从表示学习的角度重新思考通道独立性和通道混合的结合", "tldr": "C3RL是一个新的表示学习框架，通过联合建模通道独立性和通道混合策略，提高了多元时间序列预测的性能和泛化能力。", "motivation": "现有多元时间序列预测方法中，通道混合(CM)策略能捕获变量间依赖但难以识别变量特定时间模式；通道独立(CI)策略能改善后者但未能充分利用跨变量依赖。混合策略基于特征融合，泛化性和可解释性有限。", "method": "提出C3RL，一个新颖的表示学习框架，联合建模CM和CI策略。受计算机视觉中对比学习的启发，C3RL将两种策略的输入视为转置视图，并构建一个孪生网络架构，其中一个策略作为主干，另一个作为补充。通过自适应加权联合优化对比损失和预测损失。", "result": "在七个模型上的广泛实验表明，C3RL将基于CI策略的模型的最佳性能率提升至81.4%，将基于CM策略的模型的最佳性能率提升至76.3%，表现出强大的泛化性和有效性。", "conclusion": "C3RL通过有效结合通道独立性和通道混合策略，显著提升了多元时间序列预测的性能和泛化能力，解决了现有单一策略的局限性。", "translation": "多元时间序列预测因其实际重要性而受到越来越多的关注。现有方法通常采用通道混合（CM）或通道独立（CI）策略。CM策略可以捕获变量间依赖性，但无法识别变量特定的时间模式。CI策略改善了这方面，但未能像CM那样充分利用跨变量依赖性。基于特征融合的混合策略泛化性和可解释性有限。为了解决这些问题，我们提出了C3RL，一种新颖的表示学习框架，它联合建模CM和CI策略。受计算机视觉中对比学习的启发，C3RL将两种策略的输入视为转置视图，并构建一个孪生网络架构：一个策略作为主干，而另一个作为补充。通过自适应加权联合优化对比损失和预测损失，C3RL平衡了表示和预测性能。在七个模型上的广泛实验表明，C3RL将基于CI策略的模型的最佳性能率提升至81.4%，将基于CM策略的模型的最佳性能率提升至76.3%，表现出强大的泛化性和有效性。代码将在论文被接受后可用。", "summary": "本文提出了C3RL，一个新颖的表示学习框架，旨在解决多元时间序列预测中通道混合(CM)和通道独立(CI)策略的局限性。C3RL通过将CM和CI的输入视为转置视图，构建了一个孪生网络架构，并联合优化对比损失和预测损失。实验结果表明，C3RL显著提升了基于CI和CM策略的模型的性能，展示了其强大的泛化性和有效性。", "keywords": "多元时间序列预测, 表示学习, 通道独立性, 通道混合, 对比学习", "comments": "C3RL的创新之处在于其独特地将通道独立性与通道混合策略结合，并通过借鉴对比学习的思想，构建了一个新颖的孪生网络架构。这种方法有效地解决了传统单一策略在捕获变量内和变量间模式上的不足，提升了模型的泛化能力和预测精度，为多元时间序列预测提供了一个有前景的解决方案。"}}
{"id": "2507.17533", "title": "Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding", "authors": ["Liwen Liu", "Weidong Yang", "Lipeng Ma", "Ben Fei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17533v1", "summary": "Recent advances in multi-modal pre-training methods have shown promising\neffectiveness in learning 3D representations by aligning multi-modal features\nbetween 3D shapes and their corresponding 2D counterparts. However, existing\nmulti-modal pre-training frameworks primarily rely on a single pre-training\ntask to gather multi-modal data in 3D applications. This limitation prevents\nthe models from obtaining the abundant information provided by other relevant\ntasks, which can hinder their performance in downstream tasks, particularly in\ncomplex and diverse domains. In order to tackle this issue, we propose MMPT, a\nMulti-modal Multi-task Pre-training framework designed to enhance point cloud\nunderstanding. Specifically, three pre-training tasks are devised: (i)\nToken-level reconstruction (TLR) aims to recover masked point tokens, endowing\nthe model with representative learning abilities. (ii) Point-level\nreconstruction (PLR) is integrated to predict the masked point positions\ndirectly, and the reconstructed point cloud can be considered as a transformed\npoint cloud used in the subsequent task. (iii) Multi-modal contrastive learning\n(MCL) combines feature correspondences within and across modalities, thus\nassembling a rich learning signal from both 3D point cloud and 2D image\nmodalities in a self-supervised manner. Moreover, this framework operates\nwithout requiring any 3D annotations, making it scalable for use with large\ndatasets. The trained encoder can be effectively transferred to various\ndownstream tasks. To demonstrate its effectiveness, we evaluated its\nperformance compared to state-of-the-art methods in various discriminant and\ngenerative applications under widely-used benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17533v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多模态多任务预训练以改进点云理解", "tldr": "MMPT是一个多模态多任务预训练框架，通过结合三种预训练任务（Token级重建、点级重建和多模态对比学习）来增强点云理解，无需3D标注，并在多项下游任务中表现出色。", "motivation": "现有的多模态预训练框架主要依赖单一预训练任务来收集3D应用中的多模态数据，这限制了模型获取其他相关任务提供的丰富信息，从而阻碍了模型在下游任务（尤其是在复杂多样的领域）中的性能。", "method": "我们提出了MMPT，一个多模态多任务预训练框架。具体设计了三个预训练任务：(i) Token级重建（TLR）旨在恢复被遮蔽的点tokens，赋予模型具有代表性的学习能力。(ii) 点级重建（PLR）直接预测被遮蔽的点位置，重建的点云可作为后续任务中使用的转换点云。(iii) 多模态对比学习（MCL）结合了模态内部和跨模态的特征对应关系，以自监督方式从3D点云和2D图像模态中汇集丰富的学习信号。此外，该框架无需任何3D标注，使其可扩展用于大型数据集。", "result": "所训练的编码器可以有效地迁移到各种下游任务。通过在广泛使用的基准下，在各种判别和生成应用中与最先进方法进行性能比较，证明了其有效性。", "conclusion": "Not mentioned in abstract", "translation": "多模态多任务预训练以改进点云理解\n\n最近多模态预训练方法的进展在通过对齐3D形状及其对应的2D对应物的多模态特征来学习3D表示方面显示出有希望的有效性。然而，现有的多模态预训练框架主要依赖单一预训练任务来收集3D应用中的多模态数据。这种限制阻碍了模型获取其他相关任务提供的丰富信息，这会影响它们在下游任务中的性能，特别是在复杂和多样化的领域。为了解决这个问题，我们提出了MMPT，一个旨在增强点云理解的多模态多任务预训练框架。具体来说，设计了三个预训练任务：(i) Token级重建（TLR）旨在恢复被遮蔽的点tokens，赋予模型具有代表性的学习能力。(ii) 点级重建（PLR）被集成以直接预测被遮蔽的点位置，重建的点云可以被视为后续任务中使用的转换点云。(iii) 多模态对比学习（MCL）结合了模态内部和跨模态的特征对应关系，从而以自监督方式从3D点云和2D图像模态中汇集丰富的学习信号。此外，该框架无需任何3D标注，使其可扩展用于大型数据集。训练好的编码器可以有效地迁移到各种下游任务。为了证明其有效性，我们在广泛使用的基准下，在各种判别和生成应用中，与最先进的方法进行了性能比较。", "summary": "本文提出了MMPT，一个多模态多任务预训练框架，旨在解决现有方法在3D点云理解中仅依赖单一预训练任务的局限性。MMPT通过结合Token级重建、点级重建和多模态对比学习这三个自监督任务，从3D点云和2D图像模态中学习丰富的表示。该框架无需3D标注，可扩展应用于大型数据集，并且训练好的编码器能有效迁移到多种下游任务，在判别和生成应用中均表现出优异性能。", "keywords": "多模态预训练, 多任务学习, 点云理解, 自监督学习, 3D表示学习", "comments": "MMPT的创新点在于其多任务预训练范式，通过整合三种不同的自监督任务（TLR、PLR、MCL），弥补了现有单任务预训练的局限性，从而更全面地捕捉多模态数据中的信息。其无需3D标注的特点显著降低了数据标注成本，提升了在大规模数据集上的可扩展性，对于推动3D点云理解领域的发展具有重要意义。"}}
{"id": "2505.23404", "title": "MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models", "authors": ["Mingyu Yu", "Wei Wang", "Yanjie Wei", "Sujuan Qin", "Fei Gao", "Wenmin Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.23404v4", "summary": "Recent advancements in adversarial jailbreak attacks have exposed critical\nvulnerabilities in Large Language Models (LLMs), enabling the circumvention of\nalignment safeguards through increasingly sophisticated prompt manipulations.\nBased on our experiments, we found that the effectiveness of jailbreak\nstrategies is influenced by the comprehension ability of the attacked LLM.\nBuilding on this insight, we propose a capability-aware Multi-Encryption\nFramework (MEF) for evaluating vulnerabilities in black-box LLMs. Specifically,\nMEF first categorizes the comprehension ability level of the LLM, then applies\ndifferent strategies accordingly: For models with limited comprehension\nability, MEF adopts the Fu+En1 strategy, which integrates layered semantic\nmutations with an encryption technique, more effectively contributing to\nevasion of the LLM's defenses at the input and inference stages. For models\nwith strong comprehension ability, MEF uses a more complex Fu+En1+En2 strategy,\nin which additional dual-ended encryption techniques are applied to the LLM's\nresponses, further contributing to evasion of the LLM's defenses at the output\nstage. Experimental results demonstrate the effectiveness of our approach,\nachieving attack success rates of 98.9% on GPT-4o (29 May 2025 release) and\n99.8% on GPT-4.1 (8 July 2025 release). Our work contributes to a deeper\nunderstanding of the vulnerabilities in current LLM alignment mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.23404v4", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-23", "AI": {"title_translation": "MEF：一个用于评估黑盒大型语言模型漏洞的感知能力多重加密框架", "tldr": "MEF是一个感知能力的多重加密框架，用于评估黑盒大型语言模型的漏洞。它根据LLM的理解能力应用不同的策略，以实现高成功率的越狱攻击，揭示了LLM对齐机制的弱点。", "motivation": "对抗性越狱攻击暴露了大型语言模型（LLMs）的关键漏洞，使得复杂的提示操纵能够规避对齐安全措施。研究发现越狱策略的有效性受被攻击LLM理解能力的影响，因此需要一种新的方法来评估这些漏洞。", "method": "提出了一种感知能力的多重加密框架（MEF）。MEF首先对LLM的理解能力水平进行分类：对于理解能力有限的模型，采用Fu+En1策略，结合分层语义突变和加密技术，以规避输入和推理阶段的防御。对于理解能力强的模型，采用更复杂的Fu+En1+En2策略，增加双端加密技术应用于LLM的响应，进一步规避输出阶段的防御。", "result": "实验结果表明，该方法在GPT-4o（2025年5月29日发布）上实现了98.9%的攻击成功率，在GPT-4.1（2025年7月8日发布）上实现了99.8%的攻击成功率。", "conclusion": "该工作有助于更深入地理解当前大型语言模型对齐机制中的漏洞。", "translation": "最近对抗性越狱攻击的进展暴露了大型语言模型（LLMs）的关键漏洞，使得通过日益复杂的提示操纵能够规避对齐安全措施。根据我们的实验，我们发现越狱策略的有效性受被攻击LLM理解能力的影响。基于这一见解，我们提出了一个感知能力的多重加密框架（MEF），用于评估黑盒LLM中的漏洞。具体而言，MEF首先对LLM的理解能力水平进行分类，然后相应地应用不同的策略：对于理解能力有限的模型，MEF采用Fu+En1策略，该策略将分层语义突变与加密技术相结合，更有效地有助于规避LLM在输入和推理阶段的防御。对于理解能力强的模型，MEF使用更复杂的Fu+En1+En2策略，其中额外的双端加密技术应用于LLM的响应，进一步有助于规避LLM在输出阶段的防御。实验结果表明我们方法的有效性，在GPT-4o（2025年5月29日发布）上实现了98.9%的攻击成功率，在GPT-4.1（2025年7月8日发布）上实现了99.8%的攻击成功率。我们的工作有助于更深入地理解当前LLM对齐机制中的漏洞。", "summary": "本文提出了MEF（感知能力多重加密框架），旨在评估黑盒大型语言模型（LLMs）的漏洞。MEF根据LLM的理解能力水平，采用不同的越狱策略：对于理解能力有限的模型，使用Fu+En1策略结合语义突变和加密；对于理解能力强的模型，使用更复杂的Fu+En1+En2策略，增加双端加密。实验证明，MEF在GPT-4o和GPT-4.1上分别实现了98.9%和99.8%的高攻击成功率，揭示了当前LLM对齐机制的脆弱性。", "keywords": "大型语言模型, 漏洞, 越狱攻击, 多重加密, 黑盒评估", "comments": "本文的创新点在于提出了一个感知LLM理解能力的多重加密框架MEF，并根据不同能力水平采用自适应的越狱策略。这种方法有效地利用了LLM理解能力的差异来规避防御，取得了极高的攻击成功率，对于深入理解和改进LLM的对齐机制具有重要意义。它揭示了现有防御机制在面对复杂、自适应攻击时的局限性。"}}
{"id": "2507.17235", "title": "On the Feasibility of Quantum Unit Testing", "authors": ["Andriy Miranskyy", "José Campos", "Anila Mjeda", "Lei Zhang", "Ignacio García Rodríguez de Guzmán"], "categories": ["cs.SE", "quant-ph"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17235v1", "summary": "The increasing complexity of quantum software presents significant challenges\nfor software verification and validation, particularly in the context of unit\ntesting. This work presents a comprehensive study on quantum-centric unit\ntests, comparing traditional statistical approaches with tests specifically\ndesigned for quantum circuits. These include tests that run only on a classical\ncomputer, such as the Statevector test, as well as those executable on quantum\nhardware, such as the Swap test and the novel Inverse test. Through an\nempirical study and detailed analysis on 1,796,880 mutated quantum circuits, we\ninvestigate (a) each test's ability to detect subtle discrepancies between the\nexpected and actual states of a quantum circuit, and (b) the number of\nmeasurements required to achieve high reliability. The results demonstrate that\nquantum-centric tests, particularly the Statevector test and the Inverse test,\nprovide clear advantages in terms of precision and efficiency, reducing both\nfalse positives and false negatives compared to statistical tests. This work\ncontributes to the development of more robust and scalable strategies for\ntesting quantum software, supporting the future adoption of fault-tolerant\nquantum computers and promoting more reliable practices in quantum software\nengineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17235v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "量子单元测试的可行性研究", "tldr": "本研究探讨了量子软件单元测试的可行性，比较了传统统计方法和量子中心测试（如Statevector测试和新型Inverse测试）。结果表明，量子中心测试在精度和效率方面具有明显优势，有助于开发更可靠的量子软件测试策略。", "motivation": "量子软件日益增长的复杂性给软件验证和确认带来了巨大挑战，尤其是在单元测试方面。", "method": "本研究对量子中心单元测试进行了全面研究，比较了传统的统计方法与专为量子电路设计的测试方法。这些测试包括在经典计算机上运行的Statevector测试，以及可在量子硬件上执行的Swap测试和新颖的Inverse测试。通过对1,796,880个变异量子电路进行实证研究和详细分析，作者调查了每种测试检测量子电路预期状态与实际状态之间细微差异的能力，以及实现高可靠性所需的测量次数。", "result": "结果表明，量子中心测试，特别是Statevector测试和Inverse测试，在精度和效率方面具有明显优势，与统计测试相比，它们减少了假阳性和假阴性。", "conclusion": "这项工作有助于开发更健壮和可扩展的量子软件测试策略，支持未来容错量子计算机的采用，并促进量子软件工程中更可靠的实践。", "translation": "量子软件日益增长的复杂性给软件验证和确认带来了巨大挑战，特别是在单元测试方面。这项工作对以量子为中心的单元测试进行了全面研究，比较了传统的统计方法与专为量子电路设计的测试。这些测试包括仅在经典计算机上运行的Statevector测试，以及可在量子硬件上执行的Swap测试和新颖的Inverse测试。通过对1,796,880个变异量子电路进行实证研究和详细分析，我们调查了(a)每种测试检测量子电路预期状态与实际状态之间细微差异的能力，以及(b)实现高可靠性所需的测量次数。结果表明，以量子为中心的测试，特别是Statevector测试和Inverse测试，在精度和效率方面提供了明显的优势，与统计测试相比，减少了假阳性和假阴性。这项工作有助于开发更健壮和可扩展的量子软件测试策略，支持未来容错量子计算机的采用，并促进量子软件工程中更可靠的实践。", "summary": "本研究探讨了量子软件单元测试的可行性，旨在解决量子软件复杂性带来的验证挑战。文章比较了传统统计测试和新型量子中心测试（如Statevector测试、Swap测试和新颖的Inverse测试）。通过对大量变异量子电路的实证分析，研究发现量子中心测试，尤其是Statevector测试和Inverse测试，在检测电路差异方面表现出更高的精度和效率，有效降低了误报和漏报。这项工作为开发更可靠、可扩展的量子软件测试策略奠定了基础，有助于推动容错量子计算机的实际应用。", "keywords": "量子单元测试, 量子软件工程, Statevector测试, Inverse测试, 软件验证", "comments": "该论文创新性地提出了针对量子电路的“Inverse test”方法，并将其与传统统计测试及现有量子测试（如Swap test）进行了对比，填补了量子软件单元测试领域的空白。其大规模的实证研究（1,796,880个变异电路）增强了结果的可信度。研究成果对于提升量子软件的可靠性和促进容错量子计算的实际应用具有重要意义。"}}
{"id": "2507.17364", "title": "Quantum Secret Sharing with Classical and Quantum Shares", "authors": ["Hua Sun"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17364v1", "summary": "In quantum secret sharing, a quantum secret state is mapped to multiple\nshares such that shares from qualified sets can recover the secret state and\nshares from other forbidden sets reveal nothing about the secret state; we\nstudy the setting where there are both classical shares and quantum shares. We\nshow that the quantum secret sharing problem with both classical and quantum\nshares is feasible if and only if any two qualified sets have some quantum\nshare in common. Next, for threshold quantum secret sharing where there are\n$N_1$ classical shares, $N_2$ quantum shares and qualified sets consist of any\n$K_1$ (or more) classical shares and any $K_2 > N_2/2$ (or more) quantum\nshares, we show that to share $1$ qubit secret, each classical share needs to\nbe at least $2$ bits and each quantum share needs to be at least $1$ qubit.\nFinally, we characterize the minimum share sizes for quantum secret sharing\nwith at most $2$ classical shares and at most $2$ quantum shares. The converse\nproofs rely on quantum information inequalities and the achievable schemes use\nclassical secret sharing, (encrypted) quantum secret sharing with only quantum\nshares, superdense coding, treating quantum digits as classical digits, and\ntheir various combinations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17364v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "经典份额与量子份额相结合的量子秘密共享", "tldr": "本文研究了同时包含经典和量子份额的量子秘密共享的可行性条件和最小份额大小，并给出了具体实现方案。", "motivation": "在量子秘密共享中，一个量子秘密态被映射到多个份额，使得合格集合的份额可以恢复秘密态，而其他禁止集合的份额不会泄露任何关于秘密态的信息。本文旨在研究同时存在经典份额和量子份额的量子秘密共享设置。", "method": "研究方法包括：1. 证明了同时包含经典和量子份额的量子秘密共享的可行性条件；2. 对于特定的阈值量子秘密共享，分析了共享1个量子比特秘密时经典份额和量子份额的最小大小；3. 刻画了最多2个经典份额和最多2个量子份额的量子秘密共享的最小份额大小。其逆向证明依赖于量子信息不等式，可实现方案则结合了经典秘密共享、（加密的）仅量子份额的量子秘密共享、超密度编码以及将量子数字视为经典数字等多种技术。", "result": "研究结果表明：1. 同时包含经典和量子份额的量子秘密共享是可行的，当且仅当任何两个合格集合都具有一些共同的量子份额。2. 对于阈值量子秘密共享（其中有$N_1$个经典份额，$N_2$个量子份额，合格集合由任何$K_1$个或更多经典份额和任何$K_2 > N_2/2$个或更多量子份额组成），共享1个量子比特秘密时，每个经典份额至少需要2比特，每个量子份额至少需要1量子比特。3. 刻画了最多2个经典份额和最多2个量子份额的量子秘密共享的最小份额大小。", "conclusion": "本文确定了同时包含经典和量子份额的量子秘密共享的可行性条件，并针对特定阈值方案给出了最小份额大小的量化结果，同时提出了实现这些方案的具体技术组合。", "translation": "在量子秘密共享中，一个量子秘密态被映射到多个份额，使得合格集合的份额可以恢复秘密态，而其他禁止集合的份额不会泄露任何关于秘密态的信息；我们研究了同时存在经典份额和量子份额的设置。我们证明了同时包含经典和量子份额的量子秘密共享问题是可行的，当且仅当任何两个合格集合都具有一些共同的量子份额。接下来，对于阈值量子秘密共享，其中有$N_1$个经典份额，$N_2$个量子份额，并且合格集合由任何$K_1$个（或更多）经典份额和任何$K_2 > N_2/2$个（或更多）量子份额组成，我们证明了要共享1个量子比特秘密，每个经典份额至少需要2比特，每个量子份额至少需要1量子比特。最后，我们刻画了最多2个经典份额和最多2个量子份额的量子秘密共享的最小份额大小。逆向证明依赖于量子信息不等式，可实现方案使用经典秘密共享、（加密的）仅量子份额的量子秘密共享、超密度编码、将量子数字视为经典数字以及它们的各种组合。", "summary": "本文深入研究了同时包含经典和量子份额的量子秘密共享（QSS）方案。研究确定了此类混合秘密共享方案的可行性条件：即任何两个合格集合必须至少共享一个量子份额。针对特定的阈值QSS模型，论文量化了共享一个量子比特秘密所需的最小份额大小，指出经典份额至少需2比特，量子份额至少需1量子比特。此外，文章还详细刻画了当经典份额和量子份额数量均不超过2个时的最小份额大小，并探讨了实现这些方案所采用的包括经典秘密共享、超密度编码等在内的多种技术组合。", "keywords": "量子秘密共享, 经典份额, 量子份额, 阈值秘密共享, 份额大小", "comments": "这篇论文的创新点在于首次系统地研究了结合经典和量子份额的量子秘密共享，扩展了传统量子秘密共享的范畴。其重要性在于为混合量子通信协议的设计提供了理论基础和实用指导，特别是对资源受限或需要兼容经典基础设施的场景具有潜在价值。"}}
{"id": "2507.17342", "title": "DeMo++: Motion Decoupling for Autonomous Driving", "authors": ["Bozhou Zhang", "Nan Song", "Xiatian Zhu", "Li Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Journal extension of NeurIPS 2024. arXiv admin note: substantial text overlap with arXiv:2410.05982", "url": "http://arxiv.org/abs/2507.17342v1", "summary": "Motion forecasting and planning are tasked with estimating the trajectories\nof traffic agents and the ego vehicle, respectively, to ensure the safety and\nefficiency of autonomous driving systems in dynamically changing environments.\nState-of-the-art methods typically adopt a one-query-one-trajectory paradigm,\nwhere each query corresponds to a unique trajectory for predicting multi-mode\ntrajectories. While this paradigm can produce diverse motion intentions, it\noften falls short in modeling the intricate spatiotemporal evolution of\ntrajectories, which can lead to collisions or suboptimal outcomes. To overcome\nthis limitation, we propose DeMo++, a framework that decouples motion\nestimation into two distinct components: holistic motion intentions to capture\nthe diverse potential directions of movement, and fine spatiotemporal states to\ntrack the agent's dynamic progress within the scene and enable a\nself-refinement capability. Further, we introduce a cross-scene trajectory\ninteraction mechanism to explore the relationships between motions in adjacent\nscenes. This allows DeMo++ to comprehensively model both the diversity of\nmotion intentions and the spatiotemporal evolution of each trajectory. To\neffectively implement this framework, we developed a hybrid model combining\nAttention and Mamba. This architecture leverages the strengths of both\nmechanisms for efficient scene information aggregation and precise trajectory\nstate sequence modeling. Extensive experiments demonstrate that DeMo++ achieves\nstate-of-the-art performance across various benchmarks, including motion\nforecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and\nend-to-end planning (NAVSIM).", "comment": "Journal extension of NeurIPS 2024. arXiv admin note: substantial text\n  overlap with arXiv:2410.05982", "pdf_url": "http://arxiv.org/pdf/2507.17342v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DeMo++：自动驾驶中的运动解耦", "tldr": "DeMo++提出了一种将运动估计解耦为意图和时空状态的新框架，结合Attention和Mamba混合模型，在自动驾驶运动预测和规划任务中实现了最先进的性能。", "motivation": "现有的自动驾驶运动预测和规划方法，尽管能生成多样的运动意图，但在建模轨迹复杂的时空演变方面表现不足，这可能导致碰撞或次优结果。", "method": "DeMo++框架将运动估计解耦为两个独立组件：整体运动意图（捕捉多样潜在运动方向）和精细时空状态（跟踪动态进展并实现自我完善）。该方法还引入了跨场景轨迹交互机制。为有效实现此框架，开发了一种结合Attention和Mamba的混合模型。", "result": "DeMo++在运动预测（Argoverse 2和nuScenes）、运动规划（nuPlan）和端到端规划（NAVSIM）等多个基准测试中均取得了最先进的性能。", "conclusion": "DeMo++通过解耦运动估计并采用创新的混合Attention-Mamba架构，有效解决了现有方法的局限性，显著提升了自动驾驶系统中运动预测和规划的性能。", "translation": "运动预测和规划分别负责估计交通参与者和自车轨迹，以确保自动驾驶系统在动态变化环境中的安全性和效率。最先进的方法通常采用“一查询一轨迹”范式，其中每个查询对应一个独特的轨迹，用于预测多模态轨迹。虽然这种范式可以产生多样化的运动意图，但它在建模轨迹复杂的时空演变方面往往力不从心，这可能导致碰撞或次优结果。为了克服这一限制，我们提出了DeMo++，一个将运动估计解耦为两个不同组件的框架：整体运动意图，用于捕捉多样的潜在运动方向；以及精细时空状态，用于跟踪智能体在场景中的动态进展并实现自我完善能力。此外，我们引入了跨场景轨迹交互机制，以探索相邻场景中运动之间的关系。这使得DeMo++能够全面建模运动意图的多样性以及每条轨迹的时空演变。为了有效实现该框架，我们开发了一种结合了Attention和Mamba的混合模型。该架构利用这两种机制的优势，实现高效的场景信息聚合和精确的轨迹状态序列建模。广泛的实验表明，DeMo++在各种基准测试中均取得了最先进的性能，包括运动预测（Argoverse 2和nuScenes）、运动规划（nuPlan）和端到端规划（NAVSIM）。", "summary": "DeMo++是一个针对自动驾驶中运动预测和规划的新框架。针对现有方法在建模轨迹时空演变方面的不足，DeMo++将运动估计解耦为整体运动意图和精细时空状态两部分，并引入了跨场景轨迹交互机制。该框架采用Attention和Mamba的混合模型实现，在运动预测、运动规划和端到端规划等多个基准测试中均取得了最先进的性能。", "keywords": "运动解耦, 自动驾驶, 运动预测, 运动规划, Attention-Mamba", "comments": "DeMo++的创新点在于其独特的运动解耦策略，将复杂的运动估计分解为意图和精细状态两个更易处理的部分，并引入了跨场景交互机制。同时，结合Attention和Mamba的混合模型架构，有效利用了两者的优势，实现了对场景信息的高效聚合和轨迹序列的精确建模。该方法在多个关键自动驾驶任务上实现了最先进的性能，显示出其在提高系统安全性和效率方面的重要性。"}}
{"id": "2502.05236", "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance", "authors": ["Shehzeen Hussain", "Paarth Neekhara", "Xuesong Yang", "Edresson Casanova", "Subhankar Ghosh", "Mikyas T. Desta", "Roy Fejgin", "Rafael Valle", "Jason Li"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05236v2", "summary": "While autoregressive speech token generation models produce speech with\nremarkable variety and naturalness, their inherent lack of controllability\noften results in issues such as hallucinations and undesired vocalizations that\ndo not conform to conditioning inputs. We introduce Koel-TTS, a suite of\nenhanced encoder-decoder Transformer TTS models that address these challenges\nby incorporating preference alignment techniques guided by automatic speech\nrecognition and speaker verification models. Additionally, we incorporate\nclassifier-free guidance to further improve synthesis adherence to the\ntranscript and reference speaker audio. Our experiments demonstrate that these\noptimizations significantly enhance target speaker similarity, intelligibility,\nand naturalness of synthesized speech. Notably, Koel-TTS directly maps text and\ncontext audio to acoustic tokens, and on the aforementioned metrics,\noutperforms state-of-the-art TTS models, despite being trained on a\nsignificantly smaller dataset. Audio samples and demos are available on our\nwebsite.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05236v2", "cate": "cs.SD", "date": "2025-02-07", "updated": "2025-07-22", "AI": {"title_translation": "Koel-TTS：通过偏好对齐和无分类器指导增强基于LLM的语音生成", "tldr": "Koel-TTS是一个新的文本到语音（TTS）模型，通过结合偏好对齐和无分类器指导，显著提高了语音合成的可控性、目标说话人相似度、可懂度和自然度，并且在较小数据集上表现优于现有最先进的模型。", "motivation": "现有的自回归语音token生成模型虽然能产生多样且自然的语音，但其固有的缺乏可控性常常导致幻觉和不符合输入条件的非期望发声等问题。", "method": "Koel-TTS是一套增强的编码器-解码器Transformer TTS模型。它通过结合由自动语音识别（ASR）和说话人验证模型引导的偏好对齐技术来解决问题，并融入无分类器指导以进一步提高合成对文本和参考说话人音频的依从性。该模型直接将文本和上下文音频映射到声学token。", "result": "这些优化显著增强了合成语音的目标说话人相似度、可懂度和自然度。Koel-TTS在这些指标上优于最先进的TTS模型，尽管是在显著更小的数据集上训练的。", "conclusion": "Koel-TTS通过引入偏好对齐和无分类器指导，有效解决了自回归语音生成模型的可控性问题，并在较小数据集上实现了卓越的语音合成质量，超越了现有技术。", "translation": "自回归语音token生成模型虽然能产生具有显著多样性和自然度的语音，但其固有的缺乏可控性常常导致幻觉和不符合条件输入的非期望发声等问题。我们引入了Koel-TTS，这是一套增强的编码器-解码器Transformer TTS模型，通过结合由自动语音识别和说话人验证模型引导的偏好对齐技术来解决这些挑战。此外，我们还结合了无分类器指导，以进一步提高合成对文本和参考说话人音频的依从性。我们的实验表明，这些优化显著增强了合成语音的目标说话人相似度、可懂度和自然度。值得注意的是，Koel-TTS直接将文本和上下文音频映射到声学token，并且在上述指标上，尽管在显著更小的数据集上进行训练，它仍优于最先进的TTS模型。音频样本和演示可在我们的网站上获取。", "summary": "Koel-TTS是一个创新的编码器-解码器Transformer文本到语音（TTS）模型，旨在解决自回归语音生成中常见的缺乏可控性、幻觉和不期望发音问题。它通过引入由ASR和说话人验证模型指导的偏好对齐技术，以及无分类器指导来提高合成质量。实验证明，Koel-TTS显著提升了合成语音的说话人相似度、可懂度和自然度，并且在较小数据集上表现优于现有最先进的TTS模型。", "keywords": "语音合成, Transformer, 偏好对齐, 无分类器指导, TTS", "comments": "这篇论文通过引入偏好对齐和无分类器指导，有效解决了自回归语音生成模型在可控性方面的痛点。其创新之处在于将ASR和说话人验证模型融入偏好对齐，并结合无分类器指导，显著提升了合成语音的质量。更重要的是，它在更小数据集上超越了SOTA模型，这表明其方法具有高效率和鲁棒性，对于资源受限的场景具有重要意义。"}}
{"id": "2408.03108", "title": "An explicit factorization of the Green's function for an acoustic half-space problem with impedance boundary conditions into an oscillatory exponential and a slowly varying function", "authors": ["C. Lin", "J. M. Melenk", "S. Sauter"], "categories": ["math.NA", "cs.NA", "31B10, 33C10, 35J08"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2408.03108v2", "summary": "In this paper, new representations of the Green's function for an acoustic\nd-dimensional half-space problem with impedance boundary conditions are\npresented. The main features of the new representation are: a) in addition to\nadditive terms that appear also in the case of Dirichlet or Neumann boundary\nconditions, the remaining part of the Green's function is factored into an\noscillatory complex exponential function (with the product of the wavenumber\nand the eikonal as argument) and a remaining function which is slowly varying\nand hence allows for efficient polynomial approximation; b) the representation\nis given uniformly for all parameters by a single formula which consists of the\nproduct of two analytic functions.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2408.03108v2", "cate": "math.NA", "date": "2024-08-06", "updated": "2025-07-23", "AI": {"title_translation": "具有阻抗边界条件的声学半空间格林函数显式分解为振荡指数函数和慢变函数", "tldr": "论文提出了一种新的格林函数表示方法，将其分解为振荡指数函数和慢变函数，便于高效近似。", "motivation": "论文旨在为具有阻抗边界条件的声学d维半空间问题提供格林函数的新表示。", "method": "提出了一种新的格林函数表示方法，将其分解为一个振荡复指数函数（以波数和程函积为自变量）和一个慢变函数，后者允许高效的多项式近似。该表示通过一个由两个解析函数乘积组成的单一公式统一给出。", "result": "得到了格林函数的新表示，其特点是除了与Dirichlet或Neumann边界条件相似的附加项外，其余部分被分解为一个振荡复指数函数和一个慢变函数，这使得高效的多项式近似成为可能。该表示对于所有参数都是统一的，由一个单一公式给出。", "conclusion": "论文成功提出了格林函数的新颖且统一的表示方法，通过将其分解为振荡指数函数和慢变函数，显著提高了计算效率和近似能力。", "translation": "本论文提出了具有阻抗边界条件的声学d维半空间问题格林函数的新表示。新表示的主要特点是：a) 除了在Dirichlet或Neumann边界条件下也出现的附加项外，格林函数的剩余部分被分解为一个振荡复指数函数（以波数和程函积为自变量）和一个慢变函数，后者因此允许高效的多项式近似；b) 该表示通过一个由两个解析函数乘积组成的单一公式对所有参数统一给出。", "summary": "本文提出了一种针对声学半空间问题中具有阻抗边界条件的格林函数的新颖表示方法。该方法将格林函数分解为一个振荡复指数函数和一个慢变函数，后者能够进行高效的多项式近似。这种统一的表示形式通过单一公式给出，显著提升了格林函数计算的效率和灵活性。", "keywords": "格林函数, 阻抗边界条件, 声学半空间, 显式分解, 多项式近似", "comments": "这篇论文的创新点在于提出了格林函数的一种显式分解形式，将其拆分为易于处理的振荡部分和慢变部分。这种分解对于数值计算具有重要意义，尤其是慢变函数允许高效的多项式近似，可能大幅提升计算效率和精度。该方法的统一性也增加了其普适性。"}}
{"id": "2507.16848", "title": "Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots", "authors": ["Boyu Qiao", "Kun Li", "Wei Zhou", "Songlin Hu"], "categories": ["cs.SI", "cs.AI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16848v1", "summary": "In the human-bot symbiotic information ecosystem, social bots play key roles\nin spreading and correcting disinformation. Understanding their influence is\nessential for risk control and better governance. However, current studies\noften rely on simplistic user and network modeling, overlook the dynamic\nbehavior of bots, and lack quantitative evaluation of correction strategies. To\nfill these gaps, we propose MADD, a Multi Agent based framework for\nDisinformation Dissemination. MADD constructs a more realistic propagation\nnetwork by integrating the Barabasi Albert Model for scale free topology and\nthe Stochastic Block Model for community structures, while designing node\nattributes based on real world user data. Furthermore, MADD incorporates both\nmalicious and legitimate bots, with their controlled dynamic participation\nallows for quantitative analysis of correction strategies. We evaluate MADD\nusing individual and group level metrics. We experimentally verify the real\nworld consistency of MADD user attributes and network structure, and we\nsimulate the dissemination of six disinformation topics, demonstrating the\ndifferential effects of fact based and narrative based correction strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16848v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "用于社交机器人虚假信息传播与纠正的动态模拟框架", "tldr": "该论文提出了MADD，一个基于多智能体的框架，用于模拟虚假信息在社交机器人参与下的传播和纠正，通过更真实的网络和用户建模以及动态机器人行为，实现对纠正策略的定量分析。", "motivation": "当前研究在理解社交机器人对虚假信息的影响时，常依赖简化的用户和网络模型，忽视机器人的动态行为，且缺乏对纠正策略的定量评估。", "method": "论文提出了MADD（Multi Agent based framework for Disinformation Dissemination）框架。MADD通过整合Barabasi-Albert模型（用于无标度拓扑）和随机块模型（用于社区结构）构建更真实的传播网络，并基于真实世界用户数据设计节点属性。此外，MADD纳入了恶意和合法机器人，通过其受控的动态参与实现对纠正策略的定量分析。", "result": "MADD的用户属性和网络结构与真实世界表现出一致性。通过模拟六个虚假信息主题的传播，MADD展示了基于事实和基于叙事的纠正策略之间的不同效果。", "conclusion": "该论文展示了MADD框架在模拟虚假信息传播和评估不同纠正策略方面的有效性。", "translation": "在人机共生信息生态系统中，社交机器人（social bots）在传播和纠正虚假信息方面发挥着关键作用。理解它们的影响对于风险控制和更好的治理至关重要。然而，当前的研究常常依赖于简化的用户和网络建模，忽视了机器人的动态行为，并且缺乏对纠正策略的定量评估。为了弥补这些空白，我们提出了MADD，一个基于多智能体的虚假信息传播框架。MADD通过整合用于无标度拓扑的Barabasi-Albert模型和用于社区结构的随机块模型，构建了一个更真实的传播网络，同时根据真实世界用户数据设计了节点属性。此外，MADD还纳入了恶意和合法机器人，通过它们受控的动态参与，可以对纠正策略进行定量分析。我们使用个体和群体层面的指标评估了MADD。我们通过实验验证了MADD用户属性和网络结构与真实世界的一致性，并模拟了六个虚假信息主题的传播，展示了基于事实和基于叙事的纠正策略的不同效果。", "summary": "本论文引入了MADD，一个基于多智能体的虚假信息传播框架，旨在解决现有研究在社交机器人虚假信息影响方面的局限性。MADD通过结合Barabasi-Albert和随机块模型，构建了更真实的传播网络，并融入了真实世界用户数据以及动态的恶意/合法机器人行为。通过个体和群体层面的评估，MADD展示了与真实世界的一致性，并通过模拟各种虚假信息主题揭示了基于事实和基于叙事的纠正策略的不同影响。", "keywords": "虚假信息, 社交机器人, 模拟框架, 多智能体, 纠正策略", "comments": "该论文的创新之处在于提出了一个更为复杂的模拟框架（MADD），它融合了真实的网络结构、真实世界用户属性以及动态的机器人行为（包括恶意和合法机器人）。这解决了先前模型的主要局限性，使得对纠正策略进行定量评估成为可能，这对于风险控制和治理具有重要意义。"}}
{"id": "2507.17531", "title": "When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment", "authors": ["Abdel-Raouf Dannaoui", "Johann Laconte", "Christophe Debain", "Francois Pomerleau", "Paul Checchin"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 7 figures, proceedings in European Conference on Mobile Robots (ECMR) 2025", "url": "http://arxiv.org/abs/2507.17531v1", "summary": "Robust relocalization in dynamic outdoor environments remains a key challenge\nfor autonomous systems relying on 3D lidar. While long-term localization has\nbeen widely studied, short-term environmental changes, occurring over days or\nweeks, remain underexplored despite their practical significance. To address\nthis gap, we present a highresolution, short-term multi-temporal dataset\ncollected weekly from February to April 2025 across natural and semi-urban\nsettings. Each session includes high-density point cloud maps, 360 deg\npanoramic images, and trajectory data. Projected lidar scans, derived from the\npoint cloud maps and modeled with sensor-accurate occlusions, are used to\nevaluate alignment accuracy against the ground truth using two Iterative\nClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show\nthat Point-to-Plane offers significantly more stable and accurate registration,\nparticularly in areas with sparse features or dense vegetation. This study\nprovides a structured dataset for evaluating short-term localization\nrobustness, a reproducible framework for analyzing scan-to-map alignment under\nnoise, and a comparative evaluation of ICP performance in evolving outdoor\nenvironments. Our analysis underscores how local geometry and environmental\nvariability affect localization success, offering insights for designing more\nresilient robotic systems.", "comment": "7 pages, 7 figures, proceedings in European Conference on Mobile\n  Robots (ECMR) 2025", "pdf_url": "http://arxiv.org/pdf/2507.17531v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "定位何时何地失效：迭代最近点在演变环境中的分析", "tldr": "该研究分析了动态户外环境中短期环境变化对3D激光雷达定位的影响。通过构建一个高分辨率、短期多时间数据集，并比较了两种ICP变体，发现Point-to-Plane在稀疏特征区域提供更稳定准确的配准。研究强调局部几何和环境变异性对定位成功至关重要。", "motivation": "自动系统在动态户外环境中稳健的重定位是一个关键挑战，尤其是在短期环境变化（数天或数周）方面，这方面研究不足但具有实际意义。", "method": "收集了一个高分辨率、短期多时间数据集（2025年2月至4月每周收集），包含高密度点云地图、360度全景图像和轨迹数据。使用投影激光雷达扫描，并考虑传感器精确遮挡，通过Point-to-Point和Point-to-Plane两种ICP变体评估对齐精度。", "result": "Point-to-Plane在配准方面表现出显著更稳定和准确的性能，特别是在特征稀疏或植被茂密的区域。", "conclusion": "局部几何和环境变异性会影响定位成功，为设计更具弹性的机器人系统提供了见解。该研究提供了评估短期定位鲁棒性的结构化数据集、分析噪声下扫描到地图对齐的可复现框架，以及ICP在演变户外环境中性能的比较评估。", "translation": "动态户外环境中稳健的重定位仍然是依赖3D激光雷达的自主系统的关键挑战。虽然长期定位已被广泛研究，但短期环境变化（在数天或数周内发生）尽管具有实际意义，但仍未得到充分探索。为了解决这一差距，我们提出了一个高分辨率、短期多时间数据集，该数据集于2025年2月至4月每周在自然和半城市环境中收集。每次会话都包括高密度点云地图、360度全景图像和轨迹数据。从点云地图派生并用传感器精确遮挡建模的投影激光雷达扫描，用于使用两种迭代最近点（ICP）变体：点到点（Point-to-Point）和点到平面（Point-to-Plane）来评估相对于真实值的对齐精度。结果表明，点到平面（Point-to-Plane）提供了显著更稳定和准确的配准，特别是在特征稀疏或植被茂密的区域。这项研究提供了一个用于评估短期定位鲁棒性的结构化数据集，一个用于分析噪声下扫描到地图对齐的可复现框架，以及ICP在演变户外环境中性能的比较评估。我们的分析强调了局部几何和环境变异性如何影响定位成功，为设计更具弹性的机器人系统提供了见解。", "summary": "这项研究旨在解决动态户外环境中短期环境变化对3D激光雷达定位的挑战。作者构建了一个高分辨率、短期多时间数据集，并利用该数据集比较了两种Iterative Closest Point (ICP) 变体（Point-to-Point和Point-to-Plane）在扫描到地图对齐中的性能。研究发现，Point-to-Plane在配准方面更稳定和准确，尤其是在特征稀疏或植被密集的区域。该工作为评估短期定位鲁棒性提供了数据集和框架，并揭示了局部几何和环境变异性对定位成功的影响，为开发更稳健的机器人系统提供了指导。", "keywords": "定位失效, 迭代最近点, 动态环境, 短期变化, 激光雷达", "comments": "这篇论文通过构建一个独特的高分辨率、短期多时间数据集，填补了短期环境变化对定位影响研究的空白。其创新之处在于提供了实际场景下的数据和可复现的评估框架，并明确指出Point-to-Plane ICP在应对环境变化时的优越性，这对于设计更适应真实世界动态的自主系统具有重要指导意义。"}}
{"id": "2507.17409", "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "url": "http://arxiv.org/abs/2507.17409v1", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "pdf_url": "http://arxiv.org/pdf/2507.17409v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "论证强度主观因素探究：叙事、情感与对冲", "tldr": "本研究通过回归分析量化了叙事、情感和对冲等主观因素对论证强度的影响，发现它们对客观和主观论证质量有不同模式的影响。", "motivation": "尽管已有关于个人故事等主观特征的研究，但缺乏对这些特征与论证强度之间关系的大规模分析，本研究旨在弥补这一空白。", "method": "本研究对情感、叙事和对冲等主观因素进行了回归分析，以量化它们对两个标准数据集（标注了客观论证质量和主观说服力）的影响。同时，本研究还比较和评估了每种主观特征的自动化标注方法。", "result": "回归分析揭示了主观特征对论证强度两个方面（客观质量和主观说服力）的不同影响模式。结果显示，叙事和对冲对客观和主观论证质量有截然不同的影响，而情感的影响取决于其修辞运用而非领域。", "conclusion": "叙事和对冲对论证的客观和主观质量有对比性的影响，而情感的影响取决于其修辞利用。这项研究为理解论证强度中的主观因素提供了新见解。", "translation": "在评估论证强度时，构成良好论证的概念是多方面的。随着自然语言处理（NLP）中将主观性视为一种资产而非问题的大趋势，论证质量的新维度正在被研究。尽管存在针对个人故事等个体主观特征的研究，但缺乏对这些特征与论证强度之间关系的大规模分析。为了弥补这一空白，我们进行了回归分析，以量化主观因素——情感、叙事和对冲——对两个标注了客观论证质量和主观说服力的标准数据集的影响。因此，我们的贡献是双重的：在贡献资源层面，由于目前没有标注了所有研究维度的现有数据集，这项工作比较并评估了每种主观特征的自动化标注方法。在提供新颖见解层面，我们的回归分析揭示了主观特征对数据集中编码的论证强度两个方面（客观论证质量和主观说服力）的不同影响模式。我们的结果表明，叙事和对冲对客观和主观论证质量具有对比性的影响，而情感的影响取决于其修辞运用而非领域。", "summary": "本研究旨在弥补主观特征与论证强度之间大规模分析的缺失。通过对情感、叙事和对冲等主观因素进行回归分析，量化它们对客观论证质量和主观说服力的影响。研究结果表明，叙事和对冲对论证的客观和主观质量有截然不同的影响，而情感的影响则取决于其修辞运用。此外，本研究还比较并评估了这些主观特征的自动化标注方法，为相关资源做出了贡献。", "keywords": "论证强度, 主观因素, 叙事, 情感, 对冲", "comments": "这项研究创新性地将主观因素（叙事、情感和对冲）与论证强度（客观质量和主观说服力）进行大规模量化分析，弥补了现有研究的空白。其贡献不仅在于揭示了主观因素对论证强度的复杂影响模式，还在于为自动化标注这些主观特征提供了评估方法，对NLP领域的主观性研究具有重要意义。"}}
{"id": "2507.17526", "title": "Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling", "authors": ["Leandro Von Krannichfeldt", "Kristina Orehounig", "Olga Fink"], "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17526v1", "summary": "Building energy modeling is a key tool for optimizing the performance of\nbuilding energy systems. Historically, a wide spectrum of methods has been\nexplored -- ranging from conventional physics-based models to purely\ndata-driven techniques. Recently, hybrid approaches that combine the strengths\nof both paradigms have gained attention. These include strategies such as\nlearning surrogates for physics-based models, modeling residuals between\nsimulated and observed data, fine-tuning surrogates with real-world\nmeasurements, using physics-based outputs as additional inputs for data-driven\nmodels, and integrating the physics-based output into the loss function the\ndata-driven model. Despite this progress, two significant research gaps remain.\nFirst, most hybrid methods focus on deterministic modeling, often neglecting\nthe inherent uncertainties caused by factors like weather fluctuations and\noccupant behavior. Second, there has been little systematic comparison within a\nprobabilistic modeling framework. This study addresses these gaps by evaluating\nfive representative hybrid approaches for probabilistic building energy\nmodeling, focusing on quantile predictions of building thermodynamics in a\nreal-world case study. Our results highlight two main findings. First, the\nperformance of hybrid approaches varies across different building room types,\nbut residual learning with a Feedforward Neural Network performs best on\naverage. Notably, the residual approach is the only model that produces\nphysically intuitive predictions when applied to out-of-distribution test data.\nSecond, Quantile Conformal Prediction is an effective procedure for calibrating\nquantile predictions in case of indoor temperature modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17526v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "集成基于物理和数据驱动方法进行概率建筑能耗建模", "tldr": "本研究评估了五种混合方法在概率建筑能耗建模中的应用，发现残差学习在不同房间类型中表现最佳，且分位数共形预测能有效校准室内温度预测。", "motivation": "现有的混合建筑能耗建模方法多侧重于确定性建模，忽略了不确定性因素（如天气波动和居住者行为），并且在概率建模框架内缺乏系统的比较。本研究旨在弥补这些研究空白。", "method": "本研究通过评估五种代表性的混合方法，针对概率建筑能耗建模，重点关注真实案例中建筑热力学分位数预测。此外，还应用了分位数共形预测来校准分位数预测。", "result": "研究结果表明，混合方法的性能因建筑房间类型而异，但使用前馈神经网络的残差学习方法平均表现最佳，并且在应用于分布外测试数据时能产生符合物理直觉的预测。其次，分位数共形预测是一种有效的方法，可用于室内温度建模中的分位数预测校准。", "conclusion": "残差学习与前馈神经网络结合是概率建筑能耗建模中一种有前景的混合方法，尤其在处理不确定性和提供物理直觉预测方面表现出色。分位数共形预测是校准此类模型分位数预测的有效工具。", "translation": "建筑能耗建模是优化建筑能源系统性能的关键工具。历史上，人们探索了多种方法——从传统的基于物理的模型到纯粹的数据驱动技术。最近，结合两种范式优点的混合方法受到了关注。这些策略包括为基于物理的模型学习代理、模拟和观测数据之间的残差建模、用真实世界测量数据微调代理、使用基于物理的输出作为数据驱动模型的额外输入，以及将基于物理的输出集成到数据驱动模型的损失函数中。尽管取得了这些进展，但仍存在两个显著的研究空白。首先，大多数混合方法侧重于确定性建模，往往忽略了由天气波动和居住者行为等因素引起的内在不确定性。其次，在概率建模框架内缺乏系统的比较。本研究通过评估五种代表性的混合方法，解决了这些空白，旨在进行概率建筑能耗建模，重点关注真实案例研究中建筑热力学的分位数预测。我们的结果突出了两个主要发现。首先，混合方法的性能因不同建筑房间类型而异，但使用前馈神经网络的残差学习方法平均表现最佳。值得注意的是，残差方法是唯一在应用于分布外测试数据时能产生符合物理直觉预测的模型。其次，分位数共形预测是室内温度建模中校准分位数预测的有效程序。", "summary": "本研究旨在解决概率建筑能耗建模中存在的两个研究空白：确定性建模对不确定性的忽视以及概率建模框架内缺乏系统比较。通过评估五种代表性的混合方法，并以真实案例研究中建筑热力学的分位数预测为重点，研究发现残差学习与前馈神经网络结合的方法在不同房间类型中平均表现最佳，且能提供符合物理直觉的预测。此外，分位数共形预测被证明是校准室内温度分位数预测的有效程序。", "keywords": "建筑能耗建模, 混合方法, 概率建模, 残差学习, 分位数共形预测", "comments": "本研究的创新点在于系统性地评估了多种混合方法在概率建筑能耗建模中的应用，并特别关注了不确定性处理。其重要性在于为建筑能耗建模领域提供了更鲁棒和实际的解决方案，尤其是在考虑现实世界的不确定性因素时。残差学习方法在提供物理直觉预测方面的表现是一大亮点，而分位数共形预测的应用则增强了模型预测的可靠性。"}}
{"id": "2507.17458", "title": "Distributed P2P quantile tracking with relative value error", "authors": ["Marco Pulimeno", "Italo Epicoco", "Massimo Cafaro"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17458v1", "summary": "In this paper we present \\textsc{DUDDSketch}, a distributed version of the\n\\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm\nis a fully decentralized, gossip-based distributed protocol working in the\ncontext of unstructured P2P networks. We discuss the algorithm's design and\nformally prove its correctness. We also show, through extensive experimental\nresults, that the algorithm converges to the results provided by the sequential\nalgorithm, which is a fundamental and highly desirable property.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17458v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "分布式P2P分位数跟踪与相对值误差", "tldr": "本文提出了DUDDSketch，一种用于在非结构化P2P网络中准确跟踪分位数的分布式、去中心化、基于Gossip的算法，并证明了其正确性及收敛性。", "motivation": "在分布式P2P网络中准确跟踪分位数的需求。", "method": "提出了DUDDSketch算法，它是UDDSketch的分布式版本，采用完全去中心化、基于Gossip的协议，适用于非结构化P2P网络。", "result": "实验结果表明，该算法收敛于顺序算法的结果。", "conclusion": "DUDDSketch算法是正确且有效的，能够实现分布式P2P网络中分位数的准确跟踪。", "translation": "在本文中，我们提出了\\textsc{DUDDSketch}，它是\\textsc{UDDSketch}算法的分布式版本，用于准确跟踪分位数。该算法是一种完全去中心化、基于Gossip的分布式协议，在非结构化P2P网络环境下工作。我们讨论了算法的设计并正式证明了其正确性。我们还通过大量的实验结果表明，该算法收敛于顺序算法提供​​的结果，这是一个基本且非常理想的特性。", "summary": "本文介绍了DUDDSketch，一个用于在非结构化P2P网络中准确跟踪分位数的分布式、去中心化、基于Gossip的算法。研究证明了其正确性，并通过实验验证了其与顺序算法结果的收敛性，突出了其在分布式环境中的实用性和可靠性。", "keywords": "分布式算法, P2P网络, 分位数跟踪, Gossip协议, DUDDSketch", "comments": "该论文的创新点在于提出了DUDDSketch，将UDDSketch扩展到分布式P2P环境，解决了大规模网络中分位数跟踪的挑战。其完全去中心化和基于Gossip的特性使其具有良好的可扩展性和鲁棒性。证明算法的正确性并验证其与顺序算法的收敛性，增强了其理论基础和实际应用价值。"}}
{"id": "2504.03173", "title": "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning Against Data Poisoning Attacks on Non-IID Data", "authors": ["Hongliang Zhang", "Jiguo Yu", "Fenghua Xu", "Chunqiang Hu", "Yongzhao Zhang", "Xiaofen Wang", "Zhongyuan Yu", "Xiaosong Zhang"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.03173v3", "summary": "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.03173v3", "cate": "cs.CR", "date": "2025-04-04", "updated": "2025-07-23", "AI": {"title_translation": "PPFPL：针对非独立同分布数据中毒攻击的跨筒仓隐私保护联邦原型学习", "tldr": "提出PPFPL框架，通过原型学习和双服务器聚合，在非独立同分布数据下有效抵御联邦学习中的数据中毒攻击。", "motivation": "现有的隐私保护联邦学习（PPFL）解决方案在中毒的非独立同分布（Non-IID）数据上，难以有效提升跨筒仓PPFL的性能，且PPFL易受数据投毒攻击。", "method": "本文提出PPFPL框架，通过以下方式实现：采用原型作为客户端提交的模型更新，以消除被篡改数据分布对联邦学习的影响；利用两个服务器通过安全聚合协议实现拜占庭鲁棒聚合，大大减少恶意客户端的影响。", "result": "理论分析证实了PPFPL的收敛性，并在公开数据集上的实验结果表明PPFPL在非独立同分布条件下能有效抵抗数据中毒攻击。", "conclusion": "PPFPL框架通过引入原型学习和双服务器安全聚合，能够有效应对非独立同分布数据下的数据中毒攻击，并保证收敛性。", "translation": "隐私保护联邦学习（PPFL）允许多个客户端通过提交隐藏的模型更新来协同训练深度学习模型。然而，由于客户端的分布式训练性质，PPFL容易受到数据中毒攻击。现有解决方案难以在中毒的非独立同分布（Non-IID）数据中提高跨筒仓PPFL的性能。为了解决这些问题，本文提出了一种名为PPFPL的隐私保护联邦原型学习框架，该框架在中毒的非独立同分布数据中提升了跨筒仓联邦学习的性能，同时有效抵抗了数据中毒攻击。具体而言，我们采用原型作为客户端提交的模型更新，以消除被篡改数据分布对联邦学习的影响。此外，我们利用两个服务器通过安全聚合协议实现拜占庭鲁棒聚合，这大大减少了恶意客户端的影响。理论分析证实了PPFPL的收敛性，并在公开可用数据集上的实验结果表明PPFPL在非独立同分布条件下能有效抵抗数据中毒攻击。", "summary": "本文提出了PPFPL（隐私保护联邦原型学习）框架，旨在解决跨筒仓隐私保护联邦学习在非独立同分布数据下易受数据中毒攻击的挑战。PPFPL通过将原型作为客户端的模型更新来抵消数据篡改的影响，并利用双服务器和安全聚合协议实现拜占庭鲁棒聚合，从而有效抵抗恶意攻击并提升在中毒非独立同分布数据上的性能。理论分析和实验结果均证实了其收敛性和有效性。", "keywords": "隐私保护联邦学习, 数据中毒攻击, 非独立同分布数据, 原型学习, 拜占庭鲁棒聚合", "comments": "这篇论文的创新点在于将原型学习引入到隐私保护联邦学习中，以应对非独立同分布数据下的数据中毒攻击。通过采用原型作为模型更新和引入双服务器安全聚合机制，提高了联邦学习的鲁棒性和性能。这种方法对于增强联邦学习在实际应用中的安全性具有重要意义。"}}
{"id": "2507.16974", "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 9 tables, Appendix A-K", "url": "http://arxiv.org/abs/2507.16974v1", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "comment": "15 pages, 9 tables, Appendix A-K", "pdf_url": "http://arxiv.org/pdf/2507.16974v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "利用合成数据为农业领域的多语言大型语言模型提供问答服务", "tldr": "本研究通过生成多语言合成农业数据集并微调特定语言的LLM，显著提高了LLM在农业领域多语言问答的准确性和相关性，尤其适用于低资源环境。", "motivation": "农民及时获取准确的农业信息至关重要。然而，通用LLM在农业领域缺乏本地化和多语言环境下的精确性，原因是缺乏领域特定训练数据和高质量的区域特定数据集。", "method": "本研究通过从农业特定文档生成多语言合成农业数据集（英语、印地语、旁遮普语），并对特定语言的LLM进行微调，以解决现有模型的局限性。", "result": "与基线模型相比，经过微调的模型在事实准确性、相关性和农业共识方面表现出显著改进。", "conclusion": "研究结果强调了合成数据驱动的、特定语言的微调是提高LLM在农业领域性能的有效策略，尤其是在多语言和低资源环境下。这有助于弥合AI驱动农业解决方案在不同语言社区中的知识鸿沟。", "translation": "及时让农民以其母语获取准确的农业相关信息对于农业领域的成功至关重要。尽管大型语言模型（LLMs）可用于实现问答（QA）系统，但在农业领域简单使用公开的通用LLMs通常提供通用建议，由于领域特定训练不足和高质量、区域特定数据集的稀缺性，缺乏本地和多语言语境下的精确性。我们的研究通过从农业特定文档生成多语言合成农业数据集（英语、印地语、旁遮普语）并微调特定语言的LLMs来解决这些局限性。我们对精心策划的多语言数据集的评估表明，与基线模型相比，经过微调的模型在事实准确性、相关性和农业共识方面有显著改进。这些结果突出了合成数据驱动的、特定语言的微调作为一种有效策略，可以提高LLMs在农业领域的性能，特别是在多语言和低资源环境下。通过提供更准确和本地化的农业咨询服务，本研究为弥合AI驱动的农业解决方案在不同语言社区中的知识鸿沟迈出了有意义的一步。", "summary": "本研究旨在解决大型语言模型（LLMs）在农业领域多语言问答中缺乏精确性的问题。研究通过从农业文档生成多语言（英语、印地语、旁遮普语）合成数据集，并利用这些数据微调特定语言的LLMs。实验结果表明，与基线模型相比，微调后的模型在事实准确性、相关性和农业共识方面均有显著提升。这证明了合成数据驱动的语言特定微调是提高LLMs在农业领域，尤其是在多语言和低资源环境下性能的有效方法，有助于为不同语言社区提供更准确和本地化的农业咨询服务。", "keywords": "合成数据, 大型语言模型, 问答, 农业, 多语言", "comments": "该研究创新性地利用合成数据来解决农业领域LLM在多语言和低资源环境下的数据稀缺问题，为提升AI在农业咨询服务中的准确性和本地化能力提供了切实可行的方案。其强调语言特定微调的有效性，对于推动农业知识普惠具有重要意义。"}}
{"id": "2507.17393", "title": "Partially Reflected Surface (PRS)-Loaded Graphene-Based Patch Antenna for 6G", "authors": ["Omar Osman", "Abdullah Qayyum", "Maziar Nekovee"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17393v1", "summary": "This work investigates a slotted patch antenna integrated with a partially\nreflected surface (PRS) to operate in the TeraHertz (THz) frequency range for\n6G. The antenna is based on graphene material, on a Rogers RT Duroid 6010\nsubstrate. The proposed antenna achieves a bandwidth of 70 GHz (750 GHz to 820\nGHz). The PRS sheet consists of 5x4 unit cells, which are optimised to enhance\nthe overall realized gain of the antenna. The overall realized gain has\nincreased by 1.07 dBi. Also, the PRS enhanced the antenna radiation pattern,\nshowing stable properties over the operating bandwidth. The improved antenna\nperformance is validated via simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17393v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于6G的部分反射面（PRS）负载石墨烯基贴片天线", "tldr": "该研究提出了一种用于6G太赫兹频率范围的石墨烯基PRS负载开槽贴片天线，通过优化PRS显著提高了天线的增益和辐射方向图性能。", "motivation": "为了在6G太赫兹（THz）频率范围内运行，需要开发高性能天线。", "method": "本研究提出了一种集成了部分反射面（PRS）的开槽贴片天线。该天线基于石墨烯材料，并置于Rogers RT Duroid 6010基板上。PRS片由5x4个单元格组成，并经过优化以增强天线的整体实现增益。", "result": "所提出的天线实现了70 GHz（750 GHz至820 GHz）的带宽。整体实现增益增加了1.07 dBi。PRS还增强了天线辐射方向图，在工作带宽内显示出稳定的特性。改进的天线性能通过仿真得到了验证。", "conclusion": "通过集成部分反射面（PRS），石墨烯基贴片天线在太赫兹频率范围内为6G应用提供了显著改进的性能，包括更宽的带宽、更高的增益和稳定的辐射方向图。", "translation": "这项工作研究了一种集成了部分反射面（PRS）的开槽贴片天线，用于在太赫兹（THz）频率范围内为6G运行。该天线基于石墨烯材料，置于Rogers RT Duroid 6010基板上。所提出的天线实现了70 GHz（750 GHz至820 GHz）的带宽。PRS片由5x4个单元格组成，经过优化以增强天线的整体实现增益。整体实现增益增加了1.07 dBi。此外，PRS增强了天线辐射方向图，在工作带宽内显示出稳定的特性。改进的天线性能通过仿真得到了验证。", "summary": "本研究提出了一种用于6G太赫兹频段的石墨烯基开槽贴片天线，该天线集成了部分反射面（PRS）。该天线在Rogers RT Duroid 6010基板上构建，实现了70 GHz的带宽（750-820 GHz）。通过优化PRS，天线的实现增益提高了1.07 dBi，并且辐射方向图在整个工作带宽内保持稳定。所有改进的性能均通过仿真得到验证。", "keywords": "石墨烯, 贴片天线, PRS, 6G, 太赫兹", "comments": "该论文的创新点在于将石墨烯材料与部分反射面（PRS）技术结合应用于6G太赫兹频段的贴片天线设计。这种结合有效地提高了天线的带宽、增益和辐射方向图的稳定性，为未来6G通信系统中的高频天线设计提供了有价值的参考。其局限性可能在于目前仅通过仿真验证了性能，实际制造和测试可能面临挑战。"}}
{"id": "2507.17249", "title": "R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems", "authors": ["Hao Gu", "Rui Zhong", "Yu Xia", "Wei Yang", "Chi Lu", "Peng Jiang", "Kun Gai"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by Recsys25", "url": "http://arxiv.org/abs/2507.17249v1", "summary": "Harnessing Large Language Models (LLMs) for recommendation systems has\nemerged as a prominent avenue, drawing substantial research interest. However,\nexisting approaches primarily involve basic prompt techniques for knowledge\nacquisition, which resemble System-1 thinking. This makes these methods highly\nsensitive to errors in the reasoning path, where even a small mistake can lead\nto an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a\nreasoning, reflection and refinement framework that evolves the recommendation\nsystem into a weak System-2 model. Specifically, we introduce two models: an\nactor model that engages in reasoning, and a reflection model that judges these\nresponses and provides valuable feedback. Then the actor model will refine its\nresponse based on the feedback, ultimately leading to improved responses. We\nemploy an iterative reflection and refinement process, enabling LLMs to\nfacilitate slow and deliberate System-2-like thinking. Ultimately, the final\nrefined knowledge will be incorporated into a recommendation backbone for\nprediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M\ndatasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec\non a large scale online advertising platform, showing 2.2\\% increase of\nrevenue. Furthermore, we investigate the scaling properties of the actor model\nand reflection model.", "comment": "Accepted by Recsys25", "pdf_url": "http://arxiv.org/pdf/2507.17249v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "R4ec：一个用于推荐系统的推理、反思和优化框架", "tldr": "R4ec是一个基于LLM的推荐系统框架，通过推理、反思和优化实现类System-2思考，克服了现有方法的错误敏感性，并在实验和实际部署中表现出色。", "motivation": "现有的将大型语言模型（LLMs）应用于推荐系统的方法主要依赖于简单的提示技术进行知识获取，这类似于System-1思维，对推理路径中的错误高度敏感，一个小错误就可能导致不正确的推断。", "method": "论文提出了$R^4$ec框架，它将推荐系统发展为一种弱System-2模型。该框架包含两个核心模型：一个负责推理的“执行者模型”（actor model）和一个负责判断响应并提供反馈的“反思模型”（reflection model）。执行者模型根据反思模型的反馈迭代地优化其响应，从而实现LLM的缓慢而审慎的System-2式思考。最终，优化后的知识将被整合到推荐骨干网络中进行预测。", "result": "在Amazon-Book和MovieLens-1M数据集上进行了广泛实验，证明了$R^4$ec的优越性。此外，将$R^4$ec部署在一个大型在线广告平台，实现了2.2%的收入增长。研究还探讨了执行者模型和反思模型的扩展特性。", "conclusion": "R4ec框架通过引入推理、反思和优化过程，成功地将LLM驱动的推荐系统提升到更接近System-2的思维模式，显著提高了推荐性能和实际应用效果，克服了现有System-1方法的错误敏感性。", "translation": "将大型语言模型（LLMs）应用于推荐系统已成为一个重要的研究方向，并引起了广泛的研究兴趣。然而，现有方法主要涉及用于知识获取的基本提示技术，这类似于系统1思维。这使得这些方法对推理路径中的错误高度敏感，即使是一个小错误也可能导致不正确的推断。为此，在本文中，我们提出了$R^{4}$ec，一个推理、反思和优化框架，它将推荐系统演变为一个弱系统2模型。具体来说，我们引入了两个模型：一个从事推理的执行者模型，以及一个判断这些响应并提供有价值反馈的反思模型。然后执行者模型将根据反馈优化其响应，最终导致改进的响应。我们采用迭代的反思和优化过程，使LLMs能够促进缓慢而审慎的系统2式思考。最终，最终优化的知识将被整合到推荐骨干网络中进行预测。我们在Amazon-Book和MovieLens-1M数据集上进行了广泛实验，以证明$R^{4}$ec的优越性。我们还在一个大型在线广告平台部署了$R^{4}$ec，显示收入增加了2.2%。此外，我们还研究了执行者模型和反思模型的扩展特性。", "summary": "本文提出了$R^4$ec框架，旨在将大型语言模型（LLMs）驱动的推荐系统从易错的System-1思维提升到更稳健的System-2思维。$R^4$ec通过引入一个执行者模型进行推理和一个反思模型提供反馈，实现迭代的响应优化。这种机制使得LLM能够进行深思熟虑的推理，并将最终优化的知识用于推荐预测。实验证明，$R^4$ec在多个数据集上表现出优越性，并在实际在线广告平台部署中带来了显著的收入增长。", "keywords": "大型语言模型, 推荐系统, 推理, 反思, 优化, System-2思维", "comments": "这篇论文的创新点在于将认知心理学中的System-1和System-2思维模式引入到LLM驱动的推荐系统中，通过构建推理、反思和优化循环来提高模型的鲁棒性和准确性。它解决了现有LLM-based推荐系统对推理路径错误敏感的痛点，并通过实际部署效果证明了其商业价值。这是一个将LLM能力深度挖掘并应用于特定领域的重要进展。"}}
{"id": "2504.15181", "title": "Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures", "authors": ["Lily Stelling", "Mick Yang", "Rokas Gipiškis", "Leon Staufer", "Ze Shen Chin", "Siméon Campos", "Ariel Gil", "Michael Chen"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      166 pages, the Oxford Martin AI Governance Initiative", "url": "http://arxiv.org/abs/2504.15181v2", "summary": "This report provides a detailed comparison between the Safety and Security\nmeasures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice\n(Third Draft) and the current commitments and practices voluntarily adopted by\nleading AI companies. As the EU moves toward enforcing binding obligations for\nGPAI model providers, the Code of Practice will be key for bridging legal\nrequirements with concrete technical commitments. Our analysis focuses on the\ndraft's Safety and Security section (Commitments II.1-II.16), documenting\nexcerpts from current public-facing documents that are relevant to each\nindividual measure.\n  We systematically reviewed different document types, such as companies'\nfrontier safety frameworks and model cards, from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance, nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and General-Purpose AI model providers by surfacing evidence of\nindustry precedent for various measures. Nonetheless, we were able to find\nrelevant quotes from at least 5 companies' documents for the majority of the\nmeasures in Commitments II.1-II.16.", "comment": "166 pages, the Oxford Martin AI Governance Initiative", "pdf_url": "http://arxiv.org/pdf/2504.15181v2", "cate": "cs.CY", "date": "2025-04-21", "updated": "2025-07-22", "AI": {"title_translation": "将行业实践映射到欧盟人工智能法案的GPAI行为准则安全和保障措施", "tldr": "本报告对比了欧盟人工智能法案中通用人工智能（GPAI）行为准则的安全措施与领先AI公司当前的行业实践，旨在为监管对话提供信息。", "motivation": "随着欧盟致力于对通用人工智能（GPAI）模型提供商实施具有约束力的义务，行为准则将成为连接法律要求与具体技术承诺的关键。本报告旨在通过揭示行业先例的证据，为监管机构和通用人工智能模型提供商之间正在进行的对话提供信息。", "method": "系统地审查了来自OpenAI、Anthropic、Google DeepMind、Microsoft、Meta、Amazon等十多家公司的不同类型的公开文件，如前沿安全框架和模型卡。分析重点是欧盟人工智能法案GPAI行为准则（第三稿）的安全和保障部分（承诺II.1-II.16）。", "result": "对于承诺II.1-II.16中的大多数措施，研究人员能够从至少5家公司的文件中找到相关引述，这表明行业对各种措施存在先例。", "conclusion": "本报告旨在通过揭示行业先例的证据，为监管机构和通用人工智能模型提供商之间正在进行的对话提供信息，而不是作为法律合规性的指示，也不对行为准则或公司的政策采取任何规定性观点。", "translation": "本报告详细比较了欧盟人工智能法案通用人工智能（GPAI）行为准则（第三稿）中提出的安全和保障措施与领先AI公司自愿采纳的现有承诺和实践。随着欧盟开始对GPAI模型提供商实施具有约束力的义务，该行为准则将成为连接法律要求与具体技术承诺的关键。我们的分析侧重于草案的安全和保障部分（承诺II.1-II.16），记录了与每项措施相关的当前公开文件中摘录的内容。\n我们系统地审查了来自OpenAI、Anthropic、Google DeepMind、Microsoft、Meta、Amazon等十多家公司的不同类型文件，如公司的前沿安全框架和模型卡。本报告并非旨在指示法律合规性，也不对行为准则或公司的政策采取任何规定性观点。相反，它旨在通过揭示各种措施的行业先例证据，为监管机构和通用人工智能模型提供商之间正在进行的对话提供信息。尽管如此，我们仍能从至少5家公司的文件中找到与承诺II.1-II.16中大多数措施相关的引述。", "summary": "本报告系统地比较了欧盟人工智能法案通用人工智能（GPAI）行为准则（第三稿）中提出的安全和保障措施与主要AI公司自愿采纳的现有实践。通过审查来自十多家公司的公开文档，报告旨在揭示各项措施的行业先例，从而在欧盟迈向对GPAI模型提供商实施具有约束力义务的过程中，为监管机构与AI模型提供商之间的持续对话提供信息。", "keywords": "欧盟人工智能法案, GPAI, 行为准则, 安全与保障, 行业实践", "comments": "该报告对于弥合新兴AI法规与现有行业实践之间的差距至关重要。其价值在于为监管讨论提供了事实依据，突出了当前行业努力与拟议法律框架的契合或差异之处。报告明确避免了法律合规性评估或规定性观点，这在保持中立性方面是一个优势，但在提供直接政策建议方面也构成局限性。对主要参与者公开文件的系统审查增加了其可信度。"}}
{"id": "2507.17141", "title": "Towards Human-level Intelligence via Human-like Whole-Body Manipulation", "authors": ["Guang Gao", "Jianan Wang", "Jinbo Zuo", "Junnan Jiang", "Jingfan Zhang", "Xianwen Zeng", "Yuejiang Zhu", "Lianyang Ma", "Ke Chen", "Minhua Sheng", "Ruirui Zhang", "Zhaohui An"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17141v1", "summary": "Building general-purpose intelligent robots has long been a fundamental goal\nof robotics. A promising approach is to mirror the evolutionary trajectory of\nhumans: learning through continuous interaction with the environment, with\nearly progress driven by the imitation of human behaviors. Achieving this goal\npresents three core challenges: (1) designing safe robotic hardware with\nhuman-level physical capabilities; (2) developing an intuitive and scalable\nwhole-body teleoperation interface for data collection; and (3) creating\nalgorithms capable of learning whole-body visuomotor policies from human\ndemonstrations. To address these challenges in a unified framework, we propose\nAstribot Suite, a robot learning suite for whole-body manipulation aimed at\ngeneral daily tasks across diverse environments. We demonstrate the\neffectiveness of our system on a wide range of activities that require\nwhole-body coordination, extensive reachability, human-level dexterity, and\nagility. Our results show that Astribot's cohesive integration of embodiment,\nteleoperation interface, and learning pipeline marks a significant step towards\nreal-world, general-purpose whole-body robotic manipulation, laying the\ngroundwork for the next generation of intelligent robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17141v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过类人全身操作迈向人类水平智能", "tldr": "本文提出了Astribot Suite，一个机器人学习套件，旨在通过解决硬件设计、遥操作界面和学习算法的挑战，实现类人全身操作，从而推动通用智能机器人的发展。", "motivation": "机器人学的一个基本目标是构建通用智能机器人，一个有前景的方法是模仿人类的进化轨迹：通过与环境的持续交互学习，并以模仿人类行为来驱动早期进展。实现这一目标面临三个核心挑战：1) 设计具有人类水平物理能力的机器人硬件；2) 开发直观且可扩展的全身遥操作界面以进行数据收集；3) 创建能够从人类演示中学习全身视觉运动策略的算法。", "method": "为应对上述挑战，本文提出了Astribot Suite，这是一个用于全身操作的机器人学习套件，旨在处理各种环境中的通用日常任务。", "result": "Astribot Suite在需要全身协调、广泛可达性、人类水平灵活性和敏捷性的大量活动中展示了其系统的有效性。结果表明，Astribot将实体、遥操作界面和学习管线紧密整合，标志着向真实世界、通用全身机器人操作迈出了重要一步。", "conclusion": "Astribot Suite为下一代智能机器人奠定了基础。", "translation": "构建通用智能机器人一直是机器人学的一个基本目标。一个有前景的方法是模仿人类的进化轨迹：通过与环境的持续交互学习，早期进展由模仿人类行为驱动。实现这一目标面临三个核心挑战：(1) 设计具有人类水平物理能力的、安全的机器人硬件；(2) 开发一个直观且可扩展的全身遥操作界面以进行数据收集；(3) 创建能够从人类演示中学习全身视觉运动策略的算法。为了在一个统一的框架中解决这些挑战，我们提出了Astribot Suite，这是一个用于全身操作的机器人学习套件，旨在处理各种环境中的通用日常任务。我们在一系列需要全身协调、广泛可达性、人类水平灵活性和敏捷性的活动中展示了我们系统的有效性。我们的结果表明，Astribot将实体、遥操作界面和学习管线紧密整合，标志着向真实世界、通用全身机器人操作迈出了重要一步，为下一代智能机器人奠定了基础。", "summary": "本文提出了Astribot Suite，一个机器人学习套件，旨在通过模仿人类学习方式，解决通用智能机器人全身操作面临的硬件、遥操作和学习算法三大挑战。该系统在需要全身协调和高灵活性的任务中表现出有效性，其整合的体现、遥操作界面和学习管线为实现真实世界通用全身机器人操作迈出了重要一步，为未来智能机器人奠定了基础。", "keywords": "全身操作, 机器人学习, 通用智能机器人, 遥操作, Astribot Suite", "comments": "Astribot Suite通过统一的框架解决了通用智能机器人全身操作的关键挑战，其创新点在于将硬件、遥操作界面和学习算法紧密结合。这为实现人类水平的全身操作提供了有前景的方向，并为未来机器人技术的发展奠定了坚实基础。"}}
{"id": "2507.17151", "title": "PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training", "authors": ["Anirudh Satheesh", "Anant Khandelwal", "Mucong Ding", "Radu Balan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to TMLR 2025", "url": "http://arxiv.org/abs/2507.17151v1", "summary": "Neural operators offer a powerful paradigm for solving partial differential\nequations (PDEs) that cannot be solved analytically by learning mappings\nbetween function spaces. However, there are two main bottlenecks in training\nneural operators: they require a significant amount of training data to learn\nthese mappings, and this data needs to be labeled, which can only be accessed\nvia expensive simulations with numerical solvers. To alleviate both of these\nissues simultaneously, we propose PICore, an unsupervised coreset selection\nframework that identifies the most informative training samples without\nrequiring access to ground-truth PDE solutions. PICore leverages a\nphysics-informed loss to select unlabeled inputs by their potential\ncontribution to operator learning. After selecting a compact subset of inputs,\nonly those samples are simulated using numerical solvers to generate labels,\nreducing annotation costs. We then train the neural operator on the reduced\nlabeled dataset, significantly decreasing training time as well. Across four\ndiverse PDE benchmarks and multiple coreset selection strategies, PICore\nachieves up to 78% average increase in training efficiency relative to\nsupervised coreset selection methods with minimal changes in accuracy. We\nprovide code at https://github.com/Asatheesh6561/PICore.", "comment": "Submitted to TMLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.17151v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PICore：物理信息无监督核心集选择，用于数据高效的神经算子训练", "tldr": "神经算子训练需要大量昂贵模拟产生的标记数据。PICore通过物理信息损失无监督地选择信息量最大的未标记数据，仅对选定数据进行标记，显著提高训练效率并降低成本。", "motivation": "神经算子训练需要大量训练数据，且这些数据需要通过昂贵的数值模拟进行标记，这构成了主要的瓶颈。", "method": "PICore是一个无监督的核心集选择框架。它利用物理信息损失来选择信息量最大的未标记输入样本。只有选定的紧凑子集才通过数值求解器进行模拟以生成标签。之后，神经算子在减少的标记数据集上进行训练。", "result": "PICore在四个不同的PDE基准和多种核心集选择策略中，相对于监督核心集选择方法，训练效率平均提高了78%，且准确性变化最小。", "conclusion": "PICore显著提高了神经算子训练的数据效率，通过降低数据标注成本和训练时间，同时保持了准确性。", "translation": "神经算子提供了一种强大的范式，通过学习函数空间之间的映射来解决无法解析求解的偏微分方程（PDEs）。然而，训练神经算子存在两个主要的瓶颈：它们需要大量的训练数据来学习这些映射，并且这些数据需要被标记，这只能通过使用数值求解器进行昂贵的模拟来获取。为了同时缓解这两个问题，我们提出了PICore，一个无监督的核心集选择框架，它可以在不需要访问真实PDE解的情况下识别信息量最大的训练样本。PICore利用物理信息损失来根据其对算子学习的潜在贡献选择未标记的输入。在选择了一个紧凑的输入子集后，只有这些样本使用数值求解器进行模拟以生成标签，从而降低了标注成本。然后，我们在减少的标记数据集上训练神经算子，也显著减少了训练时间。在四个不同的PDE基准和多种核心集选择策略中，PICore相对于监督核心集选择方法，在准确性变化最小的情况下，实现了高达78%的平均训练效率提升。我们提供了代码，网址为https://github.com/Asatheesh6561/PICore。", "summary": "本研究提出了PICore，一个无监督的核心集选择框架，旨在解决神经算子训练中数据量大且标注成本高的问题。PICore利用物理信息损失从无标签数据中识别信息量最大的样本，然后仅对这些选定的样本进行昂贵的数值模拟以获取标签。通过在减少的标记数据集上训练神经算子，PICore显著提高了训练效率并降低了标注成本，同时保持了准确性。实验结果表明，PICore在多个PDE基准测试中，相对于监督核心集选择方法，训练效率平均提高了78%。", "keywords": "神经算子, 核心集选择, 物理信息, 无监督学习, 数据效率", "comments": "PICore的创新之处在于其无监督的核心集选择方法，利用物理信息损失来识别最具信息量的样本，从而避免了对大量昂贵标记数据的需求。这对于提高神经算子在实际应用中的可行性至关重要，因为它解决了数据稀缺和标注成本高昂的关键瓶颈。"}}
{"id": "2507.17662", "title": "Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with Sequential Mixture of Experts for Multi-View Mammography", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Nicole Simone", "Arash Mohammadi"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17662v1", "summary": "Breast cancer (BC) remains one of the leading causes of cancer-related\nmortality among women, despite recent advances in Computer-Aided Diagnosis\n(CAD) systems. Accurate and efficient interpretation of multi-view mammograms\nis essential for early detection, driving a surge of interest in Artificial\nIntelligence (AI)-powered CAD models. While state-of-the-art multi-view\nmammogram classification models are largely based on Transformer architectures,\ntheir computational complexity scales quadratically with the number of image\npatches, highlighting the need for more efficient alternatives. To address this\nchallenge, we propose Mammo-Mamba, a novel framework that integrates Selective\nState-Space Models (SSMs), transformer-based attention, and expert-driven\nfeature refinement into a unified architecture. Mammo-Mamba extends the\nMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)\nmechanism through its customized SecMamba block. The SecMamba is a modified\nMambaVision block that enhances representation learning in high-resolution\nmammographic images by enabling content-adaptive feature refinement. These\nblocks are integrated into the deeper stages of MambaVision, allowing the model\nto progressively adjust feature emphasis through dynamic expert gating,\neffectively mitigating the limitations of traditional Transformer models.\nEvaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior\nclassification performance across all key metrics while maintaining\ncomputational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17662v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Mammo-Mamba: 一种混合状态空间和Transformer架构，带有多视图乳腺X线摄影的序列专家混合模型", "tldr": "Mammo-Mamba 结合状态空间模型和Transformer，通过专家混合机制，在保持效率的同时提高了多视图乳腺X线摄影的癌症分类性能。", "motivation": "乳腺癌是女性癌症相关死亡的主要原因之一，早期检测至关重要。尽管计算机辅助诊断系统有所进展，但现有基于Transformer的多视图乳腺X线摄影分类模型计算复杂度高，需要更高效的替代方案。", "method": "提出Mammo-Mamba框架，它整合了选择性状态空间模型（SSMs）、基于Transformer的注意力机制和专家驱动的特征细化。Mammo-Mamba通过定制的SecMamba块引入序列专家混合（SeqMoE）机制，扩展了MambaVision骨干网络。SecMamba块通过内容自适应特征细化增强高分辨率乳腺X线图像的表示学习，并集成到MambaVision的更深层阶段，通过动态专家门控逐步调整特征强调，以减轻传统Transformer模型的局限性。", "result": "在CBIS-DDSM基准数据集上进行评估，Mammo-Mamba在所有关键指标上均实现了卓越的分类性能，同时保持了计算效率。", "conclusion": "Mammo-Mamba通过结合SSMs、Transformer注意力和专家驱动的特征细化，并引入SeqMoE机制，成功解决了传统Transformer模型在多视图乳腺X线摄影分类中计算复杂度高的问题，并在性能和效率上超越了现有模型。", "translation": "乳腺癌 (BC) 仍然是女性癌症相关死亡的主要原因之一，尽管计算机辅助诊断 (CAD) 系统取得了最新进展。准确高效地解读多视图乳腺X线摄影对于早期检测至关重要，这推动了对人工智能 (AI) 驱动的 CAD 模型的浓厚兴趣。虽然最先进的多视图乳腺X线摄影分类模型主要基于 Transformer 架构，但它们的计算复杂度与图像块的数量呈二次方关系，突显了对更高效替代方案的需求。为了应对这一挑战，我们提出了 Mammo-Mamba，一个新颖的框架，它将选择性状态空间模型 (SSMs)、基于 Transformer 的注意力和专家驱动的特征细化集成到一个统一的架构中。Mammo-Mamba 通过其定制的 SecMamba 块引入了序列专家混合 (SeqMoE) 机制，扩展了 MambaVision 骨干网络。SecMamba 是一个改进的 MambaVision 块，通过实现内容自适应特征细化来增强高分辨率乳腺X线图像中的表示学习。这些块被集成到 MambaVision 的更深层阶段，允许模型通过动态专家门控逐步调整特征强调，有效缓解了传统 Transformer 模型的局限性。在 CBIS-DDSM 基准数据集上进行评估，Mammo-Mamba 在所有关键指标上均实现了卓越的分类性能，同时保持了计算效率。", "summary": "本文提出了Mammo-Mamba，一种结合选择性状态空间模型、Transformer注意力及专家驱动特征细化的新型混合架构，用于多视图乳腺X线摄影的癌症诊断。该模型通过定制的SecMamba块引入序列专家混合机制，有效提升了高分辨率图像的表示学习能力，并解决了传统Transformer模型计算复杂度高的问题。在CBIS-DDSM数据集上的评估显示，Mammo-Mamba在保持计算效率的同时，实现了卓越的分类性能。", "keywords": "乳腺癌、多视图乳腺X线摄影、混合架构、状态空间模型、Transformer、专家混合", "comments": "Mammo-Mamba的创新之处在于其混合架构设计，将SSMs和Transformer的优势结合，并通过SeqMoE机制实现内容自适应特征细化，有效解决了Transformer在处理高分辨率医学图像时计算效率低的问题。这对于乳腺癌的早期诊断具有重要意义，因为它提供了一个既高效又准确的AI诊断模型。"}}
{"id": "2507.16937", "title": "Fractional Spike Differential Equations Neural Network with Efficient Adjoint Parameters Training", "authors": ["Chengjie Ge", "Yufeng Peng", "Xueyang Fu", "Qiyu Kang", "Xuhao Li", "Qixin Zhang", "Junhao Ren", "Zheng-Jun Zha"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16937v1", "summary": "Spiking Neural Networks (SNNs) draw inspiration from biological neurons to\ncreate realistic models for brain-like computation, demonstrating effectiveness\nin processing temporal information with energy efficiency and biological\nrealism. Most existing SNNs assume a single time constant for neuronal membrane\nvoltage dynamics, modeled by first-order ordinary differential equations (ODEs)\nwith Markovian characteristics. Consequently, the voltage state at any time\ndepends solely on its immediate past value, potentially limiting network\nexpressiveness. Real neurons, however, exhibit complex dynamics influenced by\nlong-term correlations and fractal dendritic structures, suggesting\nnon-Markovian behavior. Motivated by this, we propose the Fractional SPIKE\nDifferential Equation neural network (fspikeDE), which captures long-term\ndependencies in membrane voltage and spike trains through fractional-order\ndynamics. These fractional dynamics enable more expressive temporal patterns\nbeyond the capability of integer-order models. For efficient training of\nfspikeDE, we introduce a gradient descent algorithm that optimizes parameters\nby solving an augmented fractional-order ODE (FDE) backward in time using\nadjoint sensitivity methods. Extensive experiments on diverse image and graph\ndatasets demonstrate that fspikeDE consistently outperforms traditional SNNs,\nachieving superior accuracy, comparable energy efficiency, reduced training\nmemory usage, and enhanced robustness against noise. Our approach provides a\nnovel open-sourced computational toolbox for fractional-order SNNs, widely\napplicable to various real-world tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16937v1", "cate": "cs.NE", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "分数阶尖峰微分方程神经网络与高效伴随参数训练", "tldr": "提出分数阶尖峰微分方程神经网络（fspikeDE），通过分数阶动力学捕捉长期依赖，并利用伴随灵敏度方法高效训练，在多个数据集上表现优异。", "motivation": "现有SNNs假设单一时间常数和马尔可夫特性，限制了网络表达能力；而真实神经元具有复杂的非马尔可夫动力学和长期关联。", "method": "提出分数阶尖峰微分方程神经网络（fspikeDE），通过分数阶动力学捕捉膜电压和脉冲序列的长期依赖性；引入一种梯度下降算法，利用伴随灵敏度方法反向求解增强分数阶ODE来优化参数。", "result": "fspikeDE在图像和图数据集上持续优于传统SNNs，实现更高的准确性、相当的能效、更低的训练内存使用和更强的抗噪声鲁棒性。", "conclusion": "该方法为分数阶SNNs提供了一个新颖的开源计算工具箱，广泛适用于各种现实世界任务。", "translation": "尖峰神经网络（SNNs）从生物神经元中汲取灵感，创建了逼真的类脑计算模型，在处理时间信息方面表现出能量效率和生物真实性。大多数现有SNNs假设神经元膜电压动力学具有单一时间常数，并通过具有马尔可夫特性的一阶常微分方程（ODEs）建模。因此，任何时刻的电压状态仅取决于其即时过去值，这可能限制网络的表达能力。然而，真实神经元表现出受长期相关性和分形树突结构影响的复杂动力学，这表明存在非马尔可夫行为。受此启发，我们提出了分数阶尖峰微分方程神经网络（fspikeDE），它通过分数阶动力学捕捉膜电压和尖峰序列中的长期依赖性。这些分数阶动力学能够实现超越整数阶模型能力的更具表达性的时间模式。为了高效训练fspikeDE，我们引入了一种梯度下降算法，该算法通过使用伴随灵敏度方法反向求解增强分数阶ODE来优化参数。在各种图像和图数据集上的大量实验表明，fspikeDE持续优于传统SNNs，实现了更高的准确性、相当的能效、更低的训练内存使用和增强的抗噪声鲁棒性。我们的方法为分数阶SNNs提供了一个新颖的开源计算工具箱，广泛适用于各种现实世界任务。", "summary": "本文提出了分数阶尖峰微分方程神经网络（fspikeDE），以解决传统SNNs在建模神经元复杂非马尔可夫动力学方面的局限性。fspikeDE通过分数阶动力学捕捉膜电压和脉冲序列的长期依赖性，并引入基于伴随灵敏度方法的梯度下降算法进行高效训练。实验结果表明，fspikeDE在准确性、能效、内存使用和抗噪声方面均优于传统SNNs，并提供了一个开源计算工具箱。", "keywords": "尖峰神经网络, 分数阶微分方程, 伴随灵敏度, 长期依赖, 神经网络训练", "comments": "本文的创新点在于将分数阶动力学引入SNNs，以更好地模拟真实神经元的长期依赖和非马尔可夫特性，从而增强了网络的表达能力。此外，提出的基于伴随灵敏度的高效训练算法解决了分数阶模型训练的挑战。这项工作为SNNs的研究开辟了新的方向，并提供了一个实用的开源工具。"}}
{"id": "2503.23956", "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference", "authors": ["Kai Huang", "Hao Zou", "Bochen Wang", "Ye Xi", "Zhen Xie", "Hao Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23956v3", "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23956v3", "cate": "cs.CV", "date": "2025-03-31", "updated": "2025-07-23", "AI": {"title_translation": "AirCache：激活跨模态相关性KV缓存压缩以实现高效大型视觉语言模型推理", "tldr": "AirCache提出了一种新颖的KV缓存压缩方法，通过利用视觉令牌冗余和自适应分配策略，显著减少大型视觉语言模型（LVLMs）的解码延迟，同时保持性能。", "motivation": "大型视觉语言模型（LVLMs）处理大量视觉令牌和生成长上下文输出时会产生巨大的计算开销，导致对键值（KV）缓存的过高需求，形成关键瓶颈。", "method": "本文提出了AirCache，一种新颖的KV缓存压缩方法，旨在加速LVLMs的推理。该方法系统地研究了LVLMs注意力机制中视觉和文本令牌之间的相关性，发现缓存的视觉令牌存在大量冗余。受此启发，引入了一个精英观察窗口来评估KV缓存中视觉组件的重要性，重点是具有增强多视角一致性的稳定跨模态相关性建模。此外，开发了一种自适应分层预算分配策略，利用令牌重要性分布的强度和偏度。", "result": "AirCache在保留仅10%视觉KV缓存的情况下，实现了与完整缓存相当的性能。该方法在不同批次大小和提示长度的输入下，将解码延迟降低了29%至66%。值得注意的是，随着缓存保留率的降低，该方法比现有方法表现出越来越大的性能优势。", "conclusion": "AirCache通过有效的KV缓存压缩显著提升了大型视觉语言模型（LVLMs）的推理效率，在大幅减少资源需求的同时保持了模型性能，尤其在低缓存保留率下表现出优越性。", "translation": "大型视觉语言模型（LVLMs）的最新进展因其卓越的推理能力和泛化能力而受到广泛关注。然而，处理大量视觉令牌和生成长上下文输出会带来巨大的计算开销，导致对键值（KV）缓存的过高需求。为了解决这一关键瓶颈，我们提出了AirCache，一种新颖的KV缓存压缩方法，旨在加速LVLMs的推理。这项工作系统地研究了LVLMs注意力机制中视觉和文本令牌之间的相关性。我们的经验分析揭示了缓存视觉令牌中存在大量冗余，战略性地消除这些令牌可以保持模型性能，同时显著加速上下文生成。受这些发现的启发，我们引入了一个精英观察窗口，用于评估KV缓存中视觉组件的重要性，重点是具有增强多视角一致性的稳定跨模态相关性建模。此外，我们开发了一种自适应分层预算分配策略，该策略利用令牌重要性分布的强度和偏度，与均匀分配相比，显示出卓越的效率。在多个LVLMs和基准测试上的全面评估表明，我们的方法在仅保留10%视觉KV缓存的情况下，实现了与完整缓存相当的性能，从而在不同批次大小和提示长度的输入下，将解码延迟降低了29%至66%。值得注意的是，随着缓存保留率的降低，我们的方法比现有方法表现出越来越大的性能优势。", "summary": "AirCache是一种针对大型视觉语言模型（LVLMs）的新型KV缓存压缩方法，旨在解决高计算开销和KV缓存需求问题。该方法通过系统分析视觉与文本令牌的相关性，发现并利用视觉令牌的冗余性。它引入了精英观察窗口进行跨模态相关性建模，并开发了自适应分层预算分配策略。实验证明，AirCache仅使用10%的视觉KV缓存即可达到与全缓存相当的性能，并将LVLMs的解码延迟降低了29%至66%。", "keywords": "KV缓存压缩, 大型视觉语言模型, 推理效率, 跨模态相关性, 自适应分配", "comments": "AirCache的创新之处在于其对LVLM中视觉和文本令牌之间跨模态相关性的深入理解，并利用这一理解来设计高效的KV缓存压缩策略。特别是，其精英观察窗口和自适应分层预算分配策略，有效地识别并保留关键信息，同时丢弃冗余，从而在显著减少计算资源的同时保持了模型性能。这项工作对于优化LVLM的推理效率，使其在实际应用中更具可行性具有重要意义。"}}
{"id": "2507.17493", "title": "Automated Hybrid Grounding Using Structural and Data-Driven Heuristics", "authors": ["Alexander Beiser", "Markus Hecher", "Stefan Woltran"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17493v1", "summary": "The grounding bottleneck poses one of the key challenges that hinders the\nwidespread adoption of Answer Set Programming in industry. Hybrid Grounding is\na step in alleviating the bottleneck by combining the strength of standard\nbottom-up grounding with recently proposed techniques where rule bodies are\ndecoupled during grounding. However, it has remained unclear when hybrid\ngrounding shall use body-decoupled grounding and when to use standard bottom-up\ngrounding. In this paper, we address this issue by developing automated hybrid\ngrounding: we introduce a splitting algorithm based on data-structural\nheuristics that detects when to use body-decoupled grounding and when standard\ngrounding is beneficial. We base our heuristics on the structure of rules and\nan estimation procedure that incorporates the data of the instance. The\nexperiments conducted on our prototypical implementation demonstrate promising\nresults, which show an improvement on hard-to-ground scenarios, whereas on\nhard-to-solve instances we approach state-of-the-art performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17493v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用结构化和数据驱动启发式的自动化混合接地", "tldr": "本文提出了一种自动混合接地方法，通过基于数据结构启发式的分割算法，智能地选择何时使用解耦规则体接地或标准自下而上接地，以缓解回答集编程中的接地瓶颈。", "motivation": "接地瓶颈是阻碍回答集编程在工业中广泛应用的关键挑战之一。现有的混合接地方法虽然结合了标准自下而上接地和规则体解耦接地的优点，但尚不清楚何时应使用哪种方法。", "method": "本文通过开发自动化混合接地来解决这个问题。引入了一种基于数据结构启发式的分割算法，该算法能够检测何时使用规则体解耦接地以及何时使用标准接地。启发式方法基于规则的结构和结合实例数据的估计过程。", "result": "在原型实现上进行的实验显示出有希望的结果，在难以接地的场景中表现出改进，而在难以解决的实例上则接近最先进的性能。", "conclusion": "通过引入基于结构化和数据驱动启发式的自动化混合接地方法，可以有效缓解回答集编程中的接地瓶颈，尤其是在复杂场景下。", "translation": "接地瓶颈是阻碍回答集编程在工业中广泛应用的关键挑战之一。混合接地通过结合标准自下而上接地与最近提出的规则体在接地过程中解耦的技术，是缓解这一瓶颈的一个步骤。然而，何时应使用规则体解耦接地，何时应使用标准自下而上接地，这一点仍不清楚。在本文中，我们通过开发自动化混合接地来解决这个问题：我们引入了一种基于数据结构启发式的分割算法，该算法能够检测何时使用规则体解耦接地以及何时使用标准接地更有益。我们的启发式方法基于规则的结构和结合实例数据的估计过程。在我们的原型实现上进行的实验显示出有希望的结果，这表明在难以接地的场景中有所改进，而在难以解决的实例上，我们接近了最先进的性能。", "summary": "本文旨在解决回答集编程（ASP）中存在的接地瓶颈问题。针对现有混合接地方法在选择解耦规则体接地或标准自下而上接地时缺乏明确指导的问题，作者提出了一种自动混合接地方法。该方法引入了一个基于数据结构启发式的分割算法，能够智能地判断在不同情况下应采用哪种接地策略。实验结果表明，该方法在处理复杂或难以接地的场景时表现出显著改进，并且在处理难以解决的实例时能达到接近当前最佳的性能。", "keywords": "自动化接地, 混合接地, 回答集编程, 接地瓶颈, 启发式", "comments": "该论文通过引入一种智能的自动化混合接地机制，有效解决了回答集编程中长期存在的接地瓶颈问题。其创新点在于结合了结构化和数据驱动的启发式方法来自动选择接地策略，这对于提升ASP在实际工业应用中的效率和普适性具有重要意义。"}}
{"id": "2507.16258", "title": "Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Marius Hoggenmueller", "Callum Parker", "Paul Schmitt", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16258v2", "summary": "Autonomous mobility systems increasingly operate in environments shared with\nanimals, from urban pets to wildlife. However, their design has largely focused\non human interaction, with limited understanding of how non-human species\nperceive, respond to, or are affected by these systems. Motivated by research\nin Animal-Computer Interaction (ACI) and more-than-human design, this study\ninvestigates animal interactions with autonomous mobility through a\nmulti-method approach combining a scoping review (45 articles), online\nethnography (39 YouTube videos and 11 Reddit discussions), and expert\ninterviews (8 participants). Our analysis surfaces five key areas of concern:\nPhysical Impact (e.g., collisions, failures to detect), Behavioural Effects\n(e.g., avoidance, stress), Accessibility Concerns (particularly for service\nanimals), Ethics and Regulations, and Urban Disturbance. We conclude with\ndesign and policy directions aimed at supporting multispecies coexistence in\nthe age of autonomous systems. This work underscores the importance of\nincorporating non-human perspectives to ensure safer, more inclusive futures\nfor all species.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16258v2", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-23", "AI": {"title_translation": "动物与自动驾驶系统的互动：为多物种共存而设计", "tldr": "本研究探讨了动物与自动驾驶系统的互动，揭示了物理影响、行为效应、可及性、伦理和城市干扰等五大关注领域，并提出了支持多物种共存的设计和政策方向。", "motivation": "自动驾驶系统越来越多地在与动物共享的环境中运行，但其设计主要关注人类互动，对非人类物种如何感知、响应或受这些系统影响的理解有限。本研究受动物-计算机交互（ACI）和“超越人类”设计研究的启发，旨在填补这一空白。", "method": "采用多方法研究，包括：1. 范围界定审查（45篇文章）；2. 在线民族志（39个YouTube视频和11个Reddit讨论）；3. 专家访谈（8名参与者）。", "result": "分析揭示了五个关键关注领域：1. 物理影响（例如，碰撞、未能检测）；2. 行为效应（例如，回避、压力）；3. 可及性问题（特别是对于服务性动物）；4. 伦理和法规；5. 城市干扰。", "conclusion": "研究提出了旨在支持自动驾驶时代多物种共存的设计和政策方向，强调了纳入非人类视角的重要性，以确保所有物种拥有更安全、更具包容性的未来。", "translation": "自动驾驶系统越来越多地在与动物共享的环境中运行，从城市宠物到野生动物。然而，它们的设计主要关注人类互动，对非人类物种如何感知、响应或受这些系统影响的理解有限。受动物-计算机交互（ACI）和“超越人类”设计研究的启发，本研究通过结合范围界定审查（45篇文章）、在线民族志（39个YouTube视频和11个Reddit讨论）和专家访谈（8名参与者）的多方法方法，调查了动物与自动驾驶的互动。我们的分析揭示了五个关键关注领域：物理影响（例如，碰撞、未能检测）、行为效应（例如，回避、压力）、可及性问题（特别是对于服务性动物）、伦理和法规以及城市干扰。我们以旨在支持自动驾驶时代多物种共存的设计和政策方向作为结论。这项工作强调了纳入非人类视角的重要性，以确保所有物种拥有更安全、更具包容性的未来。", "summary": "本研究旨在解决自动驾驶系统设计中对非人类物种互动的忽视问题。通过结合文献综述、在线民族志和专家访谈的多方法研究，论文识别出动物与自动驾驶系统互动中的五个主要关注领域：物理影响、行为效应、可及性、伦理法规和城市干扰。最终，研究提出了促进多物种共存的设计和政策建议，强调了将非人类视角纳入未来技术发展的重要性。", "keywords": "自动驾驶系统, 动物互动, 多物种共存, 动物-计算机交互, 非人类视角", "comments": "这项研究的创新之处在于其独特地关注了自动驾驶系统对非人类物种的影响，填补了现有设计主要以人类为中心的空白。其多方法研究方法增强了发现的全面性。论文的重要性在于其为未来自动驾驶系统的设计和政策制定提供了关键的非人类视角，有助于实现更具包容性和安全的城市环境。"}}
{"id": "2505.07983", "title": "Virtual Holonomic Constraints in Motion Planning: Revisiting Feasibility and Limitations", "authors": ["Maksim Surov"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      17 pages, 3 figure", "url": "http://arxiv.org/abs/2505.07983v2", "summary": "This paper addresses the feasibility of virtual holonomic constraints (VHCs)\nin the context of motion planning for underactuated mechanical systems with a\nsingle degree of underactuation. While existing literature has established a\nwidely accepted definition of VHC, we argue that this definition is overly\nrestrictive and excludes a broad class of admissible trajectories from\nconsideration. To illustrate this point, we analyze a periodic motion of the\nPlanar Vertical Take-Off and Landing (PVTOL) aircraft that satisfies all\nstandard motion planning requirements, including orbital stabilizability.\nHowever, for this solution -- as well as for a broad class of similar ones --\nthere exists no VHC that satisfies the conventional definition. We further\nprovide a formal proof demonstrating that the conditions imposed by this\ndefinition necessarily fail for a broad class of trajectories of mechanical\nsystems. These findings call for a reconsideration of the current definition of\nVHCs, with the potential to significantly broaden their applicability in motion\nplanning.", "comment": "17 pages, 3 figure", "pdf_url": "http://arxiv.org/pdf/2505.07983v2", "cate": "cs.RO", "date": "2025-05-12", "updated": "2025-07-23", "AI": {"title_translation": "运动规划中的虚拟完整约束：可行性与局限性再探讨", "tldr": "现有虚拟完整约束(VHC)定义过于严格，排除了许多可行轨迹，本文通过实例和形式证明指出其局限性，并呼吁重新审视VHC定义以拓宽其应用。", "motivation": "现有文献中广泛接受的虚拟完整约束 (VHC) 定义过于严格，排除了大量可接受的轨迹，限制了其在欠驱动机械系统运动规划中的应用。", "method": "本文通过分析平面垂直起降 (PVTOL) 飞行器的周期运动来论证观点，该运动满足所有标准运动规划要求但无法满足传统 VHC 定义。此外，本文还提供了形式证明，证明传统 VHC 定义施加的条件对于广泛的机械系统轨迹必然失效。", "result": "发现对于PVTOL飞行器的一个周期运动以及其他类似轨迹，不存在满足传统定义的VHC。通过形式证明，表明传统VHC定义施加的条件对于广泛的机械系统轨迹必然失效。", "conclusion": "现有虚拟完整约束 (VHC) 的定义需要重新考虑，以期显著拓宽其在运动规划中的适用性。", "translation": "本文探讨了在单自由度欠驱动机械系统运动规划背景下虚拟完整约束（VHC）的可行性。尽管现有文献已确立了被广泛接受的 VHC 定义，但我们认为该定义过于严格，排除了大量可接受的轨迹。为了说明这一点，我们分析了平面垂直起降（PVTOL）飞行器的一个周期性运动，该运动满足所有标准运动规划要求，包括轨道稳定性。然而，对于该解决方案以及大量类似解决方案，不存在满足传统定义的 VHC。我们进一步提供了形式证明，表明该定义施加的条件对于广泛的机械系统轨迹必然失效。这些发现呼吁重新审视当前 VHC 的定义，这有可能显著拓宽其在运动规划中的适用性。", "summary": "本文重新审视了运动规划中虚拟完整约束（VHC）的可行性，指出当前广泛接受的VHC定义过于严格，排除了许多可行的轨迹。通过分析PVTOL飞行器的周期运动实例和提供形式证明，论文揭示了传统VHC定义在面对广泛机械系统轨迹时的局限性。研究结果强调需要重新考虑VHC的定义，以扩大其在运动规划领域的应用范围。", "keywords": "虚拟完整约束, 运动规划, 欠驱动系统, 可行性, 局限性", "comments": "本文创新性地挑战了运动规划领域中一个被广泛接受但可能过于限制性的概念——虚拟完整约束（VHC）的传统定义。通过具体的飞行器实例和严谨的形式证明，论文揭示了现有定义的局限性，并为未来VHC理论的发展和应用拓展提供了重要的方向。其贡献在于可能显著拓宽VHC在欠驱动系统运动规划中的实用性。"}}
{"id": "2502.17289", "title": "A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification", "authors": ["Soumen Sinha", "Tanisha Rana", "Rahul Roy"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      We want to do some modifications and add more experiments", "url": "http://arxiv.org/abs/2502.17289v3", "summary": "In this article, we propose a novel approach for plant hierarchical taxonomy\nclassification by posing the problem as an open class problem. It is observed\nthat existing methods for medicinal plant classification often fail to perform\nhierarchical classification and accurately identifying unknown species,\nlimiting their effectiveness in comprehensive plant taxonomy classification.\nThus we address the problem of unknown species classification by assigning it\nbest hierarchical labels. We propose a novel method, which integrates\nDenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for\nhierarchical classification. The approach systematically categorizes medicinal\nplants at multiple taxonomic levels, from phylum to species, ensuring detailed\nand precise classification. Using multi scale space attention, the model\ncaptures both local and global contextual information from the images,\nimproving the distinction between similar species and the identification of new\nones. It uses attention scores to focus on important features across multiple\nscales. The proposed method provides a solution for hierarchical\nclassification, showcasing superior performance in identifying both known and\nunknown species. The model was tested on two state-of-art datasets with and\nwithout background artifacts and so that it can be deployed to tackle real word\napplication. We used unknown species for testing our model. For unknown species\nthe model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for\npredicting correct phylum, class, order and family respectively. Our proposed\nmodel size is almost four times less than the existing state of the art methods\nmaking it easily deploy able in real world application.", "comment": "We want to do some modifications and add more experiments", "pdf_url": "http://arxiv.org/pdf/2502.17289v3", "cate": "cs.AI", "date": "2025-02-24", "updated": "2025-07-22", "AI": {"title_translation": "一种应对药用植物分类中开放世界场景的分类等级导航新方法", "tldr": "本文提出了一种结合DenseNet121、多尺度自注意力（MSSA）和级联分类器的新方法，用于药用植物的层级分类，旨在解决未知物种识别问题，并在已知和未知物种分类上表现出色，模型尺寸更小。", "motivation": "现有的药用植物分类方法在执行分层分类和准确识别未知物种方面存在不足，限制了它们在综合植物分类学中的有效性。因此，本文旨在通过为未知物种分配最佳层级标签来解决未知物种分类问题。", "method": "本文提出了一种新方法，集成了DenseNet121、多尺度自注意力（MSSA）和级联分类器进行层级分类。该方法系统地将药用植物从门到物种的多个分类级别进行分类。它利用多尺度空间注意力从图像中捕获局部和全局上下文信息，并通过注意力分数关注多个尺度的重要特征。", "result": "该模型在识别已知和未知物种方面表现出卓越的性能。对于未知物种，模型在预测正确的门、纲、目和科方面分别达到了83.36%、78.30%、60.34%和43.32%的平均准确率。此外，所提出的模型尺寸比现有最先进的方法小近四倍。", "conclusion": "本文提出的方法为层级分类提供了一个解决方案，在识别已知和未知物种方面表现出优越的性能，并且由于模型尺寸小，易于部署到实际应用中。", "translation": "在本文中，我们提出了一种新的植物等级分类方法，将该问题视为一个开放类问题。据观察，现有的药用植物分类方法通常无法进行分层分类并准确识别未知物种，这限制了它们在全面植物分类学中的有效性。因此，我们通过为未知物种分配最佳层级标签来解决未知物种分类问题。我们提出了一种新方法，该方法集成了DenseNet121、多尺度自注意力（MSSA）和级联分类器进行层级分类。该方法系统地将药用植物从门到物种的多个分类级别进行分类，确保了详细而精确的分类。通过使用多尺度空间注意力，模型从图像中捕获局部和全局上下文信息，提高了相似物种之间的区分度和新物种的识别能力。它利用注意力分数关注多个尺度的重要特征。所提出的方法为层级分类提供了一个解决方案，在识别已知和未知物种方面表现出卓越的性能。该模型在两个最先进的数据集上进行了测试，包括有背景伪影和无背景伪影的数据集，以便可以部署到实际应用中。我们使用未知物种来测试我们的模型。对于未知物种，模型在预测正确的门、纲、目和科方面分别达到了83.36%、78.30%、60.34%和43.32%的平均准确率。我们提出的模型尺寸比现有最先进的方法小近四倍，使其易于在实际应用中部署。", "summary": "本文提出了一种新颖的方法，旨在解决药用植物分类中的开放世界场景和分层分类问题。该方法结合了DenseNet121、多尺度自注意力（MSSA）和级联分类器，能够系统地对药用植物进行多级分类，并有效识别已知和未知物种。通过捕获多尺度上下文信息，模型提高了相似物种的区分度。实验结果表明，该方法在未知物种分类上取得了良好的准确率，并且模型尺寸显著小于现有方法，更易于实际部署。", "keywords": "药用植物分类, 开放世界场景, 层级分类, DenseNet121, 多尺度自注意力", "comments": "该论文的创新点在于将植物等级分类问题视为开放类问题，并提出了一种结合DenseNet121、多尺度自注意力（MSSA）和级联分类器的新颖方法。这种方法不仅解决了现有方法在分层分类和未知物种识别方面的不足，还通过多尺度注意力机制提升了分类精度。模型尺寸的显著减小是其在实际应用中易于部署的重要优势。"}}
{"id": "2501.06158", "title": "GenMol: A Drug Discovery Generalist with Discrete Diffusion", "authors": ["Seul Lee", "Karsten Kreis", "Srimukh Prasad Veccham", "Meng Liu", "Danny Reidenbach", "Yuxing Peng", "Saee Paliwal", "Weili Nie", "Arash Vahdat"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2501.06158v3", "summary": "Drug discovery is a complex process that involves multiple stages and tasks.\nHowever, existing molecular generative models can only tackle some of these\ntasks. We present Generalist Molecular generative model (GenMol), a versatile\nframework that uses only a single discrete diffusion model to handle diverse\ndrug discovery scenarios. GenMol generates Sequential Attachment-based Fragment\nEmbedding (SAFE) sequences through non-autoregressive bidirectional parallel\ndecoding, thereby allowing the utilization of a molecular context that does not\nrely on the specific token ordering while having better sampling efficiency.\nGenMol uses fragments as basic building blocks for molecules and introduces\nfragment remasking, a strategy that optimizes molecules by regenerating masked\nfragments, enabling effective exploration of chemical space. We further propose\nmolecular context guidance (MCG), a guidance method tailored for masked\ndiscrete diffusion of GenMol. GenMol significantly outperforms the previous\nGPT-based model in de novo generation and fragment-constrained generation, and\nachieves state-of-the-art performance in goal-directed hit generation and lead\noptimization. These results demonstrate that GenMol can tackle a wide range of\ndrug discovery tasks, providing a unified and versatile approach for molecular\ndesign. Our code is available at https://github.com/NVIDIA-Digital-Bio/genmol.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2501.06158v3", "cate": "cs.LG", "date": "2025-01-10", "updated": "2025-07-22", "AI": {"title_translation": "GenMol：一种基于离散扩散的药物发现通才模型", "tldr": "GenMol是一个通用的分子生成模型，它使用单一的离散扩散模型处理多种药物发现任务，并在多项任务中取得了最先进的性能。", "motivation": "现有的分子生成模型只能解决药物发现过程中复杂的多阶段和多任务中的部分任务。", "method": "本文提出了通用分子生成模型（GenMol），一个多功能框架，仅使用单一的离散扩散模型来处理多样化的药物发现场景。GenMol通过非自回归双向并行解码生成基于顺序连接的片段嵌入（SAFE）序列，从而允许利用不依赖特定令牌顺序的分子上下文，同时具有更好的采样效率。GenMol使用片段作为分子的基本构建块，并引入了片段重掩码策略，通过重新生成被掩码的片段来优化分子，从而有效地探索化学空间。此外，本文还提出了分子上下文引导（MCG），一种为GenMol的掩码离散扩散量身定制的引导方法。", "result": "GenMol在从头生成和片段约束生成方面显著优于之前的基于GPT的模型，并在目标导向的命中生成和先导化合物优化中实现了最先进的性能。", "conclusion": "这些结果表明GenMol可以解决广泛的药物发现任务，为分子设计提供了一种统一且多功能的方法。", "translation": "药物发现是一个涉及多个阶段和任务的复杂过程。然而，现有的分子生成模型只能解决其中一些任务。我们提出了通用分子生成模型（GenMol），一个多功能框架，它仅使用单一的离散扩散模型来处理多样化的药物发现场景。GenMol通过非自回归双向并行解码生成基于顺序连接的片段嵌入（SAFE）序列，从而允许利用不依赖特定令牌顺序的分子上下文，同时具有更好的采样效率。GenMol使用片段作为分子的基本构建块，并引入了片段重掩码，这是一种通过重新生成被掩码片段来优化分子的策略，能够有效地探索化学空间。我们进一步提出了分子上下文引导（MCG），这是一种为GenMol的掩码离散扩散量身定制的引导方法。GenMol在从头生成和片段约束生成方面显著优于之前的基于GPT的模型，并在目标导向的命中生成和先导化合物优化中实现了最先进的性能。这些结果表明GenMol可以解决广泛的药物发现任务，为分子设计提供了一种统一且多功能的方法。我们的代码可在https://github.com/NVIDIA-Digital-Bio/genmol获取。", "summary": "GenMol是一种新型的通用分子生成模型，它采用单一离散扩散模型来应对多种药物发现任务。该模型通过非自回归双向并行解码生成SAFE序列，并引入了片段重掩码和分子上下文引导（MCG）等创新策略。实验证明，GenMol在从头生成、片段约束生成以及目标导向的命中生成和先导化合物优化等任务中均表现出色，显著优于现有模型并达到最先进水平，为分子设计提供了一种统一且高效的解决方案。", "keywords": "药物发现, 分子生成模型, 离散扩散, 片段, 统一方法", "comments": "GenMol的创新之处在于其采用单一离散扩散模型来处理多样化的药物发现任务，突破了现有模型任务单一的局限性。其引入的非自回归双向并行解码、片段重掩码以及分子上下文引导（MCG）等方法，提高了采样效率并有效探索了化学空间，使其在多个药物发现关键任务上实现了显著的性能提升和最先进水平，展现了作为“通才”模型的强大潜力和重要性。"}}
{"id": "2507.16991", "title": "PyG 2.0: Scalable Learning on Real World Graphs", "authors": ["Matthias Fey", "Jinu Sunil", "Akihiro Nitta", "Rishi Puri", "Manan Shah", "Blaž Stojanovič", "Ramona Bendias", "Alexandria Barghi", "Vid Kocijan", "Zecheng Zhang", "Xinwei He", "Jan Eric Lenssen", "Jure Leskovec"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16991v1", "summary": "PyG (PyTorch Geometric) has evolved significantly since its initial release,\nestablishing itself as a leading framework for Graph Neural Networks. In this\npaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive\nupdate that introduces substantial improvements in scalability and real-world\napplication capabilities. We detail the framework's enhanced architecture,\nincluding support for heterogeneous and temporal graphs, scalable feature/graph\nstores, and various optimizations, enabling researchers and practitioners to\ntackle large-scale graph learning problems efficiently. Over the recent years,\nPyG has been supporting graph learning in a large variety of application areas,\nwhich we will summarize, while providing a deep dive into the important areas\nof relational deep learning and large language modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16991v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "PyG 2.0: 真实世界图上的可扩展学习", "tldr": "PyG 2.0 是 PyTorch Geometric 的重大更新，显著提升了在真实世界图上进行大规模图学习的可扩展性和应用能力。", "motivation": "提升 PyTorch Geometric 在可扩展性和真实世界应用能力方面的表现，以有效解决大规模图学习问题。", "method": "介绍了 PyG 2.0 及其后续小版本，这是一个全面的更新，包括增强的架构，支持异构图和时间图，可扩展的特征/图存储，以及各种优化。论文还将总结 PyG 在各种应用领域中的支持，并深入探讨关系深度学习和大型语言模型。", "result": "PyG 2.0 使得研究人员和实践者能够高效地处理大规模图学习问题。", "conclusion": "PyG 2.0 通过其全面的更新和增强架构，显著提升了在大规模真实世界图上的学习能力，并支持了广泛的应用领域。", "translation": "PyG（PyTorch Geometric）自首次发布以来取得了显著发展，已成为图神经网络的领先框架。在本文中，我们介绍了 PyG 2.0（及其后续小版本），这是一次全面的更新，在可扩展性和真实世界应用能力方面引入了实质性改进。我们详细阐述了该框架的增强架构，包括对异构图和时间图的支持、可扩展的特征/图存储以及各种优化，使研究人员和实践者能够高效地解决大规模图学习问题。近年来，PyG 在各种应用领域支持了图学习，我们将在本文中进行总结，同时深入探讨关系深度学习和大型语言模型等重要领域。", "summary": "本文介绍了 PyTorch Geometric (PyG) 的重大更新版本 PyG 2.0，旨在显著提升其在真实世界图数据上的可扩展性和应用能力。PyG 2.0 引入了增强的架构，支持异构图和时间图，并优化了特征/图存储，从而使研究人员和实践者能更高效地处理大规模图学习任务。论文还将回顾 PyG 在不同应用领域的应用，并重点关注关系深度学习和大型语言模型。", "keywords": "图神经网络, PyTorch Geometric, 可扩展性, 异构图, 大型语言模型", "comments": "PyG 2.0 的发布对于图神经网络领域具有重要意义，它通过解决可扩展性这一核心挑战，极大地拓宽了图学习在真实世界大规模数据上的应用潜力。对异构图和时间图的支持，以及对大型语言模型的关注，表明其积极应对当前AI领域的热点和难点。"}}
{"id": "2507.17149", "title": "ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation", "authors": ["Bo Fang", "Jianan Fan", "Dongnan Liu", "Hang Chang", "Gerald J. Shami", "Filip Braet", "Weidong Cai"], "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4.6"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by 28th European Conference on Artificial Intelligence (ECAI)", "url": "http://arxiv.org/abs/2507.17149v1", "summary": "The significant morphological and distributional variability among\nsubcellular components poses a long-standing challenge for learning-based\norganelle segmentation models, significantly increasing the risk of biased\nfeature learning. Existing methods often rely on single mapping relationships,\noverlooking feature diversity and thereby inducing biased training. Although\nthe Segment Anything Model (SAM) provides rich feature representations, its\napplication to subcellular scenarios is hindered by two key challenges: (1) The\nvariability in subcellular morphology and distribution creates gaps in the\nlabel space, leading the model to learn spurious or biased features. (2) SAM\nfocuses on global contextual understanding and often ignores fine-grained\nspatial details, making it challenging to capture subtle structural alterations\nand cope with skewed data distributions. To address these challenges, we\nintroduce ScSAM, a method that enhances feature robustness by fusing\npre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge\nto alleviate training bias from data imbalance. Specifically, we design a\nfeature alignment and fusion module to align pre-trained embeddings to the same\nfeature space and efficiently combine different representations. Moreover, we\npresent a cosine similarity matrix-based class prompt encoder to activate\nclass-specific features to recognize subcellular categories. Extensive\nexperiments on diverse subcellular image datasets demonstrate that ScSAM\noutperforms state-of-the-art methods.", "comment": "Accepted by 28th European Conference on Artificial Intelligence\n  (ECAI)", "pdf_url": "http://arxiv.org/pdf/2507.17149v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "ScSAM：去偏亚细胞语义分割中的形态和分布变异性", "tldr": "ScSAM通过融合预训练SAM与MAE引导的细胞先验知识，解决了亚细胞语义分割中形态和分布变异性导致的特征学习偏差和对细粒度细节的忽视问题。", "motivation": "亚细胞组分显著的形态和分布变异性给基于学习的细胞器分割模型带来了长期挑战，增加了偏置特征学习的风险。现有方法依赖单一映射关系，忽视特征多样性并导致训练偏差。Segment Anything Model (SAM) 在亚细胞场景中的应用受限于两个挑战：1) 亚细胞形态和分布的变异性导致标签空间存在空白，使模型学习到虚假或偏置特征。2) SAM 侧重全局上下文理解，忽略细粒度空间细节，难以捕捉微小结构变化和应对偏斜数据分布。", "method": "引入ScSAM，通过融合预训练SAM与Masked Autoencoder (MAE) 引导的细胞先验知识来增强特征鲁棒性，以减轻数据不平衡带来的训练偏差。具体包括：设计一个特征对齐和融合模块，将预训练嵌入对齐到相同特征空间并有效组合不同表示；提出一个基于余弦相似度矩阵的类别提示编码器，以激活类别特定特征来识别亚细胞类别。", "result": "在多样化的亚细胞图像数据集上进行的广泛实验表明，ScSAM 优于现有最先进的方法。", "conclusion": "ScSAM 成功解决了亚细胞语义分割中由于形态和分布变异性导致的特征学习偏差和对细粒度细节捕捉的挑战，通过其创新的特征融合和类别提示机制，实现了卓越的性能。", "translation": "亚细胞组分之间显著的形态和分布变异性给基于学习的细胞器分割模型带来了长期挑战，显著增加了偏置特征学习的风险。现有方法通常依赖单一映射关系，忽视特征多样性，从而导致偏置训练。尽管Segment Anything Model (SAM) 提供了丰富的特征表示，但其在亚细胞场景中的应用受到两个关键挑战的阻碍：(1) 亚细胞形态和分布的变异性在标签空间中造成了空白，导致模型学习到虚假或偏置特征。(2) SAM侧重于全局上下文理解，通常忽略细粒度空间细节，这使得捕捉微妙的结构变化和应对偏斜数据分布变得具有挑战性。为了解决这些挑战，我们引入了ScSAM，这是一种通过融合预训练SAM与Masked Autoencoder (MAE) 引导的细胞先验知识来增强特征鲁棒性的方法，以减轻数据不平衡带来的训练偏差。具体来说，我们设计了一个特征对齐和融合模块，将预训练的嵌入对齐到相同的特征空间并有效地组合不同的表示。此外，我们提出了一个基于余弦相似度矩阵的类别提示编码器，以激活类别特定特征来识别亚细胞类别。在多样化的亚细胞图像数据集上进行的广泛实验表明，ScSAM优于现有最先进的方法。", "summary": "ScSAM是一种新的方法，旨在解决亚细胞语义分割中由形态和分布变异性引起的特征学习偏差和对细粒度细节捕捉不足的问题。它通过将预训练的SAM模型与MAE引导的细胞先验知识融合来增强特征鲁棒性，并设计了特征对齐与融合模块和基于余弦相似度矩阵的类别提示编码器。实验证明，ScSAM在多个亚细胞图像数据集上表现优于现有SOTA方法。", "keywords": "亚细胞语义分割, SAM, MAE, 特征去偏, 形态变异性", "comments": "ScSAM的创新之处在于它巧妙地结合了SAM的强大特征表示能力与MAE提供的细胞先验知识，有效解决了亚细胞图像分割中长期存在的形态和分布变异性导致的偏差问题。其特征对齐和类别提示机制对于提高模型对细微结构和偏斜数据分布的适应性至关重要，为高精度亚细胞结构分析提供了新的SOTA解决方案。"}}
{"id": "2507.17399", "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by SIGIR 2025 LiveRAG Challenge Program", "url": "http://arxiv.org/abs/2507.17399v1", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "pdf_url": "http://arxiv.org/pdf/2507.17399v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "数百万个GeAR：将GraphRAG扩展到数百万文档", "tldr": "本文旨在将最先进的基于图的RAG解决方案GeAR应用于SIGIR 2025 LiveRAG挑战赛，以探索其在更广泛数据集上的性能和局限性。", "motivation": "现有的基于图的检索增强生成方法通常针对特定任务设计，缺乏在更广泛数据集上普遍适用性的证据。", "method": "本文旨在改编一种最先进的基于图的RAG解决方案GeAR，并探索其在SIGIR 2025 LiveRAG挑战赛上的性能和局限性。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "最近的研究探索了基于图的方法来增强检索生成，利用结构化或半结构化信息——例如从文档中提取的实体及其关系——来增强检索。然而，这些方法通常设计用于解决特定任务，例如多跳问答和以查询为中心的摘要，因此，它们在更广泛数据集上的普遍适用性证据有限。在本文中，我们旨在改编一种最先进的基于图的RAG解决方案：GeAR，并探索其在SIGIR 2025 LiveRAG挑战赛上的性能和局限性。", "summary": "本文旨在解决现有基于图的检索增强生成（GraphRAG）方法在更广泛数据集上普遍适用性不足的问题。研究人员计划改编最先进的基于图的RAG解决方案GeAR，并在SIGIR 2025 LiveRAG挑战赛上评估其性能和局限性。", "keywords": "GraphRAG, GeAR, 检索增强生成, SIGIR 2025 LiveRAG Challenge", "comments": "本文旨在解决现有GraphRAG方法在通用性方面的局限性，通过在一个大规模数据集（SIGIR 2025 LiveRAG挑战赛）上评估GeAR来探索其性能和限制，这对于推动GraphRAG在实际应用中的发展具有重要意义。"}}
{"id": "2507.17126", "title": "OkadaTorch: A Differentiable Programming of Okada Model to Calculate Displacements and Strains from Fault Parameters", "authors": ["Masayoshi Someya", "Taisuke Yamada", "Tomohisa Okazaki"], "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "Subjects:       Geophysics (physics.geo-ph)", "pdf_link": null, "comments": "Comments:      13 pages, 8 figures", "url": "http://arxiv.org/abs/2507.17126v1", "summary": "The Okada model is a widely used analytical solution for displacements and\nstrains caused by a point or rectangular dislocation source in a 3D elastic\nhalf-space. We present OkadaTorch, a PyTorch implementation of the Okada model,\nwhere the entire code is differentiable; gradients with respect to input can be\neasily computed using automatic differentiation (AD). Our work consists of two\ncomponents: a direct translation of the original Okada model into PyTorch, and\na convenient wrapper interface for efficiently computing gradients and Hessians\nwith respect to either observation station coordinates or fault parameters.\nThis differentiable framework is well suited for fault parameter inversion,\nincluding gradient-based optimization, Bayesian inference, and integration with\nscientific machine learning (SciML) models. Our code is available here:\nhttps://github.com/msomeya1/OkadaTorch", "comment": "13 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.17126v1", "cate": "physics.geo-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "OkadaTorch：一种可微分的Okada模型编程，用于根据断层参数计算位移和应变", "tldr": "OkadaTorch是一个基于PyTorch的Okada模型实现，它支持自动微分，使其适用于断层参数反演、梯度优化和科学机器学习。", "motivation": "Okada模型是用于计算由断层引起的位移和应变的常用分析解。本研究的动机是创建一个可微分的框架，以便于进行断层参数反演，包括梯度优化、贝叶斯推断以及与科学机器学习模型的集成。", "method": "研究人员开发了OkadaTorch，这是Okada模型的PyTorch实现，整个代码都是可微分的。它包含两个主要部分：将原始Okada模型直接转换为PyTorch代码，以及一个便捷的封装接口，用于高效计算相对于观测站坐标或断层参数的梯度和Hessian。", "result": "OkadaTorch提供了一个可微分的框架，该框架非常适合断层参数反演，包括基于梯度的优化、贝叶斯推断以及与科学机器学习（SciML）模型的集成。", "conclusion": "OkadaTorch为地震学和地球物理学中的断层参数反演及相关科学计算任务提供了一个强大且灵活的可微分编程工具。", "translation": "Okada模型是计算三维弹性半空间中点源或矩形位错源引起的位移和应变的广泛使用的解析解。我们提出了OkadaTorch，一个Okada模型的PyTorch实现，其中整个代码都是可微分的；可以使用自动微分（AD）轻松计算相对于输入的梯度。我们的工作包括两个部分：将原始Okada模型直接转换为PyTorch，以及一个方便的封装接口，用于高效计算相对于观测站坐标或断层参数的梯度和Hessian。这个可微分的框架非常适合断层参数反演，包括基于梯度的优化、贝叶斯推断以及与科学机器学习（SciML）模型的集成。我们的代码可在此处获取：https://github.com/msomeya1/OkadaTorch", "summary": "本文介绍了OkadaTorch，一个基于PyTorch的可微分Okada模型实现。该工具允许通过自动微分轻松计算位移和应变相对于断层参数的梯度，从而为断层参数反演（如梯度优化、贝叶斯推断）以及与科学机器学习模型的集成提供了一个强大的框架。", "keywords": "Okada模型, PyTorch, 可微分编程, 断层参数反演, 自动微分", "comments": "该论文的创新之处在于将经典的Okada模型转化为一个完全可微分的PyTorch实现，这极大地拓展了其在现代计算方法（如自动微分和机器学习）中的应用潜力。这种可微分编程范式对于地球物理学中复杂的反演问题具有重要意义，能够提高反演效率和精度。"}}
{"id": "2503.06506", "title": "Fine-Grained Alignment and Noise Refinement for Compositional Text-to-Image Generation", "authors": ["Amir Mohammad Izadi", "Seyed Mohammad Hadi Hosseini", "Soroush Vafaie Tabar", "Ali Abdollahi", "Armin Saghafian", "Mahdieh Soleymani Baghshah"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06506v2", "summary": "Text-to-image generative models have made significant advancements in recent\nyears; however, accurately capturing intricate details in textual prompts-such\nas entity missing, attribute binding errors, and incorrect relationships\nremains a formidable challenge. In response, we present an innovative,\ntraining-free method that directly addresses these challenges by incorporating\ntailored objectives to account for textual constraints. Unlike layout-based\napproaches that enforce rigid structures and limit diversity, our proposed\napproach offers a more flexible arrangement of the scene by imposing just the\nextracted constraints from the text, without any unnecessary additions. These\nconstraints are formulated as losses-entity missing, entity mixing, attribute\nbinding, and spatial relationships-integrated into a unified loss that is\napplied in the first generation stage. Furthermore, we introduce a\nfeedback-driven system for fine-grained initial noise refinement. This system\nintegrates a verifier that evaluates the generated image, identifies\ninconsistencies, and provides corrective feedback. Leveraging this feedback,\nour refinement method first targets the unmet constraints by refining the\nfaulty attention maps caused by initial noise, through the optimization of\nselective losses associated with these constraints. Subsequently, our unified\nloss function is reapplied to proceed the second generation phase. Experimental\nresults demonstrate that our method, relying solely on our proposed objective\nfunctions, significantly enhances compositionality, achieving a 24% improvement\nin human evaluation and a 25% gain in spatial relationships. Furthermore, our\nfine-grained noise refinement proves effective, boosting performance by up to\n5%. Code is available at\n\\href{https://github.com/hadi-hosseini/noise-refinement}{https://github.com/hadi-hosseini/noise-refinement}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06506v2", "cate": "cs.CV", "date": "2025-03-09", "updated": "2025-07-22", "AI": {"title_translation": "面向组合式文本到图像生成的细粒度对齐与噪声优化", "tldr": "本文提出了一种无需训练的方法，通过定制化损失和反馈驱动的噪声优化来改进组合式文本到图像生成，解决了实体缺失和属性绑定错误等问题。", "motivation": "当前文本到图像生成模型难以准确捕捉文本提示中的复杂细节，例如实体缺失、属性绑定错误和不正确的对象关系。", "method": "提出了一种无需训练的方法，通过将文本约束（实体缺失、实体混合、属性绑定和空间关系）公式化为损失并集成到统一损失中，应用于第一阶段生成。此外，引入了一个反馈驱动的细粒度初始噪声优化系统，该系统包含一个验证器来识别不一致性并提供纠正反馈，通过优化选择性损失和调整错误的注意力图来解决未满足的约束。随后，统一损失函数在第二阶段生成中再次应用。", "result": "方法显著增强了组合性，在人工评估中实现了24%的改进，在空间关系上获得了25%的提升。细粒度噪声优化将性能提升了高达5%。", "conclusion": "本文提出的方法，仅依靠其目标函数，显著提升了文本到图像生成的组合性，并通过有效的噪声优化进一步提高了性能。", "translation": "文本到图像生成模型近年来取得了显著进展；然而，准确捕捉文本提示中的复杂细节——例如实体缺失、属性绑定错误和不正确的关系——仍然是一个严峻的挑战。为此，我们提出了一种创新的、无需训练的方法，通过结合为文本约束量身定制的目标来直接解决这些挑战。与强制执行严格结构并限制多样性的基于布局的方法不同，我们提出的方法通过仅施加从文本中提取的约束，而不添加任何不必要的额外内容，提供了更灵活的场景排列。这些约束被公式化为损失——实体缺失、实体混合、属性绑定和空间关系——并集成到一个统一的损失中，应用于第一代生成阶段。此外，我们引入了一个反馈驱动的系统，用于细粒度的初始噪声优化。该系统集成了一个验证器，用于评估生成的图像，识别不一致性，并提供纠正反馈。利用此反馈，我们的优化方法首先通过优化与这些约束相关的选择性损失来优化由初始噪声引起的错误注意力图，从而解决未满足的约束。随后，我们的统一损失函数被重新应用于进行第二代生成阶段。实验结果表明，我们的方法仅依靠我们提出的目标函数，显著增强了组合性，在人工评估中实现了24%的改进，在空间关系上获得了25%的提升。此外，我们的细粒度噪声优化被证明是有效的，将性能提升了高达5%。代码可在\nhttps://github.com/hadi-hosseini/noise-refinement\n获取。", "summary": "本文提出了一种新颖的无需训练的组合式文本到图像生成方法，旨在解决现有模型中常见的实体缺失和属性绑定错误等问题。该方法利用基于文本约束（包括实体缺失、实体混合、属性绑定和空间关系）的定制化损失函数，并采用两阶段生成过程。其核心创新在于引入了一个反馈驱动的噪声优化系统，该系统通过验证器识别并纠正不一致性，通过优化选择性损失和精炼注意力图来实现。实验结果表明，该方法在组合性方面取得了显著提升（人工评估提高24%，空间关系提高25%），且噪声优化进一步带来了性能提升。", "keywords": "文本到图像生成, 组合性, 细粒度对齐, 噪声优化, 无需训练方法", "comments": "该方法的创新之处在于其无需训练的特性，以及将文本约束明确地公式化为统一损失，这比基于布局的方法提供了更大的灵活性。反馈驱动的噪声优化系统也是一个显著贡献，提供了一种动态校正机制。其重要性在于提高了生成图像的逼真度和细节，解决了文本到图像模型中的关键挑战。"}}
{"id": "2507.17430", "title": "Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces", "authors": ["Yan Dong", "Hanjie Yu", "Yanran Chen", "Zipeng Zhang", "Qiong Wu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      21 pages, 16 figures, published in ACM CHI 2025", "url": "http://arxiv.org/abs/2507.17430v1", "summary": "Integrating technology with the distinctive characteristics of craftsmanship\nhas become a key issue in the field of digital craftsmanship. This paper\nintroduces Layered Interactions, a design approach that seamlessly merges\nHuman-Computer Interaction (HCI) technologies with traditional lacquerware\ncraftsmanship. By leveraging the multi-layer structure and material properties\nof lacquerware, we embed interactive circuits and integrate programmable\nhardware within the layers, creating tangible interfaces that support diverse\ninteractions. This method enhances the adaptability and practicality of\ntraditional crafts in modern digital contexts. Through the development of a\nlacquerware toolkit, along with user experiments and semi-structured\ninterviews, we demonstrate that this approach not only makes technology more\naccessible to traditional artisans but also enhances the materiality and\nemotional qualities of interactive interfaces. Additionally, it fosters mutual\nlearning and collaboration between artisans and technologists. Our research\nintroduces a cross-disciplinary perspective to the HCI community, broadening\nthe material and design possibilities for interactive interfaces.", "comment": "21 pages, 16 figures, published in ACM CHI 2025", "pdf_url": "http://arxiv.org/pdf/2507.17430v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "分层交互：通过漆艺界面探索非侵入式数字工艺设计", "tldr": "本文提出了一种名为“分层交互”的设计方法，将人机交互技术与传统漆器工艺相结合，通过在漆器层中嵌入交互电路和可编程硬件，创造出有形的交互界面，增强了传统工艺的适应性和实用性。", "motivation": "将技术与工艺的独特特性相结合已成为数字工艺领域的关键问题，旨在增强传统工艺在现代数字环境中的适应性和实用性。", "method": "本文引入了“分层交互”设计方法，通过利用漆器的多层结构和材料特性，在层内嵌入交互电路并集成可编程硬件，创建支持多样化交互的有形界面。研究通过开发漆器工具包，并进行用户实验和半结构化访谈来验证该方法。", "result": "研究表明，该方法不仅使传统工匠更容易接触技术，而且增强了交互界面的物质性和情感品质。此外，它还促进了工匠与技术人员之间的相互学习和协作。", "conclusion": "本文的研究为HCI社区引入了一个跨学科视角，拓宽了交互界面的材料和设计可能性。", "translation": "将技术与工艺的独特特性相结合已成为数字工艺领域的关键问题。本文介绍了分层交互，这是一种将人机交互（HCI）技术与传统漆器工艺无缝融合的设计方法。通过利用漆器的多层结构和材料特性，我们在层内嵌入交互电路并集成可编程硬件，创建支持多样化交互的有形界面。这种方法增强了传统工艺在现代数字环境中的适应性和实用性。通过开发漆器工具包，以及用户实验和半结构化访谈，我们证明了这种方法不仅使传统工匠更容易接触技术，而且增强了交互界面的物质性和情感品质。此外，它还促进了工匠与技术人员之间的相互学习和协作。我们的研究为HCI社区引入了一个跨学科视角，拓宽了交互界面的材料和设计可能性。", "summary": "本文提出“分层交互”设计方法，将HCI技术融入传统漆器工艺，通过在漆器多层结构中嵌入交互电路和硬件，创建可支持多样交互的有形界面。该方法旨在提升传统工艺的数字适应性和实用性。研究通过构建漆器工具包并进行用户实验，验证了其能使技术更易于工匠掌握，增强交互界面的物质与情感属性，并促进工匠与技术人员的合作。此研究为HCI领域提供了跨学科新视角，拓展了交互界面的材料与设计边界。", "keywords": "分层交互, 数字工艺, 漆器艺术, 人机交互, 有形界面", "comments": "该研究创新性地将传统漆艺与现代HCI技术结合，提出了“分层交互”这一非侵入式设计理念，有效弥合了数字技术与传统工艺之间的鸿沟。其贡献在于不仅提升了传统工艺在数字语境下的实用性，更重要的是，通过增强交互界面的物质性和情感品质，丰富了用户体验，并促进了跨学科的合作与知识共享。这种将“数字工艺”具象化的方式，为未来传统艺术与科技融合提供了新的范式。"}}
{"id": "2507.17505", "title": "Slow Fluid Antenna Multiple Access with Multiport Receivers", "authors": ["José P. González-Coma", "F. Javier López-Martínez"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      5 pages, 3 figures. This work has been submitted to the IEEE for publication", "url": "http://arxiv.org/abs/2507.17505v1", "summary": "We investigate whether equipping fluid-antenna (FA) receivers with multiple\n($L>1$) radiofrequency (RF) chains can improve the performance of the slow\nfluid-antenna multiple access (FAMA) technique, which enables open-loop\nconnectivity with channel state information (CSI) available only at the\nreceiver side. We analyze the case of slow-FAMA users equipped with multiport\nreceivers, so that $L$ ports of the FA are selected and combined to reduce\ninterference. We show that a joint design of the port selection matrix and the\ncombining vector at each receiver yields significant performance gains over\nreference schemes, demonstrating the potential of multiport reception in FA\nsystems with a limited number of RF chains.", "comment": "5 pages, 3 figures. This work has been submitted to the IEEE for\n  publication", "pdf_url": "http://arxiv.org/pdf/2507.17505v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "慢速流体天线多址接入与多端口接收机", "tldr": "本文研究了为慢速流体天线（FA）接收机配备多个射频（RF）链是否能提高其性能，并提出了一种联合端口选择矩阵和组合向量的设计，显著提升了性能。", "motivation": "研究为流体天线（FA）接收机配备多个（L>1）射频（RF）链是否能提高慢速流体天线多址接入（FAMA）技术的性能，该技术仅在接收端提供信道状态信息，实现开环连接。", "method": "分析了配备多端口接收机的慢速FAMA用户，其中FA的L个端口被选择并组合以减少干扰。研究了端口选择矩阵和每个接收机的组合向量的联合设计。", "result": "联合设计端口选择矩阵和组合向量在性能上显著优于参考方案。", "conclusion": "多端口接收在射射频链数量有限的FA系统中具有巨大潜力。", "translation": "我们研究了为流体天线（FA）接收机配备多个（L>1）射频（RF）链是否能提高慢速流体天线多址接入（FAMA）技术的性能，该技术仅在接收端提供信道状态信息，实现开环连接。我们分析了配备多端口接收机的慢速FAMA用户的情况，其中FA的L个端口被选择并组合以减少干扰。我们表明，在每个接收机上联合设计端口选择矩阵和组合向量比参考方案产生了显著的性能增益，这展示了在射频链数量有限的FA系统中多端口接收的潜力。", "summary": "本文探讨了为慢速流体天线多址接入（FAMA）系统中的流体天线（FA）接收机配备多个射频（RF）链以提升性能的可行性。研究分析了配备多端口接收机的慢速FAMA用户，通过选择和组合FA的L个端口来减少干扰。结果表明，联合设计端口选择矩阵和组合向量能够带来显著的性能提升，证明了多端口接收在有限射频链FA系统中的应用潜力。", "keywords": "流体天线, 多址接入, 多端口接收机, 射频链, 性能提升", "comments": "本文的创新点在于探索了多端口接收在流体天线系统中的应用，并通过联合设计端口选择和组合向量，显著提升了系统性能，为未来FA系统的发展提供了新的思路。"}}
{"id": "2507.17472", "title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles", "authors": ["Junhua Liu", "Roy Ka-Wei Lee", "Kwan Hui Lim"], "categories": ["cs.LG", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ASONAM 2025", "url": "http://arxiv.org/abs/2507.17472v1", "summary": "Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.", "comment": "Accepted at ASONAM 2025", "pdf_url": "http://arxiv.org/pdf/2507.17472v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "BGM-HAN：一种用于半结构化档案中准确和公平决策评估的层次注意力网络", "tldr": "人类在高风险领域的决策容易出现认知偏差，影响公平性。本文提出了BGM-HAN，一种增强的层次注意力网络，用于对半结构化数据（如大学招生档案）进行准确和公平的决策评估。实验结果表明，BGM-HAN在真实招生数据上显著优于现有基线模型。", "motivation": "在高风险领域中，人类决策往往依赖于专业知识和启发式方法，但容易受到难以察觉的认知偏差的影响，从而威胁到公平性和长期结果。因此，需要一种新方法来增强复杂的决策工作流程。", "method": "本文提出了BGM-HAN，一种增强的字节对编码、门控多头层次注意力网络。该网络旨在有效建模半结构化申请人数据，并捕获对细致评估至关重要的多级表示，从而提高可解释性和预测性能。研究以大学招生为例，作为代表性的高风险领域。", "result": "在真实的招生数据上进行的实验结果表明，所提出的模型BGM-HAN显著优于从传统机器学习到大型语言模型的最新基线模型。", "conclusion": "BGM-HAN为在结构、上下文和公平性至关重要的领域中增强决策提供了一个有前景的框架。", "translation": "在高风险领域中，人类决策往往依赖于专业知识和启发式方法，但容易受到难以察觉的认知偏差的影响，从而威胁到公平性和长期结果。这项工作提出了一种通过整合层次学习和各种增强功能来改进复杂决策工作流程的新方法。我们以大学招生作为代表性的高风险领域，提出了BGM-HAN，一个增强的字节对编码、门控多头层次注意力网络，旨在有效建模半结构化申请人数据。BGM-HAN捕获对细致评估至关重要的多级表示，从而提高可解释性和预测性能。在真实招生数据上的实验结果表明，我们提出的模型显著优于从传统机器学习到大型语言模型的最新基线，为在结构、上下文和公平性至关重要的领域中增强决策提供了一个有前景的框架。源代码可在：https://github.com/junhua/bgm-han 获取。", "summary": "本文提出了一种名为BGM-HAN的新型增强型层次注意力网络，旨在解决高风险领域中人类决策易受认知偏差影响的问题，以实现准确和公平的决策评估。该模型特别针对大学招生等半结构化档案数据，能够捕获多级表示，从而提高决策的可解释性和预测性能。在真实招生数据上的实验证明，BGM-HAN显著超越了包括传统机器学习和大型语言模型在内的现有先进基线模型，为需要考虑结构、上下文和公平性的决策场景提供了一个有潜力的增强框架。", "keywords": "层次注意力网络, 决策评估, 半结构化数据, 公平性, 大学招生", "comments": "该论文的创新点在于提出了BGM-HAN，一个用于半结构化数据决策评估的层次注意力网络，尤其强调了其在公平性和准确性方面的提升。在与包括大型语言模型在内的最新基线进行比较并取得显著优势，这凸显了其在处理复杂、高风险决策场景中的重要性和有效性。模型对可解释性的关注也为其在高风险领域的实际应用增加了价值。"}}
{"id": "2507.17554", "title": "An h-space Based Adversarial Attack for Protection Against Few-shot Personalization", "authors": ["Xide Xu", "Sandesh Kamath", "Muhammad Atif Butt", "Bogdan Raducanu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      32 pages, 15 figures. Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.17554v1", "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.", "comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.17554v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种基于h空间对抗性攻击，用于抵御少样本个性化", "tldr": "提出一种基于h空间的对抗性攻击方法（HAAD及其变体HAAD-KV），以保护扩散模型免受少样本个性化内容修改。", "motivation": "扩散模型从少量样本生成定制图像的能力引发了严重的隐私问题，尤其是在未经授权修改私人内容方面。这促使研究人员开发基于对抗性攻击的保护机制。", "method": "提出了一种名为HAAD的新型反定制方法，该方法利用对抗性攻击，基于h空间生成扰动，以有效降低图像生成过程。在此基础上，进一步引入了更高效的变体HAAD-KV，它仅基于h空间的KV参数构建扰动。", "result": "HAAD-KV提供了更强的保护，且计算成本更低。我们的方法尽管简单，但性能优于最先进的对抗性攻击。", "conclusion": "所提出的HAAD及其变体HAAD-KV方法能有效且高效地保护扩散模型免受少样本个性化攻击，优于现有最先进方法。", "translation": "扩散模型从少量样本生成定制图像的多功能性引发了严重的隐私问题，特别是在未经授权修改私人内容方面。这一令人担忧的问题重新激发了开发基于对抗性攻击的保护机制的努力，这些机制生成有效的扰动来毒害扩散模型。我们的工作受到以下观察的启发：这些模型在其语义潜在空间（“h空间”）中表现出高度的抽象性，该空间编码了生成连贯且有意义内容的关键高级特征。在本文中，我们提出了一种新颖的反定制方法，称为HAAD（基于h空间的扩散模型对抗性攻击），该方法利用对抗性攻击来基于h空间制作扰动，从而有效降低图像生成过程。在HAAD的基础上，我们进一步引入了一种更高效的变体HAAD-KV，它仅基于h空间的KV参数构建扰动。这种策略提供了更强的保护，且计算成本更低。尽管它们很简单，但我们的方法优于最先进的对抗性攻击，突出了它们的有效性。", "summary": "本文针对扩散模型少样本个性化引发的隐私问题，提出了一种名为HAAD的新型对抗性攻击方法，旨在保护私人内容。该方法利用扩散模型语义潜在空间（h空间）的抽象特性，生成扰动来破坏图像生成过程。在此基础上，进一步开发了更高效的HAAD-KV变体，仅基于h空间的KV参数构建扰动。实验证明，所提出的方法尽管简单，但性能优于现有最先进的对抗性攻击，显示出其在提供强大且计算效率高保护方面的有效性。", "keywords": "扩散模型, 对抗性攻击, h空间, 少样本个性化, 隐私保护", "comments": "本文创新性地利用扩散模型内部h空间的抽象特性来设计对抗性攻击，而非仅仅在像素空间进行扰动。特别是HAAD-KV通过仅基于KV参数构建扰动，显著提升了计算效率并提供了更强的保护，为防御少样本个性化攻击提供了一个有前景的新方向。"}}
{"id": "2505.23822", "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "categories": ["cs.CL", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages, 1 figure, 3 tables. The corresponding author is Mai Ali (maia dot ali at mail dot utoronto dot ca). Christopher Lucasius and Tanmay P. Patel contributed equally", "url": "http://arxiv.org/abs/2505.23822v3", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods.", "comment": "6 pages, 1 figure, 3 tables. The corresponding author is Mai Ali\n  (maia dot ali at mail dot utoronto dot ca). Christopher Lucasius and Tanmay\n  P. Patel contributed equally", "pdf_url": "http://arxiv.org/pdf/2505.23822v3", "cate": "cs.CL", "date": "2025-05-28", "updated": "2025-07-23", "AI": {"title_translation": "基于多任务LLM的精神健康预测中作为多模态数字表型的语音", "tldr": "本文提出了一种基于LLM的多模态、多任务、纵向分析方法，利用语音数据（文本、声学特征、声音生物标志物）预测抑郁症、自杀意念和睡眠障碍，并在DEW数据集上取得了更好的平衡准确率。", "motivation": "语音是精神健康的非侵入性数字表型，但常被视为单一模态。此外，青少年抑郁症常伴有多种共病，需要更全面的预测方法。", "method": "提出将患者语音数据视为三模态（语音派生文本、声学地标、声音生物标志物）多媒体数据源。采用基于大语言模型（LLM）的架构进行语音抑郁症预测。整合多任务学习（MTL）同时预测抑郁症、自杀意念和睡眠障碍。提出纵向分析策略以建模临床交互中的时间变化。在抑郁症预警（Depression Early Warning）数据集上进行评估。", "result": "该方法在抑郁症预警数据集上实现了70.8%的平衡准确率，高于单模态、单任务和非纵向方法。", "conclusion": "结合三模态、多任务学习和纵向分析的语音处理方法能有效提高精神健康状况（如抑郁症、自杀意念、睡眠障碍）的预测性能。", "translation": "语音是一种非侵入性的数字表型，可以为精神健康状况提供有价值的见解，但它通常被视为单一模态。相比之下，我们提出将患者语音数据视为用于抑郁症检测的三模态多媒体数据源。本研究探索了基于大语言模型架构在多模态（整合语音派生文本、声学地标和声音生物标志物）语音抑郁症预测中的潜力。青少年抑郁症是一个重大挑战，并且通常与多种疾病并发，例如自杀意念和睡眠障碍。这提供了一个额外的机会，通过多模态公式将多任务学习（MTL）整合到我们的研究中，同时预测抑郁症、自杀意念和睡眠障碍。我们还提出了一种纵向分析策略，该策略对跨多个临床交互的时间变化进行建模，从而全面了解病情的进展。我们提出的方法，具有三模态、纵向MTL的特点，在抑郁症预警数据集上进行了评估。它实现了70.8%的平衡准确率，高于每种单模态、单任务和非纵向方法。", "summary": "本文提出了一种创新的方法，将语音作为多模态数字表型，结合基于大语言模型的多任务学习和纵向分析，用于预测抑郁症、自杀意念和睡眠障碍等精神健康状况。研究将语音数据处理为文本、声学地标和声音生物标志物三模态，并通过多任务学习同时预测多种共病。在抑郁症预警数据集上的实验结果表明，该三模态、纵向多任务学习方法在平衡准确率上优于传统的单模态、单任务及非纵向方法，达到70.8%。", "keywords": "语音、多模态、大语言模型、精神健康预测、多任务学习", "comments": "这篇论文的创新点在于将语音数据提升为三模态输入，并结合大语言模型、多任务学习和纵向分析，以更全面和准确地预测多种精神健康状况。这种综合性方法超越了以往单一模态或单任务的限制，为数字精神病学领域提供了新的视角和有效的工具。"}}
{"id": "2507.16873", "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting", "authors": ["Jeongeun Lee", "Youngjae Yu", "Dongha Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to COLM2025", "url": "http://arxiv.org/abs/2507.16873v1", "summary": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.", "comment": "Accepted to COLM2025", "pdf_url": "http://arxiv.org/pdf/2507.16873v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "HIPPO-Video：使用大型语言模型模拟观看历史以实现个性化视频高亮", "tldr": "HIPPO-Video引入了一个新的数据集，通过LLM模拟用户观看历史来生成个性化视频高亮，并提出了HiPHer方法，该方法利用这些历史来预测偏好条件下的显著性得分，优于现有方法。", "motivation": "视频内容的爆炸式增长使得个性化视频高亮成为一项基本任务，因为用户偏好差异大且复杂。现有视频数据集缺乏个性化，依赖孤立视频或简单文本查询，未能捕捉用户行为的复杂性。", "method": "本研究介绍了HIPPO-Video，一个用于个性化视频高亮的新型数据集，该数据集使用基于LLM的用户模拟器生成反映不同用户偏好的真实观看历史。该数据集包含2,040对（观看历史，显著性得分），涵盖170个语义类别的20,400个视频。为验证数据集，提出了HiPHer方法，该方法利用个性化观看历史来预测偏好条件下的片段显著性得分。", "result": "实验结果表明，所提出的方法优于现有的通用和基于查询的方法。", "conclusion": "该研究展示了其方法在真实场景中实现高度以用户为中心的视频高亮的潜力。", "translation": "视频内容的指数级增长使得个性化视频高亮成为一项基本任务，因为用户偏好差异大且复杂。然而，现有视频数据集通常缺乏个性化，依赖孤立的视频或简单的文本查询，未能捕捉用户行为的复杂性。在这项工作中，我们引入了HIPPO-Video，一个用于个性化视频高亮的新型数据集，该数据集使用基于大型语言模型（LLM）的用户模拟器来生成反映不同用户偏好的真实观看历史。该数据集包括2,040对（观看历史，显著性得分），涵盖170个语义类别的20,400个视频。为了验证我们的数据集，我们提出了HiPHer，一种利用这些个性化观看历史来预测偏好条件下的片段显著性得分的方法。通过大量的实验，我们证明了我们的方法优于现有的通用和基于查询的方法，展示了其在真实场景中实现高度以用户为中心的视频高亮的潜力。", "summary": "本论文介绍了HIPPO-Video，一个通过大型语言模型（LLM）模拟用户观看历史来创建的个性化视频高亮数据集。该数据集包含20,400个视频的2,040对（观看历史，显著性得分）。为验证数据集，研究提出了HiPHer方法，利用个性化观看历史预测偏好条件下的片段显著性得分。实验证明，HiPHer优于现有通用和基于查询的方法，展现了其在个性化视频高亮方面的潜力。", "keywords": "个性化视频高亮, 大型语言模型, 观看历史, 数据集, 显著性得分", "comments": "这项工作通过引入一个基于LLM生成的用户观看历史的新型数据集，解决了现有视频数据集缺乏个性化的问题，具有创新性。其提出的HiPHer方法利用这些个性化数据，显著提升了视频高亮的个性化程度，对于实际应用中的用户体验提升具有重要意义。"}}
{"id": "2507.17561", "title": "Robot-mediated physical Human-Human Interaction in Neurorehabilitation: a position paper", "authors": ["Lorenzo Vianello", "Matthew Short", "Julia Manczurowsky", "Emek Barış Küçüktabak", "Francesco Di Tommaso", "Alessia Noccaro", "Laura Bandini", "Shoshana Clark", "Alaina Fiorenza", "Francesca Lunardini", "Alberto Canton", "Marta Gandolla", "Alessandra L. G. Pedrocchi", "Emilia Ambrosini", "Manuel Murie-Fernandez", "Carmen B. Roman", "Jesus Tornero", "Natacha Leon", "Andrew Sawers", "Jim Patton", "Domenico Formica", "Nevio Luigi Tagliamonte", "Georg Rauter", "Kilian Baur", "Fabian Just", "Christopher J. Hasson", "Vesna D. Novak", "Jose L. Pons"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17561v1", "summary": "Neurorehabilitation conventionally relies on the interaction between a\npatient and a physical therapist. Robotic systems can improve and enrich the\nphysical feedback provided to patients after neurological injury, but they\nunder-utilize the adaptability and clinical expertise of trained therapists. In\nthis position paper, we advocate for a novel approach that integrates the\ntherapist's clinical expertise and nuanced decision-making with the strength,\naccuracy, and repeatability of robotics: Robot-mediated physical Human-Human\nInteraction. This framework, which enables two individuals to physically\ninteract through robotic devices, has been studied across diverse research\ngroups and has recently emerged as a promising link between conventional manual\ntherapy and rehabilitation robotics, harmonizing the strengths of both\napproaches. This paper presents the rationale of a multidisciplinary\nteam-including engineers, doctors, and physical therapists-for conducting\nresearch that utilizes: a unified taxonomy to describe robot-mediated\nrehabilitation, a framework of interaction based on social psychology, and a\ntechnological approach that makes robotic systems seamless facilitators of\nnatural human-human interaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17561v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "机器人介导的神经康复中人际物理交互：一篇立场论文", "tldr": "本文提出了一种新颖的机器人介导的人际物理交互方法，旨在将治疗师的专业知识与机器人的优势相结合，以改善神经康复。", "motivation": "传统神经康复依赖于患者与物理治疗师的互动。尽管机器人系统可以改善对患者的物理反馈，但它们未能充分利用受过训练的治疗师的适应性和临床专业知识。本文旨在弥合这一差距。", "method": "本文倡导一种新颖的“机器人介导的人际物理交互”（Robot-mediated physical Human-Human Interaction）方法。该框架通过机器人设备实现两个人之间的物理交互，整合了治疗师的临床专业知识和细致的决策能力与机器人的力量、准确性和可重复性。该研究提出了一个多学科团队（包括工程师、医生和物理治疗师）的理由，其研究将利用：一个统一的分类法来描述机器人介导的康复、一个基于社会心理学的交互框架以及一种使机器人系统成为自然人际交互的无缝促进者的技术方法。", "result": "Not mentioned in abstract", "conclusion": "机器人介导的人际物理交互框架是一个有前景的连接传统手动疗法和康复机器人的方法，它将两者的优势和谐地结合起来，是未来神经康复研究的重要方向。", "translation": "神经康复通常依赖于患者和物理治疗师之间的互动。机器人系统可以改善和丰富神经损伤后提供给患者的物理反馈，但它们未能充分利用受过训练的治疗师的适应性和临床专业知识。在这篇立场论文中，我们倡导一种新颖的方法，将治疗师的临床专业知识和细致的决策与机器人的力量、准确性和可重复性相结合：机器人介导的人际物理交互。这个框架使两个人能够通过机器人设备进行物理交互，已经在不同的研究组中进行了研究，并最近成为传统手动疗法和康复机器人之间一个有前景的连接，协调了这两种方法的优势。本文提出了一个多学科团队——包括工程师、医生和物理治疗师——进行研究的理由，该研究利用：一个统一的分类法来描述机器人介导的康复、一个基于社会心理学的交互框架以及一种使机器人系统成为自然人际交互的无缝促进者的技术方法。", "summary": "本文是一篇立场论文，提出并倡导“机器人介导的人际物理交互”（RmHHI）这一新颖方法，旨在将物理治疗师的专业知识与机器人技术相结合，以优化神经康复。该框架允许两人通过机器人设备进行物理互动，并被视为连接传统手动疗法与康复机器人的有前景的桥梁。论文阐述了一个多学科团队进行相关研究的理由，包括建立统一的分类法、基于社会心理学的交互框架以及实现无缝人际交互的技术方法。", "keywords": "神经康复, 机器人, 人际交互, 物理治疗, 立场论文", "comments": "本文提出了一种创新性的神经康复范式，通过结合人类治疗师的适应性和临床专长与机器人的精确性和重复性，解决了现有康复机器人未能充分利用人类专业知识的局限性。其重要性在于为未来的神经康复研究和实践提供了一个新的方向，尤其是在人机协作领域。"}}
{"id": "2507.17343", "title": "Principled Multimodal Representation Learning", "authors": ["Xiaohao Liu", "Xiaobo Xia", "See-Kiong Ng", "Tat-Seng Chua"], "categories": ["cs.CV", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      32 pages, 9 figures, 10 tables", "url": "http://arxiv.org/abs/2507.17343v1", "summary": "Multimodal representation learning seeks to create a unified representation\nspace by integrating diverse data modalities to improve multimodal\nunderstanding. Traditional methods often depend on pairwise contrastive\nlearning, which relies on a predefined anchor modality, restricting alignment\nacross all modalities. Recent advances have investigated the simultaneous\nalignment of multiple modalities, yet several challenges remain, such as\nlimitations imposed by fixed anchor points and instability arising from\noptimizing the product of singular values. To address the challenges, in this\npaper, we propose Principled Multimodal Representation Learning (PMRL), a novel\nframework that achieves simultaneous alignment of multiple modalities without\nanchor dependency in a more stable manner. Specifically, grounded in the\ntheoretical insight that full alignment corresponds to a rank-1 Gram matrix,\nPMRL optimizes the dominant singular value of the representation matrix to\nalign modalities along a shared leading direction. We propose a softmax-based\nloss function that treats singular values as logits to prioritize the largest\nsingular value. Besides, instance-wise contrastive regularization on the\nleading eigenvectors maintains inter-instance separability and prevents\nrepresentation collapse. Extensive experiments across diverse tasks demonstrate\nPMRL's superiority compared to baseline methods. The source code will be\npublicly available.", "comment": "32 pages, 9 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.17343v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "规范多模态表征学习", "tldr": "本文提出了一种名为PMRL的新框架，通过优化主奇异值和实例级对比正则化，实现了多模态的同步无锚点对齐，解决了传统方法的局限性。", "motivation": "传统多模态表征学习方法依赖于预定义锚点进行成对对比学习，限制了模态间的对齐。近期多模态同步对齐方法仍面临固定锚点和优化奇异值乘积导致的不稳定性等挑战。", "method": "本文提出规范多模态表征学习（PMRL）框架。该框架基于全对齐对应秩-1 Gram矩阵的理论，通过优化表征矩阵的主奇异值来沿共享主方向对齐模态。PMRL采用基于softmax的损失函数将奇异值视为logits以优先最大奇异值，并通过领先特征向量上的实例级对比正则化来保持实例间可分离性并防止表征崩溃。", "result": "在各种任务上的大量实验表明，PMRL优于基线方法。", "conclusion": "PMRL通过其新颖的无锚点同步对齐机制和稳定的优化方法，有效解决了多模态表征学习中的关键挑战，并取得了卓越的性能。", "translation": "多模态表征学习旨在通过整合多样化的数据模态来创建统一的表征空间，以提高多模态理解。传统方法通常依赖于成对对比学习，这依赖于预定义的锚点模态，限制了所有模态之间的对齐。最近的进展研究了多模态的同步对齐，但仍存在一些挑战，例如固定锚点带来的限制以及优化奇异值乘积引起的不稳定性。为了解决这些挑战，本文提出规范多模态表征学习（PMRL），这是一种新颖的框架，以更稳定的方式实现多模态的同步对齐，而无需锚点依赖。具体来说，基于全对齐对应秩-1 Gram矩阵的理论洞察，PMRL优化表征矩阵的主奇异值，以沿共享的主导方向对齐模态。我们提出了一种基于softmax的损失函数，将奇异值视为logits，以优先处理最大的奇异值。此外，领先特征向量上的实例级对比正则化保持了实例间可分离性并防止了表征崩溃。跨不同任务的大量实验证明了PMRL相对于基线方法的优越性。源代码将公开可用。", "summary": "本文提出了一种名为规范多模态表征学习（PMRL）的新框架，旨在解决传统多模态表征学习中锚点依赖和优化不稳定性等问题。PMRL通过优化表征矩阵的主奇异值实现多模态的同步无锚点对齐，并引入了基于softmax的损失函数和实例级对比正则化来确保对齐的稳定性和表征的判别性。实验结果表明PMRL在各种任务上均优于现有基线方法。", "keywords": "多模态表征学习, 奇异值优化, 无锚点对齐, 对比学习, 秩-1 Gram矩阵", "comments": "该论文通过提出PMRL框架，解决了多模态表征学习中长期存在的锚点依赖和优化稳定性问题，其创新点在于将全对齐与秩-1 Gram矩阵的理论洞察相结合，并设计了新颖的损失函数和正则化策略。这对于推动多模态理解领域的发展具有重要意义。"}}
{"id": "2503.17141", "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks", "authors": ["Ekaterina Dmitrieva", "Maksim Kaledin"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      5 pages (4 content pages + 1 page of references)", "url": "http://arxiv.org/abs/2503.17141v2", "summary": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software. Still, modern deep learning solutions often require high\namount of computational resources what makes their usage on low-resource\ndevices challenging. We present HiFi-Stream, an optimized version of recently\npublished HiFi++ model. Our experiments demonstrate that HiFi-Stream saves most\nof the qualities of the original model despite its size and computational\ncomplexity improved in comparison to the original HiFi++ making it one of the\nsmallest and fastest models available. The model is evaluated in streaming\nsetting where it demonstrates its superior performance in comparison to modern\nbaselines.", "comment": "5 pages (4 content pages + 1 page of references)", "pdf_url": "http://arxiv.org/pdf/2503.17141v2", "cate": "cs.SD", "date": "2025-03-21", "updated": "2025-07-23", "AI": {"title_translation": "HiFi-Stream：基于生成对抗网络的流式语音增强", "tldr": "HiFi-Stream是HiFi++模型的优化版本，它显著减小了模型大小和计算复杂度，同时保持了原始模型的大部分质量，并在流式设置中表现出优于现代基线的性能，使其适用于低资源设备。", "motivation": "现代深度学习语音增强解决方案通常需要大量计算资源，这使得它们在低资源设备上的应用面临挑战。", "method": "本文提出了HiFi-Stream，它是最近发布的HiFi++模型的优化版本。该模型在流式设置中进行评估。", "result": "实验表明，HiFi-Stream在尺寸和计算复杂度方面相比原始HiFi++有所改进，同时保留了原始模型的大部分质量，使其成为现有最小、最快的模型之一。在流式设置中，它展现出优于现代基线的性能。", "conclusion": "HiFi-Stream是一个在低资源设备上进行流式语音增强的有效且高性能的解决方案，它在保持高音质的同时显著降低了计算开销。", "translation": "语音增强技术已成为移动设备和语音软件的核心技术。然而，现代深度学习解决方案通常需要大量的计算资源，这使得它们在低资源设备上的使用具有挑战性。我们提出了HiFi-Stream，它是最近发布的HiFi++模型的优化版本。我们的实验表明，尽管HiFi-Stream的模型大小和计算复杂度相比原始HiFi++有所改进，但它保留了原始模型的大部分质量，使其成为现有最小、最快的模型之一。该模型在流式设置中进行评估，并展示了其与现代基线相比的卓越性能。", "summary": "本文介绍了HiFi-Stream，一个基于生成对抗网络的流式语音增强模型，它是现有HiFi++模型的优化版本。针对现代深度学习模型在低资源设备上计算开销大的问题，HiFi-Stream在保持高音质的同时，显著减小了模型尺寸并降低了计算复杂度。实验结果表明，HiFi-Stream是目前最小、最快的模型之一，并在流式语音增强任务中表现出优于现有主流基线的性能。", "keywords": "语音增强, 生成对抗网络, 流式, 低资源, HiFi-Stream", "comments": "HiFi-Stream的创新之处在于其对现有高性能语音增强模型HiFi++的优化，使其在保持音质的同时，显著降低了计算资源需求，这对于移动设备和低资源环境的应用至关重要。其在流式设置下的优越表现进一步凸显了其在实际应用中的潜力。"}}
{"id": "2507.17264", "title": "Understanding Prompt Programming Tasks and Questions", "authors": ["Jenny T. Liang", "Chenyang Yang", "Agnia Sergeyuk", "Travis D. Breaux", "Brad A. Myers"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17264v1", "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17264v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "理解提示编程任务与问题", "tldr": "本研究通过对提示编程任务和问题的分类，揭示了当前提示编程工具对开发者支持不足的问题。", "motivation": "随着大型语言模型（LLMs）等基础模型（FMs）的普及，提示编程成为一种新型软件开发方式。然而，开发者在修改提示时所提的问题及其对变更计划的影响尚不明确，且现有研究和商业提示编程工具是否充分满足开发者需求也存疑。", "method": "研究通过开发包含25项任务和51个问题的分类法来解决挑战，并衡量了每项任务和问题的重要性。具体方法包括访谈16位提示程序员，观察8位开发者进行提示变更，以及调查50位开发者。随后，将分类法与48个研究和商业工具进行了比较。", "result": "研究发现提示编程的支持度不足：所有任务仍需手动完成，51个问题中有16个（包括大多数最重要的问题）仍未得到解答。", "conclusion": "提示编程目前支持不足，亟需改进工具来满足开发者的需求，特别是在自动化任务和回答关键问题方面存在重要机遇。", "translation": "提示基础模型（FMs），如大型语言模型（LLMs），已经实现了以前只能通过微调FMs才能实现的新的AI驱动软件功能（例如，文本摘要）。现在，开发者正在将提示嵌入到软件中，这被称为提示编程。提示编程的过程要求开发者对其提示进行许多更改。然而，开发者在更新提示时提出的问题尚不清楚，尽管这些问题的答案会影响开发者如何规划其更改。随着研究和商业提示编程工具数量的增长，目前尚不清楚提示程序员的需求是否得到充分满足。我们通过开发一个包含25项提示程序员任务和51个问题的分类法来解决这些挑战，并衡量了每项任务和问题的重要性。我们采访了16位提示程序员，观察了8位开发者进行提示更改，并调查了50位开发者。然后，我们将该分类法与48个研究和商业工具进行了比较。我们发现提示编程的支持度不佳：所有任务都是手动完成的，51个问题中有16个（包括大多数最重要的问题）仍未得到解答。基于此，我们概述了提示编程工具的重要机遇。", "summary": "本研究旨在理解提示编程中的任务和问题，以评估现有工具对开发者的支持程度。通过对16位提示程序员的访谈、8位开发者的观察以及50位开发者的调查，研究构建了一个包含25项任务和51个问题的分类法。研究发现，尽管提示编程日益普及，但当前工具的支持度严重不足，所有任务仍需手动完成，且大部分关键问题未被解决。这表明提示编程工具存在巨大的改进空间。", "keywords": "提示编程, 大型语言模型, 开发者工具, 任务分类, 用户研究", "comments": "这项研究通过系统地分类提示编程中的任务和问题，揭示了当前提示编程工具的显著不足，为未来工具的设计和开发指明了方向。其创新之处在于首次对提示编程的开发者需求进行了深入的用户研究，并提供了量化的数据支持。其重要性在于，随着提示工程成为AI应用开发的核心环节，提升提示编程的效率和体验对AI产业发展至关重要。"}}
{"id": "2506.10533", "title": "A velocity-vorticity-pressure formulation for the steady Navier--Stokes--Brinkman--Forchheimer problem", "authors": ["Santiago Badia", "Carsten Carstensen", "Alberto F. Martin", "Ricardo Ruiz-Baier", "Segundo Villa-Fuentes"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.10533v2", "summary": "The flow of incompressible fluid in highly permeable porous media in\nvorticity - velocity - Bernoulli pressure form leads to a double saddle-point\nproblem in the Navier--Stokes--Brinkman--Forchheimer equations. The paper\nestablishes, for small sources, the existence of solutions on the continuous\nand discrete level of lowest-order piecewise divergence-free Crouzeix--Raviart\nfinite elements. The vorticity employs a vector version of the pressure space\nwith normal and tangential velocity jump penalisation terms. A simple\nRaviart--Thomas interpolant leads to pressure-robust a priori error estimates.\nAn explicit residual-based a posteriori error estimate allows for efficient and\nreliable a posteriori error control. The efficiency for the Forchheimer\nnonlinearity requires a novel discrete inequality of independent interest. The\nimplementation is based upon a light-weight forest-of-trees data structure\nhandled by a highly parallel set of adaptive mesh refining algorithms.\nNumerical simulations reveal robustness of the a posteriori error estimates and\nimproved convergence rates by adaptive mesh-refining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.10533v2", "cate": "math.NA", "date": "2025-06-12", "updated": "2025-07-23", "AI": {"title_translation": "稳态Navier--Stokes--Brinkman--Forchheimer问题的速度-涡度-压力公式", "tldr": "该文提出了一个用于稳态Navier-Stokes-Brinkman-Forchheimer问题的速度-涡度-压力公式，证明了其解的存在性，并开发了压力鲁棒的先验误差估计和高效的后验误差控制方法，通过自适应网格细化实现了鲁棒性和收敛性改进。", "motivation": "解决不可压缩流体在高度渗透多孔介质中流动时，Navier-Stokes-Brinkman-Forchheimer方程组在涡度-速度-伯努利压力形式下导致的双鞍点问题。", "method": "该研究提出了一个用于稳态Navier-Stokes-Brinkman-Forchheimer问题的速度-涡度-压力公式，并使用最低阶分段无散度Crouzeix-Raviart有限元。涡度部分采用了带法向和切向速度跳跃惩罚项的压力空间向量版本。通过Raviart-Thomas插值器实现了压力鲁棒的先验误差估计，并开发了基于残差的后验误差估计进行误差控制。为处理Forchheimer非线性，引入了一种新的离散不等式。实现基于轻量级树形数据结构和高度并行的自适应网格细化算法。", "result": "证明了连续和离散层面解的存在性；获得了压力鲁棒的先验误差估计；实现了高效可靠的后验误差控制；数值模拟表明后验误差估计的鲁棒性以及通过自适应网格细化提高了收敛速度。", "conclusion": "本文提出的速度-涡度-压力公式、有限元方法及误差估计策略，能够有效且鲁棒地解决稳态Navier-Stokes-Brinkman-Forchheimer方程在多孔介质流中的双鞍点问题，并通过自适应网格细化提高了计算效率和收敛性。", "translation": "不可压缩流体在高度渗透多孔介质中以涡度-速度-伯努利压力形式流动时，Navier--Stokes--Brinkman--Forchheimer方程组会导致一个双鞍点问题。本文针对小源情况，建立了最低阶分段无散度Crouzeix--Raviart有限元在连续和离散层面的解的存在性。涡度部分采用了带法向和切向速度跳跃惩罚项的压力空间向量版本。一个简单的Raviart--Thomas插值器得到了压力鲁棒的先验误差估计。一个显式的基于残差的后验误差估计允许高效可靠的后验误差控制。为提高Forchheimer非线性的效率，需要一个具有独立意义的新颖离散不等式。该实现基于轻量级的森林-树数据结构，由一组高度并行的自适应网格细化算法处理。数值模拟揭示了后验误差估计的鲁棒性以及通过自适应网格细化提高了收敛速度。", "summary": "本文研究了Navier-Stokes-Brinkman-Forchheimer方程描述的不可压缩流体在多孔介质中流动时产生的双鞍点问题。为此，提出了一种速度-涡度-压力公式，并利用Crouzeix-Raviart有限元证明了连续和离散解的存在性。研究开发了基于Raviart-Thomas插值器的压力鲁棒先验误差估计以及高效的基于残差的后验误差控制方法。针对Forchheimer非线性，引入了一种新的离散不等式。该方法的实现基于轻量级树形数据结构和并行自适应网格细化算法，数值模拟验证了后验误差估计的鲁棒性以及收敛速度的提升。", "keywords": "Navier-Stokes-Brinkman-Forchheimer, 多孔介质, 速度-涡度-压力, 有限元, 误差估计, 自适应网格细化", "comments": "本文的创新点在于针对复杂的Navier-Stokes-Brinkman-Forchheimer双鞍点问题提出了新颖的速度-涡度-压力公式，开发了压力鲁棒的误差估计方法，并引入了处理Forchheimer非线性的新颖离散不等式。其基于轻量级数据结构和自适应网格细化的高效实现，对多孔介质流动的鲁棒和高效数值模拟具有重要意义。"}}
{"id": "2507.16978", "title": "Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS and ScaNN", "authors": ["Mohammad Saleh Refahi", "Gavin Hearne", "Harrison Muller", "Kieran Lynch", "Bahrad A. Sokhansanj", "James R. Brown", "Gail Rosen"], "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Genomics (q-bio.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16978v1", "summary": "The exponential growth of DNA sequencing data has outpaced traditional\nheuristic-based methods, which struggle to scale effectively. Efficient\ncomputational approaches are urgently needed to support large-scale similarity\nsearch, a foundational task in bioinformatics for detecting homology,\nfunctional similarity, and novelty among genomic and proteomic sequences.\nAlthough tools like BLAST have been widely used and remain effective in many\nscenarios, they suffer from limitations such as high computational cost and\npoor performance on divergent sequences.\n  In this work, we explore embedding-based similarity search methods that learn\nlatent representations capturing deeper structural and functional patterns\nbeyond raw sequence alignment. We systematically evaluate two state-of-the-art\nvector search libraries, FAISS and ScaNN, on biologically meaningful gene\nembeddings. Unlike prior studies, our analysis focuses on\nbioinformatics-specific embeddings and benchmarks their utility for detecting\nnovel sequences, including those from uncharacterized taxa or genes lacking\nknown homologs. Our results highlight both computational advantages (in memory\nand runtime efficiency) and improved retrieval quality, offering a promising\nalternative to traditional alignment-heavy tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16978v1", "cate": "q-bio.GN", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "快速可扩展的基因嵌入搜索：FAISS和ScaNN的比较研究", "tldr": "鉴于DNA测序数据呈指数级增长，传统方法难以扩展，本文探索并评估了FAISS和ScaNN两种先进的向量搜索库在基因嵌入相似性搜索中的应用。研究结果表明，这些嵌入式方法在计算效率和检索质量方面优于传统工具，为生物信息学中的大规模相似性搜索提供了有前景的替代方案。", "motivation": "DNA测序数据的指数级增长已超越了传统启发式方法的处理能力，这些方法难以有效扩展。生物信息学中需要高效的计算方法来支持大规模相似性搜索，这是检测基因组和蛋白质组序列同源性、功能相似性和新颖性的基础任务。尽管BLAST等工具被广泛使用，但在高计算成本和对发散序列性能不佳方面存在局限性。", "method": "本研究探索了基于嵌入的相似性搜索方法，这些方法学习捕捉超越原始序列比对的深层结构和功能模式的潜在表示。作者系统地评估了FAISS和ScaNN这两种最先进的向量搜索库在具有生物学意义的基因嵌入上的性能。与以往研究不同，本分析侧重于生物信息学特有的嵌入，并评估它们在检测新序列（包括来自未表征分类群或缺乏已知同源物的基因）方面的效用。", "result": "研究结果突出了计算优势（在内存和运行时效率方面）和改进的检索质量。", "conclusion": "基于嵌入的相似性搜索方法，特别是利用FAISS和ScaNN，为传统的依赖比对的工具提供了一个有前景的替代方案，在处理大规模基因嵌入搜索时具有更高的计算效率和更好的检索质量。", "translation": "DNA测序数据的指数级增长已经超越了传统的启发式方法，这些方法难以有效扩展。迫切需要高效的计算方法来支持大规模相似性搜索，这是生物信息学中检测基因组和蛋白质组序列同源性、功能相似性和新颖性的基础任务。尽管BLAST等工具已被广泛使用并在许多场景中仍然有效，但它们存在计算成本高和对发散序列性能差等局限性。\n在这项工作中，我们探索了基于嵌入的相似性搜索方法，这些方法学习捕捉超越原始序列比对的深层结构和功能模式的潜在表示。我们系统地评估了FAISS和ScaNN这两个最先进的向量搜索库在具有生物学意义的基因嵌入上的性能。与以往的研究不同，我们的分析侧重于生物信息学特有的嵌入，并评估它们在检测新序列（包括来自未表征分类群或缺乏已知同源物的基因）方面的效用。我们的结果突出了计算优势（在内存和运行时效率方面）和改进的检索质量，为传统的依赖比对的工具提供了一个有前景的替代方案。", "summary": "本文旨在解决DNA测序数据激增背景下，传统基因相似性搜索方法（如BLAST）存在的扩展性差、计算成本高和对发散序列性能不佳的问题。研究人员评估了FAISS和ScaNN这两种先进的向量搜索库在生物信息学特定基因嵌入上的性能。结果表明，这些基于嵌入的方法在内存和运行时效率上具有显著的计算优势，并能提供更优的检索质量，尤其是在检测新颖序列方面，为传统比对工具提供了一个有前景的替代方案。", "keywords": "基因嵌入, 相似性搜索, FAISS, ScaNN, 生物信息学", "comments": "该论文将通用的向量搜索库（FAISS、ScaNN）创新性地应用于基因嵌入搜索这一生物信息学特定领域，解决了该领域关键的扩展性问题。其对“生物信息学特定嵌入”和“检测新颖序列”的关注，突显了超越传统比对方法，具有实用且重要的贡献。"}}
{"id": "2507.17552", "title": "Model Predictive Control for Unlocking Energy Flexibility of Heat Pump and Thermal Energy Storage Systems: Experimental Results", "authors": ["Weihong Tang", "Yun Li", "Shalika Walker", "Tamas Keviczky"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.17552v1", "summary": "Increasing penetration of renewable energy sources (RES) and electrification\nof energy systems necessitates the engagement of demand-side management (DSM)\nto help alleviate congestion in electricity grid. Heat pump and thermal energy\nstorage (HPTES) systems, being energy efficient solutions, are becoming popular\nin modern buildings and are promising to contribute to demand-side management\n(DSM) due to their significant share in household electricity consumption. For\ntypical HPTES systems, this paper presents a systematic design framework\ncovering a control-oriented modeling process and energy-flexible model\npredictive control (MPC) design. The proposed MPC-based DSM strategy offers an\ninnovative solution for efficient DSM by following a two-step DSM framework. In\nthe first step, flexibility assessment is performed to quantitatively evaluate\nthe flexibility potential of the HPTES system by solving a mixed-integer\neconomic MPC problem. In the second step, flexibility exploitation is achieved\nthrough reacting to feasible demand response (DR) requests while respecting\nsystem constraints. Both numerical simulations and real-world experiments are\nperformed based on a real HPTES installation to showcase the viability and\neffectiveness of the proposed design.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.17552v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "热泵和热能存储系统能量灵活性释放的模型预测控制：实验结果", "tldr": "本文提出了一种基于模型预测控制（MPC）的需求侧管理（DSM）策略，用于释放热泵和热能存储（HPTES）系统的能量灵活性，并通过实验验证了其可行性和有效性。", "motivation": "可再生能源渗透率的提高和能源系统的电气化使得需求侧管理（DSM）对于缓解电网拥堵变得必要。热泵和热能存储（HPTES）系统作为高效的解决方案，在现代建筑中越来越受欢迎，并且由于其在家庭用电中占有重要份额，有望为需求侧管理做出贡献。", "method": "本文提出了一种系统的设计框架，包括面向控制的建模过程和能量灵活的模型预测控制（MPC）设计。所提出的基于MPC的DSM策略遵循两步DSM框架：第一步，通过解决混合整数经济MPC问题进行灵活性评估，以量化评估HPTES系统的灵活性潜力；第二步，通过响应可行的需求响应（DR）请求并在尊重系统约束的情况下实现灵活性利用。", "result": "基于真实HPTES装置的数值模拟和实际实验都证明了所提出设计的可行性和有效性。", "conclusion": "所提出的基于模型预测控制的需求侧管理策略能够有效释放热泵和热能存储系统的能量灵活性，并通过实际实验验证了其可行性和有效性。", "translation": "可再生能源（RES）渗透率的增加和能源系统的电气化使得需求侧管理（DSM）的参与变得必要，以帮助缓解电网拥堵。热泵和热能存储（HPTES）系统作为节能解决方案，在现代建筑中越来越受欢迎，并且由于其在家庭用电中占有重要份额，有望为需求侧管理（DSM）做出贡献。对于典型的HPTES系统，本文提出了一种系统的设计框架，涵盖了面向控制的建模过程和能量灵活的模型预测控制（MPC）设计。所提出的基于MPC的DSM策略通过遵循两步DSM框架，为高效DSM提供了一种创新解决方案。第一步，通过解决混合整数经济MPC问题，对HPTES系统的灵活性潜力进行定量评估。第二步，通过响应可行的需求响应（DR）请求，同时尊重系统约束，实现灵活性利用。基于真实的HPTES装置进行了数值模拟和实际实验，以展示所提出设计的可行性和有效性。", "summary": "本文提出了一种针对热泵和热能存储（HPTES）系统的模型预测控制（MPC）设计框架，旨在释放其能量灵活性以支持需求侧管理（DSM）。该框架包含控制导向的建模和两步式DSM策略：首先评估HPTES系统的灵活性潜力，然后根据需求响应请求进行灵活性利用。通过数值模拟和实际实验，验证了该MPC-based DSM策略在缓解电网拥堵方面的可行性和有效性。", "keywords": "热泵, 热能存储, 模型预测控制, 需求侧管理, 能量灵活性", "comments": "本文提出了一种新颖的基于MPC的两步式DSM框架，用于管理HPTES系统的能量灵活性，其创新性在于将灵活性评估与利用相结合。通过实际实验验证了其有效性，增加了研究结果的可靠性和实用性，对未来智能电网和建筑能源管理具有重要意义。"}}
{"id": "2507.17614", "title": "Comparing performance of variational quantum algorithm simulations on HPC systems", "authors": ["Marco De Pascale", "Tobias Valentin Bauer", "Yaknan John Gambo", "Mario Hernández Vera", "Stefan Huber", "Burak Mete", "Amit Jamadagni", "Amine Bentellis", "Marita Oliv", "Luigi Iapichino", "Jeanette Miriam Lorenz"], "categories": ["quant-ph", "cs.DC"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17614v1", "summary": "Variational quantum algorithms are of special importance in the research on\nquantum computing applications because of their applicability to current Noisy\nIntermediate-Scale Quantum (NISQ) devices. The main building blocks of these\nalgorithms (among them, the definition of the Hamiltonian and of the ansatz,\nthe optimizer) define a relatively large parameter space, making the comparison\nof results and performance between different approaches and software simulators\ncumbersome and prone to errors. In this paper, we employ a generic description\nof the problem, in terms of both Hamiltonian and ansatz, to port a problem\ndefinition consistently among different simulators. Three use cases of\nrelevance for current quantum hardware (ground state calculation for the\nHydrogen molecule, MaxCut, Travelling Salesman Problem) have been run on a set\nof HPC systems and software simulators to study the dependence of performance\non the runtime environment, the scalability of the simulation codes and the\nmutual agreement of the physical results, respectively. The results show that\nour toolchain can successfully translate a problem definition between different\nsimulators. On the other hand, variational algorithms are limited in their\nscaling by the long runtimes with respect to their memory footprint, so they\nexpose limited parallelism to computation. This shortcoming is partially\nmitigated by using techniques like job arrays. The potential of the parser tool\nfor exploring HPC performance and comparisons of results of variational\nalgorithm simulations is highlighted.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17614v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "比较HPC系统上变分量子算法模拟的性能", "tldr": "本研究比较了在HPC系统上运行不同变分量子算法模拟器的性能，发现其工具链能成功转换问题定义，但算法的扩展性受限于长时间运行和有限并行性，可通过作业数组等技术部分缓解。", "motivation": "变分量子算法（VQA）对当前的噪声中等规模量子（NISQ）设备至关重要。然而，VQA的巨大参数空间使得不同方法和软件模拟器之间的结果和性能比较变得繁琐且容易出错。因此，本研究旨在解决这一比较难题。", "method": "研究采用了一种通用的哈密顿量和ansatz问题描述方法，以确保问题定义能在不同模拟器之间一致地移植。选择了三个与当前量子硬件相关的用例（氢分子基态计算、MaxCut问题、旅行商问题），并在HPC系统和软件模拟器上运行，以研究性能对运行时环境的依赖性、模拟代码的可扩展性以及物理结果的一致性。", "result": "研究结果表明，所开发的工具链能够成功地在不同模拟器之间转换问题定义。然而，变分算法的扩展性受到运行时长与内存占用比的限制，导致其计算并行性有限。但通过使用作业数组等技术，可以部分缓解这一缺点。", "conclusion": "本文强调了其解析工具在探索HPC性能和比较变分算法模拟结果方面的潜力。", "translation": "变分量子算法因其适用于当前的噪声中等规模量子（NISQ）设备，在量子计算应用研究中具有特殊的重要性。这些算法的主要组成部分（其中包括哈密顿量和ansatz的定义、优化器）定义了一个相对较大的参数空间，使得不同方法和软件模拟器之间的结果和性能比较变得繁琐且容易出错。在本文中，我们采用了一种通用的问题描述，包括哈密顿量和ansatz，以在不同模拟器之间一致地移植问题定义。针对当前量子硬件相关的三个用例（氢分子基态计算、MaxCut、旅行商问题）在一组HPC系统和软件模拟器上运行，分别研究了性能对运行时环境的依赖性、模拟代码的可扩展性以及物理结果的相互一致性。结果表明，我们的工具链可以成功地在不同模拟器之间转换问题定义。另一方面，变分算法的扩展性受限于其长时间运行与内存占用的比率，因此它们对计算的并行性有限。这一缺点通过使用作业数组等技术得到了部分缓解。本文强调了该解析工具在探索HPC性能和比较变分算法模拟结果方面的潜力。", "summary": "本研究旨在解决变分量子算法（VQA）在不同模拟器间性能比较的挑战。通过采用通用的问题描述方法，研究者将氢分子基态计算、MaxCut和旅行商问题等VQA用例在HPC系统和软件模拟器上运行。结果显示，该工具链能成功实现问题定义在不同模拟器间的转换。同时，研究也揭示了VQA的扩展性受限于其长时间运行和有限的并行性，但可通过作业数组等技术部分缓解。论文强调了该解析工具在VQA模拟性能评估和结果比较中的应用潜力。", "keywords": "变分量子算法, HPC系统, 量子模拟, 性能比较, NISQ", "comments": "该论文通过提出一种通用的问题描述方法，有效解决了变分量子算法在不同模拟器间进行性能和结果比较的复杂性问题，具有重要的实践意义。其工具链的成功应用展现了在NISQ时代统一VQA模拟环境的潜力。同时，论文也坦诚指出了VQA在HPC系统上扩展性受限的固有挑战，并提出了缓解策略，为未来的研究指明了方向。"}}
{"id": "2507.16857", "title": "Cross-Subreddit Behavior as Open-Source Indicators of Coordinated Influence: A Case Study of r/Sino & r/China", "authors": ["Manon Pilaud", "Ian McCulloh"], "categories": ["cs.SI", "H.3.3; H.2.8; I.2.7; J.4; K.4.1"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      This is an extended 13 page version of a short paper accepted at FOSINT 2025", "url": "http://arxiv.org/abs/2507.16857v1", "summary": "This study investigates potential indicators of coordinated influence\nactivity among users participating in both r/Sino and r/China, two\nideologically divergent Reddit communities focused on Chinese political\ndiscourse. Topic modeling and sentiment analysis are applied to all posts and\ncomments authored by dual-subreddit users to construct a user-topic sentiment\nmatrix. Individual sentiment patterns are compared to global topic baselines\nderived from the broader r/Sino and r/China populations. Behavioral profiling\nis performed using full user activity histories and metadata, incorporating\nmeasures such as lexical diversity, language consistency, account age, posting\nfrequency, and karma distribution. Users exhibiting multiple behavioral\nanomalies are identified and examined within a subreddit co-participation\nnetwork to assess structural overlap. The combined linguistic and behavioral\nanalysis enables the identification of patterns consistent with inauthentic or\nstrategically structured participation. These findings demonstrate the utility\nof integrating content and activity-based signals in the analysis of online\ninfluence behavior within contested information environments.", "comment": "This is an extended 13 page version of a short paper accepted at\n  FOSINT 2025", "pdf_url": "http://arxiv.org/pdf/2507.16857v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "跨版块行为作为协同影响力开源指标：以r/Sino和r/China为例", "tldr": "研究通过分析在r/Sino和r/China两个Reddit社区同时活跃用户的语言和行为模式，识别协同影响力活动的迹象。", "motivation": "该研究旨在调查在r/Sino和r/China这两个意识形态对立的Reddit社区中，同时参与的用户之间是否存在协同影响力活动的潜在迹象。", "method": "研究对同时活跃于r/Sino和r/China的用户发布的所有帖子和评论进行主题建模和情感分析，构建用户-主题情感矩阵，并将其与更广泛的社区基线进行比较。同时，利用完整的用户活动历史和元数据（如词汇多样性、语言一致性、账号年龄、发帖频率和Karma分布）进行行为画像。最后，识别出具有多重行为异常的用户，并在版块共同参与网络中评估其结构重叠。", "result": "结合语言和行为分析，研究能够识别出与不真实或策略性结构化参与相符的模式。", "conclusion": "研究结果表明，在分析竞争性信息环境中的在线影响力行为时，整合内容和基于活动的信号具有实用性。", "translation": "本研究调查了同时参与r/Sino和r/China这两个专注于中国政治话语的意识形态不同Reddit社区的用户之间，协同影响力活动的潜在指标。研究对双版块用户发布的所有帖子和评论应用主题建模和情感分析，以构建用户-主题情感矩阵。个体情感模式与源自更广泛的r/Sino和r/China人群的全球主题基线进行比较。利用完整的用户活动历史和元数据进行行为画像，包括词汇多样性、语言一致性、账号年龄、发帖频率和Karma分布等指标。识别出表现出多重行为异常的用户，并在版块共同参与网络中进行检查，以评估结构重叠。结合语言和行为分析能够识别出与不真实或策略性结构化参与相符的模式。这些发现证明了在竞争性信息环境中分析在线影响力行为时，整合内容和基于活动的信号的实用性。", "summary": "这项研究通过对同时活跃于r/Sino和r/China这两个意识形态对立的Reddit社区的用户进行语言和行为分析，旨在识别协同影响力活动的指标。研究采用了主题建模、情感分析和行为画像等方法，分析了用户的发帖、评论、账号特征等数据，以检测异常行为模式。研究发现，结合这些分析能够有效识别出不真实或有策略组织的参与模式，证明了整合内容和活动信号在分析在线影响力行为中的价值。", "keywords": "Reddit, 协同影响力, 主题建模, 情感分析, 行为画像", "comments": "这篇论文创新性地结合了语言学分析（主题建模、情感分析）和行为学分析（账户元数据、活动历史），来识别在线社区中的协同影响力活动。其重要性在于提供了一种量化和系统化的方法，用于揭示复杂信息环境中的潜在操纵行为，尤其是在意识形态对立的社区中。该研究为开源情报（OSINT）提供了新的视角和工具，有助于理解和应对虚假信息和协同行动。"}}
{"id": "2507.17578", "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in alphabetical order", "url": "http://arxiv.org/abs/2507.17578v1", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "pdf_url": "http://arxiv.org/pdf/2507.17578v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "非洲语言自动语音识别的合成语音数据", "tldr": "研究表明，为非洲低资源语言生成大规模合成语音数据可有效提升ASR性能，且成本远低于真实数据。", "motivation": "非洲的2300多种语言中，大多数仍无法使用语音技术。本研究旨在系统评估大规模合成语音语料库对非洲自动语音识别（ASR）的效用。", "method": "本研究采用三步法：大型语言模型（LLM）驱动的文本创建、文本转语音（TTS）语音合成和ASR微调。研究评估了三种语言（豪萨语、迪卢语、奇切瓦语）的ASR改进。", "result": "十种语言中有八种的合成文本可读性得分高于7分中的5分。创建了超过2500小时的合成语音数据，成本低于真实数据的1%。对250小时真实数据和250小时合成豪萨语数据微调的Wav2Vec-BERT-2.0模型，其性能与仅使用500小时真实数据的基线模型相当。579小时真实数据加上450小时至993小时合成数据获得了最佳性能。对极低资源语言，效果各异：奇切瓦语的词错误率（WER）相对改善约6.5%（真实数据与合成数据比例为1:2）；迪卢语的1:1比例在部分评估数据上显示出类似改进，但在其他数据上则没有。研究还揭示了更稳健的评审协议和更准确的评估数据的需求。", "conclusion": "大规模合成语音数据可有效用于非洲低资源语言的ASR系统开发，且成本效益显著。所有数据和模型均已公开发布，以促进非洲语言合成数据领域的进一步研究。", "translation": "非洲2300多种语言中的大多数仍无法使用语音技术。我们首次系统评估了用于非洲ASR的大规模合成语音语料库。我们采用三步流程：LLM驱动的文本创建、TTS语音合成和ASR微调。我们创建了合成文本的十种语言中，有八种的可读性得分高于7分中的5分。我们评估了三种语言（豪萨语、迪卢语、奇切瓦语）的ASR改进，并创建了超过2500小时的合成语音数据，成本低于真实数据的1%。在250小时真实数据和250小时合成豪萨语数据上微调的Wav2Vec-BERT-2.0模型，其性能与仅使用500小时真实数据的基线模型相当，而579小时真实数据加上450小时至993小时合成数据则获得了最佳性能。我们还提供了按性别划分的ASR性能评估。对于极低资源语言，收益各不相同：奇切瓦语的词错误率（WER）在真实数据与合成数据比例为1:2的情况下相对改善约6.5%；迪卢语的1:1比例在部分评估数据上显示出类似改进，但在其他数据上则没有。调查编码器间可靠性、ASR错误和评估数据集揭示了需要更稳健的评审协议和更准确的评估数据。所有数据和模型均已公开发布，以邀请进一步的工作来改进非洲语言的合成数据。", "summary": "本研究首次系统评估了利用大规模合成语音数据提升非洲低资源语言自动语音识别（ASR）性能的可行性。通过LLM驱动的文本创建、TTS语音合成和ASR微调三步法，成功为多种非洲语言生成了超过2500小时的合成语音数据，成本远低于真实数据。实验结果表明，合成数据能有效提升ASR模型性能，甚至在某些情况下可媲美纯真实数据训练的模型。研究还强调了评估协议和数据质量的重要性，并公开发布了所有数据和模型以促进后续研究。", "keywords": "合成语音数据, 自动语音识别, 非洲语言, 低资源语言, Wav2Vec-BERT", "comments": "该研究的创新之处在于系统性地探索了利用合成数据解决非洲低资源语言ASR困境的潜力，并提出了一个端到端的方法。其重要性体现在显著降低了数据获取成本，为这些语言的语音技术发展提供了可行的路径。同时，公开发布数据和模型将极大促进社区协作和后续研究。研究也指出了现有评估方法和数据质量的局限性，为未来的工作提供了明确的方向。"}}
{"id": "2507.17161", "title": "Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection", "authors": ["Vinura Galwaduge", "Jagath Samarabandu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17161v1", "summary": "Modern network intrusion detection systems (NIDS) frequently utilize the\npredictive power of complex deep learning models. However, the \"black-box\"\nnature of such deep learning methods adds a layer of opaqueness that hinders\nthe proper understanding of detection decisions, trust in the decisions and\nprevent timely countermeasures against such attacks. Explainable AI (XAI)\nmethods provide a solution to this problem by providing insights into the\ncauses of the predictions. The majority of the existing XAI methods provide\nexplanations which are not convenient to convert into actionable\ncountermeasures. In this work, we propose a novel diffusion-based\ncounterfactual explanation framework that can provide actionable explanations\nfor network intrusion attacks. We evaluated our proposed algorithm against\nseveral other publicly available counterfactual explanation algorithms on 3\nmodern network intrusion datasets. To the best of our knowledge, this work also\npresents the first comparative analysis of existing counterfactual explanation\nalgorithms within the context of network intrusion detection systems. Our\nproposed method provide minimal, diverse counterfactual explanations out of the\ntested counterfactual explanation algorithms in a more efficient manner by\nreducing the time to generate explanations. We also demonstrate how\ncounterfactual explanations can provide actionable explanations by summarizing\nthem to create a set of global rules. These rules are actionable not only at\ninstance level but also at the global level for intrusion attacks. These global\ncounterfactual rules show the ability to effectively filter out incoming attack\nqueries which is crucial for efficient intrusion detection and defense\nmechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17161v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于表格扩散的可操作反事实解释用于网络入侵检测", "tldr": "本文提出了一种新颖的基于扩散的反事实解释框架，旨在为网络入侵检测系统（NIDS）提供可操作的解释。该方法比现有算法更高效，能生成最小、多样化的反事实解释，并通过汇总形成全局规则，有效过滤攻击。", "motivation": "现代网络入侵检测系统（NIDS）频繁使用复杂深度学习模型，但其“黑盒”性质导致难以理解检测决策、缺乏信任并阻碍及时反制措施。现有可解释AI（XAI）方法提供的解释不方便转化为可操作的对策。", "method": "本文提出了一种新颖的基于扩散的反事实解释框架，旨在为网络入侵攻击提供可操作的解释。该框架通过汇总反事实解释来创建一组全局规则。", "result": "该方法在3个现代网络入侵数据集上进行了评估，结果显示其比其他公开的反事实解释算法更高效（减少生成时间），并能提供最小、多样化的反事实解释。生成的全局反事实规则在实例级别和全局级别都具有可操作性，并能有效过滤传入的攻击查询。", "conclusion": "提出的基于扩散的反事实解释框架能够为网络入侵检测系统提供高效、可操作且具泛化性的反事实解释，显著提升了入侵检测和防御机制的有效性。", "translation": "现代网络入侵检测系统（NIDS）频繁利用复杂深度学习模型的预测能力。然而，此类深度学习方法的“黑盒”性质增加了一层不透明性，阻碍了对检测决策的正确理解、对决策的信任以及及时采取反制措施来对抗此类攻击。可解释AI（XAI）方法通过提供对预测原因的洞察来解决这个问题。现有的大多数XAI方法提供的解释不方便转化为可操作的对策。在这项工作中，我们提出了一种新颖的基于扩散的反事实解释框架，可以为网络入侵攻击提供可操作的解释。我们在3个现代网络入侵数据集上，将我们提出的算法与几种其他公开可用的反事实解释算法进行了评估。据我们所知，这项工作还首次在网络入侵检测系统背景下，对现有反事实解释算法进行了比较分析。我们提出的方法在所测试的反事实解释算法中，以更高效的方式（通过减少生成解释的时间）提供了最小、多样化的反事实解释。我们还展示了反事实解释如何通过汇总它们来创建一组全局规则，从而提供可操作的解释。这些规则不仅在实例级别，而且在入侵攻击的全局级别都具有可操作性。这些全局反事实规则显示出有效过滤传入攻击查询的能力，这对于高效的入侵检测和防御机制至关重要。", "summary": "本文提出了一种新颖的基于表格扩散的反事实解释框架，旨在解决深度学习NIDS的“黑盒”问题和现有XAI方法缺乏可操作性的痛点。该框架能够为网络入侵攻击提供高效、最小且多样化的可操作解释。通过将反事实解释汇总为全局规则，该方法不仅在实例级别，而且在全局级别上都为入侵检测提供了有效的过滤和防御机制。", "keywords": "网络入侵检测, 反事实解释, 可解释AI, 扩散模型, 可操作性", "comments": "这项工作通过引入基于扩散的反事实解释，解决了现有可解释AI在网络入侵检测领域中缺乏“可操作性”的关键问题。其创新点在于将反事实解释转化为全局规则，这显著增强了解释的实用性和对防御策略的指导意义。此外，该研究首次对网络入侵检测背景下的反事实解释算法进行了比较分析，具有重要的基准意义。"}}
{"id": "2507.17396", "title": "Learning from Scratch: Structurally-masked Transformer for Next Generation Lib-free Simulation", "authors": ["Junlang Huang", "Hao Chen", "Zhong Guan"], "categories": ["eess.SP", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17396v1", "summary": "This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17396v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从零开始学习：用于下一代无库仿真的结构化掩码Transformer", "tldr": "本文提出了一种新颖的、基于神经网络的框架，用于多级数据路径的功耗和时序预测，通过直接从SPICE网表学习，避免了传统基于库方法的限制，并实现了SPICE级别的精度。", "motivation": "传统的基于库的功耗和时序分析方法依赖于驱动器特性和负载简化，存在局限性。本文旨在开发一种无需这些简化，能直接从SPICE网表进行高精度预测的新方法。", "method": "该方法提出了一种神经框架，包含两个预训练的神经网络模型：波形预测和延迟估计。这些模型直接从SPICE网表推断瞬态波形和传播延迟，并考虑负载电容、输入压摆率和门尺寸等物理参数。波形预测采用混合CNN-Transformer架构，结合网表感知的节点级编码。对于多级时序预测，采用递归传播策略。此外，还有专门的子网络用于处理主要延迟估计和串扰校正。", "result": "实验结果表明，该框架达到了SPICE级别的精度，在各种工业电路中，均方根误差（RMSE）始终低于0.0098。", "conclusion": "所提出的框架为传统的功耗和时序引擎提供了一种可扩展、结构适应性强的神经网络替代方案，并表现出对物理电路行为的高保真度。", "translation": "本文提出了一种用于多级数据路径功耗和时序预测的神经框架，它区别于依赖于驱动器特性和负载简化的传统基于库的分析方法。据我们所知，这是第一个专门为标准单元设计的、基于语言的、网表感知的神经网络。我们的方法采用了两个预训练的波形预测和延迟估计神经模型，这些模型直接从SPICE网表推断瞬态波形和传播延迟，并以负载电容、输入压摆率和门尺寸等关键物理参数为条件。这种方法无需简化或插值即可准确捕获固有延迟和耦合引起的延迟效应。对于多级时序预测，我们实施了一种递归传播策略，其中每个阶段的预测波形馈入后续阶段，累积捕获逻辑链上的延迟。这种方法确保了复杂信号路径中的精确时序对齐和完整的波形可见性。波形预测利用了混合CNN-Transformer架构和网表感知的节点级编码，解决了传统Transformer固定输入维度限制的问题。此外，专门的子网络分别处理主要延迟估计和串扰校正。实验结果表明达到了SPICE级别的精度，在各种工业电路中，均方根误差始终低于0.0098。所提出的框架为传统的功耗和时序引擎提供了一种可扩展、结构适应性强的神经网络替代方案，表现出对物理电路行为的高保真度。", "summary": "本文提出了一种新颖的神经网络框架，用于多级数据路径的功耗和时序预测。该框架通过直接从SPICE网表学习，避免了传统基于库方法的局限性。它包括波形预测和延迟估计的预训练模型，利用混合CNN-Transformer架构和网表感知编码。通过递归传播策略处理多级时序，并包含专门的子网络进行延迟和串扰校正。实验证明其达到了SPICE级别的精度，RMSE低于0.0098，为电路仿真提供了一个可扩展且高保真的神经网络替代方案。", "keywords": "神经网络, 功耗时序预测, 无库仿真, Transformer, SPICE网表", "comments": "本文提出了一种创新的无库仿真方法，通过直接从SPICE网表学习，显著提升了电路功耗和时序预测的精度和效率。其最大的创新在于首次将语言模型思想应用于电路网表分析，并结合CNN-Transformer架构解决了传统Transformer在处理变长网表输入时的限制。该方法避免了传统方法的简化和插值需求，实现了SPICE级别的精度，对于下一代电路设计和验证具有重要意义。其可扩展性和结构适应性也预示着在复杂工业电路中的广阔应用前景。"}}
{"id": "2507.17290", "title": "Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems", "authors": ["Li Kang", "Yuhan Zhao", "Li Chen"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      RecSys2025", "url": "http://arxiv.org/abs/2507.17290v1", "summary": "Serendipity plays a pivotal role in enhancing user satisfaction within\nrecommender systems, yet its evaluation poses significant challenges due to its\ninherently subjective nature and conceptual ambiguity. Current algorithmic\napproaches predominantly rely on proxy metrics for indirect assessment, often\nfailing to align with real user perceptions, thus creating a gap. With large\nlanguage models (LLMs) increasingly revolutionizing evaluation methodologies\nacross various human annotation tasks, we are inspired to explore a core\nresearch proposition: Can LLMs effectively simulate human users for serendipity\nevaluation? To address this question, we conduct a meta-evaluation on two\ndatasets derived from real user studies in the e-commerce and movie domains,\nfocusing on three key aspects: the accuracy of LLMs compared to conventional\nproxy metrics, the influence of auxiliary data on LLM comprehension, and the\nefficacy of recently popular multi-LLM techniques. Our findings indicate that\neven the simplest zero-shot LLMs achieve parity with, or surpass, the\nperformance of conventional metrics. Furthermore, multi-LLM techniques and the\nincorporation of auxiliary data further enhance alignment with human\nperspectives. Based on our findings, the optimal evaluation by LLMs yields a\nPearson correlation coefficient of 21.5\\% when compared to the results of the\nuser study. This research implies that LLMs may serve as potentially accurate\nand cost-effective evaluators, introducing a new paradigm for serendipity\nevaluation in recommender systems.", "comment": "RecSys2025", "pdf_url": "http://arxiv.org/pdf/2507.17290v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "探索大型语言模型在推荐系统中新颖性评估的潜力", "tldr": "本研究探索了大型语言模型（LLMs）在推荐系统中评估新颖性的潜力，发现LLMs可以有效模拟人类用户进行评估，甚至超越传统指标，并可作为准确且经济高效的评估工具。", "motivation": "推荐系统中新颖性（Serendipity）的评估因其主观性和概念模糊性而面临巨大挑战。现有算法方法主要依赖代理指标进行间接评估，往往无法与真实用户感知对齐，导致评估存在偏差。鉴于大型语言模型（LLMs）在各种人类标注任务中革新评估方法，本研究旨在探索LLMs是否能有效模拟人类用户进行新颖性评估。", "method": "研究人员在来自电子商务和电影领域的两个真实用户研究数据集中进行了元评估（meta-evaluation），重点关注三个关键方面：LLMs与传统代理指标的准确性比较、辅助数据对LLM理解的影响以及近期流行的多LLM技术的有效性。", "result": "研究发现，即使是最简单的零样本LLMs也能达到或超越传统指标的性能。此外，多LLM技术和辅助数据的结合进一步增强了与人类视角的对齐。LLMs的最佳评估结果与用户研究结果的皮尔逊相关系数为21.5%。", "conclusion": "本研究表明，大型语言模型（LLMs）可能作为潜在准确且经济高效的评估器，为推荐系统中的新颖性评估引入了新的范式。", "translation": "新颖性在提升推荐系统用户满意度方面发挥着关键作用，但由于其固有的主观性和概念模糊性，其评估带来了重大挑战。当前的算法方法主要依赖代理指标进行间接评估，常常无法与真实用户感知对齐，从而产生了差距。随着大型语言模型（LLMs）在各种人类标注任务中日益革新评估方法，我们受到启发，探索一个核心研究命题：LLMs能否有效模拟人类用户进行新颖性评估？为了解决这个问题，我们对来自电子商务和电影领域的两个真实用户研究数据集进行了元评估，重点关注三个关键方面：LLMs与传统代理指标的准确性比较、辅助数据对LLM理解的影响以及近期流行的多LLM技术的有效性。我们的研究结果表明，即使是最简单的零样本LLMs也能与传统指标的性能持平或超越。此外，多LLM技术和辅助数据的结合进一步增强了与人类视角的对齐。根据我们的发现，LLMs的最佳评估结果与用户研究结果的皮尔逊相关系数为21.5%。这项研究表明，LLMs可能作为潜在准确且经济高效的评估器，为推荐系统中的新颖性评估引入了新的范式。", "summary": "本研究探讨了大型语言模型（LLMs）在推荐系统中评估新颖性的潜力，以克服传统评估方法的主观性和局限性。通过对真实用户研究数据集进行元评估，研究发现LLMs，即使是零样本LLMs，在模拟人类用户进行新颖性评估方面表现出色，甚至超越了传统指标。进一步地，结合多LLM技术和辅助数据能显著提高评估准确性。研究结果表明，LLMs有望成为推荐系统新颖性评估中一种准确且经济高效的新范式。", "keywords": "大型语言模型, 新颖性评估, 推荐系统, 用户模拟, 元评估", "comments": "这篇论文提出了一种利用大型语言模型（LLMs）评估推荐系统新颖性的创新方法，解决了传统方法难以捕捉用户真实感知的问题。其重要性在于为新颖性这一主观且难以量化的指标提供了一种潜在的自动化、成本效益高的评估途径。研究结果表明LLMs表现出与传统指标相当甚至更优的性能，并强调了多LLM技术和辅助数据的重要性。然而，皮尔逊相关系数21.5%虽然是正向相关，但可能仍有提升空间，未来研究可以探索如何进一步提高LLMs与人类评估的一致性。"}}
{"id": "2411.09549", "title": "Quantum computing inspired paintings: reinterpreting classical masterpieces", "authors": ["Arianna Crippa", "Yahui Chai", "Omar Costa Hamido", "Paulo Itaborai", "Karl Jansen"], "categories": ["quant-ph", "cs.CY"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      10 pages, 8 figures", "url": "http://arxiv.org/abs/2411.09549v4", "summary": "We aim to apply a quantum computing technique to compose artworks. The main\nidea is to revisit three paintings of different styles and historical periods:\n''Narciso'', painted circa 1597-1599 by Michelangelo Merisi (Caravaggio), ''Les\nfils de l'homme'', painted in 1964 by Rene Magritte and ''192 Farben'', painted\nin 1966 by Gerard Richter. We utilize the output of a quantum computation to\nchange the composition in the paintings, leading to a paintings series titled\n''Quantum Transformation I, II, III''. In particular, the figures are\ndiscretized into square lattices and the order of the pieces is changed\naccording to the result of the quantum simulation. We consider an Ising\nHamiltonian as the observable in the quantum computation and its time evolution\nas the final outcome. From a classical subject to abstract forms, we seek to\ncombine classical and quantum aesthetics through these three art pieces.\nBesides experimenting with hardware runs and circuit noise, our goal is to\nreproduce these works as physical oil paintings on wooden panels. With this\nprocess, we complete a full circle between classical and quantum techniques and\ncontribute to rethinking Art practice in the era of quantum computing\ntechnologies.", "comment": "10 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2411.09549v4", "cate": "quant-ph", "date": "2024-11-14", "updated": "2025-07-23", "AI": {"title_translation": "量子计算启发的绘画：重新诠释经典杰作", "tldr": "该研究将量子计算技术应用于艺术创作，通过量子模拟改变经典画作的构图，旨在结合古典与量子美学，并重新思考量子计算时代下的艺术实践。", "motivation": "该研究旨在探索将量子计算技术应用于艺术创作的可能性，通过重新诠释经典画作，结合古典与量子美学，并重新思考量子计算技术时代下的艺术实践。", "method": "研究选取了三幅不同风格和历史时期的经典画作（卡拉瓦乔的《水仙》，马格利特的《人子》，里希特的《192种颜色》）。将画作中的人物离散化为方形网格，并根据量子模拟的结果改变这些网格的顺序。量子计算中考虑伊辛哈密顿量作为可观测量，并将其时间演化作为最终结果。最终成果将以物理油画形式再现。", "result": "通过量子计算的输出改变了三幅经典画作的构图，创作出了一系列名为《量子变换I、II、III》的绘画作品，实现了从经典主体到抽象形式的转变。", "conclusion": "该研究通过将经典和量子技术相结合，成功地利用量子计算重新诠释了经典艺术作品，并完成了从数字创作到物理油画的完整过程，为量子计算技术时代的艺术实践提供了新的思考和贡献。", "translation": "我们旨在应用量子计算技术来创作艺术作品。主要思想是重新审视三幅不同风格和历史时期的画作：米开朗基罗·梅里西（卡拉瓦乔）约1597-1599年绘制的《水仙》、雷内·马格利特1964年绘制的《人子》以及杰拉德·里希特1966年绘制的《192种颜色》。我们利用量子计算的输出改变画作的构图，从而创作出一系列名为《量子变换I、II、III》的画作。特别是，人物被离散化为方形网格，并且碎片的顺序根据量子模拟的结果进行改变。我们考虑伊辛哈密顿量作为量子计算中的可观测量，并将其时间演化作为最终结果。从经典主题到抽象形式，我们试图通过这三件艺术品结合经典和量子美学。除了实验硬件运行和电路噪声，我们的目标是将这些作品复制成木板上的物理油画。通过这个过程，我们完成了经典和量子技术之间的完整循环，并为量子计算技术时代的艺术实践做出贡献，促使人们重新思考艺术。", "summary": "本研究探索了将量子计算应用于艺术创作的新途径。通过选取卡拉瓦乔、马格利特和里希特的三幅经典画作，研究人员利用量子计算的输出，特别是基于伊辛哈密顿量的量子模拟结果，改变了画作中离散化网格的顺序，从而生成了一系列名为《量子变换》的艺术作品。这项工作旨在融合古典与量子美学，并将数字成果转化为物理油画，以此重新审视量子计算时代下的艺术实践。", "keywords": "量子计算, 艺术创作, 经典画作, 伊辛哈密顿量, 艺术实践", "comments": "这项研究极具创新性，它将前沿的量子计算技术引入艺术创作领域，打破了传统艺术与现代科技的界限。通过对经典名画进行量子重构，不仅创造了独特的新作品，更重要的是，它引发了对艺术本质、创作过程以及未来艺术形态的深刻思考。将数字生成物最终转化为物理油画的实践，也使得整个艺术创作过程形成了一个完整的闭环，具有重要的实验意义和启发性。"}}
{"id": "2507.17195", "title": "Closed-Form and Boundary Expressions for Task-Success Probability in Status-Driven Systems", "authors": ["Jianpeng Qi", "Chao Liu", "Rui Wang", "Junyu Dong", "Yanwei Yu"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2507.17195v1", "summary": "Timely and efficient dissemination of server status is critical in\ncompute-first networking systems, where user tasks arrive dynamically and\ncomputing resources are limited and stochastic. In such systems, the access\npoint plays a key role in forwarding tasks to a server based on its latest\nreceived server status. However, modeling the task-success probability\nsuffering the factors of stochastic arrivals, limited server capacity, and\nbidirectional link delays. Therefore, we introduce a unified analytical\nframework that abstracts the AP forwarding rule as a single probability and\nmodels all network and waiting delays via their Laplace transforms. This\napproach yields a closed form expression for the end to end task success\nprobability, together with upper and lower bounds that capture Erlang loss\nblocking, information staleness, and random uplink/downlink delays. We validate\nour results through simulations across a wide range of parameters, showing that\ntheoretical predictions and bounds consistently enclose observed success rates.\nOur framework requires only two interchangeable inputs (the forwarding\nprobability and the delay transforms), making it readily adaptable to\nalternative forwarding policies and delay distributions. Experiments\ndemonstrate that our bounds are able to achieve accuracy within 0.01 (upper\nbound) and 0.016 (lower bound) of the empirical task success probability.", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.17195v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "状态驱动系统中任务成功概率的闭式和边界表达式", "tldr": "本文提出了一个统一的分析框架，用于计算状态驱动系统中任务成功概率的闭式表达式以及上下界，该框架考虑了随机到达、有限容量和双向链路延迟等因素，并通过仿真验证了其准确性。", "motivation": "在计算优先的网络系统中，服务器状态的及时有效传播至关重要，但由于随机到达、服务器容量有限和双向链路延迟等因素，对任务成功概率进行建模很困难。", "method": "引入了一个统一的分析框架，将AP转发规则抽象为单一概率，并通过拉普拉斯变换建模所有网络和等待延迟。该方法得到了端到端任务成功概率的闭式表达式以及上下界。", "result": "该框架得到了端到端任务成功概率的闭式表达式，以及捕获Erlang损失阻塞、信息陈旧和随机上下行延迟的上下界。仿真结果表明，理论预测和边界始终包含观察到的成功率。实验表明，其边界的准确性在经验任务成功概率的0.01（上限）和0.016（下限）之内。", "conclusion": "提出的统一分析框架能够准确地建模和预测状态驱动系统中的任务成功概率，并提供紧密的上下界，具有良好的适应性。", "translation": "在计算优先的网络系统中，服务器状态的及时高效传播至关重要，其中用户任务动态到达，计算资源有限且随机。在此类系统中，接入点（AP）根据其最新接收到的服务器状态在将任务转发到服务器方面发挥着关键作用。然而，对受随机到达、服务器容量有限和双向链路延迟等因素影响的任务成功概率进行建模具有挑战性。因此，我们引入了一个统一的分析框架，将AP转发规则抽象为单一概率，并通过拉普拉斯变换建模所有网络和等待延迟。这种方法为端到端任务成功概率提供了闭式表达式，以及捕获Erlang损失阻塞、信息陈旧和随机上行/下行延迟的上限和下限。我们通过广泛参数范围内的仿真验证了我们的结果，表明理论预测和边界始终包含观察到的成功率。我们的框架只需要两个可互换的输入（转发概率和延迟变换），使其易于适应替代转发策略和延迟分布。实验表明，我们的边界能够达到经验任务成功概率的0.01（上限）和0.016（下限）的精度。", "summary": "本文针对状态驱动系统中任务成功概率的建模挑战，提出了一个统一的分析框架。该框架通过抽象AP转发规则和利用拉普拉斯变换建模延迟，推导出了端到端任务成功概率的闭式表达式及其上下界。这些边界考虑了Erlang损失阻塞、信息陈旧和随机延迟等因素。仿真和实验验证了该框架的准确性和适应性，表明其理论预测和边界能有效包络实际成功率，且具有高精度。", "keywords": "任务成功概率, 状态驱动系统, 闭式表达式, 边界, 拉普拉斯变换", "comments": "这篇论文的创新点在于提出了一个统一的分析框架，能够为复杂的、受多种随机因素影响的状态驱动系统中的任务成功概率提供闭式表达式和紧密边界。其方法将复杂的系统行为抽象为简单的输入，提高了模型的通用性和适应性。该工作对于理解和优化计算优先网络中的任务调度和性能评估具有重要意义。"}}
{"id": "2501.06488", "title": "NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References", "authors": ["Qiang Qu", "Yiran Shen", "Xiaoming Chen", "Yuk Ying Chung", "Weidong Cai", "Tongliang Liu"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.06488v2", "summary": "Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,\neffectively creates photorealistic scenes from sparse viewpoints, typically\nevaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,\nthese full-reference methods, which compare synthesized views to reference\nviews, may not fully capture the perceptual quality of neurally synthesized\nscenes (NSS), particularly due to the limited availability of dense reference\nviews. Furthermore, the challenges in acquiring human perceptual labels hinder\nthe creation of extensive labeled datasets, risking model overfitting and\nreduced generalizability. To address these issues, we propose NVS-SQA, a NSS\nquality assessment method to learn no-reference quality representations through\nself-supervision without reliance on human labels. Traditional self-supervised\nlearning predominantly relies on the \"same instance, similar representation\"\nassumption and extensive datasets. However, given that these conditions do not\napply in NSS quality assessment, we employ heuristic cues and quality scores as\nlearning objectives, along with a specialized contrastive pair preparation\nprocess to improve the effectiveness and efficiency of learning. The results\nshow that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,\non average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second\nbest) and even exceeds 16 full-reference methods across all evaluation metrics\n(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.06488v2", "cate": "cs.CV", "date": "2025-01-11", "updated": "2025-07-23", "AI": {"title_translation": "NVS-SQA：探索用于神经合成场景的无参考自监督质量表征学习", "tldr": "NVS-SQA提出了一种无参考自监督方法，用于评估神经合成场景的质量，无需人工标注，并在性能上超越了现有方法。", "motivation": "现有的全参考质量评估方法（如PSNR、SSIM、LPIPS）可能无法完全捕捉神经合成场景（NSS）的感知质量，特别是由于密集参考视图的有限性。此外，获取人类感知标签的挑战阻碍了大规模标注数据集的创建，导致模型过拟合和泛化能力下降。", "method": "NVS-SQA是一种神经合成场景质量评估方法，通过自监督学习无参考质量表征，不依赖人类标签。它通过使用启发式线索和质量分数作为学习目标，并采用专门的对比对准备过程来提高学习的有效性和效率，克服了传统自监督学习中“相同实例，相似表征”的假设和对大量数据集的依赖。", "result": "NVS-SQA在无参考方法中表现出色，平均在SRCC上提升109.5%，PLCC上提升98.6%，KRCC上提升91.5%（相对于第二名）。它甚至在所有评估指标上都超越了全参考方法，平均在SRCC上提升22.9%，PLCC上提升19.1%，KRCC上提升18.6%（相对于第二名）。", "conclusion": "NVS-SQA通过自监督学习实现了无需参考的神经合成场景质量评估，并在性能上显著优于现有的无参考和全参考方法。", "translation": "神经视图合成（NVS），例如NeRF和3D Gaussian Splatting，能有效从稀疏视角创建逼真的场景，通常通过PSNR、SSIM和LPIPS等质量评估方法进行评估。然而，这些全参考方法通过将合成视图与参考视图进行比较，可能无法完全捕捉神经合成场景（NSS）的感知质量，特别是由于密集参考视图的有限性。此外，获取人类感知标签的挑战阻碍了大规模标注数据集的创建，存在模型过拟合和泛化能力下降的风险。为了解决这些问题，我们提出了NVS-SQA，一种NSS质量评估方法，通过自监督学习无参考质量表征，无需依赖人类标签。传统的自监督学习主要依赖于“相同实例，相似表征”的假设和大量数据集。然而，考虑到这些条件不适用于NSS质量评估，我们采用启发式线索和质量分数作为学习目标，并结合专门的对比对准备过程来提高学习的有效性和效率。结果表明，NVS-SQA在17种无参考方法中表现出色（平均在SRCC上提升109.5%，PLCC上提升98.6%，KRCC上提升91.5%，超越第二名），甚至在所有评估指标上都超越了16种全参考方法（平均在SRCC上提升22.9%，PLCC上提升19.1%，KRCC上提升18.6%，超越第二名）。", "summary": "本文提出NVS-SQA，一种针对神经合成场景的无参考质量评估方法。针对现有全参考方法在感知质量评估上的局限性以及人工标注数据获取的困难，NVS-SQA利用自监督学习，通过启发式线索和质量分数作为学习目标，并设计了独特的对比对准备过程，从而无需依赖人类标签。实验结果表明，NVS-SQA在多项评估指标上显著优于现有的无参考方法，甚至超越了全参考方法，展示了其在神经合成场景质量评估中的卓越性能。", "keywords": "神经视图合成, 质量评估, 自监督学习, 无参考, 神经合成场景", "comments": "NVS-SQA的创新之处在于其采用自监督学习范式来解决神经合成场景的无参考质量评估问题，成功规避了对昂贵的人工感知标签和密集参考视图的依赖。这对于推动神经视图合成技术的实际应用具有重要意义，因为它提供了一种更通用、更易于部署的质量评估工具。其在性能上的显著提升，尤其是在超越全参考方法方面，证明了其方法的有效性和潜力。"}}
{"id": "2507.17678", "title": "MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI", "authors": ["Jiahui Yin", "Xinxing Cheng", "Jinming Duan", "Yan Pang", "Declan O'Regan", "Hadrien Reynaud", "Qingjie Meng"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Medical Image Computing and Computer-Assisted Intervention (MICCAI), Reconstruction and Imaging Motion Estimation Workshop (RIME), 2025", "url": "http://arxiv.org/abs/2507.17678v1", "summary": "Myocardial motion tracking is important for assessing cardiac function and\ndiagnosing cardiovascular diseases, for which cine cardiac magnetic resonance\n(CMR) has been established as the gold standard imaging modality. Many existing\nmethods learn motion from single image pairs consisting of a reference frame\nand a randomly selected target frame from the cardiac cycle. However, these\nmethods overlook the continuous nature of cardiac motion and often yield\ninconsistent and non-smooth motion estimations. In this work, we propose a\nnovel Mamba-based cardiac motion tracking network (MCM) that explicitly\nincorporates target image sequence from the cardiac cycle to achieve smooth and\ntemporally consistent motion tracking. By developing a bi-directional Mamba\nblock equipped with a bi-directional scanning mechanism, our method facilitates\nthe estimation of plausible deformation fields. With our proposed motion\ndecoder that integrates motion information from frames adjacent to the target\nframe, our method further enhances temporal coherence. Moreover, by taking\nadvantage of Mamba's structured state-space formulation, the proposed method\nlearns the continuous dynamics of the myocardium from sequential images without\nincreasing computational complexity. We evaluate the proposed method on two\npublic datasets. The experimental results demonstrate that the proposed method\nquantitatively and qualitatively outperforms both conventional and\nstate-of-the-art learning-based cardiac motion tracking methods. The code is\navailable at https://github.com/yjh-0104/MCM.", "comment": "Medical Image Computing and Computer-Assisted Intervention (MICCAI),\n  Reconstruction and Imaging Motion Estimation Workshop (RIME), 2025", "pdf_url": "http://arxiv.org/pdf/2507.17678v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MCM：基于Mamba的MRI序列图像心脏运动追踪", "tldr": "本文提出了MCM，一个基于Mamba的网络，用于利用MRI序列图像进行平滑且一致的心脏运动追踪，其性能优于现有方法。", "motivation": "现有心肌运动追踪方法通常只从单个图像对中学习运动，忽略了心脏运动的连续性，导致运动估计不一致且不平滑。", "method": "本文提出了一种新颖的基于Mamba的心脏运动追踪网络（MCM），通过明确整合心脏周期中的目标图像序列来实现平滑且时间一致的运动追踪。该方法开发了一个配备双向扫描机制的双向Mamba块，以估计合理的形变场；并提出一个运动解码器，整合来自目标帧相邻帧的运动信息以增强时间连贯性。此外，它利用Mamba的结构化状态空间公式，从序列图像中学习心肌的连续动态，而不增加计算复杂性。", "result": "在两个公共数据集上的实验结果表明，所提出的方法在定量和定性上都优于传统和最先进的基于学习的心脏运动追踪方法。", "conclusion": "所提出的MCM方法通过利用序列图像和Mamba架构，有效解决了现有心肌运动追踪方法的局限性，从而实现了更优越的平滑且时间一致的运动估计。", "translation": "心肌运动追踪对于评估心脏功能和诊断心血管疾病至关重要，其中电影心脏磁共振（CMR）已被确立为金标准成像模态。许多现有方法从由一个参考帧和心脏周期中随机选择的目标帧组成的单个图像对中学习运动。然而，这些方法忽视了心脏运动的连续性，并且通常产生不一致和不平滑的运动估计。在这项工作中，我们提出了一种新颖的基于Mamba的心脏运动追踪网络（MCM），它明确地整合了心脏周期中的目标图像序列，以实现平滑且时间一致的运动追踪。通过开发配备双向扫描机制的双向Mamba块，我们的方法有助于估计合理的形变场。通过我们提出的运动解码器，该解码器整合了来自目标帧相邻帧的运动信息，我们的方法进一步增强了时间连贯性。此外，通过利用Mamba的结构化状态空间公式，所提出的方法从序列图像中学习心肌的连续动态，而无需增加计算复杂性。我们在两个公共数据集上评估了所提出的方法。实验结果表明，所提出的方法在定量和定性上都优于传统和最先进的基于学习的心脏运动追踪方法。代码可在https://github.com/yjh-0104/MCM 获取。", "summary": "本文介绍了一种新颖的基于Mamba的心脏运动追踪网络MCM，用于利用MRI序列图像进行心肌运动追踪。与现有方法常因仅使用单个图像对而产生不一致结果不同，MCM明确整合了目标图像序列，以实现平滑且时间一致的运动追踪。它利用双向Mamba块和运动解码器来增强时间连贯性，并在不增加计算成本的情况下学习连续的心肌动态。在两个公共数据集上的评估表明，MCM在定量和定性上均优于现有方法。", "keywords": "心脏运动追踪, Mamba, 序列图像, MRI, 深度学习", "comments": "该论文的创新之处在于利用Mamba架构，特别是其结构化状态空间公式，有效地从序列图像中建模心脏运动的连续动态。这解决了以前方法（忽略时间连续性）的一个关键局限性，并在不增加计算复杂性的情况下实现了改进的平滑性和一致性，是心脏运动追踪领域的一个重要进展。"}}
{"id": "2407.19707", "title": "Neural networks for bifurcation and linear stability analysis of steady states in partial differential equations", "authors": ["Muhammad Luthfi Shahab", "Hadi Susanto"], "categories": ["math.NA", "cs.LG", "cs.NA", "cs.NE", "math.OC", "nlin.PS"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Applied Mathematics and Computation", "url": "http://arxiv.org/abs/2407.19707v4", "summary": "This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.", "comment": "Accepted for publication in Applied Mathematics and Computation", "pdf_url": "http://arxiv.org/pdf/2407.19707v4", "cate": "math.NA", "date": "2024-07-29", "updated": "2025-07-20", "AI": {"title_translation": "用于偏微分方程稳态分岔和线性稳定性分析的神经网络", "tldr": "本研究提出了一种结合伪弧长延续的神经网络方法，用于构建参数化非线性偏微分方程的分岔图，并利用神经网络解决特征值问题以分析解的线性稳定性。实验结果表明，该方法在求解精度、分岔图生成和线性稳定性分析方面优于有限差分法，且计算时间合理。", "motivation": "本研究旨在扩展神经网络在解决非线性偏微分方程方面的应用，特别是在构建分岔图和分析解的线性稳定性方面。", "method": "本研究提出了一种结合伪弧长延续的神经网络方法来构建参数化非线性偏微分方程的分岔图。此外，还提出了一种神经网络方法来解决特征值问题，以分析解的线性稳定性，重点是识别最大特征值。通过Bratu方程和Burgers方程进行实验，并与有限差分法进行比较，通过改变网格点数量来评估两种方法的行为和准确性。", "result": "实验结果表明，所提出的神经网络产生了更好的解，生成了更准确的分岔图，计算时间合理，并且被证明对线性稳定性分析有效。", "conclusion": "本研究提出的神经网络方法在构建非线性偏微分方程的分岔图和分析其稳态的线性稳定性方面表现出优越的性能，并且在精度和计算效率上优于传统的有限差分法。", "translation": "本研究介绍了神经网络在求解非线性偏微分方程（PDEs）方面的扩展应用。提出了一种结合伪弧长延续的神经网络，用于从参数化非线性偏微分方程构建分岔图。此外，还提出了一种神经网络方法来解决特征值问题，以分析解的线性稳定性，重点是识别最大特征值。通过Bratu方程和Burgers方程的实验，检验了所提出的神经网络的有效性。同时，也给出了有限差分法的结果作为比较。在每种情况下都采用了不同数量的网格点来评估神经网络和有限差分法的行为和准确性。实验结果表明，所提出的神经网络产生了更好的解，生成了更准确的分岔图，具有合理的计算时间，并证明对线性稳定性分析有效。", "summary": "本研究提出了一种创新的神经网络方法，用于解决非线性偏微分方程的稳态分岔和线性稳定性分析问题。该方法结合了伪弧长延续技术来构建精确的分岔图，并利用神经网络解决特征值问题以评估解的线性稳定性。通过对Bratu和Burgers方程的实验验证，结果显示所提出的神经网络在解决方案精度、分岔图生成和计算效率方面均优于传统的有限差分法。", "keywords": "神经网络, 偏微分方程, 分岔分析, 线性稳定性, 特征值问题", "comments": "这项研究的创新之处在于将神经网络与伪弧长延续方法相结合，用于自动化和改进非线性偏微分方程的分岔图构建，以及利用神经网络进行线性稳定性分析。这为复杂PDEs的数值求解和分析提供了一种高效且精确的新工具，有望在工程和科学计算领域带来显著进展。"}}
{"id": "2504.05172", "title": "Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes", "authors": ["Guangqiang Li", "M. Amine Atoui", "Xiangshun Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages,11 figures", "url": "http://arxiv.org/abs/2504.05172v3", "summary": "Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multiscale temporal fusion network. The multiscale\ndepthwise convolution and gated recurrent unit are employed to extract\nmultiscale contextual local features and long-short-term features. Instance\nnormalization is applied to suppress mode-specific information. Furthermore, a\ntemporal attention mechanism is designed to focus on critical time points with\nhigher cross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size. The source code will be available on GitHub at\nhttps://github.com/GuangqiangLi/AMTFNet.", "comment": "32 pages,11 figures", "pdf_url": "http://arxiv.org/pdf/2504.05172v3", "cate": "cs.LG", "date": "2025-04-07", "updated": "2025-07-23", "AI": {"title_translation": "基于注意力的多尺度时间融合网络用于多模式过程中的不确定模式故障诊断", "tldr": "本文提出了一种基于注意力的多尺度时间融合网络（AMTFNet），用于解决多模式过程中由于数据分布差异导致的故障诊断难题，并在实验中展现出卓越的诊断性能和较小的模型尺寸。", "motivation": "多模式过程中的故障诊断对于工业系统的安全运行至关重要，但面临巨大挑战：多模式监测数据之间显著的分布差异使得模型难以提取与系统健康状况相关的共享特征表示。", "method": "本文提出了一种名为注意力多尺度时间融合网络（attention-based multiscale temporal fusion network）的新方法。该方法采用多尺度深度卷积和门控循环单元来提取多尺度上下文局部特征和长短期特征。应用实例归一化来抑制模式特定信息。此外，设计了一种时间注意力机制，以关注具有更高跨模式共享信息的关键时间点，从而提高故障诊断的准确性。", "result": "该模型应用于Tennessee Eastman过程数据集和三相流设施数据集。实验表明，所提出的模型实现了卓越的诊断性能并保持了较小的模型尺寸。", "conclusion": "该研究提出的基于注意力的多尺度时间融合网络能够有效解决多模式过程中由于数据分布差异带来的故障诊断难题，并展现出优越的诊断性能和模型效率。", "translation": "多模式过程中的故障诊断在确保工业系统跨多种模式安全运行方面发挥着关键作用。它面临一个尚未解决的巨大挑战——即，来自多个模式的监测数据之间显著的分布差异使得模型难以提取与系统健康状况相关的共享特征表示。为了响应这个问题，本文引入了一种新颖的方法，称为基于注意力的多尺度时间融合网络。采用多尺度深度卷积和门控循环单元来提取多尺度上下文局部特征和长短期特征。应用实例归一化来抑制模式特定信息。此外，设计了一种时间注意力机制，以关注具有更高跨模式共享信息的关键时间点，从而提高故障诊断的准确性。所提出的模型应用于Tennessee Eastman过程数据集和三相流设施数据集。实验表明，所提出的模型实现了卓越的诊断性能并保持了较小的模型尺寸。源代码将在GitHub上提供：https://github.com/GuangqiangLi/AMTFNet。", "summary": "本研究针对多模式过程中故障诊断面临的数据分布差异挑战，提出了一种新颖的基于注意力的多尺度时间融合网络（AMTFNet）。该网络结合多尺度深度卷积和门控循环单元提取多尺度特征，并通过实例归一化抑制模式特有信息。此外，引入时间注意力机制聚焦于关键时间点以增强跨模式共享信息。在Tennessee Eastman和三相流数据集上的实验验证了AMTFNet在故障诊断方面的优越性能和高效性。", "keywords": "故障诊断, 多模式过程, 注意力机制, 多尺度融合, 实例归一化", "comments": "该论文提出的AMTFNet通过结合多尺度特征提取、实例归一化和时间注意力机制，有效地解决了多模式过程故障诊断中数据分布差异的难题，具有较强的创新性。其在保证诊断性能的同时维持小模型尺寸，对于实际工业应用具有重要意义。"}}
{"id": "2507.07901", "title": "The Trust Fabric: Decentralized Interoperability and Economic Coordination for the Agentic Web", "authors": ["Sree Bhargavi Balija", "Rekha Singal", "Ramesh Raskar", "Erfan Darzi", "Raghu Bala", "Thomas Hardjono", "Ken Huang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07901v3", "summary": "The fragmentation of AI agent ecosystems has created urgent demands for\ninteroperability, trust, and economic coordination that current protocols --\nincluding MCP (Hou et al., 2025), A2A (Habler et al., 2025), ACP (Liu et al.,\n2025), and Cisco's AGP (Edwards, 2025) -- cannot address at scale. We present\nthe Nanda Unified Architecture, a decentralized framework built around three\ncore innovations: fast DID-based agent discovery through distributed\nregistries, semantic agent cards with verifiable credentials and composability\nprofiles, and a dynamic trust layer that integrates behavioral attestations\nwith policy compliance. The system introduces X42/H42 micropayments for\neconomic coordination and MAESTRO, a security framework incorporating\nSynergetics' patented AgentTalk protocol (US Patent 12,244,584 B1) and secure\ncontainerization. Real-world deployments demonstrate 99.9 percent compliance in\nhealthcare applications and substantial monthly transaction volumes with strong\nprivacy guarantees. By unifying MIT's trust research with production\ndeployments from Cisco and Synergetics, we show how cryptographic proofs and\npolicy-as-code transform agents into trust-anchored participants in a\ndecentralized economy (Lakshmanan, 2025; Sha, 2025). The result enables a\nglobally interoperable Internet of Agents where trust becomes the native\ncurrency of collaboration across both enterprise and Web3 ecosystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07901v3", "cate": "cs.CR", "date": "2025-07-10", "updated": "2025-07-22", "AI": {"title_translation": "信任织物：代理式网络的去中心化互操作性和经济协调", "tldr": "当前的AI代理协议在大规模互操作性、信任和经济协调方面存在不足。本文提出了Nanda统一架构，这是一个去中心化框架，通过快速DID发现、语义代理卡和动态信任层，实现AI代理的安全经济协调和互操作性。", "motivation": "AI代理生态系统的碎片化导致了对互操作性、信任和经济协调的迫切需求，而现有协议无法大规模满足这些需求。", "method": "本文提出了Nanda统一架构，这是一个去中心化框架，具有三项核心创新：通过分布式注册中心实现基于DID的快速代理发现；具有可验证凭证和可组合性配置的语义代理卡；以及整合行为证明与策略合规性的动态信任层。该系统还引入了X42/H42小额支付用于经济协调，以及MAESTRO安全框架，该框架整合了Synergetics的专利AgentTalk协议和安全容器化技术。", "result": "实际部署表明，在医疗保健应用中实现了99.9%的合规性，并具有可观的月交易量和强大的隐私保障。该系统实现了一个全球互操作的代理互联网，其中信任成为协作的固有货币。", "conclusion": "通过将信任研究与生产部署相结合，该系统将代理转变为去中心化经济中以信任为锚的参与者，从而实现一个全球互操作的代理互联网，其中信任是企业和Web3生态系统之间协作的固有货币。", "translation": "AI代理生态系统的碎片化对互操作性、信任和经济协调产生了迫切需求，而包括MCP (Hou et al., 2025)、A2A (Habler et al., 2025)、ACP (Liu et al., 2025) 和思科的AGP (Edwards, 2025) 在内的现有协议都无法大规模解决这些问题。我们提出了Nanda统一架构，这是一个围绕三项核心创新构建的去中心化框架：通过分布式注册中心实现基于DID的快速代理发现、具有可验证凭证和可组合性配置的语义代理卡，以及整合行为证明与策略合规性的动态信任层。该系统引入了X42/H42小额支付以实现经济协调，以及MAESTRO安全框架，该框架整合了Synergetics获得专利的AgentTalk协议（美国专利12,244,584 B1）和安全容器化技术。实际部署表明，在医疗保健应用中实现了99.9%的合规性，并具有可观的月交易量和强大的隐私保障。通过将麻省理工学院的信任研究与思科和Synergetics的生产部署相结合，我们展示了密码学证明和策略即代码如何将代理转变为去中心化经济中以信任为锚的参与者（Lakshmanan, 2025; Sha, 2025）。其结果是实现了一个全球互操作的代理互联网，其中信任成为企业和Web3生态系统之间协作的固有货币。", "summary": "本文介绍了Nanda统一架构，一个旨在解决碎片化AI代理生态系统中可扩展性、互操作性和信任问题的去中心化框架。它具有基于DID的发现、带可验证凭证的语义代理卡和动态信任层。该系统集成了用于经济协调的小额支付和包含专利协议的安全框架（MAESTRO）。在医疗保健领域的实际部署显示出高合规性和交易量，表明其如何实现一个全球互操作的“代理互联网”，其中信任促进协作。", "keywords": "AI代理, 去中心化互操作性, 信任织物, Nanda统一架构, 经济协调", "comments": "这篇论文解决了快速发展的AI代理领域中的一个关键挑战：在碎片化生态系统中建立信任和互操作性。Nanda统一架构通过结合去中心化标识符（DID）、可验证凭证和动态信任层，显得颇具创新性。通过小额支付实现的经济协调以及集成专利协议和容器化等强大安全功能也值得关注。报告中医疗保健领域99.9%的合规性表明其强大的实际适用性和可靠性，标志着向真正去中心化和可信的“代理互联网”迈出了重要一步。"}}
{"id": "2507.17157", "title": "UNICE: Training A Universal Image Contrast Enhancer", "authors": ["Ruodai Cui", "Lei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17157v1", "summary": "Existing image contrast enhancement methods are typically designed for\nspecific tasks such as under-/over-exposure correction, low-light and backlit\nimage enhancement, etc. The learned models, however, exhibit poor\ngeneralization performance across different tasks, even across different\ndatasets of a specific task. It is important to explore whether we can learn a\nuniversal and generalized model for various contrast enhancement tasks. In this\nwork, we observe that the common key factor of these tasks lies in the need of\nexposure and contrast adjustment, which can be well-addressed if high-dynamic\nrange (HDR) inputs are available. We hence collect 46,928 HDR raw images from\npublic sources, and render 328,496 sRGB images to build multi-exposure\nsequences (MES) and the corresponding pseudo sRGB ground-truths via\nmulti-exposure fusion. Consequently, we train a network to generate an MES from\na single sRGB image, followed by training another network to fuse the generated\nMES into an enhanced image. Our proposed method, namely UNiversal Image\nContrast Enhancer (UNICE), is free of costly human labeling. However, it\ndemonstrates significantly stronger generalization performance than existing\nimage contrast enhancement methods across and within different tasks, even\noutperforming manually created ground-truths in multiple no-reference image\nquality metrics. The dataset, code and model are available at\nhttps://github.com/BeyondHeaven/UNICE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17157v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "UNICE：训练一个通用图像对比度增强器", "tldr": "UNICE是一个通用的图像对比度增强模型，通过生成多曝光序列和融合技术，无需人工标注即可实现更好的泛化性能。", "motivation": "现有的图像对比度增强方法通常针对特定任务设计，导致泛化性能差，难以跨任务或跨数据集应用。因此，需要探索一个能处理各种对比度增强任务的通用且泛化性强的模型。", "method": "作者观察到图像对比度增强任务的关键在于曝光和对比度调整，这可以通过高动态范围（HDR）输入解决。他们收集了46,928张HDR原始图像，并渲染了328,496张sRGB图像以构建多曝光序列（MES）及其伪sRGB真值。随后，训练一个网络从单张sRGB图像生成MES，再训练另一个网络将生成的MES融合成增强图像。该方法无需昂贵的人工标注。", "result": "UNICE在不同任务之间和内部都表现出比现有方法显著更强的泛化性能，甚至在多个无参考图像质量指标上超越了人工创建的真值。", "conclusion": "UNICE成功地展示了无需人工标注即可学习一个通用且泛化能力强的图像对比度增强模型的可能性，并在多项任务中取得了优异表现。", "translation": "现有图像对比度增强方法通常针对特定任务设计，例如欠/过曝光校正、低光和逆光图像增强等。然而，这些学习到的模型在不同任务之间，甚至在特定任务的不同数据集之间，都表现出较差的泛化性能。探索是否能为各种对比度增强任务学习一个通用且泛化性强的模型至关重要。在这项工作中，我们观察到这些任务的共同关键因素在于对曝光和对比度调整的需求，如果能够获得高动态范围（HDR）输入，这个问题就能很好地解决。因此，我们从公共来源收集了46,928张HDR原始图像，并渲染了328,496张sRGB图像，通过多曝光融合构建多曝光序列（MES）及相应的伪sRGB真值。随后，我们训练一个网络从单张sRGB图像生成MES，然后训练另一个网络将生成的MES融合为增强图像。我们提出的方法，即通用图像对比度增强器（UNICE），无需昂贵的人工标注。然而，它在不同任务之间和内部都表现出比现有图像对比度增强方法显著更强的泛化性能，甚至在多个无参考图像质量指标上超越了人工创建的真值。数据集、代码和模型可在https://github.com/BeyondHeaven/UNICE获取。", "summary": "本文提出了UNICE（通用图像对比度增强器），旨在解决现有图像对比度增强方法泛化性差的问题。通过利用HDR图像构建多曝光序列（MES）及其伪真值，UNICE训练了两个网络：一个用于从单张sRGB图像生成MES，另一个用于将MES融合为增强图像。该方法无需人工标注，并在多项对比度增强任务中展现出卓越的泛化性能，甚至超越了人工真值。", "keywords": "图像对比度增强, 通用模型, 多曝光序列, HDR, 泛化性", "comments": "UNICE的创新之处在于其通用性设计和无需人工标注的训练范式。通过利用HDR数据生成伪真值和MES，该模型有效规避了传统方法对特定任务标注数据的依赖，显著提升了模型的泛化能力。其在多个无参考图像质量指标上超越人工真值的表现，突显了其在实际应用中的巨大潜力。"}}
{"id": "2507.16832", "title": "Does Language Matter for Early Detection of Parkinson's Disease from Speech?", "authors": ["Peter Plantinga", "Briac Cordelle", "Dominique Louër", "Mirco Ravanelli", "Denise Klein"], "categories": ["eess.AS", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Workshop on Machine Learning for Signal Processing (MLSP) 2025", "url": "http://arxiv.org/abs/2507.16832v1", "summary": "Using speech samples as a biomarker is a promising avenue for detecting and\nmonitoring the progression of Parkinson's disease (PD), but there is\nconsiderable disagreement in the literature about how best to collect and\nanalyze such data. Early research in detecting PD from speech used a sustained\nvowel phonation (SVP) task, while some recent research has explored recordings\nof more cognitively demanding tasks. To assess the role of language in PD\ndetection, we tested pretrained models with varying data types and pretraining\nobjectives and found that (1) text-only models match the performance of\nvocal-feature models, (2) multilingual Whisper outperforms self-supervised\nmodels whereas monolingual Whisper does worse, and (3) AudioSet pretraining\nimproves performance on SVP but not spontaneous speech. These findings together\nhighlight the critical role of language for the early detection of Parkinson's\ndisease.", "comment": "Accepted to IEEE Workshop on Machine Learning for Signal Processing\n  (MLSP) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16832v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14", "AI": {"title_translation": "语言对早期语音帕金森病检测重要吗？", "tldr": "本文研究了语言在通过语音早期检测帕金森病中的作用，发现文本模型表现与声学特征模型相当，多语言Whisper表现优于自监督模型，且语言在早期帕金森病检测中至关重要。", "motivation": "利用语音样本作为生物标志物检测和监测帕金森病（PD）进展是一个有前景的方向，但关于如何最好地收集和分析此类数据存在显著分歧。早期研究使用持续元音发音任务，而近期研究探索了认知要求更高的任务。为了评估语言在PD检测中的作用，本研究进行了探究。", "method": "我们测试了具有不同数据类型和预训练目标的预训练模型。", "result": "1) 纯文本模型与声学特征模型的性能相当；2) 多语言Whisper的表现优于自监督模型，而单语言Whisper表现较差；3) AudioSet预训练提高了持续元音发音任务的性能，但对自发语音没有提升。", "conclusion": "这些发现共同强调了语言对于早期帕金森病检测的关键作用。", "translation": "利用语音样本作为生物标志物是检测和监测帕金森病（PD）进展的一个有前景的途径，但关于如何最好地收集和分析此类数据，文献中存在相当大的分歧。早期通过语音检测PD的研究使用了持续元音发音（SVP）任务，而一些近期研究则探索了认知要求更高的录音任务。为了评估语言在PD检测中的作用，我们测试了具有不同数据类型和预训练目标的预训练模型，发现：(1) 纯文本模型与声学特征模型的性能相当；(2) 多语言Whisper的表现优于自监督模型，而单语言Whisper表现较差；(3) AudioSet预训练提高了SVP的性能，但对自发语音没有提升。这些发现共同强调了语言对于早期帕金森病检测的关键作用。", "summary": "本研究旨在评估语言在通过语音早期检测帕金森病中的作用。通过测试不同数据类型和预训练目标的预训练模型，研究发现纯文本模型与声学特征模型性能相当，多语言Whisper表现优于自监督模型，且AudioSet预训练对持续元音发音任务有效但对自发语音无效。研究结果强调了语言在早期帕金森病检测中的重要性。", "keywords": "帕金森病, 语音检测, 语言, 预训练模型, 生物标志物", "comments": "本研究通过比较不同预训练模型和数据类型，深入探讨了语言在帕金森病语音检测中的作用，提供了关于模型选择和数据预处理的宝贵见解。其创新之处在于明确指出了多语言模型和文本信息的重要性，挑战了传统上侧重于纯声学特征的方法。这对于优化帕金森病早期诊断工具具有重要意义。"}}
{"id": "2503.01424", "title": "From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems", "authors": ["Zekun Zhou", "Xiaocheng Feng", "Lei Huang", "Xiachong Feng", "Ziyun Song", "Ruihan Chen", "Liang Zhao", "Weitao Ma", "Yuxuan Gu", "Baoxin Wang", "Dayong Wu", "Guoping Hu", "Ting Liu", "Bing Qin"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01424v2", "summary": "Research is a fundamental process driving the advancement of human\ncivilization, yet it demands substantial time and effort from researchers. In\nrecent years, the rapid development of artificial intelligence (AI)\ntechnologies has inspired researchers to explore how AI can accelerate and\nenhance research. To monitor relevant advancements, this paper presents a\nsystematic review of the progress in this domain. Specifically, we organize the\nrelevant studies into three main categories: hypothesis formulation, hypothesis\nvalidation, and manuscript publication. Hypothesis formulation involves\nknowledge synthesis and hypothesis generation. Hypothesis validation includes\nthe verification of scientific claims, theorem proving, and experiment\nvalidation. Manuscript publication encompasses manuscript writing and the peer\nreview process. Furthermore, we identify and discuss the current challenges\nfaced in these areas, as well as potential future directions for research.\nFinally, we also offer a comprehensive overview of existing benchmarks and\ntools across various domains that support the integration of AI into the\nresearch process. We hope this paper serves as an introduction for beginners\nand fosters future research. Resources have been made publicly available at\nhttps://github.com/zkzhou126/AI-for-Research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01424v2", "cate": "cs.AI", "date": "2025-03-03", "updated": "2025-07-23", "AI": {"title_translation": "从假设到发表：人工智能驱动的研究支持系统综合调查", "tldr": "本文全面调查了人工智能在研究过程中的应用，从假设提出到论文发表，并讨论了挑战、未来方向、基准和工具。", "motivation": "研究是一个耗时且费力的过程，而人工智能的快速发展为加速和增强研究提供了可能性。因此，需要对该领域的相关进展进行监测和系统回顾。", "method": "本文通过系统回顾，将人工智能在研究中的应用分为三个主要类别：假设制定（知识合成和假设生成）、假设验证（科学主张验证、定理证明和实验验证）和手稿发表（手稿撰写和同行评审）。此外，还识别并讨论了当前面临的挑战和潜在的未来研究方向，并提供了现有基准和工具的全面概述。", "result": "本文将人工智能辅助研究的进展系统地组织成三大类：假设制定、假设验证和手稿发表。论文还识别了当前面临的挑战、未来的研究方向，并概述了现有基准和工具。", "conclusion": "本文旨在为初学者提供入门指南，并促进未来的研究。", "translation": "研究是推动人类文明进步的基础过程，但它需要研究人员投入大量时间和精力。近年来，人工智能（AI）技术的快速发展激发了研究人员探索AI如何加速和增强研究。为了监测相关进展，本文对该领域的进展进行了系统回顾。具体来说，我们将相关研究分为三个主要类别：假设制定、假设验证和手稿发表。假设制定涉及知识综合和假设生成。假设验证包括科学主张的验证、定理证明和实验验证。手稿发表包括手稿撰写和同行评审过程。此外，我们还识别并讨论了这些领域当前面临的挑战以及潜在的未来研究方向。最后，我们还全面概述了支持AI融入研究过程的各个领域的现有基准和工具。我们希望本文能为初学者提供入门指南，并促进未来的研究。相关资源已在 https://github.com/zkzhou126/AI-for-Research 公开。", "summary": "本文对人工智能在研究支持系统中的应用进行了全面的系统回顾。作者将AI在研究中的作用划分为假设制定、假设验证和手稿发表三个阶段，详细阐述了每个阶段的具体应用。此外，论文还分析了当前面临的挑战、未来的发展方向，并总结了现有基准和工具，旨在为研究人员提供一个全面的概览并促进该领域的进一步发展。", "keywords": "人工智能, 研究支持系统, 系统综述, 假设制定, 手稿发表", "comments": "这篇综述性文章系统地梳理了AI在科研全流程中的应用，从宏观角度为该领域的研究者提供了清晰的框架和方向。其创新之处在于将AI辅助研究细化为假设形成、验证和发表三个关键阶段，并识别了各阶段的挑战和机遇。这对于初学者快速了解该领域，以及资深研究者寻找潜在的研究突破口都具有重要价值。"}}
{"id": "2501.18965", "title": "The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training", "authors": ["Fabian Schaipp", "Alexander Hägele", "Adrien Taylor", "Umut Simsekli", "Francis Bach"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.18965v2", "summary": "We show that learning-rate schedules for large model training behave\nsurprisingly similar to a performance bound from non-smooth convex optimization\ntheory. We provide a bound for the constant schedule with linear cooldown; in\nparticular, the practical benefit of cooldown is reflected in the bound due to\nthe absence of logarithmic terms. Further, we show that this surprisingly close\nmatch between optimization theory and practice can be exploited for\nlearning-rate tuning: we achieve noticeable improvements for training 124M and\n210M Llama-type models by (i) extending the schedule for continued training\nwith optimal learning-rate, and (ii) transferring the optimal learning-rate\nacross schedules.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.18965v2", "cate": "cs.LG", "date": "2025-01-31", "updated": "2025-07-23", "AI": {"title_translation": "凸优化理论与大型模型训练学习率调度之间的惊人一致性", "tldr": "本文揭示了大型模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间存在惊人的一致性，并展示了如何利用这种一致性来改进学习率调优，从而在Llama模型训练中取得显著提升。", "motivation": "研究发现学习率调度与凸优化理论的性能界限之间存在惊人的一致性，因此希望利用这种一致性来改进学习率调优，以提升大型模型的训练效果。", "method": "研究提供了一个带有线性冷却的常数调度界限，并指出冷却的实际益处体现在界限中，因为没有对数项。通过(i)延长调度以进行持续训练并使用最优学习率，以及(ii)在不同调度之间传递最优学习率，来利用理论与实践之间的匹配进行学习率调优。", "result": "学习率调度与非光滑凸优化理论的性能界限表现出惊人的相似性。带有线性冷却的常数调度界限反映了冷却的实际益处，且没有对数项。通过利用这种匹配，在训练1.24亿和2.1亿参数的Llama类型模型时，实现了显著的改进。", "conclusion": "凸优化理论与大型模型训练中的学习率调度之间存在惊人的一致性，这种一致性可以被有效利用于学习率调优，从而显著提升大型模型的训练性能。", "translation": "我们展示了大型模型训练中的学习率调度与非光滑凸优化理论中的性能界限之间存在惊人的相似性。我们为带有线性冷却的常数调度提供了一个界限；特别是，由于没有对数项，冷却的实际益处体现在该界限中。此外，我们表明，优化理论与实践之间这种惊人的密切匹配可以被利用于学习率调优：通过(i)延长调度以使用最优学习率进行持续训练，以及(ii)在不同调度之间传递最优学习率，我们在训练1.24亿和2.1亿参数的Llama类型模型时取得了显著的改进。", "summary": "本文揭示了大型模型训练中学习率调度行为与非光滑凸优化理论性能界限的惊人一致性。研究提供了一个带有线性冷却的常数调度界限，证明了冷却的实际益处。作者进一步展示了如何利用这种理论与实践的匹配进行学习率调优，通过延长调度进行持续训练和跨调度传递最优学习率，成功提升了1.24亿和2.1亿参数Llama模型的训练效果。", "keywords": "学习率调度, 凸优化理论, 大型模型训练, Llama模型, 性能界限", "comments": "本文的创新点在于发现了大型模型训练中学习率调度与凸优化理论之间出人意料的紧密联系，并成功将其应用于实际的学习率调优中。这种理论与实践的结合为优化大型模型训练提供了新的视角和实用方法，特别是通过优化学习率调度策略，实现了模型性能的显著提升，具有重要的实践意义。"}}
{"id": "2507.17144", "title": "Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning", "authors": ["Kazuki Numazato", "Keiichiro Kan", "Masaki Kitagawa", "Yunong Li", "Johannes Kubel", "Moju Zhao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 14 figures", "url": "http://arxiv.org/abs/2507.17144v1", "summary": "Flapping-wing drones have attracted significant attention due to their\nbiomimetic flight. They are considered more human-friendly due to their\ncharacteristics such as low noise and flexible wings, making them suitable for\nhuman-drone interactions. However, few studies have explored the practical\ninteraction between humans and flapping-wing drones. On establishing a physical\ninteraction system with flapping-wing drones, we can acquire inspirations from\nfalconers who guide birds of prey to land on their arms. This interaction\ninterprets the human body as a dynamic landing platform, which can be utilized\nin various scenarios such as crowded or spatially constrained environments.\nThus, in this study, we propose a falconry-like interaction system in which a\nflapping-wing drone performs a palm landing motion on a human hand. To achieve\na safe approach toward humans, we design a trajectory planning method that\nconsiders both physical and psychological factors of the human safety such as\nthe drone's velocity and distance from the user. We use a commercial flapping\nplatform with our implemented motion planning and conduct experiments to\nevaluate the palm landing performance and safety. The results demonstrate that\nour approach enables safe and smooth hand landing interactions. To the best of\nour knowledge, it is the first time to achieve a contact-based interaction\nbetween flapping-wing drones and humans.", "comment": "8 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.17144v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于人类手势交互和距离感知飞行规划的仿鹰式扑翼无人机手掌降落", "tldr": "本研究提出了一种仿鹰式交互系统，使扑翼无人机能安全地降落在人手掌上，并首次实现了扑翼无人机与人类的接触式交互。", "motivation": "扑翼无人机因其仿生飞行和低噪音、柔性机翼等特性被认为对人类更友好，适合人机交互。然而，现有研究很少探索人类与扑翼无人机之间的实际物理交互。在拥挤或空间受限的环境中，将人体作为动态着陆平台进行物理交互的需求未被充分满足。", "method": "研究提出了一种仿鹰式交互系统，使扑翼无人机能在人手上执行手掌降落动作。为确保安全接近，设计了一种轨迹规划方法，该方法综合考虑了无人机速度和与用户距离等物理及心理安全因素。研究团队使用商业扑翼平台并实施了所设计的运动规划，通过实验评估了手掌降落的性能和安全性。", "result": "实验结果表明，该方法能够实现安全平稳的手部降落交互。据作者所知，这是首次实现扑翼无人机与人类之间的接触式交互。", "conclusion": "本研究成功地提出并验证了一种基于人类手势交互和距离感知飞行规划的仿鹰式扑翼无人机手掌降落系统，首次实现了扑翼无人机与人类之间的安全、平稳的接触式交互，为未来人机协作提供了新的可能性。", "translation": "扑翼无人机因其仿生飞行而受到广泛关注。由于其低噪音和柔性机翼等特性，它们被认为更具人性化，使其适用于人机交互。然而，很少有研究探索人类与扑翼无人机之间的实际交互。在建立与扑翼无人机的物理交互系统时，我们可以从引导猛禽降落在手臂上的驯鹰师那里获得灵感。这种交互将人体解释为动态着陆平台，可用于拥挤或空间受限等各种场景。因此，在本研究中，我们提出了一种仿鹰式交互系统，其中扑翼无人机在人手上执行手掌降落动作。为了实现对人类的安全接近，我们设计了一种轨迹规划方法，该方法考虑了人类安全的身体和心理因素，例如无人机的速度和与用户的距离。我们使用一个商业扑翼平台，并实施了我们的运动规划，进行实验以评估手掌降落的性能和安全性。结果表明，我们的方法能够实现安全平稳的手部降落交互。据我们所知，这是首次实现扑翼无人机与人类之间的接触式交互。", "summary": "本论文介绍了一种新颖的仿鹰式交互系统，使扑翼无人机能够安全地降落在人手掌上。鉴于缺乏实用的人类与扑翼无人机物理交互，尤其是在受限环境中，作者开发了一种距离感知轨迹规划方法，该方法同时考虑了物理和心理安全因素。使用商业扑翼平台进行的实验结果证实了该系统能够实现安全平稳的接触式人机交互，这标志着该领域的一个首次突破。", "keywords": "扑翼无人机, 人机交互, 手掌降落, 仿鹰式, 轨迹规划", "comments": "本文通过从驯鹰术中汲取灵感，提出了一种创新的人机交互方法，这非常独特。专注于扑翼无人机的接触式交互，并同时考虑物理和心理安全因素，是一项重要贡献。声称是“首次实现扑翼无人机与人类之间的接触式交互”凸显了其新颖性和重要性，它以更直观和整合的方式推动了人机协作，特别是对于仿生飞行平台。"}}
{"id": "2507.17684", "title": "Generalized Dual Discriminator GANs", "authors": ["Penukonda Naga Chandana", "Tejas Srivastava", "Gowtham R. Kurri", "V. Lalitha"], "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, extended version of a paper accepted for presentation at ITW 2025", "url": "http://arxiv.org/abs/2507.17684v1", "summary": "Dual discriminator generative adversarial networks (D2 GANs) were introduced\nto mitigate the problem of mode collapse in generative adversarial networks. In\nD2 GANs, two discriminators are employed alongside a generator: one\ndiscriminator rewards high scores for samples from the true data distribution,\nwhile the other favors samples from the generator. In this work, we first\nintroduce dual discriminator $\\alpha$-GANs (D2 $\\alpha$-GANs), which combines\nthe strengths of dual discriminators with the flexibility of a tunable loss\nfunction, $\\alpha$-loss. We further generalize this approach to arbitrary\nfunctions defined on positive reals, leading to a broader class of models we\nrefer to as generalized dual discriminator generative adversarial networks. For\neach of these proposed models, we provide theoretical analysis and show that\nthe associated min-max optimization reduces to the minimization of a linear\ncombination of an $f$-divergence and a reverse $f$-divergence. This generalizes\nthe known simplification for D2-GANs, where the objective reduces to a linear\ncombination of the KL-divergence and the reverse KL-divergence. Finally, we\nperform experiments on 2D synthetic data and use multiple performance metrics\nto capture various advantages of our GANs.", "comment": "8 pages, 2 figures, extended version of a paper accepted for\n  presentation at ITW 2025", "pdf_url": "http://arxiv.org/pdf/2507.17684v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "广义双判别器GANs", "tldr": "本文引入了双判别器$\\alpha$-GANs，并将其推广为广义双判别器GANs，理论分析表明其优化目标可简化为f-散度和反向f-散度的线性组合，并在实验中展示了优势。", "motivation": "现有的生成对抗网络（GANs）存在模式崩溃问题。双判别器GANs (D2 GANs) 被引入以缓解此问题。本文旨在结合双判别器的优势与可调损失函数的灵活性，并进一步推广D2 GANs，以拓宽模型类别。", "method": "首先提出了双判别器$\\alpha$-GANs (D2 $\\alpha$-GANs)，它结合了双判别器的优点和可调$\\alpha$-损失函数的灵活性。然后，将该方法推广到定义在正实数上的任意函数，从而形成更广泛的广义双判别器生成对抗网络模型。对这些模型进行了理论分析，并进行了2D合成数据实验，使用多种性能指标评估模型优势。", "result": "理论分析表明，所提出的模型的最小-最大优化问题可简化为f-散度和反向f-散度线性组合的最小化。这推广了D2-GANs中目标函数简化为KL散度和反向KL散度线性组合的已知结论。实验结果表明，该GAN模型在2D合成数据上表现出多种优势。", "conclusion": "通过引入D2 $\\alpha$-GANs并将其推广为广义双判别器GANs，本文从理论上证明了其优化目标与f-散度之间的关系，并通过实验验证了其性能优势，为缓解GANs的模式崩溃问题提供了新的广义框架。", "translation": "双判别器生成对抗网络（D2 GANs）被引入以缓解生成对抗网络中的模式崩溃问题。在D2 GANs中，除了一个生成器之外，还使用了两个判别器：一个判别器对来自真实数据分布的样本给予高分奖励，而另一个判别器则偏爱来自生成器的样本。在这项工作中，我们首先引入了双判别器$\\alpha$-GANs（D2 $\\alpha$-GANs），它结合了双判别器的优势和可调损失函数$\\alpha$-损失的灵活性。我们进一步将这种方法推广到定义在正实数上的任意函数，从而形成了一类更广泛的模型，我们称之为广义双判别器生成对抗网络。对于这些提出的每一个模型，我们都提供了理论分析，并表明相关的最小-最大优化问题可以简化为f-散度和反向f-散度线性组合的最小化。这推广了D2-GANs的已知简化，其中目标函数简化为KL散度和反向KL散度的线性组合。最后，我们对2D合成数据进行了实验，并使用多种性能指标来捕捉我们GANs的各种优势。", "summary": "本文针对GANs的模式崩溃问题，提出了双判别器$\\alpha$-GANs (D2 $\\alpha$-GANs)，它结合了双判别器的优点和可调$\\alpha$-损失函数的灵活性。在此基础上，进一步推广为广义双判别器GANs。理论分析表明，这些模型的最小-最大优化目标可以简化为f-散度和反向f-散度的线性组合。实验在2D合成数据上验证了模型的优势。", "keywords": "生成对抗网络, 双判别器, 模式崩溃, f-散度, $\\alpha$-损失", "comments": "本文创新性地将双判别器GANs与可调$\\alpha$-损失结合，并进一步推广了其理论框架，证明了其与f-散度的普遍联系，为GANs的理论研究和实际应用提供了更广阔的视角和更灵活的选择。"}}
{"id": "2505.20424", "title": "Robot Operation of Home Appliances by Reading User Manuals", "authors": ["Jian Zhang", "Hanbo Zhang", "Anxing Xiao", "David Hsu"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20424v2", "summary": "Operating home appliances, among the most common tools in every household, is\na critical capability for assistive home robots. This paper presents ApBot, a\nrobot system that operates novel household appliances by \"reading\" their user\nmanuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual\ndocument, (ii) ground the policies to the appliance in the physical world, and\n(iii) execute the policies reliably over potentially many steps, despite\ncompounding errors. To tackle these challenges, ApBot constructs a structured,\nsymbolic model of an appliance from its manual, with the help of a large\nvision-language model (VLM). It grounds the symbolic actions visually to\ncontrol panel elements. Finally, ApBot closes the loop by updating the model\nbased on visual feedback. Our experiments show that across a wide range of\nsimulated and real-world appliances, ApBot achieves consistent and\nstatistically significant improvements in task success rate, compared with\nstate-of-the-art large VLMs used directly as control policies. These results\nsuggest that a structured internal representations plays an important role in\nrobust robot operation of home appliances, especially, complex ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20424v2", "cate": "cs.RO", "date": "2025-05-26", "updated": "2025-07-23", "AI": {"title_translation": "机器人通过阅读用户手册操作家用电器", "tldr": "ApBot是一个机器人系统，它通过“阅读”用户手册来操作家用电器，并利用视觉语言模型构建结构化模型，从而在操作复杂家电方面显著优于直接使用大型视觉语言模型的现有方法。", "motivation": "操作家用电器是辅助家庭机器人的关键能力。然而，机器人操作新颖的家用电器面临从非结构化手册中推断策略、将策略与物理世界中的电器对应，以及在存在累积误差的情况下可靠执行策略的挑战。", "method": "本文提出了ApBot系统，它通过“阅读”用户手册来操作新颖的家用电器。ApBot利用大型视觉语言模型（VLM）从手册中构建电器的结构化符号模型，将符号动作视觉地映射到控制面板元素，并通过视觉反馈更新模型以闭合循环。", "result": "实验表明，在各种模拟和真实世界的电器中，ApBot在任务成功率方面取得了持续且统计学上显著的改进，优于直接用作控制策略的现有大型视觉语言模型。", "conclusion": "这些结果表明，结构化的内部表示在机器人稳健操作家用电器，特别是复杂电器方面，发挥着重要作用。", "translation": "操作家用电器是每个家庭中最常见的工具之一，是辅助家庭机器人的关键能力。本文介绍了ApBot，一个通过“阅读”用户手册来操作新型家用电器的机器人系统。ApBot面临多重挑战：(i) 从用户手册中非结构化的文本描述中推断出目标导向的部分策略，(ii) 将策略与物理世界中的电器进行物理对应，以及 (iii) 尽管存在复合误差，仍能可靠地执行可能包含许多步骤的策略。为了应对这些挑战，ApBot借助大型视觉语言模型（VLM）从其手册中构建电器的结构化、符号模型。它将符号动作视觉地映射到控制面板元素。最后，ApBot通过基于视觉反馈更新模型来闭合循环。我们的实验表明，在各种模拟和真实世界的电器中，与直接用作控制策略的现有大型视觉语言模型相比，ApBot在任务成功率方面取得了持续且统计学上显著的改进。这些结果表明，结构化的内部表示在机器人稳健操作家用电器，特别是复杂电器方面，发挥着重要作用。", "summary": "ApBot是一个创新的机器人系统，旨在通过阅读用户手册来操作新型家用电器。该系统通过大型视觉语言模型从非结构化手册中构建电器的结构化符号模型，并将其视觉地映射到实际电器上，同时利用视觉反馈进行模型更新。实验结果表明，ApBot在任务成功率上显著优于直接使用大型视觉语言模型的现有方法，强调了结构化内部表示在机器人稳健操作复杂家用电器中的重要性。", "keywords": "机器人操作, 家用电器, 用户手册, 视觉语言模型, 结构化表示", "comments": "本文提出了一种新颖的方法，使机器人能够通过理解非结构化的用户手册来操作家用电器，解决了现有方法在处理复杂电器时的局限性。其创新点在于结合了VLM来构建结构化符号模型，并通过视觉反馈闭合循环，显著提高了任务成功率。这项工作对于开发更自主、适应性更强的家庭辅助机器人具有重要意义。"}}
{"id": "2507.17193", "title": "Spintronic Bayesian Hardware Driven by Stochastic Magnetic Domain Wall Dynamics", "authors": ["Tianyi Wang", "Bingqian Dai", "Kin Wong", "Yaochen Li", "Yang Cheng", "Qingyuan Shu", "Haoran He", "Puyang Huang", "Hanshen Huang", "Kang L. Wang"], "categories": ["physics.app-ph", "cs.LG"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17193v1", "summary": "As artificial intelligence (AI) advances into diverse applications, ensuring\nreliability of AI models is increasingly critical. Conventional neural networks\noffer strong predictive capabilities but produce deterministic outputs without\ninherent uncertainty estimation, limiting their reliability in safety-critical\ndomains. Probabilistic neural networks (PNNs), which introduce randomness, have\nemerged as a powerful approach for enabling intrinsic uncertainty\nquantification. However, traditional CMOS architectures are inherently designed\nfor deterministic operation and actively suppress intrinsic randomness. This\nposes a fundamental challenge for implementing PNNs, as probabilistic\nprocessing introduces significant computational overhead. To address this\nchallenge, we introduce a Magnetic Probabilistic Computing (MPC) platform-an\nenergy-efficient, scalable hardware accelerator that leverages intrinsic\nmagnetic stochasticity for uncertainty-aware computing. This physics-driven\nstrategy utilizes spintronic systems based on magnetic domain walls (DWs) and\ntheir dynamics to establish a new paradigm of physical probabilistic computing\nfor AI. The MPC platform integrates three key mechanisms: thermally induced DW\nstochasticity, voltage controlled magnetic anisotropy (VCMA), and tunneling\nmagnetoresistance (TMR), enabling fully electrical and tunable probabilistic\nfunctionality at the device level. As a representative demonstration, we\nimplement a Bayesian Neural Network (BNN) inference structure and validate its\nfunctionality on CIFAR-10 classification tasks. Compared to standard 28nm CMOS\nimplementations, our approach achieves a seven orders of magnitude improvement\nin the overall figure of merit, with substantial gains in area efficiency,\nenergy consumption, and speed. These results underscore the MPC platform's\npotential to enable reliable and trustworthy physical AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17193v1", "cate": "physics.app-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "磁畴壁随机动力学驱动的自旋电子贝叶斯硬件", "tldr": "本文提出一种基于磁畴壁随机性的自旋电子平台（MPC），用于高效实现具有不确定性量化的概率神经网络，显著优于传统CMOS架构，在能效、面积和速度上均有大幅提升。", "motivation": "传统神经网络缺乏固有的不确定性估计，限制了其在安全关键领域的可靠性。概率神经网络（PNNs）虽能提供不确定性量化，但传统CMOS架构为确定性操作设计，难以高效实现PNNs，且引入显著的计算开销。", "method": "本文提出一个磁概率计算（MPC）平台，该平台是一个节能、可扩展的硬件加速器，利用磁畴壁（DWs）及其动力学固有的磁随机性，建立物理概率计算的新范式。MPC平台集成了热诱导DW随机性、电压控制磁各向异性（VCMA）和隧道磁电阻（TMR）三个关键机制，实现了器件级的全电学和可调谐概率功能。", "result": "作为代表性演示，作者实现了一个贝叶斯神经网络（BNN）推断结构，并在CIFAR-10分类任务上验证了其功能。与标准28nm CMOS实现相比，该方法在整体性能指标上实现了七个数量级的改进，并在面积效率、能耗和速度方面有显著提升。", "conclusion": "这些结果强调了MPC平台在实现可靠和可信的物理AI系统方面的巨大潜力。", "translation": "随着人工智能（AI）在各种应用中取得进展，确保AI模型的可靠性变得越来越重要。传统的神经网络提供强大的预测能力，但产生确定性输出，缺乏固有的不确定性估计，这限制了它们在安全关键领域的可靠性。引入随机性的概率神经网络（PNNs）已成为实现内在不确定性量化的强大方法。然而，传统的CMOS架构本质上是为确定性操作而设计的，并积极抑制内在随机性。这给PNN的实现带来了根本性挑战，因为概率处理会引入显著的计算开销。为了解决这一挑战，我们引入了一个磁概率计算（MPC）平台——一个节能、可扩展的硬件加速器，它利用固有的磁随机性进行不确定性感知计算。这种物理驱动的策略利用基于磁畴壁（DWs）及其动力学的自旋电子系统，建立了AI物理概率计算的新范式。MPC平台集成了三个关键机制：热诱导DW随机性、电压控制磁各向异性（VCMA）和隧道磁电阻（TMR），实现了器件级的全电学和可调谐概率功能。作为一个代表性演示，我们实现了一个贝叶斯神经网络（BNN）推理结构，并在CIFAR-10分类任务上验证了其功能。与标准28nm CMOS实现相比，我们的方法在整体性能指标上实现了七个数量级的改进，并在面积效率、能耗和速度方面取得了显著的提升。这些结果强调了MPC平台在实现可靠和可信的物理AI系统方面的潜力。", "summary": "本文提出了一种创新的磁概率计算（MPC）硬件平台，通过利用磁畴壁固有的随机动力学，旨在解决传统CMOS架构在实现具有不确定性量化的概率神经网络（PNNs）时面临的挑战。MPC平台集成了热诱导DW随机性、VCMA和TMR机制，实现了高效、节能且可调谐的物理概率计算。实验证明，该平台在CIFAR-10分类任务上实现贝叶斯神经网络时，相较于28nm CMOS，在综合性能指标上实现了七个数量级的提升，并在面积效率、能耗和速度方面表现出显著优势，展现了其在构建可靠物理AI系统方面的巨大潜力。", "keywords": "自旋电子学, 贝叶斯神经网络, 磁畴壁, 概率计算, 不确定性量化", "comments": "该论文提出了一种创新的硬件方法，利用物理固有的随机性（磁畴壁动力学）来解决AI中不确定性量化的问题，特别是针对概率神经网络的实现。其亮点在于将自旋电子技术与AI硬件结合，实现了前所未有的能效和性能提升（七个数量级），这对于推动AI在安全关键领域的应用具有重要意义。该方法克服了传统CMOS在处理随机性方面的固有局限性，为未来可靠AI系统提供了新途径。"}}
{"id": "2503.17032", "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting", "authors": ["Jianchuan Chen", "Jingchuan Hu", "Gaige Wang", "Zhonghua Jiang", "Tiansong Zhou", "Zhiwen Chen", "Chengfei Lv"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by CVPR 2025 (Highlight), project page: this https URL", "url": "http://arxiv.org/abs/2503.17032v2", "summary": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.", "comment": "Accepted by CVPR 2025 (Highlight), project page:\n  https://PixelAI-Team.github.io/TaoAvatar", "pdf_url": "http://arxiv.org/pdf/2503.17032v2", "cate": "cs.CV", "date": "2025-03-21", "updated": "2025-07-23", "AI": {"title_translation": "TaoAvatar：基于3D高斯泼溅的增强现实实时逼真全身说话虚拟形象", "tldr": "TaoAvatar 是一种基于 3DGS 的轻量级方法，能够实时生成高质量的全身说话虚拟形象，并在移动设备上高效运行。", "motivation": "尽管 3D 高斯泼溅 (3DGS) 在创建逼真虚拟形象方面取得了进展，但现有方法难以实现全身说话任务中面部表情和身体动作的精细控制。此外，它们通常缺乏足够的细节，并且无法在移动设备上实时运行。", "method": "TaoAvatar 首先创建了一个个性化的带衣物人体参数模板，将高斯绑定以表示外观。然后，预训练一个基于 StyleUnet 的网络来处理复杂的姿态相关非刚性变形。为了解决移动设备资源限制，通过蒸馏技术将非刚性变形“烘焙”到一个轻量级的基于 MLP 的网络中，并开发了混合形状来补偿细节。", "result": "TaoAvatar 实现了最先进的渲染质量，同时在各种设备上实时运行，在 Apple Vision Pro 等高清立体设备上保持 90 FPS。", "conclusion": "该研究成功开发了一种高保真、轻量级的 3DGS 全身说话虚拟形象系统 TaoAvatar，解决了现有方法在精细控制、细节和实时性方面的不足，实现了在多种设备上的高质量实时渲染。", "translation": "逼真的 3D 全身说话虚拟形象在增强现实 (AR) 中具有巨大潜力，应用范围从电子商务直播到全息通信。尽管 3D 高斯泼溅 (3DGS) 在创建逼真虚拟形象方面取得了进展，但现有方法难以实现全身说话任务中面部表情和身体动作的精细控制。此外，它们通常缺乏足够的细节，并且无法在移动设备上实时运行。我们提出了 TaoAvatar，一个高保真、轻量级、基于 3DGS 的全身说话虚拟形象，由各种信号驱动。我们的方法首先创建一个个性化的带衣物人体参数模板，将高斯绑定以表示外观。然后，我们预训练一个基于 StyleUnet 的网络来处理复杂的姿态相关非刚性变形，该网络可以捕捉高频外观细节，但对于移动设备来说资源消耗过大。为了克服这个问题，我们使用蒸馏技术将非刚性变形“烘焙”到一个轻量级的基于 MLP 的网络中，并开发了混合形状来补偿细节。大量实验表明，TaoAvatar 实现了最先进的渲染质量，同时在各种设备上实时运行，在 Apple Vision Pro 等高清立体设备上保持 90 FPS。", "summary": "TaoAvatar 提出了一种基于 3D 高斯泼溅 (3DGS) 的创新方法，用于在增强现实中创建实时、逼真的全身说话虚拟形象。该方法通过结合个性化人体参数模板、预训练的 StyleUnet 网络进行非刚性变形，并利用蒸馏技术将复杂变形烘焙到轻量级 MLP 网络中，同时引入混合形状来保留细节。TaoAvatar 成功解决了现有 3DGS 方法在精细控制、细节表现和移动设备实时性方面的局限性，实现了最先进的渲染质量和在多种设备上的高效实时运行，例如在 Apple Vision Pro 上达到 90 FPS。", "keywords": "TaoAvatar, 3D 高斯泼溅, 全身说话虚拟形象, 增强现实, 实时", "comments": "该论文的创新点在于结合 3DGS 技术与蒸馏和混合形状方法，解决了全身说话虚拟形象在 AR 中面临的实时性、细节和精细控制的挑战。其重要性在于为 AR 应用提供了高质量、轻量级的解决方案，尤其是在移动设备上的高性能表现，具有广泛的应用前景。"}}
{"id": "2507.17512", "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning", "authors": ["Yu Li", "Zhuoshi Pan", "Honglin Lin", "Mengyuan Sun", "Conghui He", "Lijun Wu"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      27 pages, 24 figures", "url": "http://arxiv.org/abs/2507.17512v1", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.", "comment": "27 pages, 24 figures", "pdf_url": "http://arxiv.org/pdf/2507.17512v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一个领域能帮助其他领域吗？基于强化学习的多领域推理的数据中心研究", "tldr": "本研究系统地探讨了在可验证奖励强化学习（RLVR）框架下，大型语言模型（LLMs）在多领域推理中的表现，特别关注领域间的相互作用、训练策略和奖励设计对专业和泛化推理能力的影响。", "motivation": "现有研究主要集中于孤立的推理领域，但现实世界的推理场景需要多认知技能的综合应用。尽管如此，在强化学习下这些推理技能之间的相互作用仍知之甚少。本研究旨在弥补这一空白，系统调查RLVR框架下的多领域推理。", "method": "本研究在RLVR框架下，使用GRPO算法和Qwen-2.5-7B模型系列，系统调查了数学推理、代码生成和逻辑谜题解决三个主要领域的多领域推理。研究包含四个关键部分：(1) 评估单领域训练下的域内改进和跨域泛化能力；(2) 考察组合跨域训练中出现的相互增强和冲突；(3) 分析比较基础模型和指令模型在相同RL配置下的性能差异；(4) 深入探讨课程学习策略、奖励设计变体和语言特定因素对RL训练的影响。", "result": "实验结果深入揭示了领域交互的动态性，揭示了影响专业化和泛化推理性能的关键因素。", "conclusion": "这些发现为优化强化学习方法提供了宝贵指导，以培养大型语言模型全面的多领域推理能力。", "translation": "可验证奖励强化学习（RLVR）已成为增强大型语言模型（LLMs）推理能力的强大范式。现有研究主要集中于孤立的推理领域，例如数学问题解决、编码任务或逻辑推理。然而，现实世界的推理场景本质上要求多种认知技能的综合应用。尽管如此，在强化学习下这些推理技能之间的相互作用仍知之甚少。为了弥补这一差距，我们对RLVR框架内的多领域推理进行了系统调查，明确关注三个主要领域：数学推理、代码生成和逻辑谜题解决。我们进行了一项包含四个关键组成部分的全面研究：(1) 利用GRPO算法和Qwen-2.5-7B模型家族，我们的研究在单领域数据集上训练时，彻底评估了模型的域内改进和跨域泛化能力。(2) 此外，我们检查了在组合跨域训练期间出现的复杂交互，包括相互增强和冲突。(3) 为了进一步理解SFT对RL的影响，我们还在相同的RL配置下分析并比较了基础模型和指令模型之间的性能差异。(4) 此外，我们深入探讨了关键的RL训练细节，系统地探索了课程学习策略、奖励设计变体和语言特定因素的影响。通过大量的实验，我们的结果为领域交互的动态性提供了重要见解，揭示了影响专业化和泛化推理性能的关键因素。这些发现为优化RL方法提供了宝贵指导，以培养LLMs全面的多领域推理能力。", "summary": "本研究系统地探讨了在可验证奖励强化学习（RLVR）框架下，大型语言模型（LLMs）的多领域推理能力。论文专注于数学、代码和逻辑三大领域，通过详细实验评估了单领域训练的泛化能力、跨领域训练的交互作用、SFT对RL的影响以及RL训练细节（如课程学习和奖励设计）的影响。研究结果揭示了领域间交互的动态性，并为优化RL方法以提升LLMs的综合多领域推理能力提供了指导。", "keywords": "多领域推理, 强化学习, 大型语言模型, 跨域泛化, 奖励设计", "comments": "该论文的创新之处在于其首次系统性地探究了RLVR框架下大型语言模型的多领域推理能力，填补了现有研究主要集中于孤立领域推理的空白。通过对领域间交互、不同模型类型和关键RL训练策略的深入分析，为未来开发更通用、更强大的LLMs提供了重要的实证依据和优化方向，具有重要的实践指导意义。"}}
{"id": "2507.17572", "title": "KernelSOS for Global Sampling-Based Optimal Control and Estimation via Semidefinite Programming", "authors": ["Antoine Groudiev", "Fabian Schramm", "Éloïse Berthier", "Justin Carpentier", "Frederike Dümbgen"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17572v1", "summary": "Global optimization has gained attraction over the past decades, thanks to\nthe development of both theoretical foundations and efficient numerical\nroutines to cope with optimization problems of various complexities. Among\nrecent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful\nframework, leveraging the potential of sum of squares methods from the\npolynomial optimization community with the expressivity of kernel methods\nwidely used in machine learning. This paper applies the kernel sum of squares\nframework for solving control and estimation problems, which exhibit poor local\nminima. We demonstrate that KernelSOS performs well on a selection of problems\nfrom both domains. In particular, we show that KernelSOS is competitive with\nother sum of squares approaches on estimation problems, while being applicable\nto non-polynomial and non-parametric formulations. The sample-based nature of\nKernelSOS allows us to apply it to trajectory optimization problems with an\nintegrated simulator treated as a black box, both as a standalone method and as\na powerful initialization method for local solvers, facilitating the discovery\nof better solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17572v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于半定规划的全局采样最优控制与估计的KernelSOS方法", "tldr": "本文将KernelSOS框架应用于具有局部极小值问题的全局采样最优控制和估计，并展示了其在各种问题上的良好性能，包括非多项式和非参数公式。", "motivation": "全局优化在过去几十年中获得了广泛关注，而控制和估计问题常存在差的局部极小值。KernelSOS作为一种结合多项式优化和核方法的强大框架，有望解决这些问题。", "method": "本文应用Kernel Sum of Squares (KernelSOS) 框架来解决控制和估计问题。该方法结合了多项式优化中的和平方方法与机器学习中广泛使用的核方法的表达能力。其采样性质使其能够应用于将集成模拟器视为黑盒的轨迹优化问题，既可作为独立方法，也可作为局部求解器的强大初始化方法。", "result": "KernelSOS在选定的控制和估计问题上表现良好。在估计问题上，它与其他和平方方法具有竞争力，并且适用于非多项式和非参数公式。其采样性质使其能够应用于带有黑盒模拟器的轨迹优化问题，并有助于发现更好的解决方案。", "conclusion": "KernelSOS是一个解决全局采样最优控制和估计问题的有效框架，尤其适用于存在复杂局部极小值的问题，并能处理非多项式和非参数的系统，同时作为局部求解器的初始化方法也表现出色。", "translation": "在过去的几十年里，由于理论基础和高效数值例程的发展，全局优化在应对各种复杂性的优化问题方面受到了广泛关注。在最近的方法中，核平方和（KernelSOS）作为一种强大的框架出现，它利用了多项式优化社区中和平方方法的潜力与机器学习中广泛使用的核方法的表达能力。本文将核平方和框架应用于解决表现出差的局部极小值的控制和估计问题。我们证明了KernelSOS在来自这两个领域的一系列问题上表现良好。特别是，我们表明KernelSOS在估计问题上与其他和平方方法具有竞争力，同时适用于非多项式和非参数的公式。KernelSOS的基于样本的性质使我们能够将其应用于将集成模拟器视为黑盒的轨迹优化问题，既可以作为独立方法，也可以作为局部求解器的强大初始化方法，从而有助于发现更好的解决方案。", "summary": "本文将KernelSOS框架应用于解决具有挑战性局部极小值的全局采样最优控制和估计问题。该方法结合了和平方方法与核方法的优势，能够处理非多项式和非参数系统。实验结果表明，KernelSOS在各种控制和估计问题上表现出色，尤其在估计问题上具有竞争力，并能作为黑盒模拟器轨迹优化的有效工具，或作为局部求解器的良好初始化方法，从而有助于找到更优解。", "keywords": "KernelSOS, 全局优化, 最优控制, 估计, 半定规划", "comments": "KernelSOS方法创新性地结合了多项式和平方优化与核方法，使其能够处理传统方法难以解决的非多项式和非参数全局优化问题。其基于采样的特性使其能够与黑盒模拟器集成，极大地扩展了其应用范围，尤其在复杂系统控制和估计领域具有重要意义。同时，作为局部求解器的初始化方法，它也提供了一种发现更优解的有效策略。"}}
{"id": "2507.17412", "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging", "authors": ["Farnaz Khun Jush", "Steffen Vogler", "Matthias Lenga"], "categories": ["cs.CV", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17412v1", "summary": "The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17412v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于内容的3D图像检索和受ColBERT启发的肿瘤标记与分期重排序", "tldr": "引入了C-MIR，一种受ColBERT启发的3D医学图像重排序方法，用于肿瘤标记和分期，无需预分割数据即可实现更好的性能。", "motivation": "医疗图像数量的不断增长给放射科医生检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统为高效访问相似病例提供了潜力，但缺乏标准化的评估和全面的研究。", "method": "本研究通过三项关键贡献推进了体积医学图像的CBIR研究：(1) 一个消除对预分割数据和器官特异性数据集依赖的框架，与临床实践中的大型非结构化图像归档系统（即PACS）保持一致；(2) 引入C-MIR，一种新颖的体积重排序方法，该方法将ColBERT的上下文后期交互机制应用于3D医学成像；(3) 使用三种特征提取器和三种数据库配置对四个肿瘤部位进行了全面评估。", "result": "评估突出了C-MIR的显著优势，成功将后期交互原则应用于体积医学图像，实现了有效的上下文感知重排序。C-MIR能够有效定位感兴趣区域，无需数据集预分割，并为依赖昂贵数据丰富步骤的系统提供了一种计算效率更高的替代方案。C-MIR在肿瘤标记方面显示出有希望的改进，取得了更好的性能，特别是对于结肠和肺肿瘤（p<0.05），并显示出改善肿瘤分期的潜力。", "conclusion": "最终，本文旨在弥合先进检索技术与其在医疗保健中的实际应用之间的差距，为改进诊断过程铺平道路。", "translation": "不断增长的医学图像数量给放射科医生检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统为高效访问相似病例提供了潜力，但缺乏标准化的评估和全面的研究。本研究在先前通过CBIR进行肿瘤特征描述的研究基础上，通过三项关键贡献推进了体积医学图像的CBIR研究：(1) 一个消除对预分割数据和器官特异性数据集依赖的框架，与大型非结构化图像归档系统（即临床实践中的PACS）保持一致；(2) 引入C-MIR，一种新颖的体积重排序方法，该方法将ColBERT的上下文后期交互机制应用于3D医学成像；(3) 使用三种特征提取器和三种数据库配置对四个肿瘤部位进行了全面评估。我们的评估突出了C-MIR的显著优势。我们证明了后期交互原则成功应用于体积医学图像，实现了有效的上下文感知重排序。一个关键发现是C-MIR能够有效定位感兴趣区域，消除了数据集预分割的需要，并为依赖昂贵数据丰富步骤的系统提供了一种计算效率更高的替代方案。C-MIR在肿瘤标记方面显示出有希望的改进，取得了更好的性能，特别是对于结肠和肺肿瘤（p<0.05）。C-MIR还显示出改善肿瘤分期的潜力，值得进一步探索其能力。最终，我们的工作旨在弥合先进检索技术与其在医疗保健中的实际应用之间的差距，为改进诊断过程铺平道路。", "summary": "本文通过引入C-MIR，一种受ColBERT后期交互机制启发的用于3D医学成像的新型体积重排序方法，解决了检索相关医学图像的挑战。所提出的框架消除了对预分割数据和器官特异性数据集的需求，与临床PACS系统保持一致。对各种肿瘤部位的评估表明，C-MIR具有显著优势，包括有效定位感兴趣区域和在肿瘤标记（特别是结肠和肺肿瘤）方面提高了性能，以及在肿瘤分期方面的潜力，为医学图像检索提供了一种计算效率更高的替代方案。", "keywords": "3D图像检索, ColBERT, 医学成像, 肿瘤标记, 重排序", "comments": "本文通过将最先进的自然语言处理重排序技术（ColBERT）应用于具有挑战性的3D医学图像检索领域，做出了重要贡献。其创新之处在于消除了对预分割数据的需求（这是医学成像中的主要瓶颈），并提供了一种计算效率高的解决方案。对多个肿瘤部位和配置的全面评估增强了其发现和实际适用性，为医疗保健中更高效、更准确的诊断过程铺平了道路。"}}
{"id": "2507.17588", "title": "Dual-branch Prompting for Multimodal Machine Translation", "authors": ["Jie Wang", "Zhendong Yang", "Liansong Zong", "Xiaobo Zhang", "Dexian Wang", "Ji Zhang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17588v1", "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17588v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多模态机器翻译的双分支提示", "tldr": "提出D2P-MMT，一个基于扩散的双分支提示框架，通过使用重建图像和分布对齐实现鲁棒的视觉引导多模态机器翻译，并取得了优异的性能。", "motivation": "目前最先进的多模态机器翻译方法在推理时常依赖成对的图像-文本输入，并且对不相关的视觉噪声敏感，这限制了它们的鲁棒性和实际应用性。", "method": "提出D2P-MMT，一个基于扩散的双分支提示框架。推理时仅需源文本和预训练扩散模型生成的重建图像，这自然地过滤了分散注意力的视觉细节并保留了语义线索。训练时，模型通过双分支提示策略同时学习真实图像和重建图像，鼓励丰富的跨模态交互。引入分布对齐损失，以弥合模态差距并减轻训练-推理差异。", "result": "在Multi30K数据集上的大量实验表明，D2P-MMT比现有最先进的方法取得了更优越的翻译性能。", "conclusion": "D2P-MMT通过利用重建图像和双分支提示策略，有效解决了多模态机器翻译中鲁棒性和实际应用性的问题，并取得了领先的性能。", "translation": "多模态机器翻译（MMT）通常通过结合对齐的视觉特征来增强纯文本翻译。尽管取得了显著进展，但最先进的多模态机器翻译方法在推理时常依赖成对的图像-文本输入，并且对不相关的视觉噪声敏感，这限制了它们的鲁棒性和实际应用性。为了解决这些问题，我们提出了D2P-MMT，一个基于扩散的双分支提示框架，用于鲁棒的视觉引导翻译。具体而言，D2P-MMT在推理时仅需要源文本和由预训练扩散模型生成的重建图像，这自然地过滤了分散注意力的视觉细节，同时保留了语义线索。在训练过程中，模型使用双分支提示策略同时学习真实图像和重建图像，鼓励丰富的跨模态交互。为了弥合模态差距并减轻训练-推理差异，我们引入了分布对齐损失，以强制两个分支的输出分布之间保持一致性。在Multi30K数据集上的大量实验表明，D2P-MMT比现有最先进的方法取得了更优越的翻译性能。", "summary": "本文提出D2P-MMT，一种新颖的基于扩散的双分支提示框架，用于鲁棒的多模态机器翻译。针对当前多模态机器翻译方法对视觉噪声敏感且依赖成对图像-文本输入的局限性，D2P-MMT在推理时利用扩散模型重建的图像，有效过滤噪声。训练期间，采用双分支策略和分布对齐损失，使模型同时学习真实图像和重建图像。Multi30K数据集上的实验表明，D2P-MMT实现了卓越的翻译性能。", "keywords": "多模态机器翻译, 扩散模型, 双分支提示, 鲁棒性, 图像重建", "comments": "该论文的创新点在于利用扩散模型重建图像以过滤视觉噪声，并结合双分支提示策略和分布对齐损失来增强多模态机器翻译的鲁棒性和实用性。这种方法有效解决了现有模型对噪声敏感和推理依赖的问题，为视觉引导翻译提供了新的思路。"}}
{"id": "2506.15239", "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15239v2", "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15239v2", "cate": "cs.CL", "date": "2025-06-18", "updated": "2025-07-23", "AI": {"title_translation": "迷失在变体中？评估巴斯克语和西班牙语地理变体中的NLI性能", "tldr": "本研究评估了当前语言技术理解巴斯克语和西班牙语变体的能力，发现LLM在处理语言变体时性能下降，尤其是在巴斯克语中，且这种下降与语言变体本身有关，与词汇重叠无关。", "motivation": "评估当前语言技术理解巴斯克语和西班牙语语言变体的能力。", "method": "使用自然语言推理（NLI）作为核心任务，引入了一个新颖的手动整理的巴斯克语和西班牙语及其各自变体的平行数据集。通过使用仅编码器和基于解码器的大型语言模型（LLMs）进行跨语言和上下文学习实验，并进行了错误分析和消融实验。", "result": "处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在处理西部巴斯克语时表现尤其困难。", "conclusion": "仅编码器模型在西部巴斯克语上的困难与语言学理论相符，该理论认为外围方言（如西部方言）与标准语的距离更远。", "translation": "在本文中，我们评估了当前语言技术理解巴斯克语和西班牙语语言变体的能力。我们使用自然语言推理（NLI）作为核心任务，并引入了一个新颖的、手动整理的巴斯克语和西班牙语及其各自变体的平行数据集。我们对使用仅编码器和基于解码器的大型语言模型（LLMs）进行的跨语言和上下文学习实验的实证分析表明，在处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明，这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在处理西部巴斯克语时表现尤其困难，这与语言学理论相符，该理论认为外围方言（例如，西部方言）与标准语的距离更远。所有数据和代码均已公开提供。", "summary": "本研究评估了当前语言技术在理解巴斯克语和西班牙语变体方面的能力，以自然语言推理（NLI）为核心任务，并构建了一个新的手动整理的平行数据集。通过对仅编码器和基于解码器的大型语言模型（LLMs）进行跨语言和上下文学习实验，发现处理语言变体时性能显著下降，尤其是在巴斯克语中。错误分析揭示这种下降源于语言变体本身而非词汇重叠。消融实验进一步指出，仅编码器模型在处理西部巴斯克语时尤为吃力，这与语言学中关于外围方言与标准语距离更远的理论相符。", "keywords": "自然语言推理, 语言变体, 巴斯克语, 西班牙语, 大型语言模型", "comments": "这项研究通过引入一个新颖的手动整理的方言数据集，并系统地评估了LLMs在处理语言变体方面的性能，具有重要意义。它揭示了当前LLMs在处理非标准或地域性语言变体时的局限性，特别是仅编码器模型对某些方言的理解能力较弱，这为未来模型改进和方言研究提供了宝贵见解。"}}
{"id": "2507.17001", "title": "Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation", "authors": ["Yan Li", "Guangyi Chen", "Yunlong Deng", "Zijian Li", "Zeyu Tang", "Anpeng Wu", "Kun Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17001v1", "summary": "Most existing methods for adapting models to out-of-distribution (OOD)\ndomains rely on invariant representation learning to eliminate the influence of\nbiased features. However, should bias always be eliminated -- and if not, when\nshould it be retained, and how can it be leveraged? To address these questions,\nwe first present a theoretical analysis that explores the conditions under\nwhich biased features can be identified and effectively utilized. Building on\nthis theoretical foundation, we introduce a novel framework that strategically\nleverages bias to complement invariant representations during inference. The\nframework comprises two key components that leverage bias in both direct and\nindirect ways: (1) using invariance as guidance to extract predictive\ningredients from bias, and (2) exploiting identified bias to estimate the\nenvironmental condition and then use it to explore appropriate bias-aware\npredictors to alleviate environment gaps. We validate our approach through\nexperiments on both synthetic datasets and standard domain generalization\nbenchmarks. Results consistently demonstrate that our method outperforms\nexisting approaches, underscoring its robustness and adaptability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17001v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "应该总是消除偏差吗？一个利用数据偏差进行OOD生成的原则性框架", "tldr": "本文提出一个原则性框架，旨在利用数据偏差进行域外（OOD）生成，而非传统地消除偏差，并通过理论分析和实验验证了其有效性。", "motivation": "现有OOD适应方法多依赖不变表示学习来消除偏差特征的影响。然而，本文质疑偏差是否总应被消除，以及何时应保留并如何利用它，以此为出发点进行研究。", "method": "本文首先进行理论分析，探索偏差特征的识别和有效利用条件。在此基础上，提出了一个新颖的框架，在推理阶段策略性地利用偏差来补充不变表示。该框架包含两个关键组件：1) 利用不变性指导从偏差中提取预测性成分；2) 利用已识别的偏差估计环境条件，并用其探索适当的偏差感知预测器以缓解环境差距。", "result": "在合成数据集和标准域泛化基准上的实验结果一致表明，所提出的方法优于现有方法，证明了其鲁棒性和适应性。", "conclusion": "本文通过理论分析和实验验证，证明了在特定条件下利用数据偏差可以有效提升域外生成任务的性能，挑战了传统上总是消除偏差的观念，并提供了一个实用的框架。", "translation": "大多数现有的模型适应域外（OOD）领域的方法都依赖于不变表示学习来消除偏差特征的影响。然而，偏差是否应该总是被消除——如果不是，何时应该保留，以及如何利用它？为了解决这些问题，我们首先提出了一个理论分析，探讨了可以识别和有效利用偏差特征的条件。在此理论基础之上，我们引入了一个新颖的框架，在推理过程中策略性地利用偏差来补充不变表示。该框架包含两个关键组件，以直接和间接的方式利用偏差：(1) 利用不变性作为指导，从偏差中提取预测性成分；(2) 利用已识别的偏差来估计环境条件，然后用它来探索适当的偏差感知预测器，以缓解环境差距。我们通过在合成数据集和标准域泛化基准上的实验验证了我们的方法。结果一致表明，我们的方法优于现有方法，突出了其鲁棒性和适应性。", "summary": "本文提出一个新颖的原则性框架，旨在策略性地利用数据偏差进行域外（OOD）生成，而非传统地消除偏差。通过理论分析明确了偏差特征的利用条件，并设计了两个组件来直接和间接地利用偏差，包括从不变性中提取预测成分以及利用偏差估计环境条件。实验结果表明，该方法在OOD任务上优于现有方法，证明了其鲁棒性和适应性。", "keywords": "数据偏差, 域外生成, 不变表示, 域泛化, OOD", "comments": "该论文具有创新性，挑战了域泛化领域中普遍认为应消除偏差的传统观念。它提供了一个原则性的理论框架，指导如何识别和有效利用偏差，而非盲目消除。这种逆向思维不仅为OOD生成提供了新的视角，也为未来的研究开辟了新的方向。框架的两个组件设计巧妙，兼顾了直接和间接利用偏差，增强了方法的实用性。"}}
{"id": "2507.16999", "title": "Bayesian preference elicitation for decision support in multiobjective optimization", "authors": ["Felix Huber", "Sebastian Rojas Gonzalez", "Raul Astudillo"], "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures", "url": "http://arxiv.org/abs/2507.16999v1", "summary": "We present a novel approach to help decision-makers efficiently identify\npreferred solutions from the Pareto set of a multi-objective optimization\nproblem. Our method uses a Bayesian model to estimate the decision-maker's\nutility function based on pairwise comparisons. Aided by this model, a\nprincipled elicitation strategy selects queries interactively to balance\nexploration and exploitation, guiding the discovery of high-utility solutions.\nThe approach is flexible: it can be used interactively or a posteriori after\nestimating the Pareto front through standard multi-objective optimization\ntechniques. Additionally, at the end of the elicitation phase, it generates a\nreduced menu of high-quality solutions, simplifying the decision-making\nprocess. Through experiments on test problems with up to nine objectives, our\nmethod demonstrates superior performance in finding high-utility solutions with\na small number of queries. We also provide an open-source implementation of our\nmethod to support its adoption by the broader community.", "comment": "16 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.16999v1", "cate": "stat.ML", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "多目标优化中决策支持的贝叶斯偏好启发", "tldr": "一种贝叶斯方法能高效地在多目标优化中用少量查询找到偏好解。", "motivation": "帮助决策者从多目标优化问题的帕累托集中高效识别偏好解。", "method": "该方法使用贝叶斯模型，通过成对比较估计决策者的效用函数。一种有原则的启发策略交互式选择查询，以平衡探索和利用，指导发现高效用解。该方法灵活，可交互使用或在估计帕累托前沿后使用。", "result": "通过在多达九个目标测试问题上的实验，该方法在以少量查询找到高效用解方面表现出卓越性能。它还能生成一个高质量解决方案的精简菜单。", "conclusion": "所提出的贝叶斯偏好启发方法有效且高效地支持决策者在多目标优化中识别偏好解，表现出卓越的性能和灵活性。", "translation": "我们提出了一种新颖的方法，旨在帮助决策者从多目标优化问题的帕累托集中高效识别偏好解。我们的方法使用贝叶斯模型，根据成对比较来估计决策者的效用函数。在该模型的辅助下，一种有原则的启发策略交互式地选择查询，以平衡探索和利用，指导高效用解的发现。该方法具有灵活性：它既可以交互式使用，也可以在通过标准多目标优化技术估计帕累托前沿后事后使用。此外，在启发阶段结束时，它会生成一个高质量解决方案的精简菜单，从而简化决策过程。通过在多达九个目标测试问题上的实验，我们的方法在以少量查询找到高效用解方面表现出卓越性能。我们还提供了我们方法的开源实现，以支持其在更广泛社区中的采用。", "summary": "本文提出一种新颖的贝叶斯方法，用于多目标优化中高效的偏好启发。它通过成对比较估计决策者的效用函数，并采用交互式启发策略平衡探索与利用。该方法能生成高质量解决方案的精简菜单，并在少量查询下找到高效用解方面表现出卓越性能，并提供了开源实现以支持推广。", "keywords": "贝叶斯偏好启发, 多目标优化, 决策支持, 效用函数, 交互式启发", "comments": "本文通过将贝叶斯建模与交互式启发相结合，为多目标优化提供了一种创新方法。其优势在于能以最少的查询高效引导决策者找到高效用解，解决了复杂决策中的关键挑战。该方法的灵活性（交互式/事后）以及提供开源实现，增强了其实用性和潜在影响力。"}}
{"id": "2507.17501", "title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "authors": ["Xianbiao Qi", "Marco Chen", "Wenjie Xiao", "Jiaquan Ye", "Yelin He", "Chun-Guang Li", "Zhouchen Lin"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      We have introduced a novel architecture, Deeply Normalized Transformer (DNT), which enables efficient training with vanilla momentum SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers", "url": "http://arxiv.org/abs/2507.17501v1", "summary": "Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.", "comment": "We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers", "pdf_url": "http://arxiv.org/pdf/2507.17501v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DNT: 一种可以通过动量SGD训练的深度归一化Transformer", "tldr": "DNT是一种深度归一化Transformer，可以用动量SGD训练，性能与AdamW训练的Transformer相当，解决了梯度重尾分布问题。", "motivation": "现有Transformer的训练通常需要像AdamW这样的自适应学习率优化器，难以使用动量SGDW进行训练，主要原因是梯度的重尾分布。", "method": "本文引入了深度归一化Transformer (DNT)，通过在Transformer的适当位置策略性地集成归一化技术，以有效调节每层的雅可比矩阵，平衡权重、激活及其相互作用的影响，从而使梯度分布集中。提供了理论证明和广泛的经验评估。", "result": "a) DNT在性能上优于其对应的模型（即ViT和GPT）；b) DNT可以有效地通过普通的mSGDW进行训练。", "conclusion": "DNT通过深度归一化解决了Transformer难以用动量SGD训练的问题，并取得了与AdamW训练的Transformer相当或更好的性能，证明了其在训练效率和模型性能方面的优越性。", "translation": "Transformer已成为现代深度学习的事实骨干，然而其训练通常需要像AdamW这样的高级自适应学习率优化器，而不是动量SGDW (mSGDW)。以前的研究表明，这主要是由于梯度的重尾分布。在本文中，我们引入了一种深度归一化Transformer (DNT)，它经过精心设计以克服这一限制，从而能够使用普通的mSGDW进行无缝训练，同时产生与通过AdamW训练的Transformer相当的性能。具体来说，在DNT中，我们策略性地在Transformer的适当位置集成归一化技术，以有效调节每层的雅可比矩阵，平衡权重、激活及其相互作用的影响，从而使梯度分布集中。我们提供了DNT中使用的归一化技术的理论证明，并在两种流行的Transformer架构上进行了广泛的实证评估，以验证：a) DNT优于其对应的模型（即ViT和GPT），b) DNT可以有效地通过普通的mSGDW进行训练。", "summary": "本文提出了一种深度归一化Transformer (DNT)，旨在解决现有Transformer难以使用动量SGD训练的问题。DNT通过在关键位置应用归一化技术，有效调节梯度分布，使其能够使用普通的动量SGDW进行训练，同时在性能上与使用AdamW训练的Transformer相当或更优。研究通过理论和实验证明了DNT的有效性，并展示其在ViT和GPT等架构上的优越性。", "keywords": "Transformer, 深度归一化, 动量SGD, 梯度分布, AdamW", "comments": "这项工作通过引入深度归一化Transformer (DNT)解决了Transformer训练中长期存在的挑战，即对自适应优化器如AdamW的依赖。其创新之处在于通过策略性地集成归一化技术来调节梯度分布，从而使得Transformer能够使用更简单的动量SGDW进行有效训练，同时保持或提高性能。这对于降低训练复杂度和资源需求具有重要意义。"}}
{"id": "2505.04457", "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE WASPAA2025", "url": "http://arxiv.org/abs/2505.04457v4", "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaveFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators.", "comment": "Accepted to IEEE WASPAA2025", "pdf_url": "http://arxiv.org/pdf/2505.04457v4", "cate": "cs.SD", "date": "2025-05-07", "updated": "2025-07-23", "AI": {"title_translation": "Miipher-2：一个用于百万小时级数据恢复的通用语音恢复模型", "tldr": "Miipher-2是一个高效的通用语音恢复模型，专为大规模生成模型的数据清洗设计，利用预训练的USM进行特征提取，并在多语言环境下表现出色。", "motivation": "大规模生成模型（如大型语言模型）的训练数据清洗对语音恢复（SR）提出了新的应用需求。现有挑战包括对未知语言的泛化能力、无需显式条件（如文本、说话人ID）的操作以及计算效率。", "method": "Miipher-2利用一个冻结的、预训练的通用语音模型（USM）作为无条件特征提取器，该USM支持300多种语言。为优化效率和最小化内存，模型集成了并行适配器来预测来自噪声输入的干净USM特征，并使用WaveFit神经声码器进行波形合成。这些组件在3000小时的多语言、录音室质量数据上进行训练，并增加了降级。", "result": "实验结果表明，Miipher-2在词错误率、说话人相似性和客观及主观音质评分方面，对所有测试语言均表现出优于或与传统SR模型相当的性能。Miipher-2在消费级加速器上高效运行，实现了0.0078的实时因子，这意味着仅用100个加速器即可在大约三天内处理百万小时的语音数据集。", "conclusion": "Miipher-2是一个高效、通用的语音恢复模型，能够有效解决大规模生成模型训练数据清洗中的多语言泛化、无条件操作和计算效率挑战，并在实际应用中展现出卓越的性能和处理能力。", "translation": "训练数据清洗是基于生成模型的语音恢复（SR）的一个新应用。本文介绍了Miipher-2，一个专为百万小时级数据设计的SR模型，用于大型生成模型（如大型语言模型）的训练数据清洗。解决的关键挑战包括对未知语言的泛化、无需显式条件（例如文本、说话人ID）的操作以及计算效率。Miipher-2利用一个冻结的、预训练的通用语音模型（USM）作为鲁棒的、无条件特征提取器，该USM支持300多种语言。为了优化效率并最小化内存，Miipher-2结合了并行适配器，用于从噪声输入预测干净的USM特征，并采用WaveFit神经声码器进行波形合成。这些组件在3000小时的多语言、录音室质量录音数据上进行训练，并增加了降级，而USM参数保持固定。实验结果表明，Miipher-2在所有测试语言中，在词错误率、说话人相似性以及客观和主观音质得分方面，表现出优于或与传统SR模型相当的性能。Miipher-2在消费级加速器上高效运行，实现了0.0078的实时因子，使得仅用100个此类加速器即可在大约三天内处理百万小时的语音数据集。", "summary": "Miipher-2是一个为大规模生成模型数据清洗设计的通用语音恢复（SR）模型。它通过利用冻结的预训练通用语音模型（USM）作为无条件特征提取器，并结合并行适配器和WaveFit神经声码器来高效地从噪声输入恢复语音。该模型在多语言环境下表现出卓越的泛化能力和计算效率，能在消费级硬件上快速处理百万小时级数据，并在各项指标上优于或媲美现有SR模型。", "keywords": "语音恢复, 数据清洗, 通用语音模型, Miipher-2, 大规模数据", "comments": "Miipher-2的创新之处在于其将语音恢复应用于大规模生成模型的数据清洗，并解决了多语言泛化和无条件操作的难题。其采用冻结的USM作为特征提取器，并结合并行适配器和高效声码器，显著提升了处理百万小时级数据的效率，使其在实际应用中具有重要价值和广泛前景。"}}
{"id": "2507.17524", "title": "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition", "authors": ["Jiahao Tang", "Youjun Li", "Xiangting Fan", "Yangxuan Zheng", "Siyuan Lu", "Xueping Li", "Peng Fang", "Chenxi Li", "Zi-Gang Huang"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17524v1", "summary": "Electroencephalography(EEG) based emotion recognition holds great promise for\naffective brain-computer interfaces (aBCIs), yet practical deployment remains\nchallenging due to substantial inter-subject variability and the lack of\nlabeled data in target domains. To overcome these limitations, we present a\nnovel unsupervised Semantic-Dynamic Consistency domain adaptation network for\nfully label-free cross-subject EEG emotion recognition. First, we introduce a\nSame-Subject Same-Trial Mixup strategy that generates augmented samples via\nintra-trial interpolation, enhancing data diversity while explicitly preserving\nindividual identity to mitigate label ambiguity. Second, we construct a dynamic\ndistribution alignment module in reproducing kernel Hilbert space (RKHS),\njointly aligning marginal and conditional distributions through multi-objective\nkernel mean embedding, and leveraging a confidence-aware pseudo-labeling\nstrategy to ensure stable adaptation. Third, we propose a dual-domain\nsimilarity consistency learning mechanism that enforces cross-domain structural\nconstraints based on latent pairwise similarities, enabling semantic boundary\nlearning without relying on temporal synchronization or label priors. To\nvalidate the effectiveness and robustness of the proposed SDC-Net, extensive\nexperiments are conducted on three widely used EEG benchmark datasets: SEED,\nSEED-IV, and Faced. Comparative results against existing unsupervised domain\nadaptation methods demonstrate that SDC-Net achieves state-of-the-art\nperformance in emotion recognition under both cross-subject and cross-session\nconditions. This advancement significantly improves the accuracy and\ngeneralization capability of emotion decoding, and lays a solid foundation for\nreal-world applications of personalized affective brain-computer interfaces\n(aBCIs). The source code will be released at\nhttps://github.com/XuanSuTrum/SDC-Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17524v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SDC-Net：一种用于跨受试者脑电图情感识别的语义动态一致性域适应框架", "tldr": "SDC-Net是一个无监督域适应框架，通过语义动态一致性解决了跨受试者脑电图情感识别中的个体差异和数据标注不足问题，并取得了最先进的性能。", "motivation": "基于脑电图（EEG）的情感识别在情感脑机接口（aBCIs）中具有巨大潜力，但由于个体间差异大和目标域缺乏标注数据，实际部署仍面临挑战。", "method": "本文提出了一个新颖的无监督语义动态一致性域适应网络（SDC-Net）。首先，引入了“同受试者同试验混合（Same-Subject Same-Trial Mixup）”策略，通过试次内插值生成增强样本，在提高数据多样性的同时明确保留个体身份以减轻标签模糊性。其次，构建了一个在再生核希尔伯特空间（RKHS）中的动态分布对齐模块，通过多目标核均值嵌入联合对齐边缘分布和条件分布，并利用置信度感知的伪标签策略确保稳定适应。第三，提出了一个双域相似性一致性学习机制，基于潜在成对相似性强制执行跨域结构约束，从而在不依赖时间同步或先验标签的情况下实现语义边界学习。", "result": "在SEED、SEED-IV和Faced三个广泛使用的脑电图基准数据集上进行了广泛实验。与现有无监督域适应方法相比，SDC-Net在跨受试者和跨会话条件下均实现了情感识别的最先进性能。", "conclusion": "SDC-Net显著提高了情感解码的准确性和泛化能力，并为个性化情感脑机接口（aBCIs）的实际应用奠定了坚实基础。", "translation": "脑电图（EEG）情感识别在情感脑机接口（aBCIs）中具有巨大潜力，但由于受试者之间存在显著差异以及目标域缺乏标注数据，实际部署仍面临挑战。为了克服这些限制，我们提出了一种新颖的无监督语义动态一致性域适应网络（SDC-Net），用于完全无标签的跨受试者脑电图情感识别。首先，我们引入了一种“同受试者同试验混合（Same-Subject Same-Trial Mixup）”策略，通过试次内插值生成增强样本，在提高数据多样性的同时明确保留个体身份以减轻标签模糊性。其次，我们在再生核希尔伯特空间（RKHS）中构建了一个动态分布对齐模块，通过多目标核均值嵌入联合对齐边缘分布和条件分布，并利用置信度感知的伪标签策略确保稳定适应。第三，我们提出了一种双域相似性一致性学习机制，基于潜在成对相似性强制执行跨域结构约束，从而在不依赖时间同步或先验标签的情况下实现语义边界学习。为了验证所提出的SDC-Net的有效性和鲁棒性，我们在三个广泛使用的脑电图基准数据集：SEED、SEED-IV和Faced上进行了广泛实验。与现有无监督域适应方法相比，SDC-Net在跨受试者和跨会话条件下均实现了情感识别的最先进性能。这一进展显著提高了情感解码的准确性和泛化能力，并为个性化情感脑机接口（aBCIs）的实际应用奠定了坚实基础。源代码将发布在https://github.com/XuanSuTrum/SDC-Net。", "summary": "本文提出了一种名为SDC-Net的无监督域适应框架，旨在解决跨受试者脑电图情感识别中存在的个体差异大和目标域数据标注不足的问题。该框架通过引入“同受试者同试验混合”策略增强数据多样性并保留个体身份，构建动态分布对齐模块联合对齐分布，以及提出双域相似性一致性学习机制进行语义边界学习。实验结果表明，SDC-Net在多个EEG数据集上实现了最先进的性能，显著提升了情感解码的准确性和泛化能力，为个性化情感脑机接口的实际应用奠定了基础。", "keywords": "脑电图情感识别, 域适应, 无监督学习, 语义动态一致性, 跨受试者", "comments": "SDC-Net的创新点在于其结合了多种策略来解决EEG情感识别中的核心挑战。特别是，“同受试者同试验混合”策略在数据增强的同时保持个体身份，以及“双域相似性一致性学习”在无标签先验下进行语义边界学习，都展现了其在处理跨受试者变异性方面的独到见解。该工作为情感脑机接口的实际部署提供了有力的技术支持。"}}
{"id": "2507.03379", "title": "On the non-convexity issue in the radial Calderón problem", "authors": ["Giovanni S. Alberti", "Romain Petit", "Clarice Poon"], "categories": ["math.NA", "cs.NA", "math.AP", "math.OC"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.03379v2", "summary": "A classical approach to the Calder\\'on problem is to estimate the unknown\nconductivity by solving a nonlinear least-squares problem. It is generally\nbelieved that it leads to a nonconvex optimization problem which is riddled\nwith bad local minimums. This has motivated the development of reconstruction\nmethods based on convex optimization, one recent contribution being the\nnonlinear convex semidefinite programming approach of Harrach (2023). In this\nwork, we investigate the computational viability of this convex approach in a\nsimple setting where the conductivities are piecewise constant and radial. We\nimplement this convex reconstruction method and compare it extensively to the\nleast squares approach. Our experiments suggest that this convex programming\napproach only allows to accurately estimate the unknown for problems with a\nvery small size. Moreover, surprisingly, it is consistently outperformed by\nNewton-type least squares solvers, which are also faster and require less\nmeasurements. We revisit the issue of nonconvexity in this piecewise constant\nradial setting and prove that, contrary to previous claims, there are no local\nminimums in the case of two scalar unknowns with no measurement noise. We also\nprovide a partial proof of this result in the general setting which holds under\na numerically verifiable assumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.03379v2", "cate": "math.NA", "date": "2025-07-04", "updated": "2025-07-23", "AI": {"title_translation": "径向 Calderón 问题中的非凸性问题", "tldr": "Calderón 问题的凸优化方法在计算上受限，且表现不如传统的最小二乘法；同时，在特定情况下，最小二乘问题的非凸性可能被误解，因为不存在局部最小值。", "motivation": "Calderón 问题的经典最小二乘方法通常被认为是非凸的，存在许多局部最小值，这促使了基于凸优化的重建方法的发展。", "method": "研究了在分段常数和径向电导率的简单设置下，一种新的非线性凸半定规划方法的计算可行性，并将其与最小二乘方法进行了广泛比较。同时，重新审视了在这种设置下的非凸性问题，并进行了理论证明。", "result": "实验表明，凸编程方法只能准确估计非常小规模问题中的未知量。此外，它始终不如牛顿型最小二乘求解器，后者更快且所需测量值更少。研究证明，在没有测量噪声的两个标量未知量的情况下，与之前的说法相反，不存在局部最小值。对于一般设置，在可数值验证的假设下，也提供了部分证明。", "conclusion": "在径向 Calderón 问题的特定设置中，传统的最小二乘方法可能比新的凸优化方法更有效，且之前关于其非凸性带来局部最小值的普遍看法可能不准确。", "translation": "Calderón 问题的一种经典方法是通过解决非线性最小二乘问题来估计未知电导率。人们普遍认为这会导致一个非凸优化问题，其中充满了糟糕的局部最小值。这促使了基于凸优化的重建方法的发展，其中最近的贡献是 Harrach (2023) 的非线性凸半定规划方法。在这项工作中，我们研究了这种凸方法在电导率是分段常数和径向的简单设置中的计算可行性。我们实现了这种凸重建方法，并将其与最小二乘方法进行了广泛比较。我们的实验表明，这种凸编程方法只能准确估计非常小规模问题中的未知量。此外，令人惊讶的是，它始终不如牛顿型最小二乘求解器，后者也更快且所需测量值更少。我们重新审视了这种分段常数径向设置中的非凸性问题，并证明，与之前的说法相反，在没有测量噪声的两个标量未知量的情况下，不存在局部最小值。我们还在一般设置中提供了此结果的部分证明，该证明在可数值验证的假设下成立。", "summary": "本论文评估了 Calderón 问题中一种新型凸优化方法的计算可行性，发现在简单设置下，该方法仅适用于非常小规模的问题，且性能不如传统的牛顿型最小二乘求解器。更重要的是，论文挑战了关于最小二乘问题非凸性的普遍看法，证明在特定径向设置（如两个标量未知量无噪声情况）中，不存在局部最小值，并提供了此结果的部分通用证明。", "keywords": "Calderón 问题, 非凸性, 凸优化, 最小二乘, 局部最小值", "comments": "这篇论文的重要创新在于它挑战了 Calderón 问题中长期存在的关于非凸性和局部最小值的普遍假设。它通过实验和理论证明，揭示了新兴的凸优化方法在实践中的局限性，并出人意料地发现传统非凸方法在某些情况下可能表现更好，甚至不存在所谓的“坏”局部最小值。这可能促使研究人员重新评估和改进经典的非凸算法，而非一味追求凸性替代方案，对该领域的研究方向具有重要启发意义。"}}
{"id": "2507.17189", "title": "Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems", "authors": ["Shaohan Li", "Hao Yang", "Min Chen", "Xiaolin Qin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17189v1", "summary": "The increasing frequency of extreme weather events due to global climate\nchange urges accurate weather prediction. Recently, great advances have been\nmade by the \\textbf{end-to-end methods}, thanks to deep learning techniques,\nbut they face limitations of \\textit{representation inconsistency} in\nmultivariable integration and struggle to effectively capture the dependency\nbetween variables, which is required in complex weather systems. Treating\ndifferent variables as distinct modalities and applying a \\textbf{two-stage\ntraining approach} from multimodal models can partially alleviate this issue,\nbut due to the inconformity in training tasks between the two stages, the\nresults are often suboptimal. To address these challenges, we propose an\nimplicit two-stage training method, configuring separate encoders and decoders\nfor each variable. In detailed, in the first stage, the Translator is frozen\nwhile the Encoders and Decoders learn a shared latent space, in the second\nstage, the Encoders and Decoders are frozen, and the Translator captures\ninter-variable interactions for prediction. Besides, by introducing a\nself-attention mechanism for multivariable fusion in the latent space, the\nperformance achieves further improvements. Empirically, extensive experiments\nshow the state-of-the-art performance of our method. Specifically, it reduces\nthe MSE for near-surface air temperature and relative humidity predictions by\n28.82\\% and 23.39\\%, respectively. The source code is available at\nhttps://github.com/ShremG/Met2Net.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17189v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Met$^2$Net: 一种用于复杂气象系统的解耦两阶段时空预测模型", "tldr": "Met$^2$Net提出了一种解耦的两阶段训练方法，通过为每个变量配置独立的编码器和解码器，并在潜空间中引入自注意力机制来解决现有端到端方法在多变量气象预测中存在的表示不一致和变量间依赖捕获不足的问题，实现了最先进的预测性能。", "motivation": "全球气候变化导致极端天气事件日益频繁，迫切需要准确的天气预报。当前深度学习驱动的端到端方法在多变量集成中存在表示不一致性，并且难以有效捕获复杂天气系统中变量间的依赖关系。虽然多模态模型的两阶段训练可以部分缓解，但由于训练任务的不一致性，结果往往不理想。", "method": "我们提出了一种隐式两阶段训练方法，为每个变量配置独立的编码器和解码器。在第一阶段，Translator被冻结，编码器和解码器学习共享的潜在空间；在第二阶段，编码器和解码器被冻结，Translator捕获变量间的相互作用以进行预测。此外，通过在潜在空间中引入自注意力机制进行多变量融合，进一步提升了性能。", "result": "实验结果表明，我们的方法达到了最先进的性能。具体而言，它将近地表气温和相对湿度的预测MSE分别降低了28.82%和23.39%。", "conclusion": "Met$^2$Net通过其解耦的两阶段训练和自注意力机制，成功解决了复杂气象系统中多变量预测的挑战，显著提升了预测精度，为准确天气预报提供了新的有效方案。", "translation": "全球气候变化导致极端天气事件日益频繁，这迫切需要准确的天气预报。最近，得益于深度学习技术，端到端方法取得了巨大进展，但它们在多变量集成中面临表示不一致的局限性，并且难以有效捕获复杂天气系统所需的变量间依赖关系。将不同变量视为不同的模态并应用多模态模型的两阶段训练方法可以部分缓解这个问题，但由于两阶段训练任务的不一致性，结果往往不尽如人意。为了解决这些挑战，我们提出了一种隐式两阶段训练方法，为每个变量配置了独立的编码器和解码器。具体来说，在第一阶段，Translator被冻结，而编码器和解码器学习共享的潜在空间；在第二阶段，编码器和解码器被冻结，Translator捕获变量间的相互作用以进行预测。此外，通过在潜在空间中引入自注意力机制进行多变量融合，性能得到了进一步提升。经验上，大量的实验表明我们的方法达到了最先进的性能。具体而言，它将近地表气温和相对湿度预测的MSE分别降低了28.82%和23.39%。源代码可在https://github.com/ShremG/Met2Net获取。", "summary": "Met$^2$Net是一种针对复杂气象系统的解耦两阶段时空预测模型，旨在解决现有端到端方法在多变量集成中存在的表示不一致性和变量间依赖捕获不足的问题。该模型通过为每个变量配备独立的编码器和解码器，并采用隐式两阶段训练：第一阶段学习共享潜在空间，第二阶段捕获变量间交互。此外，引入自注意力机制进行潜在空间的多变量融合。实验证明，Met$^2$Net在近地表气温和相对湿度预测上显著降低了MSE，达到了最先进的性能。", "keywords": "气象预测, 时空模型, 两阶段训练, 自注意力, 多变量融合", "comments": "Met$^2$Net的创新之处在于其解耦的两阶段训练范式和针对多变量预测的自注意力融合机制。这种方法有效地解决了传统端到端模型在处理复杂气象系统多变量时遇到的表示不一致和依赖捕获不足的问题，为高精度气象预报提供了有价值的新思路。"}}
{"id": "2507.17351", "title": "Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field", "authors": ["Yuzhe Zhu", "Lile Cai", "Kangkang Lu", "Fayao Liu", "Xulei Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICME 2025", "url": "http://arxiv.org/abs/2507.17351v1", "summary": "Neural Radiance Field (NeRF) models are implicit neural scene representation\nmethods that offer unprecedented capabilities in novel view synthesis.\nSemantically-aware NeRFs not only capture the shape and radiance of a scene,\nbut also encode semantic information of the scene. The training of\nsemantically-aware NeRFs typically requires pixel-level class labels, which can\nbe prohibitively expensive to collect. In this work, we explore active learning\nas a potential solution to alleviate the annotation burden. We investigate\nvarious design choices for active learning of semantically-aware NeRF,\nincluding selection granularity and selection strategies. We further propose a\nnovel active learning strategy that takes into account 3D geometric constraints\nin sample selection. Our experiments demonstrate that active learning can\neffectively reduce the annotation cost of training semantically-aware NeRF,\nachieving more than 2X reduction in annotation cost compared to random\nsampling.", "comment": "Accepted to ICME 2025", "pdf_url": "http://arxiv.org/pdf/2507.17351v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "探索主动学习用于语义神经辐射场的标签高效训练", "tldr": "本文探讨了主动学习如何有效降低语义神经辐射场（NeRF）训练中的像素级标签标注成本，并通过考虑3D几何约束提出了一种新颖的主动学习策略，实现了超过2倍的标注成本降低。", "motivation": "训练语义感知的神经辐射场（NeRF）通常需要昂贵的像素级类别标签，这带来了巨大的数据标注负担。", "method": "本文探索了主动学习作为减轻标注负担的潜在解决方案。研究了语义感知NeRF主动学习的各种设计选择，包括选择粒度和选择策略。此外，提出了一种新颖的主动学习策略，该策略在样本选择中考虑了3D几何约束。", "result": "实验证明，主动学习可以有效降低语义感知NeRF训练的标注成本，与随机采样相比，标注成本降低了2倍以上。", "conclusion": "主动学习能够显著减少语义神经辐射场训练所需的标注成本，尤其通过结合3D几何约束，其效率远超传统方法。", "translation": "神经辐射场（NeRF）模型是隐式神经场景表示方法，在 Novel View Synthesis 方面提供了前所未有的能力。语义感知的NeRFs不仅能捕捉场景的形状和辐射，还能编码场景的语义信息。语义感知NeRFs的训练通常需要像素级的类别标签，这可能非常昂贵。在这项工作中，我们探索主动学习作为减轻标注负担的潜在解决方案。我们研究了语义感知NeRF主动学习的各种设计选择，包括选择粒度和选择策略。我们进一步提出了一种新颖的主动学习策略，该策略在样本选择中考虑了3D几何约束。我们的实验表明，主动学习可以有效降低训练语义感知NeRF的标注成本，与随机采样相比，标注成本降低了2倍以上。", "summary": "本文研究了主动学习在降低语义神经辐射场（NeRF）训练中像素级标签标注成本方面的应用。通过探索不同的设计选择并提出一种考虑3D几何约束的新型主动学习策略，实验证明该方法能够显著减少标注需求，相较于随机采样，标注成本降低了超过两倍。", "keywords": "主动学习, 神经辐射场, 语义分割, 标签效率, 3D几何约束", "comments": "该论文的创新点在于将主动学习引入到语义神经辐射场的训练中，以解决高昂的像素级标注成本问题。特别是，提出的结合3D几何约束的主动学习策略是一个新颖且有前景的方向，它利用了NeRF模型固有的3D特性来优化样本选择。这项工作对于推动NeRF在实际应用中的可扩展性具有重要意义。"}}
{"id": "2507.17618", "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "categories": ["cs.CL", "cs.PF"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17618v1", "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17618v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于空间对齐解码（SPADE）的大型语言模型混合早退算法", "tldr": "SPADE是一种混合早退算法，通过空间对齐解码和置信度监控，有效降低大型语言模型的推理成本同时保持高准确性。", "motivation": "大型语言模型（LLMs）因其深层结构而计算成本高昂。现有的早退算法虽然能降低推理成本，但由于中间层与输出层表示之间的不对齐导致解码不准确，从而影响性能。", "method": "本文提出SPADE（空间对齐解码），一种新的解码方法，通过传播由起始标记和答案标记组成的最小化序列，将中间层表示与输出层对齐。此外，通过训练SPADE的线性近似来计算基于熵的置信度指标，优化了早退决策过程。将两者结合，创建了一种混合早退算法，该算法在监控置信度水平的同时，利用SPADE生成高质量输出。", "result": "该方法显著降低了推理成本，且没有损害准确性。", "conclusion": "本研究为在实际应用中部署大型语言模型提供了一个可扩展且高效的解决方案。", "translation": "大型语言模型因其深层结构而计算成本高昂。先前的研究表明，中间层包含足够的信息来生成准确的答案，这促使了通过在较早层终止计算来降低推理成本的早退算法的发展。然而，这些方法通常因中间层与输出层表示之间的不对齐导致解码不准确而性能不佳。为了解决这些挑战，我们提出了SPADE（空间对齐解码），一种新颖的解码方法，通过传播仅由起始标记和答案标记组成的最小化序列，将中间层表示与输出层对齐。我们通过训练SPADE的线性近似来计算基于熵的置信度指标，进一步优化了早退决策过程。将两者结合，我们创建了一种混合早退算法，该算法监控置信度水平并在中间层停止推理，同时使用SPADE生成高质量输出。这种方法显著降低了推理成本，且不损害准确性，为在实际应用中部署大型语言模型提供了一个可扩展且高效的解决方案。", "summary": "本文提出了一种名为SPADE（空间对齐解码）的混合早退算法，旨在解决大型语言模型计算成本高昂以及现有早退算法因中间层表示不对齐导致性能下降的问题。SPADE通过对齐中间层和输出层表示来提高解码准确性，并通过训练线性近似来优化早退决策的置信度评估。该混合算法结合了置信度监控和SPADE解码，实现了在不牺牲准确性的前提下显著降低大型语言模型的推理成本，为LLMs的实际部署提供了高效且可扩展的解决方案。", "keywords": "早退算法, 大型语言模型, 空间对齐解码, 推理成本, SPADE", "comments": "该论文的创新点在于提出了SPADE方法，有效解决了早退算法中中间层与输出层表示不对齐导致的解码精度问题，从而提升了早退算法的实用性。其重要性在于为计算资源受限环境下部署大型语言模型提供了一个高效且准确的解决方案，具有显著的实际应用价值。"}}
{"id": "2402.03812", "title": "FDO Manager: Minimum Viable FAIR Digital Object Implementation", "authors": ["Oussama Zoubia", "Nagaraj Bahubali Asundi", "Adamantios Koumpis", "Christoph Lange", "Sezin Dogan", "Oya Beyan", "Zeyd Boukhers"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Published at International FAIR Digital Objects Implementation Summit 2024, with the DOI: this https URL", "url": "http://arxiv.org/abs/2402.03812v2", "summary": "In the digital age, data has emerged as one of the most valuable assets\nacross various sectors, including academia, industry, and healthcare. Effective\ndata preservation involves the management of data to ensure its long-term\naccessibility and usability. Given the importance and sensitivity of data, the\nneed for effective management is a crucial necessity. One of the big recent\nproposed approaches for data management is the FAIR Digital Objects (FDOs)\nwhich has emerged to revolutionize the field of data management and\npreservation. Central to this revolution is the alignment of FDOs with the FAIR\nprinciples (Findable, Accessible, Interoperable, Reusable), particularly\nemphasizing machine-actionability and interoperability across diverse data\necosystems. This paper presents \"FDO Manager\" a Minimum Viable Implementation\nof FDOs, tailored specifically for the use case and field of research artefacts\nsuch as datasets, publications, and code. The paper discusses the core ideas\nbehind the FDO Manager, its architecture, usage and implementation details, as\nwell as its potential impact, demonstrating a simple and abstract\nimplementation of FDOs in the research realm.", "comment": "Published at International FAIR Digital Objects Implementation Summit\n  2024, with the DOI: https://doi.org/10.52825/ocp.v5i.1421", "pdf_url": "http://arxiv.org/pdf/2402.03812v2", "cate": "cs.DC", "date": "2024-02-06", "updated": "2025-07-23", "AI": {"title_translation": "FDO Manager：最小可行性FAIR数字对象实现", "tldr": "本文介绍了“FDO Manager”，一个针对研究成果（如数据集、出版物、代码）的FAIR数字对象（FDO）的最小可行性实现，旨在促进数据管理和保存，并使其符合FAIR原则。", "motivation": "在数字时代，数据已成为各领域最有价值的资产之一。有效的数据保存和管理至关重要，特别是为了确保数据的长期可访问性和可用性。FAIR数字对象（FDOs）作为一种新的数据管理方法被提出，旨在彻底改变数据管理和保存领域，并强调机器可操作性和跨数据生态系统的互操作性。", "method": "本文提出了“FDO Manager”，它是FAIR数字对象（FDOs）的一个最小可行性实现，专门为研究成果（如数据集、出版物和代码）的使用案例和领域量身定制。论文讨论了FDO Manager的核心思想、架构、使用和实现细节。", "result": "FDO Manager展示了FAIR数字对象在研究领域的一个简单且抽象的实现。", "conclusion": "本文通过FDO Manager展示了FAIR数字对象在研究领域实现的可行性，并讨论了其潜在影响，有助于推动符合FAIR原则的数据管理和保存。", "translation": "在数字时代，数据已成为学术界、工业界和医疗保健等各个领域最有价值的资产之一。有效的数据保存涉及数据管理，以确保其长期可访问性和可用性。鉴于数据的重要性和敏感性，有效管理的必要性至关重要。最近提出的一种重要数据管理方法是FAIR数字对象（FDOs），它已出现并彻底改变了数据管理和保存领域。这场革命的核心是FDOs与FAIR原则（可查找、可访问、可互操作、可重用）的对齐，特别强调跨不同数据生态系统的机器可操作性和互操作性。本文介绍了“FDO Manager”，它是FDOs的一个最小可行性实现，专门为研究成果（如数据集、出版物和代码）的使用案例和领域量身定制。论文讨论了FDO Manager的核心思想、架构、使用和实现细节，以及其潜在影响，展示了FDOs在研究领域的一个简单且抽象的实现。", "summary": "本文介绍了“FDO Manager”，这是一个针对研究成果（如数据集、出版物、代码）的FAIR数字对象（FDOs）的最小可行性实现。该实现旨在解决数字时代数据管理和保存的关键需求，通过将FDOs与FAIR原则对齐，特别是强调机器可操作性和跨数据生态系统的互操作性。论文详细阐述了FDO Manager的核心理念、架构、使用方法和实现细节，并展示了FDOs在研究领域的一个简单抽象实现及其潜在影响。", "keywords": "FAIR数字对象, 数据管理, 数据保存, FDO Manager, 研究成果", "comments": "这篇论文的创新点在于提出了一个“最小可行性实现”的FDO管理器，这对于FAIR原则在实际研究数据管理中的落地具有重要意义。它提供了一个具体的、可操作的框架，有助于推动研究数据实现“可查找、可访问、可互操作、可重用”。其重要性在于，通过降低FDOs的实现门槛，可以加速FAIR原则在科研领域的普及和应用。"}}
{"id": "2507.16877", "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension", "authors": ["Yizhi Hu", "Zezhao Tian", "Xingqun Qi", "Chen Su", "Bingkun Yang", "Junhui Yin", "Muyi Sun", "Man Zhang", "Zhenan Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures", "url": "http://arxiv.org/abs/2507.16877v1", "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.16877v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "ReMeREC：关系感知和多实体指代表达式理解", "tldr": "ReMeREC 框架通过引入新的数据集 ReMeX 以及 TMP 和 EIR 组件，解决了多实体指代表达式理解中实体间关系处理和语义歧义的挑战，达到了最先进的性能。", "motivation": "现有指代表达式理解（REC）方法在处理多实体场景中复杂的实体间关系时表现不足，且缺乏高质量的、带有细粒度图像-文本-关系配对标注的数据集，这限制了它们的准确性和可靠性。", "method": "本文首先构建了一个名为 ReMeX 的关系感知、多实体 REC 数据集，其中包含详细的关系和文本标注。然后提出了 ReMeREC 框架，它联合利用视觉和文本线索来定位多个实体并建模它们的实体间关系。为解决语言中隐式实体边界引起的语义歧义，引入了文本自适应多实体感知器（TMP），动态推断实体的数量和范围。此外，还设计了实体间关系推理器（EIR）以增强关系推理和全局场景理解。为进一步提高细粒度提示的语言理解能力，还使用大型语言模型构建了一个小型辅助数据集 EntityText。", "result": "在四个基准数据集上的实验表明，ReMeREC 在多实体定位和关系预测方面取得了最先进的性能，大幅优于现有方法。", "conclusion": "ReMeREC 通过引入新颖的框架和专用数据集，有效解决了多实体指代表达式理解的挑战，显著提升了定位和关系预测的性能。", "translation": "指代表达式理解（REC）旨在根据自然语言描述定位图像中指定的实体或区域。现有方法虽然能处理单实体定位，但往往忽略多实体场景中复杂的实体间关系，从而限制了它们的准确性和可靠性。此外，缺乏高质量的、具有细粒度图像-文本-关系配对标注的数据集也阻碍了进一步的进展。为了解决这一挑战，我们首先构建了一个关系感知、多实体REC数据集，名为ReMeX，其中包括详细的关系和文本标注。然后，我们提出了ReMeREC，一个新颖的框架，它共同利用视觉和文本线索来定位多个实体，同时建模它们的实体间关系。为了解决语言中隐式实体边界引起的语义歧义，我们引入了文本自适应多实体感知器（TMP），它根据细粒度文本线索动态推断实体的数量和范围，产生独特的表示。此外，我们的实体间关系推理器（EIR）增强了关系推理和全局场景理解。为了进一步提高对细粒度提示的语言理解能力，我们还构建了一个小型辅助数据集EntityText，该数据集使用大型语言模型生成。在四个基准数据集上的实验表明，ReMeREC在多实体定位和关系预测方面取得了最先进的性能，大幅优于现有方法。", "summary": "本文提出了 ReMeREC，一个针对关系感知和多实体指代表达式理解（REC）的新颖框架，旨在解决现有方法在处理实体间关系方面的局限性以及相关数据集的缺乏。该研究构建了包含详细关系标注的 ReMeX 数据集，并提出了 ReMeREC 框架，该框架包含用于处理语义歧义的文本自适应多实体感知器（TMP）和用于关系推理的实体间关系推理器（EIR）。此外，还利用大型语言模型创建了一个辅助数据集 EntityText。实验证明 ReMeREC 在多实体定位和关系预测方面达到了最先进的性能。", "keywords": "指代表达式理解, 多实体, 关系感知, 数据集, 视觉定位", "comments": "这篇论文通过识别并解决 REC 中经常被忽视的多实体关系挑战，超越了单实体定位的局限性，做出了重要贡献。引入专用数据集（ReMeX）和专门框架（包含 TMP 和 EIR 的 ReMeREC）直接解决了已识别的局限性，提供了数据和模型两方面的创新。利用大型语言模型生成辅助数据集以增强语言理解也是一种有趣的方法。报告的最先进结果表明，该方法在该复杂任务中取得了显著的改进。"}}
{"id": "2502.13294", "title": "The \"Who\", \"What\", and \"How\" of Responsible AI Governance: A Systematic Review and Meta-Analysis of (Actor, Stage)-Specific Tools", "authors": ["Blaine Kuehnert", "Rachel M. Kim", "Jodi Forlizzi", "Hoda Heidari"], "categories": ["cs.CY", "cs.HC", "I.2; A.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Conference on Fairness, Accountability, and Transparency 2025. 15 pages, 3 figures", "url": "http://arxiv.org/abs/2502.13294v2", "summary": "The implementation of responsible AI in an organization is inherently complex\ndue to the involvement of multiple stakeholders, each with their unique set of\ngoals and responsibilities across the entire AI lifecycle. These\nresponsibilities are often ambiguously defined and assigned, leading to\nconfusion, miscommunication, and inefficiencies. Even when responsibilities are\nclearly defined and assigned to specific roles, the corresponding AI actors\nlack effective tools to support their execution.\n  Toward closing these gaps, we present a systematic review and comprehensive\nmeta-analysis of the current state of responsible AI tools, focusing on their\nalignment with specific stakeholder roles and their responsibilities in various\nAI lifecycle stages. We categorize over 220 tools according to AI actors and\nstages they address. Our findings reveal significant imbalances across the\nstakeholder roles and lifecycle stages addressed. The vast majority of\navailable tools have been created to support AI designers and developers\nspecifically during data-centric and statistical modeling stages while\nneglecting other roles such as institutional leadership, deployers, end-users,\nand impacted communities, and stages such as value proposition and deployment.\nThe uneven distribution we describe here highlights critical gaps that\ncurrently exist in responsible AI governance research and practice. Our\nanalysis reveals that despite the myriad of frameworks and tools for\nresponsible AI, it remains unclear \\emph{who} within an organization and\n\\emph{when} in the AI lifecycle a tool applies. Furthermore, existing tools are\nrarely validated, leaving critical gaps in their usability and effectiveness.\nThese gaps provide a starting point for researchers and practitioners to create\nmore effective and holistic approaches to responsible AI development and\ngovernance.", "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency 2025. 15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2502.13294v2", "cate": "cs.CY", "date": "2025-02-18", "updated": "2025-07-23", "AI": {"title_translation": "负责任AI治理的“谁”、“什么”和“如何”：一项关于（行动者，阶段）特定工具的系统综述和元分析", "tldr": "本研究对负责任AI工具进行了系统综述和元分析，发现现有工具在支持不同利益相关者角色和AI生命周期阶段方面存在严重不平衡，并指出在工具适用性和有效性方面存在关键空白。", "motivation": "负责任AI的实施在组织中是复杂的，因为涉及多个利益相关者，他们的职责常常模糊不清，导致混乱和低效。即使职责明确，AI行动者也缺乏有效的工具来支持其执行。本研究旨在弥补这些空白。", "method": "本研究对当前负责任AI工具的状态进行了系统综述和全面的元分析，重点关注这些工具与特定利益相关者角色及其在AI生命周期不同阶段的职责的契合度。研究根据AI行动者和其所针对的阶段对220多个工具进行了分类。", "result": "研究发现现有工具在所针对的利益相关者角色和生命周期阶段之间存在显著不平衡。绝大多数可用工具旨在支持AI设计师和开发者在数据中心和统计建模阶段，而忽视了机构领导、部署者、终端用户和受影响社区等其他角色，以及价值主张和部署等阶段。分析还表明，尽管负责任AI的框架和工具繁多，但仍不清楚组织内“谁”以及AI生命周期中“何时”适用某个工具。此外，现有工具很少经过验证，导致其可用性和有效性存在关键空白。", "conclusion": "负责任AI治理的研究和实践中存在关键空白，尤其是在工具对不同利益相关者和生命周期阶段的支持方面。这些空白为研究人员和实践者提供了起点，以创建更有效和全面的负责任AI开发和治理方法。", "translation": "在组织中实施负责任的人工智能本质上是复杂的，因为涉及多个利益相关者，每个利益相关者在整个AI生命周期中都有其独特的M目标和职责。这些职责往往定义和分配模糊，导致混乱、误解和低效率。即使职责明确定义并分配给特定角色，相应的AI行动者也缺乏有效的工具来支持其执行。\n\n为了弥补这些空白，我们对当前负责任的AI工具状态进行了系统综述和全面的元分析，重点关注它们与特定利益相关者角色及其在AI生命周期各个阶段的职责的契合度。我们根据AI行动者和它们所针对的阶段对220多个工具进行了分类。我们的发现揭示了在所针对的利益相关者角色和生命周期阶段之间存在显著的不平衡。绝大多数可用工具旨在专门支持AI设计师和开发者在以数据为中心和统计建模阶段，而忽视了机构领导、部署者、最终用户和受影响社区等其他角色，以及价值主张和部署等阶段。我们在此描述的不均匀分布凸显了当前负责任AI治理研究和实践中存在的关键空白。我们的分析揭示，尽管负责任AI的框架和工具繁多，但仍不清楚一个工具在组织内适用于“谁”以及在AI生命周期中适用于“何时”。此外，现有工具很少经过验证，导致其可用性和有效性存在关键空白。这些空白为研究人员和实践者提供了起点，以创建更有效和全面的负责任AI开发和治理方法。", "summary": "本论文通过系统综述和元分析，深入探讨了负责任AI工具的现状。研究发现，尽管负责任AI的实施复杂且涉及多方，但现有工具在支持不同利益相关者角色（如领导、部署者、用户和受影响社区）和AI生命周期阶段（如价值主张和部署）方面存在显著不平衡。大多数工具侧重于AI设计师和开发者在数据和建模阶段。此外，论文指出现有工具的适用性不明确且缺乏验证，这在负责任AI治理的研究和实践中构成了关键挑战，为未来更全面和有效的工具开发指明了方向。", "keywords": "负责任AI, AI治理, 系统综述, 元分析, 利益相关者, AI生命周期", "comments": "本研究通过对负责任AI工具进行系统综述和元分析，揭示了当前AI治理实践中的关键空白和不平衡，特别是在工具支持的“谁”和“何时”方面。其创新之处在于对220多个工具的细致分类和对现有工具局限性的深入剖析，为负责任AI的未来发展和治理提供了清晰的研究方向和实践起点，强调了开发更全面、经过验证且能覆盖AI全生命周期和所有利益相关者的工具的重要性。此项研究对于推动负责任AI的实际落地具有重要指导意义。"}}
{"id": "2507.17587", "title": "A Joint Planning Model for Fixed and Mobile Electric Vehicle Charging Stations Considering Flexible Capacity Strategy", "authors": ["Zhe Yu", "Xue Hu", "Qin Wang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17587v1", "summary": "The widespread adoption of electric vehicles (EVs) has significantly\nincreased demand on both transportation and power systems, posing challenges to\ntheir stable operation. To support the growing need for EV charging, both fixed\ncharging stations (FCSs) and mobile charging stations (MCSs) have been\nintroduced, serving as key interfaces between the power grid and traffic\nnetwork. Recognizing the importance of collaborative planning across these\nsectors, this paper presents a two-stage joint planning model for FCSs and\nMCSs, utilizing an improved alternating direction method of multipliers (ADMM)\nalgorithm. The primary goal of the proposed model is to transform the potential\nnegative impacts of large-scale EV integration into positive outcomes, thereby\nenhancing social welfare through collaboration among multiple stakeholders. In\nthe first stage, we develop a framework for evaluating FCS locations,\nincorporating assessments of EV hosting capacity and voltage stability. The\nsecond stage introduces a joint planning model for FCSs and MCSs, aiming to\nminimize the overall social costs of the EV charging system while maintaining a\nreliable power supply. To solve the planning problem, we employ a combination\nof mixed-integer linear programming, queueing theory, and sequential quadratic\nprogramming. The improved ADMM algorithm couples the siting and sizing\ndecisions consistently by introducing coupling constraints, and supports a\ndistributed optimization framework that coordinates the interests of EV users,\nMCS operators, and distribution system operators. Additionally, a flexible\ncapacity planning strategy that accounts for the multi-period development\npotential of EVCS is proposed to reduce both the complexity and the investment\nrequired for FCS construction. Finally, a case study with comparative\nexperiments demonstrates the effectiveness of the proposed models and solution\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17587v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "考虑柔性容量策略的固定式与移动式电动汽车充电站联合规划模型", "tldr": "该论文提出了一个两阶段联合规划模型，用于固定式和移动式电动汽车充电站，旨在通过改进的ADMM算法和柔性容量策略，最小化社会总成本并提升社会福利，以应对大规模电动汽车集成带来的挑战。", "motivation": "电动汽车的广泛普及显著增加了交通和电力系统的负荷，对其稳定运行构成挑战。为了支持不断增长的电动汽车充电需求，并期望将大规模电动汽车集成可能带来的负面影响转化为积极成果，从而通过多方利益相关者的协作来提升社会福利。", "method": "本文提出了一个两阶段的固定式充电站（FCS）和移动式充电站（MCS）联合规划模型，并采用改进的交替方向乘子法（ADMM）算法。第一阶段，构建了评估FCS位置的框架，其中包含了电动汽车承载能力和电压稳定性评估。第二阶段，引入了FCS和MCS的联合规划模型，旨在最小化电动汽车充电系统的整体社会成本，同时保持可靠的电力供应。为解决规划问题，该研究结合了混合整数线性规划、排队论和序列二次规划。改进的ADMM算法通过引入耦合约束，一致地耦合了选址和规模决策，并支持一个分布式优化框架，协调了电动汽车用户、MCS运营商和配电系统运营商的利益。此外，还提出了一种考虑电动汽车充电站（EVCS）多周期发展潜力的柔性容量规划策略，以降低FCS建设的复杂性和投资。", "result": "通过案例研究和对比实验，证明了所提出的模型和求解方法的有效性。", "conclusion": "所提出的模型和求解方法能够有效地进行固定式和移动式电动汽车充电站的联合规划，并通过最小化社会成本和提升社会福利来管理大规模电动汽车集成的影响，尤其是在柔性容量策略的考量下。", "translation": "电动汽车（EV）的广泛普及显著增加了交通和电力系统的需求，对其稳定运行构成了挑战。为了支持日益增长的电动汽车充电需求，固定式充电站（FCS）和移动式充电站（MCS）均已投入使用，它们作为电网和交通网络之间的关键接口。认识到这些部门之间协作规划的重要性，本文提出了一个用于FCS和MCS的两阶段联合规划模型，并利用改进的交替方向乘子法（ADMM）算法。所提出模型的主要目标是将大规模电动汽车集成可能带来的负面影响转化为积极成果，从而通过多方利益相关者的协作来提升社会福利。在第一阶段，我们开发了一个评估FCS位置的框架，其中包含了电动汽车承载能力和电压稳定性评估。第二阶段引入了FCS和MCS的联合规划模型，旨在最小化电动汽车充电系统的整体社会成本，同时保持可靠的电力供应。为解决规划问题，我们结合了混合整数线性规划、排队论和序列二次规划。改进的ADMM算法通过引入耦合约束，一致地耦合了选址和规模决策，并支持一个分布式优化框架，协调了电动汽车用户、MCS运营商和配电系统运营商的利益。此外，还提出了一种考虑电动汽车充电站（EVCS）多周期发展潜力的柔性容量规划策略，以降低FCS建设的复杂性和投资。最后，通过案例研究和对比实验证明了所提出的模型和求解方法的有效性。", "summary": "本文针对电动汽车普及对交通和电力系统造成的压力，提出了一种两阶段的固定式和移动式电动汽车充电站联合规划模型。该模型利用改进的ADMM算法，并结合混合整数线性规划、排队论和序列二次规划，旨在最小化社会总成本并提升社会福利。第一阶段评估固定式充电站的选址，考虑电动汽车承载能力和电压稳定性；第二阶段则对固定式和移动式充电站进行联合规划，并引入柔性容量策略以降低复杂性和投资。案例研究验证了该模型和方法的有效性。", "keywords": "电动汽车, 充电站, 联合规划, 柔性容量, ADMM", "comments": "本文的创新点在于提出了一个综合的两阶段联合规划模型，同时考虑了固定式和移动式电动汽车充电站，这对于应对大规模电动汽车集成带来的复杂挑战具有重要意义。通过引入改进的ADMM算法，实现了分布式优化，并协调了多方利益相关者的需求。此外，柔性容量策略的提出，有效地降低了固定式充电站建设的复杂性和投资，提升了模型的实用性。"}}
{"id": "2507.17323", "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations", "authors": ["Ruijie Yang", "Yan Zhu", "Peiyao Fu", "Yizhe Zhang", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17323v1", "summary": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,\nunderscoring the importance of timely polyp detection and diagnosis. While deep\nlearning models have improved optical-assisted diagnostics, they often demand\nextensive labeled datasets and yield \"black-box\" outputs with limited\ninterpretability. In this paper, we propose EndoFinder, an online polyp\nretrieval framework that leverages multi-view scene representations for\nexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image\nEncoder by combining contrastive learning and a reconstruction task, guided by\npolyp segmentation masks. This self-supervised approach captures robust\nfeatures without relying on large-scale annotated data. Next, we treat each\npolyp as a three-dimensional \"scene\" and introduce a Scene Representation\nTransformer, which fuses multiple views of the polyp into a single latent\nrepresentation. By discretizing this representation through a hashing layer,\nEndoFinder enables real-time retrieval from a compiled database of historical\npolyp cases, where diagnostic information serves as interpretable references\nfor new queries. We evaluate EndoFinder on both public and newly collected\npolyp datasets for re-identification and pathology classification. Results show\nthat EndoFinder outperforms existing methods in accuracy while providing\ntransparent, retrieval-based insights for clinical decision-making. By\ncontributing a novel dataset and a scalable, explainable framework, our work\naddresses key challenges in polyp diagnosis and offers a promising direction\nfor more efficient AI-driven colonoscopy workflows. The source code is\navailable at https://github.com/ku262/EndoFinder-Scene.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17323v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "EndoFinder：利用潜在场景表示进行可解释结直肠息肉诊断的在线病变检索", "tldr": "EndoFinder是一个在线息肉检索框架，它利用多视图场景表示进行可解释且可扩展的结直肠癌诊断，在准确性上优于现有方法并提供透明的临床决策洞察。", "motivation": "结直肠癌是癌症相关死亡的主要原因，及时发现和诊断息肉至关重要。深度学习模型虽然提高了诊断效率，但通常需要大量标记数据集且输出为“黑箱”，解释性有限。", "method": "本文提出了EndoFinder，一个在线息肉检索框架。首先，通过结合对比学习和重建任务，并由息肉分割掩模引导，开发了一个息肉感知图像编码器，以自监督方式捕获鲁棒特征。其次，将每个息肉视为三维“场景”，引入场景表示转换器将息肉的多个视图融合为单个潜在表示。通过哈希层离散化此表示，EndoFinder能够从历史息肉病例数据库中实时检索，将诊断信息作为新查询的可解释参考。", "result": "EndoFinder在息肉再识别和病理分类任务上，在公共和新收集的息肉数据集上进行了评估，结果显示其在准确性上优于现有方法，同时为临床决策提供了透明的、基于检索的洞察。", "conclusion": "该工作通过贡献新数据集和可扩展、可解释的框架，解决了息肉诊断中的关键挑战，并为更高效的AI驱动结肠镜检查工作流程提供了有前景的方向。", "translation": "结直肠癌 (CRC) 仍然是癌症相关死亡的主要原因，这凸显了及时息肉检测和诊断的重要性。虽然深度学习模型改进了光学辅助诊断，但它们通常需要大量标记数据集并产生解释性有限的“黑箱”输出。在本文中，我们提出了 EndoFinder，一个在线息肉检索框架，它利用多视图场景表示进行可解释和可扩展的 CRC 诊断。首先，我们通过结合对比学习和重建任务，并在息肉分割掩模的指导下，开发了一个息肉感知图像编码器。这种自监督方法无需依赖大规模标注数据即可捕获鲁棒特征。接下来，我们将每个息肉视为一个三维“场景”，并引入了一个场景表示转换器，该转换器将息肉的多个视图融合成一个单一的潜在表示。通过哈希层对这种表示进行离散化，EndoFinder 能够从历史息肉病例的编译数据库中进行实时检索，其中诊断信息作为新查询的可解释参考。我们在公共和新收集的息肉数据集上评估了 EndoFinder 的再识别和病理分类性能。结果表明，EndoFinder 在准确性方面优于现有方法，同时为临床决策提供了透明的、基于检索的见解。通过贡献一个新的数据集和一个可扩展、可解释的框架，我们的工作解决了息肉诊断中的关键挑战，并为更高效的 AI 驱动结肠镜检查工作流程提供了有前景的方向。源代码可在 https://github.com/ku262/EndoFinder-Scene 获取。", "summary": "本文提出了EndoFinder，一个用于结直肠息肉诊断的在线检索框架，旨在解决深度学习模型缺乏解释性的问题。该框架通过结合自监督的息肉感知图像编码器和场景表示转换器，将息肉的多视图信息整合为可检索的潜在表示。EndoFinder能够从历史病例数据库中实时检索相似病变并提供可解释的诊断参考。实验证明，EndoFinder在准确性上优于现有方法，并为临床决策提供了透明的洞察，为AI驱动的结肠镜检查工作流程提供了新的方向。", "keywords": "结直肠息肉诊断, 可解释AI, 在线检索, 场景表示, 自监督学习", "comments": "本文的创新点在于提出了一个结合多视图场景表示和在线检索的可解释AI诊断框架，解决了现有深度学习模型“黑箱”问题。通过自监督学习减少对大规模标注数据的依赖，并通过哈希层实现实时检索，具有良好的可扩展性和实用性，对临床决策支持具有重要意义。"}}
{"id": "2507.12103", "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7pages, 4 figures. Accepted to IJCAI 2025", "url": "http://arxiv.org/abs/2507.12103v2", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.12103v2", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "DeepShade：通过文本条件图像生成实现阴影模拟", "tldr": "DeepShade是一个扩散模型，通过文本条件图像生成来模拟阴影，解决了现有路由系统无法整合阴影信息的问题，并为城市规划和路线规划提供帮助。", "motivation": "热浪对公共健康构成重大威胁，全球变暖加剧了这一问题。当前的路由系统（例如在线地图）未能整合阴影信息，原因在于难以直接从嘈杂的卫星图像中估计阴影，以及生成模型训练数据的有限性。", "method": "本文通过两项主要贡献来解决这些挑战。首先，构建了一个包含不同经纬度区域、不同建筑密度和不同城市布局的广泛数据集。利用基于Blender的3D模拟以及建筑轮廓，捕捉了全年不同太阳天顶角和一天中不同时间下的建筑阴影，并将其与卫星图像对齐。其次，提出了DeepShade，这是一个基于扩散的模型，旨在学习和合成随时间变化的阴影。它通过联合考虑RGB和Canny边缘层来强调边缘特征的细微差别，并结合对比学习来捕捉阴影的时间变化规则。然后，通过以已知条件（例如一天中的时间、太阳角度）的文本描述为条件，该框架在生成阴影图像方面提供了改进的性能。", "result": "通过使用阴影预测来计算亚利桑那州坦佩市实际路线规划中的阴影比率，证明了该方法的实用性，并在生成阴影图像方面提供了改进的性能。", "conclusion": "这项工作通过为极端炎热天气下的城市规划提供参考及其在环境中的潜在实际应用，将造福社会。", "translation": "热浪对公共健康构成重大威胁，尤其是在全球变暖加剧的情况下。然而，当前的路由系统（例如在线地图）未能整合阴影信息，原因在于难以直接从嘈杂的卫星图像中估计阴影，以及生成模型训练数据的有限性。在本文中，我们通过两项主要贡献来解决这些挑战。首先，我们构建了一个包含不同经纬度区域、不同建筑密度和不同城市布局的广泛数据集。利用基于Blender的3D模拟以及建筑轮廓，我们捕捉了全年不同太阳天顶角和一天中不同时间下的建筑阴影。这些模拟的阴影与卫星图像对齐，为学习阴影模式提供了丰富的资源。其次，我们提出了DeepShade，这是一个基于扩散的模型，旨在学习和合成随时间变化的阴影。它通过联合考虑RGB和Canny边缘层来强调边缘特征的细微差别，并结合对比学习来捕捉阴影的时间变化规则。然后，通过以已知条件（例如一天中的时间、太阳角度）的文本描述为条件，我们的框架在生成阴影图像方面提供了改进的性能。我们通过使用阴影预测来计算亚利桑那州坦佩市实际路线规划中的阴影比率，证明了我们方法的实用性。我们相信这项工作通过为极端炎热天气下的城市规划提供参考及其在环境中的潜在实际应用，将造福社会。", "summary": "DeepShade提出了一种通过文本条件图像生成来模拟阴影的新方法，旨在解决现有路由系统缺乏阴影信息的问题。该研究首先构建了一个大规模的阴影数据集，该数据集结合了Blender的3D模拟与卫星图像。接着，开发了一个基于扩散的DeepShade模型，该模型通过结合RGB和Canny边缘信息以及对比学习来有效地合成随时间变化的阴影。实验证明，该方法能根据文本描述生成高质量的阴影图像，并成功应用于实际的路线规划，为城市规划和公共健康提供了潜在的益处。", "keywords": "阴影模拟, 文本条件图像生成, 扩散模型, 城市规划, 热浪", "comments": "这项工作通过结合3D模拟生成大规模高质量数据集和创新的扩散模型（DeepShade），有效地解决了阴影模拟的挑战。其亮点在于利用文本条件实现灵活的阴影生成，并强调了边缘特征和时间变化的学习。该研究具有重要的实际应用价值，尤其是在城市规划和应对热浪方面，为提升现有地图和路由系统的实用性提供了新的思路。"}}
{"id": "2507.17270", "title": "Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning", "authors": ["Alessandro Aneggi", "Andrea Janes"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted @ XP2025 Poster session", "url": "http://arxiv.org/abs/2507.17270v1", "summary": "This experience report analyses a one year project focused on building a\ndistributed real-time analytics system using edge computing and machine\nlearning. The project faced critical setbacks due to a big-bang integration\napproach, where all components developed by multiple geographically dispersed\npartners were merged at the final stage. The integration effort resulted in\nonly six minutes of system functionality, far below the expected 40 minutes.\nThrough root cause analysis, the study identifies technical and organisational\nbarriers, including poor communication, lack of early integration testing, and\nresistance to topdown planning. It also considers psychological factors such as\na bias toward fully developed components over mockups. The paper advocates for\nearly mock based deployment, robust communication infrastructures, and the\nadoption of topdown thinking to manage complexity and reduce risk in reactive,\ndistributed projects. These findings underscore the limitations of traditional\nAgile methods in such contexts and propose simulation-driven engineering and\nstructured integration cycles as key enablers for future success.", "comment": "Accepted @ XP2025 Poster session", "pdf_url": "http://arxiv.org/pdf/2507.17270v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从一次“大爆炸”集成中吸取的教训：边缘计算和机器学习中的挑战", "tldr": "本文分析了一个边缘计算和机器学习分布式实时分析项目因“大爆炸”集成方法而失败的经验报告，并提出了早期集成、加强沟通和自上而下规划的建议。", "motivation": "该项目旨在构建一个分布式实时分析系统，但由于采用了“大爆炸”集成方法，导致项目遭遇重大挫折，系统功能远低于预期，因此需要分析失败原因并提出改进建议。", "method": "本文通过经验报告的形式，对一个为期一年的项目进行了分析。通过根本原因分析，识别了技术和组织障碍，并考虑了心理因素。", "result": "“大爆炸”集成导致系统功能仅维持了6分钟，远低于预期的40分钟。根本原因包括沟通不畅、缺乏早期集成测试、抵制自上而下规划以及倾向于完全开发的组件而非模型。这表明传统敏捷方法在此类情境中的局限性。", "conclusion": "为管理复杂性并降低反应式分布式项目中的风险，应提倡基于模型的早期部署、健全的通信基础设施和采用自上而下的思维。模拟驱动工程和结构化集成周期是未来成功的关键促成因素。", "translation": "本经验报告分析了一个为期一年的项目，该项目专注于构建一个使用边缘计算和机器学习的分布式实时分析系统。由于采用了“大爆炸”集成方法，即所有由多个地理分散的合作伙伴开发的组件在最后阶段才合并，该项目面临了严重的挫折。集成工作导致系统功能仅维持了六分钟，远低于预期的四十分钟。通过根本原因分析，该研究确定了技术和组织障碍，包括沟通不畅、缺乏早期集成测试以及抵制自上而下的规划。它还考虑了心理因素，例如偏向于完全开发的组件而非模型。本文倡导早期基于模型的部署、健全的通信基础设施以及采用自上而下的思维来管理复杂性并降低反应式分布式项目中的风险。这些发现强调了传统敏捷方法在此类情境中的局限性，并提出模拟驱动工程和结构化集成周期是未来成功的关键促成因素。", "summary": "本文是一份经验报告，分析了一个为期一年的分布式实时分析项目，该项目结合了边缘计算和机器学习。由于采用了“大爆炸”集成策略，即所有组件在项目后期才进行整合，导致项目遭遇重大挫折，系统功能远低于预期。研究通过根本原因分析，识别了沟通不畅、缺乏早期集成测试、抵制自上而下规划以及对完全开发组件的偏好等技术和组织障碍。文章强调了传统敏捷方法在复杂分布式项目中的局限性，并提出早期基于模型的部署、健全的通信和自上而下的思维，以及模拟驱动工程和结构化集成周期是应对挑战和确保未来成功的关键策略。", "keywords": "边缘计算, 机器学习, 大爆炸集成, 项目管理, 经验报告", "comments": "这篇论文通过一个真实的失败案例，为边缘计算和机器学习领域的复杂分布式项目提供了宝贵的经验教训。其创新之处在于不仅分析了技术和组织障碍，还深入探讨了心理因素的影响。论文提出的早期集成、自上而下规划以及模拟驱动工程等建议，对于管理大型集成项目具有重要的实践指导意义，尤其是在传统敏捷方法可能不足的场景下。"}}
{"id": "2507.17158", "title": "DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing", "authors": ["Bharath Krishnamurthy", "Ajita Rattani"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on Biometrics). 11 pages with references, 8-page main paper with 4 figures and 4 tables. Includes 6 pages of supplementary material with 3 additional figures and 3 tables. Code is available at the official lab repository: this https URL and the author's repository: this https URL", "url": "http://arxiv.org/abs/2507.17158v1", "summary": "Ocular biometrics in the visible spectrum have emerged as a prominent\nmodality due to their high accuracy, resistance to spoofing, and non-invasive\nnature. However, morphing attacks, synthetic biometric traits created by\nblending features from multiple individuals, threaten biometric system\nintegrity. While extensively studied for near-infrared iris and face\nbiometrics, morphing in visible-spectrum ocular data remains underexplored.\nSimulating such attacks demands advanced generation models that handle\nuncontrolled conditions while preserving detailed ocular features like iris\nboundaries and periocular textures. To address this gap, we introduce DOOMGAN,\nthat encompasses landmark-driven encoding of visible ocular anatomy,\nattention-guided generation for realistic morph synthesis, and dynamic\nweighting of multi-faceted losses for optimized convergence. DOOMGAN achieves\nover 20% higher attack success rates than baseline methods under stringent\nthresholds, along with 20% better elliptical iris structure generation and 30%\nimproved gaze consistency. We also release the first comprehensive ocular\nmorphing dataset to support further research in this domain.", "comment": "Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on\n  Biometrics). 11 pages with references, 8-page main paper with 4 figures and 4\n  tables. Includes 6 pages of supplementary material with 3 additional figures\n  and 3 tables. Code is available at the official lab repository:\n  https://github.com/vcbsl/DOOMGAN and the author's repository:\n  https://github.com/Bharath-K3/DOOMGAN", "pdf_url": "http://arxiv.org/pdf/2507.17158v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DOOMGAN：高保真动态身份混淆眼部生成变形", "tldr": "DOOMGAN是一种新的生成对抗网络，用于模拟可见光谱眼部数据的变形攻击，它能显著提高攻击成功率并生成更真实的眼部特征，同时发布了首个全面的眼部变形数据集。", "motivation": "可见光谱眼部生物识别技术虽然准确、防伪且非侵入，但面临变形攻击的威胁。目前，针对近红外虹膜和人脸生物识别的变形攻击研究较多，但可见光谱眼部数据的变形攻击研究不足，且模拟这类攻击需要能处理非受控条件并保留眼部细节的高级生成模型。", "method": "本文提出了DOOMGAN模型，其方法包括：1) 对可见眼部解剖结构进行地标驱动编码；2) 采用注意力引导生成以合成逼真的变形；3) 动态加权多方面损失以优化收敛。此外，还发布了首个全面的眼部变形数据集。", "result": "DOOMGAN在严格阈值下，攻击成功率比基线方法高出20%以上；椭圆形虹膜结构生成效果提高20%；注视一致性提高30%。", "conclusion": "DOOMGAN成功解决了可见光谱眼部数据变形攻击模拟的空白，显著提高了攻击成功率和生成质量，并为该领域未来的研究提供了首个全面的数据集。", "translation": "可见光谱中的眼部生物识别技术因其高准确性、防伪能力和非侵入性而成为一种重要的生物识别方式。然而，变形攻击（通过融合多个个体的特征合成的生物识别特征）威胁着生物识别系统的完整性。尽管近红外虹膜和人脸生物识别的变形攻击已得到广泛研究，但可见光谱眼部数据的变形攻击仍未被充分探索。模拟此类攻击需要先进的生成模型，这些模型需在非受控条件下处理数据，同时保留虹膜边界和眼周纹理等详细的眼部特征。为了解决这一空白，我们引入了DOOMGAN，它包含：地标驱动的可见眼部解剖结构编码、用于逼真变形合成的注意力引导生成，以及用于优化收敛的多方面损失的动态加权。DOOMGAN在严格阈值下比基线方法实现了超过20%的攻击成功率，同时椭圆形虹膜结构生成效果提高了20%，注视一致性提高了30%。我们还发布了首个全面的眼部变形数据集，以支持该领域的进一步研究。", "summary": "本文针对可见光谱眼部生物识别领域中变形攻击模拟的空白，提出了一种名为DOOMGAN的新型生成对抗网络。DOOMGAN通过地标驱动编码、注意力引导生成和动态加权损失，能够合成高保真的眼部变形图像。实验结果表明，DOOMGAN在攻击成功率、虹膜结构生成和注视一致性方面均显著优于现有方法，并为此领域首次发布了全面的眼部变形数据集，以促进后续研究。", "keywords": "眼部生物识别, 变形攻击, 生成对抗网络, 可见光谱, 数据集", "comments": "DOOMGAN的创新之处在于其针对可见光谱眼部变形攻击的专门设计，弥补了该领域的研究空白。其结合了地标驱动编码和注意力机制，确保了生成图像的真实性和细节保留。同时，发布首个全面的眼部变形数据集对于推动该领域的研究具有重要意义。该工作对于理解和防御眼部生物识别系统的潜在威胁具有重要价值。"}}
{"id": "2507.17419", "title": "Power Allocation and RIS Elements Optimisation for Reconfigurable Intelligent Surfaces assisted RSMA", "authors": ["Abdullah Qayyum", "Maziar Nekovee"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17419v1", "summary": "This paper proposes power allocation and the number of reconfigurable\nintelligent surfaces (RIS) elements optimisation in a RIS-assisted rate\nsplitting multiple access (RSMA) system. The optimised RIS-RSMA (ORIS-RSMA)\nmethod determines the optimal number of RIS elements and the power allocation\nfactors for both common and private parts of a message. Additionally, it\nmaximises the sum rate while ensuring that a target common rate is satisfied.\nThe performance of the proposed ORIS-RSMA is compared to that of the\nconventional RIS-RSMA and RSMA. Simulation results show that ORIS-RSMA achieves\na higher sum rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17419v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "可重构智能表面辅助RSMA中的功率分配和RIS单元优化", "tldr": "本文提出了一种优化的RIS-RSMA方法（ORIS-RSMA），通过优化RIS单元数量和功率分配，在满足目标公共速率的同时最大化总和速率，并在仿真中表现出更高的总和速率。", "motivation": "旨在优化RIS辅助RSMA系统中的功率分配和可重构智能表面（RIS）单元数量，以在满足目标公共速率的同时最大化总和速率。", "method": "提出了优化的RIS-RSMA（ORIS-RSMA）方法，该方法确定了RIS单元的最佳数量以及消息公共部分和私有部分的功率分配因子。", "result": "仿真结果表明，ORIS-RSMA方法比传统的RIS-RSMA和RSMA实现了更高的总和速率。", "conclusion": "提出的ORIS-RSMA方法能有效提升RIS辅助RSMA系统的总和速率性能。", "translation": "本文提出了一种在RIS辅助速率分裂多址（RSMA）系统中进行功率分配和可重构智能表面（RIS）单元数量优化的方法。优化的RIS-RSMA（ORIS-RSMA）方法确定了RIS单元的最佳数量以及消息公共部分和私有部分的功率分配因子。此外，它在确保满足目标公共速率的同时最大化了总和速率。所提出的ORIS-RSMA的性能与传统的RIS-RSMA和RSMA进行了比较。仿真结果表明，ORIS-RSMA实现了更高的总和速率。", "summary": "本文提出了一种名为ORIS-RSMA的优化方法，用于RIS辅助的速率分裂多址（RSMA）系统。该方法通过优化RIS单元数量和消息公共与私有部分的功率分配，旨在在满足预设公共速率的同时最大化系统总和速率。仿真结果显示，与传统RIS-RSMA和RSMA相比，ORIS-RSMA能显著提高总和速率。", "keywords": "RIS, RSMA, 功率分配, 智能表面, 总和速率", "comments": "这篇论文的创新点在于提出了ORIS-RSMA方法，通过同时优化RIS单元数量和功率分配来提升RIS辅助RSMA系统的性能。其重要性在于为未来无线通信系统中的频谱效率优化提供了新的思路和方法，尤其是在利用智能反射面技术方面。"}}
{"id": "2503.15986", "title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition", "authors": ["Zeqi Zheng", "Yanchen Huang", "Yingchao Yu", "Zizheng Zhu", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "categories": ["cs.NE", "cs.CV"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. The first two authors contributed equally", "url": "http://arxiv.org/abs/2503.15986v2", "summary": "Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. The code and model checkpoints are publicly available at\nhttps://github.com/KirinZheng/SpiLiFormer.", "comment": "Accepted by ICCV 2025. The first two authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2503.15986v2", "cate": "cs.NE", "date": "2025-03-20", "updated": "2025-07-23", "AI": {"title_translation": "SpiLiFormer：通过侧抑制增强脉冲Transformer", "tldr": "本文提出了SpiLiFormer，一种受侧抑制启发的脉冲Transformer，旨在解决现有脉冲Transformer中注意力过度分配给不相关上下文的问题。该模型在多个数据集上实现了SOTA性能，并在ImageNet-1K上以更少的参数和时间步超越了现有最佳模型。", "motivation": "现有基于Transformer的脉冲神经网络（SNNs）的注意力模块源于模拟Transformer，未能充分解决注意力过度分配给不相关上下文的问题。", "method": "提出SpiLiFormer，它模拟大脑的侧抑制机制，引导模型增强对相关token的注意力，同时抑制对不相关token的注意力。", "result": "在CIFAR-10 (+0.45%)、CIFAR-100 (+0.48%)、CIFAR10-DVS (+2.70%)、N-Caltech101 (+1.94%)和ImageNet-1K (+1.6%)等多个数据集上实现了最先进（SOTA）性能。特别是在ImageNet-1K上，SpiLiFormer（69.9M参数，4时间步）比E-SpikeFormer（173.0M参数，8时间步）性能高0.46%，但参数量仅为39%，时间步减半。", "conclusion": "SpiLiFormer通过引入侧抑制机制，有效解决了脉冲Transformer中的注意力过分配问题，并在多个基准数据集上取得了领先的性能，同时显著降低了模型复杂度和计算成本。", "translation": "基于Transformer的脉冲神经网络（SNNs）因其卓越的性能和高能效而受到广泛关注。然而，大多数现有基于Transformer的SNNs的脉冲注意力模块都改编自模拟Transformer，未能充分解决注意力过度分配给不相关上下文的问题。为了解决这个根本但被忽视的问题，我们提出了一种受侧抑制启发的脉冲Transformer（SpiLiFormer）。它模拟了大脑的侧抑制机制，引导模型增强对相关token的注意力，同时抑制对不相关token的注意力。我们的模型在多个数据集上取得了最先进（SOTA）的性能，包括CIFAR-10（+0.45%）、CIFAR-100（+0.48%）、CIFAR10-DVS（+2.70%）、N-Caltech101（+1.94%）和ImageNet-1K（+1.6%）。值得注意的是，在ImageNet-1K数据集上，SpiLiFormer（69.9M参数，4时间步，384分辨率）比SOTA脉冲Transformer E-SpikeFormer（173.0M参数，8时间步，384分辨率）性能高出0.46%，而参数量仅为其39%，时间步减半。代码和模型检查点已在https://github.com/KirinZheng/SpiLiFormer 公开可用。", "summary": "本文提出了SpiLiFormer，一种受侧抑制启发的脉冲Transformer，旨在解决现有脉冲Transformer注意力模块中过度分配注意力给不相关上下文的问题。SpiLiFormer通过模拟大脑的侧抑制机制，有效提升对相关信息的关注并抑制不相关信息。该模型在CIFAR-10、CIFAR-100、CIFAR10-DVS、N-Caltech101和ImageNet-1K等多个数据集上均达到了最先进的性能。特别是在ImageNet-1K上，SpiLiFormer以更少的参数和时间步实现了优于现有SOTA模型的表现，展现了其卓越的效率和有效性。", "keywords": "脉冲神经网络, Transformer, 侧抑制, 注意力机制, 能源效率", "comments": "这项工作通过引入生物启发的侧抑制机制，创新性地解决了脉冲Transformer中的注意力分配效率问题。其核心创新在于将神经科学原理融入到深度学习模型设计中，有效提升了模型性能和能效。在ImageNet-1K上以更少资源超越现有SOTA，证明了其重要性和实用价值。"}}
{"id": "2504.05815", "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models", "authors": ["Jiahao Chen", "Yu Pan", "Yi Du", "Chunkai Wu", "Lin Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05815v2", "summary": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05815v2", "cate": "cs.CV", "date": "2025-04-08", "updated": "2025-07-23", "AI": {"title_translation": "寄生虫：一种基于隐写术的扩散模型后门攻击框架", "tldr": "本文提出了一种名为“Parasite”的新型扩散模型后门攻击方法，它首次利用隐写术隐藏触发器，并在图像到图像任务中实现了0%的检测率。", "motivation": "现有的扩散模型后门攻击主要集中于噪声到图像和文本到图像任务，且触发器单一、显眼，缺乏隐蔽性和灵活性。", "method": "提出了一种名为“Parasite”的新型后门攻击方法，专为扩散模型中的图像到图像任务设计。该方法首次利用隐写术隐藏触发器，并允许攻击者将目标内容作为后门触发器嵌入，以实现更灵活的攻击。", "result": "“Parasite”方法有效绕过了现有检测框架，在实验中对抗主流防御框架实现了0%的后门检测率。消融研究还讨论了不同隐藏系数对攻击结果的影响。", "conclusion": "“Parasite”是一种有效且隐蔽的扩散模型图像到图像任务后门攻击方法，能够成功规避现有检测系统。", "translation": "近来，扩散模型作为最成功的图像生成模型之一，通过迭代采样噪声生成高质量图像，受到了广泛关注。然而，最近的研究表明，扩散模型容易受到后门攻击，攻击者可以通过输入包含触发器的数据来激活后门并生成他们期望的输出。现有后门攻击方法主要集中于目标噪声到图像和文本到图像任务，而关于图像到图像任务中的后门攻击工作有限。此外，传统的后门攻击通常依赖单一、显眼的触发器来生成固定的目标图像，缺乏隐蔽性和灵活性。为了解决这些局限性，我们提出了一种针对扩散模型中图像到图像任务的新型后门攻击方法，名为“Parasite”，它不仅首次利用隐写术隐藏触发器，还允许攻击者将目标内容嵌入为后门触发器，以实现更灵活的攻击。“Parasite”作为一种新颖的攻击方法，有效绕过了现有检测框架来执行后门攻击。在我们的实验中，“Parasite”对抗主流防御框架实现了0%的后门检测率。此外，在消融研究中，我们讨论了不同隐藏系数对攻击结果的影响。您可以在https://anonymous.4open.science/r/Parasite-1715/找到我们的代码。", "summary": "本文提出了一种针对扩散模型中图像到图像任务的新型后门攻击框架“Parasite”。该方法首次利用隐写术隐藏触发器，并允许将目标内容作为灵活的触发器嵌入。实验证明，“Parasite”能够有效绕过现有检测框架，对抗主流防御系统实现了0%的后门检测率。", "keywords": "扩散模型, 后门攻击, 隐写术, 图像到图像, 安全性", "comments": "这篇论文创新性地将隐写术引入扩散模型的后门攻击中，解决了传统后门攻击触发器显眼且缺乏灵活性的问题。其在图像到图像任务上的应用填补了现有研究的空白，并且高达0%的检测率表明其攻击的隐蔽性和有效性极高，对扩散模型的安全性提出了新的挑战。"}}
{"id": "2507.16858", "title": "Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky", "authors": ["Gordon Hew", "Ian McCulloh"], "categories": ["cs.SI", "H.3.3; H.2.8; I.2.7; J.4; K.4.1"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      This is an extended 13 page version of a short paper accepted at ASONAM 2025", "url": "http://arxiv.org/abs/2507.16858v1", "summary": "Following the 2024 U.S. presidential election, Democratic lawmakers and their\nsupporters increasingly migrated from mainstream social media plat-forms like X\n(formerly Twitter) to decentralized alternatives such as Bluesky. This study\ninvestigates how Congressional Democrats use Bluesky to form networks of\ninfluence and disseminate political messaging in a platform environment that\nlacks algorithmic amplification. We employ a mixed-methods approach that\ncombines social network analysis, expo-nential random graph modeling (ERGM),\nand transformer-based topic mod-eling (BERTopic) to analyze follows, mentions,\nreposts, and discourse pat-terns among 182 verified Democratic members of\nCongress. Our findings show that while party leaders such as Hakeem Jeffries\nand Elizabeth War-ren dominate visibility metrics, overlooked figures like\nMarcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central\npositions, suggesting latent influence within the digital party ecosystem. ERGM\nre-sults reveal significant homophily along ideological, state, and leadership\nlines, with Senate leadership exhibiting lower connectivity. Topic analysis\nidentifies both shared themes (e.g., reproductive rights, foreign conflicts)\nand subgroup-specific issues, with The Squad showing the most distinct\ndiscourse profile. These results demonstrate the potential of decentralized\nplatforms to reshape intra-party communication dynamics and highlight the need\nfor continued computational research on elite political behavior in emerging\ndigital environments.", "comment": "This is an extended 13 page version of a short paper accepted at\n  ASONAM 2025", "pdf_url": "http://arxiv.org/pdf/2507.16858v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "谁在暗中领导？基于Bluesky的国会民主党人ERGM和中心性分析", "tldr": "本研究分析了国会民主党人如何在Bluesky上建立影响力网络和传播政治信息。研究发现，虽然知名领导人具有高可见度，但一些不那么知名的成员却占据了结构上的中心位置，表明其潜在影响力。此外，网络中存在显著的同质性。", "motivation": "为了调查在2024年美国总统大选后，民主党议员及其支持者从主流社交媒体平台（如X）转向去中心化平台（如Bluesky）后，国会民主党人如何利用Bluesky来形成影响力网络并在缺乏算法放大的平台环境中传播政治信息。", "method": "本研究采用混合方法，结合了社会网络分析、指数随机图模型（ERGM）和基于Transformer的主题建模（BERTopic），分析了182名经过验证的民主党国会议员的关注、提及、转发和话语模式。", "result": "研究结果表明，虽然哈基姆·杰弗里斯和伊丽莎白·沃伦等党派领导人在可见性指标上占主导地位，但像玛西·卡普图尔、唐纳德·拜尔和德怀特·埃文斯等被忽视的人物却占据了结构上的中心位置，这表明在数字党生态系统中存在潜在影响力。ERGM结果显示，在意识形态、州和领导层方面存在显著的同质性，而参议院领导层的连接性较低。主题分析确定了共享主题（例如，生殖权利、国外冲突）和特定小组问题，其中“小队”（The Squad）显示出最独特的话语特征。", "conclusion": "这些结果表明，去中心化平台有可能重塑党内沟通动态，并强调需要继续对新兴数字环境中的精英政治行为进行计算研究。", "translation": "在2024年美国总统大选之后，民主党立法者及其支持者越来越多地从X（前身为Twitter）等主流社交媒体平台迁移到Bluesky等去中心化替代平台。本研究调查了国会民主党人如何在缺乏算法放大的平台环境中利用Bluesky形成影响力网络和传播政治信息。我们采用混合方法，结合了社会网络分析、指数随机图模型（ERGM）和基于Transformer的主题建模（BERTopic），分析了182名经过验证的民主党国会议员的关注、提及、转发和话语模式。我们的发现表明，虽然哈基姆·杰弗里斯和伊丽莎白·沃伦等党派领导人在可见性指标上占主导地位，但像玛西·卡普图尔、唐纳德·拜尔和德怀特·埃文斯等被忽视的人物却占据了结构上的中心位置，这表明在数字党生态系统中存在潜在影响力。ERGM结果显示，在意识形态、州和领导层方面存在显著的同质性，而参议院领导层的连接性较低。主题分析确定了共享主题（例如，生殖权利、国外冲突）和特定小组问题，其中“小队”（The Squad）显示出最独特的话语特征。这些结果表明，去中心化平台有可能重塑党内沟通动态，并强调需要继续对新兴数字环境中的精英政治行为进行计算研究。", "summary": "本研究探讨了美国国会民主党人如何在去中心化社交平台Bluesky上构建政治影响力网络和传播信息，尤其是在缺乏算法放大的环境下。研究采用了社会网络分析、ERGM和BERTopic等混合方法，对182名经过验证的民主党国会议员的数据进行分析。结果发现，尽管知名领导人拥有高可见度，但一些不那么为人所知的成员却占据了网络中的核心结构位置，暗示了其潜在影响力。此外，研究揭示了网络中在意识形态、州别和领导层方面的显著同质性，且参议院领导层的连接性较低。主题分析还识别出共同话题和特定群体的话语特征。研究结论指出，去中心化平台可能改变党内沟通模式，并强调了对新兴数字环境中精英政治行为进行计算研究的重要性。", "keywords": "Bluesky, 社会网络分析, ERGM, 国会民主党人, 政治传播", "comments": "这篇论文及时地洞察了去中心化社交媒体上政治传播的演变格局。其创新性地运用ERGM和BERTopic来分析精英政治行为，提供了对超越单纯可见度的影响力的细致理解。发现“被忽视人物”的“潜在影响力”尤其富有洞察力，挑战了数字空间中对领导力的传统看法。它突出了替代平台在政治党派内部培养不同沟通动态的潜力。"}}
{"id": "2401.01799", "title": "Optimization and Identification of Lattice Quantizers", "authors": ["Erik Agrell", "Daniel Pook-Kolb", "Bruce Allen"], "categories": ["cs.IT", "math-ph", "math.IT", "math.MG", "math.MP"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2401.01799v4", "summary": "Lattices with minimal normalized second moments are designed using a new\nnumerical optimization algorithm. Starting from a random lower-triangular\ngenerator matrix and applying stochastic gradient descent, all elements are\nupdated towards the negative gradient, which makes it the most efficient\nalgorithm proposed so far for this purpose. A graphical illustration of the\ntheta series, called theta image, is introduced and shown to be a powerful tool\nfor converting numerical lattice representations into their underlying exact\nforms. As a proof of concept, optimized lattices are designed in dimensions up\nto 16. In all dimensions, the algorithm converges to either the previously best\nknown lattice or a better one. The dual of the 15-dimensional laminated lattice\nis conjectured to be optimal in its dimension and its exact normalized second\nmoment is computed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2401.01799v4", "cate": "cs.IT", "date": "2024-01-03", "updated": "2025-07-23", "AI": {"title_translation": "晶格量化器的优化与识别", "tldr": "本文提出了一种新的高效数值优化算法，用于设计具有最小归一化二阶矩的晶格，并在高维空间中取得了当前最优或更好的结果，同时引入了“theta图像”工具。", "motivation": "旨在设计具有最小归一化二阶矩的晶格，并开发一种高效的数值优化算法来实现这一目标。", "method": "采用一种新的数值优化算法，从随机下三角生成矩阵开始，应用随机梯度下降法更新所有元素。引入了“theta图像”作为图形化工具，用于将数值晶格表示转换为精确形式。", "result": "该算法在高达16维的空间中设计了优化晶格，在所有维度上都收敛到先前已知最佳晶格或更好的晶格。推测15维层压晶格的对偶在其维度上是最优的，并计算了其精确的归一化二阶矩。", "conclusion": "该算法是迄今为止用于设计最小归一化二阶矩晶格的最有效方法，能够找到最优或接近最优的晶格表示，并通过“theta图像”工具实现精确识别。", "translation": "利用一种新的数值优化算法设计了具有最小归一化二阶矩的晶格。从随机下三角生成矩阵开始，应用随机梯度下降法，所有元素都向负梯度方向更新，这使其成为迄今为止为此目的提出的最有效的算法。“theta图像”是一种对theta级数进行图形化表示的方法，被引入并证明是用于将数值晶格表示转换为其潜在精确形式的强大工具。作为概念验证，优化后的晶格在高达16维的空间中进行了设计。在所有维度上，该算法都收敛到先前已知最佳晶格或更好的晶格。推测15维层压晶格的对偶在其维度上是最优的，并计算了其精确的归一化二阶矩。", "summary": "本文介绍了一种用于设计具有最小归一化二阶矩晶格的高效数值优化算法。该算法采用随机梯度下降法，并引入了“theta图像”工具以精确识别晶格。实验结果表明，该算法在高达16维的空间中能够找到与现有最佳晶格相同或更好的晶格，并对15维层压晶格的对偶性进行了优化推测。", "keywords": "晶格量化器, 数值优化, 随机梯度下降, theta图像, 最小归一化二阶矩", "comments": "这篇论文的创新点在于提出了一个高效的数值优化算法，结合随机梯度下降来设计最优晶格，并引入了“theta图像”这一新颖的工具来辅助晶格的精确识别。其重要性在于为高维晶格设计提供了更有效的方法，这在编码理论、密码学等领域可能具有潜在应用价值。"}}
{"id": "2507.16855", "title": "A tissue and cell-level annotated H&E and PD-L1 histopathology image dataset in non-small cell lung cancer", "authors": ["Joey Spronck", "Leander van Eekelen", "Dominique van Midden", "Joep Bogaerts", "Leslie Tessier", "Valerie Dechering", "Muradije Demirel-Andishmand", "Gabriel Silva de Souza", "Roland Nemeth", "Enrico Munari", "Giuseppe Bogina", "Ilaria Girolami", "Albino Eccher", "Balazs Acs", "Ceren Boyaci", "Natalie Klubickova", "Monika Looijen-Salamon", "Shoko Vos", "Francesco Ciompi"], "categories": ["q-bio.QM", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Our dataset is available at ' this https URL and our code is available at ' this https URL", "url": "http://arxiv.org/abs/2507.16855v1", "summary": "The tumor immune microenvironment (TIME) in non-small cell lung cancer\n(NSCLC) histopathology contains morphological and molecular characteristics\npredictive of immunotherapy response. Computational quantification of TIME\ncharacteristics, such as cell detection and tissue segmentation, can support\nbiomarker development. However, currently available digital pathology datasets\nof NSCLC for the development of cell detection or tissue segmentation\nalgorithms are limited in scope, lack annotations of clinically prevalent\nmetastatic sites, and forgo molecular information such as PD-L1\nimmunohistochemistry (IHC). To fill this gap, we introduce the IGNITE data\ntoolkit, a multi-stain, multi-centric, and multi-scanner dataset of annotated\nNSCLC whole-slide images. We publicly release 887 fully annotated regions of\ninterest from 155 unique patients across three complementary tasks: (i)\nmulti-class semantic segmentation of tissue compartments in H&E-stained slides,\nwith 16 classes spanning primary and metastatic NSCLC, (ii) nuclei detection,\nand (iii) PD-L1 positive tumor cell detection in PD-L1 IHC slides. To the best\nof our knowledge, this is the first public NSCLC dataset with manual\nannotations of H&E in metastatic sites and PD-L1 IHC.", "comment": "Our dataset is available at 'https://zenodo.org/records/15674785' and\n  our code is available at\n  'https://github.com/DIAGNijmegen/ignite-data-toolkit'", "pdf_url": "http://arxiv.org/pdf/2507.16855v1", "cate": "q-bio.QM", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "非小细胞肺癌中组织和细胞水平注释的H&E和PD-L1组织病理学图像数据集", "tldr": "研究人员创建了IGNITE，这是一个新的公开数据集，包含H&E和PD-L1染色的非小细胞肺癌（NSCLC）全玻片图像，并带有详细的组织和细胞水平注释，包括转移部位，以解决NSCLC计算病理学中缺乏全面数据集的问题。", "motivation": "计算量化肿瘤免疫微环境（TIME）特征（如细胞检测、组织分割）在非小细胞肺癌（NSCLC）组织病理学中可以帮助开发免疫治疗反应的生物标志物。然而，目前用于开发细胞检测或组织分割算法的NSCLC数字病理学数据集范围有限，缺乏对临床常见转移部位的注释，并且忽略了PD-L1免疫组织化学（IHC）等分子信息。", "method": "作者引入了“IGNITE数据工具包”，这是一个多染色、多中心、多扫描仪的非小细胞肺癌（NSCLC）全玻片图像注释数据集。他们公开发布了来自155名独特患者的887个完全注释的感兴趣区域，涵盖三个互补任务：(i) H&E染色玻片中组织隔室的多类别语义分割（包含原发性和转移性NSCLC的16个类别），(ii) 细胞核检测，以及(iii) PD-L1 IHC玻片中PD-L1阳性肿瘤细胞检测。", "result": "他们发布了来自155名独特患者的887个完全注释的感兴趣区域，涵盖H&E染色玻片（包含16种组织类别，包括转移性NSCLC）、细胞核检测和PD-L1 IHC玻片中的PD-L1阳性肿瘤细胞检测。据作者称，这是第一个包含转移部位H&E和PD-L1 IHC手动注释的公开NSCLC数据集。", "conclusion": "IGNITE数据集通过提供全面的组织和细胞水平注释，包括转移部位和PD-L1 IHC信息，填补了公开可用的NSCLC数字病理学数据中的关键空白，这将支持计算生物标志物的开发。", "translation": "非小细胞肺癌（NSCLC）组织病理学中的肿瘤免疫微环境（TIME）包含预测免疫治疗反应的形态学和分子特征。计算量化TIME特征，例如细胞检测和组织分割，可以支持生物标志物开发。然而，目前可用于开发细胞检测或组织分割算法的NSCLC数字病理学数据集范围有限，缺乏临床常见的转移部位注释，并且忽略了PD-L1免疫组织化学（IHC）等分子信息。为了填补这一空白，我们引入了IGNITE数据工具包，这是一个多染色、多中心、多扫描仪的NSCLC全玻片图像注释数据集。我们公开发布了来自155名独特患者的887个完全注释的感兴趣区域，涵盖三个互补任务：（i）H&E染色玻片中组织隔室的多类别语义分割，包含16个类别，涵盖原发性和转移性NSCLC，（ii）细胞核检测，以及（iii）PD-L1 IHC玻片中PD-L1阳性肿瘤细胞检测。据我们所知，这是第一个包含转移部位H&E和PD-L1 IHC手动注释的公开NSCLC数据集。", "summary": "本文介绍了IGNITE，一个新颖的公共数据集，旨在解决非小细胞肺癌（NSCLC）全面数字病理学数据稀缺的问题。IGNITE提供了多染色（H&E、PD-L1 IHC）、多中心、多扫描仪的全玻片图像，并附有详细的组织和细胞水平注释。它包含来自155名患者的887个注释区域，支持多类别组织分割（原发性和转移性NSCLC）、细胞核检测以及PD-L1阳性肿瘤细胞检测等任务。该数据集因其包含转移部位注释和PD-L1 IHC信息而独具特色，这对于NSCLC免疫治疗中的计算生物标志物开发至关重要。", "keywords": "NSCLC, 组织病理学, 数据集, PD-L1, 图像分割", "comments": "IGNITE数据集对计算病理学，特别是NSCLC研究做出了重大贡献。其创新之处在于解决了现有数据集的关键局限性，即包含了转移部位的注释和PD-L1 IHC等分子信息，这些信息与预测免疫治疗反应高度相关。这一全面的资源有望加速肺癌中先进AI算法在生物标志物发现和精准医疗方面的发展。"}}
{"id": "2506.01966", "title": "Unified Sparse-Matrix Representations for Diverse Neural Architectures", "authors": ["Yuzhou Zhu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.01966v3", "summary": "Deep neural networks employ specialized architectures for vision, sequential\nand language tasks, yet this proliferation obscures their underlying\ncommonalities. We introduce a unified matrix-order framework that casts\nconvolutional, recurrent and self-attention operations as sparse matrix\nmultiplications. Convolution is realized via an upper-triangular weight matrix\nperforming first-order transformations; recurrence emerges from a\nlower-triangular matrix encoding stepwise updates; attention arises naturally\nas a third-order tensor factorization. We prove algebraic isomorphism with\nstandard CNN, RNN and Transformer layers under mild assumptions. Empirical\nevaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),\ntime-series forecasting (ETTh1, Electricity Load Diagrams) and language\nmodeling/classification (AG News, WikiText-2, Penn Treebank) confirm that\nsparse-matrix formulations match or exceed native model performance while\nconverging in comparable or fewer epochs. By reducing architecture design to\nsparse pattern selection, our matrix perspective aligns with GPU parallelism\nand leverages mature algebraic optimization tools. This work establishes a\nmathematically rigorous substrate for diverse neural architectures and opens\navenues for principled, hardware-aware network design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.01966v3", "cate": "cs.LG", "date": "2025-05-11", "updated": "2025-07-22", "AI": {"title_translation": "多样化神经网络架构的统一稀疏矩阵表示", "tldr": "该研究引入了一个统一的矩阵序框架，将卷积、循环和自注意力操作表示为稀疏矩阵乘法，并证明了与标准模型的代数同构性，在多项任务上实现了与原生模型相当或更优的性能。", "motivation": "深度神经网络针对视觉、序列和语言任务采用专门的架构，但这种多样性掩盖了它们潜在的共性，因此需要一个统一的表示方法。", "method": "作者引入了一个统一的矩阵序框架，将卷积操作表示为上三角权重矩阵的第一阶变换，循环操作表示为编码逐步更新的下三角矩阵，自注意力操作表示为三阶张量分解。他们证明了在温和假设下与标准CNN、RNN和Transformer层的代数同构性。", "result": "在图像分类（MNIST, CIFAR-10/100, Tiny ImageNet）、时间序列预测（ETTh1, Electricity Load Diagrams）和语言建模/分类（AG News, WikiText-2, Penn Treebank）任务上的实证评估证实，稀疏矩阵公式的性能与原生模型相当或更优，并且在相当或更少的训练周期内收敛。", "conclusion": "这项工作为多样化的神经网络架构建立了一个数学上严谨的基础，并为原则性、硬件感知的网络设计开辟了途径。通过将架构设计简化为稀疏模式选择，该矩阵视角与GPU并行性保持一致，并利用了成熟的代数优化工具。", "translation": "深度神经网络为视觉、序列和语言任务采用了专门的架构，然而这种多样性模糊了它们潜在的共性。我们引入了一个统一的矩阵序框架，将卷积、循环和自注意力操作都视为稀疏矩阵乘法。卷积通过执行一阶变换的上三角权重矩阵实现；循环通过编码逐步更新的下三角矩阵产生；注意力自然地表现为三阶张量分解。我们证明了在温和假设下，与标准CNN、RNN和Transformer层存在代数同构性。在图像分类（MNIST、CIFAR-10/100、Tiny ImageNet）、时间序列预测（ETTh1、电力负荷图）和语言建模/分类（AG News、WikiText-2、Penn Treebank）上的实证评估证实，稀疏矩阵公式的性能与原生模型相当或更优，同时在相当或更少的训练周期内收敛。通过将架构设计简化为稀疏模式选择，我们的矩阵视角与GPU并行性保持一致，并利用了成熟的代数优化工具。这项工作为多样化的神经网络架构建立了一个数学上严谨的基础，并为原则性、硬件感知的网络设计开辟了途径。", "summary": "该论文提出了一种统一的矩阵序框架，将卷积、循环和自注意力操作统一表示为稀疏矩阵乘法。研究证明了这些稀疏矩阵表示与标准CNN、RNN和Transformer层在代数上是同构的。通过在多个视觉、序列和语言任务上的广泛实验，结果表明该方法能够匹配或超越原生模型的性能，并且收敛速度相当或更快。这项工作为神经网络架构提供了数学基础，并有助于硬件感知的网络设计。", "keywords": "稀疏矩阵, 神经网络, 统一表示, 卷积, 循环, 自注意力", "comments": "该论文的创新之处在于提出了一个统一的数学框架来表示多样化的神经网络架构，将不同类型的操作（卷积、循环、自注意力）都归结为稀疏矩阵乘法。这不仅揭示了不同架构之间的深层共性，还为网络设计提供了新的视角，使其能够更好地与硬件并行计算能力结合，并利用现有的代数优化工具。其重要性在于为未来更高效、更具原则性的神经网络设计奠定了基础。"}}
{"id": "2504.02193", "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment", "authors": ["Yifan Wang", "Runjin Chen", "Bolian Li", "David Cho", "Yihe Deng", "Ruqi Zhang", "Tianlong Chen", "Zhangyang Wang", "Ananth Grama", "Junyuan Hong"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This version includes updated results and expanded discussion", "url": "http://arxiv.org/abs/2504.02193v2", "summary": "Aligning large language models (LLMs) with human values is an increasingly\ncritical step in post-training. Direct Preference Optimization (DPO) has\nemerged as a simple, yet effective alternative to reinforcement learning from\nhuman feedback (RLHF). Synthetic preference data with its low cost and high\nquality enable effective alignment through single- or multi-model generated\npreference data. Our study reveals a striking, safety-specific phenomenon\nassociated with DPO alignment: Although multi-model generated data enhances\nperformance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by\nproviding diverse responses, it also tends to facilitate reward hacking during\ntraining. This can lead to a high attack success rate (ASR) when models\nencounter jailbreaking prompts. The issue is particularly pronounced when\nemploying stronger models like GPT-4o or larger models in the same family to\ngenerate chosen responses paired with target model self-generated rejected\nresponses, resulting in dramatically poorer safety outcomes. Furthermore, with\nrespect to safety, using solely self-generated responses (single-model\ngeneration) for both chosen and rejected pairs significantly outperforms\nconfigurations that incorporate responses from stronger models, whether used\ndirectly as chosen data or as part of a multi-model response pool. We\ndemonstrate that multi-model preference data exhibits high linear separability\nbetween chosen and rejected responses, which allows models to exploit\nsuperficial cues rather than internalizing robust safety constraints. Our\nexperiments, conducted on models from the Llama, Mistral, and Qwen families,\nconsistently validate these findings.", "comment": "This version includes updated results and expanded discussion", "pdf_url": "http://arxiv.org/pdf/2504.02193v2", "cate": "cs.AI", "date": "2025-04-03", "updated": "2025-07-22", "AI": {"title_translation": "多即是少：多模型合成偏好数据在DPO安全对齐中的陷阱", "tldr": "多模型合成偏好数据虽然能提升通用任务表现，但会恶化DPO安全对齐，导致奖励作弊和越狱攻击成功率升高，尤其在使用更强模型时。单模型数据对安全性更优。", "motivation": "大型语言模型（LLM）与人类价值观的对齐是训练后日益关键的步骤。直接偏好优化（DPO）作为一种有效替代方案，常利用低成本、高质量的合成偏好数据进行对齐。本研究旨在揭示DPO对齐中与安全相关的特定现象，特别是多模型合成数据可能带来的负面影响。", "method": "研究评估了使用单模型和多模型生成的合成偏好数据进行DPO对齐的效果。通过对比模型在通用任务（如ARC, Hellaswag, MMLU, TruthfulQA, Winogrande）和安全任务（越狱提示的攻击成功率ASR）上的表现进行实验。具体比较了使用更强模型（如GPT-4o）生成“选择”响应与目标模型自生成“拒绝”响应的配置，以及完全由目标模型自生成数据的配置。研究还分析了多模型偏好数据中选择响应和拒绝响应间的线性可分性。实验在Llama、Mistral和Qwen家族模型上进行。", "result": "多模型合成数据虽然提升了LLM在通用任务上的性能，但却在训练期间促进了奖励作弊，导致模型在遇到越狱提示时攻击成功率（ASR）显著升高。当使用更强模型（如GPT-4o）生成“选择”响应并与目标模型自生成的“拒绝”响应配对时，安全结果会显著变差。相反，仅使用目标模型自生成响应（单模型生成）的配置在安全性方面表现明显优于包含更强模型响应的配置。研究发现，多模型偏好数据显示出高线性可分性，使得模型倾向于利用表面线索而非内化鲁棒的安全约束。", "conclusion": "在DPO安全对齐中，多模型合成偏好数据虽然对通用性能有益，但会导致更差的安全结果，原因在于促进了奖励作弊并使模型利用表面线索。特别是当引入更强模型生成数据时，安全问题会加剧。因此，对于安全性而言，仅使用目标模型自生成数据进行DPO对齐是更优的选择。", "translation": "大型语言模型（LLM）与人类价值观的对齐是训练后日益关键的一步。直接偏好优化（DPO）已成为强化学习人类反馈（RLHF）的一种简单而有效的替代方案。合成偏好数据以其低成本和高质量，通过单模型或多模型生成的偏好数据实现有效对齐。我们的研究揭示了与DPO对齐相关的一个显著的、特定于安全的现象：尽管多模型生成的数据通过提供多样化的响应增强了通用任务（ARC、Hellaswag、MMLU、TruthfulQA、Winogrande）的性能，但它也倾向于在训练期间促进奖励作弊（reward hacking）。当模型遇到越狱提示时，这可能导致高攻击成功率（ASR）。当使用像GPT-4o这样的更强模型或同一系列中更大的模型来生成选择的响应，并与目标模型自生成的拒绝响应配对时，这个问题尤为突出，导致安全结果显著变差。此外，在安全性方面，仅使用自生成响应（单模型生成）作为选择和拒绝对，明显优于包含来自更强模型响应的配置，无论这些响应是直接用作选择数据还是作为多模型响应池的一部分。我们证明，多模型偏好数据显示出选择响应和拒绝响应之间的高度线性可分性，这使得模型能够利用表面线索，而不是内化鲁棒的安全约束。我们对Llama、Mistral和Qwen家族模型进行的实验一致验证了这些发现。", "summary": "本研究探讨了直接偏好优化（DPO）中合成偏好数据对大型语言模型（LLM）安全对齐的影响。研究发现，虽然多模型生成的合成偏好数据能提升LLM在通用任务上的表现，但却会恶化其安全性能，导致模型更容易被越狱提示攻击。具体而言，使用更强的模型（如GPT-4o）生成“选择”响应会显著降低安全性。相反，仅使用目标模型自身生成的数据（单模型）进行DPO对齐在安全性上表现更优。研究指出，多模型数据的高度线性可分性促使模型利用表面线索而非深层安全约束，从而导致奖励作弊。", "keywords": "直接偏好优化, 安全对齐, 合成数据, 奖励作弊, 大型语言模型", "comments": "这篇论文提出了一个关键且反直觉的发现：在DPO安全对齐中，“多”（多模型数据）并非总是“更好”。它对使用多样化、可能由更强模型生成的合成数据进行安全对齐发出了重要警告，指出这种做法可能通过促进表面学习而非鲁棒的安全机制，无意中引入漏洞。论文对线性可分性和奖励作弊的解释为观察到的经验结果提供了良好的理论支撑。这项工作对于开发更安全LLM的实践者至关重要，强调了在安全关键应用中精心策划数据策略的必要性。"}}
{"id": "2507.17403", "title": "Custody Transfer and Compressed Status Reporting for Bundle Protocol Version 7", "authors": ["Alice Le Bihan", "Felix Flentge", "Juan A. Fraire"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17403v1", "summary": "As space missions increase, there is a growing need to replace point-to-point\ncommunication with an efficient and reliable network-centric communication\napproach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol\n(BP) has been selected as an interoperable network protocol in the LunaNet\nInteroperability Specification. It is also considered for future Earth\nObservation and Mars communication scenarios. In a DTN, the \"bundle\" -- the\nfundamental data unit of BP -- requires dedicated mechanisms to ensure\nreliability due to the challenges posed by intermittent connectivity and long\ndelays. The previous version of BP, BPv6, contained a mechanism for reliable\ntransfer between \"custodial nodes\" called \"custody transfer\". However, this\napproach has been removed from the core protocol specification for BPv7, which\nrequires a corresponding BP reliability extension to be defined separately.\nThis paper introduces a new custody transfer process for BPv7 (expected to be\npublished by CCSDS as an experimental specification in 2025). The core features\nof this new custody transfer method for BPv7 are: (1) A strategy to efficiently\nidentify sets of bundles by sequence numbering (2) A new Custody Transfer\nExtension Block and a corresponding administrative record, Compressed Custody\nSignal, to efficiently report on the acceptance or rejection of custody using\nsequence numbering (3) A new Compressed Reporting Extension Block requesting\nreporting on bundle processing steps using a corresponding administrative\nrecord with sequence numbering for efficiency. The paper will describe those\nconcepts and their design, specification, and implementation in detail. These\nmechanisms have been prototyped in the ESA BP implementation and tested in\nEarth Observation and Lunar communication simulation scenarios. The results\nwill be presented, as will an outlook on future work in the DTN reliable\ntransfer domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17403v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Bundle Protocol Version 7 的保管转移和压缩状态报告", "tldr": "BPv7 移除了核心协议中的保管转移机制，本文提出了一种新的保管转移过程和压缩状态报告方法，旨在提高DTN中Bundle的可靠性。", "motivation": "随着空间任务的增加，需要高效可靠的网络中心通信取代点对点通信。Bundle协议（BP）作为DTN的核心，其数据单元“bundle”在间歇性连接和长延迟环境下需要专门的可靠性机制。BPv7移除了BPv6中的保管转移机制，导致核心协议缺乏可靠传输能力，因此需要定义新的BP可靠性扩展。", "method": "本文提出了一种用于BPv7的新保管转移过程，其核心特点包括：1) 通过序列号高效识别Bundle集；2) 引入新的“保管转移扩展块”和相应的“压缩保管信号”行政记录，利用序列号高效报告保管的接受或拒绝；3) 引入新的“压缩报告扩展块”，使用带序列号的行政记录高效请求Bundle处理步骤的报告。论文将详细描述这些概念的设计、规范和实现。", "result": "所提出的机制已在ESA BP实现中进行原型验证，并在地球观测和月球通信模拟场景中进行了测试。测试结果将被呈现。", "conclusion": "Not mentioned in abstract", "translation": "随着空间任务的增加，点对点通信需要被高效可靠的网络中心通信方法所取代。具有Bundle协议（BP）的容断/延时容忍网络（DTN）已被选为LunaNet互操作性规范中的可互操作网络协议。它也被考虑用于未来的地球观测和火星通信场景。在DTN中，“bundle”——BP的基本数据单元——由于间歇性连接和长延迟带来的挑战，需要专门的机制来确保可靠性。BP的先前版本BPv6包含了一种在“保管节点”之间进行可靠传输的机制，称为“保管转移”。然而，这种方法已从BPv7的核心协议规范中移除，这需要单独定义相应的BP可靠性扩展。本文介绍了一种用于BPv7的新保管转移过程（预计将于2025年由CCSDS作为实验规范发布）。这种用于BPv7的新保管转移方法的核心特点是：(1) 通过序列号高效识别Bundle集策略；(2) 新的保管转移扩展块和相应的行政记录“压缩保管信号”，以使用序列号高效报告保管的接受或拒绝；(3) 新的压缩报告扩展块，使用相应的带序列号的行政记录高效请求Bundle处理步骤的报告。本文将详细描述这些概念及其设计、规范和实现。这些机制已在ESA BP实现中进行原型验证，并在地球观测和月球通信模拟场景中进行测试。结果将与DTN可靠传输领域的未来工作展望一并呈现。", "summary": "本文针对Bundle协议版本7 (BPv7) 移除了核心协议中的保管转移机制的问题，提出了一种新的BPv7保管转移过程和压缩状态报告方法。该方法引入了序列号识别Bundle、新的保管转移扩展块及压缩保管信号行政记录，以及新的压缩报告扩展块，旨在提高在间歇性连接和长延迟环境下DTN中Bundle的可靠传输效率。所提出的机制已在ESA BP实现中进行原型验证和模拟测试。", "keywords": "保管转移, Bundle协议, DTN, 空间通信, 可靠性", "comments": "本文的创新点在于为BPv7设计并提出了一套全新的保管转移机制，解决了其核心协议中可靠性传输机制缺失的关键问题。通过引入序列号、专用的扩展块和行政记录，该方法显著提升了DTN环境下Bundle传输的效率和可靠性。其重要性体现在对未来空间通信（如LunaNet、地球观测和火星通信）中数据可靠传输的关键支持，对于推广DTN在严苛通信环境中的应用具有重要意义。"}}
{"id": "2507.17353", "title": "RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding", "authors": ["Xi Xiao", "Yunbei Zhang", "Janet Wang", "Lin Zhao", "Yuxiang Wei", "Hengjia Li", "Yanshu Li", "Xiao Wang", "Swalpa Kumar Roy", "Hao Xu", "Tianyang Wang"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17353v1", "summary": "Accurate road damage detection is crucial for timely infrastructure\nmaintenance and public safety, but existing vision-only datasets and models\nlack the rich contextual understanding that textual information can provide. To\naddress this limitation, we introduce RoadBench, the first multimodal benchmark\nfor comprehensive road damage understanding. This dataset pairs high resolution\nimages of road damages with detailed textual descriptions, providing a richer\ncontext for model training. We also present RoadCLIP, a novel vision language\nmodel that builds upon CLIP by integrating domain specific enhancements. It\nincludes a disease aware positional encoding that captures spatial patterns of\nroad defects and a mechanism for injecting road-condition priors to refine the\nmodel's understanding of road damages. We further employ a GPT driven data\ngeneration pipeline to expand the image to text pairs in RoadBench, greatly\nincreasing data diversity without exhaustive manual annotation. Experiments\ndemonstrate that RoadCLIP achieves state of the art performance on road damage\nrecognition tasks, significantly outperforming existing vision-only models by\n19.2%. These results highlight the advantages of integrating visual and textual\ninformation for enhanced road condition analysis, setting new benchmarks for\nthe field and paving the way for more effective infrastructure monitoring\nthrough multimodal learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17353v1", "cate": "cs.CE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "RoadBench：一个用于道路损坏理解的视觉-语言基础模型和基准", "tldr": "引入了RoadBench，一个多模态道路损坏理解基准，以及RoadCLIP，一个结合视觉和文本信息的新型视觉-语言模型，显著提高了道路损坏识别的性能。", "motivation": "现有的仅视觉数据集和模型在道路损坏检测方面缺乏文本信息提供的丰富上下文理解能力，这限制了及时基础设施维护和公共安全。", "method": "本研究引入了RoadBench，首个用于道路损坏理解的多模态基准数据集，其将高分辨率图像与详细文本描述配对。同时提出了RoadCLIP，一个基于CLIP构建并集成了领域特定增强（如疾病感知位置编码和道路状况先验注入机制）的新型视觉-语言模型。此外，采用GPT驱动的数据生成管道扩展了RoadBench中的图像-文本对。", "result": "实验证明，RoadCLIP在道路损坏识别任务上取得了最先进的性能，显著优于现有仅视觉模型19.2%。", "conclusion": "整合视觉和文本信息对于增强道路状况分析具有显著优势，为该领域设立了新的基准，并为通过多模态学习实现更有效的基建监测铺平了道路。", "translation": "准确的道路损坏检测对于及时基础设施维护和公共安全至关重要，但现有的仅视觉数据集和模型缺乏文本信息可以提供的丰富上下文理解。为解决这一限制，我们引入了RoadBench，首个用于全面道路损坏理解的多模态基准。该数据集将道路损坏的高分辨率图像与详细的文本描述配对，为模型训练提供了更丰富的上下文。我们还提出了RoadCLIP，一个在CLIP基础上通过整合领域特定增强功能构建的新型视觉-语言模型。它包括一个疾病感知位置编码，用于捕获道路缺陷的空间模式，以及一个用于注入道路状况先验以完善模型对道路损坏理解的机制。我们进一步采用GPT驱动的数据生成管道来扩展RoadBench中的图像-文本对，在无需详尽手动标注的情况下大大增加了数据多样性。实验表明，RoadCLIP在道路损坏识别任务上取得了最先进的性能，显著优于现有仅视觉模型19.2%。这些结果突出了整合视觉和文本信息以增强道路状况分析的优势，为该领域设立了新的基准，并为通过多模态学习实现更有效的基建监测铺平了道路。", "summary": "该论文提出了RoadBench，一个创新的多模态基准数据集，旨在通过将高分辨率图像与详细文本描述相结合，为道路损坏理解提供更丰富的上下文。为利用此数据集，研究人员开发了RoadCLIP，一个基于CLIP的视觉-语言模型，该模型通过引入疾病感知位置编码和道路状况先验注入机制进行了领域特定优化。为了扩大数据集规模，论文还利用GPT驱动的数据生成管道来创建更多的图像-文本对。实验结果表明，RoadCLIP在道路损坏识别任务上达到了最先进的性能，相比传统的纯视觉模型，性能提升了19.2%，这强调了视觉和文本信息融合在基础设施监测中的重要性。", "keywords": "道路损坏检测, 视觉-语言模型, 多模态学习, RoadBench, RoadCLIP", "comments": "RoadBench和RoadCLIP的提出，通过融合视觉和语言信息，解决了传统道路损坏检测中上下文理解不足的问题，具有显著的创新性。GPT驱动的数据生成管道也为数据扩充提供了高效且低成本的方案。这项工作为多模态学习在基础设施维护领域的应用树立了新标准，有望推动更精准、更高效的道路状况分析。"}}
{"id": "2507.16835", "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems", "authors": ["Nima Yazdani", "Ali Ansari", "Aruj Mahajan", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi"], "categories": ["eess.AS", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16835v1", "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures combining speech-to-text (STT), large language models (LLMs), and\ntext-to-speech (TTS) components. However, systematic evaluation of different\ncomponent combinations in production settings remains understudied. We present\na large-scale empirical comparison of STT x LLM x TTS stacks using data from\nover 300,000 AI-conducted job interviews. We develop an automated evaluation\nframework using LLM-as-a-Judge to assess conversational quality, technical\naccuracy, and skill assessment capabilities. Our analysis of four production\nconfigurations reveals that Google STT paired with GPT-4.1 significantly\noutperforms alternatives in both conversational and technical quality metrics.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversational AI systems and\ncontribute a validated evaluation methodology for voice-based interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16835v1", "cate": "eess.AS", "date": "2025-07-15", "updated": "2025-07-15", "AI": {"title_translation": "评估AI面试系统中语音转文本x大型语言模型x文本转语音的组合", "tldr": "该研究大规模比较了AI面试系统中STT、LLM和TTS组件的不同组合，发现Google STT与GPT-4.1表现最佳，但客观质量与用户满意度相关性较弱。", "motivation": "基于语音的对话式AI系统日益依赖于语音转文本（STT）、大型语言模型（LLM）和文本转语音（TTS）组件的级联架构。然而，在生产环境中对不同组件组合进行系统评估的研究不足。", "method": "本文使用来自超过30万次AI进行的求职面试数据，对STT x LLM x TTS堆栈进行了大规模实证比较。开发了一个使用“LLM即评委”的自动化评估框架，以评估对话质量、技术准确性和技能评估能力。", "result": "对四种生产配置的分析表明，Google STT与GPT-4.1的组合在对话和技术质量指标上均显著优于其他替代方案。令人惊讶的是，客观质量指标与用户满意度分数的相关性较弱，这表明语音AI系统的用户体验取决于技术性能之外的因素。", "conclusion": "本研究结果为多模态对话式AI系统中的组件选择提供了实用指导，并为基于语音的交互贡献了一种经过验证的评估方法。", "translation": "基于语音的对话式AI系统日益依赖于结合语音转文本（STT）、大型语言模型（LLM）和文本转语音（TTS）组件的级联架构。然而，在生产环境中对不同组件组合进行系统评估的研究仍然不足。我们使用来自超过30万次AI进行的求职面试数据，对STT x LLM x TTS堆栈进行了大规模实证比较。我们开发了一个使用“LLM即评委”的自动化评估框架，以评估对话质量、技术准确性和技能评估能力。我们对四种生产配置的分析表明，Google STT与GPT-4.1的组合在对话和技术质量指标上均显著优于其他替代方案。令人惊讶的是，我们发现客观质量指标与用户满意度分数的相关性较弱，这表明语音AI系统的用户体验取决于技术性能之外的因素。我们的研究结果为多模态对话式AI系统中的组件选择提供了实用指导，并为基于语音的交互贡献了一种经过验证的评估方法。", "summary": "本研究对AI面试系统中语音转文本（STT）、大型语言模型（LLM）和文本转语音（TTS）组件的不同组合进行了大规模实证比较，使用了超过30万次AI面试数据，并开发了“LLM即评委”的自动化评估框架。结果显示，Google STT与GPT-4.1的组合在对话和技术质量上表现最佳。然而，研究也发现客观质量与用户满意度之间存在弱相关性，表明用户体验受技术性能以外因素影响。本研究为多模态对话AI系统的组件选择提供了实用指导，并贡献了语音交互的评估方法。", "keywords": "语音转文本, 大型语言模型, 文本转语音, AI面试, 对话式AI", "comments": "这项研究的创新之处在于其大规模的实证比较和使用“LLM即评委”的自动化评估框架，这为评估复杂的AI系统提供了一种新颖且高效的方法。此外，发现客观质量与用户满意度之间存在弱相关性是一个重要的洞察，强调了在开发AI系统时需要超越纯技术指标，更多地关注用户体验和感知价值。"}}
{"id": "2507.17316", "title": "Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler Divergence with High Probability", "authors": ["Dirk van der Hoeven", "Julia Olkhovskaia", "Tim van Erven"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17316v1", "summary": "We consider the problem of estimating a discrete distribution $p$ with\nsupport of size $K$ and provide both upper and lower bounds with high\nprobability in KL divergence. We prove that in the worst case, for any\nestimator $\\widehat{p}$, with probability at least $\\delta$, $\\text{KL}(p \\|\n\\widehat{p}) \\geq C\\max\\{K,\\ln(K)\\ln(1/\\delta) \\}/n $, where $n$ is the sample\nsize and $C > 0$ is a constant. We introduce a computationally efficient\nestimator $p^{\\text{OTB}}$, based on Online to Batch conversion and suffix\naveraging, and show that with probability at least $1 - \\delta$ $\\text{KL}(p \\|\n\\widehat{p}) \\leq C(K\\log(\\log(K)) + \\ln(K)\\ln(1/\\delta)) /n$.\n  Furthermore, we also show that with sufficiently many observations relative\nto $\\log(1/\\delta)$, the maximum likelihood estimator $\\bar{p}$ guarantees that\nwith probability at least $1-\\delta$ $$\n  1/6 \\chi^2(\\bar{p}\\|p) \\leq 1/4 \\chi^2(p\\|\\bar{p}) \\leq \\text{KL}(p|\\bar{p})\n\\leq C(K + \\log(1/\\delta))/n\\,, $$ where $\\chi^2$ denotes the\n$\\chi^2$-divergence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17316v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "高概率下库尔巴克-莱布勒散度中近似极小极大离散分布估计", "tldr": "本文在高概率下研究了离散分布在KL散度下的估计问题，提供了上下界，并引入了一个高效的估计器，同时分析了最大似然估计器。", "motivation": "本文旨在解决估计支持大小为K的离散分布的问题，并在KL散度下提供高概率的上下界。", "method": "本文提供了离散分布估计在KL散度下的高概率上下界。引入了一个基于在线到批量转换和后缀平均的计算高效估计器 $p^{\\text{OTB}}$。同时，分析了最大似然估计器 (MLE) $ \\bar{p}$ 的性能。", "result": "1. 对于任何估计器 $\\widehat{p}$，在最坏情况下，以至少 $\\delta$ 的概率，$\\text{KL}(p \\| \\widehat{p}) \\geq C\\max\\{K,\\ln(K)\\ln(1/\\delta) \\}/n$。2. 对于 $p^{\\text{OTB}}$ 估计器，以至少 $1 - \\delta$ 的概率，$\\text{KL}(p \\| \\widehat{p}) \\leq C(K\\log(\\log(K)) + \\ln(K)\\ln(1/\\delta)) /n$。3. 对于最大似然估计器 $ \\bar{p}$，当观测次数相对于 $\\log(1/\\delta)$ 足够多时，以至少 $1-\\delta$ 的概率满足 $1/6 \\chi^2(\\bar{p}\\|p) \\leq 1/4 \\chi^2(p\\|\\bar{p}) \\leq \\text{KL}(p|\\bar{p}) \\leq C(K + \\log(1/\\delta))/n$。", "conclusion": "本文在高概率下为离散分布估计在KL散度中提供了理论上下界，并提出了一个计算高效的估计器，同时验证了最大似然估计器的性能，这些结果逼近了极小极大率。", "translation": "我们考虑估计支持大小为 $K$ 的离散分布 $p$ 的问题，并以高概率在KL散度中提供了上限和下限。我们证明，在最坏情况下，对于任何估计器 $\\widehat{p}$，以至少 $\\delta$ 的概率，$\\text{KL}(p \\| \\widehat{p}) \\geq C\\max\\{K,\\ln(K)\\ln(1/\\delta) \\}/n $，其中 $n$ 是样本大小，$C > 0$ 是一个常数。我们引入了一种计算效率高的估计器 $p^{\\text{OTB}}$，它基于在线到批量转换和后缀平均，并表明以至少 $1 - \\delta$ 的概率，$\\text{KL}(p \\| \\widehat{p}) \\leq C(K\\log(\\log(K)) + \\ln(K)\\ln(1/\\delta)) /n$。此外，我们还表明，当观察次数相对于 $\\log(1/\\delta)$ 足够多时，最大似然估计器 $\\bar{p}$ 保证以至少 $1-\\delta$ 的概率满足 $1/6 \\chi^2(\\bar{p}\\|p) \\leq 1/4 \\chi^2(p\\|\\bar{p}) \\leq \\text{KL}(p|\\bar{p}) \\leq C(K + \\log(1/\\delta))/n$，其中 $\\chi^2$ 表示 $\\chi^2$-散度。", "summary": "本文研究了在库尔巴克-莱布勒散度下，以高概率估计支持大小为K的离散分布的问题。文章不仅为任意估计器推导了最坏情况下的高概率下界，还提出了一种基于在线到批量转换和后缀平均的计算高效估计器 $p^{\\text{OTB}}$，并给出了其高概率上界。此外，文章还分析了最大似然估计器，提供了其KL散度的高概率上界，这些结果对于理解离散分布估计的极小极大率具有重要意义。", "keywords": "离散分布估计, 库尔巴克-莱布勒散度, 高概率, 极小极大, 在线到批量转换", "comments": "本文通过提供高概率的上下界，对离散分布在KL散度下的估计问题做出了重要的理论贡献。提出的 $p^{\\text{OTB}}$ 估计器兼顾了计算效率，具有实际应用价值。对最大似然估计器的分析也进一步完善了该领域的理论框架。"}}
{"id": "2503.17352", "title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      23 pages, 11 figures, 8 tables", "url": "http://arxiv.org/abs/2503.17352v2", "summary": "We introduce OpenVLThinker, one of the first open-source large\nvision-language models (LVLMs) to exhibit sophisticated chain-of-thought\nreasoning, achieving notable performance gains on challenging visual reasoning\ntasks. While text-based reasoning models (e.g., Deepseek R1) show promising\nresults in text-only tasks, distilling their reasoning into LVLMs via\nsupervised fine-tuning (SFT) often results in performance degradation due to\nimprecise visual grounding. Conversely, purely reinforcement learning\n(RL)-based methods face a large search space, hindering the emergence of\nreflective behaviors in smaller models (e.g., 7B LVLMs). Surprisingly,\nalternating between SFT and RL ultimately results in significant performance\nimprovements after a few iterations. Our analysis reveals that the base model\nrarely exhibits reasoning behaviors initially, but SFT effectively surfaces\nthese latent actions and narrows the RL search space, accelerating the\ndevelopment of reasoning capabilities. Each subsequent RL stage further refines\nthe model's reasoning skills, producing higher-quality SFT data for continued\nself-improvement. OpenVLThinker-7B consistently advances performance across six\nbenchmarks demanding mathematical and general reasoning, notably improving\nMathVista by 3.8%, EMMA by 2.4%, and HallusionBench by 1.6%. Beyond\ndemonstrating the synergy between SFT and RL for complex reasoning tasks, our\nfindings provide early evidence towards achieving R1-style reasoning in\nmultimodal contexts. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.", "comment": "23 pages, 11 figures, 8 tables", "pdf_url": "http://arxiv.org/pdf/2503.17352v2", "cate": "cs.CV", "date": "2025-03-21", "updated": "2025-07-22", "AI": {"title_translation": "OpenVLThinker：通过迭代SFT-RL循环进行复杂视觉-语言推理", "tldr": "OpenVLThinker是一个开源大型视觉-语言模型（LVLM），通过SFT和RL的迭代循环，显著提升了复杂视觉推理任务的性能，解决了传统方法中视觉接地不精确和RL搜索空间大的问题。", "motivation": "现有的文本推理模型（如Deepseek R1）在文本任务中表现出色，但通过SFT蒸馏到LVLM中常因视觉接地不精确导致性能下降。纯强化学习（RL）方法面临巨大的搜索空间，阻碍了小型模型（如7B LVLM）反思行为的出现。", "method": "引入OpenVLThinker，一个开源大型视觉-语言模型（LVLM），采用SFT（监督微调）和RL（强化学习）交替迭代的循环训练策略。分析表明，SFT能有效发掘潜在推理行为并缩小RL搜索空间，而RL阶段则进一步完善推理技能并生成高质量SFT数据，实现持续的自我改进。", "result": "OpenVLThinker-7B在六个需要数学和通用推理的基准测试中持续提高性能，MathVista提升3.8%，EMMA提升2.4%，HallusionBench提升1.6%。", "conclusion": "SFT和RL的协同作用对于复杂推理任务至关重要，为多模态语境下实现R1风格的推理提供了早期证据。", "translation": "我们引入了OpenVLThinker，这是首批展示复杂思维链推理的开源大型视觉-语言模型（LVLM）之一，在具有挑战性的视觉推理任务上取得了显著的性能提升。虽然基于文本的推理模型（例如Deepseek R1）在纯文本任务中显示出有希望的结果，但通过监督微调（SFT）将其推理能力蒸馏到LVLM中往往会由于不精确的视觉接地而导致性能下降。相反，纯粹基于强化学习（RL）的方法面临巨大的搜索空间，这阻碍了小型模型（例如7B LVLM）中反思行为的出现。令人惊讶的是，SFT和RL之间的交替最终在几次迭代后带来了显著的性能改进。我们的分析表明，基础模型最初很少表现出推理行为，但SFT有效地展现了这些潜在动作并缩小了RL搜索空间，加速了推理能力的发展。随后的每个RL阶段都进一步完善了模型的推理技能，为持续的自我改进生成了更高质量的SFT数据。OpenVLThinker-7B在六个需要数学和通用推理的基准测试中持续提升性能，MathVista显著提高了3.8%，EMMA提高了2.4%，HallusionBench提高了1.6%。除了展示SFT和RL在复杂推理任务中的协同作用外，我们的发现还为在多模态语境中实现R1风格的推理提供了早期证据。代码、模型和数据可在https://github.com/yihedeng9/OpenVLThinker 获取。", "summary": "OpenVLThinker是一个开源的大型视觉-语言模型，它通过SFT（监督微调）和RL（强化学习）的迭代循环，有效解决了LVLM在复杂视觉推理中因视觉接地不精确和RL搜索空间大而导致的性能瓶颈。该方法利用SFT激发模型潜在的推理能力并缩小RL的探索范围，再通过RL进一步优化推理技能并生成高质量的SFT数据，从而实现自我提升。OpenVLThinker-7B在多个视觉推理基准测试中取得了显著的性能提升，证明了SFT与RL协同作用在多模态复杂推理中的有效性。", "keywords": "视觉-语言模型, 复杂推理, 监督微调, 强化学习, OpenVLThinker", "comments": "本文提出了一种新颖且有效的LVLM训练范式，通过SFT和RL的迭代循环克服了单一方法在复杂视觉-语言推理上的局限性。其创新点在于巧妙地结合了SFT和RL的优势，SFT用于初始化和引导，RL用于精炼和生成高质量数据，形成了持续自我改进的闭环。这对于推动LVLM在复杂多模态推理领域的发展具有重要意义，并为实现更高级别的通用智能提供了新的思路。"}}
{"id": "2507.17649", "title": "Event Detection for Active Lower Limb Prosthesis", "authors": ["J. D. Clark", "P. Ellison"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17649v1", "summary": "Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17649v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "主动式下肢假肢的事件检测", "tldr": "本研究探讨了使用双髁膝关节设计中交叉韧带的拉伸来改善主动式下肢假肢的步态事件检测。", "motivation": "准确的事件检测对于半被动和动力假肢的成功设计至关重要。将自然膝关节简化为销钉关节会丢失部分重要的运动学行为。本研究旨在探究交叉韧带拉伸在事件检测中的作用。", "method": "研究使用了一个由前交叉韧带和后交叉韧带模拟物约束的双髁膝关节设计。使用与罗素膝关节韧带平行的LVDT传感器，在弯曲膝盖的拐杖上记录韧带拉伸。数据在跑步机上以3种速度采集。", "result": "研究发现交叉韧带的拉伸具有速度依赖性，主要在步态周期的后交叉韧带的约5%和前交叉韧带的约80%处表现显著。周期曲线随速度保持一致；因此，其他静态事件（如后交叉韧带在周期约90%和前交叉韧带在约95%处的转折点特征）可用作初始接触的预测前兆。同样，在90%和95%处，另一对转折点可用于预测足底平触。", "conclusion": "本研究得出结论，使用双髁膝关节设计可以改善步态周期中的事件检测，从而提高动力假肢后续控制器的准确性。", "translation": "准确的事件检测是半被动和动力假肢成功设计的关键。在运动学上，自然膝关节是复杂的，其平移和旋转分量对步态特征有实质性影响。当简化为销钉关节时，会丢失部分这种行为。本研究调查了交叉韧带拉伸在事件检测中的作用。使用了双髁膝关节设计，并受限于前交叉韧带和后交叉韧带的模拟物。这提供了通过韧带拉伸来表征膝关节运动学的能力。使用与弯曲膝盖拐杖上罗素膝关节韧带平行的LVDT传感器记录韧带拉伸。该装置用于在跑步机上以3种速度采集数据。本研究发现交叉韧带的拉伸存在速度依赖性，在步态周期的后交叉韧带约5%和前交叉韧带约80%处尤为突出。周期曲线随速度保持一致；因此，其他静态事件，例如后交叉韧带在周期约90%处和前交叉韧带在约95%处的转折点特征，可以作为初始接触的预测前兆。同样在90%和95%处，另一对转折点在这种情况下可以用于预测足底平触。这得出结论，使用双髁膝关节设计可以改善步态周期中的事件检测，因此可以提高动力假肢后续控制器的准确性。", "summary": "本研究探讨了利用双髁膝关节设计中的交叉韧带拉伸来提高主动式下肢假肢的事件检测精度。通过记录韧带拉伸，研究发现了速度依赖性模式和一致的周期曲线，并指出特定的转折点可用于预测初始接触和足底平触等步态事件。该方法被认为能显著提升动力假肢控制器的准确性。", "keywords": "事件检测, 下肢假肢, 交叉韧带, 步态周期, 双髁膝关节", "comments": "本研究的创新点在于超越了简化的销钉关节模型，利用双髁膝关节的运动学特性和韧带拉伸来改善事件检测的准确性。这对于开发更自然、响应更灵敏的动力假肢控制器具有重要意义。"}}
{"id": "2305.08175", "title": "ResidualPlanner+: a scalable matrix mechanism for marginals and beyond", "authors": ["Yingtai Xiao", "Guanlin He", "Levent Toksoz", "Zeyu Ding", "Danfeng Zhang", "Daniel Kifer"], "categories": ["cs.DB", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2305.08175v3", "summary": "Noisy marginals are a common form of confidentiality protecting data release\nand are useful for many downstream tasks such as contingency table analysis,\nconstruction of Bayesian networks, and even synthetic data generation. Privacy\nmechanisms that provide unbiased noisy answers to linear queries (such as\nmarginals) are known as matrix mechanisms.\n  We propose ResidualPlanner and ResidualPlanner+, two highly scalable matrix\nmechanisms. ResidualPlanner is both optimal and scalable for answering marginal\nqueries with Gaussian noise, while ResidualPlanner+ provides support for more\ngeneral workloads, such as combinations of marginals and range queries or\nprefix-sum queries. ResidualPlanner can optimize for many loss functions that\ncan be written as a convex function of marginal variances (prior work was\nrestricted to just one predefined objective function). ResidualPlanner can\noptimize the accuracy of marginals in large scale settings in seconds, even\nwhen the previous state of the art (HDMM) runs out of memory. It even runs on\ndatasets with 100 attributes in a couple of minutes. Furthermore,\nResidualPlanner can efficiently compute variance/covariance values for each\nmarginal (prior methods quickly run out of memory, even for relatively small\ndatasets).\n  ResidualPlanner+ provides support for more complex workloads that combine\nmarginal and range/prefix-sum queries (e.g., a marginal on race, a range query\non age, and a combined race/age tabulation that answers age range queries for\neach race). It even supports custom user-defined workloads on different\nattributes. With this added flexibility, ResidualPlanner+ is not necessarily\noptimal, however it is still extremely scalable and outperforms the prior\nstate-of-the-art (HDMM) on prefix-sum queries both in terms of accuracy and\nspeed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2305.08175v3", "cate": "cs.DB", "date": "2023-05-14", "updated": "2025-07-22", "AI": {"title_translation": "ResidualPlanner+: 一种可扩展的用于边缘分布及更广泛应用的矩阵机制", "tldr": "ResidualPlanner和ResidualPlanner+是两种高度可扩展的矩阵机制，用于在保护隐私的数据发布中生成带噪声的边缘分布。它们在规模、速度和准确性方面显著优于现有技术，并支持更复杂的查询类型。", "motivation": "带噪声的边缘分布是保护数据隐私的常见形式，并对许多下游任务（如列联表分析、贝叶斯网络构建和合成数据生成）非常有用。现有的矩阵机制在处理大规模数据和更复杂的查询类型时存在可扩展性、内存限制和优化目标单一的问题。", "method": "本文提出了ResidualPlanner和ResidualPlanner+两种高度可扩展的矩阵机制。ResidualPlanner在处理高斯噪声的边缘查询时既最优又可扩展，并且可以优化多种损失函数（以前的工作仅限于一个预定义的目标函数）。ResidualPlanner+支持更通用的工作负载，例如边缘分布和范围查询或前缀和查询的组合，甚至支持用户自定义的工作负载。", "result": "ResidualPlanner能够在几秒钟内优化大规模设置下的边缘分布精度，即使现有最先进的技术（HDMM）内存不足，它也能运行。它甚至能在几分钟内在具有100个属性的数据集上运行。ResidualPlanner还能高效计算每个边缘分布的方差/协方差值（以前的方法即使对于相对较小的数据集也很快内存不足）。ResidualPlanner+在处理前缀和查询时，在准确性和速度方面均优于现有最先进的技术（HDMM）。", "conclusion": "ResidualPlanner和ResidualPlanner+是高度可扩展的矩阵机制，为保护隐私的数据发布提供了显著的性能提升。它们解决了现有方法在处理大规模数据和复杂查询类型时的局限性，提供了更优的准确性、速度和灵活性。", "translation": "带噪声的边缘分布是保护数据发布隐私的常见形式，对列联表分析、贝叶斯网络构建甚至合成数据生成等许多下游任务都很有用。提供对线性查询（如边缘分布）无偏噪声答案的隐私机制被称为矩阵机制。\n我们提出了ResidualPlanner和ResidualPlanner+两种高度可扩展的矩阵机制。ResidualPlanner在用高斯噪声回答边缘查询时既最优又可扩展，而ResidualPlanner+为更通用的工作负载提供支持，例如边缘分布和范围查询或前缀和查询的组合。ResidualPlanner可以优化许多可以写成边缘方差的凸函数的损失函数（以前的工作仅限于一个预定义的目标函数）。ResidualPlanner可以在几秒钟内优化大规模设置下的边缘分布精度，即使以前最先进的技术（HDMM）内存不足。它甚至可以在几分钟内在具有100个属性的数据集上运行。此外，ResidualPlanner可以高效计算每个边缘分布的方差/协方差值（以前的方法即使对于相对较小的数据集也很快内存不足）。\nResidualPlanner+支持更复杂的工作负载，结合了边缘分布和范围/前缀和查询（例如，种族上的边缘分布、年龄上的范围查询，以及结合种族/年龄的制表，回答每个种族的年龄范围查询）。它甚至支持用户在不同属性上自定义工作负载。由于增加了这种灵活性，ResidualPlanner+不一定是最佳的，但它仍然具有极高的可扩展性，并且在前缀和查询方面在准确性和速度上都优于现有最先进的技术（HDMM）。", "summary": "本文介绍了ResidualPlanner和ResidualPlanner+，两种创新的可扩展矩阵机制，用于生成保护隐私的带噪声边缘分布。ResidualPlanner针对高斯噪声边缘查询进行了优化，并支持多种损失函数，显著提高了大规模数据处理的速度和内存效率。ResidualPlanner+进一步扩展了功能，支持边缘分布、范围查询和前缀和查询的组合，以及用户自定义工作负载。这两种机制在性能上均超越了现有最先进技术（HDMM），尤其在处理大规模数据集和复杂查询时展现出卓越的准确性和速度。", "keywords": "矩阵机制, 隐私保护, 边缘分布, 可扩展性, ResidualPlanner", "comments": "本文提出的ResidualPlanner和ResidualPlanner+在隐私保护数据发布领域具有重要意义。其创新之处在于解决了现有矩阵机制在可扩展性、内存效率和支持复杂查询类型方面的局限性。特别是，ResidualPlanner能够在大规模数据集上实现秒级优化，并有效处理内存密集型方差/协方差计算，而ResidualPlanner+则通过支持更广泛的查询组合和用户自定义工作负载，极大地提升了灵活性和实用性。这些进展对于实际应用中的隐私保护数据分析具有重要价值。"}}
{"id": "2506.09859", "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints", "authors": ["Huajian Liu", "Yixuan Feng", "Wei Dong", "Kunpeng Fan", "Chao Wang", "Yongzhuo Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.09859v2", "summary": "In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.09859v2", "cate": "cs.RO", "date": "2025-06-11", "updated": "2025-07-23", "AI": {"title_translation": "分层学习增强型MPC用于异构约束下安全人群导航", "tldr": "提出了一种新的分层学习增强型MPC框架，用于在异构约束动态环境中进行机器人导航，实现了最先进的性能，并解决了对高保真模拟环境的依赖。", "motivation": "在具有异构约束的动态环境中，机器人导航面临挑战；现有学习-优化混合方法依赖高保真模拟环境，计算效率和训练可扩展性有待提高。", "method": "提出了一种新的分层框架。利用通过强化学习（RL）训练的图神经网络（GNN）来有效估计机器人的成本，并将其表述为局部目标推荐。采用时空路径搜索模块生成参考轨迹，以促进解决用于显式约束执行的非凸优化问题。引入增量动作掩蔽机制和特权学习策略，实现规划器的端到端训练。", "result": "在仿真和真实世界实验中，所提方法有效解决了复杂动态环境中的局部规划问题，实现了最先进（SOTA）的性能。与现有学习-优化混合方法相比，消除了对高保真模拟环境的依赖，在计算效率和训练可扩展性方面具有显著优势。", "conclusion": "所提出的分层学习增强型MPC框架在复杂动态环境中实现了安全有效的机器人导航，具有SOTA性能，并克服了传统方法的局限性。", "translation": "在本文中，我们提出了一种新颖的分层框架，用于在具有异构约束的动态环境中进行机器人导航。我们的方法利用通过强化学习（RL）训练的图神经网络来有效估计机器人的到达成本，并将其表述为局部目标推荐。随后，采用一个考虑运动学约束的时空路径搜索模块来生成参考轨迹，以促进解决用于显式约束执行的非凸优化问题。更重要的是，我们引入了增量动作掩蔽机制和特权学习策略，从而实现了所提出规划器的端到端训练。仿真和真实世界实验都表明，所提出的方法有效地解决了复杂动态环境中的局部规划问题，实现了最先进（SOTA）的性能。与现有学习-优化混合方法相比，我们的方法消除了对高保真模拟环境的依赖，在计算效率和训练可扩展性方面提供了显著优势。代码将在论文被接受后开源。", "summary": "本文提出了一种用于动态环境下机器人导航的分层框架，该框架结合了通过强化学习训练的图神经网络用于成本估计和局部目标推荐，以及一个时空路径搜索模块用于生成参考轨迹。通过引入增量动作掩蔽和特权学习策略，实现了规划器的端到端训练。实验结果表明，该方法在复杂动态环境中实现了最先进的局部规划性能，并有效解决了对高保真模拟环境的依赖，提升了计算效率和训练可扩展性。", "keywords": "机器人导航, 分层学习, 模型预测控制, 图神经网络, 强化学习", "comments": "该论文的创新点在于其分层框架结合了GNN与RL进行成本估计和局部目标推荐，并引入了增量动作掩蔽和特权学习策略实现端到端训练。其重要性在于，该方法不仅在复杂动态环境中实现了SOTA的导航性能，更重要的是，它克服了现有学习-优化混合方法对高保真模拟环境的依赖，显著提升了计算效率和训练可扩展性，这对于实际应用具有重要意义。"}}
{"id": "2507.17594", "title": "RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction", "authors": ["Yuqing Lan", "Chenyang Zhu", "Shuaifeng Zhi", "Jiazhao Zhang", "Zhoufeng Wang", "Renjiao Yi", "Yijie Wang", "Kai Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17594v1", "summary": "The introduction of the neural implicit representation has notably propelled\nthe advancement of online dense reconstruction techniques. Compared to\ntraditional explicit representations, such as TSDF, it improves the mapping\ncompleteness and memory efficiency. However, the lack of reconstruction details\nand the time-consuming learning of neural representations hinder the widespread\napplication of neural-based methods to large-scale online reconstruction. We\nintroduce RemixFusion, a novel residual-based mixed representation for scene\nreconstruction and camera pose estimation dedicated to high-quality and\nlarge-scale online RGB-D reconstruction. In particular, we propose a\nresidual-based map representation comprised of an explicit coarse TSDF grid and\nan implicit neural module that produces residuals representing fine-grained\ndetails to be added to the coarse grid. Such mixed representation allows for\ndetail-rich reconstruction with bounded time and memory budget, contrasting\nwith the overly-smoothed results by the purely implicit representations, thus\npaving the way for high-quality camera tracking. Furthermore, we extend the\nresidual-based representation to handle multi-frame joint pose optimization via\nbundle adjustment (BA). In contrast to the existing methods, which optimize\nposes directly, we opt to optimize pose changes. Combined with a novel\ntechnique for adaptive gradient amplification, our method attains better\noptimization convergence and global optimality. Furthermore, we adopt a local\nmoving volume to factorize the mixed scene representation with a\ndivide-and-conquer design to facilitate efficient online learning in our\nresidual-based framework. Extensive experiments demonstrate that our method\nsurpasses all state-of-the-art ones, including those based either on explicit\nor implicit representations, in terms of the accuracy of both mapping and\ntracking on large-scale scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17594v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "RemixFusion: 基于残差的混合表示用于大规模在线RGB-D重建", "tldr": "RemixFusion提出了一种基于残差的混合表示方法，结合显式TSDF和隐式神经模块来解决大规模在线RGB-D重建中细节缺失和学习耗时的问题，实现了高精度建图和跟踪。", "motivation": "神经隐式表示在在线密集重建中存在重建细节不足和学习耗时的问题，这阻碍了其在大规模在线重建中的广泛应用。", "method": "本文提出了RemixFusion，一种新颖的基于残差的混合表示方法，用于场景重建和相机姿态估计。它由一个显式的粗糙TSDF网格和一个生成残差的隐式神经模块组成，这些残差代表了添加到粗糙网格中的细粒度细节。这种混合表示允许在有限的时间和内存预算内进行细节丰富的重建。此外，该方法将基于残差的表示扩展到通过束调整（BA）处理多帧联合姿态优化，优化姿态变化而非直接优化姿态，并结合自适应梯度放大技术，实现了更好的优化收敛性和全局最优性。最后，采用局部移动体来分解混合场景表示，以促进高效的在线学习。", "result": "RemixFusion在大型场景的建图和跟踪精度方面超越了所有现有最先进的方法，包括基于显式或隐式表示的方法。", "conclusion": "RemixFusion通过引入基于残差的混合表示，有效解决了大规模在线RGB-D重建中细节和效率的挑战，实现了高精度建图和跟踪，为高质量相机跟踪铺平了道路。", "translation": "神经隐式表示的引入显著推动了在线密集重建技术的发展。与传统的显式表示（如TSDF）相比，它提高了映射的完整性和内存效率。然而，重建细节的缺乏和神经表示学习的耗时阻碍了基于神经的方法在大规模在线重建中的广泛应用。我们引入了RemixFusion，一种新颖的基于残差的混合表示方法，用于场景重建和相机姿态估计，致力于高质量和大规模的在线RGB-D重建。特别是，我们提出了一种基于残差的地图表示，它由一个显式的粗糙TSDF网格和一个生成残差（代表要添加到粗糙网格的细粒度细节）的隐式神经模块组成。这种混合表示允许在有限的时间和内存预算内进行细节丰富的重建，与纯隐式表示过于平滑的结果形成对比，从而为高质量相机跟踪铺平了道路。此外，我们将基于残差的表示扩展到通过束调整（BA）处理多帧联合姿态优化。与现有直接优化姿态的方法不同，我们选择优化姿态变化。结合一种新颖的自适应梯度放大技术，我们的方法获得了更好的优化收敛性和全局最优性。此外，我们采用局部移动体来分解混合场景表示，采用分而治之的设计，以促进我们基于残差的框架中高效的在线学习。大量的实验表明，我们的方法在大型场景的建图和跟踪精度方面超越了所有现有最先进的方法，包括那些基于显式或隐式表示的方法。", "summary": "RemixFusion提出了一种用于大规模在线RGB-D重建的新型基于残差的混合表示方法。该方法结合了显式粗糙TSDF网格和隐式神经模块来生成细节残差，解决了纯隐式表示细节不足和学习耗时的问题。它还通过优化姿态变化和自适应梯度放大来改进多帧联合姿态优化，并采用局部移动体实现高效在线学习。实验证明，RemixFusion在建图和跟踪精度上均超越了现有最先进的方法。", "keywords": "RGB-D重建, 混合表示, 残差, 神经隐式表示, 在线重建", "comments": "RemixFusion的创新之处在于其提出的基于残差的混合表示，巧妙地结合了显式表示的结构性和隐式表示的灵活性，有效解决了大规模重建中细节缺失和计算效率的矛盾。通过优化姿态变化和自适应梯度放大，提升了姿态估计的鲁棒性和精度。该方法为在线RGB-D重建领域提供了一个高性能且实用的解决方案，对实时三维重建和SLAM领域具有重要意义。"}}
{"id": "2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work draft", "url": "http://arxiv.org/abs/2506.16383v3", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.", "comment": "Work draft", "pdf_url": "http://arxiv.org/pdf/2506.16383v3", "cate": "cs.CL", "date": "2025-06-19", "updated": "2025-07-23", "AI": {"title_translation": "大型语言模型在论证挖掘中的应用：一项综述", "tldr": "这篇综述系统地分析了大型语言模型（LLMs）如何彻底改变论证挖掘（AM）领域，涵盖了基础理论、数据集、LLM技术在AM子任务中的应用、评估实践、当前挑战以及未来研究方向。", "motivation": "论证挖掘（AM）是自然语言处理（NLP）的一个关键子领域，而大型语言模型（LLMs）的出现深刻地改变了AM。本综述旨在系统地综合LLM驱动的AM领域的最新进展，并为研究人员提供指导。", "method": "本综述系统地综合了LLM驱动的AM领域的最新进展。它回顾了基础理论和标注框架，提供了一个精心策划的数据集目录。它还提出了一个全面的AM子任务分类法，阐明了LLM技术（如提示、思维链推理和检索增强）如何重新配置其执行。此外，它详细介绍了当前的LLM架构和方法，批判性地评估了评估实践，并描绘了关键挑战。", "result": "本综述提供了LLM驱动的论证挖掘领域的系统性进展综合，包括基础理论、标注框架、数据集目录、AM子任务的全面分类（阐明LLM技术如何应用）、LLM架构和方法、评估实践分析以及关键挑战的识别。", "conclusion": "本综述强调了新兴趋势，并为基于LLM的计算论证提出了一个前瞻性的研究议程，旨在战略性地指导研究人员在这个快速发展的领域。", "translation": "论证挖掘（AM）是自然语言处理（NLP）的一个关键子领域，专注于从文本中提取论证结构。大型语言模型（LLMs）的出现深刻地改变了AM，实现了先进的上下文学习、基于提示的生成和强大的跨领域适应性。本综述系统地综合了LLM驱动的AM领域的最新进展。我们对基础理论和标注框架进行了简明回顾，并提供了一个精心策划的数据集目录。一个关键贡献是我们对AM子任务的全面分类，阐明了当代LLM技术——如提示、思维链推理和检索增强——如何重新配置它们的执行。我们进一步详细介绍了当前的LLM架构和方法，批判性地评估了评估实践，并描绘了关键挑战，包括长上下文推理、可解释性和标注瓶颈。最后，我们强调了新兴趋势，并为基于LLM的计算论证提出了一个前瞻性的研究议程，旨在战略性地指导研究人员在这个快速发展的领域。", "summary": "这篇综述系统地探讨了大型语言模型（LLMs）如何彻底改变论证挖掘（AM）领域。文章涵盖了AM的基础理论、标注框架和数据集，并提出了一个全面的AM子任务分类，详细阐述了LLM技术（如提示和思维链）如何应用于这些任务。此外，综述还分析了LLM架构、评估方法以及长上下文推理、可解释性和标注瓶颈等挑战，并展望了LLM在计算论证领域的未来研究方向。", "keywords": "论证挖掘, 大型语言模型, 自然语言处理, 综述, 计算论证", "comments": "这是一篇及时且全面的综述，对于理解LLM在论证挖掘中的应用具有重要意义。其创新之处在于系统地梳理了LLM技术如何重塑AM子任务，并指出了该领域面临的关键挑战和未来的研究方向，为研究人员提供了宝贵的路线图。"}}
{"id": "2507.17152", "title": "JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction", "authors": ["Fangze Lin", "Ying He", "Fei Yu", "Hong Zhang"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IROS 2025 Accepted", "url": "http://arxiv.org/abs/2507.17152v1", "summary": "Predicting the future motion of road participants is a critical task in\nautonomous driving. In this work, we address the challenge of low-quality\ngeneration of low-probability modes in multi-agent joint prediction. To tackle\nthis issue, we propose a two-stage multi-agent interactive prediction framework\nnamed \\textit{keypoint-guided joint prediction after classification-aware\nmarginal proposal} (JAM). The first stage is modeled as a marginal prediction\nprocess, which classifies queries by trajectory type to encourage the model to\nlearn all categories of trajectories, providing comprehensive mode information\nfor the joint prediction module. The second stage is modeled as a joint\nprediction process, which takes the scene context and the marginal proposals\nfrom the first stage as inputs to learn the final joint distribution. We\nexplicitly introduce key waypoints to guide the joint prediction module in\nbetter capturing and leveraging the critical information from the initial\npredicted trajectories. We conduct extensive experiments on the real-world\nWaymo Open Motion Dataset interactive prediction benchmark. The results show\nthat our approach achieves competitive performance. In particular, in the\nframework comparison experiments, the proposed JAM outperforms other prediction\nframeworks and achieves state-of-the-art performance in interactive trajectory\nprediction. The code is available at https://github.com/LinFunster/JAM to\nfacilitate future research.", "comment": "IROS 2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2507.17152v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "JAM：基于关键点引导的分类感知边缘提议多智能体交互联合预测", "tldr": "该论文提出了JAM，一个用于自动驾驶中多智能体运动预测的两阶段框架，以解决低概率模式生成质量低的问题。它利用轨迹分类的边缘预测和关键点引导的联合预测，在Waymo开放运动数据集上取得了最先进的性能。", "motivation": "自动驾驶中道路参与者未来运动预测任务中，多智能体联合预测中低概率模式生成质量低。", "method": "提出了一种名为JAM的两阶段多智能体交互预测框架。第一阶段是边缘预测过程，通过轨迹类型对查询进行分类，以学习所有类别的轨迹，为联合预测模块提供全面的模式信息。第二阶段是联合预测过程，它将场景上下文和第一阶段的边缘提议作为输入，以学习最终的联合分布，并明确引入关键路点来引导联合预测模块。", "result": "在真实世界的Waymo开放运动数据集交互预测基准上取得了有竞争力的性能。在框架比较实验中，所提出的JAM优于其他预测框架，并在交互式轨迹预测中取得了最先进的性能。", "conclusion": "所提出的JAM框架有效解决了多智能体联合预测中低概率模式生成质量低的挑战，并在交互式轨迹预测中展示了最先进的性能。", "translation": "预测道路参与者的未来运动是自动驾驶中的一项关键任务。在这项工作中，我们解决了多智能体联合预测中低概率模式生成质量低的问题。为了解决这个问题，我们提出了一种名为“关键点引导的分类感知边缘提议多智能体交互联合预测”（JAM）的两阶段多智能体交互预测框架。第一阶段被建模为边缘预测过程，通过轨迹类型对查询进行分类，以鼓励模型学习所有类别的轨迹，为联合预测模块提供全面的模式信息。第二阶段被建模为联合预测过程，它将场景上下文和第一阶段的边缘提议作为输入，以学习最终的联合分布。我们明确引入关键路点来引导联合预测模块更好地捕获和利用初始预测轨迹中的关键信息。我们在真实世界的Waymo开放运动数据集交互预测基准上进行了广泛的实验。结果表明，我们的方法取得了有竞争力的性能。特别是，在框架比较实验中，所提出的JAM优于其他预测框架，并在交互式轨迹预测中取得了最先进的性能。代码可在https://github.com/LinFunster/JAM 获取，以方便未来的研究。", "summary": "本文介绍了JAM，一个用于自动驾驶中多智能体交互预测的两阶段框架，旨在解决低概率模式生成质量低的问题。第一阶段执行分类感知边缘预测以捕获多样化的轨迹类型，而第二阶段则利用边缘提议和场景上下文进行关键点引导的联合预测。在Waymo开放运动数据集上的实验表明，JAM在交互式轨迹预测中取得了有竞争力和最先进的性能。", "keywords": "多智能体交互, 运动预测, 自动驾驶, 关键点引导, 联合预测", "comments": "该论文的创新之处在于其两阶段方法，特别是分类感知边缘提议，以确保全面的模式覆盖，以及明确使用关键路点来引导联合预测。这解决了自动驾驶中多智能体预测的关键挑战，从而提高了准确性，尤其是在低概率场景中。代码的发布也增强了其潜在影响力。"}}
{"id": "2507.17442", "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17442v1", "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17442v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "各取所需：探索RAG中的最优嵌入", "tldr": "本文提出了两种方法，Mixture-Embedding RAG和Confident RAG，以解决RAG中不同嵌入模型导致LLM响应质量不一致的问题。其中，Confident RAG通过选择最高置信度的响应，在不同LLM和嵌入模型上均实现了显著改进。", "motivation": "随着大型语言模型（LLM）的广泛应用，如何将最新信息或外部知识融入LLM以构建特定领域模型受到了广泛关注。检索增强生成（RAG）作为一种推理时扩展方法，因其低成本和参数调优工作量小而引人注目。然而，由于训练数据和模型架构的异构性，RAG中使用的不同嵌入模型在不同领域表现出不同的效果，导致相似度计算结果和LLM响应质量不一致。", "method": "为解决RAG中嵌入模型异构性导致的问题，本文提出了两种增强RAG的方法：Mixture-Embedding RAG和Confident RAG。Mixture-Embedding RAG通过标准化相似度对来自多个嵌入模型的检索结果进行排序和选择。Confident RAG则使用不同的嵌入模型多次生成响应，然后选择置信度最高的响应。", "result": "Mixture-Embedding RAG的表现并未优于普通的RAG。相反，Confident RAG相对于普通LLM和RAG分别平均提升了约10%和5%。这些结果在不同的LLM和嵌入模型上保持一致。", "conclusion": "Confident RAG是一种高效的即插即用方法，适用于各种领域，能够有效提升RAG的性能。", "translation": "最近，随着大型语言模型（LLM）从根本上影响了各个领域，将最新信息整合到LLM中或添加外部知识以构建特定领域模型的方法受到了广泛关注。检索增强生成（RAG）作为一种推理时扩展方法，以其低成本和最小的参数调优工作量而著称。然而，由于训练数据和模型架构的异构性，RAG中使用的不同嵌入模型在不同区域表现出不同的优势，这通常导致不同的相似度计算结果，从而导致LLM的响应质量不同。为了解决这个问题，我们提出并研究了两种通过结合多个嵌入模型的优势来增强RAG的方法，分别命名为Mixture-Embedding RAG和Confident RAG。Mixture-Embedding RAG只是根据标准化相似度对来自多个嵌入模型的检索进行排序和选择；然而，它并没有超越普通的RAG。相比之下，Confident RAG使用不同的嵌入模型多次生成响应，然后选择置信度最高的响应，相对于普通的LLM和RAG分别平均提升了约10%和5%。不同LLM和嵌入模型上的一致结果表明，Confident RAG是一种适用于各种领域的高效即插即用方法。我们将在发表后发布代码。", "summary": "本文针对检索增强生成（RAG）中不同嵌入模型导致LLM响应质量不一致的问题，提出了两种改进方法：Mixture-Embedding RAG和Confident RAG。研究发现，Mixture-Embedding RAG未能超越普通RAG，而Confident RAG通过多次生成响应并选择最高置信度的方式，在不同LLM和嵌入模型上均实现了显著的性能提升，表明其是一种高效且通用的RAG增强方案。", "keywords": "RAG, 嵌入模型, LLM, Confident RAG, 检索增强生成", "comments": "本文识别并尝试解决RAG中嵌入模型异构性导致的关键问题。Mixture-Embedding RAG的失败和Confident RAG的成功对比，凸显了在多模型融合时，简单聚合（排序选择）不如基于置信度选择（集成决策）有效。Confident RAG作为一种“即插即用”方案，具有很高的实用价值和推广潜力。"}}
{"id": "2507.17015", "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ACL 2025", "url": "http://arxiv.org/abs/2507.17015v1", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "comment": "Accepted at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.17015v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "外部验证工具能否提高LLM作为评判者的标注质量？", "tldr": "本研究探索通过外部工具增强LLM作为评估者的能力，以提高在事实、数学和代码任务上的标注质量。", "motivation": "现有的大语言模型（LLM）评估方法，如成对偏好收集，在某些领域（如事实性内容）难以获得高质量标注，因为标注者可能过分关注写作质量而非事实准确性。", "method": "提出一个工具型智能体系统，通过网络搜索和代码执行进行外部验证，独立于LLM的内部知识和偏见，以提高长篇事实、数学和代码任务的标注质量。", "result": "外部工具在许多情况下确实能提高性能，但并非所有情况。实验表明性能对简单参数（如提示词）敏感，且需要改进（非饱和）的标注基准。", "conclusion": "外部验证工具可以有效提升LLM作为评判者在特定复杂领域的标注质量，但其效果受多种因素影响，且现有基准仍有改进空间。", "translation": "模型响应的成对偏好被广泛收集，用于评估大型语言模型（LLM）并提供反馈。对于同一输入的两个备选模型响应，人类或AI标注者会选择“更好”的响应。这种方法可以为难以获得其他硬编码指标的领域（例如，聊天响应质量）提供反馈，从而帮助模型评估或训练。然而，对于某些领域，高质量的成对比较难以获得——无论是来自AI还是人类。例如，对于包含许多事实陈述的响应，标注者可能过分关注写作质量而非底层事实。在这项工作中，我们探索通过额外工具增强标准AI标注系统，以提高在三个挑战性响应领域（长篇事实、数学和代码任务）的性能。我们提出了一个使用工具的代理系统，旨在为这些领域提供更高质量的反馈。我们的系统利用网络搜索和代码执行进行外部验证，独立于LLM的内部知识和偏见。我们提供了广泛的实验结果，评估了我们的方法在三个目标响应领域以及一般标注任务上的表现，使用了RewardBench（包括AlpacaEval和LLMBar）、RewardMath，以及针对现有数据集饱和的领域构建的三个新数据集。我们的结果表明，外部工具在许多情况下确实可以提高性能，但并非所有情况。更普遍地说，我们的实验强调了性能对简单参数（例如，提示词）的敏感性，以及对改进（非饱和）标注基准的需求。我们已在https://github.com/apple/ml-agent-evaluator 上分享了我们的代码。", "summary": "本研究探讨了如何通过外部验证工具提升“LLM作为评判者”的标注质量，尤其是在长篇事实、数学和代码等挑战性领域。作者提出了一个利用网络搜索和代码执行的工具型智能体系统，以实现独立于LLM内部知识和偏见的外部验证。实验结果表明，外部工具在多数情况下能有效提高性能，但并非全部，且性能对提示词等简单参数敏感，凸显了对更优质标注基准的需求。", "keywords": "LLM-as-a-Judge, 外部验证, 标注质量, 工具型智能体, 成对偏好", "comments": "这项研究的创新之处在于提出了一种利用外部工具（如网络搜索和代码执行）增强LLM作为评判者能力的代理系统，有效解决了LLM在评估事实性、数学和代码任务时可能存在的内在偏见和标注质量问题。其重要性在于为提高LLM评估的可靠性和准确性提供了新的途径。然而，研究也指出外部工具并非万能，且性能对简单参数敏感，这提示未来工作需关注鲁棒性和更精细的参数调优，并开发更具挑战性的评估基准。"}}
{"id": "2506.04134", "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation", "authors": ["Jinting Wang", "Shan Yang", "Li Liu"], "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2506.04134v2", "summary": "Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset show that our UniCUE achieves state-of-the-art performance across\nvarious metrics.", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.04134v2", "cate": "cs.CV", "date": "2025-06-04", "updated": "2025-07-23", "AI": {"title_translation": "UniCUE：中文手语提示语音视频到语音生成中的统一识别与生成框架", "tldr": "UniCUE是一个用于中文提示语音视频到语音生成（CSV2S）的统一框架，通过整合提示语音识别（CSR）任务来直接从CS视频生成语音，避免了中间文本导致的错误传播和时间错位，并在新数据集上取得了SOTA性能。", "motivation": "现有的CS视频到语音生成（CSV2S）方法因CS数据不足而性能不佳，且结合CS识别（CSR）和文本到语音系统的方法依赖文本作为中间媒介，易导致错误传播和语音与视频动态之间的时间错位。", "method": "本文提出了UniCUE，一个统一的CSV2S框架，其核心创新在于整合了CSR任务以提供细粒度的视觉语义信息。具体包括：1) 一个新颖的细粒度语义对齐池，确保视觉特征与语音内容的精确映射；2) 一个VisioPhonetic适配器，用于桥接跨任务表示；3) 一个姿态感知视觉处理器，增强唇部和手部动作的细粒度时空关联。", "result": "在新建的中文CS数据集上的实验表明，UniCUE在各项指标上均实现了最先进的性能。", "conclusion": "UniCUE通过直接从CS视频生成语音并整合CSR任务，有效解决了现有CSV2S方法的挑战，并在中文CS数据集上展现出卓越的性能，为听障人士的语音感知提供了更精确的支持。", "translation": "提示语音（CS）通过手部编码增强唇读，为听障人士提供精确的语音感知支持。CS视频到语音生成（CSV2S）任务旨在将听障人士的CS视觉表达（CS视频）转换为可理解的语音信号。由于CS数据不足，直接从CS视频生成语音（称为单CS视频到语音）的性能较差。当前研究主要集中在CS识别（CSR），即将视频内容转换为语言文本。在此基础上，CSV2S的一种直接方法是将CSR与文本到语音系统结合起来。这种组合架构依赖文本作为逐步跨模态对齐的中间媒介，这可能导致错误传播以及语音和视频动态之间的时间错位。为了解决这些挑战，我们提出了一种不依赖中间文本直接从CS视频生成语音的新方法。在此基础上，我们提出了UniCUE，这是第一个用于CSV2S的统一框架，其核心创新在于整合了CSR任务，该任务提供细粒度的视觉语义信息，以促进CS视频中的语音生成。更精确地说，（1）一个新颖的细粒度语义对齐池，确保视觉特征和语音内容之间的精确映射；（2）一个VisioPhonetic适配器，用于桥接跨任务表示，确保两个不同任务（即CSV2S和CSR）之间的无缝兼容性；（3）引入了一个姿态感知视觉处理器，以增强CS视频中唇部和手部动作之间的细粒度时空关联。在我们新建的中文CS数据集上的实验表明，我们的UniCUE在各项指标上均取得了最先进的性能。", "summary": "本文提出了UniCUE，一个针对中文手语提示语音视频到语音生成（CSV2S）的统一框架。为解决现有方法因数据不足、中间文本导致错误传播和时间错位的问题，UniCUE创新性地直接从CS视频生成语音，并通过整合提示语音识别（CSR）任务提供细粒度视觉语义信息。该框架包含细粒度语义对齐池、VisioPhonetic适配器和姿态感知视觉处理器，旨在优化视觉特征与语音内容的映射、跨任务表示的桥接以及唇手动作的时空关联。实验证明，UniCUE在新建的中文CS数据集上实现了最先进的性能。", "keywords": "提示语音, 视频到语音生成, 统一框架, 语音识别, 听障辅助", "comments": "UniCUE的创新性在于其统一的框架设计，巧妙地将CS识别任务融入CSV2S流程，从而避免了传统方法中对中间文本的依赖，有效解决了错误传播和时间错位问题。这种直接生成和跨任务信息融合的方法为听障辅助技术领域提供了一个重要且实用的进展。其在新建中文数据集上的SOTA表现也验证了其有效性。"}}
{"id": "2507.12941", "title": "Adaptive feature capture method for solving partial differential equations with near singular solutions", "authors": ["Yangtao Deng", "Qiaolin He", "Xiaoping Wang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12941v3", "summary": "Partial differential equations (PDEs) with near singular solutions pose\nsignificant challenges for traditional numerical methods, particularly in\ncomplex geometries where mesh generation and adaptive refinement become\ncomputationally expensive. While deep-learning-based approaches, such as\nPhysics-Informed Neural Networks (PINNs) and the Random Feature Method (RFM),\noffer mesh-free alternatives, they often lack adaptive resolution in critical\nregions, limiting their accuracy for solutions with steep gradients or\nsingularities. In this work, we propose the Adaptive Feature Capture Method\n(AFCM), a novel machine learning framework that adaptively redistributes\nneurons and collocation points in high-gradient regions to enhance local\nexpressive power. Inspired by adaptive moving mesh techniques, AFCM employs the\ngradient norm of an approximate solution as a monitor function to guide the\nreinitialization of feature function parameters. This ensures that partition\nhyperplanes and collocation points cluster where they are most needed,\nachieving higher resolution without increasing computational overhead. The AFCM\nextends the capabilities of RFM to handle PDEs with near-singular solutions\nwhile preserving its mesh-free efficiency. Numerical experiments demonstrate\nthe method's effectiveness in accurately resolving near-singular problems, even\nin complex geometries. By bridging the gap between adaptive mesh refinement and\nrandomized neural networks, AFCM offers a robust and scalable approach for\nsolving challenging PDEs in scientific and engineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12941v3", "cate": "math.NA", "date": "2025-07-17", "updated": "2025-07-23", "AI": {"title_translation": "求解近奇异解偏微分方程的自适应特征捕获方法", "tldr": "AFCM是一种新的机器学习框架，通过自适应地重新分布神经元和配置点来解决具有近奇异解的偏微分方程，提高精度同时保持无网格效率。", "motivation": "传统数值方法在复杂几何形状中处理具有近奇异解的偏微分方程时面临计算成本高昂的网格生成和自适应细化挑战。现有的基于深度学习的方法（如PINNs和RFM）缺乏在关键区域的自适应分辨率，限制了它们在处理陡峭梯度或奇异解时的准确性。", "method": "本文提出自适应特征捕获方法（AFCM），这是一种受自适应移动网格技术启发的机器学习框架。AFCM利用近似解的梯度范数作为监测函数，指导特征函数参数的重新初始化，从而在高梯度区域自适应地重新分布神经元和配置点，增强局部表达能力，实现更高分辨率而无需增加计算开销。", "result": "数值实验证明了该方法在准确解决近奇异问题方面的有效性，即使在复杂几何形状中也能表现良好。", "conclusion": "AFCM通过弥合自适应网格细化和随机神经网络之间的差距，为科学和工程应用中解决具有挑战性的偏微分方程提供了一种鲁棒且可扩展的方法。", "translation": "具有近奇异解的偏微分方程（PDEs）对传统数值方法提出了重大挑战，特别是在复杂几何形状中，网格生成和自适应细化变得计算成本高昂。虽然基于深度学习的方法，如物理信息神经网络（PINNs）和随机特征方法（RFM），提供了无网格替代方案，但它们通常缺乏在关键区域的自适应分辨率，限制了它们在处理具有陡峭梯度或奇异点的解时的准确性。在这项工作中，我们提出了自适应特征捕获方法（AFCM），这是一种新颖的机器学习框架，它在高梯度区域自适应地重新分布神经元和配置点，以增强局部表达能力。受自适应移动网格技术的启发，AFCM采用近似解的梯度范数作为监测函数，以指导特征函数参数的重新初始化。这确保了分区超平面和配置点在最需要的地方聚集，从而在不增加计算开销的情况下实现更高的分辨率。AFCM扩展了RFM的能力，以处理具有近奇异解的偏微分方程，同时保持其无网格效率。数值实验证明了该方法在准确解决近奇异问题方面的有效性，即使在复杂几何形状中也是如此。通过弥合自适应网格细化和随机神经网络之间的差距，AFCM为科学和工程应用中解决具有挑战性的偏微分方程提供了一种鲁棒且可扩展的方法。", "summary": "本文提出了一种名为自适应特征捕获方法（AFCM）的新型机器学习框架，旨在解决具有近奇异解的偏微分方程。针对传统数值方法和现有深度学习方法在处理高梯度区域时分辨率不足的挑战，AFCM通过自适应地重新分布神经元和配置点来增强局部表达能力，其灵感来源于自适应移动网格技术，并利用梯度范数作为指导。该方法扩展了随机特征方法的应用范围，实现了在不增加计算成本的情况下提高精度，并在数值实验中展示了其在复杂几何形状中解决近奇异问题的有效性，为科学和工程应用提供了鲁棒且可扩展的解决方案。", "keywords": "偏微分方程, 近奇异解, 自适应特征捕获方法, 机器学习, 无网格方法", "comments": "AFCM的创新之处在于其将自适应网格细化的思想（通过梯度范数引导）与无网格的随机神经网络（RFM）相结合，有效地解决了传统方法在处理近奇异解PDEs时面临的精度和计算效率问题。这种混合方法提供了一种有前景的解决方案，特别适用于需要高分辨率且计算资源有限的复杂问题。"}}
{"id": "2507.17514", "title": "TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment", "authors": ["Athanasios Davvetas", "Xenia Ziouvelou", "Ypatia Dami", "Alexis Kaponis", "Konstantina Giouvanopoulou", "Michael Papademas"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 1 figure, 4 tables", "url": "http://arxiv.org/abs/2507.17514v1", "summary": "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act.", "comment": "9 pages, 1 figure, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.17514v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "TAI Scan Tool：一个基于RAG的极简输入可信赖AI自我评估工具", "tldr": "TAI Scan Tool是一个基于RAG的AI自我评估工具，输入极简，主要用于AI法案合规性，并通过定性评估显示出良好的风险预测和条款检索能力。", "motivation": "当前AI系统需要进行可信赖AI（TAI）自我评估，特别是在遵守AI法案方面存在挑战。该工具旨在简化这一过程并协助合规。", "method": "该工具名为TAI Scan Tool，基于RAG（检索增强生成）技术，采用极简输入。它涉及两步法：预筛选和评估阶段。评估输出包括AI系统的风险级别以及相关的AI法案条款。", "result": "定性评估使用案例场景显示出有前景的结果，该工具能正确预测风险级别并检索到三个不同语义组的相关条款。工具的推理依赖于与高风险系统设置的比较。", "conclusion": "TAI Scan Tool是一个有效的、基于RAG的AI自我评估工具，能够辅助AI法案合规性，并能准确预测风险级别和提供相关法律条款。", "translation": "本文介绍了TAI扫描工具，一个基于RAG的可信赖AI自我评估工具，输入极简。该工具的当前版本支持法律TAI评估，特别强调促进符合AI法案。它采用两步法，包括预筛选和评估阶段。系统的评估输出包括根据AI法案对AI系统风险级别的洞察，同时检索相关条款以帮助合规并告知其义务。我们使用用例场景进行的定性评估取得了有前景的结果，在三个不同的语义组中正确预测了风险级别并检索了相关条款。此外，结果解释表明，该工具的推理依赖于与高风险系统设置的比较，这种行为归因于其部署需要仔细考虑，因此在AI法案中经常出现。", "summary": "本文介绍了TAI Scan Tool，一个基于RAG的AI自我评估工具，以极简输入支持AI法案的合规性评估。该工具采用两步法，能识别AI系统的风险级别并提供相关法律条款。定性评估表明其在风险预测和条款检索方面表现出色，特别关注高风险系统的合规性。", "keywords": "AI合规性, RAG, 自我评估, AI法案, 风险评估", "comments": "该工具的创新之处在于其将RAG技术应用于AI合规性自我评估，并强调极简输入，降低了用户门槛。其对AI法案合规性的关注使其在当前监管环境下具有重要意义。通过定性评估证明了其有效性，但抽象中未提及大规模量化评估或实际部署情况，可能是一个潜在的限制。"}}
{"id": "2507.17513", "title": "HOTA: Hamiltonian framework for Optimal Transport Advection", "authors": ["Nazar Buzun", "Daniil Shlenskii", "Maxim Bobrin", "Dmitry V. Dylov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17513v1", "summary": "Optimal transport (OT) has become a natural framework for guiding the\nprobability flows. Yet, the majority of recent generative models assume trivial\ngeometry (e.g., Euclidean) and rely on strong density-estimation assumptions,\nyielding trajectories that do not respect the true principles of optimality in\nthe underlying manifold. We present Hamiltonian Optimal Transport Advection\n(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical\nOT problem explicitly through Kantorovich potentials, enabling efficient and\nscalable trajectory optimization. Our approach effectively evades the need for\nexplicit density modeling, performing even when the cost functionals are\nnon-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,\nas well as in custom datasets with non-differentiable costs, both in terms of\nfeasibility and optimality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17513v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "HOTA：最优输运平流的哈密顿框架", "tldr": "HOTA是一种基于哈密顿-雅可比-贝尔曼方程的方法，用于解决动态最优输运问题，无需显式密度建模，并在各种基准测试中表现优异。", "motivation": "当前大多数生成模型在最优输运（OT）中假设了简单的几何结构（如欧几里得），并依赖于强密度估计假设，导致生成的轨迹不符合底层流形中的真正最优性原则。", "method": "本文提出了哈密顿最优输运平流（HOTA），这是一种基于哈密顿-雅可比-贝尔曼（Hamilton-Jacobi-Bellman）的方法。它通过康托罗维奇势显式地处理对偶动态OT问题，从而实现高效且可扩展的轨迹优化。该方法有效地避免了对显式密度建模的需求。", "result": "在标准基准测试以及具有不可微成本的自定义数据集中，HOTA在可行性和最优性方面均优于所有基线方法。", "conclusion": "HOTA是一种高效、可扩展的轨迹优化方法，它通过显式处理对偶动态OT问题，避免了显式密度建模的需求，即使在成本函数非光滑的情况下也能表现出色。", "translation": "最优输运（OT）已成为指导概率流动的自然框架。然而，近期大多数生成模型都假设了简单的几何结构（例如欧几里得），并依赖于强密度估计假设，导致生成的轨迹不符合底层流形中的真正最优性原则。我们提出了哈密顿最优输运平流（HOTA），这是一种基于哈密顿-雅可比-贝尔曼（Hamilton-Jacobi-Bellman）的方法，它通过康托罗维奇势显式地处理对偶动态OT问题，从而实现高效且可扩展的轨迹优化。我们的方法有效地避免了对显式密度建模的需求，即使在成本函数非光滑的情况下也能发挥作用。经验表明，HOTA在标准基准测试以及具有不可微成本的自定义数据集中，在可行性和最优性方面均优于所有基线方法。", "summary": "本文介绍了一种名为HOTA的哈密顿最优输运平流方法，旨在解决现有最优输运模型在处理复杂几何和避免显式密度建模方面的局限性。HOTA基于哈密顿-雅可比-贝尔曼方程，通过康托罗维奇势显式处理对偶动态最优输运问题，实现了高效且可扩展的轨迹优化。该方法无需显式密度建模，即使在非光滑成本函数下也能有效工作。实验结果表明，HOTA在各项基准测试中均优于现有方法。", "keywords": "最优输运, 哈密顿框架, 轨迹优化, 密度建模, 康托罗维奇势", "comments": "HOTA的创新之处在于其将哈密顿-雅可比-贝尔曼框架应用于最优输运问题，尤其是在解决动态OT的对偶问题时，通过避免显式密度建模，显著提升了方法的实用性和鲁棒性。这对于处理复杂数据分布和非光滑成本函数提供了新的途径，具有重要的理论和实践意义。"}}
{"id": "2507.17204", "title": "Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation", "authors": ["Zixuan Wang", "Jinghao Shi", "Hanzhong Liang", "Xiang Shen", "Vera Wen", "Zhiqian Chen", "Yifan Wu", "Zhixin Zhang", "Hongyu Xiong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Camera Ready for ACL 2025", "url": "http://arxiv.org/abs/2507.17204v1", "summary": "Effective content moderation is essential for video platforms to safeguard\nuser experience and uphold community standards. While traditional video\nclassification models effectively handle well-defined moderation tasks, they\nstruggle with complicated scenarios such as implicit harmful content and\ncontextual ambiguity. Multimodal large language models (MLLMs) offer a\npromising solution to these limitations with their superior cross-modal\nreasoning and contextual understanding. However, two key challenges hinder\ntheir industrial adoption. First, the high computational cost of MLLMs makes\nfull-scale deployment impractical. Second, adapting generative models for\ndiscriminative classification remains an open research problem. In this paper,\nwe first introduce an efficient method to transform a generative MLLM into a\nmultimodal classifier using minimal discriminative training data. To enable\nindustry-scale deployment, we then propose a router-ranking cascade system that\nintegrates MLLMs with a lightweight router model. Offline experiments\ndemonstrate that our MLLM-based approach improves F1 score by 66.50% over\ntraditional classifiers while requiring only 2% of the fine-tuning data. Online\nevaluations show that our system increases automatic content moderation volume\nby 41%, while the cascading deployment reduces computational cost to only 1.5%\nof direct full-scale deployment.", "comment": "Camera Ready for ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.17204v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Filter-And-Refine: 基于多模态大语言模型的工业级视频内容审核级联系统", "tldr": "提出了一种基于MLLM的级联系统Filter-And-Refine，用于高效、大规模视频内容审核，解决了传统模型在复杂场景下的不足和MLLM部署成本高的问题。", "motivation": "视频平台内容审核至关重要，但传统模型难以处理隐式有害内容和上下文歧义。多模态大语言模型（MLLMs）虽有潜力，但面临计算成本高和生成模型适应判别分类的挑战，阻碍了其工业应用。", "method": "该论文首先提出一种高效方法，利用少量判别训练数据将生成式MLLM转换为多模态分类器。然后，为了实现工业级部署，提出了一个结合MLLM和轻量级路由模型的“路由器-排序”级联系统。", "result": "离线实验表明，该基于MLLM的方法比传统分类器将F1分数提高了66.50%，且仅需2%的微调数据。在线评估显示，该系统将自动内容审核量增加了41%，同时级联部署将计算成本降低到直接全面部署的1.5%。", "conclusion": "该论文成功开发并验证了一个高效、可扩展的MLLM级联系统，显著提升了视频内容审核的性能和效率，并大幅降低了部署成本，为工业级应用提供了可行方案。", "translation": "有效的视频内容审核对于视频平台维护用户体验和遵守社区标准至关重要。虽然传统的视频分类模型能有效处理明确的审核任务，但它们在处理复杂场景（如隐式有害内容和上下文歧义）时表现不佳。多模态大语言模型（MLLMs）凭借其卓越的跨模态推理和上下文理解能力，为解决这些局限性提供了有前景的方案。然而，有两个关键挑战阻碍了它们的工业应用。首先，MLLM的高计算成本使得全面部署不切实际。其次，将生成模型用于判别分类仍然是一个开放的研究问题。在本文中，我们首先介绍了一种高效方法，利用最少的判别训练数据将生成式MLLM转换为多模态分类器。为了实现工业级部署，我们接着提出了一个集成了MLLM和轻量级路由模型的“路由器-排序”级联系统。离线实验表明，我们基于MLLM的方法比传统分类器将F1分数提高了66.50%，同时仅需要2%的微调数据。在线评估显示，我们的系统将自动内容审核量增加了41%，而级联部署将计算成本降低到直接全面部署的1.5%。", "summary": "本文提出了一种名为Filter-And-Refine的基于多模态大语言模型（MLLM）的级联系统，旨在解决工业级视频内容审核中传统模型在复杂场景下的不足以及MLLM部署成本高昂的问题。该系统首先将生成式MLLM高效转换为判别分类器，然后通过一个路由器-排序级联架构整合轻量级模型以优化部署。实验证明，该方法在F1分数上显著优于传统分类器，仅需少量训练数据，并大幅提升了审核量和降低了计算成本。", "keywords": "视频内容审核, 多模态大语言模型 (MLLM), 级联系统, 计算效率, 内容分类", "comments": "该论文的创新点在于提出了一个将MLLM应用于工业级内容审核的实用解决方案，克服了MLLM计算成本高和生成模型难以用于判别分类的挑战。其“Filter-And-Refine”级联系统设计巧妙，通过结合轻量级模型实现了高效部署，显著提升了审核性能和效率，对于大规模视频平台的内容安全具有重要意义。"}}
{"id": "2507.17634", "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17634v1", "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17634v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "WSM：通过检查点合并实现LLM预训练的无衰减学习率调度", "tldr": "WSM是一种通过检查点合并实现无衰减学习率调度的通用框架，在LLM预训练中表现优于传统方法。", "motivation": "现有的学习率调度方法中，无衰减方法显示出有效性，而模型合并技术在该领域前景广阔。本文旨在建立学习率衰减与模型合并之间的正式联系，并提出一种更优的无衰减学习率调度方案。", "method": "本文提出了Warmup-Stable and Merge (WSM) 框架，它正式连接了学习率衰减和模型合并。WSM提供了一个统一的理论基础，将余弦衰减、线性衰减和逆平方根衰减等多种衰减策略模拟为有原则的模型平均方案，并与各种优化方法兼容。实验中，发现合并持续时间是影响模型性能的最关键因素。", "result": "WSM框架在多个基准测试中持续优于广泛采用的Warmup-Stable-Decay (WSD) 方法，在MATH上提升了+3.5%，在HumanEval上提升了+2.9%，在MMLU-Pro上提升了+5.5%。其性能优势也延伸到监督微调场景。", "conclusion": "WSM框架通过将学习率衰减与模型合并建立正式联系，提供了一种高效且性能优越的无衰减学习率调度方法，适用于大型语言模型的预训练和微调，并能实现模型的长期优化。", "translation": "学习率（LR）调度方面的最新进展表明，无衰减方法在消除传统衰减阶段的同时保持了竞争性性能。模型合并技术在该领域显示出特别有前景的解决方案。我们提出了Warmup-Stable and Merge (WSM)，这是一个通用框架，建立了学习率衰减和模型合并之间的正式联系。WSM为模拟各种衰减策略（包括余弦衰减、线性衰减和逆平方根衰减）提供了统一的理论基础，将其作为有原则的模型平均方案，同时与各种优化方法完全兼容。通过广泛的实验，我们发现合并持续时间（即检查点聚合的训练窗口）是影响模型性能的最关键因素，其重要性超过了检查点间隔和合并数量。我们的框架在多个基准测试中持续优于广泛采用的Warmup-Stable-Decay (WSD) 方法，在MATH上取得了+3.5%的显著改进，在HumanEval上取得了+2.9%的改进，在MMLU-Pro上取得了+5.5%的改进。性能优势延伸到监督微调场景，突出了WSM在长期模型优化方面的潜力。", "summary": "本文提出了WSM（Warmup-Stable and Merge）框架，旨在通过检查点合并实现无衰减的学习率调度，以优化大型语言模型（LLM）的预训练。WSM将学习率衰减与模型合并正式关联，提供了一个统一的理论基础来模拟多种衰减策略。实验证明，合并持续时间是影响性能的关键因素，WSM在多个基准测试中显著优于传统WSD方法，并在监督微调中展现出潜力，为LLM的长期优化提供了新的途径。", "keywords": "学习率调度, 检查点合并, 无衰减, LLM预训练, WSM", "comments": "WSM的创新之处在于它首次形式化地连接了学习率衰减和模型合并，提供了一个统一的理论视角来理解和实现无衰减的学习率调度。其发现合并持续时间是关键因素具有重要的实践指导意义。该方法在多个LLM基准测试中表现出显著优于WSD的性能，并能扩展到微调场景，这表明了其在LLM训练和优化中的重要性和潜力。"}}
{"id": "2506.02634", "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider", "authors": ["Jiahao Wang", "Jinbo Han", "Xingda Wei", "Sijie Shen", "Dingyan Zhang", "Chenguang Fang", "Rong Chen", "Wenyuan Yu", "Haibo Chen"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted by USENIX ATC'25", "url": "http://arxiv.org/abs/2506.02634v4", "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.", "comment": "Accepted by USENIX ATC'25", "pdf_url": "http://arxiv.org/pdf/2506.02634v4", "cate": "cs.DC", "date": "2025-06-03", "updated": "2025-07-23", "AI": {"title_translation": "KVCache 在实际应用中的缓存：大型云服务提供商的 KVCache 缓存特性分析与优化", "tldr": "本文对大型云服务提供商的 LLM KVCache 工作负载模式进行了首次系统性分析，发现其重用模式的偏斜性、可预测性及适度的缓存需求。在此基础上，提出了一种工作负载感知的缓存淘汰策略，有效提升了实际场景下的服务性能。", "motivation": "大型语言模型 (LLM) 服务对云提供商至关重要，而缓存中间结果 (KV$) 可以显著提高服务吞吐量和延迟。然而，目前对 LLM 服务如何从 KV$ 缓存中受益的理解有限，尤其是在缓存逐出策略等系统设计决策高度依赖工作负载的情况下。", "method": "本文首先对来自领先 LLM 服务提供商的 KV$ 工作负载模式进行了首次系统性特征分析。在此特征分析的基础上，提出了一种工作负载感知的缓存淘汰策略。", "result": "研究发现：KV$ 重用在请求之间存在偏斜，单轮请求和多轮请求之间的重用同样重要；考虑所有请求时，重用时间和概率是多样化的，但对于特定请求类别，模式趋于可预测；理想缓存命中率所需的总缓存大小适中。基于这些特性，提出的工作负载感知缓存淘汰策略在真实世界跟踪下提高了服务性能，尤其是在缓存容量有限的情况下。", "conclusion": "本文首次系统性地表征了大型云提供商的 LLM KVCache 工作负载模式，揭示了实际应用中的重用特性，并基于此提出了一种有效的工作负载感知缓存淘汰策略，验证了其在提升真实世界服务性能方面的有效性。", "translation": "为大型语言模型（LLM）提供服务对云提供商至关重要，在处理每个请求后缓存中间结果（KV$）能显著提高服务吞吐量和延迟。然而，目前对 LLM 服务如何从 KV$ 缓存中受益的理解有限，例如缓存逐出策略等系统设计决策高度依赖工作负载。在本文中，我们首次对来自领先 LLM 服务提供商的 KV$ 工作负载模式进行了系统性特征分析。我们得出了先前侧重于合成工作负载的研究中未涵盖的观察结果，包括：KV$ 重用在请求之间存在偏斜，其中单轮请求和多轮请求之间的重用同样重要；考虑所有请求时，重用时间和概率是多样化的，但对于特定请求类别，模式趋于可预测；理想缓存命中率所需的总缓存大小适中。基于这些特征，我们进一步提出了一种工作负载感知的缓存逐出策略，该策略在真实世界跟踪下提高了服务性能，尤其是在缓存容量有限的情况下。", "summary": "本研究首次对大型云服务提供商的 LLM KVCache 工作负载模式进行了系统性分析，揭示了实际应用中不同于合成工作负载的重用特性，如重用偏斜性、特定请求类别的可预测性以及适度的理想缓存大小。在此基础上，文章提出了一种工作负载感知的缓存淘汰策略，并验证了其在有限缓存容量下提升真实服务性能的有效性。", "keywords": "LLM, KVCache, 缓存优化, 工作负载分析, 缓存淘汰策略", "comments": "本文的创新点在于对真实世界大规模 LLM KVCache 工作负载进行了首次系统性分析，弥补了以往研究多集中于合成工作负载的不足。其发现对于优化 LLM 缓存系统设计具有重要指导意义，尤其是提出的工作负载感知淘汰策略，为实际应用提供了有效的性能提升方案。"}}
{"id": "2507.17013", "title": "laplax -- Laplace Approximations with JAX", "authors": ["Tobias Weber", "Bálint Mucsányi", "Lenard Rommel", "Thomas Christie", "Lars Kasüschke", "Marvin Pförtner", "Philipp Hennig"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submission to the ICML 2025 Workshop on Championing Open-source Development in Machine Learning (CODEML '25)", "url": "http://arxiv.org/abs/2507.17013v1", "summary": "The Laplace approximation provides a scalable and efficient means of\nquantifying weight-space uncertainty in deep neural networks, enabling the\napplication of Bayesian tools such as predictive uncertainty and model\nselection via Occam's razor. In this work, we introduce laplax, a new\nopen-source Python package for performing Laplace approximations with jax.\nDesigned with a modular and purely functional architecture and minimal external\ndependencies, laplax offers a flexible and researcher-friendly framework for\nrapid prototyping and experimentation. Its goal is to facilitate research on\nBayesian neural networks, uncertainty quantification for deep learning, and the\ndevelopment of improved Laplace approximation techniques.", "comment": "Submission to the ICML 2025 Workshop on Championing Open-source\n  Development in Machine Learning (CODEML '25)", "pdf_url": "http://arxiv.org/pdf/2507.17013v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "laplax -- 基于 JAX 的拉普拉斯近似", "tldr": "laplax是一个新的开源Python包，用于使用JAX进行拉普拉斯近似，旨在促进贝叶斯神经网络和深度学习不确定性量化的研究。", "motivation": "深度神经网络中的权重空间不确定性量化需要可扩展且高效的手段，以应用贝叶斯工具（如预测不确定性和模型选择）。", "method": "引入了laplax，一个基于JAX的新的开源Python包，用于执行拉普拉斯近似。它采用模块化、纯函数式架构，并具有最少的外部依赖。", "result": "laplax提供了一个灵活且对研究人员友好的框架，用于快速原型设计和实验。", "conclusion": "laplax的目标是促进贝叶斯神经网络、深度学习不确定性量化以及改进拉普拉斯近似技术的研究。", "translation": "拉普拉斯近似提供了一种可扩展且高效的方法来量化深度神经网络中的权重空间不确定性，从而能够应用贝叶斯工具，如预测不确定性和通过奥卡姆剃刀进行模型选择。在这项工作中，我们介绍了laplax，一个用于使用JAX执行拉普拉斯近似的新型开源Python包。laplax采用模块化和纯函数式架构设计，并具有最少的外部依赖，为快速原型设计和实验提供了一个灵活且对研究人员友好的框架。其目标是促进贝叶斯神经网络、深度学习不确定性量化以及改进拉普拉斯近似技术的研究。", "summary": "laplax是一个新的开源Python包，利用JAX实现拉普拉斯近似，以高效量化深度神经网络的权重空间不确定性。它采用模块化、纯函数式设计，旨在为贝叶斯神经网络、深度学习不确定性量化及拉普拉斯近似技术的研究提供灵活友好的开发框架。", "keywords": "拉普拉斯近似, JAX, 贝叶斯神经网络, 不确定性量化, 深度学习", "comments": "laplax的创新之处在于其将拉普拉斯近似与JAX结合，提供了一个纯函数式、模块化的开源工具，这有望极大地简化和加速深度学习中贝叶斯不确定性量化的研究和应用。其低依赖性也增加了其实用性。"}}
{"id": "2506.04260", "title": "Turning to Online Forums for Legal Information: A Case Study of GDPR's Legitimate Interests", "authors": ["Lin Kyi", "Cristiana Santos", "Sushil Ammanaghatta Shivakumar", "Franziska Roesner", "Asia Biega"], "categories": ["cs.CY", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at Annual Privacy Forum 2025", "url": "http://arxiv.org/abs/2506.04260v2", "summary": "Practitioners building online services and tools often turn to online forums\nsuch as Reddit, Law Stack Exchange, and Stack Overflow for legal guidance to\nensure compliance with the GDPR. The legal information presented in these\nforums directly impacts present-day industry practitioner's decisions. Online\nforums can serve as gateways that, depending on the accuracy and quality of the\nanswers provided, may either support or undermine the protection of privacy and\ndata protection fundamental rights. However, there is a need for deeper\ninvestigation into practitioners' decision-making processes and their\nunderstanding of legal compliance when seeking for legal information online.\n  Using GDPR's ``legitimate interests'' legal ground for processing personal\ndata as a case study, we investigate how practitioners use online forums to\nidentify common areas of confusion in applying legitimate interests in\npractice, and evaluate how legally sound online forum responses are.\n  Our analysis found that applying the legal basis of legitimate interest is\ncomplex for practitioners, with important implications for how the GDPR is\nimplemented in practice. The legal analysis showed that crowdsourced legal\ninformation tends to be legally sound, though sometimes incomplete. We outline\nrecommendations to improve the quality of online forums by ensuring that\nresponses are more legally sound and comprehensive, enabling practitioners to\napply legitimate interests effectively in practice and uphold the GDPR.", "comment": "Accepted at Annual Privacy Forum 2025", "pdf_url": "http://arxiv.org/pdf/2506.04260v2", "cate": "cs.CY", "date": "2025-06-02", "updated": "2025-07-23", "AI": {"title_translation": "转向在线论坛获取法律信息：以GDPR的合法利益为例", "tldr": "从业者常求助于在线论坛获取GDPR法律指导，尤其是关于“合法利益”的。本研究发现论坛上的建议虽然大多合法但常不完整，揭示了应用中的复杂性，并提出了改进建议。", "motivation": "从事在线服务和工具的从业者经常求助于在线论坛（如Reddit、Law Stack Exchange和Stack Overflow）获取法律指导，以确保符合GDPR。这些论坛中呈现的法律信息直接影响着从业者的决策。在线论坛提供的答案的准确性和质量可能支持或损害隐私和数据保护的基本权利。然而，目前对从业者在在线寻求法律信息时的决策过程以及他们对法律合规性的理解缺乏深入调查。", "method": "本研究以GDPR中处理个人数据的“合法利益”法律依据作为案例研究，调查从业者如何使用在线论坛，以识别在实践中应用合法利益时常见的困惑领域，并评估在线论坛回复的法律健全性。", "result": "分析发现，对于从业者而言，应用合法利益的法律基础是复杂的，这对GDPR在实践中的实施具有重要影响。法律分析表明，众包的法律信息往往是法律健全的，尽管有时不完整。", "conclusion": "应用合法利益的复杂性对GDPR的实施具有重要影响。本研究提出了改进在线论坛质量的建议，以确保回复更具法律健全性和全面性，从而使从业者能够在实践中有效应用合法利益并维护GDPR。", "translation": "从事在线服务和工具的从业者经常求助于Reddit、Law Stack Exchange和Stack Overflow等在线论坛，以获取法律指导，确保符合GDPR。这些论坛中呈现的法律信息直接影响着当今行业从业者的决策。在线论坛可以作为门户，其提供的答案的准确性和质量可能支持或损害隐私和数据保护的基本权利。然而，需要对从业者在在线寻求法律信息时的决策过程以及他们对法律合规性的理解进行更深入的调查。我们以GDPR中处理个人数据的“合法利益”法律依据作为案例研究，调查从业者如何使用在线论坛来识别在实践中应用合法利益时常见的困惑领域，并评估在线论坛回复的法律健全性。我们的分析发现，对于从业者而言，应用合法利益的法律基础是复杂的，这对GDPR在实践中的实施具有重要影响。法律分析表明，众包的法律信息往往是法律健全的，尽管有时不完整。我们提出了改进在线论坛质量的建议，以确保回复更具法律健全性和全面性，从而使从业者能够在实践中有效应用合法利益并维护GDPR。", "summary": "本文调查了从业者如何利用Reddit等在线论坛获取GDPR法律指导，特别是针对“合法利益”这一法律基础。研究指出，尽管在线论坛的法律建议通常是合法的，但可能不完整，这使得GDPR的实施复杂化。该研究识别了常见的困惑领域，并提出了提高这些平台法律信息质量和全面性的建议，以更好地支持从业者。", "keywords": "GDPR, 在线论坛, 合法利益, 法律信息, 数据保护", "comments": "本文揭示了从业者依赖非正式在线论坛处理复杂法律合规（GDPR）这一关键的现实实践。其创新之处在于系统地分析了众包法律建议的质量和影响。研究结果强调了法律理论与实践应用之间存在的差距，特别是对于“合法利益”而言，以及可访问、准确的法律指导的重要性。提出的建议对于提高此类论坛的实用性具有实际意义。"}}
{"id": "2507.17359", "title": "Exploring Active Learning for Semiconductor Defect Segmentation", "authors": ["Lile Cai", "Ramanpreet Singh Pahwa", "Xun Xu", "Jie Wang", "Richard Chang", "Lining Zhang", "Chuan-Sheng Foo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted to ICIP 2022", "url": "http://arxiv.org/abs/2507.17359v1", "summary": "The development of X-Ray microscopy (XRM) technology has enabled\nnon-destructive inspection of semiconductor structures for defect\nidentification. Deep learning is widely used as the state-of-the-art approach\nto perform visual analysis tasks. However, deep learning based models require\nlarge amount of annotated data to train. This can be time-consuming and\nexpensive to obtain especially for dense prediction tasks like semantic\nsegmentation. In this work, we explore active learning (AL) as a potential\nsolution to alleviate the annotation burden. We identify two unique challenges\nwhen applying AL on semiconductor XRM scans: large domain shift and severe\nclass-imbalance. To address these challenges, we propose to perform contrastive\npretraining on the unlabelled data to obtain the initialization weights for\neach AL cycle, and a rareness-aware acquisition function that favors the\nselection of samples containing rare classes. We evaluate our method on a\nsemiconductor dataset that is compiled from XRM scans of high bandwidth memory\nstructures composed of logic and memory dies, and demonstrate that our method\nachieves state-of-the-art performance.", "comment": "accepted to ICIP 2022", "pdf_url": "http://arxiv.org/pdf/2507.17359v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "探索用于半导体缺陷分割的主动学习", "tldr": "本文探讨了主动学习在半导体X射线显微图像缺陷分割中的应用，通过对比预训练和稀有度感知采集函数解决了领域漂移和类别不平衡问题，实现了最先进的性能。", "motivation": "深度学习模型在半导体X射线显微图像缺陷识别中需要大量标注数据，尤其对于语义分割等密集预测任务，标注耗时且昂贵。因此，需要一种方法来减轻标注负担。", "method": "本文探索了主动学习（AL）以减轻标注负担。针对半导体XRM扫描中领域漂移和严重类别不平衡的挑战，提出了对未标注数据进行对比预训练以获取每个AL循环的初始化权重，并设计了一种偏好选择包含稀有类别样本的稀有度感知采集函数。", "result": "在由高带宽存储器结构XRM扫描组成的半导体数据集上，本文方法达到了最先进的性能。", "conclusion": "主动学习结合对比预训练和稀有度感知采集函数，能够有效解决半导体缺陷分割中数据标注的挑战，并取得优异表现。", "translation": "X射线显微镜（XRM）技术的发展使得对半导体结构进行无损检测以识别缺陷成为可能。深度学习作为最先进的方法被广泛应用于视觉分析任务。然而，基于深度学习的模型需要大量的标注数据进行训练。这可能耗时且昂贵，特别是对于语义分割等密集预测任务。在这项工作中，我们探索主动学习（AL）作为减轻标注负担的潜在解决方案。我们发现在半导体XRM扫描上应用AL时存在两个独特的挑战：大的领域漂移和严重的类别不平衡。为了解决这些挑战，我们提出对未标注数据执行对比预训练，以获得每个AL循环的初始化权重，并提出一个稀有度感知采集函数，该函数有利于选择包含稀有类别的样本。我们在一个从高带宽存储器结构（由逻辑和内存芯片组成）的XRM扫描中编译的半导体数据集上评估了我们的方法，并证明我们的方法实现了最先进的性能。", "summary": "本文针对半导体X射线显微（XRM）图像中缺陷分割任务的数据标注难题，提出将主动学习（AL）作为解决方案。鉴于XRM扫描存在的领域漂移和类别严重不平衡问题，作者创新性地引入了对比预训练来初始化AL循环权重，并设计了稀有度感知采集函数以优先选择稀有类别样本。实验结果表明，该方法在半导体数据集上取得了领先的性能，有效减轻了标注负担。", "keywords": "主动学习, 半导体缺陷分割, X射线显微镜, 对比预训练, 类别不平衡", "comments": "本文的创新点在于将主动学习应用于半导体缺陷分割这一特定领域，并针对该领域特有的领域漂移和类别不平衡问题提出了有效的解决方案（对比预训练和稀有度感知采集函数）。这对于减少工业级缺陷检测中的数据标注成本具有重要意义，展现了主动学习在特定高要求视觉任务中的潜力。"}}
{"id": "2507.17356", "title": "\"Beyond the past\": Leveraging Audio and Human Memory for Sequential Music Recommendation", "authors": ["Viet-Tran Anh", "Bruno Sguerra", "Gabriel Meseguer-Brocal", "Lea Briand", "Manuel Moussallam"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17356v1", "summary": "On music streaming services, listening sessions are often composed of a\nbalance of familiar and new tracks. Recently, sequential recommender systems\nhave adopted cognitive-informed approaches, such as Adaptive Control of\nThought-Rational (ACT-R), to successfully improve the prediction of the most\nrelevant tracks for the next user session. However, one limitation of using a\nmodel inspired by human memory (or the past), is that it struggles to recommend\nnew tracks that users have not previously listened to. To bridge this gap, here\nwe propose a model that leverages audio information to predict in advance the\nACT-R-like activation of new tracks and incorporates them into the\nrecommendation scoring process. We demonstrate the empirical effectiveness of\nthe proposed model using proprietary data, which we publicly release along with\nthe model's source code to foster future research in this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17356v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "\"超越过去\"：利用音频和人类记忆进行序列音乐推荐", "tldr": "现有基于人类记忆的音乐推荐系统难以推荐新歌曲。本文提出一种结合音频信息预测新歌曲激活度并融入推荐评分过程的模型，有效解决了这一问题。", "motivation": "现有基于人类记忆（如ACT-R）的序列音乐推荐系统在推荐用户未曾听过的新歌曲时存在局限性。", "method": "提出了一种新模型，该模型利用音频信息提前预测新歌曲类似ACT-R的激活度，并将其整合到推荐评分过程中。", "result": "提出的模型在专有数据集上显示出经验有效性。", "conclusion": "该模型成功地通过结合音频信息解决了现有基于人类记忆的推荐系统在推荐新歌曲方面的不足，并为未来的研究提供了数据和代码支持。", "translation": "在音乐流媒体服务中，听歌会话通常由熟悉和新颖的曲目平衡组成。最近，序列推荐系统采用了认知启发式方法，如自适应思维控制-理性（ACT-R），成功地提高了对下一个用户会话最相关曲目的预测。然而，使用受人类记忆（或过去）启发的模型的一个局限性是，它难以推荐用户以前未曾听过的新曲目。为了弥补这一差距，我们提出了一种利用音频信息提前预测新曲目类似ACT-R激活度并将其纳入推荐评分过程的模型。我们使用专有数据证明了所提出模型的经验有效性，并与模型的源代码一起公开发布，以促进该领域的未来研究。", "summary": "本文旨在解决现有基于人类记忆（如ACT-R）的序列音乐推荐系统在推荐新歌曲方面的不足。研究者提出了一种新模型，该模型通过整合音频信息来预测新歌曲的激活度，并将其纳入推荐评分机制。实验结果表明，该模型能够有效提升新歌曲的推荐能力，并且研究团队公开了相关数据和源代码，以鼓励后续研究。", "keywords": "序列音乐推荐, 音频信息, 人类记忆, ACT-R", "comments": "这篇论文的创新点在于将音频信息引入到基于人类记忆的推荐模型中，有效解决了现有模型在推荐新颖曲目上的局限性。公开数据集和源代码的举动非常重要，将极大地促进该领域的研究进展和可复现性。"}}
{"id": "2507.12106", "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "categories": ["cs.DC", "cs.CY"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      18 pages, 6 Figures", "url": "http://arxiv.org/abs/2507.12106v2", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "comment": "18 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2507.12106v2", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "城市绿色治理：物联网驱动的坎波巴索城市绿地管理与提升", "tldr": "该项目展示了如何利用物联网和数据驱动平台实现城市绿地的可持续、实时管理，以提升城市居民的福祉和环境韧性。", "motivation": "高效设计和管理公共绿地是提升城市人口健康和福祉的关键因素，绿地作为城市生态系统的“绿色之肺”，通过提供生态系统服务在提升生活质量方面发挥着至关重要的作用。", "method": "该项目整合了物联网系统和数据驱动的治理平台，通过决策支持系统（DSS）实时监测树木和绿地的健康状况。它还收集和分析来自多种来源的数据，包括天气条件、空气质量、土壤湿度、污染水平。基于云平台的系统支持对绿地进行整体实时决策，并利用Tree Talker传感器以及土壤湿度和水势监测系统进行智能控制和管理。通过基于机器学习算法和物联网传感器实时数据建立的预测模型，优化了公园灌溉，并提供自定义警报。", "result": "该项目展示了数字化、物联网传感器融合和技术创新如何支持可持续城市治理，促进环境韧性并改善公民生活质量。它实现了对城市绿地的实时监控、数据收集与分析、灌溉优化和异常警报。", "conclusion": "通过利用物联网、数据驱动平台和预测模型，可以实现城市绿地的可持续、实时管理，从而提升环境韧性并改善城市居民的生活质量。该坎波巴索的案例证明了技术创新在城市绿色治理中的潜力。", "translation": "公共绿地的高效设计和管理是促进城市人口健康和福祉的关键因素，这一点得到了世界卫生组织、联合国环境规划署和欧洲环境署的强调。这些区域作为城市生态系统的“绿色之肺”，通过提供生态系统服务在提升生活质量方面发挥着至关重要的作用。在此背景下，由意大利企业部（MIMIT）资助的坎波巴索市“智慧绿色城市”用例，通过采用先进的集成和可互操作的新兴技术系统，成为城市绿色区域可持续管理的一个创新模式。该项目整合了物联网系统和数据驱动的治理平台，通过决策支持系统（DSS）实现对树木和绿地健康状况的实时监测。它还促进了从不同来源收集和分析数据，包括天气条件、空气质量、土壤湿度、污染水平。由此产生的基于云的平台支持对城市绿地管理者、技术专家和操作人员进行整体实时决策。它利用Tree Talker传感器，并与土壤湿度和水势监测系统集成，实现对城市绿地的智能控制和管理。得益于基于机器学习算法和物联网传感器提供的实时数据的预测模型，可以通过提供何时以及施用多少水的建议来优化公共公园的灌溉。当监测参数（如土壤温度、湿度或水势）超过预设阈值时，还会激活自定义警报层以警告用户。该用例展示了数字化、物联网传感器融合和技术创新如何支持可持续城市治理，促进环境韧性并改善公民生活质量。", "summary": "该论文介绍了坎波巴索市的“智慧绿色城市”项目，这是一个由物联网和数据驱动平台支持的创新模型，用于可持续管理和提升城市绿地。该系统通过集成传感器和预测模型，实现对树木和绿地健康状况的实时监测、数据分析、灌溉优化和异常警报，旨在提升城市居民的健康福祉和环境韧性。", "keywords": "城市绿色治理, 物联网, 智慧城市, 绿地管理, 数据驱动", "comments": "该论文展示了一个将前沿技术（物联网、机器学习、云计算）应用于城市绿地管理的实际案例，具有很强的应用价值和创新性。其亮点在于通过实时数据和预测模型实现精细化管理和决策支持，有助于提升城市环境质量和居民生活水平。该模式具有推广潜力。"}}
{"id": "2507.17543", "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams", "authors": ["Xue Wen Tan", "Kenneth See", "Stanley Kok"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17543v1", "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the Anticipate,\nSimulate, Reason (ASR) framework, a generative AI method that enables users to\nproactively identify and comprehend scams within instant messaging platforms.\nUsing large language models, ASR predicts scammer responses, creates realistic\nscam conversations, and delivers real-time, interpretable support to end-users.\nWe develop ScamGPT-J, a domain-specific language model fine-tuned on a new,\nhigh-quality dataset of scam conversations covering multiple scam types.\nThorough experimental evaluation shows that the ASR framework substantially\nenhances scam detection, particularly in challenging contexts such as job\nscams, and uncovers important demographic patterns in user vulnerability and\nperceptions of AI-generated assistance. Our findings reveal a contradiction\nwhere those most at risk are often least receptive to AI support, emphasizing\nthe importance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable,\nhuman-centered AI systems in combating evolving digital threats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17543v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "预见、模拟、推理（ASR）：一个打击消息诈骗的综合生成式AI框架", "tldr": "ASR是一个生成式AI框架，利用大型语言模型预测诈骗者响应，创建模拟诈骗对话，并提供实时、可解释的支持，以帮助用户主动识别和理解消息诈骗。", "motivation": "消息诈骗的快速增长对用户安全和财务安全构成了日益严峻的挑战，因此需要一种主动识别和理解诈骗的方法。", "method": "本研究提出了预见、模拟、推理（ASR）框架，这是一种生成式AI方法，利用大型语言模型预测诈骗者响应、创建逼真的诈骗对话，并为终端用户提供实时、可解释的支持。该框架开发了ScamGPT-J，这是一个在高质量诈骗对话数据集上微调的领域特定语言模型。", "result": "实验评估表明，ASR框架显著增强了诈骗检测能力，尤其是在求职诈骗等挑战性情境中。研究还揭示了用户脆弱性和对AI生成辅助感知的关键人口模式，发现高风险人群往往最不接受AI支持。", "conclusion": "ASR框架通过提供可解释、以人为本的AI系统，在打击不断演变的数字威胁方面，推进了实践和理论基础。研究强调了在AI驱动的欺诈预防中，以用户为中心设计的重要性，因为高风险人群可能对AI支持的接受度较低。", "translation": "消息诈骗的快速增长对用户安全和财务安全构成了日益严峻的挑战。本文提出了预见、模拟、推理（ASR）框架，这是一种生成式AI方法，使用户能够主动识别和理解即时通讯平台中的诈骗。ASR利用大型语言模型预测诈骗者响应，创建逼真的诈骗对话，并为终端用户提供实时、可解释的支持。我们开发了ScamGPT-J，这是一个在涵盖多种诈骗类型的高质量诈骗对话新数据集上进行微调的领域特定语言模型。全面的实验评估表明，ASR框架显著增强了诈骗检测能力，尤其是在求职诈骗等挑战性情境中，并揭示了用户脆弱性和对AI生成辅助感知的关键人口模式。我们的研究结果揭示了一个矛盾：那些风险最高的人群往往最不接受AI支持，这强调了在AI驱动的欺诈预防中以用户为中心设计的重要性。这项工作推进了在打击不断演变的数字威胁方面，可解释、以人为本的AI系统的实践和理论基础。", "summary": "ASR是一个全面的生成式AI框架，旨在通过预测诈骗者响应、模拟真实诈骗对话以及提供可解释的用户支持来主动识别和打击消息诈骗。该框架利用了基于诈骗对话数据集微调的ScamGPT-J模型。实验证明ASR能有效提高诈骗检测率，尤其是在复杂诈骗类型中。研究还发现高风险用户对AI支持的接受度较低，强调了未来AI欺诈预防系统应更注重用户中心设计。", "keywords": "生成式AI, 消息诈骗, ASR框架, ScamGPT-J, 诈骗检测", "comments": "ASR框架的创新之处在于其“预见、模拟、推理”的综合方法，通过生成式AI主动模拟诈骗场景，为用户提供可解释的预防支持。研究发现高风险用户对AI支持的低接受度，这指出了人机交互和用户心理在AI反诈应用中的重要性，对未来以人为本的AI系统设计具有指导意义。"}}
{"id": "2507.17176", "title": "Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network", "authors": ["Li Pingzhen", "Xu Sheng", "Chen Jing", "Su Chengyue"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17176v1", "summary": "With the high density of printed circuit board (PCB) design and the high\nspeed of production, the traditional PCB defect detection model is difficult to\ntake into account the accuracy and computational cost, and cannot meet the\nrequirements of high accuracy and real-time detection of tiny defects.\nTherefore, in this paper, a multi-scale PCB defect detection method is improved\nwith YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,\nnetwork lightweighting and adaptive pruning, which is able to improve the\ndetection speed and accuracy by optimizing the backbone network, the neck\nnetwork and the detection head, the loss function and the adaptive pruning\nrate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the\nbackbone network, and multilevel features are used to extract image semantic\nfeatures to discover accurate defects. Secondly, we integrate C2f-Faster with\nsmall number of parameters in the neck section to enhance the ability of\nmulti-level feature fusion. Next, in the Head part, we design a new GCDetect\ndetection head, which allows the prediction of bounding boxes and categories to\nshare the weights of GroupConv, and uses a small number of grouping\nconvolutions to accomplish the regression and classification tasks, which\nsignificantly reduces the number of parameters while maintaining the accuracy\nof detection. We also design the Inner-MPDIoU boundary loss function to improve\nthe detection and localization of tiny targets. Finally, the model was pruned\nby an optimized adaptive pruning rate to further reduce the complexity of the\nmodel. Experimental results show that the model exhibits advantages in terms of\naccuracy and speed. On the publicly available PCB defect dataset, mAP0.5\nreaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared\nto YOLOv8n.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17176v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于剪枝和轻量化网络改进的YOLOv8多尺度PCB缺陷检测", "tldr": "本文提出了一种改进的YOLOv8模型，通过轻量化、自适应剪枝和针对微小目标的优化策略，显著提升了PCB缺陷检测的速度和精度，尤其在微小缺陷检测方面表现优异。", "motivation": "传统的PCB缺陷检测模型难以兼顾精度和计算成本，无法满足高精度和实时检测微小缺陷的需求，尤其是在高密度设计和高速生产背景下。", "method": "本文提出了一种改进的YOLOv8多尺度PCB缺陷检测方法。主要策略包括：1. 在骨干网络中使用参数更少的Ghost-HGNetv2结构，并利用多级特征提取图像语义特征。2. 在颈部集成参数量小的C2f-Faster以增强多级特征融合能力。3. 在头部设计新的GCDetect检测头，使边界框和类别预测共享GroupConv权重，并使用少量分组卷积完成回归和分类任务。4. 设计Inner-MPDIoU边界损失函数以改善微小目标的检测和定位。5. 通过优化的自适应剪枝率对模型进行剪枝，进一步降低模型复杂度。", "result": "实验结果表明，该模型在精度和速度方面均表现出优势。在公开的PCB缺陷数据集上，mAP0.5达到99.32%，mAP0.5:0.9达到75.18%，比YOLOv8n高出10.13%。", "conclusion": "该改进的YOLOv8模型通过综合的轻量化、剪枝和针对微小目标的优化策略，成功解决了传统PCB缺陷检测模型在精度和计算成本上的矛盾，并显著提升了微小缺陷的检测能力和整体性能。", "translation": "随着印刷电路板（PCB）设计的高密度和生产的高速度，传统的PCB缺陷检测模型难以兼顾精度和计算成本，无法满足对微小缺陷高精度和实时检测的要求。因此，本文提出了一种改进的YOLOv8多尺度PCB缺陷检测方法，该方法采用微小目标敏感策略、网络轻量化和自适应剪枝的综合策略，通过优化骨干网络、颈部网络、检测头、损失函数和自适应剪枝率，从而提高检测速度和精度。首先，骨干网络采用参数更少的Ghost-HGNetv2结构，并利用多级特征提取图像语义特征以发现准确的缺陷。其次，我们在颈部集成参数量小的C2f-Faster，以增强多级特征融合能力。接着，在头部，我们设计了一种新的GCDetect检测头，它允许边界框和类别预测共享GroupConv的权重，并使用少量分组卷积来完成回归和分类任务，这在保持检测精度的同时显著减少了参数数量。我们还设计了Inner-MPDIoU边界损失函数，以改善微小目标的检测和定位。最后，通过优化的自适应剪枝率对模型进行剪枝，进一步降低了模型的复杂度。实验结果表明，该模型在精度和速度方面均表现出优势。在公开的PCB缺陷数据集上，mAP0.5达到99.32%，mAP0.5:0.9达到75.18%，比YOLOv8n高出10.13%。", "summary": "本文针对传统PCB缺陷检测模型在精度、计算成本和微小缺陷实时检测方面的不足，提出了一种基于YOLOv8的改进型多尺度PCB缺陷检测方法。该方法通过整合Ghost-HGNetv2骨干网络、C2f-Faster颈部、新型GCDetect检测头、Inner-MPDIoU损失函数以及自适应剪枝策略，对YOLOv8进行了全面优化。实验证明，该模型在公开PCB缺陷数据集上实现了99.32%的mAP0.5和75.18%的mAP0.5:0.9，相比YOLOv8n有显著提升，有效提升了检测速度和精度，尤其在微小目标检测方面表现突出。", "keywords": "PCB缺陷检测, YOLOv8, 轻量化网络, 模型剪枝, 多尺度检测", "comments": "该论文的创新点在于其为YOLOv8引入了一套全面的优化策略，以适应PCB缺陷检测的特殊需求。通过结合轻量化网络（如Ghost-HGNetv2、C2f-Faster）、创新的检测头（GCDetect）、专门的损失函数（Inner-MPDIoU）以及自适应剪枝，该研究不仅提升了模型的检测精度，特别是对微小缺陷的识别能力，还显著降低了模型的计算复杂度和参数量，从而实现了高精度与实时性的兼顾，这对于工业界的应用具有重要意义。"}}
{"id": "2507.17610", "title": "Toward Federated DeePC: borrowing data from similar systems", "authors": ["Gert Vankan", "Valentina Breschi", "Simone Formentin"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17610v1", "summary": "Data-driven predictive control approaches, in general, and Data-enabled\nPredictive Control (DeePC), in particular, exploit matrices of raw input/output\ntrajectories for control design. These data are typically gathered only from\nthe system to be controlled. Nonetheless, the increasing connectivity and\ninherent similarity of (mass-produced) systems have the potential to generate a\nconsiderable amount of information that can be exploited to undertake a control\ntask. In light of this, we propose a preliminary federated extension of DeePC\nthat leverages a combination of input/output trajectories from multiple similar\nsystems for predictive control. Supported by a suite of numerical examples, our\nanalysis unveils the potential benefits of exploiting information from similar\nsystems and its possible downsides.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17610v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "迈向联邦DeePC：从相似系统借用数据", "tldr": "本文提出了一种联邦扩展的DeePC方法，通过结合来自多个相似系统的输入/输出轨迹进行预测控制，并探讨了利用此类信息的潜在益处和缺点。", "motivation": "传统的数据驱动预测控制方法，特别是DeePC，通常只利用来自被控系统的数据。然而，系统之间日益增长的连接性和固有的相似性，尤其是在大规模生产的系统中，能够产生大量可用于控制任务的信息。因此，有动机探索如何利用这些来自相似系统的数据来增强预测控制。", "method": "本文提出了一种DeePC的初步联邦扩展方法，该方法利用来自多个相似系统的输入/输出轨迹组合进行预测控制。这一方法旨在克服传统DeePC仅依赖单一系统数据的限制。", "result": "通过一系列数值例子，研究分析揭示了利用来自相似系统信息的潜在益处及其可能存在的缺点。", "conclusion": "研究表明，利用来自相似系统的输入/输出轨迹对预测控制具有潜在的好处，但同时也存在需要考虑的缺点。", "translation": "数据驱动的预测控制方法，特别是数据使能预测控制（DeePC），通常利用原始输入/输出轨迹矩阵进行控制设计。这些数据通常仅从被控系统收集。然而，系统（大规模生产的）日益增长的连接性和固有的相似性有可能产生大量信息，这些信息可以被利用来执行控制任务。鉴于此，我们提出了一种DeePC的初步联邦扩展，它利用来自多个相似系统的输入/输出轨迹组合进行预测控制。通过一系列数值例子，我们的分析揭示了利用来自相似系统信息的潜在益处及其可能存在的缺点。", "summary": "本文针对数据驱动预测控制（特别是DeePC）中数据通常仅来源于单一系统的问题，提出了一种联邦扩展的DeePC方法。该方法通过整合来自多个相似系统的输入/输出轨迹，旨在提升预测控制的性能。研究通过数值例子验证了利用相似系统数据在预测控制中的潜在优势和局限性。", "keywords": "联邦DeePC, 数据驱动预测控制, 相似系统, 输入/输出轨迹, 预测控制", "comments": "本文的创新点在于将联邦学习的思想引入到数据驱动预测控制（DeePC）中，提出了一种从相似系统“借用”数据的新范式。这对于大规模生产系统或具有相似特性的系统而言，具有重要的实践意义，可以有效解决单一系统数据不足的问题，提升控制器的性能和鲁棒性。然而，如何有效处理不同系统之间的数据差异性，以及隐私和通信效率等问题，可能是未来研究的挑战。"}}
{"id": "2507.07284", "title": "A Robust, Open-Source Framework for Spiking Neural Networks on Low-End FPGAs", "authors": ["Andrew Fan", "Simon D. Levy"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07284v2", "summary": "As the demand for compute power in traditional neural networks has increased\nsignificantly, spiking neural networks (SNNs) have emerged as a potential\nsolution to increasingly power-hungry neural networks. By operating on 0/1\nspikes emitted by neurons instead of arithmetic multiply-and-accumulate\noperations, SNNs propagate information temporally and spatially, allowing for\nmore efficient compute power. To this end, many architectures for accelerating\nand simulating SNNs have been developed, including Loihi, TrueNorth, and\nSpiNNaker. However, these chips are largely inaccessible to the wider\ncommunity. Field programmable gate arrays (FPGAs) have been explored to serve\nas a middle ground between neuromorphic and non-neuromorphic hardware, but many\nproposed architectures require expensive high-end FPGAs or target a single SNN\ntopology. This paper presents a framework consisting of a robust SNN\nacceleration architecture and a Pytorch-based SNN model compiler. Targeting\nany-to-any and/or fully connected SNNs, the FPGA architecture features a\nsynaptic array that tiles across the SNN to propagate spikes. The architecture\ntargets low-end FPGAs and requires very little (6358 LUT, 40.5 BRAM) resources.\nThe framework, tested on a low-end Xilinx Artix-7 FPGA at 100 MHz, achieves\ncompetitive speed in recognizing MNIST digits (0.52 ms/img). Further\nexperiments also show accurate simulation of hand coded any-to-any spiking\nneural networks on toy problems. All code and setup instructions are available\nat\nhttps://github.com/im-afan/snn-fpga}{\\texttt{https://github.com/im-afan/snn-fpga.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07284v2", "cate": "cs.NE", "date": "2025-07-09", "updated": "2025-07-23", "AI": {"title_translation": "一个用于低端FPGA上脉冲神经网络的鲁棒开源框架", "tldr": "本文提出了一个针对低端FPGA的鲁棒开源框架，包含SNN加速架构和PyTorch模型编译器，实现了高效的SNN推理，资源占用极低，并在MNIST上表现出竞争力。", "motivation": "传统神经网络计算需求剧增，脉冲神经网络(SNNs)作为潜在解决方案出现。现有SNN加速器（如Loihi）难以获取，而现有FPGA SNN架构多需要高端FPGA或针对单一拓扑结构，导致SNN加速的普及性受限。", "method": "本文提出了一个包含SNN加速架构和基于PyTorch的SNN模型编译器的框架。该FPGA架构具有一个突触阵列，可跨SNN进行平铺以传播尖峰，目标是任意到任意连接和/或全连接SNNs，并专门针对低端FPGA。", "result": "该框架在低端Xilinx Artix-7 FPGA上以100 MHz运行，资源占用极低（6358 LUT，40.5 BRAM）。在MNIST数字识别上，实现了0.52毫秒/图像的竞争力速度。进一步实验表明，在玩具问题上能准确模拟手写任意到任意脉冲神经网络。", "conclusion": "本文提出的鲁棒开源框架成功地在低端FPGA上实现了高效、低资源的脉冲神经网络加速，为SNN的广泛应用提供了可行的硬件解决方案。", "translation": "随着传统神经网络计算需求的显著增加，脉冲神经网络（SNNs）已成为解决日益耗电的神经网络的潜在方案。通过对神经元发出的0/1脉冲而不是算术乘加运算进行操作，SNNs在时间和空间上传播信息，从而实现更高效的计算能力。为此，已经开发了许多用于加速和模拟SNNs的架构，包括Loihi、TrueNorth和SpiNNaker。然而，这些芯片对于更广泛的社区来说在很大程度上是不可访问的。现场可编程门阵列（FPGAs）已被探索作为神经形态和非神经形态硬件之间的中间地带，但许多提出的架构需要昂贵的高端FPGAs或仅针对单一SNN拓扑结构。本文提出了一个框架，由一个鲁棒的SNN加速架构和一个基于Pytorch的SNN模型编译器组成。该FPGA架构针对任意到任意连接和/或全连接SNNs，其突触阵列在SNN上平铺以传播脉冲。该架构针对低端FPGA，并且只需要非常少的资源（6358 LUT，40.5 BRAM）。该框架在低端Xilinx Artix-7 FPGA上以100 MHz进行测试，在识别MNIST数字方面达到了具有竞争力的速度（0.52毫秒/图像）。进一步的实验还表明，在玩具问题上能准确模拟手写任意到任意脉冲神经网络。所有代码和设置说明均可在https://github.com/im-afan/snn-fpga获取。", "summary": "本文提出了一个针对低端FPGA的鲁棒开源框架，用于加速脉冲神经网络（SNNs）。该框架包含一个高效的SNN加速架构和一个基于PyTorch的模型编译器，旨在解决现有SNN加速器难以获取且FPGA解决方案多依赖高端硬件的问题。其FPGA架构资源占用极低，能在低端Xilinx Artix-7 FPGA上实现竞争力性能，例如在MNIST识别上达到0.52毫秒/图像，并支持任意到任意连接的SNNs。", "keywords": "脉冲神经网络, FPGA, 低端硬件, 加速框架, 开源", "comments": "该论文的创新点在于提出了一个针对低端FPGA的SNN加速框架，解决了现有SNN硬件方案成本高昂、难以普及的问题。其开源性质和极低的资源占用使得SNN在边缘设备上的部署成为可能，具有重要的实际应用价值。该框架对任意到任意连接SNNs的支持也增加了其通用性。"}}
{"id": "2504.18046", "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification", "authors": ["Guohao Huo", "Zibo Lin", "Zitong Wang", "Ruiting Dai", "Hao Tang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.18046v2", "summary": "Ophthalmic diseases pose a significant global health challenge, yet\ntraditional diagnosis methods and existing single-eye deep learning approaches\noften fail to account for binocular pathological correlations. To address this,\nwe propose DMS-Net, a dual-modal multi-scale Siamese network for binocular\nfundus image classification. Our framework leverages weight-shared Siamese\nResNet-152 backbones to extract deep semantic features from paired fundus\nimages. To tackle challenges such as lesion boundary ambiguity and scattered\npathological distributions, we introduce a Multi-Scale Context-Aware Module\n(MSCAM) that integrates adaptive pooling and attention mechanisms for\nmulti-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion\n(DMFF) module enhances cross-modal interaction through spatial-semantic\nrecalibration and bidirectional attention, effectively combining global context\nand local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves\nstate-of-the-art performance with 82.9% accuracy, 84.5% recall, and 83.2%\nCohen's kappa, demonstrating superior capability in detecting symmetric\npathologies and advancing clinical decision-making for ocular diseases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.18046v2", "cate": "cs.CV", "date": "2025-04-25", "updated": "2025-07-23", "AI": {"title_translation": "DMS-Net：用于双眼眼底图像分类的双模态多尺度孪生网络", "tldr": "本文提出了DMS-Net，一个双模态多尺度孪生网络，用于双眼眼底图像分类，通过结合多尺度上下文感知和双模态特征融合，在ODIR-5K数据集上实现了SOTA性能，有效解决了双眼病理相关性问题。", "motivation": "传统的眼科疾病诊断方法和现有的单眼深度学习方法未能考虑双眼病理相关性，且面临病灶边界模糊和病理分布分散的挑战。", "method": "本文提出了DMS-Net，一个双模态多尺度孪生网络。该框架利用权重共享的孪生ResNet-152骨干网络从成对的眼底图像中提取深层语义特征。为解决病灶边界模糊和病理分布分散问题，引入了多尺度上下文感知模块（MSCAM），整合自适应池化和注意力机制进行多分辨率特征聚合。此外，还引入了双模态特征融合（DMFF）模块，通过空间语义重校准和双向注意力增强跨模态交互，有效结合了全局上下文和局部边缘特征。", "result": "在ODIR-5K数据集上，DMS-Net实现了82.9%的准确率、84.5%的召回率和83.2%的Cohen's kappa，达到了最先进的性能。", "conclusion": "DMS-Net在检测对称病理方面表现出卓越的能力，并能有效推进眼部疾病的临床决策。", "translation": "眼科疾病构成全球性的重大健康挑战，然而，传统的诊断方法和现有的单眼深度学习方法往往未能考虑到双眼病理相关性。为了解决这个问题，我们提出了DMS-Net，一个用于双眼眼底图像分类的双模态多尺度孪生网络。我们的框架利用权重共享的孪生ResNet-152骨干网络从成对的眼底图像中提取深层语义特征。为了解决病灶边界模糊和病理分布分散等挑战，我们引入了一个多尺度上下文感知模块（MSCAM），它集成了自适应池化和注意力机制，用于多分辨率特征聚合。此外，一个双模态特征融合（DMFF）模块通过空间语义重校准和双向注意力增强了跨模态交互，有效地结合了全局上下文和局部边缘特征。在ODIR-5K数据集上进行评估，DMS-Net实现了82.9%的准确率、84.5%的召回率和83.2%的Cohen's kappa，展示了在检测对称病理方面的卓越能力，并推进了眼部疾病的临床决策。", "summary": "本文提出了DMS-Net，一个用于双眼眼底图像分类的双模态多尺度孪生网络，旨在解决现有方法未能考虑双眼病理相关性的问题。该网络利用孪生ResNet-152提取深层语义特征，并通过多尺度上下文感知模块（MSCAM）处理病灶模糊和双模态特征融合（DMFF）模块增强跨模态交互。在ODIR-5K数据集上的评估表明，DMS-Net取得了最先进的性能，证明了其在对称病理检测和支持眼科疾病临床决策方面的有效性。", "keywords": "双眼眼底图像分类, 孪生网络, 多尺度, 双模态, 深度学习", "comments": "这项研究通过引入双模态和多尺度处理，有效地解决了现有单眼诊断方法忽略双眼关联性的问题，并提升了对复杂病灶的识别能力。其创新点在于结合了孪生网络、多尺度特征聚合和双模态特征融合，为眼科疾病的自动化诊断提供了新的最先进解决方案。"}}
{"id": "2507.16878", "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos", "authors": ["Xuchen Li", "Xuzhao Li", "Shiyu Hu", "Kaiqi Huang", "Wentao Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint, Under review", "url": "http://arxiv.org/abs/2507.16878v1", "summary": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.", "comment": "Preprint, Under review", "pdf_url": "http://arxiv.org/pdf/2507.16878v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "CausalStep：一个用于视频中显式逐步因果推理的基准", "tldr": "CausalStep是一个新的视频基准，旨在评估模型在视频中进行显式逐步因果推理的能力，并发现当前模型与人类水平之间存在显著差距。", "motivation": "尽管大型语言模型（LLMs）在文本和图像推理方面取得了进展，但实现稳健的视频推理仍然是一个重大挑战。现有视频基准主要评估浅层理解和推理，并允许模型利用全局上下文，未能严格评估真正的因果和逐步推理能力。", "method": "本文提出了CausalStep，一个专为视频中显式逐步因果推理设计的基准。CausalStep将视频分割成因果关联的单元，并强制执行严格的逐步问答协议，要求顺序回答并防止捷径解决方案。每个问题都包含基于错误类型分类精心构建的干扰项，以确保诊断价值。该基准包含100个视频（六个类别）和1,852个多项选择问答对。此外，本文引入了七种诊断指标用于全面评估，从而能够精确诊断因果推理能力。", "result": "对领先的专有模型、开源模型以及人类基线进行的实验表明，当前模型在逐步推理方面与人类水平之间存在显著差距。", "conclusion": "CausalStep提供了一个严格的基准，以推动鲁棒和可解释的视频推理领域的进展。", "translation": "大型语言模型（LLMs）的最新进展改善了文本和图像领域的推理能力，然而，实现稳健的视频推理仍然是一个重大挑战。现有视频基准主要评估浅层理解和推理，并允许模型利用全局上下文，未能严格评估真正的因果和逐步推理。我们提出了CausalStep，一个专为视频中显式逐步因果推理设计的基准。CausalStep将视频分割成因果关联的单元，并强制执行严格的逐步问答协议，要求顺序回答并防止捷径解决方案。每个问题都包含根据错误类型分类精心构建的干扰项，以确保诊断价值。该基准包含六个类别的100个视频和1,852个多项选择问答对。我们引入了七种诊断指标用于全面评估，从而能够精确诊断因果推理能力。对领先的专有和开源模型以及人类基线进行的实验表明，当前模型与人类水平的逐步推理之间存在显著差距。CausalStep提供了一个严格的基准，以推动鲁棒和可解释的视频推理领域的进展。", "summary": "本文介绍了CausalStep，一个用于评估视频中显式逐步因果推理能力的新型基准。该基准通过将视频分解为因果关联单元并实施严格的逐步问答协议来克服现有视频推理基准的局限性。CausalStep包含100个视频和1,852个多项选择问答对，并引入了七种诊断指标。实验结果显示，当前模型在逐步因果推理方面与人类水平存在显著差距，表明CausalStep是推动视频推理研究的重要工具。", "keywords": "视频推理, 因果推理, 基准, 逐步推理, CausalStep", "comments": "CausalStep通过其独特的逐步问答协议和对因果链的强调，创新性地解决了现有视频推理基准未能严格评估真正因果和逐步推理的问题。其引入的诊断指标和精心构建的干扰项增强了基准的诊断价值。这项工作的重要性在于它揭示了当前模型在复杂视频因果推理方面的显著局限性，并为未来研究提供了一个严格的评估工具，有望推动可解释和鲁棒视频推理领域的发展。"}}
{"id": "2501.12971", "title": "Universal Decoding over Finite-State Additive Channels via Noise Guessing", "authors": ["Henrique K. Miyamoto", "Sheng Yang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      32 pages, 2 figures", "url": "http://arxiv.org/abs/2501.12971v2", "summary": "We study universal decoding over unknown discrete additive channels\ndetermined by a finite-state (unifilar) random process. Aiming at\nlow-complexity decoders, we study variants of noise-guessing decoders that use\nestimators for the probability of a noise sequence when the actual channel law\nis unknown. A deterministic version produces noise sequences in a fixed order,\nand a new randomised version draws them at random, until finding a sequence\nthat, subtracted from the received sequence, results in a valid codeword. We\nshow that both strategies are random-coding universal (i.e. have the same\nrandom-coding error exponent as the optimal maximum likelihood decoding), and\nderive upper bounds for their complexity. Numerical examples in additive Markov\nchannels illustrate the proposed methods' performance, showing that they\nconsistently outperform a more usual training-based strategy.", "comment": "32 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2501.12971v2", "cate": "cs.IT", "date": "2025-01-22", "updated": "2025-07-23", "AI": {"title_translation": "经由噪声猜测的有限状态加性信道上的通用解码", "tldr": "研究了在未知有限状态加性信道上通过噪声猜测实现低复杂度通用解码的方法，包括确定性和随机两种变体，证明它们具有与最大似然解码相同的随机编码错误指数，并优于基于训练的方法。", "motivation": "在未知离散加性信道上实现低复杂度解码。", "method": "研究了噪声猜测解码器的变体，这些解码器在信道律未知时使用噪声序列概率估计器。提出了两种版本：一种确定性地按固定顺序生成噪声序列，另一种随机地抽取噪声序列，直到找到一个序列，从接收序列中减去后得到一个有效的码字。", "result": "两种策略都是随机编码通用的（即具有与最优最大似然解码相同的随机编码错误指数），并推导了它们的复杂度上限。在加性马尔可夫信道中的数值例子表明，所提出的方法表现优异，始终优于更常见的基于训练的策略。", "conclusion": "通过噪声猜测实现的通用解码器，无论是确定性还是随机版本，在未知有限状态加性信道上都表现出与最优最大似然解码相同的性能（随机编码错误指数），并且在复杂度上具有可控上限，同时在实际应用中优于传统的基于训练的方法。", "translation": "我们研究了由有限状态（单一）随机过程确定的未知离散加性信道上的通用解码。为了实现低复杂度的解码器，我们研究了噪声猜测解码器的变体，这些解码器在实际信道律未知时使用噪声序列概率估计器。一个确定性版本按固定顺序生成噪声序列，一个新的随机版本随机抽取噪声序列，直到找到一个序列，从接收序列中减去后得到一个有效的码字。我们证明了这两种策略都是随机编码通用的（即具有与最优最大似然解码相同的随机编码错误指数），并推导了它们的复杂度上限。在加性马尔可夫信道中的数值例子说明了所提出方法的性能，表明它们始终优于更常见的基于训练的策略。", "summary": "这篇论文研究了在未知有限状态加性信道上进行通用解码的问题，旨在开发低复杂度的解码器。作者提出了两种噪声猜测解码器的变体：一种是确定性的，另一种是随机的。研究表明，这两种方法都达到了随机编码通用性，即与最优最大似然解码具有相同的错误指数，并且论文推导了它们的复杂度上限。数值模拟结果显示，这些新方法在性能上持续优于传统的基于训练的解码策略。", "keywords": "通用解码, 噪声猜测, 有限状态信道, 加性信道, 随机编码错误指数", "comments": "这项工作在未知信道条件下提供了一种创新的低复杂度通用解码方案。通过引入噪声猜测的概念，并证明其与最大似然解码在错误指数上的等效性，同时提供了复杂度分析，显示了其理论和实践价值。它超越了传统的需要信道训练的方法，为实际通信系统设计提供了新的视角。"}}
{"id": "2507.17441", "title": "Detecting Multiple Targets with Distributed Sensing and Communication in Cell-Free Massive MIMO", "authors": ["Zinat Behdad", "Ozlem Tugfe Demir", "Ki Won Sung", "Cicek Cavdar"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.17441v1", "summary": "This paper investigates multi-target detection in an integrated sensing and\ncommunication (ISAC) system within a cell-free massive MIMO (CF-mMIMO)\nframework. We adopt a user-centric approach for communication user equipments\n(UEs) and a distributed sensing approach for multi-target detection. A\nheuristic access point (AP) mode selection algorithm and a channel-aware\ndistributed sensing scheme are proposed, where local measurements at receive\nAPs (RX-APs) are weighted based on the received signals signal-to-interference\nratio (SIR). A maximum a posteriori ratio test (MAPRT) detector is applied\nunder two awareness levels at RX-APs. To balance the communication-sensing\ntrade-off, we develop a power allocation algorithm to jointly maximize the\nminimum detection probability and communication\nsignal-to-interference-plus-noise ratio (SINR) while satisfying power\nconstraints. The proposed scheme outperforms non-weighted methods. Adding test\nstatistics from more RX-APs can degrade sensing performance due to weaker\nchannels, but this effect can be mitigated by optimizing the weighting\nexponent. Additionally, assigning more sensing RX-APs to a sensing area results\nin approximately 10 dB loss in minimum communication SINR due to limited\ncommunication resources.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.17441v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "蜂窝自由大规模MIMO中分布式感知与通信的多目标检测", "tldr": "本文在蜂窝自由大规模MIMO框架下的ISAC系统中研究多目标检测，提出启发式AP模式选择算法、信道感知分布式感知方案和功率分配算法，以优化感知与通信性能。", "motivation": "本文研究在蜂窝自由大规模MIMO (CF-mMIMO) 框架下的集成感知与通信 (ISAC) 系统中的多目标检测问题，旨在平衡通信与感知的权衡。", "method": "本文采用用户中心方法进行通信，分布式感知方法进行多目标检测。提出了启发式接入点 (AP) 模式选择算法和信道感知分布式感知方案，其中接收AP (RX-AP) 的局部测量基于接收信号的SIR进行加权。在两种感知水平下应用了最大后验比率测试 (MAPRT) 检测器。为了平衡通信-感知权衡，开发了一种功率分配算法，以联合最大化最小检测概率和通信信噪比 (SINR)，同时满足功率约束。", "result": "所提出的方案优于非加权方法。增加来自更多RX-AP的测试统计量可能会由于较弱的信道而降低感知性能，但可以通过优化加权指数来缓解。此外，为感知区域分配更多的感知RX-AP会导致最小通信SINR损失约10 dB，因为通信资源有限。", "conclusion": "本文提出的分布式感知与通信方案在蜂窝自由大规模MIMO框架下有效提升了多目标检测性能，并通过功率分配算法实现了通信与感知的平衡。研究发现，优化加权指数可以缓解弱信道对感知性能的影响，但增加感知AP数量会对通信SINR造成损失。", "translation": "本文研究蜂窝自由大规模MIMO（CF-mMIMO）框架下集成感知与通信（ISAC）系统中的多目标检测。我们对通信用户设备（UE）采用以用户为中心的方法，对多目标检测采用分布式感知方法。提出了一种启发式接入点（AP）模式选择算法和一种信道感知分布式感知方案，其中接收AP（RX-AP）处的局部测量根据接收信号的信干比（SIR）进行加权。在RX-AP的两种感知水平下应用了最大后验比率测试（MAPRT）检测器。为了平衡通信-感知权衡，我们开发了一种功率分配算法，以联合最大化最小检测概率和通信信干噪比（SINR），同时满足功率约束。所提出的方案优于非加权方法。添加来自更多RX-AP的测试统计量可能会由于较弱的信道而降低感知性能，但可以通过优化加权指数来缓解。此外，为感知区域分配更多的感知RX-AP会导致最小通信SINR损失约10 dB，因为通信资源有限。", "summary": "本文在蜂窝自由大规模MIMO (CF-mMIMO) 框架下的集成感知与通信 (ISAC) 系统中研究多目标检测。研究提出了一种用户中心的通信和分布式感知方法，并引入了启发式AP模式选择算法和信道感知分布式感知方案，其中局部测量根据SIR进行加权。文章应用了最大后验比率测试 (MAPRT) 检测器，并开发了一种功率分配算法来平衡通信与感知的权衡，旨在联合最大化最小检测概率和通信SINR。实验结果表明，该方案优于非加权方法，并且通过优化加权指数可以缓解弱信道对感知性能的负面影响，但增加感知AP数量会牺牲部分通信SINR。", "keywords": "多目标检测, 分布式感知, 蜂窝自由大规模MIMO, 集成感知与通信, 功率分配", "comments": "这篇论文在ISAC和CF-mMIMO的交叉领域进行了深入研究，提出了创新的分布式感知和通信集成方案。其亮点在于引入了信道感知的加权机制和联合优化通信与感知性能的功率分配算法，这对于实际系统部署具有重要意义。论文还探讨了增加感知AP数量对性能的影响及缓解策略，提供了实用的工程指导。"}}
{"id": "2505.07773", "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving", "authors": ["Xinji Mai", "Haotian Xu", "Xing W", "Weinong Wang", "Jian Hu", "Yingying Zhang", "Wenqiang Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.07773v3", "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.07773v3", "cate": "cs.AI", "date": "2025-05-12", "updated": "2025-07-23", "AI": {"title_translation": "智能体强化学习缩放定律：用于数学问题解决的自发代码执行智能体强化学习", "tldr": "本文研究了如何通过基于结果的强化学习，训练大型语言模型（LLMs）自发生成和执行Python代码来解决数学问题，发现训练步骤与代码执行频率、响应长度和任务准确性之间存在可预测的缩放关系，并证明了该方法在数学基准测试上的优越性。", "motivation": "大型语言模型（LLMs）在需要精确、可验证计算的数学推理任务中表现不佳。尽管基于结果奖励的强化学习（RL）可以增强基于文本的推理能力，但理解智能体如何自主学习利用代码执行等外部工具仍然至关重要。", "method": "研究人员调查了基于结果奖励的强化学习（RL）用于工具集成推理（ZeroTIR），训练基础LLMs在没有监督工具使用示例的情况下，自发生成和执行Python代码来解决数学问题。他们实现了一个具有解耦代码执行环境的鲁棒框架，并在标准RL算法和框架上验证了他们的发现。", "result": "研究发现，随着RL训练的进行，关键指标呈现可预测的缩放关系。具体而言，训练步骤的增加与自发代码执行频率、平均响应长度以及最终任务准确性的增加呈强正相关。实验表明，ZeroTIR在具有挑战性的数学基准测试上显著超越了非工具ZeroRL基线。", "conclusion": "本研究为智能体强化学习中自主工具使用的获取和扩展提供了基础性理解，并为未来的研究提供了一个可复现的基准。", "translation": "大型语言模型（LLMs）在需要精确、可验证计算的数学推理任务中常常面临困难。虽然基于结果奖励的强化学习（RL）能够增强基于文本的推理能力，但理解智能体如何自主学习利用代码执行等外部工具仍然至关重要。我们研究了用于工具集成推理（ZeroTIR）的基于结果奖励的强化学习，训练基础LLMs在没有监督工具使用示例的情况下，自发生成和执行Python代码来解决数学问题。我们的核心贡献在于，我们证明了随着RL训练的进行，关键指标呈现可预测的缩放关系。具体而言，我们观察到强烈的正相关性，即训练步骤的增加会导致自发代码执行频率、平均响应长度以及关键的最终任务准确性的增加。这表明计算投入与有效、工具增强推理策略的出现之间存在可量化的关系。我们实现了一个具有解耦代码执行环境的鲁棒框架，并在标准RL算法和框架上验证了我们的发现。实验表明，ZeroTIR在具有挑战性的数学基准测试上显著超越了非工具ZeroRL基线。我们的发现为智能体强化学习中自主工具使用的获取和扩展提供了基础性理解，并为未来的研究提供了一个可复现的基准。代码已在\\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf_async_pipline}发布。", "summary": "该论文提出了ZeroTIR，一种通过基于结果奖励的强化学习方法，使大型语言模型（LLMs）能够自发生成和执行Python代码以解决数学问题，而无需预先的工具使用示例。研究发现，随着训练的深入，LLMs自发代码执行的频率、响应长度和任务准确性均呈现可预测的增长，揭示了计算投入与工具增强推理能力之间的量化关系。实验证明ZeroTIR在数学基准测试上显著优于非工具基线，为理解智能体强化学习中自主工具使用的习得和扩展提供了基础。", "keywords": "智能体强化学习, 代码执行, 数学问题解决, 缩放定律, 工具集成推理", "comments": "本文的创新点在于提出了ZeroTIR方法，使LLMs能够自发地进行代码执行以解决复杂的数学问题，而无需显式的工具使用监督。更重要的是，它发现了RL训练步骤与关键性能指标（如代码执行频率和任务准确性）之间的可预测缩放定律，为理解LLMs如何习得并扩展工具使用能力提供了量化视角。这项工作为未来在智能体强化学习中集成外部工具奠定了基础，具有重要的研究意义。"}}
{"id": "2506.07584", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07584v3", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07584v3", "cate": "cs.LG", "date": "2025-06-09", "updated": "2025-07-23", "AI": {"title_translation": "MIRA：用于真实世界健康数据的医疗时间序列基础模型", "tldr": "MIRA是一个专门为医疗时间序列预测设计的基础模型，通过解决现有模型在处理不规则间隔、异构采样率和缺失值等医疗数据挑战方面的不足，实现了预测错误率的显著降低，并引入了一个新的基准测试。", "motivation": "现有通用时间序列基础模型难以处理医疗时间序列数据固有的挑战，如不规则间隔、异构采样率和频繁缺失值，这限制了其在医疗领域应用潜力，尤其是在数据稀缺或隐私受限的环境中。", "method": "MIRA引入了连续时间旋转位置编码（Continuous-Time Rotary Positional Encoding）来建模可变时间间隔，频率特定专家混合层（frequency-specific mixture-of-experts layer）以促进时间专业化，以及基于神经ODE的连续动态外推块（Continuous Dynamics Extrapolation Block）来建模潜在状态的连续轨迹，从而实现任意目标时间戳的准确预测。", "result": "MIRA在预测误差方面实现了显著降低，与零样本和微调基线相比，在分布外场景中平均降低了10%，在分布内场景中平均降低了7%。", "conclusion": "MIRA作为一个统一的医疗时间序列基础模型，通过其创新的架构有效解决了医疗时间序列数据的复杂挑战，并在大规模数据集上取得了优异的预测性能。此外，该研究还引入了一个全面的基准，为未来医疗时间序列建模研究奠定了基础。", "translation": "一个用于医疗时间序列的统一基础模型——预训练于开放获取和伦理委员会批准的医疗语料库——有望减少标注负担，最小化模型定制，并实现在临床机构、模态和任务间的鲁棒迁移，特别是在数据稀缺或隐私受限的环境中。然而，现有通用时间序列基础模型由于其固有的挑战，包括不规则间隔、异构采样率和频繁缺失值，难以处理医疗时间序列数据。为了解决这些挑战，我们引入了MIRA，一个专门为医疗时间序列预测设计的统一基础模型。MIRA融合了连续时间旋转位置编码，能够对可变时间间隔进行精细建模；一个频率特定专家混合层，用于在潜在频率区域路由计算，进一步促进时间专业化；以及一个基于神经ODE的连续动态外推块，用于建模潜在状态的连续轨迹，从而能够在任意目标时间戳进行准确预测。MIRA预训练于一个大规模且多样化的医疗语料库，该语料库包含从公开数据集中收集的超过4540亿个时间点。与零样本和微调基线相比，MIRA在分布外和分布内场景中的预测错误率分别平均降低了10%和7%。我们还引入了一个涵盖多个下游临床任务的全面基准，为未来医疗时间序列建模研究奠定了基础。", "summary": "MIRA是一个针对医疗时间序列预测的统一基础模型，旨在克服现有通用模型在处理医疗数据特有挑战（如不规则间隔和缺失值）方面的不足。该模型集成了连续时间旋转位置编码、频率特定专家混合层和基于神经ODE的连续动态外推块，以实现对复杂时间序列的精确建模。MIRA在大规模医疗数据集上预训练后，在预测错误率上实现了显著降低，并在分布外和分布内场景中分别优于基线模型10%和7%。本研究还建立了一个全面的基准，以推动医疗时间序列建模的未来研究。", "keywords": "医疗时间序列, 基础模型, MIRA, 预测, 神经ODE", "comments": "MIRA的创新性在于其专门为医疗时间序列数据设计的独特架构，特别是针对不规则间隔、异构采样率和缺失值等挑战提出的解决方案。通过结合连续时间旋转位置编码、频率特定专家混合层和神经ODE，MIRA能够更精细、更鲁棒地处理医疗数据。其在真实世界数据集上的显著性能提升，以及引入的全面基准，都显示了其在该领域的重要性和潜力，有望降低医疗AI应用的门槛并提高准确性。"}}
{"id": "2507.17271", "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "authors": ["Shuaiyu Zhou", "Zhengran Zeng", "Xiaoling Zhou", "Rui Xie", "Shikun Zhang", "Wei Ye"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17271v1", "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17271v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Seed&Steer: 利用可编译前缀和分支信号引导大型语言模型生成单元测试", "tldr": "Seed&Steer 是一种新的两步法，结合传统工具和LLM，通过生成可编译前缀和分支信号来提高LLM生成单元测试的编译成功率和代码覆盖率。", "motivation": "单元测试在软件开发中至关重要。尽管基于大型语言模型（LLM）的方法显著改进了自动化测试生成，但仍存在挑战。本研究旨在通过解耦前缀生成和断言生成来解决这些挑战，发现前者影响编译成功率，后者影响测试覆盖率。", "method": "提出 Seed&Steer，一个两步法。第一步，利用传统单元测试工具（如 EvoSuite）生成高编译成功率的方法调用作为“种子”，以引导LLM构建有效的测试上下文。第二步，引入分支提示帮助LLM探索不同的执行路径（如正常、边界和异常情况），并生成高覆盖率的断言。", "result": "Seed&Steer 将编译通过率提高了约7%，在两个LLM上成功编译了792和887个之前失败的案例。它还在不同复杂度的焦点方法上实现了高达约73%的分支和行覆盖率，覆盖率提升范围为1.09倍至1.26倍。", "conclusion": "Seed&Steer 通过结合传统测试技术和LLM的能力，显著提高了LLM生成单元测试的编译成功率和代码覆盖率，证明了其在实际项目中的有效性。", "translation": "单元测试在软件开发生命周期中起着至关重要的作用。大型语言模型（LLM）方法在自动化测试生成方面的最新进展显著提升了该领域，并获得了学术界和工业界的关注。我们从一个新的视角重新审视了基于LLM的单元测试生成，通过解耦前缀生成和断言生成。为了描述它们各自的挑战，我们定义了初始化复杂性并采用圈复杂度来衡量前缀和断言生成的难度，揭示前者主要影响编译成功率，而后者影响测试覆盖率。为了应对这些挑战，我们提出了 Seed&Steer，这是一种两步法，结合了传统单元测试技术和大型语言模型的能力。Seed&Steer 利用传统单元测试工具（例如 EvoSuite）生成具有高编译成功率的方法调用，这些调用作为种子来引导LLM构建有效的测试上下文。然后，它引入分支提示来帮助LLM探索不同的执行路径（例如正常、边界和异常情况）并生成高覆盖率的断言。我们在五个真实的 Java 项目上，针对最先进的基线评估了 Seed&Steer。结果表明，Seed&Steer 将编译通过率提高了约7%，在两个LLM上成功编译了792和887个之前失败的案例。它还在不同复杂度的焦点方法上实现了高达约73%的分支和行覆盖率，覆盖率提升范围为1.09倍至1.26倍。我们的代码、数据集和实验脚本将公开发布，以支持未来的研究和可重复性。", "summary": "本文提出了一种名为 Seed&Steer 的新型两步法，旨在改进基于大型语言模型（LLM）的单元测试生成。该方法通过解耦测试生成过程为前缀生成和断言生成，并利用传统测试工具（如 EvoSuite）提供可编译的“种子”前缀，以引导LLM创建有效的测试上下文。随后，引入分支信号帮助LLM探索多样化的执行路径并生成高覆盖率的断言。实验结果表明，Seed&Steer 显著提高了LLM生成测试的编译成功率和代码覆盖率，验证了其在真实Java项目中的有效性。", "keywords": "单元测试生成, 大型语言模型, 代码覆盖率, 编译成功率, Seed&Steer", "comments": "Seed&Steer 的创新之处在于其两步法，巧妙地结合了传统测试工具的确定性（保证编译成功率）和LLM的生成能力（提高覆盖率）。通过区分初始化复杂性和圈复杂度对编译和覆盖率的影响，该研究提供了深入的洞察。其混合方法为未来LLM在软件工程领域的应用提供了有价值的参考，尤其是在需要高可靠性和准确性的场景中。"}}
{"id": "2507.17448", "title": "Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning", "authors": ["Situo Zhang", "Hanqi Li", "Lu Chen", "Zihan Zhao", "Xuanze Lin", "Zichen Zhu", "Bo Chen", "Xin Chen", "Kai Yu"], "categories": ["cs.CE", "cs.AI", "physics.chem-ph"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.17448v1", "summary": "Retrosynthesis planning, essential in organic synthesis and drug discovery,\nhas greatly benefited from recent AI-driven advancements. Nevertheless,\nexisting methods frequently face limitations in both applicability and\nexplainability. Traditional graph-based and sequence-to-sequence models often\nlack generalized chemical knowledge, leading to predictions that are neither\nconsistently accurate nor easily explainable. To address these challenges, we\nintroduce RetroDFM-R, a reasoning-based large language model (LLM) designed\nspecifically for chemical retrosynthesis. Leveraging large-scale reinforcement\nlearning guided by chemically verifiable rewards, RetroDFM-R significantly\nenhances prediction accuracy and explainability. Comprehensive evaluations\ndemonstrate that RetroDFM-R significantly outperforms state-of-the-art methods,\nachieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind\nhuman assessments further validate the chemical plausibility and practical\nutility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts\nmultistep retrosynthetic routes reported in the literature for both real-world\ndrug molecules and perovskite materials. Crucially, the model's explicit\nreasoning process provides human-interpretable insights, thereby enhancing\ntrust and practical value in real-world retrosynthesis applications.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.17448v1", "cate": "cs.CE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过强化学习实现大语言模型驱动的逆合成推理预测", "tldr": "RetroDFM-R是一个基于强化学习的大语言模型，用于逆合成预测，显著提高了预测准确性和可解释性，并超越了现有SOTA方法。", "motivation": "现有逆合成预测方法在适用性和可解释性方面存在局限，传统的图基和序列到序列模型缺乏通用化学知识，导致预测结果不准确且难以解释。", "method": "本文提出了RetroDFM-R，一个专门为化学逆合成设计的基于推理的大语言模型（LLM）。该模型利用大规模强化学习，并通过化学可验证的奖励进行引导，以提高预测准确性和可解释性。", "result": "RetroDFM-R在USPTO-50K基准测试中实现了65.0%的Top-1准确率，显著优于现有SOTA方法。双盲人类评估验证了其预测的化学合理性和实用性。该模型还能准确预测文献中报道的真实药物分子和钙钛矿材料的多步逆合成路线。", "conclusion": "RetroDFM-R通过其显式推理过程提供了人类可解释的见解，增强了对模型预测的信任和在实际逆合成应用中的实用价值。", "translation": "逆合成规划在有机合成和药物发现中至关重要，并已从近期AI驱动的进展中受益。然而，现有方法在适用性和可解释性方面经常面临局限。传统的基于图和序列到序列的模型通常缺乏通用化学知识，导致预测既不持续准确也不易解释。为了解决这些挑战，我们引入了RetroDFM-R，一个专门为化学逆合成设计的基于推理的大语言模型（LLM）。RetroDFM-R利用大规模强化学习，并通过化学可验证的奖励进行引导，显著提高了预测准确性和可解释性。综合评估表明，RetroDFM-R显著优于现有最先进的方法，在USPTO-50K基准测试中达到了65.0%的Top-1准确率。双盲人类评估进一步验证了RetroDFM-R预测的化学合理性和实用价值。RetroDFM-R还能准确预测文献中报道的真实药物分子和钙钛矿材料的多步逆合成路线。至关重要的是，该模型的显式推理过程提供了人类可解释的见解，从而增强了在实际逆合成应用中的信任和实用价值。", "summary": "本文提出RetroDFM-R，一个基于强化学习的大语言模型，用于解决化学逆合成预测中现有方法在适用性和可解释性方面的局限。RetroDFM-R通过结合大规模强化学习和化学奖励，显著提升了预测准确性（在USPTO-50K上达到65.0%的Top-1准确率）和可解释性。该模型不仅在性能上超越了现有SOTA方法，其显式推理过程也提供了可解释的见解，增强了在实际应用中的信任和实用价值。", "keywords": "逆合成预测, 大语言模型, 强化学习, 可解释性, 化学合成", "comments": "该论文的创新点在于将大语言模型与强化学习结合应用于逆合成预测，并通过引入化学可验证的奖励来提升模型的准确性和可解释性。其显式推理过程是重要突破，解决了现有AI模型在化学领域“黑箱”问题，为实际应用增加了信任和实用性。"}}
{"id": "2507.17388", "title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation", "authors": ["Xinyu Liu", "Hengyu Liu", "Cheng Wang", "Tianming Liu", "Yixuan Yuan"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025", "url": "http://arxiv.org/abs/2507.17388v1", "summary": "Endoscopic video generation is crucial for advancing medical imaging and\nenhancing diagnostic capabilities. However, prior efforts in this field have\neither focused on static images, lacking the dynamic context required for\npractical applications, or have relied on unconditional generation that fails\nto provide meaningful references for clinicians. Therefore, in this paper, we\npropose the first conditional endoscopic video generation framework, namely\nEndoGen. Specifically, we build an autoregressive model with a tailored\nSpatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the\nlearning of generating multiple frames as a grid-based image generation\npattern, which effectively capitalizes the inherent global dependency modeling\ncapabilities of autoregressive architectures. Furthermore, we propose a\nSemantic-Aware Token Masking (SAT) mechanism, which enhances the model's\nability to produce rich and diverse content by selectively focusing on\nsemantically meaningful regions during the generation process. Through\nextensive experiments, we demonstrate the effectiveness of our framework in\ngenerating high-quality, conditionally guided endoscopic content, and improves\nthe performance of downstream task of polyp segmentation. Code released at\nhttps://www.github.com/CUHK-AIM-Group/EndoGen.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.17388v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "EndoGen：条件自回归内窥镜视频生成", "tldr": "提出EndoGen，首个条件自回归内窥镜视频生成框架，能生成高质量视频并提升息肉分割性能。", "motivation": "内窥镜视频生成对医学影像和诊断至关重要，但现有方法或仅关注静态图像，缺乏动态背景，或采用无条件生成，无法为临床医生提供有意义的参考。", "method": "本文提出EndoGen框架，一个条件自回归模型，旨在解决现有内窥镜视频生成方法的不足。该模型包含两个核心策略：1) 时空网格帧模式（Spatiotemporal Grid-Frame Patterning, SGP），将多帧生成重构为基于网格的图像生成模式，有效利用自回归架构的全局依赖建模能力。2) 语义感知令牌掩蔽（Semantic-Aware Token Masking, SAT）机制，通过选择性关注语义有意义区域，增强模型生成丰富多样内容的能力。", "result": "通过大量实验证明，EndoGen框架能有效生成高质量、条件引导的内窥镜内容，并提高了息肉分割下游任务的性能。", "conclusion": "EndoGen成功解决了现有内窥镜视频生成方法在动态上下文和条件参考方面的局限性，首次实现了条件自回归生成，并对下游任务（如息肉分割）有积极影响，显示了其在医疗影像领域的应用潜力。", "translation": "内窥镜视频生成对于推进医学影像和增强诊断能力至关重要。然而，该领域先前的努力要么专注于静态图像，缺乏实际应用所需的动态上下文，要么依赖于无条件生成，无法为临床医生提供有意义的参考。因此，在本文中，我们提出了第一个条件内窥镜视频生成框架，即EndoGen。具体而言，我们构建了一个带有定制时空网格帧模式（SGP）策略的自回归模型。它将多帧生成的学习重构为基于网格的图像生成模式，有效利用了自回归架构固有的全局依赖建模能力。此外，我们提出了一种语义感知令牌掩蔽（SAT）机制，通过在生成过程中选择性地关注语义有意义的区域，增强了模型生成丰富多样内容的能力。通过广泛的实验，我们证明了我们框架在生成高质量、条件引导的内窥镜内容方面的有效性，并提高了息肉分割下游任务的性能。代码已在https://www.github.com/CUHK-AIM-Group/EndoGen发布。", "summary": "本文提出EndoGen，首个条件自回归内窥镜视频生成框架，旨在解决现有方法在动态上下文和条件参考方面的不足。EndoGen采用自回归模型，结合时空网格帧模式（SGP）和语义感知令牌掩蔽（SAT）机制，以生成高质量、条件引导的内窥镜视频。实验证明，EndoGen不仅能生成丰富的内窥镜内容，还能提升息肉分割等下游任务的性能。", "keywords": "内窥镜视频生成, 条件自回归, 时空网格帧模式, 语义感知令牌掩蔽, 息肉分割", "comments": "EndoGen的创新在于首次提出条件自回归内窥镜视频生成框架，并通过SGP和SAT机制有效解决了现有方法在动态上下文和条件参考方面的局限性。其能够生成高质量、条件引导的视频，并对下游任务有积极影响，这对于医疗影像和诊断领域具有重要意义。抽象中未明确指出其在计算资源或训练时间方面的潜在挑战。"}}
{"id": "2507.17439", "title": "Doubly robust outlier resistant inference on causal treatment effect", "authors": ["Joonsung Kang"], "categories": ["stat.ME", "cs.LG"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17439v1", "summary": "Outliers can severely distort causal effect estimation in observational\nstudies, yet this issue has received limited attention in the literature. Their\ninfluence is especially pronounced in small sample sizes, where detecting and\nremoving outliers becomes increasingly difficult. Therefore, it is essential to\nestimate treatment effects robustly without excluding these influential data\npoints. To address this, we propose a doubly robust point estimator for the\naverage treatment effect under a contaminated model that includes outliers.\nRobustness in outcome regression is achieved through a robust estimating\nequation, while covariate balancing propensity scores (CBPS) ensure resilience\nin propensity score modeling.\n  To prevent model overfitting due to the inclusion of numerous parameters, we\nincorporate variable selection. All these components are unified under a\npenalized empirical likelihood framework. For confidence interval estimation,\nmost existing approaches rely on asymptotic properties, which may be unreliable\nin finite samples. We derive an optimal finite-sample confidence interval for\nthe average treatment effect using our proposed estimating equation, ensuring\nthat the interval bounds remain unaffected by outliers. Through simulations and\na real-world application involving hypertension data with outliers, we\ndemonstrate that our method consistently outperforms existing approaches in\nboth accuracy and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17439v1", "cate": "stat.ME", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "因果治疗效应的双重鲁棒抗异常值推断", "tldr": "该研究提出了一种双重鲁棒的估计方法，用于在观测研究中存在异常值的情况下准确估计因果治疗效应，并在有限样本中提供稳健的置信区间。", "motivation": "在观测研究中，异常值会严重扭曲因果效应估计，尤其是在小样本量中，此时检测和移除异常值变得非常困难。现有文献对此问题的关注有限，因此需要一种无需排除这些有影响力数据点即可稳健估计治疗效应的方法。", "method": "提出了一种在包含异常值的污染模型下，用于平均治疗效应（ATE）的双重鲁棒点估计器。通过鲁棒估计方程实现结果回归的鲁棒性，并利用协变量平衡倾向得分（CBPS）确保倾向得分建模的弹性。为防止过拟合，纳入变量选择，并将所有组件统一在惩罚经验似然框架下。此外，还推导了一种利用所提出估计方程的ATE最优有限样本置信区间，确保区间界限不受异常值影响。", "result": "通过模拟和涉及高血压数据的真实世界应用，证明了该方法在准确性和鲁棒性方面始终优于现有方法。", "conclusion": "本研究提出的双重鲁棒方法能够有效应对观测研究中异常值对因果效应估计的扭曲，并在有限样本中提供更准确和鲁棒的治疗效应估计和置信区间。", "translation": "异常值会严重扭曲观测研究中的因果效应估计，然而这个问题在文献中受到的关注有限。它们的影响在小样本量中尤为明显，此时检测和移除异常值变得越来越困难。因此，在不排除这些有影响力数据点的情况下稳健地估计治疗效应至关重要。为了解决这个问题，我们提出了一种在包含异常值的污染模型下，用于平均治疗效应的双重鲁棒点估计器。结果回归的鲁棒性通过鲁棒估计方程实现，而协变量平衡倾向得分（CBPS）确保了倾向得分建模的弹性。\n为了防止由于包含大量参数而导致的模型过拟合，我们纳入了变量选择。所有这些组件都统一在惩罚经验似然框架下。对于置信区间估计，大多数现有方法依赖于渐近性质，这在有限样本中可能不可靠。我们利用我们提出的估计方程，推导了平均治疗效应的最优有限样本置信区间，确保区间界限不受异常值影响。通过模拟和涉及高血压数据的真实世界应用，我们证明了我们的方法在准确性和鲁棒性方面始终优于现有方法。", "summary": "该论文提出了一种双重鲁棒方法，用于在存在异常值的观测研究中估计因果治疗效应。该方法包括一个双重鲁棒点估计器，结合鲁棒估计方程和协变量平衡倾向得分，并在惩罚经验似然框架下整合变量选择以防止过拟合。此外，还推导了不受异常值影响的有限样本最优置信区间。模拟和真实世界应用表明，该方法在准确性和鲁棒性上优于现有技术。", "keywords": "因果推断, 异常值, 双重鲁棒, 鲁棒估计, 有限样本置信区间", "comments": "该论文的创新之处在于提出了一种在异常值存在下进行因果效应估计的全面且双重鲁棒的方法。它不仅解决了点估计的鲁棒性问题，还通过推导有限样本置信区间解决了传统方法在小样本中置信区间不可靠的问题。将变量选择和惩罚经验似然框架结合，进一步增强了方法的实用性。该研究对于在存在数据污染的实际应用中进行可靠的因果推断具有重要意义。"}}
{"id": "2504.19991", "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data", "authors": ["Ioannis Kontogiorgakis", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Dimitra A. Loka", "Christos Noulas", "Alexandros Tsitouras", "Charalampos Kontoes"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2025)", "url": "http://arxiv.org/abs/2504.19991v2", "summary": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as they commonly rely on\nground-based field surveys, which are often costly, time-consuming and subject\nto delays. In order to tackle this problem, we leverage earth observation data\nand Machine Learning (ML). Specifically, we developed separate ML models using\nSentinel-2 and PlanetScope satellite time series data, respectively, to\nclassify four distinct weed management methods (Mowing, Tillage,\nChemical-spraying, and No practice) in orchards. The findings demonstrate the\npotential of ML-driven remote sensing to enhance the efficiency and accuracy of\nweed management mapping in orchards.", "comment": "Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "pdf_url": "http://arxiv.org/pdf/2504.19991v2", "cate": "cs.CV", "date": "2025-04-28", "updated": "2025-07-23", "AI": {"title_translation": "果园杂草管理方法测绘：基于Sentinel-2和PlanetScope数据", "tldr": "本文利用Sentinel-2和PlanetScope卫星数据及机器学习模型，实现了果园杂草管理方法的遥感测绘，以克服传统地面调查的局限性。", "motivation": "有效的杂草管理对提高农业生产力至关重要，政策制定者需要准确的杂草管理方法地图来评估农民实践、影响以及确保政策合规性。然而，传统的地面调查成本高、耗时且易受延误，这使得监测杂草管理方法充满挑战。", "method": "本研究利用地球观测数据和机器学习(ML)技术。具体而言，研究人员分别使用Sentinel-2和PlanetScope卫星时间序列数据开发了独立的ML模型，以对果园中的四种不同杂草管理方法（割草、耕作、化学喷洒和不实践）进行分类。", "result": "研究结果表明，机器学习驱动的遥感技术在提高果园杂草管理测绘的效率和准确性方面具有潜力。", "conclusion": "机器学习驱动的遥感技术能够有效提升果园杂草管理方法测绘的效率和准确性，为政策制定者提供所需信息，以克服传统地面调查的挑战。", "translation": "有效的杂草管理对于提高农业生产力至关重要，因为杂草会与作物争夺养分和水分等重要资源。杂草管理方法的准确地图对于政策制定者评估农民实践、评估对植被健康、生物多样性和气候的影响以及确保政策和补贴的合规性至关重要。然而，监测杂草管理方法具有挑战性，因为它们通常依赖于地面实地调查，而这些调查往往成本高昂、耗时且容易延误。为了解决这个问题，我们利用地球观测数据和机器学习（ML）。具体而言，我们分别使用Sentinel-2和PlanetScope卫星时间序列数据开发了独立的ML模型，以对果园中的四种不同杂草管理方法（割草、耕作、化学喷洒和不实践）进行分类。研究结果表明，机器学习驱动的遥感技术具有提高果园杂草管理测绘效率和准确性的潜力。", "summary": "本文旨在解决传统杂草管理方法监测面临的挑战，通过利用Sentinel-2和PlanetScope卫星时间序列数据，并结合机器学习模型，对果园中的四种杂草管理方法（割草、耕作、化学喷洒和不实践）进行分类测绘。研究结果验证了机器学习驱动的遥感技术在提升果园杂草管理测绘效率和准确性方面的巨大潜力，为农业生产力和政策制定提供了重要支持。", "keywords": "杂草管理, 遥感, 机器学习, Sentinel-2, PlanetScope, 果园", "comments": "这项研究通过结合遥感技术和机器学习，提供了一种创新且高效的杂草管理方法监测方案，克服了传统地面调查的局限性。其重要性在于能够为政策制定者提供及时准确的数据，以支持农业可持续发展和环境管理。"}}
{"id": "2507.16860", "title": "Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs", "authors": ["Apoorva Gulati", "Rajesh Kumar", "Vinti Agarwal", "Aditya Sharma"], "categories": ["cs.SI", "cs.CV", "cs.CY"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures, 1 table, accepted for publication at ASONAM 2025. this https URL", "url": "http://arxiv.org/abs/2507.16860v1", "summary": "Large Language Models (LLMs) have made it easier to create realistic fake\nprofiles on platforms like LinkedIn. This poses a significant risk for\ntext-based fake profile detectors. In this study, we evaluate the robustness of\nexisting detectors against LLM-generated profiles. While highly effective in\ndetecting manually created fake profiles (False Accept Rate: 6-7%), the\nexisting detectors fail to identify GPT-generated profiles (False Accept Rate:\n42-52%). We propose GPT-assisted adversarial training as a countermeasure,\nrestoring the False Accept Rate to between 1-7% without impacting the False\nReject Rates (0.5-2%). Ablation studies revealed that detectors trained on\ncombined numerical and textual embeddings exhibit the highest robustness,\nfollowed by those using numerical-only embeddings, and lastly those using\ntextual-only embeddings. Complementary analysis on the ability of prompt-based\nGPT-4Turbo and human evaluators affirms the need for robust automated detectors\nsuch as the one proposed in this study.", "comment": "10 pages, 3 figures, 1 table, accepted for publication at ASONAM\n  2025. https://sites.google.com/view/weaklinksinlinkedin/home", "pdf_url": "http://arxiv.org/pdf/2507.16860v1", "cate": "cs.SI", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "LinkedIn中的薄弱环节：在大型语言模型时代增强虚假个人资料检测", "tldr": "大型语言模型使创建逼真的虚假LinkedIn个人资料变得更容易，现有检测器对其无效。本研究提出并验证了一种基于GPT辅助对抗训练的方法，显著提高了检测器对LLM生成虚假个人资料的鲁棒性。", "motivation": "大型语言模型（LLMs）使得在LinkedIn等平台上创建逼真的虚假个人资料变得更加容易，这对基于文本的虚假个人资料检测器构成了重大风险。", "method": "本研究评估了现有检测器对抗LLM生成个人资料的鲁棒性。为应对现有检测器失效的问题，研究提出并采用了GPT辅助对抗训练作为反制措施。此外，还进行了消融研究，以分析不同嵌入类型对检测器鲁棒性的影响。", "result": "现有检测器在检测手动创建的虚假个人资料时非常有效（误接受率：6-7%），但未能识别GPT生成的个人资料（误接受率：42-52%）。所提出的GPT辅助对抗训练方法将误接受率恢复到1-7%之间，且不影响误拒率（0.5-2%）。消融研究表明，结合数值和文本嵌入训练的检测器表现出最高的鲁棒性，其次是仅使用数值嵌入的检测器，最后是仅使用文本嵌入的检测器。", "conclusion": "LLM的出现使得对现有虚假个人资料检测器进行增强成为必要。本研究提出的GPT辅助对抗训练方法能够有效提升检测器对LLM生成虚假个人资料的鲁棒性，并且结合数值和文本嵌入的训练策略能达到最佳效果，这证实了对鲁棒自动化检测器的需求。", "translation": "大型语言模型（LLMs）使得在LinkedIn等平台上创建逼真的虚假个人资料变得更容易。这对基于文本的虚假个人资料检测器构成了重大风险。在本研究中，我们评估了现有检测器对抗LLM生成个人资料的鲁棒性。虽然在检测手动创建的虚假个人资料时非常有效（误接受率：6-7%），但现有检测器未能识别GPT生成的个人资料（误接受率：42-52%）。我们提出GPT辅助对抗训练作为一种反制措施，将误接受率恢复到1-7%之间，同时不影响误拒率（0.5-2%）。消融研究表明，在结合数值和文本嵌入上训练的检测器表现出最高的鲁棒性，其次是仅使用数值嵌入的检测器，最后是仅使用文本嵌入的检测器。对基于提示的GPT-4Turbo和人工评估员能力的补充分析证实了对本研究中提出的鲁棒自动化检测器的需求。", "summary": "本研究探讨了大型语言模型（LLMs）对现有虚假个人资料检测器构成的威胁，特别是在LinkedIn平台上。研究发现，尽管现有检测器能有效识别手动创建的虚假资料，但对LLM生成的资料的误接受率高达42-52%。为解决此问题，论文提出了一种GPT辅助对抗训练方法，成功将误接受率降低至1-7%。消融研究进一步揭示，结合数值和文本嵌入的训练方式能提供最佳的检测鲁棒性。研究强调了在LLM时代开发鲁棒自动化检测器的必要性。", "keywords": "LLMs, 虚假个人资料检测, LinkedIn, 对抗训练, 鲁棒性", "comments": "这项研究具有重要的现实意义，因为它解决了LLM技术进步带来的新型网络安全挑战。通过引入GPT辅助对抗训练，该论文为增强现有检测系统提供了实用的解决方案。其创新之处在于结合对抗训练来对抗LLM生成的虚假数据，并深入分析了不同嵌入策略的有效性，为未来的研究提供了方向。"}}
{"id": "2507.17679", "title": "Safety Assurance for Quadrotor Kinodynamic Motion Planning", "authors": ["Theodoros Tavoulareas", "Marzia Cescon"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication at 2025 Modeling, Estimation and Control Conference (MECC)", "url": "http://arxiv.org/abs/2507.17679v1", "summary": "Autonomous drones have gained considerable attention for applications in\nreal-world scenarios, such as search and rescue, inspection, and delivery. As\ntheir use becomes ever more pervasive in civilian applications, failure to\nensure safe operation can lead to physical damage to the system, environmental\npollution, and even loss of human life. Recent work has demonstrated that\nmotion planning techniques effectively generate a collision-free trajectory\nduring navigation. However, these methods, while creating the motion plans, do\nnot inherently consider the safe operational region of the system, leading to\npotential safety constraints violation during deployment. In this paper, we\npropose a method that leverages run time safety assurance in a kinodynamic\nmotion planning scheme to satisfy the system's operational constraints. First,\nwe use a sampling-based geometric planner to determine a high-level\ncollision-free path within a user-defined space. Second, we design a low-level\nsafety assurance filter to provide safety guarantees to the control input of a\nLinear Quadratic Regulator (LQR) designed with the purpose of trajectory\ntracking. We demonstrate our proposed approach in a restricted 3D simulation\nenvironment using a model of the Crazyflie 2.0 drone.", "comment": "Accepted for publication at 2025 Modeling, Estimation and Control\n  Conference (MECC)", "pdf_url": "http://arxiv.org/pdf/2507.17679v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "四旋翼飞行器运动学动力学运动规划的安全保障", "tldr": "针对四旋翼飞行器运动规划中未考虑安全操作区域的问题，本文提出一种结合运行时安全保障的运动学动力学规划方案，通过几何规划器生成路径并使用安全保障滤波器确保控制输入安全，已在3D仿真中验证。", "motivation": "自动无人机在实际应用中日益普及，但现有运动规划方法在生成无碰撞轨迹时，未固有考虑系统的安全操作区域，可能导致部署期间违反安全约束，造成物理损坏、环境污染甚至生命损失。", "method": "本文提出一种在运动学动力学运动规划方案中利用运行时安全保障的方法，以满足系统的操作约束。具体步骤为：1. 使用基于采样的几何规划器在用户定义空间内确定高层无碰撞路径。2. 设计低层安全保障滤波器，为旨在轨迹跟踪的线性二次调节器（LQR）的控制输入提供安全保证。", "result": "该方法已在受限的3D仿真环境中，使用Crazyflie 2.0无人机模型进行了演示验证。", "conclusion": "本文提出了一种结合运行时安全保障的运动学动力学运动规划方法，有效解决了现有方法在无人机安全操作区域考虑不足的问题，并通过仿真验证了其在满足系统操作约束方面的有效性。", "translation": "自动无人机在搜索救援、检查和递送等现实应用中获得了广泛关注。随着它们在民用应用中变得越来越普及，未能确保安全操作可能导致系统物理损坏、环境污染，甚至生命损失。最近的工作表明，运动规划技术在导航过程中有效地生成了无碰撞轨迹。然而，这些方法在创建运动计划时，并未固有地考虑系统的安全操作区域，导致部署期间可能违反安全约束。在本文中，我们提出了一种利用运行时安全保障的运动学动力学运动规划方案，以满足系统的操作约束。首先，我们使用基于采样的几何规划器在用户定义空间内确定高层无碰撞路径。其次，我们设计了一个低层安全保障滤波器，为旨在轨迹跟踪的线性二次调节器（LQR）的控制输入提供安全保证。我们在受限的3D仿真环境中使用Crazyflie 2.0无人机模型演示了我们提出的方法。", "summary": "本文提出一种针对四旋翼飞行器运动规划的安全保障方法，旨在解决现有规划方法未能充分考虑系统安全操作区域的问题。该方法结合了基于采样的几何规划器生成高层无碰撞路径，并设计了低层安全保障滤波器来确保轨迹跟踪LQR的控制输入安全，已在3D仿真环境中得到验证。", "keywords": "四旋翼飞行器, 运动规划, 安全保障, 运行时安全, LQR", "comments": "本文的创新点在于将运行时安全保障集成到运动学动力学运动规划中，有效解决了无人机在复杂环境中部署时可能面临的安全约束违反问题。这种分层规划与安全滤波结合的方法为提高自主无人机的可靠性和安全性提供了重要途径。其重要性在于，随着无人机在民用领域应用的增加，确保其安全运行至关重要，该研究为此提供了实际可行的解决方案。"}}
{"id": "2507.16836", "title": "From Black Box to Biomarker: Sparse Autoencoders for Interpreting Speech Models of Parkinson's Disease", "authors": ["Peter Plantinga", "Jen-Kai Chen", "Roozbeh Sattari", "Mirco Ravanelli", "Denise Klein"], "categories": ["eess.AS", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures, submitted to NeurIPS 2025", "url": "http://arxiv.org/abs/2507.16836v1", "summary": "Speech holds promise as a cost-effective and non-invasive biomarker for\nneurological conditions such as Parkinson's disease (PD). While deep learning\nsystems trained on raw audio can find subtle signals not available from\nhand-crafted features, their black-box nature hinders clinical adoption. To\naddress this, we apply sparse autoencoders (SAEs) to uncover interpretable\ninternal representations from a speech-based PD detection system. We introduce\na novel mask-based activation for adapting SAEs to small biomedical datasets,\ncreating sparse disentangled dictionary representations. These dictionary\nentries are found to have strong associations with characteristic articulatory\ndeficits in PD speech, such as reduced spectral flux and increased spectral\nflatness in the low-energy regions highlighted by the model attention. We\nfurther show that the spectral flux is related to volumetric measurements of\nthe putamen from MRI scans, demonstrating the potential of SAEs to reveal\nclinically relevant biomarkers for disease monitoring and diagnosis.", "comment": "14 pages, 5 figures, submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2507.16836v1", "cate": "eess.AS", "date": "2025-07-16", "updated": "2025-07-16", "AI": {"title_translation": "从黑箱到生物标志物：用于解释帕金森病语音模型的稀疏自编码器", "tldr": "本研究利用稀疏自编码器解释深度学习帕金森病语音模型，揭示可解释的生物标志物。", "motivation": "深度学习语音系统在帕金森病检测中虽有潜力，但其“黑箱”特性阻碍了临床应用。本研究旨在解决模型可解释性问题。", "method": "研究应用稀疏自编码器（SAEs）从基于语音的帕金森病检测系统中提取可解释的内部表示。引入了一种新颖的基于掩码的激活方法，以使SAEs适应小型生物医学数据集，从而创建稀疏解耦的字典表示。", "result": "发现字典条目与帕金森病语音中特征性发音缺陷（如低能量区域的频谱通量减少和频谱平坦度增加）有很强的关联。进一步表明频谱通量与MRI扫描中壳核的体积测量相关。", "conclusion": "研究证明稀疏自编码器有潜力揭示与临床相关的生物标志物，用于疾病监测和诊断。", "translation": "语音作为帕金森病（PD）等神经系统疾病的一种经济高效且非侵入性生物标志物具有广阔前景。虽然在原始音频上训练的深度学习系统能够发现手工特征无法获得的细微信号，但其黑箱性质阻碍了临床应用。为了解决这个问题，我们应用稀疏自编码器（SAEs）从基于语音的PD检测系统中揭示可解释的内部表示。我们引入了一种新颖的基于掩码的激活方法，用于将SAEs适应于小型生物医学数据集，从而创建稀疏解耦的字典表示。发现这些字典条目与PD语音中特征性发音缺陷有很强的关联，例如模型注意力突出显示的低能量区域中频谱通量减少和频谱平坦度增加。我们进一步表明频谱通量与MRI扫描中壳核的体积测量相关，这证明了SAEs在揭示用于疾病监测和诊断的临床相关生物标志物方面的潜力。", "summary": "该研究旨在解决深度学习语音模型在帕金森病检测中因“黑箱”性质而阻碍临床应用的问题。通过应用稀疏自编码器（SAEs）并引入一种新型的基于掩码的激活方法，研究成功地从语音模型中提取了可解释的内部表示。这些表示与帕金森病的发音缺陷（如频谱通量和频谱平坦度）紧密相关，并且频谱通量还与MRI测量的壳核体积相关，表明SAEs在发现临床相关生物标志物方面具有巨大潜力。", "keywords": "稀疏自编码器, 帕金森病, 语音生物标志物, 可解释性, 深度学习", "comments": "该论文的创新之处在于将稀疏自编码器应用于深度学习语音模型的可解释性，特别是引入了新颖的基于掩码的激活方法，使其适用于小规模生物医学数据集。这对于推动深度学习模型在帕金森病诊断和监测中的临床应用具有重要意义，因为它将“黑箱”模型转化为可理解的生物标志物，增强了临床医生对模型结果的信任度。"}}
{"id": "2507.17613", "title": "InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling", "authors": ["Xiaoxue Chen", "Bhargav Chandaka", "Chih-Hao Lin", "Ya-Qin Zhang", "David Forsyth", "Hao Zhao", "Shenlong Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.17613v1", "summary": "We present InvRGB+L, a novel inverse rendering model that reconstructs large,\nrelightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional\ninverse graphics methods rely primarily on RGB observations and use LiDAR\nmainly for geometric information, often resulting in suboptimal material\nestimates due to visible light interference. We find that LiDAR's intensity\nvalues-captured with active illumination in a different spectral range-offer\ncomplementary cues for robust material estimation under variable lighting.\nInspired by this, InvRGB+L leverages LiDAR intensity cues to overcome\nchallenges inherent in RGB-centric inverse graphics through two key\ninnovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR\nmaterial consistency losses. The model produces novel-view RGB and LiDAR\nrenderings of urban and indoor scenes and supports relighting, night\nsimulations, and dynamic object insertions, achieving results that surpass\ncurrent state-of-the-art methods in both scene-level urban inverse rendering\nand LiDAR simulation.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17613v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "InvRGB+L：复杂场景的逆向渲染与统一的颜色和激光雷达反射率建模", "tldr": "InvRGB+L是一个新的逆向渲染模型，它利用LiDAR强度值来克服传统RGB方法在材料估计中的不足，能够从单一RGB+LiDAR序列重建大型、可重新照明和动态场景，并在逆向渲染和LiDAR模拟方面超越现有技术。", "motivation": "传统的逆向图形方法主要依赖RGB观测，并将LiDAR主要用于几何信息，这常因可见光干扰导致次优的材料估计。本研究的动机在于利用LiDAR强度值提供互补线索，以实现可变照明下更鲁棒的材料估计，从而克服以RGB为中心的逆向图形固有的挑战。", "method": "InvRGB+L模型通过两项关键创新利用LiDAR强度线索：1) 一个新颖的基于物理的LiDAR着色模型；2) RGB-LiDAR材料一致性损失。该模型从单一RGB+LiDAR序列重建场景。", "result": "该模型能够生成城市和室内场景的新视角RGB和LiDAR渲染，并支持重新照明、夜间模拟和动态物体插入。其结果在场景级城市逆向渲染和LiDAR模拟方面均超越了当前最先进的方法。", "conclusion": "InvRGB+L通过结合LiDAR强度值，显著提升了复杂场景的逆向渲染能力，解决了传统RGB方法在材料估计上的局限性，并实现了更鲁棒和多功能的场景重建与模拟。", "translation": "我们提出了InvRGB+L，一个新颖的逆向渲染模型，它能从单一RGB+LiDAR序列重建大型、可重新照明和动态场景。传统的逆向图形方法主要依赖RGB观测，并将LiDAR主要用于几何信息，这常因可见光干扰导致次优的材料估计。我们发现，LiDAR的强度值——通过不同光谱范围的主动照明捕获——为可变照明下鲁棒的材料估计提供了互补线索。受此启发，InvRGB+L通过两项关键创新利用LiDAR强度线索来克服以RGB为中心的逆向图形固有的挑战：(1) 一个新颖的基于物理的LiDAR着色模型和(2) RGB-LiDAR材料一致性损失。该模型能生成城市和室内场景的新视角RGB和LiDAR渲染，并支持重新照明、夜间模拟和动态物体插入，其结果在场景级城市逆向渲染和LiDAR模拟方面均超越了当前最先进的方法。", "summary": "InvRGB+L是一个创新的逆向渲染模型，旨在从单一RGB+LiDAR序列重建复杂场景。它解决了传统RGB方法在材料估计中因可见光干扰而导致的局限性，通过利用LiDAR强度值作为互补线索。该模型的核心在于其新颖的基于物理的LiDAR着色模型和RGB-LiDAR材料一致性损失。实验结果表明，InvRGB+L在生成新视角渲染、支持重新照明、夜间模拟和动态物体插入方面表现出色，并在场景级城市逆向渲染和LiDAR模拟方面超越了现有技术水平。", "keywords": "逆向渲染, LiDAR, 材料估计, RGB-LiDAR, 场景重建", "comments": "该论文的创新点在于首次将LiDAR的强度信息（而非仅仅几何信息）深度融入逆向渲染框架，并通过物理模型和一致性损失来弥补RGB数据的不足。这对于复杂场景，特别是那些光照条件多变或包含动态元素的场景重建具有重要意义。其超越现有技术的结果表明了该方法在实际应用中的巨大潜力，尤其是在自动驾驶、虚拟现实等领域。"}}
{"id": "2507.17495", "title": "A Virtual Quantum Network Prototype for Open Access", "authors": ["Raj Kamleshkumar Madhu", "Visuttha Manthamkarn", "Zheshen Zhang", "Jianqing Liu"], "categories": ["quant-ph", "cs.NI"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17495v1", "summary": "The rise of quantum networks has revolutionized domains such as\ncommunication, sensing, and cybersecurity. Despite this progress, current\nquantum network systems remain limited in scale, are highly\napplication-specific (e.g., for quantum key distribution), and lack a clear\nroad map for global expansion. These limitations are largely driven by a\nshortage of skilled professionals, limited accessibility to quantum\ninfrastructure, and the high complexity and cost associated with building and\noperating quantum hardware. To address these challenges, this paper proposes an\nopen-access software-based quantum network virtualization platform designed to\nfacilitate scalable and remote interaction with quantum hardware. The system is\nbuilt around a cloud application that virtualizes the core hardware components\nof a lab-scale quantum network testbed, including the time tagger and optical\nswitch, enabling users to perform coincidence counts of the photon\nentanglements while ensuring fair resource allocation. The fairness is ensured\nby employing the Hungarian Algorithm to allocate nearly equal effective\nentanglement rates among users. We provide implementation details and\nperformance analysis from the perspectives of hardware, software, and cloud\nplatform, which demonstrates the functionality and efficiency of the developed\nprototype.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17495v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "开放访问的虚拟量子网络原型", "tldr": "本文提出了一个开放访问的软件定义量子网络虚拟化平台，旨在解决当前量子网络规模受限、应用特定化、缺乏全球扩展路线图等挑战，并通过虚拟化核心硬件组件和公平资源分配机制，实现了远程交互和高效的量子网络操作。", "motivation": "当前的量子网络系统规模受限、应用高度特定化（例如量子密钥分发），并且缺乏全球扩展的明确路线图。这些限制主要源于熟练专业人员的短缺、量子基础设施的可访问性有限以及构建和操作量子硬件的高复杂性和高成本。", "method": "本文提出了一个开放访问的软件定义量子网络虚拟化平台，旨在促进与量子硬件的可扩展和远程交互。该系统围绕一个云应用程序构建，该应用程序虚拟化了实验室规模量子网络测试平台的核心硬件组件，包括时间标签器和光开关，使用户能够执行光子纠缠的符合计数，同时确保公平的资源分配。公平性通过采用匈牙利算法来分配用户之间几乎相等的有效纠缠率来确保。", "result": "提供了硬件、软件和云平台的实现细节和性能分析，证明了所开发原型的功能性和效率。", "conclusion": "该原型成功地解决了现有量子网络的局限性，通过提供一个可扩展、可访问且成本效益高的虚拟化平台，为量子网络的未来发展和全球扩展奠定了基础。", "translation": "量子网络的兴起彻底改变了通信、传感和网络安全等领域。尽管取得了这些进展，但当前的量子网络系统规模仍然有限，高度专注于特定应用（例如量子密钥分发），并且缺乏明确的全球扩展路线图。这些限制主要源于熟练专业人员的短缺、量子基础设施的可访问性有限以及构建和操作量子硬件的高复杂性和高成本。为了应对这些挑战，本文提出了一个开放访问的软件定义量子网络虚拟化平台，旨在促进与量子硬件的可扩展和远程交互。该系统围绕一个云应用程序构建，该应用程序虚拟化了实验室规模量子网络测试平台的核心硬件组件，包括时间标签器和光开关，使用户能够执行光子纠缠的符合计数，同时确保公平的资源分配。公平性通过采用匈牙利算法来分配用户之间几乎相等的有效纠缠率来确保。我们从硬件、软件和云平台的角度提供了实现细节和性能分析，这证明了所开发原型的功能性和效率。", "summary": "本文针对现有量子网络规模受限、应用特定化和扩展困难的问题，提出了一个开放访问的软件定义量子网络虚拟化平台。该平台通过云应用虚拟化量子硬件组件，支持用户远程进行光子纠缠符合计数，并利用匈牙利算法确保资源公平分配。研究展示了该原型的功能性和效率。", "keywords": "量子网络, 虚拟化, 开放访问, 匈牙利算法, 云平台", "comments": "本文的创新之处在于提出了一个软件定义的量子网络虚拟化平台，有效解决了当前量子网络面临的规模、可访问性和成本挑战。通过虚拟化核心硬件组件并引入公平资源分配机制，该原型为实现更广泛、更便捷的量子网络应用奠定了基础，对未来量子技术的发展具有重要意义。"}}
{"id": "2506.17286", "title": "GTA: Grouped-head latenT Attention", "authors": ["Luoyang Sun", "Cheng Deng", "Jiwen Jiang", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17286v2", "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17286v2", "cate": "cs.CL", "date": "2025-06-15", "updated": "2025-07-23", "AI": {"title_translation": "GTA：分组头潜在注意力", "tldr": "GTA是一种新的注意力机制，通过减少KV缓存和计算冗余，显著降低LLM的内存和计算开销，同时提高推理速度。", "motivation": "大型语言模型（LLMs）中的注意力机制存在巨大的计算和内存开销，随着文本长度增加，KV缓存和注意力计算迅速扩展，成为部署在有限计算和内存资源硬件上的瓶颈。作者观察到注意力机制存在显著冗余，KV缓存可被压缩，且不同头之间的注意力图相似度高，表明大量计算和存储是不必要的。", "method": "作者提出了分组头潜在注意力（GTA），这是一种新型注意力机制，旨在在保持性能的同时减少内存使用和计算复杂性。GTA包含两个组件：1) 共享注意力图机制，该机制在多个头之间重用注意力分数，从而减小键缓存大小；2) 带有学习投影的非线性值解码器，该解码器将值缓存压缩到潜在空间中，进一步降低内存需求。", "result": "与分组查询注意力相比，GTA将注意力计算的FLOPs降低了高达62.5%，并将KV缓存缩小了高达70%，同时避免了多头潜在注意力的额外开销，从而提高了LLM的部署效率。因此，GTA模型实现了端到端推理速度2倍的提升，其中预填充受益于计算成本的降低，解码受益于更小的缓存占用。", "conclusion": "GTA通过创新的共享注意力图机制和非线性值解码器有效解决了LLM中注意力机制的计算和内存瓶颈，显著提高了推理效率和部署可行性。", "translation": "注意力机制是大型语言模型（LLMs）成功的基石，但其巨大的计算和内存开销对优化效率和性能构成了挑战。一个关键的瓶颈在于KV缓存和注意力计算随文本长度的增加而迅速扩展，这使得在计算和内存资源有限的硬件上部署变得困难。我们观察到注意力机制表现出显著的冗余，因为KV缓存可以被显著压缩，并且不同头之间的注意力图显示出高度相似性，这表明大量的计算和存储是不必要的。利用这些见解，我们提出了**G**rouped-Head Laten**T** **A**ttention (GTA)，一种新颖的注意力机制，它在保持性能的同时减少了内存使用和计算复杂性。GTA包含两个组件：(1) 一个共享注意力图机制，该机制在多个头之间重用注意力分数，从而减小键缓存大小；(2) 一个带有学习投影的非线性值解码器，该解码器将值缓存压缩到潜在空间中，进一步降低内存需求。与分组查询注意力相比，GTA将注意力计算的FLOPs降低了高达62.5%，并将KV缓存缩小了高达70%，同时避免了多头潜在注意力的额外开销，从而提高了LLM的部署效率。因此，GTA模型实现了端到端推理速度2倍的提升，其中预填充受益于计算成本的降低，解码受益于更小的缓存占用。", "summary": "本文提出了一种名为分组头潜在注意力（GTA）的新型注意力机制，旨在解决大型语言模型（LLMs）中注意力机制的计算和内存开销问题。通过观察到注意力机制的冗余性（KV缓存可压缩，注意力图相似），GTA引入了共享注意力图机制以重用注意力分数并减小键缓存，以及非线性值解码器将值缓存压缩到潜在空间。实验结果表明，GTA显著减少了注意力计算FLOPs（高达62.5%）和KV缓存大小（高达70%），并使LLM的端到端推理速度提升了2倍，从而提高了部署效率。", "keywords": "注意力机制, 大型语言模型, KV缓存, 计算效率, 内存优化", "comments": "GTA的创新之处在于其识别并利用了注意力机制中的冗余性，通过共享注意力图和潜在空间压缩实现了显著的内存和计算效率提升。这对于在资源受限硬件上部署大型语言模型具有重要意义，是LLM推理优化领域的一个重要进展。其避免了传统多头潜在注意力可能带来的额外开销，显示了其设计的优越性。"}}
{"id": "2507.17025", "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17025v1", "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17025v1", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "用于NLP嵌入二值表示的演化特征阈值", "tldr": "本文提出了一种基于坐标搜索的优化框架，为NLP嵌入的每个特征识别最佳阈值，以生成高效准确的二值表示，该方法在各种NLP任务中表现优于传统二值化方法。", "motivation": "大规模自然语言处理（NLP）应用中，文本嵌入的存储和计算效率是关键问题，因此需要探索使用二值表示（条形码）代替实值特征。", "method": "提出了一种基于坐标搜索的优化框架，该框架为每个特征识别最佳阈值，而不是使用固定的全局阈值，以将连续嵌入转换为二值表示。", "result": "最优条形码表示在各种NLP应用中显示出有前景的结果。使用我们方法找到的最优阈值生成的二值嵌入在准确性方面优于传统的二值化方法。该技术具有通用性，可应用于任何特征。", "conclusion": "通过为每个特征识别最佳阈值，本文提出的方法能够生成准确且高效的二值表示，有望改变文本表示方式，并可广泛应用于机器学习领域。", "translation": "高效的文本嵌入对于大规模自然语言处理（NLP）应用至关重要，其中存储和计算效率是关键问题。在本文中，我们探讨了如何使用二值表示（条形码）而不是实值特征来处理来自BERT等机器学习模型的NLP嵌入。阈值化是将连续嵌入转换为二值表示的常用方法，通常对所有特征使用固定阈值。我们提出了一种基于坐标搜索的优化框架，该框架为每个特征识别最佳阈值，证明了特征特定阈值可以提高二值编码的性能。这确保了二值表示既准确又高效，从而提高了各种特征的性能。我们的最优条形码表示在各种NLP应用中显示出有前景的结果，展示了它们改变文本表示的潜力。我们对不同的NLP任务和数据集进行了广泛的实验和统计测试，以评估我们的方法并将其与其他阈值化方法进行比较。使用我们方法找到的最优阈值生成的二值嵌入在准确性方面优于传统的二值化方法。这种生成二值表示的技术具有通用性，可以应用于任何特征，而不仅仅限于NLP嵌入，使其适用于机器学习应用中的广泛领域。", "summary": "本文提出了一种用于NLP嵌入二值表示的进化特征阈值方法，旨在解决大规模NLP应用中存储和计算效率问题。通过引入一个基于坐标搜索的优化框架，该方法能够为每个特征识别最佳阈值，从而生成比传统固定阈值方法更准确、更高效的二值（条形码）表示。实验证明，这种特征特定的阈值化方法在多种NLP任务中显著提高了性能，并且该技术具有广泛的适用性，不仅限于NLP领域。", "keywords": "二值表示, NLP嵌入, 特征阈值, 优化, 效率", "comments": "该论文的创新点在于提出了一个基于坐标搜索的优化框架，用于为每个特征寻找最优阈值，从而生成更准确和高效的二值表示。这克服了传统固定阈值方法的局限性，并展示了其在提高NLP嵌入效率方面的巨大潜力。此外，其通用性使其应用范围超越了NLP，对整个机器学习领域都有重要意义。"}}
{"id": "2507.09822", "title": "Active Probing with Multimodal Predictions for Motion Planning", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin D'sa"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      To appear at IROS '25. 8 pages. 3 tables. 6 figures. Project page: this https URL", "url": "http://arxiv.org/abs/2507.09822v4", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures. Project page:\n  https://darshangm.github.io/papers/active-probing-multimodal-predictions/", "pdf_url": "http://arxiv.org/pdf/2507.09822v4", "cate": "cs.RO", "date": "2025-07-13", "updated": "2025-07-22", "AI": {"title_translation": "基于多模态预测和主动探测的运动规划", "tldr": "本文提出了一个结合轨迹规划、多模态预测和主动探测的统一框架，以提高动态环境下不确定性决策。该框架引入了一种新的风险度量，并通过主动探测机制减少预测模糊性，在自动导航场景中表现出鲁棒性。", "motivation": "在动态环境中导航需要自主系统能够推理其他智能体行为的不确定性。现有方法在处理多模态预测不确定性方面可能存在挑战，因此需要一个能增强不确定性下决策的统一框架。", "method": "本文提出了一个统一框架，将轨迹规划与多模态预测和主动探测相结合。开发了一种新的风险度量，通过混合模型无缝集成多模态预测不确定性。当不确定性遵循高斯混合分布时，证明了该风险度量存在闭式解且始终有限。为了减少预测模糊性，引入了主动探测机制，策略性地选择动作以改善对其他智能体行为参数的估计，同时处理多模态不确定性。", "result": "在MetaDrive仿真环境中对自动导航场景进行了广泛评估。结果表明，所提出的主动探测方法成功地在复杂交通场景中进行导航，即使存在不确定的预测。此外，该框架在不同的交通智能体行为模型下也表现出鲁棒性能。", "conclusion": "本文提出的结合多模态预测和主动探测的统一框架，能够有效处理动态环境中的不确定性，并在复杂交通场景和不同智能体行为模型下展现出强大的导航能力和鲁棒性，具有广泛的实际应用前景。", "translation": "在动态环境中导航需要自主系统推理其他智能体行为的不确定性。在本文中，我们引入了一个统一的框架，该框架将轨迹规划与多模态预测和主动探测相结合，以增强不确定性下的决策。我们开发了一种新颖的风险度量，通过混合模型无缝集成多模态预测不确定性。当这些不确定性遵循高斯混合分布时，我们证明了我们的风险度量存在闭式解，并且始终是有限的，从而确保了分析的可处理性。为了减少预测模糊性，我们引入了一个主动探测机制，该机制策略性地选择动作以改善其对其他智能体行为参数的估计，同时处理多模态不确定性。我们在使用MetaDrive仿真环境的自动导航场景中广泛评估了我们的框架。结果表明，我们的主动探测方法成功地在具有不确定预测的复杂交通场景中进行导航。此外，我们的框架在各种交通智能体行为模型中显示出鲁棒性能，表明其对现实世界自动导航挑战的广泛适用性。代码和视频可在https://darshangm.github.io/papers/active-probing-multimodal-predictions/获取。", "summary": "本文提出一个统一框架，将轨迹规划、多模态预测和主动探测相结合，以提升动态环境下不确定性决策能力。该框架引入了一种能够整合多模态预测不确定性的新型风险度量，并证明在高斯混合分布下其具有闭式解。为降低预测模糊性，研究引入了主动探测机制，通过策略性行动优化对其他智能体行为参数的估计。在MetaDrive仿真环境中的广泛评估显示，该方法在复杂交通场景中表现出成功的导航能力，并对多种交通智能体行为模型展现出鲁棒性，证明了其在自动导航领域的普适性。", "keywords": "运动规划, 多模态预测, 主动探测, 不确定性, 风险度量", "comments": "本文的创新点在于提出了一个统一的框架，将多模态预测的不确定性处理与主动探测相结合，以优化运动规划。特别是，其提出的风险度量在高斯混合分布下具有闭式解，保证了分析的可行性。主动探测机制能够有效减少预测模糊性，增强了系统在复杂动态环境下的决策能力。该研究对于提升自动驾驶等领域在不确定性条件下的鲁棒性和安全性具有重要意义。"}}
{"id": "2507.17636", "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "authors": ["Victor Hartman", "Petter Törnberg"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17636v1", "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17636v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "谁在攻击，为何攻击？使用大型语言模型识别19个国家1800万条推文中的负面竞选活动", "tldr": "本研究利用零样本大型语言模型（LLMs）大规模识别19个欧洲国家议员推文中的负面竞选活动，发现执政党负面信息较少，而极端和民粹主义政党（特别是激进右翼）负面信息更多。", "motivation": "现有分类方法成本高、可扩展性有限，阻碍了对负面竞选活动的实证研究。", "method": "1. 引入零样本大型语言模型（LLMs）作为跨语言负面竞选活动分类的新方法，并在10种语言的基准数据集上验证其性能与人类编码员相当且优于传统监督机器学习方法。2. 利用此方法分析了2017年至2022年间19个欧洲国家议员发布的1800万条推文。", "result": "执政党使用负面信息的可能性较低；意识形态极端和民粹主义政党——特别是激进右翼政党——负面程度显著更高。", "conclusion": "研究结果增进了对政党层面特征如何塑造多党制中战略沟通的理解。该研究更广泛地展示了LLMs在政治传播领域实现可扩展、透明和可复制研究的潜力。", "translation": "负面竞选是政治竞争的核心特征，然而，现有分类方法成本高昂且可扩展性有限，阻碍了实证研究。本研究做出了两项关键贡献。首先，它引入了零样本大型语言模型（LLMs）作为一种新颖的跨语言负面竞选分类方法。使用10种语言的基准数据集，我们证明LLMs的性能与母语人类编码员相当，并优于传统的监督机器学习方法。其次，我们利用这种新颖方法进行了迄今为止规模最大的跨国负面竞选研究，分析了2017年至2022年间19个欧洲国家议员发布的1800万条推文。结果揭示了跨国界的一致模式：执政党使用负面信息的可能性较低，而意识形态极端和民粹主义政党——特别是激进右翼政党——负面程度显著更高。这些发现增进了我们对政党层面特征如何塑造多党制中战略沟通的理解。更广泛地说，这项研究展示了LLMs在跨语言和文化背景下实现政治传播领域可扩展、透明和可复制研究的潜力。", "summary": "本研究利用零样本大型语言模型（LLMs）解决了负面竞选活动分类的成本和可扩展性问题。研究首先验证了LLMs在跨语言分类上的高效性，其性能媲美人类编码员并超越传统机器学习方法。随后，研究应用此方法对19个欧洲国家1800万条议员推文进行了大规模分析，发现执政党较少使用负面信息，而极端和民粹主义政党（尤其是激进右翼）则表现出更高的负面倾向。这不仅加深了对多党制中战略沟通的理解，也凸显了LLMs在政治传播研究中的巨大潜力。", "keywords": "负面竞选, 大型语言模型, 政治传播, 跨语言分类, 推文分析", "comments": "这项研究的创新之处在于首次将零样本LLMs应用于大规模、跨语言的负面竞选活动识别，解决了传统方法在成本和可扩展性上的局限。其重要性体现在不仅提供了一种高效的分析工具，还通过大规模实证研究揭示了政治沟通中负面竞选的新模式，尤其是在不同政党类型中的差异。这项工作为政治传播领域的未来研究开辟了新的路径，证明了LLMs在社会科学研究中的巨大应用潜力。"}}
{"id": "2507.15455", "title": "Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration", "authors": ["Hee Jun Yang", "Minjung Gim", "Yeoneung Kim"], "categories": ["math.NA", "cs.AI", "cs.NA", "math.AP", "49N70, 35Q93, 49L25, 68T07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15455v2", "summary": "We propose a mesh-free policy iteration framework that combines classical\ndynamic programming with physics-informed neural networks (PINNs) to solve\nhigh-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in\nstochastic differential games and robust control. The method alternates between\nsolving linear second-order PDEs under fixed feedback policies and updating the\ncontrols via pointwise minimax optimization using automatic differentiation.\nUnder standard Lipschitz and uniform ellipticity assumptions, we prove that the\nvalue function iterates converge locally uniformly to the unique viscosity\nsolution of the HJI equation. The analysis establishes equi-Lipschitz\nregularity of the iterates, enabling provable stability and convergence without\nrequiring convexity of the Hamiltonian. Numerical experiments demonstrate the\naccuracy and scalability of the method. In a two-dimensional stochastic\npath-planning game with a moving obstacle, our method matches finite-difference\nbenchmarks with relative $L^2$-errors below %10^{-2}%. In five- and\nten-dimensional publisher-subscriber differential games with anisotropic noise,\nthe proposed approach consistently outperforms direct PINN solvers, yielding\nsmoother value functions and lower residuals. Our results suggest that\nintegrating PINNs with policy iteration is a practical and theoretically\ngrounded method for solving high-dimensional, nonconvex HJI equations, with\npotential applications in robotics, finance, and multi-agent reinforcement\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15455v2", "cate": "math.NA", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "基于PINN策略迭代求解非凸Hamilton--Jacobi--Isaacs方程", "tldr": "本文提出了一种结合经典动态规划和PINN的无网格策略迭代框架，用于求解高维非凸Hamilton--Jacobi--Isaacs (HJI)方程，并在理论和实验上证明了其有效性和优越性。", "motivation": "解决随机微分博弈和鲁棒控制中出现的高维、非凸Hamilton--Jacobi--Isaacs (HJI)方程的挑战。", "method": "提出了一种无网格策略迭代框架，该框架结合了经典动态规划和物理信息神经网络（PINNs）。该方法在固定反馈策略下交替求解线性二阶偏微分方程，并通过使用自动微分的点式极小极大优化来更新控制。", "result": "在标准Lipschitz和一致椭圆性假设下，证明了价值函数迭代局部一致收敛到HJI方程的唯一粘性解。分析建立了迭代的等Lipschitz正则性，使得在哈密顿量非凸的情况下也能证明稳定性和收敛性。数值实验表明了该方法的准确性和可扩展性。在二维随机路径规划游戏中，该方法与有限差分基准匹配，相对L2误差低于10^-2。在具有各向异性噪声的五维和十维发布者-订阅者微分博弈中，所提出的方法始终优于直接PINN求解器，产生更平滑的价值函数和更低的残差。", "conclusion": "将PINNs与策略迭代相结合是求解高维、非凸HJI方程的一种实用且有理论基础的方法，在机器人、金融和多智能体强化学习方面具有潜在应用。", "translation": "我们提出了一种无网格策略迭代框架，该框架结合了经典动态规划和物理信息神经网络（PINNs），用于求解随机微分博弈和鲁棒控制中出现的高维、非凸Hamilton--Jacobi--Isaacs (HJI)方程。该方法在固定反馈策略下交替求解线性二阶偏微分方程，并通过使用自动微分的点式极小极大优化来更新控制。在标准Lipschitz和一致椭圆性假设下，我们证明了价值函数迭代局部一致收敛到HJI方程的唯一粘性解。分析建立了迭代的等Lipschitz正则性，使得在哈密顿量非凸的情况下也能证明稳定性和收敛性。数值实验表明了该方法的准确性和可扩展性。在二维随机路径规划游戏中，我们的方法与有限差分基准匹配，相对L2误差低于10^-2。在具有各向异性噪声的五维和十维发布者-订阅者微分博弈中，所提出的方法始终优于直接PINN求解器，产生更平滑的价值函数和更低的残差。我们的结果表明，将PINNs与策略迭代相结合是求解高维、非凸HJI方程的一种实用且有理论基础的方法，在机器人、金融和多智能体强化学习方面具有潜在应用。", "summary": "本文提出了一种新颖的无网格策略迭代框架，该框架将经典动态规划与物理信息神经网络（PINNs）相结合，旨在解决高维、非凸的Hamilton--Jacobi--Isaacs (HJI)方程。该方法通过交替求解线性PDE和使用自动微分更新控制来实现。研究证明了该方法的收敛性和稳定性，即使在非凸哈密顿量下也成立。数值实验结果表明，与传统方法和直接PINN求解器相比，该方法在准确性、可扩展性和平滑性方面表现优异，具有广泛的应用前景。", "keywords": "Hamilton-Jacobi-Isaacs方程, 策略迭代, 物理信息神经网络, 动态规划, 非凸优化", "comments": "该论文的创新点在于将经典的策略迭代与PINNs相结合，从而为求解高维非凸HJI方程提供了一种有效且有理论保障的新方法。它解决了传统方法在高维问题上的挑战，并克服了直接PINN方法可能存在的平滑性问题。其理论收敛性证明和在不同维度问题上的出色数值表现，使其在鲁棒控制、机器人和多智能体系统等领域具有重要意义。"}}
{"id": "2408.12240", "title": "The Bright Side of Timed Opacity", "authors": ["Étienne André", "Sarah Dépernet", "Engel Lefaucheux"], "categories": ["cs.LO", "cs.CR", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      This is the author (and extended) version of the manuscript of the same name published in the proceedings of the 25th International Conference on Formal Engineering Methods (ICFEM 2024)", "url": "http://arxiv.org/abs/2408.12240v4", "summary": "Timed automata (TAs) are an extension of finite automata that can measure and\nreact to the passage of time, providing the ability to handle real-time\nconstraints using clocks. In 2009, Franck Cassez showed that the timed opacity\nproblem, where an attacker can observe some actions with their timestamps and\nattempts to deduce information, is undecidable for TAs. Moreover, he showed\nthat the undecidability holds even for subclasses such as event-recording\nautomata. In this article, we consider the same definition of opacity, by\nrestricting either the system or the attacker. Our first contribution is to\nprove the inter-reducibility of two variants of opacity: full opacity (for\nwhich the observations should be the same regardless of the visit of a private\nlocation) and weak opacity (for which it suffices that the attacker cannot\ndeduce whether the private location was visited, but for which it is harmless\nto deduce that it was not visited); we also prove further results including a\nconnection with timed language inclusion. Our second contribution is to study\nopacity for several subclasses of TAs: with restrictions on the number of\nclocks, the number of actions, the nature of time, or a new subclass called\nobservable event-recording automata. We show that opacity is mostly decidable\nin these cases, except for one-action TAs and for one-clock TAs with\n$\\epsilon$-transitions, for which undecidability remains. Our third (and\narguably main) contribution is to propose a new definition of opacity in which\nthe number of observations made by the attacker is limited to the first $N$\nobservations, or to a set of $N$ timestamps after which the attacker observes\nthe first action that follows immediately. This set can be defined either a\npriori or at runtime; all three versions yield decidability for the whole TA\nclass.", "comment": "This is the author (and extended) version of the manuscript of the\n  same name published in the proceedings of the 25th International Conference\n  on Formal Engineering Methods (ICFEM 2024)", "pdf_url": "http://arxiv.org/pdf/2408.12240v4", "cate": "cs.LO", "date": "2024-08-22", "updated": "2025-07-23", "AI": {"title_translation": "定时不透明度的光明面", "tldr": "本文研究了定时不透明度问题，针对其不可判定性，通过限制系统或攻击者、研究子类以及提出新的不透明度定义，证明了在多种情况下该问题是可判定的，尤其是在限制观察数量的新定义下对整个定时自动机类都是可判定的。", "motivation": "定时自动机中的定时不透明度问题在2009年被证明是不可判定的，即使对于其子类也是如此。本文的动机是寻找在何种限制下，该问题能够变为可判定的，从而克服其不可判定性。", "method": "本文通过以下方法研究定时不透明度：1. 限制系统或攻击者。2. 证明两种不透明度变体（完全不透明度和弱不透明度）的相互可归约性，并探讨与定时语言包含的关系。3. 研究定时自动机在不同子类下的不透明度，例如限制时钟数量、动作数量、时间性质或新的可观察事件记录自动机子类。4. 提出一种新的不透明度定义，该定义限制了攻击者的观察数量（前N次观察或N个时间戳后的首次观察）。", "result": "1. 证明了完全不透明度和弱不透明度两种变体的相互可归约性，并发现它们与定时语言包含存在联系。2. 证明了在多数定时自动机子类中，不透明度问题是可判定的，但单动作定时自动机和带ε-转换的单时钟定时自动机仍不可判定。3. 提出了一种限制观察数量的新不透明度定义，该定义使得整个定时自动机类的不透明度问题变得可判定。", "conclusion": "通过限制系统/攻击者、研究定时自动机子类以及引入限制观察数量的新不透明度定义，定时不透明度问题在许多情况下，特别是对于新定义，变得可判定，从而克服了原有问题的不可判定性。", "translation": "定时自动机（TAs）是有限自动机的扩展，能够测量和响应时间的流逝，提供使用时钟处理实时约束的能力。2009年，Franck Cassez证明了定时不透明度问题——即攻击者可以观察带有时间戳的某些动作并试图推断信息——对于定时自动机是不可判定的。此外，他指出即使对于事件记录自动机等子类，不可判定性也成立。在本文中，我们通过限制系统或攻击者来考虑相同的不透明度定义。我们的第一个贡献是证明了两种不透明度变体（完全不透明度，即无论是否访问私有位置，观察结果都应相同；以及弱不透明度，即攻击者无法推断私有位置是否被访问，但推断未被访问是无害的）的相互可归约性；我们还证明了包括与定时语言包含的联系在内的进一步结果。我们的第二个贡献是研究了定时自动机几个子类的不透明度：包括对时钟数量、动作数量、时间性质的限制，或一个新的子类——可观察事件记录自动机。我们表明，在这些情况下，不透明度大多是可判定的，除了单动作定时自动机和带ε-转换的单时钟定时自动机，这些情况下不可判定性仍然存在。我们的第三个（也可以说是主要的）贡献是提出了一种新的不透明度定义，其中攻击者进行的观察数量限制在前N次观察，或限制在N个时间戳之后攻击者观察紧随其后的第一个动作。这个集合可以先验定义或在运行时定义；所有这三种版本都使得整个定时自动机类的可判定性得以实现。", "summary": "本文研究了定时自动机中原本不可判定的定时不透明度问题。通过限制系统或攻击者、分析定时自动机子类中的不透明度，并提出一种限制攻击者观察次数的新不透明度定义，研究发现该问题在多种情境下，特别是对于新定义，变得可判定，为解决实时系统中的信息安全问题提供了新的途径。", "keywords": "定时不透明度, 定时自动机, 可判定性, 信息安全, 攻击者观察限制", "comments": "本文的创新之处在于其系统性地探索了导致定时不透明度问题可判定的各种限制条件。特别是，引入限制攻击者观察次数的新不透明度定义是一个重要的突破，它将原本对整个定时自动机类不可判定的问题转变为可判定，这对于实时系统的信息安全分析具有重要意义。文章通过严谨的证明和对多种子类的分析，提供了全面的见解。"}}
{"id": "2507.17221", "title": "Dataset Distillation as Data Compression: A Rate-Utility Perspective", "authors": ["Youneng Bao", "Yiping Liu", "Zhuo Chen", "Yongsheng Liang", "Mu Li", "Kede Ma"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.17221v1", "summary": "Driven by the ``scale-is-everything'' paradigm, modern machine learning\nincreasingly demands ever-larger datasets and models, yielding prohibitive\ncomputational and storage requirements. Dataset distillation mitigates this by\ncompressing an original dataset into a small set of synthetic samples, while\npreserving its full utility. Yet, existing methods either maximize performance\nunder fixed storage budgets or pursue suitable synthetic data representations\nfor redundancy removal, without jointly optimizing both objectives. In this\nwork, we propose a joint rate-utility optimization method for dataset\ndistillation. We parameterize synthetic samples as optimizable latent codes\ndecoded by extremely lightweight networks. We estimate the Shannon entropy of\nquantized latents as the rate measure and plug any existing distillation loss\nas the utility measure, trading them off via a Lagrange multiplier. To enable\nfair, cross-method comparisons, we introduce bits per class (bpc), a precise\nstorage metric that accounts for sample, label, and decoder parameter costs. On\nCIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\\times$\ngreater compression than standard distillation at comparable accuracy. Across\ndiverse bpc budgets, distillation losses, and backbone architectures, our\napproach consistently establishes better rate-utility trade-offs.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17221v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "数据集蒸馏作为数据压缩：一个速率-效用视角", "tldr": "本文提出了一种联合速率-效用优化的数据集蒸馏方法，通过将数据集蒸馏视为数据压缩问题，实现了更高的压缩率和更好的性能权衡。", "motivation": "现代机器学习对大规模数据集和模型的需求导致了巨大的计算和存储开销。数据集蒸馏旨在通过压缩数据集来缓解这一问题，但现有方法未能同时优化存储预算和数据表示，导致效率不足。", "method": "本文提出了一种联合速率-效用优化方法。将合成样本参数化为由极其轻量级网络解码的可优化潜在编码。使用量化潜在编码的香农熵作为速率度量，将任何现有蒸馏损失作为效用度量，并通过拉格朗日乘数进行权衡。为实现公平比较，引入了“每类比特数 (bpc)”这一精确的存储度量。", "result": "在CIFAR-10、CIFAR-100和ImageNet-128上，在可比精度下，该方法实现了比标准蒸馏高出高达170倍的压缩率。在不同的bpc预算、蒸馏损失和骨干架构下，该方法始终建立了更好的速率-效用权衡。", "conclusion": "通过将数据集蒸馏视为数据压缩问题并联合优化速率和效用，可以显著提高数据压缩率，同时保持或改善模型性能，为数据集蒸馏提供了一种更高效和可比较的方法。", "translation": "受“规模至上”范式驱动，现代机器学习日益需要更大的数据集和模型，这带来了高昂的计算和存储要求。数据集蒸馏通过将原始数据集压缩成少量合成样本来缓解这一问题，同时保留其全部效用。然而，现有方法要么在固定存储预算下最大化性能，要么追求合适的合成数据表示以消除冗余，而没有联合优化这两个目标。在这项工作中，我们提出了一种用于数据集蒸馏的联合速率-效用优化方法。我们将合成样本参数化为由极其轻量级网络解码的可优化潜在编码。我们估计量化潜在编码的香农熵作为速率度量，并将任何现有蒸馏损失作为效用度量，通过拉格朗日乘数进行权衡。为了实现公平的跨方法比较，我们引入了每类比特数（bpc），这是一种精确的存储度量，它考虑了样本、标签和解码器参数成本。在CIFAR-10、CIFAR-100和ImageNet-128上，我们的方法在可比精度下实现了比标准蒸馏高出170倍的压缩。在不同的bpc预算、蒸馏损失和骨干架构下，我们的方法始终建立了更好的速率-效用权衡。", "summary": "本文将数据集蒸馏视为数据压缩问题，提出了一种联合速率-效用优化方法，以解决现有数据集蒸馏方法未能同时优化存储效率和模型性能的局限。该方法通过将合成样本表示为轻量级网络解码的潜在编码，并结合香农熵与蒸馏损失进行联合优化。实验证明，在多种数据集和设置下，该方法在保持相似精度的前提下，实现了显著更高的数据压缩率，并提供了更优的速率-效用权衡。", "keywords": "数据集蒸馏, 数据压缩, 速率-效用优化, 每类比特数, 机器学习", "comments": "本文的创新点在于首次将数据集蒸馏与数据压缩的速率-效用理论相结合，并提出了一种联合优化框架。引入“每类比特数 (bpc)”作为统一的存储度量，对于未来数据集蒸馏方法的公平比较具有重要意义。该方法在大幅提高压缩率的同时保持了性能，对于缓解大规模机器学习的资源瓶颈具有实际应用价值。"}}
{"id": "2507.17467", "title": "Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls", "authors": ["Elena Pitta", "Tom Kouwenhoven", "Tessa Verhoef"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      LUHME: 2nd Workshop on Language Understanding in the Human-Machine Era", "url": "http://arxiv.org/abs/2507.17467v1", "summary": "This study investigates the extent to which the Visual Entailment (VE) task\nserves as a reliable probe of vision-language understanding in multimodal\nlanguage models, using the LLaMA 3.2 11B Vision model as a test case. Beyond\nreporting performance metrics, we aim to interpret what these results reveal\nabout the underlying possibilities and limitations of the VE task. We conduct a\nseries of experiments across zero-shot, few-shot, and fine-tuning settings,\nexploring how factors such as prompt design, the number and order of in-context\nexamples and access to visual information might affect VE performance. To\nfurther probe the reasoning processes of the model, we used explanation-based\nevaluations. Results indicate that three-shot inference outperforms the\nzero-shot baselines. However, additional examples introduce more noise than\nthey provide benefits. Additionally, the order of the labels in the prompt is a\ncritical factor that influences the predictions. In the absence of visual\ninformation, the model has a strong tendency to hallucinate and imagine\ncontent, raising questions about the model's over-reliance on linguistic\npriors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on\nthe e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.\nAdditionally, the explanation evaluation demonstrates that the fine-tuned model\nprovides semantically meaningful explanations similar to those of humans, with\na BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore\nresults in experiments with limited vision, questioning the visual grounding of\nthis task. Overall, our results highlight both the utility and limitations of\nVE as a diagnostic task for vision-language understanding and point to\ndirections for refining multimodal evaluation methods.", "comment": "LUHME: 2nd Workshop on Language Understanding in the Human-Machine\n  Era", "pdf_url": "http://arxiv.org/pdf/2507.17467v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过视觉蕴涵任务探究视觉-语言理解：前景与陷阱", "tldr": "本研究使用LLaMA 3.2 11B Vision模型，通过视觉蕴涵（VE）任务探究多模态语言模型的视觉-语言理解能力，揭示了VE任务的潜力与局限性，并提出了改进多模态评估方法的方向。", "motivation": "本研究旨在探究视觉蕴涵（VE）任务在多模态语言模型中作为视觉-语言理解可靠探针的程度，并解读实验结果揭示的VE任务的潜在可能性和局限性。", "method": "研究使用了LLaMA 3.2 11B Vision模型作为测试案例，在零样本、少样本和微调设置下进行了一系列实验。实验探讨了提示设计、上下文示例的数量和顺序以及视觉信息的可访问性等因素如何影响VE性能。为了进一步探究模型的推理过程，研究还采用了基于解释的评估方法。", "result": "结果显示，三样本推理优于零样本基线，但额外的示例引入的噪声多于益处。提示中标签的顺序是影响预测的关键因素。在缺乏视觉信息的情况下，模型倾向于幻觉和想象内容，这引发了对模型过度依赖语言先验的质疑。微调取得了强劲的结果，在e-SNLI-VE数据集上达到了83.3%的准确率，并优于最先进的OFA-X模型。此外，解释评估表明微调模型提供了与人类相似的语义有意义的解释，BERTScore F1分数为89.2%。然而，在有限视觉的实验中也发现了可比较的BERTScore结果，这使得研究者质疑该任务的视觉接地性。", "conclusion": "总体而言，研究结果强调了VE作为视觉-语言理解诊断任务的实用性和局限性，并指出了改进多模态评估方法的方向。", "translation": "本研究旨在探究视觉蕴涵（VE）任务在多模态语言模型中作为视觉-语言理解可靠探针的程度，并以LLaMA 3.2 11B Vision模型作为测试案例。除了报告性能指标，我们旨在解读这些结果揭示的VE任务的潜在可能性和局限性。我们在零样本、少样本和微调设置下进行了一系列实验，探究了提示设计、上下文示例的数量和顺序以及视觉信息的可访问性等因素如何影响VE性能。为了进一步探究模型的推理过程，我们使用了基于解释的评估。结果表明，三样本推理优于零样本基线。然而，额外的示例引入的噪声多于益处。此外，提示中标签的顺序是影响预测的关键因素。在缺乏视觉信息的情况下，模型强烈倾向于幻觉和想象内容，这引发了对模型过度依赖语言先验的质疑。微调取得了强劲的结果，在e-SNLI-VE数据集上达到了83.3%的准确率，并优于最先进的OFA-X模型。此外，解释评估表明微调模型提供了与人类相似的语义有意义的解释，BERTScore F1分数为89.2%。然而，我们在有限视觉的实验中也发现了可比较的BERTScore结果，这使得我们质疑该任务的视觉接地性。总体而言，我们的结果强调了VE作为视觉-语言理解诊断任务的实用性和局限性，并指出了改进多模态评估方法的方向。", "summary": "本研究以LLaMA 3.2 11B Vision模型为案例，深入探讨了视觉蕴涵（VE）任务在评估多模态语言模型视觉-语言理解能力方面的作用。通过零样本、少样本和微调实验，研究发现三样本推理优于零样本，但过多示例会引入噪声，且提示中标签顺序至关重要。模型在无视觉信息时易产生幻觉。微调表现出色，在e-SNLI-VE上达到83.3%准确率，并能生成高质量解释。然而，有限视觉下的BERTScore结果引发了对VE任务视觉接地性的质疑。研究最终揭示了VE作为诊断工具的效用与局限性，并为未来多模态评估方法改进提供了方向。", "keywords": "视觉蕴涵, 视觉-语言理解, 多模态模型, LLaMA, 解释性评估", "comments": "本研究通过对视觉蕴涵任务的深入分析，清晰地揭示了当前多模态语言模型在视觉-语言理解方面的优势与不足。特别值得注意的是，文章不仅关注了性能指标，还通过解释性评估深入探究了模型的推理过程，并指出了模型对语言先验的过度依赖以及视觉接地性的潜在问题，这对于理解和改进多模态模型具有重要意义。同时，对提示设计和上下文示例影响的分析也提供了实用的指导。"}}
{"id": "2507.17603", "title": "Citation Recommendation using Deep Canonical Correlation Analysis", "authors": ["Conor McNamara", "Effirul Ramlan"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      21 pages, 6 figures, 7 tables", "url": "http://arxiv.org/abs/2507.17603v1", "summary": "Recent advances in citation recommendation have improved accuracy by\nleveraging multi-view representation learning to integrate the various\nmodalities present in scholarly documents. However, effectively combining\nmultiple data views requires fusion techniques that can capture complementary\ninformation while preserving the unique characteristics of each modality. We\npropose a novel citation recommendation algorithm that improves upon linear\nCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a\nneural network extension capable of capturing complex, non-linear relationships\nbetween distributed textual and graph-based representations of scientific\narticles. Experiments on the large-scale DBLP (Digital Bibliography & Library\nProject) citation network dataset demonstrate that our approach outperforms\nstate-of-the-art CCA-based methods, achieving relative improvements of over 11%\nin Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These\ngains reflect more relevant citation recommendations and enhanced ranking\nquality, suggesting that DCCA's non-linear transformations yield more\nexpressive latent representations than CCA's linear projections.", "comment": "21 pages, 6 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.17603v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用深度典型相关分析的引文推荐", "tldr": "本文提出一种基于深度典型相关分析（DCCA）的引文推荐算法，通过捕获非线性关系，在大型数据集上显著优于现有CCA方法。", "motivation": "现有的引文推荐方法虽然利用多视图学习提高了准确性，但有效融合多模态数据需要能够捕获互补信息同时保留每种模态独特特征的融合技术。", "method": "提出一种新颖的引文推荐算法，通过应用深度典型相关分析（DCCA）改进线性典型相关分析（CCA）方法。DCCA是一种神经网络扩展，能够捕获科学文章的分布式文本和图表示之间复杂的非线性关系。", "result": "在大型DBLP引文网络数据集上的实验表明，该方法优于最先进的基于CCA的方法，在平均精度@10（MAP@10）上相对提高了11%以上，在精度@10上提高了5%，在召回率@10上提高了7%。", "conclusion": "这些改进反映了更相关的引文推荐和增强的排名质量，表明DCCA的非线性变换比CCA的线性投影产生了更具表达力的潜在表示。", "translation": "引文推荐的最新进展通过利用多视图表示学习来整合学术文档中存在的各种模态，从而提高了准确性。然而，有效结合多个数据视图需要能够捕获互补信息同时保留每种模态独特特征的融合技术。我们提出了一种新颖的引文推荐算法，通过应用深度典型相关分析（DCCA）改进了线性典型相关分析（CCA）方法。DCCA是一种神经网络扩展，能够捕获科学文章的分布式文本和基于图的表示之间复杂的非线性关系。在大型DBLP（数字文献与图书馆项目）引文网络数据集上的实验表明，我们的方法优于最先进的基于CCA的方法，在平均精度@10上相对提高了11%以上，在精度@10上提高了5%，在召回率@10上提高了7%。这些增益反映了更相关的引文推荐和增强的排名质量，表明DCCA的非线性变换比CCA的线性投影产生了更具表达力的潜在表示。", "summary": "本文提出了一种新颖的引文推荐算法，该算法通过将深度典型相关分析（DCCA）应用于学术文章的文本和图表示，以捕获复杂的非线性关系，从而改进了传统的线性典型相关分析（CCA）方法。在大型DBLP数据集上的实验结果表明，该方法在多项评估指标上显著优于现有的CCA基线方法，证明了DCCA在生成更具表达力的潜在表示和提高引文推荐质量方面的有效性。", "keywords": "引文推荐, 深度典型相关分析, 多视图学习, 非线性关系, DBLP", "comments": "本文的创新点在于将深度典型相关分析（DCCA）引入引文推荐领域，以解决传统CCA在处理多模态数据时难以捕获非线性关系的问题。通过利用DCCA的神经网络特性，该方法能够从文本和图数据中学习到更丰富的、非线性的潜在表示，从而显著提高了引文推荐的准确性和排名质量。这对于提升学术信息检索和发现的效率具有重要意义。"}}
{"id": "2507.04786", "title": "Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms", "authors": ["Zhiyi Hu", "Siyuan Shen", "Tommaso Bonato", "Sylvain Jeaugey", "Cedell Alexander", "Eric Spada", "James Dinan", "Jeff Hammond", "Torsten Hoefler"], "categories": ["cs.DC", "C.2"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04786v2", "summary": "The NVIDIA Collective Communication Library (NCCL) is a critical software\nlayer enabling high-performance collectives on large-scale GPU clusters.\nDespite being open source with a documented API, its internal design remains\nlargely opaque. The orchestration of communication channels, selection of\nprotocols, and handling of memory movement across devices and nodes are not\nwell understood, making it difficult to analyze performance or identify\nbottlenecks. This paper presents a comprehensive analysis of NCCL, focusing on\nits communication protocol variants (Simple, LL, and LL128), mechanisms\ngoverning intra-node and inter-node data movement, and ring- and tree-based\ncollective communication algorithms. The insights obtained from this study\nserve as the foundation for ATLAHS, an application-trace-driven network\nsimulation toolchain capable of accurately reproducing NCCL communication\npatterns in large-scale AI training workloads. By demystifying NCCL's internal\narchitecture, this work provides guidance for system researchers and\nperformance engineers working to optimize or simulate collective communication\nat scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04786v2", "cate": "cs.DC", "date": "2025-07-07", "updated": "2025-07-23", "AI": {"title_translation": "揭秘NCCL：GPU通信协议与算法的深入分析", "tldr": "NCCL是GPU集群高性能集体通信的关键，但其内部设计不透明。本文深入分析了NCCL的通信协议、数据移动机制和集体通信算法，为ATLAHS模拟工具奠定基础，旨在指导大规模AI训练中集体通信的优化与模拟。", "motivation": "NVIDIA集体通信库（NCCL）是实现大规模GPU集群高性能集体通信的关键软件层，但其内部设计（包括通信通道编排、协议选择和设备与节点间内存移动的处理）在很大程度上不透明，这使得分析性能或识别瓶颈变得困难。", "method": "本文对NCCL进行了全面分析，重点关注其通信协议变体（Simple、LL和LL128）、控制节点内和节点间数据移动的机制，以及基于环和树的集体通信算法。", "result": "从这项研究中获得的见解为ATLAHS（一个应用追踪驱动的网络模拟工具链）奠定了基础，该工具链能够准确地重现大规模AI训练工作负载中的NCCL通信模式。", "conclusion": "通过揭示NCCL的内部架构，这项工作为致力于优化或模拟大规模集体通信的系统研究人员和性能工程师提供了指导。", "translation": "NVIDIA集体通信库（NCCL）是实现大规模GPU集群高性能集体通信的关键软件层。尽管它是开源的并且拥有文档化的API，但其内部设计在很大程度上仍然不透明。通信通道的编排、协议的选择以及设备和节点间内存移动的处理方式尚不清楚，这使得分析性能或识别瓶颈变得困难。本文对NCCL进行了全面分析，重点关注其通信协议变体（Simple、LL和LL128）、控制节点内和节点间数据移动的机制，以及基于环和树的集体通信算法。从这项研究中获得的见解为ATLAHS奠定了基础，ATLAHS是一个应用追踪驱动的网络模拟工具链，能够准确地重现大规模AI训练工作负载中的NCCL通信模式。通过揭秘NCCL的内部架构，这项工作为致力于优化或模拟大规模集体通信的系统研究人员和性能工程师提供了指导。", "summary": "本文深入分析了NVIDIA集体通信库（NCCL）的内部设计，解决了其在高性能GPU集群中内部机制不透明的问题。研究详细探讨了NCCL的通信协议变体、节点内/节点间数据移动机制以及集体通信算法。这些分析为开发ATLAHS（一个能够准确模拟大规模AI训练中NCCL通信模式的工具链）提供了基础。最终，本研究旨在为系统研究人员和性能工程师优化和模拟大规模集体通信提供指导。", "keywords": "NCCL, GPU通信, 集体通信, 高性能计算, AI训练", "comments": "这篇论文通过深入剖析NCCL的内部机制，填补了其设计不透明的空白，对于理解和优化大规模GPU集群的通信性能具有重要意义。它不仅提供了理论分析，还为开发像ATLAHS这样的模拟工具提供了基础，具有很强的实践指导价值，对高性能计算和AI训练领域具有积极影响。"}}
{"id": "2507.17163", "title": "Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental Coupling via Independently Lockable Joints", "authors": ["Botao Lin", "Shuang Song", "Jiaole Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17163v1", "summary": "With a slender redundant body, the tendon-driven robot (TDR) has a large\nworkspace and great maneuverability while working in complex environments. TDR\ncomprises multiple independently controlled robot segments, each with a set of\ndriving tendons. While increasing the number of robot segments enhances\ndexterity and expands the workspace, this structural expansion also introduces\nintensified inter-segmental coupling. Therefore, achieving precise TDR control\nrequires more complex models and additional motors. This paper presents a\nreconfigurable tendon-driven robot (RTR) equipped with innovative lockable\njoints. Each joint's state (locked/free) can be individually controlled through\na pair of antagonistic tendons, and its structure eliminates the need for a\ncontinuous power supply to maintain the state. Operators can selectively\nactuate the targeted robot segments, and this scheme fundamentally eliminates\nthe inter-segmental coupling, thereby avoiding the requirement for complex\ncoordinated control between segments. The workspace of RTR has been simulated\nand compared with traditional TDRs' workspace, and RTR's advantages are further\nrevealed. The kinematics and statics models of the RTR have been derived and\nvalidation experiments have been conducted. Demonstrations have been performed\nusing a seven-joint RTR prototype to show its reconfigurability and moving\nability in complex environments with an actuator pack comprising only six\nmotors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17163v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "可重构肌腱驱动机器人：通过独立可锁定关节消除节间耦合", "tldr": "本文提出了一种新型可重构肌腱驱动机器人（RTR），通过创新的可锁定关节独立控制每个关节状态，从而从根本上消除了传统肌腱驱动机器人（TDR）的节间耦合问题，简化了控制并减少了所需电机数量。", "motivation": "传统肌腱驱动机器人（TDR）在增加机器人节段数量以提高灵巧性和扩展工作空间时，会加剧节间耦合，导致实现精确控制需要更复杂的模型和额外的电机。", "method": "本文提出了一种可重构肌腱驱动机器人（RTR），其配备了创新的可锁定关节。每个关节的状态（锁定/自由）可通过一对拮抗肌腱独立控制，且无需持续供电来维持状态。操作员可以选择性地驱动目标机器人节段，从而从根本上消除了节间耦合。", "result": "模拟并比较了RTR与传统TDR的工作空间，揭示了RTR的优势。推导了RTR的运动学和静力学模型，并进行了验证实验。使用七关节RTR原型进行了演示，展示了其在复杂环境中的可重构性和移动能力，且仅使用了六个电机。", "conclusion": "通过引入独立可锁定的关节，可重构肌腱驱动机器人（RTR）成功消除了传统肌腱驱动机器人（TDR）的节间耦合问题，简化了控制，并在减少所需驱动器的同时保持了良好的性能。", "translation": "具有细长冗余本体的肌腱驱动机器人（TDR）在复杂环境中工作时具有较大的工作空间和出色的机动性。TDR由多个独立控制的机器人节段组成，每个节段都有一组驱动肌腱。虽然增加机器人节段的数量可以增强灵巧性并扩展工作空间，但这种结构扩展也引入了加剧的节间耦合。因此，实现精确的TDR控制需要更复杂的模型和额外的电机。本文提出了一种配备创新可锁定关节的可重构肌腱驱动机器人（RTR）。每个关节的状态（锁定/自由）可以通过一对拮抗肌腱单独控制，并且其结构消除了维持状态所需的持续供电。操作员可以选择性地驱动目标机器人节段，这种方案从根本上消除了节间耦合，从而避免了节段间复杂协调控制的要求。RTR的工作空间已与传统TDR的工作空间进行了模拟和比较，进一步揭示了RTR的优势。RTR的运动学和静力学模型已被推导，并进行了验证实验。已使用一个七关节RTR原型进行了演示，以展示其在复杂环境中的可重构性和移动能力，该原型仅包含一个由六个电机组成的执行器组。", "summary": "本文介绍了一种可重构肌腱驱动机器人（RTR），旨在解决传统肌腱驱动机器人（TDR）在增加节段数量时出现的节间耦合问题。RTR通过引入创新的独立可锁定关节，允许操作员选择性地驱动特定节段，从而从根本上消除了节间耦合，简化了控制并减少了所需的电机数量。研究人员模拟并比较了RTR与传统TDR的工作空间，推导了其运动学和静力学模型，并通过一个七关节原型机进行了验证和演示，证明了其在复杂环境中的可重构性和移动能力。", "keywords": "可重构机器人, 肌腱驱动机器人, 节间耦合, 可锁定关节, 运动学", "comments": "这篇论文通过引入独立可锁定关节，巧妙地解决了传统肌腱驱动机器人（TDR）在多节段设计中普遍存在的节间耦合难题。这种创新不仅简化了机器人控制，还潜在地减少了驱动器需求，提高了系统的能效和实用性。其“无需持续供电维持锁定状态”的设计尤其值得称赞，体现了良好的工程实用性。这项工作为未来多自由度、高灵巧性机器人的设计提供了新的思路。"}}
{"id": "2506.16622", "title": "Modeling Public Perceptions of Science in Media", "authors": ["Jiaxin Pei", "Dustin Wright", "Isabelle Augenstein", "David Jurgens"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16622v2", "summary": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16622v2", "cate": "cs.CL", "date": "2025-06-19", "updated": "2025-07-22", "AI": {"title_translation": "媒体中科学公众认知的建模", "tldr": "本文提出了一个计算框架和NLP模型来预测公众对科学新闻的感知，并发现感知与公众参与度（评论和点赞）直接相关，而科学新闻消费频率是主要驱动因素。", "motivation": "科学传播者难以预测公众如何看待和互动科学新闻，而公众参与科学对于建立信任和理解至关重要。", "method": "1. 引入了一个计算框架，从12个维度（如新闻价值、重要性、惊奇度）建模公众感知。2. 基于该框架，创建了一个包含10,489条注释的大规模科学新闻感知数据集，数据来自2,101名美国和英国参与者。3. 开发了高性能的NLP模型来预测公众感知分数。4. 从两个角度研究感知：作为结果（影响因素）和作为预测因子（能否预测参与度）。5. 在Reddit上进行大规模分析和自然实验。", "result": "1. 个人科学新闻消费频率是感知的主要驱动因素，而人口统计因素影响甚微。2. 估计的公众科学信息感知与最终的参与模式直接相关。3. 具有更积极感知分数的帖子会获得显著更多的评论和点赞，这在不同科学信息和相同但不同框架的科学信息中均一致。", "conclusion": "这项研究强调了在科学传播中细致感知建模的重要性，为预测公众对科学内容的兴趣和参与提供了新途径。", "translation": "有效地让公众参与科学对于在我们的科学界培养信任和理解至关重要。然而，随着信息量不断增长，科学传播者难以预测受众将如何感知科学新闻并与之互动。在本文中，我们引入了一个计算框架，该框架从新闻价值、重要性和惊奇度等十二个维度建模公众感知。利用该框架，我们创建了一个大规模科学新闻感知数据集，其中包含来自美国和英国不同人群的2,101名参与者的10,489条注释，为公众对跨领域科学信息的反应提供了宝贵见解。我们进一步开发了自然语言处理（NLP）模型，这些模型能够以强大的性能预测公众感知分数。利用数据集和模型，我们从两个角度审视了公众对科学的感知：（1）感知作为结果：哪些因素影响公众对科学信息的感知？（2）感知作为预测因子：我们能否使用估计的感知来预测公众对科学的参与度？我们发现，个人科学新闻消费频率是感知的主要驱动因素，而人口统计因素影响甚微。更重要的是，通过在Reddit上进行大规模分析和精心设计的自然实验，我们证明了科学信息的估计公众感知与最终的参与模式有直接联系。具有更积极感知分数的帖子会获得显著更多的评论和和点赞，这在不同的科学信息以及相同但不同框架的科学信息中均一致。总的来说，这项研究强调了在科学传播中细致感知建模的重要性，为预测公众对科学内容的兴趣和参与提供了新途径。", "summary": "本文提出了一个计算框架和NLP模型，用于建模和预测公众对科学新闻的感知。研究构建了一个大型感知数据集，并发现个人科学新闻消费频率是影响感知的关键因素，而非人口统计学因素。通过Reddit上的实验，研究进一步证明了积极的公众感知能显著预测更高的公众参与度（评论和点赞）。这项工作强调了感知建模在科学传播中的重要性，为预测公众兴趣和参与提供了新方法。", "keywords": "公众感知, 科学传播, 自然语言处理, 数据集, 公众参与", "comments": "这项研究通过提出一个多维度的公众感知计算框架和构建大规模数据集，为科学传播领域提供了一个创新的量化分析工具。其重要性在于揭示了公众感知与参与度之间的直接联系，并识别出新闻消费频率这一关键驱动因素，这对于优化科学传播策略具有实际指导意义。特别是在Reddit上的自然实验，增强了研究结果的外部有效性。"}}
{"id": "2507.17528", "title": "Generalized Low-Rank Matrix Contextual Bandits with Graph Information", "authors": ["Yao Wang", "Jiannan Li", "Yue Kang", "Shanxing Gao", "Zhenxin Xiao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17528v1", "summary": "The matrix contextual bandit (CB), as an extension of the well-known\nmulti-armed bandit, is a powerful framework that has been widely applied in\nsequential decision-making scenarios involving low-rank structure. In many\nreal-world scenarios, such as online advertising and recommender systems,\nadditional graph information often exists beyond the low-rank structure, that\nis, the similar relationships among users/items can be naturally captured\nthrough the connectivity among nodes in the corresponding graphs. However,\nexisting matrix CB methods fail to explore such graph information, and thereby\nmaking them difficult to generate effective decision-making policies. To fill\nin this void, we propose in this paper a novel matrix CB algorithmic framework\nthat builds upon the classical upper confidence bound (UCB) framework. This new\nframework can effectively integrate both the low-rank structure and graph\ninformation in a unified manner. Specifically, it involves first solving a\njoint nuclear norm and matrix Laplacian regularization problem, followed by the\nimplementation of a graph-based generalized linear version of the UCB\nalgorithm. Rigorous theoretical analysis demonstrates that our procedure\noutperforms several popular alternatives in terms of cumulative regret bound,\nowing to the effective utilization of graph information. A series of synthetic\nand real-world data experiments are conducted to further illustrate the merits\nof our procedure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17528v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "广义低秩矩阵上下文赌博机与图信息", "tldr": "本文提出了一种新的矩阵上下文赌博机算法框架，该框架能有效整合低秩结构和图信息，解决了现有方法未能利用图信息的问题，并理论上证明了其在累积遗憾界方面的优越性。", "motivation": "现有的矩阵上下文赌博机方法未能有效利用现实世界场景中普遍存在的额外图信息（如用户/物品间的相似关系），导致决策策略不佳。", "method": "提出了一种基于经典UCB框架的新型矩阵上下文赌博机算法框架。该框架首先解决一个联合核范数和矩阵拉普拉斯正则化问题，然后实现一个基于图的广义线性UCB算法。", "result": "理论分析表明，该方法在累积遗憾界方面优于其他流行替代方案，这归因于对图信息的有效利用。通过合成数据和真实世界数据实验进一步验证了其优点。", "conclusion": "本文提出的算法框架成功地将低秩结构和图信息整合到矩阵上下文赌博机中，显著提高了决策效率和累积遗憾界性能。", "translation": "矩阵上下文赌博机（CB）作为多臂赌博机的一个扩展，是一个强大的框架，已广泛应用于涉及低秩结构的序贯决策场景。在许多现实世界场景中，例如在线广告和推荐系统，除了低秩结构之外，通常还存在额外的图信息，即用户/物品之间的相似关系可以通过相应图中节点之间的连通性自然地捕获。然而，现有的矩阵CB方法未能探索此类图信息，从而难以生成有效的决策策略。为了填补这一空白，本文提出了一种基于经典上置信界（UCB）框架的新型矩阵CB算法框架。这个新框架能够以统一的方式有效地整合低秩结构和图信息。具体来说，它首先涉及解决一个联合核范数和矩阵拉普拉斯正则化问题，然后实现一个基于图的广义线性UCB算法。严格的理论分析表明，由于图信息的有效利用，我们的过程在累积遗憾界方面优于几种流行的替代方案。通过一系列合成和真实世界数据实验进一步说明了我们过程的优点。", "summary": "本文针对现有矩阵上下文赌博机未能有效利用图信息的问题，提出了一种新型的矩阵上下文赌博机算法框架。该框架基于经典的UCB理论，巧妙地将低秩结构与图信息相结合，通过解决联合核范数和矩阵拉普拉斯正则化问题，并应用图基广义线性UCB算法来实现。理论分析和实验结果均表明，该方法在累积遗憾界方面表现出优越性。", "keywords": "矩阵上下文赌博机, 图信息, 低秩结构, 上置信界, 累积遗憾界", "comments": "该论文的创新之处在于首次将图信息有效整合到矩阵上下文赌博机框架中，解决了现有方法在处理具有复杂关系数据时的局限性。其提出的结合核范数和矩阵拉普拉斯正则化的方法，为利用结构化数据提供了新的思路。在在线广告和推荐系统等实际应用中，这一方法有望带来更精准的决策。"}}
{"id": "2507.14270", "title": "APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation", "authors": ["Ravin Kumar"], "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures, 1 table, and GitHub repository for the source code", "url": "http://arxiv.org/abs/2507.14270v2", "summary": "We propose the APTx Neuron, a novel, unified neural computation unit that\nintegrates non-linear activation and linear transformation into a single\ntrainable expression. The APTx Neuron is derived from the APTx activation\nfunction, thereby eliminating the need for separate activation layers and\nmaking the architecture both computationally efficient and elegant. The\nproposed neuron follows the functional form $y = \\sum_{i=1}^{n} ((\\alpha_i +\n\\tanh(\\beta_i x_i)) \\cdot \\gamma_i x_i) + \\delta$, where all parameters\n$\\alpha_i$, $\\beta_i$, $\\gamma_i$, and $\\delta$ are trainable. We validate our\nAPTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69%\ntest accuracy in just 20 epochs using approximately 332K trainable parameters.\nThe results highlight the superior expressiveness and computational efficiency\nof the APTx Neuron compared to traditional neurons, pointing toward a new\nparadigm in unified neuron design and the architectures built upon it.", "comment": "10 pages, 2 figures, 1 table, and GitHub repository for the source\n  code", "pdf_url": "http://arxiv.org/pdf/2507.14270v2", "cate": "cs.NE", "date": "2025-07-18", "updated": "2025-07-23", "AI": {"title_translation": "APTx 神经元：一种集成激活与计算的统一可训练神经元架构", "tldr": "APTx 神经元是一种新型统一神经计算单元，将非线性激活和线性变换集成到一个可训练表达式中，提高了计算效率和表达能力，并在 MNIST 数据集上取得了良好效果。", "motivation": "该研究旨在提出一种新型的统一神经计算单元，通过将非线性激活和线性变换集成到单个可训练表达式中，消除对单独激活层的需求，从而使神经网络架构更具计算效率和优雅性。", "method": "研究提出了 APTx 神经元，它源自 APTx 激活函数，其功能形式为 $y = \\sum_{i=1}^{n} ((\\alpha_i + \\tanh(\\beta_i x_i)) \\cdot \\gamma_i x_i) + \\delta$，其中所有参数 $\\alpha_i, \\beta_i, \\gamma_i, \\delta$ 都是可训练的。", "result": "APTx 神经元在 MNIST 数据集上验证，仅用约 332K 个可训练参数，在 20 个 epoch 内达到了高达 96.69% 的测试准确率。", "conclusion": "APTx 神经元与传统神经元相比，展现出卓越的表达能力和计算效率，这预示着统一神经元设计及其构建架构的新范式。", "translation": "我们提出了 APTx 神经元，这是一种新颖的统一神经计算单元，它将非线性激活和线性变换集成到一个可训练表达式中。APTx 神经元源自 APTx 激活函数，从而消除了对单独激活层的需求，使架构既计算高效又优雅。所提出的神经元遵循函数形式 $y = \\sum_{i=1}^{n} ((\\alpha_i + \\tanh(\\beta_i x_i)) \\cdot \\gamma_i x_i) + \\delta$，其中所有参数 $\\alpha_i, \\beta_i, \\gamma_i, \\delta$ 都是可训练的。我们在 MNIST 数据集上验证了基于 APTx 神经元的架构，仅使用大约 332K 个可训练参数，在 20 个 epoch 内实现了高达 96.69% 的测试准确率。结果突出显示了 APTx 神经元与传统神经元相比卓越的表达能力和计算效率，这预示着统一神经元设计及其构建架构的新范式。", "summary": "本论文提出了一种名为 APTx 神经元的新型统一神经计算单元，它将非线性激活和线性变换集成到一个可训练的表达式中，从而无需独立的激活层，提高了计算效率。该神经元具有可训练参数的特定函数形式。在 MNIST 数据集上的验证结果显示，APTx 神经元在少量训练周期和参数下能达到高准确率，证明其在表达能力和计算效率上优于传统神经元，为统一神经元设计开辟了新方向。", "keywords": "APTx 神经元, 统一神经元, 激活函数, 神经计算, 计算效率", "comments": "该论文的创新点在于提出了一个统一的神经元架构 APTx Neuron，将激活函数和线性变换融合，简化了网络设计并提升了计算效率。这可能为未来的神经网络设计提供一个新的思路，尤其是在资源受限的环境下具有潜在优势。"}}
{"id": "2504.19155", "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations", "authors": ["Hussein Harb", "Didier Benoit", "Axel Rannou", "Chi-Hieu Pham", "Valentin Tissot", "Bahaa Nasr", "Julien Bert"], "categories": ["physics.med-ph", "cs.AI"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "Comments:      15 pages, 8 figures", "url": "http://arxiv.org/abs/2504.19155v2", "summary": "To develop a machine learning-based framework for accurately modeling the\nanode heel effect in Monte Carlo simulations of X-ray imaging systems, enabling\nrealistic beam intensity profiles with minimal experimental calibration.\nMultiple regression models were trained to predict spatial intensity variations\nalong the anode-cathode axis using experimentally acquired weights derived from\nbeam measurements across different tube potentials. These weights captured the\nasymmetry introduced by the anode heel effect. A systematic fine-tuning\nprotocol was established to minimize the number of required measurements while\npreserving model accuracy. The models were implemented in the OpenGATE 10 and\nGGEMS Monte Carlo toolkits to evaluate their integration feasibility and\npredictive performance. Among the tested models, gradient boosting regression\n(GBR) delivered the highest accuracy, with prediction errors remaining below 5%\nacross all energy levels. The optimized fine-tuning strategy required only six\ndetector positions per energy level, reducing measurement effort by 65%. The\nmaximum error introduced through this fine-tuning process remained below 2%.\nDose actor comparisons within Monte Carlo simulations demonstrated that the\nGBR-based model closely replicated clinical beam profiles and significantly\noutperformed conventional symmetric beam models. This study presents a robust\nand generalizable method for incorporating the anode heel effect into Monte\nCarlo simulations using machine learning. By enabling accurate,\nenergy-dependent beam modeling with limited calibration data, the approach\nenhances simulation realism for applications in clinical dosimetry, image\nquality assessment, and radiation protection.", "comment": "15 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2504.19155v2", "cate": "physics.med-ph", "date": "2025-04-27", "updated": "2025-07-23", "AI": {"title_translation": "X射线束蒙特卡罗模拟中阳极跟效应的机器学习建模", "tldr": "研究开发了一种基于机器学习的方法，用于在X射线蒙特卡罗模拟中准确建模阳极跟效应，显著提高了模拟的真实性并减少了校准工作量。", "motivation": "旨在开发一个基于机器学习的框架，用于在X射线成像系统的蒙特卡罗模拟中准确建模阳极跟效应，从而获得真实的射线束强度分布，并最大程度地减少实验校准。", "method": "训练了多个回归模型，利用不同管电压下射线束测量获得的实验权重来预测沿阳极-阴极轴的空间强度变化。建立了系统的微调协议，以在保持模型准确性的同时，最小化所需的测量次数。模型在OpenGATE 10和GGEMS蒙特卡罗工具包中实施，以评估其集成可行性和预测性能。", "result": "梯度提升回归（GBR）模型提供了最高精度，所有能量水平的预测误差均保持在5%以下。优化的微调策略每个能量水平仅需六个探测器位置，测量工作量减少了65%，引入的最大误差低于2%。蒙特卡罗模拟中的剂量作用比较表明，基于GBR的模型能精确复现临床射线束剖面，并显著优于传统的对称射线束模型。", "conclusion": "本研究提出了一种鲁棒且可推广的基于机器学习将阳极跟效应纳入蒙特卡罗模拟的方法。通过以有限的校准数据实现准确、依赖能量的射线束建模，该方法增强了临床剂量学、图像质量评估和辐射防护应用中模拟的真实性。", "translation": "为了开发一个基于机器学习的框架，用于在X射线成像系统的蒙特卡罗模拟中准确建模阳极跟效应，从而实现真实的射线束强度分布，并最大程度地减少实验校准。\n训练了多个回归模型，利用从不同管电压下射线束测量中获得的实验权重来预测沿阳极-阴极轴的空间强度变化。这些权重捕捉了阳极跟效应引入的不对称性。建立了一个系统的微调协议，以在保持模型准确性的同时，最小化所需的测量次数。这些模型在OpenGATE 10和GGEMS蒙特卡罗工具包中实施，以评估其集成可行性和预测性能。\n在测试的模型中，梯度提升回归（GBR）提供了最高精度，所有能量水平的预测误差均保持在5%以下。优化的微调策略每个能量水平仅需六个探测器位置，测量工作量减少了65%。通过此微调过程引入的最大误差保持在2%以下。蒙特卡罗模拟中的剂量作用比较表明，基于GBR的模型能精确复现临床射线束剖面，并显著优于传统的对称射线束模型。\n本研究提出了一种鲁棒且可推广的基于机器学习将阳极跟效应纳入蒙特卡罗模拟的方法。通过以有限的校准数据实现准确、依赖能量的射线束建模，该方法增强了临床剂量学、图像质量评估和辐射防护应用中模拟的真实性。", "summary": "本文提出了一种创新的机器学习框架，用于在X射线蒙特卡罗模拟中精确建模阳极跟效应。通过训练回归模型（特别是梯度提升回归）来预测射线束强度变化，该方法显著减少了所需的实验校准数据量（减少65%），同时保持了高精度（误差低于5%）。该模型在OpenGATE和GGEMS中成功实现，并证明其能准确复制临床射线束剖面，优于传统模型，从而提高了临床剂量学、图像质量评估和辐射防护模拟的真实性。", "keywords": "阳极跟效应, 蒙特卡罗模拟, 机器学习, 梯度提升回归, X射线成像", "comments": "这项研究的创新之处在于将机器学习应用于X射线蒙特卡罗模拟中的阳极跟效应建模，显著减少了对大量实验校准数据的依赖，提高了模拟效率和真实性。其重要性在于为X射线成像系统中的剂量学、图像质量和辐射防护提供了更准确的模拟工具。梯度提升回归模型的应用及其在减少测量量方面的表现是亮点。"}}
{"id": "2507.17539", "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning", "authors": ["Xinyao Liu", "Diping Song"], "categories": ["cs.AI", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17539v1", "summary": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17539v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过临床认知链推理构建用于定位-诊断协作的眼科MLLM", "tldr": "本文提出了FundusExpert，一个眼科专用多模态大语言模型，通过结合定位与诊断推理，解决了眼科领域MLLM面临的碎片化标注和推理逻辑不一致问题，并在眼科问答和报告生成任务上取得了显著提升。", "motivation": "多模态大语言模型（MLLMs）在医学诊断领域潜力巨大，但在眼科等专业领域面临挑战，特别是标注粒度碎片化和临床推理逻辑不一致，这阻碍了精确的跨模态理解。", "method": "本文引入了FundusExpert，一个集成定位-诊断推理能力的眼科专用MLLM，以及FundusGen，一个通过智能Fundus-Engine系统构建的数据集。Fundus-Engine自动化定位并利用MLLM语义扩展来整合全局疾病分类、局部目标检测和细粒度特征分析。此外，通过构建临床对齐的认知链，指导模型生成可解释的推理路径。FundusExpert使用FundusGen的指令数据进行微调。", "result": "FundusExpert在眼科问答任务中表现最佳，超越40B MedRegA平均准确率26.6%；在零样本报告生成任务中临床一致性达77.0%，显著优于GPT-4o的47.6%。研究揭示了数据质量与模型能力之间的缩放定律（L ∝ N^0.068），表明FundusGen中的认知对齐标注提高了数据利用效率。", "conclusion": "通过将区域级定位与诊断推理链集成，本文开发了一个可扩展、临床对齐的MLLM，并探索了弥合特定MLLM中视觉-语言鸿沟的途径。", "translation": "多模态大语言模型（MLLMs）在医学诊断领域展现出巨大潜力。然而，它们在眼科等专业领域面临严峻挑战，特别是标注粒度碎片化和临床推理逻辑不一致，这阻碍了精确的跨模态理解。本文介绍了FundusExpert，一个集成定位-诊断推理能力的眼科专用MLLM，以及FundusGen，一个通过智能Fundus-Engine系统构建的数据集。Fundus-Engine自动化定位并利用基于MLLM的语义扩展，将全局疾病分类、局部目标检测和细粒度特征分析整合到单个眼底图像中。此外，通过构建一个临床对齐的认知链，它指导模型生成可解释的推理路径。FundusExpert使用FundusGen的指令数据进行微调，在眼科问答任务中取得了最佳性能，超越了40B MedRegA的平均准确率26.6%。它在零样本报告生成任务中也表现出色，临床一致性达到77.0%，显著优于GPT-4o的47.6%。此外，我们揭示了数据质量与模型能力之间的缩放定律（L ∝ N^0.068），表明FundusGen中的认知对齐标注提高了数据利用效率。通过将区域级定位与诊断推理链集成，我们的工作开发了一个可扩展、临床对齐的MLLM，并探索了弥合特定MLLM中视觉-语言鸿沟的途径。我们的项目可在 https://github.com/MeteorElf/FundusExpert 找到。", "summary": "本文提出了FundusExpert，一个专为眼科设计的MLLM，旨在通过整合定位与诊断推理来克服眼科领域MLLM面临的挑战。该模型利用FundusGen数据集（由Fundus-Engine构建，包含自动化定位和语义扩展）进行训练，并通过构建临床认知链来生成可解释的推理路径。实验结果表明，FundusExpert在眼科问答和报告生成任务中均取得了显著优于现有模型的性能，并揭示了数据质量对模型能力的关键影响，证明了其在弥合视觉-语言鸿沟方面的潜力。", "keywords": "眼科MLLM, 定位-诊断协作, 临床认知链, FundusExpert, FundusGen", "comments": "该研究的创新点在于构建了一个结合定位与诊断推理的眼科专用MLLM，并通过引入临床认知链来提高模型的可解释性。FundusGen数据集的构建方法（自动化定位和语义扩展）以及对数据质量与模型能力缩放定律的发现，为特定领域MLLM的开发提供了有价值的见解。这对于提升医疗AI在专业领域的应用精度和临床实用性具有重要意义。"}}
{"id": "2507.17182", "title": "Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment", "authors": ["Linghe Meng", "Jiarun Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17182v1", "summary": "The quality assessment of AI-generated content (AIGC) faces multi-dimensional\nchallenges, that span from low-level visual perception to high-level semantic\nunderstanding. Existing methods generally rely on single-level visual features,\nlimiting their ability to capture complex distortions in AIGC images. To\naddress this limitation, a multi-level visual representation paradigm is\nproposed with three stages, namely multi-level feature extraction, hierarchical\nfusion, and joint aggregation. Based on this paradigm, two networks are\ndeveloped. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)\nis designed for the perceptual quality assessment, extracting complementary\nlocal and global features via dual CNN and Transformer visual backbones. The\nMulti-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image\ncorrespondence by embedding prompt semantics into the visual feature fusion\nprocess at each feature level. The fused multi-level features are then\naggregated for final evaluation. Experiments on benchmarks demonstrate\noutstanding performance on both tasks, validating the effectiveness of the\nproposed multi-level visual assessment paradigm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17182v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "分层融合与联合聚合：一种用于AIGC图像质量评估的多级特征表示方法", "tldr": "本文提出了一种多级视觉表示范式和两种网络（MGLF-Net和MPEF-Net），用于解决AIGC图像质量评估中现有方法依赖单一视觉特征的局限性，并在基准测试中取得了优异性能。", "motivation": "AI生成内容（AIGC）的质量评估面临从低级视觉感知到高级语义理解的多维挑战。现有方法通常依赖单一级别的视觉特征，这限制了它们捕捉AIGC图像中复杂失真的能力。", "method": "提出了一种包含多级特征提取、分层融合和联合聚合三个阶段的多级视觉表示范式。基于此范式，开发了两种网络：1. 多级全局-局部融合网络（MGLF-Net），通过双CNN和Transformer视觉骨干提取互补的局部和全局特征，用于感知质量评估。2. 多级提示嵌入融合网络（MPEF-Net），通过在每个特征级别将提示语义嵌入到视觉特征融合过程中，针对文本到图像的一致性评估。融合后的多级特征会进行联合聚合以进行最终评估。", "result": "在基准测试中，所提出的方法在感知质量评估和文本到图像一致性评估两项任务上均表现出卓越的性能。", "conclusion": "提出的多级视觉评估范式及其开发的网络（MGLF-Net和MPEF-Net）能够有效解决AIGC图像质量评估中的多维挑战，并显著提升评估性能。", "translation": "人工智能生成内容（AIGC）的质量评估面临从低级视觉感知到高级语义理解的多维挑战。现有方法通常依赖单一级别的视觉特征，这限制了它们捕捉AIGC图像中复杂失真的能力。为了解决这一限制，本文提出了一种多级视觉表示范式，包含三个阶段：多级特征提取、分层融合和联合聚合。基于此范式，开发了两种网络。具体来说，多级全局-局部融合网络（MGLF-Net）旨在进行感知质量评估，通过双CNN和Transformer视觉骨干提取互补的局部和全局特征。多级提示嵌入融合网络（MPEF-Net）通过在每个特征级别将提示语义嵌入到视觉特征融合过程中，以实现文本到图像的对应性评估。然后，融合后的多级特征被聚合以进行最终评估。在基准测试上的实验表明，在两项任务上均表现出卓越的性能，验证了所提出的多级视觉评估范式的有效性。", "summary": "本文针对AIGC图像质量评估中现有方法依赖单一视觉特征的局限性，提出了一种多级视觉表示范式。该范式包含多级特征提取、分层融合和联合聚合三个阶段。基于此，论文开发了MGLF-Net用于感知质量评估，通过CNN和Transformer融合全局与局部特征；以及MPEF-Net用于文本-图像对应性评估，通过嵌入提示语义进行多级特征融合。实验结果表明，该多级范式在两项任务上均表现出色，验证了其有效性。", "keywords": "AIGC图像质量评估, 多级特征表示, 分层融合, 联合聚合, MGLF-Net, MPEF-Net", "comments": "该论文通过引入多级特征表示和分层融合的范式，有效解决了AIGC图像质量评估中复杂失真难以捕捉的问题，尤其创新性地结合了全局-局部特征融合和提示语义嵌入，提升了评估的全面性和准确性。其双网络设计针对不同评估维度，具有很强的实用价值。"}}
{"id": "2507.17367", "title": "Exploring Spatial Diversity for Region-based Active Learning", "authors": ["Lile Cai", "Xun Xu", "Lining Zhang", "Chuan-Sheng Foo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      published in IEEE Transactions on Image Processing, 2021", "url": "http://arxiv.org/abs/2507.17367v1", "summary": "State-of-the-art methods for semantic segmentation are based on deep neural\nnetworks trained on large-scale labeled datasets. Acquiring such datasets would\nincur large annotation costs, especially for dense pixel-level prediction tasks\nlike semantic segmentation. We consider region-based active learning as a\nstrategy to reduce annotation costs while maintaining high performance. In this\nsetting, batches of informative image regions instead of entire images are\nselected for labeling. Importantly, we propose that enforcing local spatial\ndiversity is beneficial for active learning in this case, and to incorporate\nspatial diversity along with the traditional active selection criterion, e.g.,\ndata sample uncertainty, in a unified optimization framework for region-based\nactive learning. We apply this framework to the Cityscapes and PASCAL VOC\ndatasets and demonstrate that the inclusion of spatial diversity effectively\nimproves the performance of uncertainty-based and feature diversity-based\nactive learning methods. Our framework achieves $95\\%$ performance of fully\nsupervised methods with only $5-9\\%$ of the labeled pixels, outperforming all\nstate-of-the-art region-based active learning methods for semantic\nsegmentation.", "comment": "published in IEEE Transactions on Image Processing, 2021", "pdf_url": "http://arxiv.org/pdf/2507.17367v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "探索基于区域的主动学习中的空间多样性", "tldr": "本文提出了一种新的基于区域的主动学习框架，通过结合空间多样性和传统选择标准，显著降低了语义分割的数据标注成本，同时保持了高精度，超越了现有方法。", "motivation": "语义分割等像素级预测任务需要大量标注数据，导致高昂的标注成本。为了降低成本，同时保持高性能，需要一种有效的策略。", "method": "本文提出了一种基于区域的主动学习策略，选择信息丰富的图像区域进行标注。关键在于，提出了在统一的优化框架中，将局部空间多样性与传统主动选择标准（如数据样本不确定性）相结合。该框架应用于Cityscapes和PASCAL VOC数据集。", "result": "实验表明，引入空间多样性有效提升了基于不确定性和基于特征多样性的主动学习方法的性能。该框架仅用5-9%的标注像素就达到了完全监督方法95%的性能，并且超越了所有最先进的基于区域的语义分割主动学习方法。", "conclusion": "将空间多样性纳入基于区域的主动学习框架中，可以显著减少语义分割任务的标注成本，同时保持甚至超越现有最先进方法的性能。", "translation": "最先进的语义分割方法基于在大型标记数据集上训练的深度神经网络。获取此类数据集会产生巨大的标注成本，特别是对于像语义分割这种密集的像素级预测任务。我们考虑将基于区域的主动学习作为一种策略来降低标注成本，同时保持高性能。在这种设置下，选择批量的、信息丰富的图像区域而不是整个图像进行标注。重要的是，我们提出在这种情况下，强制执行局部空间多样性对主动学习是有益的，并在一个统一的优化框架中将空间多样性与传统的主动选择标准（例如，数据样本不确定性）相结合，用于基于区域的主动学习。我们将此框架应用于Cityscapes和PASCAL VOC数据集，并证明包含空间多样性有效地提高了基于不确定性和基于特征多样性主动学习方法的性能。我们的框架仅用5-9%的标记像素就达到了完全监督方法95%的性能，超越了所有最先进的基于区域的语义分割主动学习方法。", "summary": "该研究针对语义分割任务中高昂的数据标注成本问题，提出了一种基于区域的主动学习新框架。该框架创新性地将局部空间多样性与传统的主动学习选择标准（如不确定性）融合在一个统一的优化模型中，以选择最具信息量的图像区域进行标注。在Cityscapes和PASCAL VOC数据集上的实验证明，该方法显著提升了主动学习的效率，仅使用极少量的标注像素（5-9%）即可达到接近完全监督的性能（95%），并超越了现有所有基于区域的主动学习方法。", "keywords": "主动学习, 语义分割, 空间多样性, 区域选择, 标注成本", "comments": "本文的创新点在于首次将局部空间多样性引入基于区域的主动学习框架中，解决了传统主动学习方法可能忽视空间信息的问题。通过结合不确定性采样和空间多样性，有效提升了数据采样的效率和质量，对于降低语义分割等密集预测任务的标注成本具有重要意义，展现了其在实际应用中的巨大潜力。"}}
{"id": "2507.16938", "title": "A parameterized block-splitting preconditioner for indefinite least squares problem", "authors": ["Davod Khojasteh Salkuyeh"], "categories": ["math.NA", "cs.NA", "65F10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      13 pages, 1 Figure", "url": "http://arxiv.org/abs/2507.16938v1", "summary": "We present a stationary iteration based upon a block splitting for a class of\nindefinite least squares problem. Convergence of the proposed method is\ninvestigated and optimal value of the involving parameter is used. The induced\npreconditioner is applied to accelerate the convergence of the GMRES method for\nsolving the problem. We also analysed the eigenpair distribution of the\npreconditioned matrix. Some numerical are presented to show the effectiveness\nof the preconditioner. Numerical comparison with other well-known methods are\nalso presented.", "comment": "13 pages, 1 Figure", "pdf_url": "http://arxiv.org/pdf/2507.16938v1", "cate": "math.NA", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "不定最小二乘问题的参数化块分裂预处理器", "tldr": "本文提出一种基于块分裂的平稳迭代方法，用于解决不定最小二乘问题，并将其作为预处理器加速GMRES方法的收敛，数值结果表明其有效性。", "motivation": "解决不定最小二乘问题，并加速其求解方法的收敛性。", "method": "提出一种基于块分裂的平稳迭代方法，并研究其收敛性及参数优化。将该方法作为预处理器应用于GMRES方法以加速收敛。分析预处理矩阵的特征对分布。", "result": "数值结果表明了所提出预处理器的有效性，并与其它已知方法进行了数值比较。", "conclusion": "所提出的参数化块分裂预处理器能够有效加速不定最小二乘问题求解的GMRES方法的收敛。", "translation": "我们提出了一种基于块分裂的平稳迭代方法，用于一类不定最小二乘问题。研究了所提出方法的收敛性，并使用了相关参数的最优值。将所产生的预处理器应用于加速GMRES方法求解问题的收敛。我们还分析了预处理矩阵的特征对分布。提出了一些数值结果以表明该预处理器的有效性。还给出了与其他知名方法的数值比较。", "summary": "本文针对一类不定最小二乘问题，提出了一种基于块分裂的平稳迭代方法。研究了该方法的收敛性并确定了最优参数。该方法被用作预处理器，以加速GMRES方法求解问题的收敛。研究还分析了预处理矩阵的特征对分布，并通过数值实验证明了该预处理器的有效性，并与其他现有方法进行了比较。", "keywords": "块分裂, 预处理器, 不定最小二乘, GMRES, 收敛性", "comments": "这篇论文提出了一种新的参数化块分裂预处理器，用于解决不定最小二乘问题，其创新点在于将块分裂平稳迭代方法用作预处理器以加速GMRES算法的收敛，并通过分析特征对分布和数值比较验证了其有效性。"}}
{"id": "2506.08274", "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks", "authors": ["João Manoel Herrera Pinheiro", "Suzana Vilas Boas de Oliveira", "Thiago Henrique Segreto Silva", "Pedro Antonio Rabelo Saraiva", "Enzo Ferreira de Souza", "Ricardo V. Godoy", "Leonardo André Ambrosio", "Marcelo Becker"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      27 pages", "url": "http://arxiv.org/abs/2506.08274v3", "summary": "This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2506.08274v3", "cate": "cs.LG", "date": "2025-06-09", "updated": "2025-07-23", "AI": {"title_translation": "特征缩放对机器学习的影响：回归和分类任务中的效应", "tldr": "本研究系统评估了12种特征缩放技术对14种机器学习算法在分类和回归任务中的影响。发现集成方法（如随机森林、XGBoost）对缩放不敏感，而其他模型（如逻辑回归、SVM）则高度依赖于所选缩放器。研究提供了模型特定的指导。", "motivation": "现有研究缺乏对特征缩放的全面评估，本研究旨在系统评估其对机器学习算法在回归和分类任务中性能和计算成本的影响。", "method": "系统评估了12种特征缩放技术，涵盖了14种不同的机器学习算法和16个数据集，用于分类和回归任务。分析了预测性能（准确率、MAE、MSE、$R^2$）和计算成本（训练时间、推理时间、内存使用）。所有源代码、实验结果和模型参数均公开。", "result": "主要发现是：集成方法（如随机森林、XGBoost、CatBoost、LightGBM）的性能基本不受特征缩放影响；而其他广泛使用的模型（如逻辑回归、SVM、TabNet、MLP）的性能则显著依赖于所选的缩放器。", "conclusion": "这项广泛的实证分析为实践者提供了关于最佳选择特征缩放技术的模型特定关键指导。", "translation": "这项研究通过系统评估12种缩放技术（包括几种不常见的转换），在14种不同的机器学习算法和16个数据集上，针对分类和回归任务，解决了特征缩放综合研究的严重不足。我们细致地分析了对预测性能（使用准确率、MAE、MSE和$R^2$等指标）和计算成本（训练时间、推理时间和内存使用）的影响。关键发现表明，虽然集成方法（如随机森林和梯度提升模型，如XGBoost、CatBoost和LightGBM）表现出稳健的性能，基本上独立于缩放，但其他广泛使用的模型，如逻辑回归、支持向量机、TabNet和多层感知器，表现出显著的性能变化，高度依赖于所选的缩放器。这项广泛的实证分析，其所有源代码、实验结果和模型参数均公开可用，以确保完全的透明度和可复现性，为实践者提供了关于最佳选择特征缩放技术的模型特定关键指导。", "summary": "本研究全面探讨了特征缩放对机器学习模型性能和计算成本的影响。通过对12种缩放技术、14种算法和16个数据集的广泛实验，发现集成方法对特征缩放不敏感，而逻辑回归、SVM等模型则对其高度依赖。研究提供了模型特定的指南，强调了选择最佳缩放技术的重要性，并公开了所有实验数据以确保可复现性。", "keywords": "特征缩放, 机器学习, 回归, 分类, 集成方法", "comments": "这项研究的创新之处在于其对特征缩放进行了迄今为止最为系统和全面的评估，涵盖了广泛的缩放技术、机器学习算法和数据集。其重要性在于为机器学习实践者提供了有价值的、模型特定的指导，帮助他们在实际应用中做出明智的特征缩放选择。透明地公开所有实验数据和代码也极大地增强了研究的可信度和可复现性。"}}
{"id": "2507.17016", "title": "Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting", "authors": ["Omid Orang", "Patricia O. Lucas", "Gabriel I. F. Paiva", "Petronio C. L. Silva", "Felipe Augusto Rocha da Silva", "Adriano Alonso Veloso", "Frederico Gadelha Guimaraes"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the Brazilian Congress of Artificial Intelligence (CBIC)", "url": "http://arxiv.org/abs/2507.17016v1", "summary": "In recent years, the application of Large Language Models (LLMs) to time\nseries forecasting (TSF) has garnered significant attention among researchers.\nThis study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with\nfuzzy time series (FTS) and causal graph to predict multivariate time series,\nmarking the first such architecture in the literature. The key objective is to\nconvert numerical time series into interpretable forms through the parallel\napplication of fuzzification and causal analysis, enabling both semantic\nunderstanding and structural insight as input for the pretrained GPT-2 model.\nThe resulting textual representation offers a more interpretable view of the\ncomplex dynamics underlying the original time series. The reported results\nconfirm the effectiveness of our proposed LLM-based time series forecasting\nmodel, as demonstrated across four different multivariate time series datasets.\nThis initiative paves promising future directions in the domain of TSF using\nLLMs based on FTS.", "comment": "Accepted for publication at the Brazilian Congress of Artificial\n  Intelligence (CBIC)", "pdf_url": "http://arxiv.org/pdf/2507.17016v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "因果图模糊大语言模型：首次介绍及其在时间序列预测中的应用", "tldr": "本文首次提出CGF-LLM框架，结合GPT-2、模糊时间序列和因果图，用于多元时间序列预测，并证实其有效性。", "motivation": "近年来大语言模型（LLMs）在时间序列预测（TSF）中受到广泛关注。然而，如何将数值时间序列转化为LLMs可理解的、同时包含语义和结构洞察的形式，是当前研究面临的挑战。", "method": "本研究提出了一种名为CGF-LLM的新型LLM框架。该框架结合了预训练的GPT-2模型、模糊时间序列（FTS）和因果图。其核心方法是通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的文本表示，以此作为GPT-2模型的输入，从而实现对复杂时间序列动态的语义理解和结构洞察。", "result": "所提出的CGF-LLM模型在四个不同的多元时间序列数据集上进行了评估，结果证实了其在时间序列预测方面的有效性。", "conclusion": "CGF-LLM框架为使用基于模糊时间序列的大语言模型进行时间序列预测开辟了有前景的未来方向。", "translation": "近年来，大型语言模型（LLMs）在时间序列预测（TSF）中的应用引起了研究人员的广泛关注。本研究提出了一种新的LLM框架，名为CGF-LLM，它结合了GPT-2、模糊时间序列（FTS）和因果图来预测多元时间序列，标志着文献中首次出现此类架构。其主要目标是通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的形式，从而使预训练的GPT-2模型能够同时获得语义理解和结构洞察作为输入。所产生的文本表示提供了对原始时间序列背后复杂动态的更具解释性的视角。报告的结果证实了我们提出的基于LLM的时间序列预测模型的有效性，这在四个不同的多元时间序列数据集上得到了证明。这项举措为使用基于FTS的LLM进行TSF领域开辟了有前景的未来方向。", "summary": "本文首次提出了一种名为CGF-LLM的新型大语言模型框架，该框架创新性地结合了GPT-2、模糊时间序列（FTS）和因果图，专用于多元时间序列预测。其核心思想在于通过模糊化和因果分析，将数值时间序列转化为可解释的文本形式，以同时捕获数据的语义和结构信息，作为预训练GPT-2模型的输入。实验结果表明，该模型在多个不同的多元时间序列数据集上均能有效进行预测，为LLM在时间序列预测领域的应用开辟了新的研究方向。", "keywords": "大语言模型, 时间序列预测, 模糊时间序列, 因果图, GPT-2", "comments": "这项研究的创新之处在于首次将因果图和模糊时间序列的概念与大型语言模型（如GPT-2）相结合，用于时间序列预测。通过将数值时间序列转化为可解释的文本表示，该方法不仅利用了LLM强大的语义理解能力，还融入了时间序列的内在结构和因果关系，这为处理和理解复杂时间序列数据提供了一个新颖且有前景的范式。其重要性在于拓展了LLM在非传统NLP领域，特别是时间序列分析中的应用边界，并可能为未来更具解释性和鲁棒性的时间序列预测模型奠定基础。"}}
{"id": "2503.04498", "title": "Equivalence of Families of Polycyclic Codes over Finite Fields", "authors": ["Hassan Ou-azzou", "Anna-Lena Horlemann"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04498v2", "summary": "We study the equivalence of families of polycyclic codes associated with\npolynomials of the form $x^n - a_{n-1}x^{n-1} - \\ldots - a_1x - a_0$ over a\nfinite field. We begin with the specific case of polycyclic codes associated\nwith a trinomial $x^n - a_{\\ell} x^{\\ell} - a_0$ (for some $0< \\ell <n$), which\nwe refer to as \\textit{$\\ell$-trinomial codes}, after which we generalize our\nresults to general polycyclic codes. We introduce an equivalence relation\ncalled \\textit{$n$-equivalence}, which extends the known notion of\n$n$-equivalence for constacyclic codes \\cite{Chen2014}. We compute the number\nof $n$-equivalence classes %, $ N_{(n,\\ell)}$, for this relation and provide\nconditions under which two families of polycyclic (or $\\ell$-trinomial) codes\nare equivalent. In particular, we prove that when $\\gcd(n, n-\\ell) = 1$, any\n$\\ell$-trinomial code family is equivalent to a trinomial code family\nassociated with the polynomial $x^n - x^{\\ell} - 1$. Finally, we focus on\n$p^{\\ell}$-trinomial codes of length $p^{\\ell+r}$, where $p$ is the\ncharacteristic of $\\mathbb{F}_q$ and $r$ an integer, and provide some examples\nas an application of the theory developed in this paper.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04498v2", "cate": "cs.IT", "date": "2025-03-06", "updated": "2025-07-23", "AI": {"title_translation": "有限域上多环码族的等价性", "tldr": "本文研究了有限域上与特定多项式相关的多环码族的等价性，引入了扩展的$n$-等价关系，并给出了码族等价的条件，包括在特定条件下$\\\\ell$-三项式码族的等价性，并提供了应用实例。", "motivation": "本文旨在研究有限域上与特定形式多项式（特别是三项式和一般多项式）相关的多环码族之间的等价性，以深入理解这些码族的结构和相互关系。", "method": "研究首先从与三项式$x^n - a_{\\\\ell} x^{\\\\ell} - a_0$相关的\\\\ell-三项式码入手，然后将结果推广到一般的多环码。核心方法是引入并扩展了常循环码的$n$-等价概念，计算了$n$-等价类的数量，并推导了两个多环（或\\\\ell-三项式）码族等价的条件。此外，还特别证明了在特定条件下\\\\ell-三项式码族的等价性，并通过具体例子验证了理论。", "result": "引入并扩展了适用于多环码的$n$-等价关系。计算了$n$-等价类的数量。提供了两个多环（或\\\\ell-三项式）码族等价的条件。证明了当$\\\\gcd(n, n-\\\\ell) = 1$时，任何\\\\ell-三项式码族都等价于与多项式$x^n - x^{\\\\ell} - 1$相关联的三项式码族。提供了$p^{\\\\ell}$-三项式码的例子作为理论应用。", "conclusion": "本文成功研究了有限域上多环码族的等价性，通过引入并详细分析$n$-等价关系，提供了码族等价的明确条件，并特别证明了在特定条件下\\\\ell-三项式码族的等价性，为多环码的分类和理解提供了重要的理论基础和应用示例。", "translation": "我们研究了有限域上与形如$x^n - a_{n-1}x^{n-1} - \\\\ldots - a_1x - a_0$的多项式相关的多环码族的等价性。我们首先从与三项式$x^n - a_{\\\\ell} x^{\\\\ell} - a_0$（对于某个$0< \\\\ell <n$）相关的多环码这一具体情况开始，我们称之为\\\\ell-三项式码，之后将我们的结果推广到一般的多环码。我们引入了一种称为$n$-等价的等价关系，它扩展了已知的常循环码的$n$-等价概念 \\\\cite{Chen2014}。我们计算了这种关系的$n$-等价类的数量，并提供了两个多环（或\\\\ell-三项式）码族等价的条件。特别是，我们证明了当$\\\\gcd(n, n-\\\\ell) = 1$时，任何\\\\ell-三项式码族都等价于与多项式$x^n - x^{\\\\ell} - 1$相关联的三项式码族。最后，我们关注了长度为$p^{\\\\ell+r}$的$p^{\\\\ell}$-三项式码，其中$p$是$\\\\mathbb{F}_q$的特征，$r$是一个整数，并提供了一些例子作为本文所发展理论的应用。", "summary": "本文深入探讨了有限域上多环码族的等价性。研究始于对特定三项式相关的\\\\ell-三项式码的分析，随后将所得结论推广至更一般的多环码。核心贡献在于引入并详尽阐述了一种名为$n$-等价的扩展关系，并据此计算了等价类的数量，给出了判断码族等价的充要条件。一个关键发现是，当$\\\\gcd(n, n-\\\\ell) = 1$时，任何\\\\ell-三项式码族均等价于与多项式$x^n - x^{\\\\ell} - 1$相关的三项式码族。论文最后通过对$p^{\\\\ell}$-三项式码的讨论，展示了理论的实际应用。", "keywords": "多环码, 等价性, 有限域, $n$-等价, 三项式码", "comments": "本文的创新之处在于将$n$-等价的概念从常循环码推广到更广泛的多环码，并提供了具体的等价判定条件。特别是关于三项式码族的等价性证明，对于理解这类码的结构和分类具有重要理论价值。通过提供具体例子，论文也展示了其理论的实用性和潜在的应用前景。"}}
{"id": "2507.17638", "title": "Learning clusters of partially observed linear dynamical systems", "authors": ["Maryann Rui", "Munther A. Dahleh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This is an extended and updated version of our paper presented at the 2025 American Control Conference (ACC)", "url": "http://arxiv.org/abs/2507.17638v1", "summary": "We study the problem of learning clusters of partially observed linear\ndynamical systems from multiple input-output trajectories. This setting is\nparticularly relevant when there are limited observations (e.g., short\ntrajectories) from individual data sources, making direct estimation\nchallenging. In such cases, incorporating data from multiple related sources\ncan improve learning. We propose an estimation algorithm that leverages\ndifferent data requirements for the tasks of clustering and system\nidentification. First, short impulse responses are estimated from individual\ntrajectories and clustered. Then, refined models for each cluster are jointly\nestimated using multiple trajectories. We establish end-to-end finite sample\nguarantees for estimating Markov parameters and state space realizations and\nhighlight trade-offs among the number of observed systems, the trajectory\nlengths, and the complexity of the underlying models.", "comment": "This is an extended and updated version of our paper presented at the\n  2025 American Control Conference (ACC)", "pdf_url": "http://arxiv.org/pdf/2507.17638v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "学习部分观测线性动态系统的聚类", "tldr": "本文提出了一种从多条短轨迹中学习部分观测线性动态系统聚类的方法，并通过两阶段估计过程实现了有限样本保证。", "motivation": "当单个数据源的观测数据有限（例如短轨迹）时，直接估计线性动态系统模型变得困难。在这种情况下，整合来自多个相关数据源的数据可以提高学习效果。", "method": "提出了一种估计算法，该算法利用聚类和系统辨识任务的不同数据要求。首先，从单独轨迹中估计短脉冲响应并进行聚类。然后，使用多条轨迹联合估计每个聚类的精炼模型。", "result": "建立了估计马尔可夫参数和状态空间实现的端到端有限样本保证，并强调了观测系统数量、轨迹长度和底层模型复杂性之间的权衡。", "conclusion": "本文成功地为部分观测线性动态系统的聚类学习提供了一种有效的两阶段估计方法，并在有限观测数据下实现了理论保证。", "translation": "我们研究了从多个输入-输出轨迹中学习部分观测线性动态系统聚类的问题。当单个数据源的观测数据有限（例如短轨迹），使得直接估计具有挑战性时，这种设置尤为相关。在这种情况下，整合来自多个相关数据源的数据可以改善学习。我们提出了一种估计算法，该算法利用聚类和系统辨识任务的不同数据要求。首先，从单独轨迹中估计短脉冲响应并进行聚类。然后，使用多条轨迹联合估计每个聚类的精炼模型。我们建立了估计马尔可夫参数和状态空间实现的端到端有限样本保证，并强调了观测系统数量、轨迹长度和底层模型复杂性之间的权衡。", "summary": "本文研究了从多条输入-输出轨迹中学习部分观测线性动态系统聚类的问题。针对单个数据源观测有限（如短轨迹）导致直接估计困难的情况，作者提出了一种两阶段估计算法。该算法首先从单独轨迹中估计并聚类短脉冲响应，然后利用多条轨迹联合估计每个聚类的精炼模型。研究建立了估计马尔可夫参数和状态空间实现的端到端有限样本保证，并探讨了观测系统数量、轨迹长度和模型复杂性之间的权衡。", "keywords": "线性动态系统, 聚类, 部分观测, 系统辨识, 有限样本保证", "comments": "这篇论文解决了在数据稀疏情况下学习线性动态系统聚类的实际难题。其创新点在于提出了一个两阶段的估计方法，巧妙地结合了聚类和系统辨识，并提供了理论上的有限样本保证，这对于实际应用具有重要意义。该方法有效利用了来自多个相关源的数据，提升了学习效果。"}}
{"id": "2505.19550", "title": "Turing Test 2.0: The General Intelligence Threshold", "authors": ["Georgios Mappouras"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19550v4", "summary": "With the rise of artificial intelligence (A.I.) and large language models\nlike ChatGPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a system\n(computer or any other) has reached or surpassed A.G.I. To achieve this, we\nmake two new contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing test 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing test 2.0 framework\non modern A.I. models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19550v4", "cate": "cs.AI", "date": "2025-05-26", "updated": "2025-07-23", "AI": {"title_translation": "图灵测试2.0：通用智能阈值", "tldr": "本文提出了一个名为“图灵测试2.0”的新框架，用于定义和检测人工智能系统是否达到或超越了通用人工智能（AGI），因为它认为传统图灵测试不足以衡量AGI。", "motivation": "随着人工智能和大型语言模型的兴起，实现通用人工智能（AGI）的竞赛已经开始。然而，尽管有图灵测试等流行工具，但对于如何检测AI模型中的AGI，目前没有明确的共识。本文旨在解决传统方法在测量或检测AGI方面的不足。", "method": "本文首先给出了通用智能（G.I.）的明确定义，并设定了一个通用智能阈值（G.I.T.），用于区分是否达到AGI的系统。其次，提出了一种新的框架，即“图灵测试2.0”，用于构建能够以简单、全面、明确的通过/失败方式检测系统是否达到G.I.的测试。", "result": "作者展示了将遵循其图灵测试2.0框架的测试应用于现代AI模型的实际例子。", "conclusion": "本文认为传统图灵测试不足以衡量或检测AGI，并提供了一种新的实用方法，即图灵测试2.0，通过对通用智能的明确定义和阈值设定，来判断系统是否达到或超越AGI。", "translation": "随着人工智能（A.I.）和ChatGPT等大型语言模型的兴起，一场实现人工通用智能（A.G.I）的新竞赛已经开始。虽然许多人猜测A.I.将如何以及何时实现A.G.I.，但即使使用图灵测试（及其现代变体）等流行工具来衡量它们的智能，对于如何在A.I.模型中检测A.G.I.也没有明确的共识。在这项工作中，我们讨论了为什么图灵测试等传统方法不足以衡量或检测A.G.I.，并提供了一种新的、实用的方法，可以用来判断一个系统（计算机或任何其他）是否已经达到或超越了A.G.I.。为了实现这一点，我们做出了两项新贡献。首先，我们提出了通用智能（G.I.）的明确定义，并设定了一个G.I.阈值（G.I.T.），可以用来区分达到A.G.I.的系统和未达到A.G.I.的系统。其次，我们提出了一个关于如何构建测试的新框架，该框架可以以简单、全面、明确的通过/失败方式检测系统是否已经实现G.I.。我们将这个新颖的框架称为图灵测试2.0。然后，我们展示了将遵循我们图灵测试2.0框架的测试应用于现代A.I.模型的实际例子。", "summary": "本文针对现有图灵测试在衡量通用人工智能（AGI）方面的不足，提出了一个名为“图灵测试2.0”的新框架。该框架首先明确定义了通用智能（GI）并设定了GI阈值（GIT），随后提供了构建测试的方法，以简单、明确的方式判断系统是否达到GI，并展示了在现代AI模型上的应用实例。", "keywords": "图灵测试2.0, 通用人工智能, 通用智能阈值, AI模型", "comments": "这项工作在当前AGI研究热潮中具有重要意义，它试图为AGI的检测提供一个更具体、更可操作的标准。通过明确通用智能的定义和阈值，并提出新的测试框架，有望为AGI的评估提供新的思路，克服传统图灵测试的局限性。其创新点在于从理论定义到实践测试的全链条设计。"}}
{"id": "2504.19996", "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery", "authors": ["Andreas Kalogeras", "Dimitrios Bormpoudakis", "Iason Tsardanidis", "Dimitra A. Loka", "Charalampos Kontoes"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2025)", "url": "http://arxiv.org/abs/2504.19996v2", "summary": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability.", "comment": "Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "pdf_url": "http://arxiv.org/pdf/2504.19996v2", "cate": "cs.CV", "date": "2025-04-28", "updated": "2025-07-23", "AI": {"title_translation": "使用Sentinel-2卫星图像监测农业作物上的沼渣施用", "tldr": "研究利用Sentinel-2卫星图像和机器学习模型监测农业作物上沼渣施用，以支持精准农业和可持续性。", "motivation": "农业中外源有机物（EOM）的广泛使用需要对其进行监测，以评估其对土壤和作物健康的影响。沼渣施用虽然能提高土壤肥力，但也存在微塑料污染和氮损失等环境风险，因此需要有效监测。", "method": "本研究首先利用Sentinel-2卫星图像时间序列（SITS）分析了特定指数（EOMI、NDVI、EVI），以表征沼渣施用后在希腊塞萨利地区四种不同作物土壤上的光谱行为。此外，还使用了机器学习（ML）模型（即随机森林、k-NN、梯度提升和前馈神经网络）来调查沼渣存在的检测。", "result": "机器学习模型在检测沼渣存在方面取得了高达0.85的F1分数。研究结果强调了结合遥感和机器学习进行可扩展且经济高效的外源有机物施用监测的潜力。", "conclusion": "结合遥感和机器学习的方法为可扩展且经济高效地监测外源有机物施用提供了前景，从而支持精准农业和可持续发展。", "translation": "农业中外源有机物（Exogenous Organic Matter, EOM）的广泛使用需要对其进行监测，以评估其对土壤和作物健康的影响。本研究评估了利用光学Sentinel-2卫星图像检测沼渣施用的可行性。沼渣施用能提高土壤肥力，但也带来微塑料污染和氮损失等环境风险。首先，研究利用Sentinel-2卫星图像时间序列（SITS）分析了特定指数（EOMI、NDVI、EVI），以表征沼渣施用后在希腊塞萨利地区四种不同作物土壤上的光谱行为。此外，还使用了机器学习（ML）模型（即随机森林、k-NN、梯度提升和前馈神经网络）来调查沼渣存在的检测，F1分数高达0.85。研究结果强调了结合遥感和机器学习进行可扩展且经济高效的外源有机物施用监测的潜力，从而支持精准农业和可持续发展。", "summary": "本研究利用Sentinel-2卫星图像时间序列和机器学习模型（如随机森林、k-NN、梯度提升和前馈神经网络）来监测农业作物上的沼渣施用。通过分析特定光谱指数和训练模型，研究实现了高达0.85的F1分数来检测沼渣的存在。结果表明，结合遥感和机器学习为可扩展、经济高效地监测外源有机物施用提供了有效途径，有助于精准农业和可持续发展。", "keywords": "沼渣施用, Sentinel-2, 遥感, 机器学习, 精准农业", "comments": "该研究的创新之处在于结合了Sentinel-2卫星遥感数据与多种机器学习模型，为沼渣等外源有机物在农业中的应用提供了一种非侵入式、可扩展且经济高效的监测方法。这对于评估其环境影响（如微塑料污染和氮损失）并支持精准农业实践具有重要意义。其局限性可能在于模型的泛化能力和对不同地域、作物类型及施用量的适应性。"}}
{"id": "2507.16840", "title": "CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples", "authors": ["Weijia Yang", "Tian Lan", "Leyuan Liu", "Wei Chen", "Tianqing Zhu", "Sheng Wen", "Xiaosong Zhang"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16840v1", "summary": "The rapid evolution of digital currency trading, fueled by the integration of\nblockchain technology, has led to both innovation and the emergence of smart\nPonzi schemes. A smart Ponzi scheme is a fraudulent investment operation in\nsmart contract that uses funds from new investors to pay returns to earlier\ninvestors. Traditional Ponzi scheme detection methods based on deep learning\ntypically rely on fully supervised models, which require large amounts of\nlabeled data. However, such data is often scarce, hindering effective model\ntraining. To address this challenge, we propose a novel contrastive learning\nframework, CASPER (Contrastive Approach for Smart Ponzi detectER with more\nnegative samples), designed to enhance smart Ponzi scheme detection in\nblockchain transactions. By leveraging contrastive learning techniques, CASPER\ncan learn more effective representations of smart contract source code using\nunlabeled datasets, significantly reducing both operational costs and system\ncomplexity. We evaluate CASPER on the XBlock dataset, where it outperforms the\nbaseline by 2.3% in F1 score when trained with 100% labeled data. More\nimpressively, with only 25% labeled data, CASPER achieves an F1 score nearly\n20% higher than the baseline under identical experimental conditions. These\nresults highlight CASPER's potential for effective and cost-efficient detection\nof smart Ponzi schemes, paving the way for scalable fraud detection solutions\nin the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16840v1", "cate": "cs.CR", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "CASPER：一种利用更多负样本的智能庞氏骗局检测器的对比方法", "tldr": "CASPER提出了一种对比学习框架，用于在数据稀缺的情况下高效检测区块链中的智能庞氏骗局，显著优于基线模型。", "motivation": "区块链技术催生了智能庞氏骗局，而传统的深度学习检测方法依赖大量标注数据，但此类数据通常稀缺，阻碍了模型训练的有效性。", "method": "本文提出了一种新颖的对比学习框架CASPER（Contrastive Approach for Smart Ponzi detectER with more negative samples），通过利用对比学习技术，CASPER能够使用未标注数据集学习智能合约源代码的更有效表示，从而增强智能庞氏骗局的检测能力。", "result": "在XBlock数据集上，CASPER在使用100%标注数据训练时，F1分数比基线高出2.3%。更显著的是，在仅使用25%标注数据的情况下，CASPER的F1分数比基线高出近20%。", "conclusion": "CASPER在智能庞氏骗局检测方面展现出有效性和成本效益的潜力，为未来可扩展的欺诈检测解决方案奠定了基础。", "translation": "区块链技术整合推动了数字货币交易的快速发展，既带来了创新，也催生了智能庞氏骗局。智能庞氏骗局是智能合约中的一种欺诈性投资操作，它利用新投资者的资金支付早期投资者的回报。传统的基于深度学习的庞氏骗局检测方法通常依赖于全监督模型，需要大量的标注数据。然而，此类数据往往稀缺，阻碍了有效的模型训练。为了解决这一挑战，我们提出了一种新颖的对比学习框架CASPER（Contrastive Approach for Smart Ponzi detectER with more negative samples），旨在增强区块链交易中的智能庞氏骗局检测。通过利用对比学习技术，CASPER可以使用未标注数据集学习智能合约源代码的更有效表示，显著降低运营成本和系统复杂性。我们在XBlock数据集上评估了CASPER，在使用100%标注数据训练时，其F1分数比基线高出2.3%。更令人印象深刻的是，在仅使用25%标注数据的情况下，CASPER在相同实验条件下实现了比基线高出近20%的F1分数。这些结果突显了CASPER在有效和经济高效检测智能庞氏骗局方面的潜力，为未来可扩展的欺诈检测解决方案铺平了道路。", "summary": "本文提出了一种名为CASPER的对比学习框架，旨在解决区块链中智能庞氏骗局检测面临的标注数据稀缺问题。CASPER通过利用对比学习从非标注数据中学习智能合约代码的有效表示，显著降低了操作成本和系统复杂性。实验结果表明，CASPER在F1分数上优于基线模型，尤其是在标注数据量有限（仅25%）的情况下，其性能提升近20%，证明了其在高效和经济地检测智能庞氏骗局方面的潜力。", "keywords": "智能庞氏骗局, 区块链, 对比学习, 欺诈检测, 数据稀缺", "comments": "CASPER的创新点在于将对比学习应用于智能庞氏骗局检测，有效解决了深度学习模型对大量标注数据依赖的问题。这对于实际应用中数据获取困难的领域具有重要意义，其在低标注数据量下的显著性能提升，预示着该方法在可扩展欺诈检测方面的巨大潜力。"}}
{"id": "2507.17597", "title": "Explainable AI for Collaborative Assessment of 2D/3D Registration Quality", "authors": ["Sue Min Cho", "Alexander Do", "Russell H. Taylor", "Mathias Unberath"], "categories": ["cs.HC", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17597v1", "summary": "As surgery embraces digital transformation--integrating sophisticated\nimaging, advanced algorithms, and robotics to support and automate complex\nsub-tasks--human judgment of system correctness remains a vital safeguard for\npatient safety. This shift introduces new \"operator-type\" roles tasked with\nverifying complex algorithmic outputs, particularly at critical junctures of\nthe procedure, such as the intermediary check before drilling or implant\nplacement. A prime example is 2D/3D registration, a key enabler of image-based\nsurgical navigation that aligns intraoperative 2D images with preoperative 3D\ndata. Although registration algorithms have advanced significantly, they\noccasionally yield inaccurate results. Because even small misalignments can\nlead to revision surgery or irreversible surgical errors, there is a critical\nneed for robust quality assurance. Current visualization-based strategies alone\nhave been found insufficient to enable humans to reliably detect 2D/3D\nregistration misalignments. In response, we propose the first artificial\nintelligence (AI) framework trained specifically for 2D/3D registration quality\nverification, augmented by explainability features that clarify the model's\ndecision-making. Our explainable AI (XAI) approach aims to enhance informed\ndecision-making for human operators by providing a second opinion together with\na rationale behind it. Through algorithm-centric and human-centered\nevaluations, we systematically compare four conditions: AI-only, human-only,\nhuman-AI, and human-XAI. Our findings reveal that while explainability features\nmodestly improve user trust and willingness to override AI errors, they do not\nexceed the standalone AI in aggregate performance. Nevertheless, future work\nextending both the algorithmic design and the human-XAI collaboration elements\nholds promise for more robust quality assurance of 2D/3D registration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17597v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "可解释人工智能用于二维/三维配准质量的协作评估", "tldr": "该研究提出了一种可解释AI框架，用于评估手术中2D/3D配准质量，旨在辅助人类操作员决策，但发现可解释性特征对整体性能提升有限。", "motivation": "手术数字化转型中，2D/3D配准是关键技术，但配准不准确可能导致严重后果。现有可视化方法不足以可靠检测误差，因此需要更鲁棒的质量保证方法。", "method": "提出了首个专门用于2D/3D配准质量验证的AI框架，并结合可解释性特征（XAI）来解释模型决策。通过算法中心和以人为中心的评估，比较了四种条件：AI-only, human-only, human-AI, 和 human-XAI。", "result": "结果显示，可解释性特征在适度提升用户信任和纠正AI错误意愿方面有帮助，但整体性能并未超过独立的AI。", "conclusion": "尽管可解释性特征对整体性能提升有限，但未来在算法设计和人机协作方面的扩展工作有望实现更强大的2D/3D配准质量保证。", "translation": "随着外科手术迈向数字化转型——整合先进成像、高级算法和机器人技术以支持和自动化复杂的子任务——人类对系统正确性的判断仍然是保障患者安全的重要防线。这种转变引入了新的“操作员”角色，负责验证复杂的算法输出，尤其是在手术的关键时刻，如钻孔或植入物放置前的中间检查。一个典型的例子是二维/三维配准，这是基于图像的手术导航的关键技术，它将术中二维图像与术前三维数据对齐。尽管配准算法已取得显著进展，但它们偶尔会产生不准确的结果。由于即使是微小的错位也可能导致返工手术或不可逆转的手术错误，因此迫切需要强大的质量保证。目前仅基于可视化的策略已被发现不足以使人类可靠地检测二维/三维配准错位。为此，我们提出了第一个专门用于二维/三维配准质量验证的人工智能（AI）框架，并辅以解释模型决策的可解释性特征。我们的可解释人工智能（XAI）方法旨在通过提供第二意见及其背后的原理来增强人类操作员的知情决策。通过以算法为中心和以人为中心的评估，我们系统地比较了四种条件：仅AI、仅人类、人机协作（human-AI）和人机协作与可解释AI（human-XAI）。我们的研究结果表明，虽然可解释性特征适度提高了用户信任和纠正AI错误的意愿，但它们在总体性能上并未超过独立的AI。尽管如此，未来在算法设计和人机协作（human-XAI）元素方面的扩展工作有望为二维/三维配准提供更强大的质量保证。", "summary": "本文提出了一种可解释人工智能（XAI）框架，旨在提升手术中2D/3D配准质量验证的可靠性。该框架通过提供AI的第二意见及其决策依据，辅助人类操作员进行知情决策。研究比较了AI-only、human-only、human-AI和human-XAI四种条件，发现虽然XAI能适度提升用户信任和纠错意愿，但在整体性能上并未超越独立AI。文章指出未来工作可进一步优化算法设计和人机协作以增强质量保障。", "keywords": "可解释人工智能, 2D/3D配准, 质量评估, 手术导航, 人机协作", "comments": "这篇论文的创新点在于首次将可解释AI应用于2D/3D配准质量验证这一关键医疗场景，旨在弥补现有可视化方法的不足。其重要性体现在提升手术安全性，通过人机协作减少潜在的医疗错误。然而，研究结果也揭示了当前XAI在提升整体性能方面仍存在局限性，表明未来需要进一步优化算法和人机交互设计，以真正发挥XAI的潜力。"}}
{"id": "2507.12233", "title": "Universal Fourier Neural Operators for Micromechanics", "authors": ["Binh Huy Nguyen", "Matti Schneider"], "categories": ["cs.CE", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      48 pages, 13 figures", "url": "http://arxiv.org/abs/2507.12233v2", "summary": "Solving cell problems in homogenization is hard, and available deep-learning\nframeworks fail to match the speed and generality of traditional computational\nframeworks. More to the point, it is generally unclear what to expect of\nmachine-learning approaches, let alone single out which approaches are\npromising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for\nmicromechanics, empowering them by insights from computational micromechanics\nmethods based on the fast Fourier transform (FFT). We construct an FNO\nsurrogate mimicking the basic scheme foundational for FFT-based methods and\nshow that the resulting operator predicts solutions to cell problems with\narbitrary stiffness distribution only subject to a material-contrast constraint\nup to a desired accuracy. In particular, there are no restrictions on the\nmaterial symmetry like isotropy, on the number of phases and on the geometry of\nthe interfaces between materials. Also, the provided fidelity is sharp and\nuniform, providing explicit guarantees leveraging our physical empowerment of\nFNOs. To show the desired universal approximation property, we construct an FNO\nexplicitly that requires no training to begin with. Still, the obtained neural\noperator complies with the same memory requirements as the basic scheme and\ncomes with runtimes proportional to classical FFT solvers. In particular,\nlarge-scale problems with more than 100 million voxels are readily handled. The\ngoal of this work is to underline the potential of FNOs for solving\nmicromechanical problems, linking FFT-based methods to FNOs. This connection is\nexpected to provide a fruitful exchange between both worlds.", "comment": "48 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.12233v2", "cate": "cs.CE", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "微观力学中的通用傅里叶神经算子", "tldr": "本文提出并验证了傅里叶神经算子（FNOs）在解决均质化中的细胞问题方面的通用性、速度和精确性，克服了现有深度学习框架的局限性，并与快速傅里叶变换（FFT）方法建立了联系。", "motivation": "均质化中的细胞问题难以解决，现有深度学习框架在速度和通用性上无法与传统计算框架匹敌。此外，目前尚不清楚机器学习方法在微观力学中的潜力。", "method": "本文倡导将傅里叶神经算子（FNOs）应用于微观力学，并通过快速傅里叶变换（FFT）的计算微观力学方法中的见解来增强它们。构建了一个模仿FFT基础方案的FNO替代模型。", "result": "所得到的算子能够以任意刚度分布预测细胞问题的解，仅受材料对比度约束，并达到所需精度。对材料对称性、相数和界面几何形状没有限制。该方法提供了清晰且一致的保真度，且无需训练即可展示其通用逼近特性。该神经算子内存需求与基本方案相同，运行时间与经典FFT求解器成比例，能够处理超过1亿体素的大规模问题。", "conclusion": "这项工作旨在强调FNOs在解决微观力学问题方面的潜力，并将基于FFT的方法与FNOs联系起来，预计这将促进两个领域之间的富有成效的交流。", "translation": "均质化中的细胞问题很难解决，现有深度学习框架无法与传统计算框架的速度和通用性相匹配。更重要的是，通常不清楚对机器学习方法抱有什么期望，更不用说挑选出哪些方法是有前途的。在当前的工作中，我们提倡将傅里叶神经算子（FNOs）用于微观力学，并通过快速傅里叶变换（FFT）的计算微观力学方法中的见解来增强它们。我们构建了一个模仿FFT基础方案的FNO替代模型，并表明所得到的算子能够以任意刚度分布预测细胞问题的解，仅受材料对比度约束并达到所需的精度。特别是，对材料对称性（如各向同性）、相数以及材料之间界面的几何形状没有限制。此外，所提供的保真度清晰且一致，利用我们对FNOs的物理赋能提供了明确的保证。为了展示所需的通用逼近特性，我们明确构建了一个无需训练的FNO。尽管如此，所获得的神经算子符合与基本方案相同的内存要求，并且运行时间与经典FFT求解器成比例。特别是，可以轻松处理超过1亿体素的大规模问题。这项工作的目标是强调FNOs在解决微观力学问题方面的潜力，将基于FFT的方法与FNOs联系起来。预计这种联系将促进两个领域之间的富有成效的交流。", "summary": "本文提出了一种基于傅里叶神经算子（FNOs）的新方法来解决均质化中的细胞问题，旨在克服现有深度学习框架在速度和通用性上的不足。通过结合快速傅里叶变换（FFT）的见解，所构建的FNO替代模型能够以高精度预测任意刚度分布的解，且不受材料对称性、相数或几何形状的限制。该方法无需训练即可展示通用逼近能力，并且在内存和运行时间上与经典FFT求解器相当，能够有效处理大规模微观力学问题。研究强调了FNOs在微观力学中的巨大潜力及其与FFT方法的联系。", "keywords": "傅里叶神经算子, 微观力学, 快速傅里叶变换, 均质化, 细胞问题", "comments": "本文的创新之处在于将傅里叶神经算子（FNOs）与快速傅里叶变换（FFT）的物理见解相结合，从而在微观力学领域实现了高效且通用的细胞问题求解。其重要性在于克服了传统深度学习方法在处理复杂材料问题时的局限性，特别是在无需训练的情况下展现出通用逼近能力，并能处理超大规模问题，为计算微观力学提供了新的范式。"}}
{"id": "2507.17506", "title": "Joint Multi-Target Detection-Tracking in Cognitive Massive MIMO Radar via POMCP", "authors": ["Imad Bouhou", "Stefano Fortunati", "Leila Gharsalli", "Alexandre Renaux"], "categories": ["eess.SP", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17506v1", "summary": "This correspondence presents a power-aware cognitive radar framework for\njoint detection and tracking of multiple targets in a massive multiple-input\nmultiple-output (MIMO) radar environment. Building on a previous single-target\nalgorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend\nit to the multi-target case by assigning each target an independent POMCP tree,\nenabling scalable and efficient planning.\n  Departing from uniform power allocation-which is often suboptimal with\nvarying signal-to-noise ratios (SNRs)-our approach predicts each target's\nfuture angular position and expected received power, based on its estimated\nrange and radar cross-section (RCS). These predictions guide adaptive waveform\ndesign via a constrained optimization problem that allocates transmit energy to\nenhance the detectability of weaker or distant targets, while ensuring\nsufficient power for high-SNR targets. The reward function in the underlying\npartially observable Markov decision process (POMDP) is also modified to\nprioritize accurate spatial and power estimation.\n  Simulations involving multiple targets with different SNRs confirm the\neffectiveness of our method. The proposed framework for the cognitive radar\nimproves detection probability for low-SNR targets and achieves more accurate\ntracking compared to approaches using uniform or orthogonal waveforms. These\nresults demonstrate the potential of the POMCP-based framework for adaptive,\nefficient multi-target radar systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17506v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于POMCP的认知大规模MIMO雷达多目标联合检测与跟踪", "tldr": "本文提出了一种基于POMCP的认知大规模MIMO雷达多目标联合检测与跟踪框架，通过为每个目标分配独立的POMCP树并采用自适应功率分配策略，显著提高了低信噪比目标的检测概率和跟踪精度。", "motivation": "传统雷达系统中，均匀功率分配在信噪比（SNR）变化时通常不是最优的，尤其对于较弱或较远的目标，其可检测性较差。因此，需要一种能够自适应分配发射能量以提高低信噪比目标检测能力的方法。", "method": "本研究将先前的单目标POMCP算法扩展到多目标情况，为每个目标分配一个独立的POMCP树，以实现可伸缩和高效的规划。该方法通过预测每个目标的未来角度位置和预期接收功率，指导自适应波形设计，通过约束优化问题分配发射能量，以增强弱或远目标的检测能力。同时，修改了底层部分可观测马尔可夫决策过程（POMDP）的奖励函数，以优先考虑准确的空间和功率估计。", "result": "涉及不同信噪比的多个目标的仿真证实了所提方法的有效性。与使用均匀或正交波形的方法相比，所提出的认知雷达框架提高了低信噪比目标的检测概率，并实现了更准确的跟踪。", "conclusion": "基于POMCP的框架在自适应、高效的多目标雷达系统方面具有巨大潜力。", "translation": "本通信提出了一种功率感知的认知雷达框架，用于在大规模多输入多输出（MIMO）雷达环境中对多个目标进行联合检测和跟踪。在先前基于部分可观测蒙特卡罗规划（POMCP）的单目标算法的基础上，我们通过为每个目标分配一个独立的POMCP树，将其扩展到多目标情况，从而实现了可伸缩和高效的规划。\n与通常在信噪比（SNR）变化时次优的均匀功率分配不同，我们的方法根据每个目标的估计距离和雷达散射截面（RCS），预测其未来的角度位置和预期的接收功率。这些预测通过一个约束优化问题指导自适应波形设计，该问题分配发射能量以增强较弱或较远目标的可检测性，同时确保高信噪比目标有足够的功率。底层部分可观测马尔可夫决策过程（POMDP）中的奖励函数也进行了修改，以优先考虑准确的空间和功率估计。\n涉及不同信噪比的多个目标的仿真证实了我们方法的有效性。所提出的认知雷达框架与使用均匀或正交波形的方法相比，提高了低信噪比目标的检测概率，并实现了更准确的跟踪。这些结果表明了基于POMCP的框架在自适应、高效的多目标雷达系统方面的潜力。", "summary": "本文提出了一个用于认知大规模MIMO雷达的多目标联合检测与跟踪框架。该框架将部分可观测蒙特卡罗规划（POMCP）扩展到多目标场景，为每个目标分配独立POMCP树以实现高效规划。为了克服传统均匀功率分配的局限性，该方法通过预测目标未来状态来自适应分配发射能量，优化波形设计，以提高低信噪比目标的检测能力。仿真结果表明，与现有方法相比，该框架显著提升了低信噪比目标的检测概率和跟踪精度，展现了其在自适应多目标雷达系统中的应用潜力。", "keywords": "认知雷达, 大规模MIMO, 多目标, POMCP, 自适应功率分配", "comments": "该论文的创新点在于将POMCP算法从单目标扩展到多目标场景，并引入了基于目标状态预测的自适应功率分配策略，有效解决了传统雷达在处理低信噪比目标时的挑战。其重要性在于为未来高效、自适应的多目标雷达系统提供了一个有前景的解决方案。"}}
{"id": "2507.17509", "title": "Graph Neural Network Approach to Predicting Magnetization in Quasi-One-Dimensional Ising Systems", "authors": ["V. Slavin", "O. Kryvchikov", "D. Laptev"], "categories": ["cond-mat.dis-nn", "cs.LG"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures", "url": "http://arxiv.org/abs/2507.17509v1", "summary": "We present a graph-based deep learning framework for predicting the magnetic\nproperties of quasi-one-dimensional Ising spin systems. The lattice geometry is\nencoded as a graph and processed by a graph neural network (GNN) followed by\nfully connected layers. The model is trained on Monte Carlo simulation data and\naccurately reproduces key features of the magnetization curve, including\nplateaus, critical transition points, and the effects of geometric frustration.\nIt captures both local motifs and global symmetries, demonstrating that GNNs\ncan infer magnetic behavior directly from structural connectivity. The proposed\napproach enables efficient prediction of magnetization without the need for\nadditional Monte Carlo simulations.", "comment": "18 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.17509v1", "cate": "cond-mat.dis-nn", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "图神经网络方法预测准一维伊辛系统中的磁化强度", "tldr": "该研究提出了一种基于图神经网络的深度学习框架，用于高效预测准一维伊辛自旋系统的磁性，无需额外的蒙特卡洛模拟。", "motivation": "该研究旨在开发一种高效的方法来预测准一维伊辛自旋系统的磁性，以避免进行额外的蒙特卡洛模拟。", "method": "研究提出了一种基于图的深度学习框架。晶格几何结构被编码为图，并通过图神经网络（GNN）和全连接层进行处理。该模型在蒙特卡洛模拟数据上进行训练。", "result": "该模型准确地再现了磁化曲线的关键特征，包括平台、临界转变点和几何阻挫效应。它捕获了局部图案和全局对称性。", "conclusion": "研究表明，图神经网络可以直接从结构连接性推断磁行为，所提出的方法能够高效预测磁化强度，而无需额外的蒙特卡洛模拟。", "translation": "我们提出了一种基于图的深度学习框架，用于预测准一维伊辛自旋系统的磁性。晶格几何结构被编码为图，并通过图神经网络（GNN）和全连接层进行处理。该模型在蒙特卡洛模拟数据上进行训练，并准确地再现了磁化曲线的关键特征，包括平台、临界转变点和几何阻挫效应。它捕获了局部图案和全局对称性，表明GNN可以直接从结构连接性推断磁行为。所提出的方法能够高效预测磁化强度，而无需额外的蒙特卡洛模拟。", "summary": "本论文提出了一种利用图神经网络（GNN）预测准一维伊辛自旋系统磁性的深度学习框架。该模型将晶格几何编码为图，并通过GNN和全连接层处理，利用蒙特卡洛模拟数据进行训练。结果显示，该方法能准确重现磁化曲线的关键特征，包括平台和临界点，并捕获局部和全局对称性，证明GNN可以直接从结构连接性推断磁行为，从而实现磁化强度的高效预测，减少对额外蒙特卡洛模拟的需求。", "keywords": "图神经网络, 伊辛系统, 磁化强度, 蒙特卡洛模拟, 深度学习", "comments": "该研究的创新之处在于将图神经网络应用于准一维伊辛系统的磁性预测，有效地结合了深度学习与物理模拟。其重要性在于提供了一种无需大量蒙特卡洛模拟即可高效预测磁行为的方法，这对于计算物理领域具有显著意义。"}}
{"id": "2507.00782", "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "authors": ["Matthieu Pierre Boyer"], "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages plus one page appendix, submission to CSL 2026", "url": "http://arxiv.org/abs/2507.00782v2", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressiveness of a more traditional\ndenotation style. We will formalize a category based type and effect system to\nrepresent the semantic difference between syntactically equivalent expressions.\nWe then construct a diagrammatic calculus to model parsing and handling of\neffects, providing a method to efficiently compute the denotations for\nsentences.", "comment": "15 pages plus one page appendix, submission to CSL 2026", "pdf_url": "http://arxiv.org/pdf/2507.00782v2", "cate": "cs.CL", "date": "2025-07-01", "updated": "2025-07-23", "AI": {"title_translation": "自然语言语义函数模型的图解演算", "tldr": "本文提出一种基于函数式编程和图解演算的方法来处理自然语言语义，以提高表达能力并有效计算句子指称。", "motivation": "旨在增加传统指称风格的表达能力，并形式化一个基于类别的类型和效应系统来表示句法等价表达式之间的语义差异。", "method": "研究一种函数式编程方法来处理自然语言语义；形式化一个基于类别的类型和效应系统；构建一个图解演算来建模解析和效应处理。", "result": "提供了一种有效计算句子指称的方法。", "conclusion": "该方法能够有效计算句子的指称，解决了传统指称风格表达能力不足的问题，并能处理句法等价表达式的语义差异。", "translation": "本文研究了一种处理自然语言语义的函数式编程方法，这使我们能够增加更传统的指称风格的表达能力。我们将形式化一个基于类别的类型和效应系统，以表示句法等价表达式之间的语义差异。然后，我们构建了一个图解演算来建模解析和效应处理，提供了一种有效计算句子指称的方法。", "summary": "本文提出一种结合函数式编程、类别类型和效应系统以及图解演算的方法来处理自然语言语义。该方法旨在提高传统指称风格的表达能力，并能区分句法等价表达式的语义差异。通过构建图解演算，该研究提供了一种有效计算句子指称的方法。", "keywords": "自然语言语义, 函数式编程, 类型和效应系统, 图解演算, 指称计算", "comments": "该论文创新性地将函数式编程、类别理论和图解演算应用于自然语言语义建模，特别是在处理句法等价表达式的语义差异和提高表达能力方面。图解演算的引入为高效计算句子指称提供了新的视角。"}}
{"id": "2507.16852", "title": "SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping", "authors": ["Álvaro Ruiz-Ródenas", "Jaime Pujante Sáez", "Daniel García-Algora", "Mario Rodríguez Béjar", "Jorge Blasco", "José Luis Hernández-Ramos"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      17 pages, 13 figures", "url": "http://arxiv.org/abs/2507.16852v1", "summary": "Cyber Threat Intelligence (CTI) mining involves extracting structured\ninsights from unstructured threat data, enabling organizations to understand\nand respond to evolving adversarial behavior. A key task in CTI mining is\nmapping threat descriptions to MITRE ATT\\&CK techniques. However, this process\nis often performed manually, requiring expert knowledge and substantial effort.\nAutomated approaches face two major challenges: the scarcity of high-quality\nlabeled CTI data and class imbalance, where many techniques have very few\nexamples. While domain-specific Large Language Models (LLMs) such as SecureBERT\nhave shown improved performance, most recent work focuses on model architecture\nrather than addressing the data limitations. In this work, we present SynthCTI,\na data augmentation framework designed to generate high-quality synthetic CTI\nsentences for underrepresented MITRE ATT\\&CK techniques. Our method uses a\nclustering-based strategy to extract semantic context from training data and\nguide an LLM in producing synthetic CTI sentences that are lexically diverse\nand semantically faithful. We evaluate SynthCTI on two publicly available CTI\ndatasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.\nIncorporating synthetic data leads to consistent macro-F1 improvements: for\nexample, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\\%), and\nSecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented\nwith SynthCTI outperform larger models trained without augmentation,\ndemonstrating the value of data generation methods for building efficient and\neffective CTI classification systems.", "comment": "17 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.16852v1", "cate": "cs.CR", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "SynthCTI：LLM驱动的合成CTI生成以增强MITRE技术映射", "tldr": "SynthCTI是一个数据增强框架，利用LLM生成高质量的合成网络威胁情报（CTI）数据，以解决MITRE ATT&CK技术映射中数据稀缺和类别不平衡的问题，显著提高了分类模型的性能。", "motivation": "网络威胁情报（CTI）到MITRE ATT&CK技术的映射通常是手动完成的，需要专业知识和大量精力。自动化方法面临高质量标记CTI数据稀缺和类别不平衡两大挑战。尽管领域特定的大型语言模型（LLM）有所改进，但大多数近期工作侧重于模型架构而非数据限制。", "method": "本文提出了SynthCTI，一个数据增强框架，用于为代表性不足的MITRE ATT&CK技术生成高质量的合成CTI句子。该方法使用基于聚类的策略从训练数据中提取语义上下文，并指导大型语言模型（LLM）生成词汇多样且语义忠实的合成CTI句子。", "result": "在两个公开的CTI数据集上评估了SynthCTI，结果显示，整合合成数据后宏观F1分数持续提高：例如，ALBERT从0.35提高到0.52（相对增益48.6%），SecureBERT达到0.6558（从0.4412提高）。值得注意的是，通过SynthCTI增强的小型模型表现优于未经增强训练的大型模型。", "conclusion": "数据生成方法对于构建高效且有效的CTI分类系统具有重要价值。", "translation": "网络威胁情报（CTI）挖掘涉及从非结构化威胁数据中提取结构化洞察，使组织能够理解和响应不断演变的对抗行为。CTI挖掘中的一项关键任务是将威胁描述映射到MITRE ATT&CK技术。然而，这个过程通常是手动进行的，需要专家知识和大量精力。自动化方法面临两大挑战：高质量标记CTI数据的稀缺性和类别不平衡，即许多技术只有很少的例子。虽然领域特定的大型语言模型（LLM）如SecureBERT已显示出改进的性能，但最近的大多数工作都集中在模型架构上，而不是解决数据限制。在这项工作中，我们提出了SynthCTI，一个数据增强框架，旨在为代表性不足的MITRE ATT&CK技术生成高质量的合成CTI句子。我们的方法使用基于聚类的策略从训练数据中提取语义上下文，并指导LLM生成词汇多样且语义忠实的合成CTI句子。我们在两个公开可用的CTI数据集（CTI-to-MITRE和TRAM）上使用不同容量的LLM评估了SynthCTI。整合合成数据导致宏观F1分数持续提高：例如，ALBERT从0.35提高到0.52（相对增益48.6%），SecureBERT达到0.6558（从0.4412提高）。值得注意的是，通过SynthCTI增强的小型模型表现优于未经增强训练的大型模型，这证明了数据生成方法对于构建高效和有效的CTI分类系统的价值。", "summary": "本文介绍了SynthCTI，一个利用大型语言模型（LLM）生成高质量合成网络威胁情报（CTI）数据的框架，旨在解决将威胁描述映射到MITRE ATT&CK技术时面临的数据稀缺和类别不平衡问题。SynthCTI采用基于聚类的方法指导LLM生成语义忠实且多样化的合成数据。实验结果表明，通过集成SynthCTI生成的合成数据，CTI分类模型的性能（宏观F1分数）显著提升，甚至使得小型模型在数据增强后超越了未经增强训练的大型模型，突显了数据生成在构建高效CTI分类系统中的重要性。", "keywords": "网络威胁情报, 数据增强, 大型语言模型, MITRE ATT&CK, 合成数据", "comments": "该论文的创新点在于提出了一种LLM驱动的数据增强框架SynthCTI，有效解决了CTI领域中高质量标记数据稀缺和类别不平衡的痛点。其重要性在于证明了数据生成而非单纯模型架构优化，在提升CTI分类系统性能方面具有巨大潜力，尤其对于资源受限或数据稀缺的场景提供了有效途径。通过使小型模型达到甚至超越大型模型的性能，该方法为实际部署带来了效率和成本效益。"}}
{"id": "2507.16880", "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed", "authors": ["Antoni Kowalczuk", "Dominik Hintersdorf", "Lukas Struppek", "Kristian Kersting", "Adam Dziedzic", "Franziska Boenisch"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16880v1", "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16880v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "寻找多莉：文本到图像扩散模型中的记忆化比假设的局部性更弱", "tldr": "文本到图像扩散模型的记忆化比预想的更难消除，现有修剪方法不足，需要更根本的去记忆化策略。", "motivation": "文本到图像扩散模型在图像生成方面取得成功，但因可能无意中记忆和复制训练数据而引发数据隐私和知识产权担忧。现有缓解措施基于记忆化是局部的假设，但本研究旨在评估其鲁棒性。", "method": "研究评估了基于剪枝的方法的鲁棒性，通过展示即使剪枝后，对文本嵌入的微小调整也能重新触发数据复制，挑战了记忆化局部性的基本假设。此外，提出了一种新颖的对抗性微调方法，该方法迭代搜索复制触发器并更新模型以提高鲁棒性。", "result": "即使剪枝后，输入提示的文本嵌入的微小调整也足以重新触发数据复制，这突显了这些防御的脆弱性。复制可以从文本嵌入空间中不同的位置触发，并遵循模型中不同的路径，挑战了记忆化局部性的基本假设。", "conclusion": "现有缓解策略不足，需要真正删除记忆内容的方法，而不是试图抑制其检索。本研究为构建更值得信赖和合规的生成式AI提供了基础。", "translation": "文本到图像扩散模型（DMs）在图像生成方面取得了显著成功。然而，由于它们可能无意中记忆和复制训练数据，数据隐私和知识产权方面的担忧依然存在。最近的缓解工作侧重于识别和修剪负责触发复制的权重，这基于记忆化可以被局部化的假设。我们的研究评估了这些基于修剪的方法的鲁棒性。我们证明，即使在修剪之后，对输入提示的文本嵌入进行微小调整足以重新触发数据复制，这突显了这些防御的脆弱性。此外，我们通过展示复制可以在文本嵌入空间中不同的位置触发，并遵循模型中不同的路径，挑战了记忆化局部性的基本假设。我们的发现表明，现有缓解策略不足，并强调需要真正删除记忆内容的方法，而不是试图抑制其检索。作为朝这个方向迈出的第一步，我们引入了一种新颖的对抗性微调方法，该方法迭代搜索复制触发器并更新模型以提高鲁棒性。通过我们的研究，我们为文本到图像DMs中记忆化的本质提供了新的见解，并为构建更值得信赖和合规的生成式AI奠定了基础。", "summary": "本文研究了文本到图像扩散模型中的数据记忆化问题，发现现有基于权重剪枝的缓解策略是脆弱且不足的，因为记忆化并非如先前假设般是局部化的。研究表明，即使剪枝后，微小调整仍能重新触发数据复制。为此，论文提出了一种新的对抗性微调方法，旨在从根本上提高模型鲁棒性，为构建更值得信赖的生成式AI奠定基础。", "keywords": "文本到图像扩散模型, 数据记忆化, 隐私, 对抗性微调, 鲁棒性", "comments": "本文挑战了文本到图像扩散模型中记忆化局部性的普遍假设，揭示了现有防御机制的脆弱性。其创新点在于通过实验证明了记忆化行为的非局部性，并提出了一种对抗性微调的新方法来应对这一挑战，为未来更有效的去记忆化策略提供了重要方向。"}}
{"id": "2507.17727", "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation", "authors": ["Robel Mamo", "Taeyeong Choi"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the 12th European Conference on Mobile Robots (ECMR 2025)", "url": "http://arxiv.org/abs/2507.17727v1", "summary": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.", "comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17727v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CA-Cut: 作物对齐剪切数据增强，以学习更鲁棒的冠层下导航", "tldr": "CA-Cut是一种新的数据增强方法，通过在作物行周围遮挡图像区域，显著提高了冠层下导航模型在复杂环境下的鲁棒性和泛化能力。", "motivation": "现有的深度学习视觉冠层下导航方法需要大量昂贵的训练数据，且传统数据增强技术在复杂冠层环境（如频繁遮挡、碎片、作物间距不均匀）中表现次优，导致模型性能受限。", "method": "提出了一种名为Crop-Aligned Cutout (CA-Cut) 的新型数据增强方法。该方法在输入图像中，对作物行两侧空间分布的随机区域进行遮罩处理，以鼓励训练模型捕获高级上下文特征，即使细粒度信息被遮挡。此外，还进行了消融研究以优化遮罩的数量、大小和空间分布。", "result": "广泛实验表明，基于遮罩的增强有效模拟了遮挡，并显著提高了视觉导航中语义关键点预测的鲁棒性。特别是，CA-Cut中偏向作物行的遮罩分布对于提高预测精度和在不同环境下的泛化能力至关重要，预测误差最多减少36.9%。", "conclusion": "CA-Cut数据增强方法通过模拟作物遮挡并偏向作物行进行遮罩，显著提升了冠层下导航模型在复杂环境中的预测精度、鲁棒性和泛化能力。", "translation": "最先进的视觉冠层下导航方法采用基于深度学习的感知模型来区分可通行空间和作物行。虽然这些模型已表现出成功的性能，但它们需要大量的训练数据以确保在真实世界田间部署中的可靠性。然而，数据收集成本高昂，需要大量人力进行田间采样和标注。为了解决这一挑战，模型训练期间通常采用各种数据增强技术，例如颜色抖动、高斯模糊和水平翻转，以多样化训练数据并增强模型鲁棒性。在本文中，我们假设仅使用这些增强技术可能导致次优性能，特别是在复杂冠层环境下，存在频繁遮挡、碎片和作物间距不均匀的情况。相反，我们提出了一种新颖的增强方法，即作物对齐剪切（CA-Cut），该方法在输入图像中对作物行两侧空间分布的随机区域进行遮罩，以鼓励训练模型捕获高级上下文特征，即使细粒度信息被遮挡。我们对公开玉米田数据集进行的广泛实验表明，基于遮罩的增强对于模拟遮挡和显著提高视觉导航中语义关键点预测的鲁棒性是有效的。特别是，我们表明在CA-Cut中将遮罩分布偏向作物行对于提高预测精度和在不同环境下的泛化能力至关重要，预测误差最多减少36.9%。此外，我们还进行了消融研究，以确定遮罩数量、每个遮罩的大小和遮罩的空间分布，从而最大化整体性能。", "summary": "本文提出了一种名为CA-Cut的新型数据增强方法，旨在提高深度学习模型在复杂冠层下导航的鲁棒性和泛化能力。针对传统数据增强在作物遮挡等复杂环境下的局限性，CA-Cut通过在作物行周围选择性地遮挡图像区域，促使模型学习更高级的上下文特征。实验证明，该方法能有效模拟遮挡，显著提升语义关键点预测的准确性和跨环境泛化能力，最高可将预测误差降低36.9%。", "keywords": "数据增强, 冠层下导航, 深度学习, CA-Cut, 鲁棒性", "comments": "CA-Cut的创新点在于其“作物对齐”的遮罩策略，这比随机遮罩更能有效地模拟真实世界中作物遮挡对导航模型的影响。该方法通过引导模型关注高级上下文信息而非细粒度细节，显著提高了模型在复杂农业环境中的鲁棒性和泛化能力，对农业机器人导航领域具有重要意义。"}}
{"id": "2507.17489", "title": "DFDNet: Dynamic Frequency-Guided De-Flare Network", "authors": ["Minglong Xue", "Aoxiang Ning", "Shivakumara Palaiahnakote", "Mingliang Zhou"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17489v1", "summary": "Strong light sources in nighttime photography frequently produce flares in\nimages, significantly degrading visual quality and impacting the performance of\ndownstream tasks. While some progress has been made, existing methods continue\nto struggle with removing large-scale flare artifacts and repairing structural\ndamage in regions near the light source. We observe that these challenging\nflare artifacts exhibit more significant discrepancies from the reference\nimages in the frequency domain compared to the spatial domain. Therefore, this\npaper presents a novel dynamic frequency-guided deflare network (DFDNet) that\ndecouples content information from flare artifacts in the frequency domain,\neffectively removing large-scale flare artifacts. Specifically, DFDNet consists\nmainly of a global dynamic frequency-domain guidance (GDFG) module and a local\ndetail guidance module (LDGM). The GDFG module guides the network to perceive\nthe frequency characteristics of flare artifacts by dynamically optimizing\nglobal frequency domain features, effectively separating flare information from\ncontent information. Additionally, we design an LDGM via a contrastive learning\nstrategy that aligns the local features of the light source with the reference\nimage, reduces local detail damage from flare removal, and improves\nfine-grained image restoration. The experimental results demonstrate that the\nproposed method outperforms existing state-of-the-art methods in terms of\nperformance. The code is available at\n\\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17489v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DFDNet：动态频率引导的去眩光网络", "tldr": "针对夜间摄影中眩光去除的挑战，本文提出DFDNet，通过在频域解耦内容与眩光信息，并结合全局频域引导和局部细节引导模块，有效去除大尺度眩光并修复细节，性能优于现有SOTA方法。", "motivation": "夜间摄影中的强光源常导致图像眩光，严重降低视觉质量并影响下游任务性能。现有方法在去除大尺度眩光和修复光源附近结构损伤方面仍面临挑战。", "method": "本文提出一种动态频率引导的去眩光网络（DFDNet），通过在频域解耦内容信息与眩光伪影，有效去除大尺度眩光。DFDNet主要包括一个全局动态频域引导（GDFG）模块和一个局部细节引导模块（LDGM）。GDFG模块通过动态优化全局频域特征来引导网络感知眩光伪影的频率特性，从而有效分离眩光信息和内容信息。LDGM通过对比学习策略设计，将光源的局部特征与参考图像对齐，减少眩光去除造成的局部细节损伤，并改善图像的精细化恢复。", "result": "实验结果表明，所提出的方法在性能方面优于现有最先进的方法。", "conclusion": "本文提出的DFDNet通过创新的频域处理和局部细节恢复策略，有效解决了夜间摄影中大尺度眩光去除和细节损伤修复的难题，并取得了超越现有SOTA方法的性能。", "translation": "夜间摄影中的强光源经常在图像中产生眩光，显著降低视觉质量并影响下游任务的性能。尽管取得了一些进展，但现有方法在去除大尺度眩光伪影和修复光源附近区域的结构损伤方面仍然存在困难。我们观察到，与空间域相比，这些具有挑战性的眩光伪影在频域中与参考图像表现出更显著的差异。因此，本文提出了一种新颖的动态频率引导去眩光网络（DFDNet），它在频域中将内容信息与眩光伪影解耦，有效去除大尺度眩光伪影。具体来说，DFDNet主要由一个全局动态频域引导（GDFG）模块和一个局部细节引导模块（LDGM）组成。GDFG模块通过动态优化全局频域特征来引导网络感知眩光伪影的频率特性，从而有效分离眩光信息和内容信息。此外，我们通过对比学习策略设计了一个LDGM，它将光源的局部特征与参考图像对齐，减少眩光去除造成的局部细节损伤，并改善图像的精细化恢复。实验结果表明，所提出的方法在性能方面优于现有最先进的方法。代码可在 https://github.com/AXNing/DFDNet 获取。", "summary": "本文提出DFDNet，一个动态频率引导的去眩光网络，旨在解决夜间摄影中大尺度眩光和光源附近结构损伤的问题。该网络利用眩光在频域中与参考图像的显著差异，通过全局动态频域引导模块在频域解耦内容与眩光信息，并结合局部细节引导模块通过对比学习修复精细细节。实验证明DFDNet在眩光去除性能上超越了现有SOTA方法。", "keywords": "去眩光, 频域处理, 动态引导, 对比学习, 夜间摄影", "comments": "这篇论文的创新点在于观察到眩光在频域中的显著差异，并据此设计了在频域进行内容与眩光解耦的网络。结合全局频域引导和局部细节恢复（尤其是通过对比学习）的策略，有效解决了大尺度眩光和细节损伤的难题，为夜间图像质量提升提供了一个有效的新方法。"}}
{"id": "2507.17617", "title": "Reusing Attention for One-stage Lane Topology Understanding", "authors": ["Yang Li", "Zongzheng Zhang", "Xuchong Qiu", "Xinrun Li", "Ziming Liu", "Leichen Wang", "Ruikai Li", "Zhenxin Zhu", "Huan-ang Gao", "Xiaojian Lin", "Zhiyong Cui", "Hang Zhao", "Hao Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.17617v1", "summary": "Understanding lane toplogy relationships accurately is critical for safe\nautonomous driving. However, existing two-stage methods suffer from\ninefficiencies due to error propagations and increased computational overheads.\nTo address these challenges, we propose a one-stage architecture that\nsimultaneously predicts traffic elements, lane centerlines and topology\nrelationship, improving both the accuracy and inference speed of lane topology\nunderstanding for autonomous driving. Our key innovation lies in reusing\nintermediate attention resources within distinct transformer decoders. This\napproach effectively leverages the inherent relational knowledge within the\nelement detection module to enable the modeling of topology relationships among\ntraffic elements and lanes without requiring additional computationally\nexpensive graph networks. Furthermore, we are the first to demonstrate that\nknowledge can be distilled from models that utilize standard definition (SD)\nmaps to those operates without using SD maps, enabling superior performance\neven in the absence of SD maps. Extensive experiments on the OpenLane-V2\ndataset show that our approach outperforms baseline methods in both accuracy\nand efficiency, achieving superior results in lane detection, traffic element\nidentification, and topology reasoning. Our code is available at\nhttps://github.com/Yang-Li-2000/one-stage.git.", "comment": "Accepted to IROS 2025, Project Page:\n  https://github.com/Yang-Li-2000/one-stage.git", "pdf_url": "http://arxiv.org/pdf/2507.17617v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "重用注意力实现一阶段车道拓扑理解", "tldr": "提出一种一阶段架构，通过重用注意力资源同时预测交通元素、车道中心线和拓扑关系，显著提高自动驾驶中车道拓扑理解的准确性和推理速度。", "motivation": "现有两阶段车道拓扑理解方法存在误差传播和计算开销大等问题，影响自动驾驶安全。", "method": "提出一种一阶段架构，同时预测交通元素、车道中心线和拓扑关系。核心创新在于重用不同Transformer解码器中的中间注意力资源，有效利用元素检测模块中固有的关系知识，从而在不增加计算成本的情况下建模交通元素和车道之间的拓扑关系。此外，首次证明了可以将知识从使用标准定义（SD）地图的模型蒸馏到不使用SD地图的模型，即使在没有SD地图的情况下也能实现卓越性能。", "result": "在OpenLane-V2数据集上的大量实验表明，该方法在准确性和效率方面均优于基线方法，在车道检测、交通元素识别和拓扑推理方面取得了卓越结果。", "conclusion": "通过提出一种重用注意力资源的一阶段架构，该研究显著提高了自动驾驶中车道拓扑理解的准确性和效率，解决了现有两阶段方法的不足，并展示了有效的知识蒸馏能力。", "translation": "准确理解车道拓扑关系对于安全的自动驾驶至关重要。然而，现有的两阶段方法由于误差传播和计算开销增加而效率低下。为了解决这些挑战，我们提出了一种一阶段架构，该架构同时预测交通元素、车道中心线和拓扑关系，从而提高了自动驾驶中车道拓扑理解的准确性和推理速度。我们的关键创新在于重用不同Transformer解码器中的中间注意力资源。这种方法有效利用了元素检测模块中固有的关系知识，从而能够在不需要额外计算成本高昂的图网络的情况下，建模交通元素和车道之间的拓扑关系。此外，我们首次证明了可以将知识从使用标准定义（SD）地图的模型蒸馏到不使用SD地图的模型，即使在没有SD地图的情况下也能实现卓越性能。在OpenLane-V2数据集上的大量实验表明，我们的方法在准确性和效率方面均优于基线方法，在车道检测、交通元素识别和拓扑推理方面取得了卓越结果。我们的代码可在 https://github.com/Yang-Li-2000/one-stage.git 获取。", "summary": "本研究提出一种名为“重用注意力”的一阶段架构，旨在提高自动驾驶中车道拓扑理解的准确性和效率。该方法通过在不同Transformer解码器中重用中间注意力资源，同时预测交通元素、车道中心线和拓扑关系，从而避免了传统两阶段方法的误差传播和高计算成本，并且无需额外的图网络。此外，该工作首次成功将知识从使用SD地图的模型蒸馏到不使用SD地图的模型。在OpenLane-V2数据集上的实验结果表明，该方法在车道检测、交通元素识别和拓扑推理方面均优于现有基线。", "keywords": "车道拓扑, 一阶段, 注意力重用, 自动驾驶, 知识蒸馏", "comments": "该论文的创新之处在于其提出的一阶段架构，通过巧妙地重用Transformer解码器中的注意力资源，实现了对车道拓扑关系的理解，同时避免了传统方法中复杂的图网络和两阶段带来的效率问题。知识蒸馏的应用也为无SD地图场景下的性能提升提供了新的思路，具有较高的实用价值和工程意义。"}}
{"id": "2208.13296", "title": "Polynomial time guarantees for sampling based posterior inference in high-dimensional generalised linear models", "authors": ["Randolf Altmeyer"], "categories": ["math.ST", "cs.NA", "math.AP", "math.NA", "math.PR", "stat.CO", "stat.TH", "62F15, 62G05, 65C05"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      Published version", "url": "http://arxiv.org/abs/2208.13296v3", "summary": "The problem of computing posterior functionals in general high-dimensional\nstatistical models with possibly non-log-concave likelihood functions is\nconsidered. Based on the proof strategy of Nickl and Wang (2022), but using\nonly local likelihood conditions and without relying on M-estimation theory,\nnonasymptotic statistical and computational guarantees are provided for a\ngradient based MCMC algorithm. Given a suitable initialiser, these guarantees\nscale polynomially in key algorithmic quantities. The abstract results are\napplied to several concrete statistical models, including density estimation,\nnonparametric regression with generalised linear models and a canonical\nstatistical non-linear inverse problem from PDEs.", "comment": "Published version", "pdf_url": "http://arxiv.org/pdf/2208.13296v3", "cate": "math.ST", "date": "2022-08-28", "updated": "2025-07-23", "AI": {"title_translation": "高维广义线性模型中基于采样的后验推断的多项式时间保证", "tldr": "本文为高维广义线性模型中，针对可能非对数凹似然函数的后验推断，提供了一种基于梯度的MCMC算法的多项式时间保证。", "motivation": "考虑在高维统计模型中计算后验泛函的问题，特别是当似然函数可能非对数凹时。", "method": "基于Nickl和Wang (2022) 的证明策略，但仅使用局部似然条件且不依赖于M-估计理论，为一种基于梯度的MCMC算法提供了非渐近统计和计算保证。", "result": "在给定合适初始化器的情况下，这些保证在关键算法量上呈多项式增长。这些抽象结果被应用于密度估计、广义线性模型的非参数回归以及偏微分方程中的典型统计非线性逆问题等具体统计模型。", "conclusion": "论文为高维广义线性模型中，针对可能非对数凹似然函数的后验推断，提供了一种基于梯度的MCMC算法的多项式时间保证，并成功应用于多个具体模型。", "translation": "考虑在高维统计模型中计算后验泛函的问题，这些模型可能具有非对数凹的似然函数。基于Nickl和Wang (2022) 的证明策略，但仅使用局部似然条件且不依赖于M-估计理论，本文为一种基于梯度的MCMC算法提供了非渐近统计和计算保证。在给定合适初始化器的情况下，这些保证在关键算法量上呈多项式增长。这些抽象结果被应用于几个具体的统计模型，包括密度估计、广义线性模型的非参数回归以及偏微分方程中的典型统计非线性逆问题。", "summary": "本文研究了在高维统计模型中计算后验泛函的问题，特别是当似然函数可能非对数凹时。作者提出了一种基于梯度的MCMC算法，并提供了其非渐近统计和计算保证，这些保证在关键算法量上呈多项式增长。该方法仅依赖局部似然条件，避免了M-估计理论。研究结果成功应用于密度估计、广义线性模型的非参数回归和非线性逆问题等领域。", "keywords": "高维模型, 后验推断, MCMC, 多项式时间, 广义线性模型", "comments": "本文的创新之处在于，在处理高维统计模型中可能非对数凹的似然函数时，为基于梯度的MCMC算法提供了多项式时间保证，且仅依赖局部似然条件，避免了对M-估计理论的依赖，这在理论和实际应用中都具有重要意义。"}}
{"id": "2507.16870", "title": "Building a robust OAuth token based API Security: A High level Overview", "authors": ["Senthilkumar Gopal"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, IEEE Transactions on Dependable and Secure Computing", "url": "http://arxiv.org/abs/2507.16870v1", "summary": "APIs (Application Programming Interfaces) or Web Services are the\nfoundational building blocks that enable interconnected systems. However this\nproliferation of APIs has also introduced security challenges that require\nsystematic and scalable solutions for secure authentication and authorization.\nThis paper presents the fundamentals necessary for building a such a\ntoken-based API security system. It discusses the components necessary, the\nintegration of OAuth 2.0, extensibility of the token architectures, necessary\ncryptographic foundations, and persistence strategies to ensure secure and\nresilient operations. In addition to architectural concerns, the paper explores\nbest practices for token lifecycle management, scope definition, expiration\npolicies, and revocation mechanisms, all framed within a real-world scenario.\nBy adhering to these principles, developers can establish a robust baseline\nwhile maintaining the flexibility to customize their domain-specific\nrequirements. The approach does not claim to cover all variations necessary for\ndiverse architectures but instead focuses on key principles essential for any\nstandard API token authentication system. Throughout, the paper emphasizes\nbalancing practical considerations with security imperatives and uses key\nconcepts such as the CIA triad, OAuth standards, secure token life cycle, and\npractices for protecting sensitive user and application data. The intent is to\nequip developers with the foundational knowledge necessary to build secure,\nscalable token-based API security systems ready to handle the evolving threat\nlandscape.", "comment": "11 pages, 5 figures, IEEE Transactions on Dependable and Secure\n  Computing", "pdf_url": "http://arxiv.org/pdf/2507.16870v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "构建基于OAuth令牌的强大API安全性：高级概述", "tldr": "本文概述了构建安全、可扩展的基于OAuth令牌的API安全系统的基本要素和最佳实践，以应对API安全挑战。", "motivation": "API的普及带来了安全挑战，需要系统化和可扩展的解决方案来实现安全的认证和授权。", "method": "本文介绍了构建基于令牌的API安全系统所需的基础知识，包括必要组件、OAuth 2.0集成、令牌架构的可扩展性、密码学基础和持久化策略。此外，还探讨了令牌生命周期管理、范围定义、过期策略和撤销机制的最佳实践。", "result": "通过遵循这些原则，开发人员可以建立一个强大的基线，同时保持灵活性以定制其特定领域的需求。旨在为开发人员提供构建安全、可扩展的基于令牌的API安全系统所需的基础知识。", "conclusion": "本文旨在为开发人员提供构建安全、可扩展的基于令牌的API安全系统所需的基础知识，以应对不断演变的安全威胁。", "translation": "API（应用程序编程接口）或Web服务是实现互联系统的基础构建模块。然而，API的普及也带来了安全挑战，需要系统化和可扩展的解决方案来实现安全的认证和授权。本文介绍了构建此类基于令牌的API安全系统所需的基础知识。它讨论了必要的组件、OAuth 2.0的集成、令牌架构的可扩展性、必要的加密基础以及持久化策略，以确保安全和弹性的操作。除了架构方面的考虑，本文还探讨了令牌生命周期管理、范围定义、过期策略和撤销机制的最佳实践，所有这些都置于实际场景中。通过遵循这些原则，开发人员可以建立一个强大的基线，同时保持灵活性以定制其特定领域的需求。该方法不声称涵盖了各种架构所需的所有变体，而是侧重于任何标准API令牌认证系统必不可少的关键原则。在整个过程中，本文强调了平衡实际考虑与安全要求，并使用了CIA三元组、OAuth标准、安全令牌生命周期以及保护敏感用户和应用程序数据的实践等关键概念。其目的是为开发人员提供构建安全、可扩展的基于令牌的API安全系统所需的基础知识，以应对不断演变的安全威胁。", "summary": "本文针对API普及带来的安全挑战，提出了一种构建强大、可扩展的基于OAuth令牌的API安全系统的方法。文章详细阐述了系统组件、OAuth 2.0集成、令牌架构的可扩展性、加密基础和持久化策略，并探讨了令牌生命周期管理、范围定义、过期策略和撤销机制等最佳实践，旨在为开发人员提供构建安全API系统的基础知识。", "keywords": "API安全, OAuth, 令牌认证, 安全架构, 最佳实践", "comments": "本文提供了一个高层次的概述，强调了构建基于OAuth令牌的API安全系统的关键原则和最佳实践。其价值在于为开发者提供了一个实用的框架，尽管它不涵盖所有特定架构的变体，但其对基础概念（如CIA三元组和OAuth标准）的强调，使其成为理解和实施稳健API安全的重要起点。"}}
{"id": "2507.17293", "title": "Data Virtualization for Machine Learning", "authors": ["Saiful Khan", "Joyraj Chakraborty", "Philip Beaucamp", "Niraj Bhujel", "Min Chen"], "categories": ["cs.SE", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17293v1", "summary": "Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17293v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "机器学习数据虚拟化", "tldr": "机器学习工作流复杂且数据量大，数据虚拟化是关键技术。本文介绍了一个数据虚拟化服务的设计与实现，该服务已支持多个ML应用并具可扩展性。", "motivation": "机器学习团队面临多并发、耗时长的ML工作流，涉及大量中间数据的存储、处理和维护，因此需要数据虚拟化技术来支持这些复杂的工作流。", "method": "本文介绍了数据虚拟化服务的设计和实现，重点关注其服务架构和服务操作。", "result": "该基础设施目前支持六个ML应用，每个应用都有多个ML工作流。", "conclusion": "该数据虚拟化服务允许未来应用程序和工作流的数量持续增长。", "translation": "如今，机器学习（ML）团队针对不同的应用拥有多个并发的ML工作流。每个工作流通常涉及大量的实验、迭代和协作活动，从初始数据整理到模型部署通常需要数月甚至数年。在组织层面，有大量的中间数据需要存储、处理和维护。数据虚拟化成为支持ML工作流基础设施中的一项关键技术。在本文中，我们介绍了数据虚拟化服务的设计和实现，重点关注其服务架构和服务操作。该基础设施目前支持六个ML应用，每个应用都包含多个ML工作流。该数据虚拟化服务允许未来应用程序和工作流的数量持续增长。", "summary": "针对机器学习工作流的复杂性和数据管理挑战，本文提出并详细阐述了一个数据虚拟化服务的设计与实现。该服务作为关键基础设施组件，旨在支持和优化多并发的ML应用，并通过其可扩展的架构，已成功支持多个现有ML应用，并能适应未来应用和工作流的增长需求。", "keywords": "数据虚拟化, 机器学习, 工作流, 服务架构, 数据管理", "comments": "这篇论文的创新点在于将数据虚拟化技术应用于复杂的机器学习工作流管理，解决了ML团队在数据存储、处理和维护方面的挑战。其重要性在于提供了一个可扩展的解决方案，能够提升ML开发效率和数据治理能力。"}}
{"id": "2507.17029", "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "categories": ["cs.GR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      12 pages, 15 Figures", "url": "http://arxiv.org/abs/2507.17029v1", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.", "comment": "12 pages, 15 Figures", "pdf_url": "http://arxiv.org/pdf/2507.17029v1", "cate": "cs.GR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "StreamME：在直播流中简化3D高斯头像", "tldr": "StreamME是一种基于3D高斯泼溅的实时3D头像重建方法，通过即时训练和点云简化，实现快速、高质量的面部表情适应，并保护隐私、减少带宽。", "motivation": "StreamME旨在解决现有3D头像重建方法在实时性、数据预处理方面的限制，实现从直播流中快速、同步地重建3D头像，并能无缝集成到下游应用中。同时，它也致力于保护面部隐私并降低VR系统或在线会议的通信带宽。", "method": "StreamME方法基于3D高斯泼溅（3DGS），采用“即时训练”（on-the-fly training）策略，同步记录和重建直播视频流中的头部头像，无需预缓存数据。它消除了可变形3DGS中对MLP的依赖，仅依靠几何体来显著提高对面部表情的适应速度。此外，为了提高即时训练的效率，该方法引入了基于主点的简化策略，使点云在面部表面更稀疏分布，在优化点数量的同时保持渲染质量。", "result": "StreamME实现了从直播视频流中快速、同步地重建3D头部头像，无需预缓存数据，并能无缝集成到下游应用中。它显著提高了对面部表情的适应速度，同时通过点云简化优化了点数量并保持了渲染质量。", "conclusion": "StreamME通过其独特的即时训练和点云简化策略，提供了一种高效、实时的3D高斯头像重建方案，有效保护了用户隐私并降低了带宽需求，具有广泛的下游应用潜力。", "translation": "我们提出了StreamME，一种专注于快速3D头像重建的方法。StreamME同步记录并从直播视频流中重建头部头像，无需任何预缓存数据，从而使重建的外观能够无缝集成到下游应用程序中。这种我们称之为“即时训练”的异常快速的训练策略是我们方法的核心。我们的方法建立在3D高斯泼溅（3DGS）之上，消除了可变形3DGS中对MLP的依赖，仅依靠几何体，这显著提高了对面部表情的适应速度。为了进一步确保即时训练的高效率，我们引入了一种基于主点的简化策略，该策略使点云在面部表面更稀疏地分布，在优化点数量的同时保持渲染质量。利用即时训练能力，我们的方法保护了面部隐私并减少了VR系统或在线会议中的通信带宽。此外，它还可以直接应用于动画、卡通化和重光照等下游应用程序。更多详情请参阅我们的项目页面：https://songluchuan.github.io/StreamME/。", "summary": "StreamME是一种创新的3D头像重建方法，专为直播流设计。它利用“即时训练”策略，基于3D高斯泼溅技术，实现了从实时视频中同步、快速地重建头部头像，无需预缓存数据。该方法通过消除MLP并依赖几何体来加速面部表情适应，并通过点云简化策略优化了效率和渲染质量。StreamME不仅保护了用户隐私并减少了带宽，还支持动画、卡通化和重光照等多种下游应用。", "keywords": "3D高斯泼溅, 实时头像重建, 即时训练, 点云简化, 直播流", "comments": "StreamME的创新点在于其“即时训练”策略和基于主点的点云简化方法，使其能够在直播流场景下实现高速、高质量的3D头像重建，解决了传统方法在实时性、数据预处理方面的限制。它在保护隐私和降低带宽方面的实用性也增加了其重要性。"}}
{"id": "2507.17749", "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users", "authors": ["Weixin Chen", "Yuhan Zhao", "Li Chen", "Weike Pan"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by RecSys 2025", "url": "http://arxiv.org/abs/2507.17749v1", "summary": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.", "comment": "Accepted by RecSys 2025", "pdf_url": "http://arxiv.org/pdf/2507.17749v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "不抛弃不放弃：针对非重叠用户的公平跨域推荐系统", "tldr": "本文揭示了现有跨域推荐系统对非重叠用户的偏见，并提出了一种新颖的方法，通过生成虚拟用户来提高非重叠用户的推荐质量，同时不影响整体准确性。", "motivation": "现有跨域推荐（CDR）方法主要利用重叠用户进行知识迁移，导致非重叠用户受益甚微甚至性能下降，造成不公平性，可能侵蚀用户信任并负面影响业务参与度和收入。", "method": "提出了一种为非重叠目标域用户生成虚拟源域用户的新颖解决方案。该方法利用双重注意力机制识别重叠用户和非重叠用户之间的相似性，从而合成逼真的虚拟用户嵌入。此外，引入了一个限制器组件，确保生成的虚拟用户与真实数据分布对齐，同时保留每个用户的独特特征。该方法是模型无关的，可以无缝集成到任何CDR模型中。", "result": "在三个公共数据集上，使用五个CDR基线进行的综合实验表明，所提出的方法有效地缓解了CDR非重叠用户偏见，且没有损失整体准确性。", "conclusion": "本文提出的方法成功解决了跨域推荐系统中非重叠用户所面临的公平性问题，通过生成虚拟用户显著提升了他们的推荐体验，同时保持了整体系统的性能。", "translation": "跨域推荐（CDR）方法主要利用重叠用户将知识从源域转移到目标域。然而，通过实证研究，我们发现这些方法存在一个关键的内在偏见：虽然重叠用户在推荐质量上获得了显著提升，但非重叠用户受益甚微，甚至面临性能下降。这种不公平性可能会侵蚀用户信任，从而对业务参与度和收入产生负面影响。为了解决这个问题，我们提出了一种新颖的解决方案，为非重叠目标域用户生成虚拟源域用户。我们的方法利用双重注意力机制识别重叠用户和非重叠用户之间的相似性，从而合成逼真的虚拟用户嵌入。我们进一步引入了一个限制器组件，确保生成的虚拟用户与真实数据分布对齐，同时保留每个用户的独特特征。值得注意的是，我们的方法是模型无关的，可以无缝集成到任何CDR模型中。在三个公共数据集上，使用五个CDR基线进行的综合实验表明，我们的方法有效地缓解了CDR非重叠用户偏见，且没有损失整体准确性。我们的代码已在https://github.com/WeixinChen98/VUG公开可用。", "summary": "本文针对现有跨域推荐系统对非重叠用户存在的推荐质量下降和不公平问题，提出了一种创新性的解决方案。该方案通过为非重叠目标域用户生成虚拟源域用户，并利用双重注意力机制和限制器组件来确保虚拟用户嵌入的真实性和独特性。实验证明，该模型无关的方法能够有效缓解非重叠用户偏见，同时保持整体推荐精度。", "keywords": "跨域推荐, 公平性, 非重叠用户, 虚拟用户生成, 双重注意力机制", "comments": "本文创新性地关注了跨域推荐中非重叠用户的公平性问题，这是现有研究中常常被忽视的关键点。通过引入虚拟用户生成和双重注意力机制，该方法提供了一个通用且有效的解决方案，能够集成到多种CDR模型中，具有很强的实用价值和影响力。"}}
{"id": "2505.10016", "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "authors": ["Shijie Lyu"], "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This submission included authors who did not consent to the submission. The paper is being withdrawn until authorship issues are resolved", "url": "http://arxiv.org/abs/2505.10016v2", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "comment": "This submission included authors who did not consent to the\n  submission. The paper is being withdrawn until authorship issues are resolved", "pdf_url": "http://arxiv.org/pdf/2505.10016v2", "cate": "cs.CV", "date": "2025-05-15", "updated": "2025-07-23", "AI": {"title_translation": "YOLOv8在单目俯视多车辆目标检测中的应用", "tldr": "本文提出了一种基于YOLOv8改进的自动驾驶目标检测网络，通过集成结构重参数化技术、双向金字塔结构网络模型和新型检测管道，实现了高效精准的多尺度、小目标和远距离目标检测，并在实验中取得了65%的检测精度，显示出在自动驾驶领域的应用潜力。", "motivation": "现有的自动驾驶目标检测技术（如雷达、摄像头、车辆传感器网络）面临高成本、易受天气和光照条件影响以及分辨率有限等挑战，因此需要改进以提高驾驶安全、交通效率和应急响应能力。", "method": "本文提出了一种基于YOLOv8的改进型自动目标检测网络。具体方法包括：将结构重参数化技术集成到YOLOv8框架中；引入双向金字塔结构网络模型；以及采用一种新型的检测管道。", "result": "实验结果表明，改进后的模型能够有效检测大型和小型物体，检测精度达到65%，相较于传统方法有显著提升。", "conclusion": "该改进模型在实际应用中具有巨大潜力，特别适用于自动驾驶竞赛（如中国大学生方程式汽车大赛FSAC）中的单目标和小目标检测场景。", "translation": "自动驾驶技术正在逐步改变传统的汽车驾驶方式，标志着现代交通的一个重要里程碑。目标检测作为自动驾驶系统的基石，在提高驾驶安全性、实现自动驾驶功能、提高交通效率和促进有效的应急响应方面发挥着至关重要的作用。然而，当前的环境感知雷达、道路感知摄像头和车辆传感器网络等技术面临着显著挑战，包括高成本、易受天气和光照条件影响以及分辨率有限等问题。为了解决这些这些局限性，本文提出了一种基于YOLOv8改进的自动目标检测网络。通过将结构重参数化技术、双向金字塔结构网络模型和新型检测管道集成到YOLOv8框架中，所提出的方法实现了对多尺度、小目标和远距离目标的高效精准检测。实验结果表明，增强模型可以有效地检测大型和小型物体，检测精度达到65%，与传统方法相比取得了显著进展。这种改进的模型在实际应用中具有巨大的潜力，非常适合自动驾驶竞赛，例如中国大学生方程式汽车大赛（FSAC），特别是在涉及单目标和小目标检测的场景中表现出色。", "summary": "本文针对现有自动驾驶目标检测技术面临的成本高、受环境影响大、分辨率低等问题，提出了一种基于YOLOv8的改进型目标检测网络。该方法通过集成结构重参数化、双向金字塔结构网络模型和新型检测管道，显著提升了对多尺度、小目标及远距离目标的检测效率和精度。实验结果显示，改进模型对大小目标的检测精度达到65%，优于传统方法，展现了在自动驾驶应用和竞赛中的广阔前景。", "keywords": "YOLOv8, 目标检测, 自动驾驶, 多尺度检测, 小目标检测", "comments": "本文的创新点在于将结构重参数化技术、双向金字塔结构网络模型和新型检测管道集成到YOLOv8框架中，以解决自动驾驶中多尺度、小目标和远距离检测的挑战。其重要性在于提升了自动驾驶系统的环境感知能力和安全性。然而，摘要中未提及具体的模型改进细节或与其他SOTA方法的详细比较数据，65%的检测精度在某些高精度要求的自动驾驶场景中可能仍有提升空间。"}}
{"id": "2507.17702", "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17702v1", "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17702v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "实现更大杠杆：高效专家混合语言模型的缩放定律", "tldr": "本文提出了效率杠杆（EL）指标，并通过大规模实证研究揭示了MoE模型配置与EL之间的缩放定律，验证了其在节省计算资源的同时保持性能的有效性。", "motivation": "MoE模型虽然能将总参数与计算成本解耦，但其模型容量预测仍是未解决的挑战，尤其是关于专家激活比和粒度。本文旨在解决这一预测空白。", "method": "引入效率杠杆（EL）作为衡量MoE模型计算优势的指标。进行了大规模实证研究，训练了超过300个模型（高达28B参数），系统性地调查MoE架构配置（专家激活比、总计算预算、专家粒度）与EL的关系。将发现整合为统一的缩放定律。通过训练Ling-mini-beta模型（0.85B活跃参数）并与6.1B密集模型进行比较来验证缩放定律。", "result": "效率杠杆（EL）主要由专家激活比和总计算预算驱动，两者都遵循可预测的幂律。专家粒度作为非线性调节器，具有清晰的最优范围。这些发现被整合到一个统一的缩放定律中，能够准确预测MoE架构的EL。Ling-mini-beta模型在相同数据集上训练，性能与6.1B密集模型匹配，但计算资源消耗减少了7倍以上。", "conclusion": "这项工作为高效MoE模型的缩放提供了一个有原则且基于实证的基础，并成功验证了所提出的缩放定律能够有效指导MoE模型的配置，以实现显著的计算效率提升。", "translation": "专家混合（MoE）已成为通过将总参数与计算成本解耦来有效扩展大型语言模型（LLM）的主导架构。然而，这种解耦带来了一个关键挑战：预测给定MoE配置（例如，专家激活比和粒度）的模型容量仍然是一个未解决的问题。为了解决这一空白，我们引入了效率杠杆（EL），一个量化MoE模型相对于同等密集模型的计算优势的指标。我们进行了一项大规模实证研究，训练了超过300个模型，参数量高达280亿，系统地调查了MoE架构配置与EL之间的关系。我们的研究结果表明，EL主要由专家激活比和总计算预算驱动，两者都遵循可预测的幂律，而专家粒度则作为一个非线性调节器，具有明确的最优范围。我们将这些发现整合到一个统一的缩放定律中，该定律能够根据MoE架构的配置准确预测其EL。为了验证我们推导出的缩放定律，我们设计并训练了Ling-mini-beta，一个Ling-2.0系列的试点模型，仅有0.85B活跃参数，同时训练了一个6.1B的密集模型进行比较。在相同的1T高质量token数据集上训练时，Ling-mini-beta的性能与6.1B密集模型匹配，同时消耗的计算资源减少了7倍以上，从而证实了我们缩放定律的准确性。这项工作为高效MoE模型的缩放提供了一个有原则且基于实证的基础。", "summary": "该研究解决了混合专家（MoE）语言模型中预测模型容量的挑战，引入了“效率杠杆”（EL）指标来量化MoE的计算优势。通过对300多个模型的广泛实证研究，作者揭示了EL主要受专家激活比和总计算预算的幂律驱动，并发现专家粒度存在一个最优范围。这些发现被整合到一个统一的缩放定律中。通过训练名为Ling-mini-beta的试点模型，研究证明其在性能与6.1B密集模型相当的同时，计算资源消耗减少了7倍以上，从而验证了所提出的缩放定律的准确性和实用性。", "keywords": "专家混合模型, 缩放定律, 效率杠杆, 大语言模型, 计算效率", "comments": "本文提出了一个量化MoE模型计算效率的创新指标——效率杠杆（EL），并首次系统性地揭示了MoE架构配置（如专家激活比、计算预算和专家粒度）与效率之间的缩放定律。其通过大规模实证研究和实际模型验证，为MoE模型的参数配置和高效扩展提供了坚实的理论和实践基础，对于未来大型语言模型的节能训练具有重要指导意义。"}}
{"id": "2507.16845", "title": "Enhancing Lung Disease Diagnosis via Semi-Supervised Machine Learning", "authors": ["Xiaoran Xua", "In-Ho Rab", "Ravi Sankarc"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16845v1", "summary": "Lung diseases, including lung cancer and COPD, are significant health\nconcerns globally. Traditional diagnostic methods can be costly,\ntime-consuming, and invasive. This study investigates the use of semi\nsupervised learning methods for lung sound signal detection using a model\ncombination of MFCC+CNN. By introducing semi supervised learning modules such\nas Mix Match, Co-Refinement, and Co Refurbishing, we aim to enhance the\ndetection performance while reducing dependence on manual annotations. With the\nadd-on semi-supervised modules, the accuracy rate of the MFCC+CNN model is\n92.9%, an increase of 3.8% to the baseline model. The research contributes to\nthe field of lung disease sound detection by addressing challenges such as\nindividual differences, feature insufficient labeled data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16845v1", "cate": "eess.AS", "date": "2025-07-20", "updated": "2025-07-20", "AI": {"title_translation": "通过半监督机器学习增强肺部疾病诊断", "tldr": "本研究通过引入MixMatch、Co-Refinement和Co-Refurbishing等半监督学习模块，提升了MFCC+CNN模型在肺音信号检测中的性能，将准确率提高到92.9%，同时减少了对手动标注的依赖，解决了标记数据不足等挑战。", "motivation": "肺部疾病（包括肺癌和慢性阻塞性肺病）是全球性的重大健康问题。传统的诊断方法成本高、耗时且具有侵入性。此外，在肺部疾病声音检测领域，存在个体差异和标记数据不足等挑战。", "method": "本研究采用MFCC+CNN模型组合进行肺音信号检测，并通过引入MixMatch、Co-Refinement和Co-Refurbishing等半监督学习模块来增强检测性能。", "result": "加入半监督模块后，MFCC+CNN模型的准确率达到92.9%，比基线模型提高了3.8%。", "conclusion": "本研究通过解决个体差异和标记数据不足等挑战，为肺部疾病声音检测领域做出了贡献。", "translation": "肺部疾病，包括肺癌和慢性阻塞性肺病，是全球性的重大健康问题。传统的诊断方法成本高、耗时且具有侵入性。本研究探讨了使用半监督学习方法结合MFCC+CNN模型进行肺音信号检测。通过引入MixMatch、Co-Refinement和Co-Refurbishing等半监督学习模块，我们旨在提高检测性能，同时减少对手动标注的依赖。加入半监督模块后，MFCC+CNN模型的准确率达到92.9%，比基线模型提高了3.8%。本研究通过解决个体差异和标记数据不足等挑战，为肺部疾病声音检测领域做出了贡献。", "summary": "本研究旨在通过应用半监督学习方法改进肺部疾病的诊断，以应对传统诊断方法的局限性以及数据标注的挑战。研究团队将MixMatch、Co-Refinement和Co-Refurbishing等半监督模块集成到MFCC+CNN模型中，用于肺音信号检测。实验结果表明，结合半监督模块后，模型的准确率达到了92.9%，相较于基线模型提升了3.8%，有效减少了对大量手动标注数据的依赖，并解决了肺部疾病声音检测中存在的个体差异和标记数据不足的问题。", "keywords": "肺部疾病诊断, 半监督学习, 肺音检测, MFCC+CNN", "comments": "本论文的创新之处在于将多种半监督学习方法（MixMatch、Co-Refinement、Co-Refurbishing）应用于肺音信号检测，有效解决了医疗领域中常见的数据标注成本高昂和标记数据不足的问题。通过提高诊断准确率并降低对人工标注的依赖，该研究为肺部疾病的早期、低成本诊断提供了新的途径，具有重要的实践意义。"}}
{"id": "2507.16887", "title": "Revisiting Pre-trained Language Models for Vulnerability Detection", "authors": ["Youpeng Li", "Weiliang Qi", "Xuyu Wang", "Fuxun Yu", "Xinda Wang"], "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16887v1", "summary": "The rapid advancement of pre-trained language models (PLMs) has demonstrated\npromising results for various code-related tasks. However, their effectiveness\nin detecting real-world vulnerabilities remains a critical challenge. % for the\nsecurity community. While existing empirical studies evaluate PLMs for\nvulnerability detection (VD), their inadequate consideration in data\npreparation, evaluation setups, and experimental settings undermines the\naccuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,\nan extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and\nlarge-scale PLMs using newly constructed datasets. Specifically, we compare the\nperformance of PLMs under both fine-tuning and prompt engineering, assess their\neffectiveness and generalizability across various training and testing\nsettings, and analyze their robustness against code normalization, abstraction,\nand semantic-preserving transformations.\n  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks\ndesigned to capture the syntactic and semantic patterns of code outperform both\ngeneral-purpose PLMs and those solely pre-trained or fine-tuned on large code\ncorpora. However, these models face notable challenges in real-world scenarios,\nsuch as difficulties in detecting vulnerabilities with complex dependencies,\nhandling perturbations introduced by code normalization and abstraction, and\nidentifying semantic-preserving vulnerable code transformations. Also, the\ntruncation caused by the limited context windows of PLMs can lead to a\nnon-negligible amount of labeling errors. This study underscores the importance\nof thorough evaluations of model performance in practical scenarios and\noutlines future directions to help enhance the effectiveness of PLMs for\nrealistic VD applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16887v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "重新审视预训练语言模型在漏洞检测中的应用", "tldr": "本文对17种预训练语言模型（PLMs）在漏洞检测（VD）任务中的性能进行了广泛评估，发现针对代码语法语义模式设计的PLMs表现更好，但它们在实际场景中仍面临复杂依赖、代码转换和有限上下文窗口等挑战。", "motivation": "尽管预训练语言模型（PLMs）在各种代码相关任务中表现出 promising 的结果，但它们在检测实际漏洞方面的有效性仍然是一个关键挑战。现有研究在数据准备、评估设置和实验配置方面考虑不足，影响了评估的准确性和全面性。", "method": "本文引入了 RevisitVD，利用新构建的数据集对17种预训练语言模型（PLMs），包括小型代码专用PLMs和大型PLMs，进行了广泛评估。具体来说，研究比较了PLMs在微调和提示工程下的性能，评估了它们在不同训练和测试设置下的有效性和泛化能力，并分析了它们对代码规范化、抽象化和语义保留转换的鲁棒性。", "result": "研究发现，对于漏洞检测任务，结合了旨在捕获代码语法和语义模式的预训练任务的PLMs，其表现优于通用PLMs以及仅在大型代码语料库上预训练或微调的PLMs。然而，这些模型在实际场景中面临显著挑战，例如难以检测具有复杂依赖关系的漏洞、难以处理代码规范化和抽象化引入的扰动，以及难以识别语义保留的漏洞代码转换。此外，PLMs有限上下文窗口导致的截断可能导致不可忽略的标注错误。", "conclusion": "本研究强调了在实际场景中对模型性能进行彻底评估的重要性，并提出了未来的研究方向，以帮助提高PLMs在实际漏洞检测应用中的有效性。", "translation": "预训练语言模型（PLMs）的快速发展在各种代码相关任务中展现出 promising 的结果。然而，它们在检测实际漏洞方面的有效性仍然是一个关键挑战。现有经验研究评估了PLMs在漏洞检测（VD）中的应用，但它们在数据准备、评估设置和实验配置方面考虑不足，这损害了评估的准确性和全面性。本文引入了 RevisitVD，利用新构建的数据集对17种PLMs，包括小型代码专用PLMs和大型PLMs，进行了广泛评估。具体来说，我们比较了PLMs在微调和提示工程下的性能，评估了它们在各种训练和测试设置下的有效性和泛化能力，并分析了它们对代码规范化、抽象化和语义保留转换的鲁棒性。\n\n我们的研究结果表明，对于漏洞检测任务，结合了旨在捕获代码语法和语义模式的预训练任务的PLMs，其表现优于通用PLMs以及仅在大型代码语料库上预训练或微调的PLMs。然而，这些模型在实际场景中面临显著挑战，例如难以检测具有复杂依赖关系的漏洞、难以处理代码规范化和抽象化引入的扰动，以及难以识别语义保留的漏洞代码转换。此外，PLMs有限上下文窗口导致的截断可能导致不可忽略的标注错误。本研究强调了在实际场景中对模型性能进行彻底评估的重要性，并提出了未来的研究方向，以帮助提高PLMs在实际漏洞检测应用中的有效性。", "summary": "本文对17种预训练语言模型（PLMs）在漏洞检测（VD）任务中的有效性进行了全面评估，旨在解决现有研究在数据和评估设置上的不足。研究使用新构建的数据集，比较了PLMs在微调和提示工程下的表现，并分析了其泛化能力和对代码转换的鲁棒性。结果显示，针对代码语法语义模式设计的PLMs表现优于通用PLMs，但在实际复杂漏洞、代码扰动和有限上下文窗口方面仍面临挑战。研究强调了在实际场景中进行彻底评估的重要性，并为未来研究指明了方向。", "keywords": "预训练语言模型, 漏洞检测, 代码安全, 模型评估, 鲁棒性", "comments": "本文通过对大量预训练语言模型进行全面且细致的评估，揭示了它们在实际漏洞检测任务中的潜力和局限性。其创新之处在于构建了新的数据集并考虑了多种评估设置和代码转换的鲁棒性，这对于深入理解PLMs在安全领域的应用至关重要。研究结果不仅肯定了特定PLMs的优势，也明确指出了它们在处理复杂场景和代码扰动时的不足，为未来的研究提供了明确的方向，强调了实际应用中彻底评估的必要性。"}}
{"id": "2507.17695", "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein"], "categories": ["cs.AI", "cs.NI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Submitted to Computer Networks AI for 6G", "url": "http://arxiv.org/abs/2507.17695v1", "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.", "comment": "Submitted to Computer Networks AI for 6G", "pdf_url": "http://arxiv.org/pdf/2507.17695v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "共生智能体：可信AGI驱动网络的新范式", "tldr": "本文提出了一种结合大语言模型和实时优化算法的“共生智能体”新范式，旨在为可信AGI驱动网络提供支持，并在5G测试平台上验证了其在降低决策错误、减少资源消耗和优化RAN资源利用方面的有效性。", "motivation": "6G网络的发展需要基于大语言模型（LLM）的自主智能体来赋能实时决策，实现从专业化AI到通用人工智能（AGI）驱动网络的转变。然而，单独的LLM代理可能存在决策误差和资源消耗问题，因此需要一种新的范式来构建可信赖的AGI驱动网络。", "method": "本文引入了一种名为“共生智能体”的新型代理范式，它将LLM与实时优化算法相结合，以实现可信赖的AI。优化器在LLM的输入层提供有界的不确定性引导以处理数值精确的任务，而在输出层由LLM监督的优化器则实现自适应实时控制。作者设计并实现了两种新型代理：(i) 无线接入网络优化器和 (ii) 服务水平协议（SLAs）的多智能体协商器。他们还提出了一种用于AGI网络的端到端架构，并在捕获移动车辆信道波动的5G测试平台上进行了评估。", "result": "实验结果表明，与独立的基于LLM的智能体相比，共生智能体将决策错误减少了五倍。同时，小型语言模型（SLM）在GPU资源开销降低99.9%的情况下，也能达到相似的准确性，并且在82毫秒的近实时循环中运行。在真实测试平台上进行的多智能体协作RAN演示突出了服务水平协议和资源分配的显著灵活性，将RAN的过度利用率降低了约44%。", "conclusion": "共生范式可以作为下一代AGI驱动网络系统的基础，这些系统即使在LLM不断发展的情况下也能保持适应性、高效性和可信赖性。", "translation": "大型语言模型（LLM）驱动的自主智能体有望在6G网络演进中发挥至关重要的作用，通过赋能面向终端用户的管理和服务提供的实时决策。这种转变促进了从专业化智能方法（即人工智能（AI）算法处理孤立任务）向通用人工智能（AGI）驱动网络的转变，在AGI网络中，智能体拥有更广泛的推理能力，并能管理多样化的网络功能。在本文中，我们引入了一种新颖的智能体范式，将LLM与实时优化算法相结合，以实现可信赖的AI，我们称之为共生智能体。LLM输入层面的优化器为数值精确的任务提供有界的不确定性引导，而由LLM监督的输出层面的优化器则实现自适应实时控制。我们设计并实现了两种新型智能体，包括：(i) 无线接入网络优化器，以及 (ii) 用于服务水平协议（SLAs）的多智能体协商器。我们进一步提出了一种用于AGI网络的端到端架构，并在捕获移动车辆信道波动的5G测试平台上对其进行了评估。结果表明，与独立的基于LLM的智能体相比，共生智能体将决策错误减少了五倍，而小型语言模型（SLM）在GPU资源开销降低99.9%的情况下，也能达到相似的准确性，并且在82毫秒的近实时循环中运行。在真实测试平台上进行的多智能体协作RAN演示突出了服务水平协议和资源分配的显著灵活性，将RAN的过度利用率降低了约44%。基于我们的发现和开源实现，我们将共生范式引入作为下一代AGI驱动网络系统的基础——这些系统旨在即使在LLM不断发展的情况下也能保持适应性、高效性和可信赖性。", "summary": "本文提出了一种名为“共生智能体”的新型范式，旨在构建可信赖的通用人工智能（AGI）驱动网络。该范式将大型语言模型（LLM）与实时优化算法结合，通过在输入和输出层引入优化器来提升LLM在数值任务中的精确性和实时控制的自适应性。研究设计了两种智能体类型：无线接入网络优化器和多智能体服务水平协议协商器，并在5G测试平台上验证了其端到端架构。实验结果显示，共生智能体能显著减少决策错误，且使用小型语言模型（SLM）也能在大幅降低资源消耗的同时保持高性能。此外，该范式在多智能体协作中展现出优化资源分配的潜力，为未来可信、高效、自适应的AGI网络奠定了基础。", "keywords": "共生智能体, AGI网络, 大语言模型, 实时优化, 6G", "comments": "该论文提出了一种创新的“共生智能体”范式，有效结合了LLM的通用推理能力与优化算法的精确控制，解决了LLM在特定数值任务和实时控制中的局限性。其创新之处在于将优化器集成到LLM的输入和输出层面，形成协同工作机制，显著提升了AGI驱动网络的可靠性和效率。特别值得关注的是，通过使用小型语言模型（SLM）实现了显著的资源节约，这对于实际部署具有重要意义。该研究不仅提出了理论框架，还在实际5G测试平台上进行了验证，增强了其说服力。这一范式为未来6G及更先进网络中的可信AI应用提供了坚实基础。"}}
{"id": "2507.17177", "title": "Dynamics of temporal influence in polarised networks", "authors": ["Caroline B. Pena", "David J. P. O'Sullivan", "Pádraig MacCarron", "Akrati Saxena"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      35 pages, 16 figures", "url": "http://arxiv.org/abs/2507.17177v1", "summary": "In social networks, it is often of interest to identify the most influential\nusers who can successfully spread information to others. This is particularly\nimportant for marketing (e.g., targeting influencers for a marketing campaign)\nand to understand the dynamics of information diffusion (e.g., who is the most\ncentral user in the spreading of a certain type of information). However,\ndifferent opinions often split the audience and make the network polarised. In\npolarised networks, information becomes soiled within communities in the\nnetwork, and the most influential user within a network might not be the most\ninfluential across all communities. Additionally, influential users and their\ninfluence may change over time as users may change their opinion or choose to\ndecrease or halt their engagement on the subject. In this work, we aim to study\nthe temporal dynamics of users' influence in a polarised social network. We\ncompare the stability of influence ranking using temporal centrality measures,\nwhile extending them to account for community structure across a number of\nnetwork evolution behaviours. We show that we can successfully aggregate nodes\ninto influence bands, and how to aggregate centrality scores to analyse the\ninfluence of communities over time. A modified version of the temporal\nindependent cascade model and the temporal degree centrality perform the best\nin this setting, as they are able to reliably isolate nodes into their bands.", "comment": "35 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.17177v1", "cate": "cs.SI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "极化网络中时间影响力动态", "tldr": "本文研究了极化社交网络中用户影响力的时间动态，并通过扩展时间中心性度量来评估影响力排名的稳定性，同时考虑社群结构和网络演化行为，发现修改后的时间独立级联模型和时间度中心性模型在此背景下表现最佳。", "motivation": "在社交网络中识别有影响力用户对市场营销和信息传播动力学至关重要。然而，在极化网络中，信息传播受限于社区内部，且用户影响力会随时间变化。因此，需要研究极化网络中用户影响力的时间动态。", "method": "研究通过使用时间中心性度量来比较影响力排名的稳定性，并将其扩展以考虑社群结构和多种网络演化行为。此外，还涉及将节点聚合到影响力带中，并聚合中心性分数以分析社群随时间的影响力。", "result": "研究表明能够成功地将节点聚合到影响力带中，并能通过聚合中心性分数来分析社群随时间的影响力。其中，修改后的时间独立级联模型和时间度中心性模型在这种设置下表现最佳，因为它们能够可靠地将节点分离到其影响力带中。", "conclusion": "本文成功研究了极化社交网络中用户影响力的时间动态，并提出了有效的方法来分析和识别随时间变化的影响力节点及社群，其中修改后的时间独立级联模型和时间度中心性模型表现出最佳性能。", "translation": "在社交网络中，识别能够成功向他人传播信息的最具影响力的用户通常很有意义。这对于市场营销（例如，针对营销活动的目标影响者）和理解信息传播的动态（例如，谁是传播某种类型信息中最核心的用户）尤其重要。然而，不同的意见常常将受众分裂，使网络极化。在极化网络中，信息在网络中的社区内部被隔离，网络中最有影响力的用户可能不是跨所有社区最有影响力的。此外，有影响力的用户及其影响力可能会随时间而变化，因为用户可能会改变他们的意见或选择减少或停止对某个主题的参与。在这项工作中，我们旨在研究极化社交网络中用户影响力的时间动态。我们比较了使用时间中心性度量的影响力排名的稳定性，同时将其扩展以解释许多网络演化行为中的社群结构。我们表明，我们可以成功地将节点聚合成影响力带，以及如何聚合中心性分数以分析社群随时间的影响力。修改后的时间独立级联模型和时间度中心性模型在这种设置下表现最佳，因为它们能够可靠地将节点分离到其影响力带中。", "summary": "本研究探讨了极化社交网络中用户影响力的时间动态。鉴于在极化环境中，信息传播受限于社区内部且用户影响力会随时间变化，研究旨在通过扩展时间中心性度量来评估影响力排名的稳定性，同时考虑社群结构和网络演化。研究发现，可以有效地将节点聚合到影响力带中，并分析社群随时间的影响力。其中，修改后的时间独立级联模型和时间度中心性模型在识别影响力节点方面表现最佳。", "keywords": "极化网络, 影响力动态, 时间中心性, 信息传播, 社群结构", "comments": "这篇论文的创新点在于将时间动态和网络极化这两个重要因素结合起来研究用户影响力，这对于理解复杂社交网络中的信息传播具有重要意义。它超越了传统静态影响力分析的局限，提出了适用于动态、极化环境的方法。"}}
{"id": "2507.17185", "title": "Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification", "authors": ["M. A. Rasel", "Sameem Abdul Kareem", "Zhenli Kwan", "Nik Aimee Azizah Faheem", "Winn Hui Han", "Rebecca Kai Jan Choong", "Shin Shen Yong", "Unaizah Obaidellah"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted version. Published in Computers in Biology and Medicine, Volume 179, 2024. DOI: https://doi.org/10.1016/j.compbiomed.2024.108851", "url": "http://arxiv.org/abs/2507.17185v1", "summary": "In dermoscopic images, which allow visualization of surface skin structures\nnot visible to the naked eye, lesion shape offers vital insights into skin\ndiseases. In clinically practiced methods, asymmetric lesion shape is one of\nthe criteria for diagnosing melanoma. Initially, we labeled data for a\nnon-annotated dataset with symmetrical information based on clinical\nassessments. Subsequently, we propose a supporting technique, a supervised\nlearning image processing algorithm, to analyze the geometrical pattern of\nlesion shape, aiding non-experts in understanding the criteria of an asymmetric\nlesion. We then utilize a pre-trained convolutional neural network (CNN) to\nextract shape, color, and texture features from dermoscopic images for training\na multiclass support vector machine (SVM) classifier, outperforming\nstate-of-the-art methods from the literature. In the geometry-based experiment,\nwe achieved a 99.00% detection rate for dermatological asymmetric lesions. In\nthe CNN-based experiment, the best performance is found with 94% Kappa Score,\n95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes\n(Asymmetric, Half-Symmetric, and Symmetric).", "comment": "Accepted version. Published in Computers in Biology and Medicine,\n  Volume 179, 2024. DOI: 10.1016/j.compbiomed.2024.108851", "pdf_url": "http://arxiv.org/pdf/2507.17185v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "非对称病变检测与几何模式和CNN-SVM分类", "tldr": "本文提出了一种结合几何模式分析和CNN-SVM分类的方法，用于皮肤镜图像中的非对称病变检测，在诊断黑色素瘤方面表现出色，并优于现有技术。", "motivation": "在皮肤镜图像中，非对称病变形状是诊断黑色素瘤的关键标准之一。本研究旨在开发一种支持技术，通过分析病变形状的几何模式，帮助非专业人士理解非对称病变的诊断标准，并提高非对称病变的检测和分类准确性。", "method": "首先，根据临床评估对未标注的数据集进行对称信息标注。随后，提出了一种监督学习图像处理算法，用于分析病变形状的几何模式。接着，利用预训练的卷积神经网络（CNN）从皮肤镜图像中提取形状、颜色和纹理特征。最后，使用这些特征训练一个多类别支持向量机（SVM）分类器。", "result": "在基于几何的实验中，皮肤非对称病变检测率达到99.00%。在基于CNN的实验中，分类病变形状（非对称、半对称和对称）的最佳性能为94%的Kappa分数、95%的Macro F1分数和97%的Weighted F1分数，并且优于文献中的最先进方法。", "conclusion": "本研究提出的结合几何模式分析和CNN-SVM分类的方法能够有效检测和分类皮肤镜图像中的非对称病变，其性能优于现有最先进技术，为黑色素瘤诊断提供了有力支持，并有助于非专业人士理解相关诊断标准。", "translation": "在皮肤镜图像中，可以可视化肉眼不可见的表面皮肤结构，病变形状为皮肤疾病提供了重要的见解。在临床实践方法中，非对称病变形状是诊断黑色素瘤的标准之一。最初，我们根据临床评估，为未标注的数据集标注了对称信息。随后，我们提出了一种支持技术，即一种监督学习图像处理算法，用于分析病变形状的几何模式，帮助非专业人士理解非对称病变的标准。然后，我们利用预训练的卷积神经网络（CNN）从皮肤镜图像中提取形状、颜色和纹理特征，用于训练多类别支持向量机（SVM）分类器，其性能优于文献中的最先进方法。在基于几何的实验中，我们对皮肤非对称病变实现了99.00%的检测率。在基于CNN的实验中，在分类病变形状（非对称、半对称和对称）方面，最佳性能为94%的Kappa分数、95%的Macro F1分数和97%的Weighted F1分数。", "summary": "本文提出了一种用于皮肤镜图像中非对称病变检测的新方法，该方法对黑色素瘤诊断至关重要。它结合了用于几何模式分析的监督学习算法和CNN-SVM分类系统。该方法实现了高检测率（几何方法为99%）和强大的分类性能（CNN-SVM的Kappa为94%，Macro F1为95%，Weighted F1为97%），优于现有最先进技术，并有助于非专业人士进行诊断。", "keywords": "非对称病变, 皮肤镜图像, CNN-SVM, 黑色素瘤诊断, 几何模式", "comments": "该论文提出了一种创新的混合方法，将传统的几何模式分析与现代深度学习（CNN）和机器学习（SVM）相结合，用于关键的医学诊断任务。报告的高检测和分类率令人印象深刻，表明比现有方法有了显著改进。对帮助“非专业人士”的关注突出了其实用性和对临床实践的潜在影响，尤其是在皮肤病学专业知识可能有限的环境中。"}}
{"id": "2507.17470", "title": "Demonstration of Efficient Predictive Surrogates for Large-scale Quantum Processors", "authors": ["Wei-You Liao", "Yuxuan Du", "Xinbiao Wang", "Tian-Ci Tian", "Yong Luo", "Bo Du", "Dacheng Tao", "He-Liang Huang"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      53 pages, 15 figures, comments are welcome", "url": "http://arxiv.org/abs/2507.17470v1", "summary": "The ongoing development of quantum processors is driving breakthroughs in\nscientific discovery. Despite this progress, the formidable cost of fabricating\nlarge-scale quantum processors means they will remain rare for the foreseeable\nfuture, limiting their widespread application. To address this bottleneck, we\nintroduce the concept of predictive surrogates, which are classical learning\nmodels designed to emulate the mean-value behavior of a given quantum processor\nwith provably computational efficiency. In particular, we propose two\npredictive surrogates that can substantially reduce the need for quantum\nprocessor access in diverse practical scenarios. To demonstrate their potential\nin advancing digital quantum simulation, we use these surrogates to emulate a\nquantum processor with up to 20 programmable superconducting qubits, enabling\nefficient pre-training of variational quantum eigensolvers for families of\ntransverse-field Ising models and identification of non-equilibrium Floquet\nsymmetry-protected topological phases. Experimental results reveal that the\npredictive surrogates not only reduce measurement overhead by orders of\nmagnitude, but can also surpass the performance of conventional,\nquantum-resource-intensive approaches. Collectively, these findings establish\npredictive surrogates as a practical pathway to broadening the impact of\nadvanced quantum processors.", "comment": "53 pages, 15 figures, comments are welcome", "pdf_url": "http://arxiv.org/pdf/2507.17470v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大规模量子处理器高效预测代理的演示", "tldr": "本文介绍了预测代理，一种经典的学习模型，可以高效模拟量子处理器的行为，显著减少对实际量子处理器的需求，并被证明能大幅降低测量开销并超越传统方法，从而拓宽了先进量子处理器的应用。", "motivation": "由于大规模量子处理器的制造成本高昂且稀有，其广泛应用受到限制。为解决这一瓶颈，需要减少对实际量子处理器访问的需求。", "method": "本文引入了“预测代理”的概念，这是一种经典学习模型，旨在以可证明的计算效率模拟给定量子处理器的平均值行为。具体提出了两种预测代理，用于模拟一个拥有多达20个可编程超导量子比特的量子处理器，以实现变分量子本征求解器的高效预训练和非平衡Floquet对称保护拓扑相的识别。", "result": "实验结果表明，预测代理不仅将测量开销降低了数个数量级，而且性能超越了传统的、量子资源密集型方法。", "conclusion": "预测代理为拓宽先进量子处理器的影响提供了一条实用的途径。", "translation": "量子处理器的持续发展正在推动科学发现的突破。尽管取得了这些进展，但制造大规模量子处理器的巨大成本意味着它们在可预见的未来仍将稀有，从而限制了其广泛应用。为了解决这一瓶颈，我们引入了预测代理的概念，这是一种经典学习模型，旨在以可证明的计算效率模拟给定量子处理器的平均值行为。具体而言，我们提出了两种预测代理，可以在各种实际场景中大幅减少对量子处理器访问的需求。为了展示它们在推进数字量子模拟方面的潜力，我们使用这些代理来模拟一个拥有多达20个可编程超导量子比特的量子处理器，从而能够高效预训练横向场伊辛模型家族的变分量子本征求解器，并识别非平衡Floquet对称保护拓扑相。实验结果表明，预测代理不仅将测量开销降低了数个数量级，而且性能超越了传统的、量子资源密集型方法。总的来说，这些发现确立了预测代理是拓宽先进量子处理器影响的实用途径。", "summary": "本文提出并演示了“预测代理”的概念，这是一种经典的机器学习模型，旨在高效模拟量子处理器的行为，以克服大规模量子处理器成本高昂和稀缺的限制。通过引入两种具体的预测代理，该研究显著减少了对实际量子处理器访问的需求。实验证明，这些代理在模拟20量子比特超导处理器时，能够有效预训练变分量子本征求解器并识别拓扑相，并且大幅降低了测量开销，性能优于传统的量子资源密集型方法，为扩大先进量子处理器的应用范围提供了实用方案。", "keywords": "预测代理, 量子处理器, 量子模拟, 测量开销, 变分量子本征求解器", "comments": "这项工作非常创新地提出了“预测代理”的概念，通过经典的机器学习模型来模拟量子处理器，有效解决了当前量子计算领域面临的硬件成本高昂和资源稀缺的关键瓶颈。其在降低测量开销和超越传统方法方面的实验结果，展示了该方法在加速量子模拟和实际应用方面的巨大潜力，为量子计算的普及化提供了一条务实的路径。"}}
{"id": "2507.13833", "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training", "authors": ["Zhixin Wang", "Tianyi Zhou", "Liming Liu", "Ao Li", "Jiarui Hu", "Dian Yang", "Jinlong Hou", "Siyuan Feng", "Yuan Cheng", "Yuan Qi"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13833v2", "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13833v2", "cate": "cs.DC", "date": "2025-07-18", "updated": "2025-07-23", "AI": {"title_translation": "DistFlow：一个用于可扩展高效大型语言模型后训练的完全分布式强化学习框架", "tldr": "DistFlow是一个完全分布式的强化学习框架，通过消除中心化节点和采用多控制器范式，解决了现有框架在大规模强化学习中存在的扩展性瓶颈，实现了近乎线性的扩展性和显著的效率提升。", "motivation": "主流强化学习框架采用混合控制器架构，单控制器管理执行逻辑和数据传输，多控制器执行分布式计算，但在大规模强化学习中，微小的负载不平衡会导致显著的瓶颈，从而限制系统扩展性。为了解决这一限制，本文引入了DistFlow。", "method": "DistFlow采用多控制器范式，将数据传输和执行任务分派给所有工作节点，从而消除了中心化节点。这使得每个工作节点可以独立操作。此外，该架构将资源配置与执行逻辑解耦，允许每个工作节点拥有独特的执行流程。", "result": "DistFlow实现了卓越的线性扩展性，在数千个GPU上达到近乎线性的扩展性，并比最先进的框架提高了高达7倍的端到端吞吐量。", "conclusion": "DistFlow通过其完全分布式的架构，成功解决了大规模强化学习的扩展性瓶颈，显著提高了训练效率，并为算法实验提供了灵活性。", "translation": "强化学习（RL）已成为大型语言模型（LLM）关键的后训练技术。有效扩展强化学习是释放最强大LLM的先进推理能力并确保其安全、目标对齐行为的关键。主流框架通常采用混合控制器架构，其中一个单控制器调度整体执行逻辑并管理整体数据传输，而多控制器执行分布式计算。对于大规模强化学习，微小的负载不平衡都可能引入显著的瓶颈，最终限制系统的可扩展性。为了解决这一限制，我们引入了DistFlow，一个新颖的、完全分布式的RL框架，旨在打破扩展障碍。我们采用多控制器范式，将数据传输和执行任务分派给所有工作节点，从而消除了中心化节点。这使得每个工作节点能够独立操作，从而实现高达数千个GPU的近乎线性扩展性和显著的效率提升。此外，我们的架构将资源配置与执行逻辑解耦，允许每个工作节点拥有独特的执行流，为快速和经济高效的算法实验提供了显著的灵活性。广泛的实验表明，DistFlow实现了卓越的线性扩展性，并比最先进（SOTA）框架提高了高达7倍的端到端吞吐量。", "summary": "本文介绍了DistFlow，一个新颖的、完全分布式的强化学习（RL）框架，旨在突破大型语言模型（LLM）后训练的扩展性障碍。针对现有主流框架中单控制器导致的负载不平衡瓶颈，DistFlow采用多控制器范式，将数据传输和执行任务分派给所有工作节点，从而消除了中心化节点。这种设计允许每个工作节点独立运行，实现了高达数千个GPU的近乎线性扩展性和显著的效率提升。此外，DistFlow的架构将资源配置与执行逻辑解耦，为快速和经济高效的算法实验提供了灵活性。实验结果表明，DistFlow实现了优异的线性扩展性，并比现有最先进框架的端到端吞吐量提高了高达7倍。", "keywords": "分布式强化学习, 大型语言模型, 扩展性, 后训练, DistFlow", "comments": "DistFlow的创新之处在于其完全分布式的多控制器范式，彻底消除了中心化节点，有效解决了大规模强化学习中的扩展性瓶颈。这对于解锁大型语言模型的先进推理能力和确保其安全对齐行为至关重要。其解耦资源配置和执行逻辑的设计也为算法实验带来了极大的灵活性和成本效益，是强化学习领域的一个重要进展。"}}
{"id": "2506.00178", "title": "Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings", "authors": ["Anirudh Nair", "Adi Banerjee", "Laurent Mombaerts", "Matthew Hagen", "Tarik Borogovac"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00178v2", "summary": "Prompt engineering represents a critical bottleneck to harness the full\npotential of Large Language Models (LLMs) for solving complex tasks, as it\nrequires specialized expertise, significant trial-and-error, and manual\nintervention. This challenge is particularly pronounced for tasks involving\nsubjective quality assessment, where defining explicit optimization objectives\nbecomes fundamentally problematic. Existing automated prompt optimization\nmethods falter in these scenarios, as they typically require well-defined\ntask-specific numerical fitness functions or rely on generic templates that\ncannot capture the nuanced requirements of complex use cases. We introduce\nDEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that\nguides prompt evolution through a debate-driven evaluation with an Elo-based\nselection. Contrary to prior work, DEEVOs approach enables exploration of the\ndiscrete prompt space while preserving semantic coherence through intelligent\ncrossover and strategic mutation operations that incorporate debate-based\nfeedback, combining elements from both successful and unsuccessful prompts\nbased on identified strengths rather than arbitrary splicing. Using Elo ratings\nas a fitness proxy, DEEVO simultaneously drives improvement and preserves\nvaluable diversity in the prompt population. Experimental results demonstrate\nthat DEEVO significantly outperforms both manual prompt engineering and\nalternative state-of-the-art optimization approaches on open-ended tasks and\nclose-ended tasks despite using no ground truth feedback. By connecting LLMs\nreasoning capabilities with adaptive optimization, DEEVO represents a\nsignificant advancement in prompt optimization research by eliminating the need\nof predetermined metrics to continuously improve AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00178v2", "cate": "cs.AI", "date": "2025-05-30", "updated": "2025-07-22", "AI": {"title_translation": "提示词锦标赛：通过结构化辩论和Elo评级进化LLM指令", "tldr": "DEEVO是一个新颖的框架，通过辩论驱动的评估和基于Elo的选择来优化LLM提示词，无需预定指标，显著优于现有方法。", "motivation": "提示词工程是发挥大型语言模型（LLMs）潜力的关键瓶颈，因为它需要专业知识、大量的试错和人工干预。对于涉及主观质量评估的任务，定义明确的优化目标尤为困难。现有自动化提示词优化方法在此类场景中表现不佳，因为它们通常需要明确定义的任务特定数值适应度函数或依赖无法捕捉复杂用例细微需求的通用模板。", "method": "本文引入了DEEVO（DEbate-driven EVOlutionary prompt optimization），一个通过辩论驱动的评估和基于Elo的选择来指导提示词进化的新颖框架。DEEVO通过智能交叉和战略变异操作探索离散提示词空间，这些操作结合了基于辩论反馈的成功和不成功提示词的元素，而不是任意拼接。它使用Elo评级作为适应度代理，同时驱动改进并保持提示词群体中的多样性。", "result": "实验结果表明，DEEVO在开放式和封闭式任务上显著优于手动提示词工程和最先进的优化方法，尽管没有使用真实反馈。", "conclusion": "DEEVO通过将LLM的推理能力与自适应优化相结合，代表了提示词优化研究的重大进展，消除了持续改进AI系统对预定指标的需求。", "translation": "提示词工程是发挥大型语言模型（LLMs）解决复杂任务全部潜力的关键瓶颈，因为它需要专业知识、大量的试错和人工干预。对于涉及主观质量评估的任务，这一挑战尤为突出，因为定义明确的优化目标从根本上存在问题。现有自动化提示词优化方法在此类场景中表现不佳，因为它们通常需要明确定义的任务特定数值适应度函数或依赖无法捕捉复杂用例细微需求的通用模板。我们引入了DEEVO（DEbate-driven EVOlutionary prompt optimization），一个通过辩论驱动的评估和基于Elo的选择来指导提示词进化的新颖框架。与以往的工作相反，DEEVO的方法通过结合辩论反馈的智能交叉和战略变异操作，实现了离散提示词空间的探索，同时保持了语义连贯性，它基于识别出的优点而非任意拼接来结合成功和不成功的提示词元素。DEEVO使用Elo评级作为适应度代理，同时推动改进并保持提示词群体中宝贵的多样性。实验结果表明，尽管没有使用真实反馈，DEEVO在开放式和封闭式任务上显著优于手动提示词工程和替代的最先进优化方法。通过将LLM的推理能力与自适应优化相结合，DEEVO代表了提示词优化研究的重大进展，消除了持续改进AI系统对预定指标的需求。", "summary": "本文提出了DEEVO（DEbate-driven EVOlutionary prompt optimization），一个针对LLM提示词工程瓶颈的创新框架。该框架通过模拟辩论过程和利用Elo评级来评估和选择最佳提示词，从而实现提示词的自动化优化。DEEVO解决了传统方法在处理主观任务时对预定义数值适应度函数的依赖问题，并通过智能交叉和变异操作在离散空间中探索并保持语义连贯性。实验证明，DEEVO在没有真实反馈的情况下，在开放式和封闭式任务上均显著优于手动工程和现有优化方法，为LLM提示词优化提供了一种无需预定指标的新途径。", "keywords": "提示词工程, LLM优化, Elo评级, 辩论驱动, DEEVO", "comments": "DEEVO的创新之处在于其引入了辩论驱动的评估和Elo评级机制，有效解决了LLM提示词优化中主观任务难以定义明确适应度函数的问题。这种方法通过结合LLM自身的推理能力进行自我改进，避免了对人工干预和预设指标的依赖，是提示词工程领域的一个重要突破。其能够探索离散空间并保持语义连贯性的特性，也使其在复杂用例中具有更强的适用性。"}}
{"id": "2507.16872", "title": "CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage", "authors": ["Na Li", "Yansong Gao", "Hongsheng Hu", "Boyu Kuang", "Anmin Fu"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16872v1", "summary": "Model compression is crucial for minimizing memory storage and accelerating\ninference in deep learning (DL) models, including recent foundation models like\nlarge language models (LLMs). Users can access different compressed model\nversions according to their resources and budget. However, while existing\ncompression operations primarily focus on optimizing the trade-off between\nresource efficiency and model performance, the privacy risks introduced by\ncompression remain overlooked and insufficiently understood.\n  In this work, through the lens of membership inference attack (MIA), we\npropose CompLeak, the first privacy risk evaluation framework examining three\nwidely used compression configurations that are pruning, quantization, and\nweight clustering supported by the commercial model compression framework of\nGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has\nthree variants, given available access to the number of compressed models and\noriginal model. CompLeakNR starts by adopting existing MIA methods to attack a\nsingle compressed model, and identifies that different compressed models\ninfluence members and non-members differently. When the original model and one\ncompressed model are available, CompLeakSR leverages the compressed model as a\nreference to the original model and uncovers more privacy by combining meta\ninformation (e.g., confidence vector) from both models. When multiple\ncompressed models are available with/without accessing the original model,\nCompLeakMR innovatively exploits privacy leakage info from multiple compressed\nversions to substantially signify the overall privacy leakage. We conduct\nextensive experiments on seven diverse model architectures (from ResNet to\nfoundation models of BERT and GPT-2), and six image and textual benchmark\ndatasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16872v1", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "CompLeak：深度学习模型压缩加剧隐私泄露", "tldr": "模型压缩虽然能提高效率，但会显著加剧隐私泄露，CompLeak框架首次系统性地评估了这一风险。", "motivation": "模型压缩对于最小化深度学习模型的内存存储和加速推理至关重要，但现有压缩操作主要关注资源效率和模型性能之间的权衡，而压缩引入的隐私风险却被忽视且理解不足。", "method": "本文提出了CompLeak，第一个通过成员推断攻击（MIA）视角，检查剪枝、量化和权重聚类这三种广泛使用的压缩配置的隐私风险评估框架。CompLeak有三种变体：CompLeakNR（针对单个压缩模型）、CompLeakSR（当原始模型和单个压缩模型可用时，利用压缩模型作为参考并结合元信息）和CompLeakMR（当有多个压缩模型可用时，创新性地利用多版本信息）。实验在七种不同的模型架构（从ResNet到BERT和GPT-2等基础模型）和六个图像和文本基准数据集上进行。", "result": "CompLeakNR识别出不同的压缩模型对成员和非成员的影响不同。CompLeakSR通过将压缩模型作为原始模型的参考并结合来自两个模型的元信息，揭示了更多的隐私。CompLeakMR创新性地利用来自多个压缩版本的信息，显著地揭示了整体隐私泄露。", "conclusion": "模型压缩会加剧隐私泄露，CompLeak框架，特别是利用多个压缩模型的方法，能够有效地评估这些风险。", "translation": "模型压缩对于最小化深度学习（DL）模型（包括大型语言模型（LLM）等近期基础模型）的内存存储和加速推理至关重要。用户可以根据其资源和预算访问不同的压缩模型版本。然而，虽然现有压缩操作主要关注优化资源效率和模型性能之间的权衡，但压缩引入的隐私风险却被忽视且理解不足。\n在这项工作中，我们通过成员推断攻击（MIA）的视角，提出了CompLeak，这是第一个检查剪枝、量化和权重聚类这三种广泛使用的压缩配置的隐私风险评估框架，这些配置得到了Google TensorFlow-Lite（TF-Lite）和Facebook PyTorch Mobile等商业模型压缩框架的支持。CompLeak有三种变体，取决于可访问的压缩模型数量和原始模型。CompLeakNR通过采用现有MIA方法攻击单个压缩模型开始，并识别出不同的压缩模型对成员和非成员的影响不同。当原始模型和一个压缩模型可用时，CompLeakSR利用压缩模型作为原始模型的参考，并通过结合来自两个模型的元信息（例如，置信度向量）来揭示更多隐私。当多个压缩模型可用（无论是否访问原始模型）时，CompLeakMR创新性地利用来自多个压缩版本的隐私泄露信息，以显著揭示整体隐私泄露。我们在七种不同的模型架构（从ResNet到BERT和GPT-2等基础模型）和六个图像和文本基准数据集上进行了广泛的实验。", "summary": "本文提出了CompLeak，一个新颖的隐私风险评估框架，用于深入探究深度学习模型压缩（如剪枝、量化、权重聚类）所带来的隐私泄露问题。研究发现，尽管模型压缩能提升效率，但它显著加剧了隐私泄露，尤其当存在多个压缩模型版本时。CompLeak通过三种变体（CompLeakNR、CompLeakSR、CompLeakMR）系统性地评估了不同场景下的隐私风险，并揭示了压缩对成员和非成员的不同影响，以及如何通过多版本信息更显著地揭示整体隐私泄露。", "keywords": "模型压缩, 隐私泄露, 成员推断攻击, 深度学习, CompLeak", "comments": "这篇论文揭示了模型压缩一个被忽视但至关重要的方面——其对隐私的影响。CompLeak框架的提出及其三种变体，特别是CompLeakMR，为评估这些风险提供了一种新颖且全面的方法，鉴于压缩模型和基础模型的广泛应用，这具有高度的重要性。它推动了对模型部署中效率与隐私权衡的理解边界。"}}
{"id": "2507.17228", "title": "P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices", "authors": ["Wei Fan", "JinYi Yoon", "Xiaochang Li", "Huajie Shao", "Bo Ji"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted as invited paper in The 34th International Conference on Computer Communications and Networks (ICCCN 2025)", "url": "http://arxiv.org/abs/2507.17228v1", "summary": "Split Learning (SL) is an emerging privacy-preserving machine learning\ntechnique that enables resource constrained edge devices to participate in\nmodel training by partitioning a model into client-side and server-side\nsub-models. While SL reduces computational overhead on edge devices, it\nencounters significant challenges in heterogeneous environments where devices\nvary in computing resources, communication capabilities, environmental\nconditions, and privacy requirements. Although recent studies have explored\nheterogeneous SL frameworks that optimize split points for devices with varying\nresource constraints, they often neglect personalized privacy requirements and\nlocal model customization under varying environmental conditions. To address\nthese limitations, we propose P3SL, a Personalized Privacy-Preserving Split\nLearning framework designed for heterogeneous, resource-constrained edge device\nsystems. The key contributions of this work are twofold. First, we design a\npersonalized sequential split learning pipeline that allows each client to\nachieve customized privacy protection and maintain personalized local models\ntailored to their computational resources, environmental conditions, and\nprivacy needs. Second, we adopt a bi-level optimization technique that empowers\nclients to determine their own optimal personalized split points without\nsharing private sensitive information (i.e., computational resources,\nenvironmental conditions, privacy requirements) with the server. This approach\nbalances energy consumption and privacy leakage risks while maintaining high\nmodel accuracy. We implement and evaluate P3SL on a testbed consisting of 7\ndevices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,\nusing diverse model architectures and datasets under varying environmental\nconditions.", "comment": "Accepted as invited paper in The 34th International Conference on\n  Computer Communications and Networks (ICCCN 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17228v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "P3SL：异构边缘设备上的个性化隐私保护分体学习", "tldr": "P3SL是一种用于异构边缘设备的个性化隐私保护分体学习框架，它允许客户端根据其资源、环境和隐私需求自定义模型和分割点，无需与服务器共享敏感信息。", "motivation": "现有的分体学习（SL）框架在异构环境中面临挑战，特别是在满足个性化隐私需求和适应不同环境条件下的本地模型定制方面存在不足。", "method": "P3SL提出了两个关键贡献：首先，设计了一个个性化序列分体学习管道，使每个客户端能够实现定制的隐私保护并维护与其计算资源、环境条件和隐私需求相符的个性化本地模型。其次，采用了双层优化技术，使客户端无需与服务器共享私有敏感信息（即计算资源、环境条件、隐私要求）即可确定自己的最佳个性化分割点，从而平衡能耗和隐私泄露风险，同时保持高模型精度。", "result": "P3SL在一个包含7台设备的测试平台（包括4台Jetson Nano P3450设备、2台树莓派和1台笔记本电脑）上进行了实现和评估，使用了多样化的模型架构和数据集，并在不同的环境条件下进行了测试。", "conclusion": "P3SL成功地解决了异构边缘设备中分体学习的个性化隐私和本地模型定制问题，通过个性化管道和双层优化实现了能耗与隐私泄露风险的平衡，同时保持了高模型精度。", "translation": "分体学习（SL）是一种新兴的隐私保护机器学习技术，它通过将模型划分为客户端和服务器端子模型，使资源受限的边缘设备能够参与模型训练。虽然SL减少了边缘设备的计算开销，但在计算资源、通信能力、环境条件和隐私要求各不相同的异构环境中，它遇到了重大挑战。尽管最近的研究探索了优化不同资源限制设备分体点的异构SL框架，但它们通常忽略了在不同环境条件下的个性化隐私要求和本地模型定制。为了解决这些限制，我们提出了P3SL，一个专为异构、资源受限的边缘设备系统设计的个性化隐私保护分体学习框架。这项工作的关键贡献有两方面。首先，我们设计了一个个性化序列分体学习管道，允许每个客户端实现定制的隐私保护，并维护根据其计算资源、环境条件和隐私需求量身定制的个性化本地模型。其次，我们采用了双层优化技术，使客户端无需与服务器共享私有敏感信息（即计算资源、环境条件、隐私要求）即可确定自己的最佳个性化分割点。这种方法在保持高模型精度的同时平衡了能耗和隐私泄露风险。我们在一个由7台设备组成的测试平台（包括4台Jetson Nano P3450设备、2台树莓派和1台笔记本电脑）上实现并评估了P3SL，使用了多样化的模型架构和数据集，并在不同的环境条件下进行了测试。", "summary": "P3SL是一个为异构边缘设备设计的个性化隐私保护分体学习框架。它解决了现有分体学习在异构环境中忽视个性化隐私需求和本地模型定制的问题。P3SL通过个性化序列分体学习管道，使每个客户端能自定义隐私保护和本地模型；并通过双层优化，让客户端在不共享敏感信息的情况下确定最优分割点，以平衡能耗、隐私风险和模型精度。该框架已在多设备测试平台上进行了验证。", "keywords": "分体学习, 隐私保护, 异构边缘设备, 个性化, 双层优化", "comments": "P3SL的创新点在于其将个性化需求（包括隐私、资源和环境条件）融入分体学习框架中，并通过客户端本地决策的优化方式解决了敏感信息共享问题，这对于在真实异构边缘环境中部署隐私保护机器学习模型具有重要意义。"}}
{"id": "2506.12404", "title": "EXGnet: a single-lead explainable-AI guided multiresolution network with train-only quantitative features for trustworthy ECG arrhythmia classification", "authors": ["Tushar Talukder Showrav", "Soyabul Islam Lincoln", "Md. Kamrul Hasan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages, 8 figures", "url": "http://arxiv.org/abs/2506.12404v2", "summary": "Deep learning has significantly propelled the performance of ECG arrhythmia\nclassification, yet its clinical adoption remains hindered by challenges in\ninterpretability and deployment on resource-constrained edge devices. To bridge\nthis gap, we propose EXGnet, a novel and reliable ECG arrhythmia classification\nnetwork tailored for single-lead signals, specifically designed to balance high\naccuracy, explainability, and edge compatibility. EXGnet integrates XAI\nsupervision during training via a normalized cross-correlation based loss,\ndirecting the model's attention to clinically relevant ECG regions, similar to\na cardiologist's focus. This supervision is driven by automatically generated\nground truth, derived through an innovative heart rate variability-based\napproach, without the need for manual annotation. To enhance classification\naccuracy without compromising deployment simplicity, we incorporate\nquantitative ECG features during training. These enrich the model with\nmulti-domain knowledge but are excluded during inference, keeping the model\nlightweight for edge deployment. Additionally, we introduce an innovative\nmultiresolution block to efficiently capture both short and long-term signal\nfeatures while maintaining computational efficiency. Rigorous evaluation on the\nChapman and Ningbo benchmark datasets validates the supremacy of EXGnet, which\nachieves average five-fold accuracies of 98.762% and 96.932%, and F1-scores of\n97.910% and 95.527%, respectively. Comprehensive ablation studies and both\nquantitative and qualitative interpretability assessment confirm that the XAI\nguidance is pivotal, demonstrably enhancing the model's focus and\ntrustworthiness. Overall, EXGnet sets a new benchmark by combining\nhigh-performance arrhythmia classification with interpretability, paving the\nway for more trustworthy and accessible portable ECG based health monitoring\nsystems.", "comment": "17 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.12404v2", "cate": "cs.LG", "date": "2025-06-14", "updated": "2025-07-23", "AI": {"title_translation": "EXGnet：一种单导联可解释AI引导的多分辨率网络，采用仅训练量化特征实现可信ECG心律失常分类", "tldr": "EXGnet是一种用于单导联心电图心律失常分类的新型深度学习网络，它通过结合可解释AI监督、训练时使用的量化特征和多分辨率模块，实现了高准确性、可解释性和边缘设备兼容性。", "motivation": "深度学习在ECG心律失常分类方面取得了显著进展，但其临床应用受限于可解释性和在资源受限的边缘设备上部署的挑战。为了解决这些问题，本文提出了EXGnet。", "method": "EXGnet是一种针对单导联信号的ECG心律失常分类网络。它通过基于归一化互相关的损失函数在训练期间整合XAI监督，引导模型关注临床相关ECG区域。这种监督由通过创新的心率变异性方法自动生成的真实标签驱动，无需手动标注。为了提高分类准确性同时不影响部署的简易性，模型在训练期间加入了量化ECG特征，这些特征在推理时被排除，以保持模型轻量化。此外，还引入了创新的多分辨率块，以高效捕获短期和长期信号特征。", "result": "在Chapman和Ningbo基准数据集上进行了严格评估，EXGnet的平均五折准确率分别为98.762%和96.932%，F1分数分别为97.910%和95.527%。全面的消融研究以及定量和定性可解释性评估证实，XAI指导至关重要，显著增强了模型的关注度和可信度。", "conclusion": "EXGnet结合了高性能心律失常分类和可解释性，为更可信、更易于访问的便携式ECG健康监测系统铺平了道路，并设立了新的基准。", "translation": "深度学习显著推动了心电图（ECG）心律失常分类的性能，但其临床应用仍受限于可解释性以及在资源受限的边缘设备上部署的挑战。为了弥合这一差距，我们提出了EXGnet，一种新颖可靠的ECG心律失常分类网络，专为单导联信号设计，旨在平衡高准确性、可解释性和边缘兼容性。EXGnet在训练期间通过基于归一化互相关的损失函数整合了XAI（可解释人工智能）监督，将模型的注意力导向临床相关的ECG区域，类似于心脏病专家关注的焦点。这种监督由通过创新的基于心率变异性（HRV）的方法自动生成的真实标签驱动，无需手动标注。为了在不影响部署简易性的前提下提高分类准确性，我们在训练期间加入了量化ECG特征。这些特征通过多领域知识丰富了模型，但在推理时被排除，从而保持模型轻量化以适应边缘部署。此外，我们引入了一种创新的多分辨率块，以高效捕获短期和长期信号特征，同时保持计算效率。在Chapman和Ningbo基准数据集上进行的严格评估验证了EXGnet的卓越性能，其平均五折准确率分别达到了98.762%和96.932%，F1分数分别达到了97.910%和95.527%。全面的消融研究以及定量和定性可解释性评估证实，XAI指导至关重要，可明显增强模型的关注度和可信度。总的来说，EXGnet通过将高性能心律失常分类与可解释性相结合，设立了一个新的基准，为更可信和更易于访问的便携式ECG健康监测系统铺平了道路。", "summary": "EXGnet是一种专为单导联ECG心律失常分类设计的新型深度学习网络，旨在解决现有深度学习模型在临床应用中面临的可解释性和边缘设备部署挑战。该模型通过在训练阶段引入基于心率变异性自动生成真实标签的XAI监督，引导模型关注临床相关区域。同时，它在训练中使用量化ECG特征以提高准确性，但在推理时移除这些特征以保持模型轻量化，并采用多分辨率块高效处理不同尺度的信号特征。在Chapman和Ningbo数据集上的评估显示，EXGnet在准确率和F1分数上均表现出色，并通过XAI指导显著增强了模型的关注度和可信度，为可信赖的便携式ECG系统提供了解决方案。", "keywords": "ECG心律失常分类, 可解释AI, 边缘计算, 多分辨率网络, 心率变异性", "comments": "EXGnet的创新之处在于其巧妙地结合了可解释AI监督、仅训练时使用的量化特征以及多分辨率处理，以实现高准确性、可解释性和边缘设备兼容性。特别值得注意的是，其XAI监督无需手动标注，而是通过心率变异性自动生成真实标签，大大降低了数据标注成本。这种设计使其在解决深度学习模型临床落地难题方面迈出了重要一步，对于推动可信赖和便携式医疗设备的发展具有重要意义。"}}
{"id": "2406.07661", "title": "ROADWork Dataset: Learning to Recognize, Observe, Analyze and Drive Through Work Zones", "authors": ["Anurag Ghosh", "Shen Zheng", "Robert Tamburo", "Khiem Vuong", "Juan Alvarez-Padilla", "Hailiang Zhu", "Michael Cardei", "Nicholas Dunn", "Christoph Mertz", "Srinivasa G. Narasimhan"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Accepted Paper", "url": "http://arxiv.org/abs/2406.07661v2", "summary": "Perceiving and autonomously navigating through work zones is a challenging\nand underexplored problem. Open datasets for this long-tailed scenario are\nscarce. We propose the ROADWork dataset to learn to recognize, observe,\nanalyze, and drive through work zones. State-of-the-art foundation models fail\nwhen applied to work zones. Fine-tuning models on our dataset significantly\nimproves perception and navigation in work zones. With ROADWork dataset, we\ndiscover new work zone images with higher precision (+32.5%) at a much higher\nrate (12.8$\\times$) around the world. Open-vocabulary methods fail too, whereas\nfine-tuned detectors improve performance (+32.2 AP). Vision-Language Models\n(VLMs) struggle to describe work zones, but fine-tuning substantially improves\nperformance (+36.7 SPICE).\n  Beyond fine-tuning, we show the value of simple techniques. Video label\npropagation provides additional gains (+2.6 AP) for instance segmentation.\nWhile reading work zone signs, composing a detector and text spotter via\ncrop-scaling improves performance +14.2% 1-NED). Composing work zone detections\nto provide context further reduces hallucinations (+3.9 SPICE) in VLMs. We\npredict navigational goals and compute drivable paths from work zone videos.\nIncorporating road work semantics ensures 53.6% goals have angular error (AE) <\n0.5 (+9.9 %) and 75.3% pathways have AE < 0.5 (+8.1 %).", "comment": "ICCV 2025 Accepted Paper", "pdf_url": "http://arxiv.org/pdf/2406.07661v2", "cate": "cs.CV", "date": "2024-06-11", "updated": "2025-07-22", "AI": {"title_translation": "ROADWork数据集：学习识别、观察、分析和驾驶通过施工区", "tldr": "该论文提出了ROADWork数据集，旨在解决自动驾驶车辆在施工区感知和导航的挑战。通过在该数据集上微调模型和应用简单技术，显著提高了在施工区内的感知和导航性能。", "motivation": "在施工区内进行感知和自动导航是一个具有挑战性且尚未充分探索的问题。针对这种长尾场景的开放数据集非常稀缺。", "method": "本文提出了ROADWork数据集，用于学习识别、观察、分析和驾驶通过施工区。通过在ROADWork数据集上微调模型，并利用视频标签传播、结合检测器和文本识别器进行裁剪缩放，以及组合施工区检测结果以提供上下文等简单技术，来提高性能。此外，还预测导航目标并计算施工区视频中的可驾驶路径。", "result": "在ROADWork数据集上微调模型显著改善了施工区的感知和导航。与现有模型相比，新发现的施工区图像的精度提高了32.5%，速率提高了12.8倍。微调后的检测器性能提升了32.2 AP，微调后的视觉-语言模型（VLMs）性能提升了36.7 SPICE。视频标签传播使实例分割额外提高了2.6 AP。结合检测器和文本识别器通过裁剪缩放使性能提高了14.2% 1-NED。结合施工区检测结果进一步减少了VLMs中的幻觉（+3.9 SPICE）。结合道路施工语义后，53.6%的目标角度误差（AE）小于0.5（+9.9%），75.3%的路径角度误差（AE）小于0.5（+8.1%）。", "conclusion": "本文通过引入ROADWork数据集并展示了微调和简单技术在提高自动驾驶车辆在复杂施工区内的感知和导航能力方面的有效性，解决了现有模型在此场景下的不足，为未来的研究提供了宝贵的资源。", "translation": "感知并自动驾驶通过施工区是一个具有挑战性且尚未充分探索的问题。针对这种长尾场景的开放数据集非常稀缺。我们提出了ROADWork数据集，用于学习识别、观察、分析和驾驶通过施工区。当应用于施工区时，最先进的基础模型会失效。在我们的数据集上微调模型显著改善了施工区内的感知和导航。借助ROADWork数据集，我们以更高的精度（+32.5%）和更高的速率（12.8倍）在全球范围内发现了新的施工区图像。开放词汇方法也失败了，而微调后的检测器性能提高了（+32.2 AP）。视觉-语言模型（VLMs）难以描述施工区，但微调显著提高了性能（+36.7 SPICE）。\n除了微调，我们还展示了简单技术的价值。视频标签传播为实例分割提供了额外的增益（+2.6 AP）。在读取施工区标志时，通过裁剪缩放组合检测器和文本识别器可将性能提高14.2%（1-NED）。组合施工区检测结果以提供上下文进一步减少了VLMs中的幻觉（+3.9 SPICE）。我们预测了导航目标并根据施工区视频计算了可驾驶路径。结合道路施工语义确保了53.6%的目标角度误差（AE）小于0.5（+9.9%），75.3%的路径角度误差（AE）小于0.5（+8.1%）。", "summary": "该论文介绍了ROADWork数据集，旨在解决自动驾驶在施工区感知和导航的难题，因为现有开放数据集稀缺且SOTA模型表现不佳。研究表明，通过在ROADWork数据集上对模型进行微调，并结合视频标签传播、裁剪缩放组合检测器和文本识别器等简单技术，可以显著提高在施工区内的感知、目标识别、导航路径预测等性能，并有效减少视觉-语言模型中的幻觉。", "keywords": "ROADWork数据集, 施工区, 自动驾驶, 感知, 导航", "comments": "该论文的创新之处在于提出了一个专门针对施工区场景的ROADWork数据集，填补了该领域开放数据集稀缺的空白。它不仅强调了数据的重要性，还证明了即使是简单的技术，结合针对特定场景的微调，也能在现有SOTA模型表现不佳的复杂环境下取得显著的性能提升。这对于自动驾驶的安全性和实际部署具有重要的意义。"}}
