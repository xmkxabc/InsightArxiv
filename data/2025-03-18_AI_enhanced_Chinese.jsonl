{"id": "2503.11731", "pdf": "https://arxiv.org/pdf/2503.11731", "abs": "https://arxiv.org/abs/2503.11731", "authors": ["Xianming Zeng", "Sicong Du", "Qifeng Chen", "Lizhe Liu", "Haoyu Shu", "Jiaxuan Gao", "Jiarun Liu", "Jiulong Xu", "Jianyun Xu", "Mingxia Chen", "Yiru Zhao", "Peng Chen", "Yapeng Xue", "Chunming Zhao", "Sheng Yang", "Qiang Li"], "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor simulation is pivotal for scalable validation of autonomous driving\nsystems, yet existing Neural Radiance Fields (NeRF) based methods face\napplicability and efficiency challenges in industrial workflows. This paper\nintroduces a Gaussian Splatting (GS) based system to address these challenges:\nWe first break down sensor simulator components and analyze the possible\nadvantages of GS over NeRF. Then in practice, we refactor three crucial\ncomponents through GS, to leverage its explicit scene representation and\nreal-time rendering: (1) choosing the 2D neural Gaussian representation for\nphysics-compliant scene and sensor modeling, (2) proposing a scene editing\npipeline to leverage Gaussian primitives library for data augmentation, and (3)\ncoupling a controllable diffusion model for scene expansion and harmonization.\nWe implement this framework on a proprietary autonomous driving dataset\nsupporting cameras and LiDAR sensors. We demonstrate through ablation studies\nthat our approach reduces frame-wise simulation latency, achieves better\ngeometric and photometric consistency, and enables interpretable explicit scene\nediting and expansion. Furthermore, we showcase how integrating such a GS-based\nsensor simulator with traffic and dynamic simulators enables full-stack testing\nof end-to-end autonomy algorithms. Our work provides both algorithmic insights\nand practical validation, establishing GS as a cornerstone for industrial-grade\nsensor simulation.", "AI": {"title_translation": "通过高斯泼溅实现工业级传感器模拟：一个用于可扩展编辑和全栈验证的模块化框架", "tldr": "本文提出了一种基于高斯泼溅（GS）的工业级传感器模拟模块化框架，旨在解决现有神经辐射场（NeRF）方法在自动驾驶工作流中面临的效率和适用性挑战，实现可扩展编辑和全栈验证。", "motivation": "现有的基于神经辐射场（NeRF）的传感器模拟方法在工业工作流程中面临适用性和效率挑战，这阻碍了自动驾驶系统的可扩展验证。", "method": "本文提出一个基于高斯泼溅（GS）的系统。该系统利用GS的显式场景表示和实时渲染能力，重构了三个关键组件：1) 采用2D神经高斯表示进行符合物理的场景和传感器建模；2) 提出一个利用高斯基元库进行数据增强的场景编辑流程；3) 耦合一个可控扩散模型进行场景扩展和协调。该框架在一个支持摄像头和激光雷达传感器的专有自动驾驶数据集上实现并验证。", "result": "我们的方法减少了逐帧模拟延迟，实现了更好的几何和光度一致性，并支持可解释的显式场景编辑和扩展。此外，将此GS传感器模拟器与交通和动态模拟器集成后，能够实现端到端自主算法的全栈测试。", "conclusion": "高斯泼溅（GS）可以作为工业级传感器模拟的基石，提供了算法洞察和实际验证。", "translation": "传感器模拟对于自动驾驶系统的可扩展验证至关重要，然而现有的基于神经辐射场（NeRF）的方法在工业流程中面临适用性和效率挑战。本文引入了一个基于高斯泼溅（GS）的系统来解决这些挑战：我们首先分解传感器模拟器组件，并分析GS相对于NeRF的潜在优势。然后，在实践中，我们通过GS重构了三个关键组件，以利用其明确的场景表示和实时渲染能力：（1）选择2D神经高斯表示进行物理兼容的场景和传感器建模；（2）提出一个场景编辑流程，利用高斯基元库进行数据增强；（3）耦合一个可控扩散模型用于场景扩展和协调。我们在一个支持摄像头和激光雷达传感器的专有自动驾驶数据集上实现了这个框架。我们通过消融研究证明，我们的方法减少了逐帧模拟延迟，实现了更好的几何和光度一致性，并支持可解释的显式场景编辑和扩展。此外，我们展示了如何将这种基于GS的传感器模拟器与交通和动态模拟器集成，从而实现端到端自主算法的全栈测试。我们的工作提供了算法洞察和实际验证，将GS确立为工业级传感器模拟的基石。", "summary": "本文提出了一种基于高斯泼溅（GS）的模块化框架，用于工业级传感器模拟，旨在解决现有基于神经辐射场（NeRF）方法在自动驾驶工作流中面临的效率和适用性限制。该框架利用GS的显式场景表示和实时渲染能力，通过重构关键组件实现：包括使用2D神经高斯表示进行物理兼容建模、利用高斯基元库进行数据增强的场景编辑流程，以及一个用于场景扩展和协调的可控扩散模型。在专有数据集上的实验表明，该方法显著降低了模拟延迟，提高了几何和光度一致性，并支持可解释的场景编辑和全栈验证。", "keywords": "高斯泼溅, 传感器模拟, 自动驾驶, 神经辐射场, 工业级", "comments": "该论文的创新之处在于将高斯泼溅技术应用于工业级传感器模拟，有效解决了先前基于NeRF的方法在效率和可扩展性方面遇到的关键问题。其模块化设计以及与数据增强和扩散模型相结合以实现场景扩展的能力，对于实际的自动驾驶开发尤为重要，标志着在真实高效模拟方面迈出了重要一步。"}}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Incà", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.", "AI": {"title_translation": "通过不安全权重操作实现安全的视觉-语言模型", "tldr": "本文引入SafeGround评估视觉-语言模型（VLM）安全性，发现现有训练方法在安全输入上表现不佳。为此，提出不基于训练的“不安全权重操作”（UWM）方法，通过校准集识别并修改关键参数，实现在不牺牲知识保留的情况下显著提升VLM在不安全和安全查询上的安全性。", "motivation": "视觉-语言模型（VLMs）继承了训练数据中的偏见和不安全关联。现有缓解不安全行为的方法主要关注模型在不安全输入上的安全性，却忽略了其在安全输入上的潜在缺陷。本文旨在修正这一评估不足，并探索无需训练即可提升VLM安全性的方法。", "method": "首先，引入新的安全评估指标集SafeGround，用于多粒度评估安全性。其次，基于发现训练方法在安全输入上降低模型安全性的问题，提出“不安全权重操作”（UWM）。UWM利用一个包含安全和不安全实例的校准集，比较两者激活，识别处理不安全内容最重要的参数，并通过取反操作来修改这些参数的值。", "result": "通过SafeGround指标发现，现有的基于训练的方法会降低模型在安全输入上的安全性。实验表明，UWM在安全性和知识保留之间实现了最佳权衡，持续改进了VLM在不安全查询上的表现，并且在安全查询上甚至优于基于训练的最新方法。", "conclusion": "本文修正了VLM安全评估方法，并提出了无需训练的UWM方法。UWM通过直接操作与不安全内容相关的权重，有效提升了VLM的安全性，同时避免了传统训练方法对模型在安全输入上表现的负面影响，实现了在安全性和知识保留方面的最佳平衡。", "translation": "视觉-语言模型（VLMs）通常会继承其大规模训练数据中存在的偏见和不安全关联。尽管最近的方法旨在缓解不安全行为，但它们的评估侧重于模型在不安全输入上的安全性，却忽视了在安全输入上的潜在缺陷。在本文中，我们首先通过引入SafeGround——一套评估不同粒度安全性的新指标，来修订安全评估。通过这个指标，我们揭示了基于训练的方法一个令人惊讶的问题：它们会降低模型在安全输入上的安全性。基于这一发现，我们采取了不同的方向，探索是否有可能在不训练的情况下使模型更安全，并引入了不安全权重操作（UWM）。UWM使用一组安全和不安全实例的校准集来比较安全和不安全内容之间的激活，识别处理后者最重要的参数。然后通过取反操作来操纵它们的值。实验表明，UWM在安全性和知识保留之间实现了最佳权衡，持续改进了VLMs在不安全查询上的表现，同时在安全查询上甚至优于基于训练的最新方法。", "summary": "本文针对视觉-语言模型（VLMs）的安全评估不足及训练方法在安全输入上的缺陷，提出了新的安全评估指标SafeGround。研究发现，现有训练方法会降低VLM在安全输入上的安全性。为此，本文创新性地提出了一种无需训练的“不安全权重操作”（UWM）方法。UWM通过分析安全和不安全内容的激活差异，识别并反转处理不安全内容的关键参数权重，从而在不牺牲知识保留的前提下，显著提升VLM在不安全和安全查询上的安全性，表现优于现有SOTA训练方法。", "keywords": "视觉-语言模型, 模型安全, 权重操作, 安全评估, 偏见缓解", "comments": "本文的创新点在于提出了非训练式的VLM安全增强方法UWM，并修正了安全评估的盲点。UWM通过直接操纵权重，提供了一种高效且对模型知识保留影响较小的解决方案，解决了现有训练方法在安全输入上表现不佳的问题。这对于VLM的实际部署具有重要意义，尤其是在对安全性要求较高的应用场景中。"}}
{"id": "2503.11750", "pdf": "https://arxiv.org/pdf/2503.11750", "abs": "https://arxiv.org/abs/2503.11750", "authors": ["Shuyang Hao", "Yiwei Wang", "Bryan Hooi", "Jun Liu", "Muhao Chen", "Zi Huang", "Yujun Cai"], "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.", "AI": {"title_translation": "让每一步都有效：通过分层KV均衡越狱大型视觉-语言模型", "tldr": "本文提出了HKVE框架，通过选择性接受梯度优化结果来提高大型视觉-语言模型越狱攻击的成功率，并减少计算成本。", "motivation": "在大型视觉-语言模型（LVLMs）的对抗性越狱攻击中，研究人员发现并非每个对抗性优化步骤都能带来积极结果，盲目接受每一步的优化结果可能会降低整体攻击成功率。", "method": "本文提出了HKVE（分层键值均衡）框架，通过基于不同层注意力分数分布选择性地接受梯度优化结果，确保每个优化步骤都对攻击产生积极贡献。", "result": "HKVE在MiniGPT4上攻击成功率达到75.08%，LLaVA上达到85.84%，Qwen-VL上达到81.00%，分别比现有方法高出20.43%、21.01%和26.43%。此外，该方法还减少了迭代次数，降低了计算成本。", "conclusion": "HKVE框架通过确保每一步优化都有效，显著提高了大型视觉-语言模型的越狱攻击成功率，并降低了计算成本。", "translation": "在大型视觉-语言模型（LVLMs）领域，对抗性越狱攻击作为一种红队方法，旨在识别这些模型及其相关防御机制的安全漏洞。然而，我们发现一个关键限制：并非每个对抗性优化步骤都能带来积极结果，不加选择地接受每一步的优化结果可能会降低整体攻击成功率。为了解决这一挑战，我们引入了HKVE（分层键值均衡），这是一个创新的越狱框架，它根据不同层注意力分数的分布选择性地接受梯度优化结果，确保每个优化步骤都对攻击产生积极贡献。大量实验表明，HKVE具有显著的有效性，在MiniGPT4上攻击成功率达到75.08%，在LLaVA上达到85.84%，在Qwen-VL上达到81.00%，分别比现有方法高出20.43%、21.01%和26.43%。此外，让每一步都有效不仅能提高攻击成功率，还能减少迭代次数，从而降低计算成本。警告：本文包含潜在有害的示例数据。", "summary": "本文提出了一种名为HKVE（分层键值均衡）的创新框架，旨在解决大型视觉-语言模型（LVLMs）越狱攻击中优化步骤效率低下的问题。传统方法中，并非所有优化步骤都对攻击成功有益，甚至可能降低成功率。HKVE通过根据注意力分数的分布选择性地接受梯度优化结果，确保了每一步优化都能积极促进攻击。实验结果表明，HKVE在多个LVLMs上显著提高了攻击成功率，并能减少迭代次数，从而降低计算成本。", "keywords": "越狱攻击, 大型视觉-语言模型, HKVE, 对抗性优化, 安全漏洞", "comments": "本文的创新点在于提出了HKVE框架，通过对梯度优化结果进行选择性接受，解决了传统越狱攻击中优化效率低下的问题。这种“让每一步都有效”的策略不仅显著提升了攻击成功率，还在降低计算成本方面展现出其重要性。这对于红队测试和理解LVLMs的安全漏洞具有重要意义。"}}
{"id": "2503.11780", "pdf": "https://arxiv.org/pdf/2503.11780", "abs": "https://arxiv.org/abs/2503.11780", "authors": ["Tianyi Zhao", "Boyang Liu", "Yanglei Gao", "Yiming Sun", "Maoxun Yuan", "Xingxing Wei"], "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem that the\ndecreased feature extraction capability in multi-modal joint learning. This\nleads to an unreasonable but prevalent phenomenon--Fusion Degradation, which\nhinders the performance improvement of the MMOD model. Motivated by this, in\nthis paper, we introduce linear probing evaluation to the multi-modal detectors\nand rethink the multi-modal object detection task from the mono-modality\nlearning perspective. Therefore, we construct an novel framework called\nM$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method\nand the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework\nfacilitates the sufficient learning of mono-modality during multi-modal joint\ntraining and explores a lightweight yet effective feature fusion manner to\nachieve superior object detection performance. Extensive experiments conducted\non three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates\nthe Fusion Degradation phenomenon and outperforms the previous SOTA detectors.", "AI": {"title_translation": "从单模态特征学习视角重新思考多模态目标检测", "tldr": "本文重新审视了多模态目标检测中的融合退化问题，提出M$^2$D-LIF框架，通过单模态蒸馏和局部光照感知融合来提升性能。", "motivation": "现有的多模态目标检测（MMOD）方法忽略了多模态联合学习中单模态特征提取能力下降的问题，导致“融合退化”现象，阻碍了MMOD模型性能的提升。", "method": "本文引入线性探测评估来分析多模态检测器，并从单模态学习角度重新思考MMOD任务。为此，构建了一个名为M$^2$D-LIF的新颖框架，该框架包含单模态蒸馏（M$^2$D）方法和局部光照感知融合（LIF）模块。M$^2$D-LIF框架促进了多模态联合训练期间单模态的充分学习，并探索了一种轻量级而有效的特征融合方式。", "result": "在三个MMOD数据集上进行了大量实验，结果表明M$^2$D-LIF有效地缓解了融合退化现象，并优于以往的最先进检测器。", "conclusion": "本文提出的M$^2$D-LIF框架通过解决单模态学习不足和融合退化问题，显著提升了多模态目标检测的性能。", "translation": "多模态目标检测（MMOD）由于其对各种复杂环境的更强适应性，已广泛应用于各种应用中。大量研究致力于RGB-IR目标检测，主要关注如何整合来自RGB-IR模态的互补特征。然而，它们忽视了在多模态联合学习中特征提取能力下降导致的单模态学习不足问题。这导致了一种不合理但普遍存在的现象——融合退化，它阻碍了MMOD模型性能的提升。受此启发，本文将线性探测评估引入多模态检测器，并从单模态学习的角度重新思考多模态目标检测任务。因此，我们构建了一个名为M$^2$D-LIF的新颖框架，该框架由单模态蒸馏（M$^2$D）方法和局部光照感知融合（LIF）模块组成。M$^2$D-LIF框架促进了多模态联合训练期间单模态的充分学习，并探索了一种轻量级而有效的特征融合方式，以实现卓越的目标检测性能。在三个MMOD数据集上进行的大量实验表明，我们的M$^2$D-LIF有效地缓解了融合退化现象，并优于以往的最先进检测器。", "summary": "本文针对多模态目标检测（MMOD）中存在的“融合退化”问题，即多模态联合学习导致单模态特征提取能力下降，提出了M$^2$D-LIF框架。该框架包含单模态蒸馏（M$^2$D）方法和局部光照感知融合（LIF）模块，旨在促进单模态的充分学习并实现高效的特征融合。实验证明，M$^2$D-LIF有效缓解了融合退化，并超越了现有SOTA检测器。", "keywords": "多模态目标检测, 融合退化, 单模态蒸馏, RGB-IR, 特征融合", "comments": "本文的创新点在于首次明确提出了多模态目标检测中的“融合退化”问题，并从单模态学习的角度重新审视了多模态融合的挑战。通过引入单模态蒸馏和局部光照感知融合，为解决该问题提供了一个有效且新颖的解决方案。"}}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN", "AI": {"title_translation": "使用超网络科尔莫哥洛夫-阿诺德网络进行色彩匹配", "tldr": "cmKAN是一个通用的色彩匹配框架，它利用基于超网络的科尔莫哥洛夫-阿诺德网络（KANs）来准确地将源图像的颜色映射到目标颜色分布，并在监督和无监督设置下均优于现有方法，同时引入了首个大型配对图像数据集。", "motivation": "现有色彩匹配方法可能不够有效和准确，无法将源图像的颜色映射到目标颜色分布。本文旨在提出一个通用的框架，能够有效且准确地在监督和无监督设置下完成色彩匹配。", "method": "本文提出了cmKAN框架，它利用科尔莫哥洛夫-阿诺德网络（KANs）的样条能力来建模源和目标分布之间的色彩匹配。具体来说，开发了一个超网络，用于生成空间变化的权重图，以控制KAN的非线性样条，从而实现精确的色彩匹配。此外，还引入了首个大型配对图像数据集，用于评估色彩匹配方法。", "result": "cmKAN在各种色彩匹配任务（包括raw-to-raw、raw-to-sRGB和sRGB-to-sRGB映射）中进行了评估。结果显示，与现有方法相比，cmKAN在监督和无监督情况下平均性能提高了37.3%，同时保持了轻量级。", "conclusion": "cmKAN是一个有效且准确的色彩匹配框架，它利用超网络控制的KANs实现了卓越的性能，并在多个色彩匹配任务中优于现有方法，同时保持轻量级。此外，该工作还引入了一个新的大型配对图像数据集。", "translation": "我们提出了cmKAN，一个通用的色彩匹配框架。给定一个具有源色彩分布颜色的输入图像，我们的方法能够在监督和无监督设置下，有效且准确地将这些颜色映射以匹配目标色彩分布。我们的框架利用科尔莫哥洛夫-阿诺德网络（KANs）的样条能力来建模源和目标分布之间的色彩匹配。具体来说，我们开发了一个超网络，用于生成空间变化的权重图，以控制KAN的非线性样条，从而实现精确的色彩匹配。作为这项工作的一部分，我们引入了首个由两台不同相机捕获的配对图像大型数据集，并评估了我们和现有方法在色彩匹配方面的功效。我们在各种色彩匹配任务中评估了我们的方法，包括：(1) raw-to-raw映射，其中源色彩分布在一个相机的原始色彩空间中，目标在另一个相机的原始空间中；(2) raw-to-sRGB映射，其中源色彩分布在一个相机的原始空间中，目标在显示器sRGB空间中，模拟相机ISP的色彩渲染；以及(3) sRGB-to-sRGB映射，其目标是将颜色从源sRGB空间（例如，由源相机ISP生成）传输到目标sRGB空间（例如，来自不同的相机ISP）。结果表明，在监督和无监督情况下，我们的方法平均优于现有方法37.3%，同时与其他方法相比保持轻量级。代码、数据集和预训练模型可在以下网址获取：https://github.com/gosha20777/cmKAN", "summary": "本文提出了cmKAN，一个基于超网络科尔莫哥洛夫-阿诺德网络（KANs）的通用色彩匹配框架。该方法通过一个生成空间变化权重图的超网络来控制KAN的非线性样条，从而实现将源图像颜色有效且准确地映射到目标颜色分布。研究引入了首个大型配对图像数据集，并在raw-to-raw、raw-to-sRGB和sRGB-to-sRGB等多种色彩匹配任务中进行了评估。实验结果表明，cmKAN在监督和无监督设置下，性能平均优于现有方法37.3%，且保持轻量级。", "keywords": "色彩匹配, 超网络, 科尔莫哥洛夫-阿诺德网络, 图像处理, 深度学习", "comments": "本文的创新点在于将超网络与科尔莫哥洛夫-阿诺德网络（KANs）结合应用于色彩匹配任务，利用KANs的样条能力和超网络生成空间变化的权重图，实现了更精确的非线性映射。此外，构建并发布首个大型配对图像数据集，对于色彩匹配领域的研究具有重要意义，有助于推动该领域的发展。该方法在性能上显著优于现有方法，且保持轻量级，显示出其在实际应用中的潜力。"}}
{"id": "2503.11787", "pdf": "https://arxiv.org/pdf/2503.11787", "abs": "https://arxiv.org/abs/2503.11787", "authors": ["Samuel W. Remedios", "Shuwen Wei", "Shuo Han", "Jinwei Zhang", "Aaron Carass", "Kurt G. Schilling", "Dzung L. Pham", "Jerry L. Prince", "Blake E. Dewey"], "title": "ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In clinical imaging, magnetic resonance (MR) image volumes are often acquired\nas stacks of 2D slices, permitting decreased scan times, improved\nsignal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.\nWhile this is sufficient for clinical evaluation, automated algorithms designed\nfor 3D analysis perform sub-optimally on 2D-acquired scans, especially those\nwith thick slices and gaps between slices. Super-resolution (SR) methods aim to\naddress this problem, but previous methods do not address all of the following:\nslice profile shape estimation, slice gap, domain shift, and non-integer /\narbitrary upsampling factors. In this paper, we propose ECLARE (Efficient\nCross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method\nthat addresses each of these factors. ECLARE estimates the slice profile from\nthe 2D-acquired multi-slice MR volume, trains a network to learn the mapping\nfrom low-resolution to high-resolution in-plane patches from the same volume,\nand performs SR with anti-aliasing. We compared ECLARE to cubic B-spline\ninterpolation, SMORE, and other contemporary SR methods. We used realistic and\nrepresentative simulations so that quantitative performance against a ground\ntruth could be computed, and ECLARE outperformed all other methods in both\nsignal recovery and downstream tasks. On real data for which there is no ground\ntruth, ECLARE demonstrated qualitative superiority over other methods as well.\nImportantly, as ECLARE does not use external training data it cannot suffer\nfrom domain shift between training and testing. Our code is open-source and\navailable at https://www.github.com/sremedios/eclare.", "AI": {"title_translation": "ECLARE：用于各向异性分辨率增强的高效跨平面学习", "tldr": "ECLARE是一种自超分辨率方法，用于提高临床MR图像的各向异性分辨率，解决了切片轮廓估计、切片间隙、域偏移和任意上采样因子等问题，并在模拟和真实数据上优于现有方法。", "motivation": "临床磁共振（MR）图像通常以2D切片堆栈形式获取，这虽然足以进行临床评估，但专为3D分析设计的自动化算法在2D采集的扫描（尤其是厚切片和切片之间有间隙的扫描）上表现不佳。现有的超分辨率（SR）方法未能全面解决切片轮廓形状估计、切片间隙、域偏移和非整数/任意上采样因子等问题。", "method": "本研究提出ECLARE（用于各向异性分辨率增强的高效跨平面学习），这是一种自超分辨率方法，它解决了切片轮廓形状估计、切片间隙、域偏移和非整数/任意上采样因子等问题。ECLARE从2D采集的多切片MR体素中估计切片轮廓，训练网络学习从低分辨率到高分辨率的同平面图像块映射，并执行抗混叠的SR。", "result": "在逼真和有代表性的模拟中，ECLARE在信号恢复和下游任务方面均优于立方B样条插值、SMORE和其他当代SR方法。在没有真实数据的实际数据上，ECLARE也表现出优于其他方法的定性优势。", "conclusion": "ECLARE是一种有效的自超分辨率方法，能够解决2D采集MR图像的各向异性分辨率增强问题，并且由于不使用外部训练数据，因此不会受到训练和测试之间域偏移的影响。", "translation": "在临床成像中，磁共振（MR）图像通常以2D切片堆栈形式获取，这可以缩短扫描时间、提高信噪比，并产生2D MR脉冲序列特有的图像对比度。虽然这足以进行临床评估，但专为3D分析设计的自动化算法在2D采集的扫描上表现不佳，尤其是那些具有厚切片和切片之间存在间隙的扫描。超分辨率（SR）方法旨在解决这个问题，但以前的方法未能全面解决以下所有问题：切片轮廓形状估计、切片间隙、域偏移和非整数/任意上采样因子。在本文中，我们提出了ECLARE（用于各向异性分辨率增强的高效跨平面学习），这是一种自超分辨率方法，可以解决这些因素中的每一个。ECLARE从2D采集的多切片MR体素中估计切片轮廓，训练网络学习从低分辨率到高分辨率的同平面图像块映射，并执行抗混叠的SR。我们将ECLARE与立方B样条插值、SMORE和其他当代SR方法进行了比较。我们使用了逼真和有代表性的模拟，以便可以计算相对于真实值的定量性能，并且ECLARE在信号恢复和下游任务方面均优于所有其他方法。在没有真实数据的实际数据上，ECLARE也表现出优于其他方法的定性优势。重要的是，由于ECLARE不使用外部训练数据，因此它不会受到训练和测试之间域偏移的影响。我们的代码是开源的，可在https://www.github.com/sremedios/eclare获取。", "summary": "本论文提出了一种名为ECLARE的自超分辨率方法，旨在解决临床MR图像2D采集导致的各向异性分辨率问题。该方法能够估计切片轮廓、学习低分辨率到高分辨率的映射并进行抗混叠的超分辨率处理。ECLARE解决了现有SR方法未能全面处理的切片轮廓、切片间隙、域偏移和任意上采样因子等挑战。实验结果表明，ECLARE在模拟数据上优于现有方法，并在真实数据上展现出定性优势。其核心创新在于不依赖外部训练数据，从而避免了域偏移问题。", "keywords": "超分辨率, 磁共振成像, 各向异性分辨率增强, 自学习, 域偏移", "comments": "ECLARE的创新之处在于其自超分辨率的特性，通过从同一体素中学习，有效地避免了外部训练数据可能导致的域偏移问题，这在临床医学图像处理中尤为重要。它全面考虑了2D MR图像采集中的多种挑战，包括切片轮廓和切片间隙，使其在实际应用中更具鲁棒性。该方法的开源代码也促进了其在社区内的应用和进一步研究。"}}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.", "AI": {"title_translation": "StyleMorpheus: 一种基于风格的三维感知可变形人脸模型", "tldr": "StyleMorpheus是首个基于风格的3DMM，可在野外图像上训练，实现高真实感3D人脸重建和解耦控制，并支持实时渲染和人脸编辑。", "motivation": "现有的参数化可控三维感知人脸模型训练依赖于大规模的实验室采集数据集。", "method": "本文提出了StyleMorpheus，它是一个基于风格的神经三维可变形人脸模型（3DMM），首次在野外图像上进行训练。它采用自编码器结构，编码器学习代表性的解耦参数编码空间，解码器利用形状和外观相关的风格编码在不同子模块中改进解耦。此外，通过基于风格的生成对抗学习对解码器进行微调，以实现高真实感的三维渲染质量。该模型继承了3DMM的解耦可控性（对人脸身份、表情和外观），但无需精确重建的显式三维形状。", "result": "StyleMorpheus实现了最先进的三维感知人脸重建结果，并允许对重建人脸进行解耦控制。该模型达到了实时渲染速度，可应用于虚拟现实。它还展示了在人脸编辑应用中的能力，如风格混合和颜色编辑。", "conclusion": "StyleMorpheus成功地将基于风格的设计引入到3DMM中，解决了对实验室数据集的依赖，并在野外图像上实现了高真实感和可控的3D人脸建模，具备实时性能和编辑能力。", "translation": "对于三维人脸建模，最近开发的三维感知神经渲染方法能够渲染任意视角下的逼真人脸图像。然而，参数化可控三维感知人脸模型的训练仍然依赖于大规模的实验室采集数据集。为了解决这个问题，本文引入了“StyleMorpheus”，这是首个基于风格的神经三维可变形人脸模型（3DMM），它在野外图像上进行训练。它继承了3DMM的解耦可控性（对人脸身份、表情和外观），但无需精确重建的显式三维形状。StyleMorpheus采用自编码器结构。编码器旨在学习一个代表性的解耦参数编码空间，解码器通过网络中不同子模块中的形状和外观相关风格编码来改进解耦。此外，我们通过基于风格的生成对抗学习对解码器进行微调，以实现逼真的三维渲染质量。所提出的基于风格的设计使StyleMorpheus能够实现最先进的三维感知人脸重建结果，同时还允许对重建人脸进行解耦控制。我们的模型实现了实时渲染速度，可用于虚拟现实应用。我们还展示了所提出的基于风格的设计在人脸编辑应用中的能力，例如风格混合和颜色编辑。项目主页：https://github.com/ubc-3d-vision-lab/StyleMorpheus。", "summary": "StyleMorpheus是首个基于风格的神经三维可变形人脸模型（3DMM），旨在解决传统3DMM对大规模实验室数据集的依赖，并在野外图像上进行训练。该模型采用自编码器架构，通过风格编码实现人脸身份、表情和外观的解耦控制，并利用基于风格的GAN进行微调以生成逼真图像。StyleMorpheus实现了先进的3D人脸重建、实时渲染，并支持风格混合和颜色编辑等应用。", "keywords": "3D人脸模型, 风格基模型, 3DMM, 神经渲染, 野外图像", "comments": "StyleMorpheus的创新之处在于它是首个将基于风格的设计引入到3DMM中，并成功在“野外”图像上进行训练，这大大降低了对昂贵实验室数据的依赖。其解耦控制能力和实时渲染速度使其在虚拟现实和人脸编辑等应用中具有重要潜力。"}}
{"id": "2503.11794", "pdf": "https://arxiv.org/pdf/2503.11794", "abs": "https://arxiv.org/abs/2503.11794", "authors": ["Bangzheng Li", "Fei Wang", "Wenxuan Zhou", "Nan Xu", "Ben Zhou", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.", "AI": {"title_translation": "语义剪裁：通过语义引导的视觉选择实现高效视觉-语言建模", "tldr": "提出SEMCLIP框架，通过语义引导视觉选择，提高VLM处理细粒度视觉细节的效率和效果，无需VLM重训练。", "motivation": "现有VLM在处理细粒度视觉细节时，通过图像裁剪增加视觉token数量，导致效率低下和LLM分心，且存在图像表示的泛化挑战。", "method": "提出轻量级通用框架SEMCLIP，与现有VLM无缝集成。它利用文本语义识别关键视觉区域，将文本信号融入视觉编码过程，无需VLM重训练即可提高VQA性能。", "result": "SEMCLIP使7B VLM LLaVA-1.5在7个基准上视觉理解能力平均提升3.3%，在挑战性细节理解基准V*上提升5.3%。", "conclusion": "SEMCLIP通过语义引导的视觉选择和文本信号融入，有效提升了VLM的细粒度视觉理解能力，且具有高效性和通用性。", "translation": "视觉-语言模型（VLM）利用对齐的视觉编码器将图像转换为视觉标记，使其能够被骨干大型语言模型（LLM）像文本一样处理。这种统一的输入范式使VLM在视觉问答（VQA）等视觉-语言任务中表现出色。为了改善细粒度视觉推理，视觉-语言建模的最新进展引入了图像裁剪技术，将所有编码的子图像输入到模型中。然而，这种方法显著增加了视觉标记的数量，导致效率低下并可能分散LLM的注意力。为了解决VLM中图像表示的泛化挑战，我们提出了一个轻量级、通用的框架，可以与现有VLM无缝集成，以增强其处理细粒度细节的能力。我们的方法利用文本语义来识别关键视觉区域，在不要求VLM进行任何重新训练的情况下提高VQA性能。此外，它将文本信号融入视觉编码过程，提高效率和有效性。所提出的方法SEMCLIP使7B VLM LLaVA-1.5在7个基准测试中平均视觉理解能力提高了3.3%，尤其是在具有挑战性的细节理解基准V*上提高了5.3%。", "summary": "本文提出了一个名为SEMCLIP的轻量级通用框架，旨在解决现有视觉-语言模型（VLM）在处理细粒度视觉细节时效率低下和泛化能力不足的问题。SEMCLIP通过利用文本语义来引导视觉区域选择，并将文本信号融入视觉编码过程，从而提高VLM的细粒度视觉理解能力和效率，无需对现有VLM进行重训练。实验结果表明，SEMCLIP使7B VLM LLaVA-1.5在多个基准测试中表现出显著的性能提升。", "keywords": "视觉-语言模型, 语义剪裁, 细粒度视觉理解, 视觉问答", "comments": "本文提出了一种创新的语义引导视觉选择方法，有效解决了VLM在处理细粒度视觉信息时效率和准确性的痛点。其无需重训练现有VLM的特点使其具有很高的实用性和普适性。将文本语义融入视觉编码过程，是其提升性能的关键创新点。"}}
{"id": "2503.11806", "pdf": "https://arxiv.org/pdf/2503.11806", "abs": "https://arxiv.org/abs/2503.11806", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "categories": ["cs.CV"], "comment": "Project page: https://www.projectaria.com/scenescript/", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts.", "AI": {"title_translation": "通过填充实现3D场景布局的人机交互式局部校正", "tldr": "本文提出了一种新颖的人机交互方法，通过引入局部校正任务和利用“填充”概念，显著提升了3D场景布局的局部校正能力，同时保持了全局预测性能。", "motivation": "现有3D场景布局估计方法在局部错误修正方面可能存在不足，需要一种更有效的方式让人类用户参与进来，识别并自动修正这些局部错误。", "method": "本文提出了一种新颖的人机交互方法，用于估计3D场景布局。该方法引入了一个新的局部校正任务，用户可以识别局部错误并提示模型自动校正。它基于先进的SceneScript框架，将局部校正问题构建为“填充”任务（类似于自然语言处理中的填充）。研究人员训练了一个多任务版本的SceneScript模型。", "result": "训练后的多任务SceneScript模型在保持全局预测性能的同时，显著提高了其局部校正能力。该系统集成到人机交互流程中，用户可以通过低摩擦的“一键修复”工作流迭代地改进场景布局估计。最终的精炼布局可以偏离训练分布，从而更准确地建模复杂布局。", "conclusion": "本文提出的基于人机交互的局部校正系统，通过引入“填充”任务和改进SceneScript模型，有效解决了3D场景布局中的局部错误修正问题，并允许生成更复杂和准确的布局。", "translation": "我们提出了一种新颖的人机交互方法来估计3D场景布局，该方法利用来自以自我为中心视角的反馈。我们通过引入一个新的局部校正任务来研究这种方法，在该任务中，用户识别局部错误并提示模型自动校正它们。基于SceneScript（一个利用结构化语言进行3D场景布局估计的最新框架），我们提出了一种将该问题结构化为“填充”的解决方案，这是一项在自然语言处理中研究的任务。我们训练了一个多任务版本的SceneScript，它在保持全局预测性能的同时，显著提高了其局部校正能力。我们将此集成到一个人机交互系统中，使用户能够通过低摩擦的“一键修复”工作流迭代地完善场景布局估计。我们的系统使最终的精炼布局能够偏离训练分布，从而更准确地建模复杂布局。", "summary": "本文介绍了一种新颖的人机交互方法，用于3D场景布局的估计和局部校正。该方法通过引入一个让用户识别并自动修正局部错误的新任务来运作，并借鉴了自然语言处理中的“填充”概念。研究人员基于SceneScript框架，训练了一个多任务模型，该模型在保持全局预测性能的同时，显著提升了局部校正能力。最终，该系统允许用户通过简单的“一键修复”操作迭代优化布局，并能生成更复杂、更准确的3D场景模型。", "keywords": "3D场景布局, 人机交互, 局部校正, 填充, SceneScript", "comments": "本文的创新点在于将人机交互引入3D场景布局的局部校正，并巧妙地将这一问题建模为类似自然语言处理中的“填充”任务。通过改进现有的SceneScript框架，该方法不仅提升了局部修正的效率和准确性，还使得最终的布局能够超越训练数据分布，从而能更好地处理复杂和独特的场景，这对于实际应用具有重要意义。"}}
{"id": "2503.11807", "pdf": "https://arxiv.org/pdf/2503.11807", "abs": "https://arxiv.org/abs/2503.11807", "authors": ["Sanayya A", "Amoolya Shetty", "Abhijeet Sharma", "Venkatesh Ravichandran", "Masthan Wali Gosuvarapalli", "Sarthak Jain", "Priyamvada Nanjundiah", "Ujjal Kr Dutta", "Divya Sharma"], "title": "Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted In IEEE India Geoscience and Remote Sensing Symposium\n  (InGARSS) 2024", "summary": "In agricultural management, precise Ground Truth (GT) data is crucial for\naccurate Machine Learning (ML) based crop classification. Yet, issues like crop\nmislabeling and incorrect land identification are common. We propose a\nmulti-level GT cleaning framework while utilizing multi-temporal Sentinel-2\ndata to address these issues. Specifically, this framework utilizes generating\nembeddings for farmland, clustering similar crop profiles, and identification\nof outliers indicating GT errors. We validated clusters with False Colour\nComposite (FCC) checks and used distance-based metrics to scale and automate\nthis verification process. The importance of cleaning the GT data became\napparent when the models were trained on the clean and unclean data. For\ninstance, when we trained a Random Forest model with the clean GT data, we\nachieved upto 70\\% absolute percentage points higher for the F1 score metric.\nThis approach advances crop classification methodologies, with potential for\napplications towards improving loan underwriting and agricultural\ndecision-making.", "AI": {"title_translation": "缓解基于监督机器学习的作物分类中不良地面真实数据：一个利用Sentinel-2图像的多级框架", "tldr": "该研究提出了一个多级地面真实数据清洗框架，利用多时相Sentinel-2数据解决作物分类中地面真实数据不准确的问题，显著提升了模型性能。", "motivation": "在农业管理中，精确的地面真实（GT）数据对于基于机器学习的作物分类至关重要，但作物错标和土地识别错误等问题普遍存在。", "method": "该框架利用多时相Sentinel-2数据，通过为农田生成嵌入、聚类相似作物剖面以及识别表示GT错误的异常值来清洗GT数据。集群通过假彩色合成（FCC）检查进行验证，并使用基于距离的度量来扩展和自动化验证过程。", "result": "使用清洗后的GT数据训练模型时，性能显著提升。例如，使用清洗后的GT数据训练随机森林模型，F1分数指标提高了高达70%的绝对百分点。", "conclusion": "所提出的方法改进了作物分类方法，并有望应用于改善贷款承保和农业决策。", "translation": "在农业管理中，精确的地面真实（GT）数据对于基于机器学习（ML）的作物分类至关重要。然而，作物错标和土地识别错误等问题很常见。我们提出了一个多级GT清洗框架，同时利用多时相Sentinel-2数据来解决这些问题。具体而言，该框架利用为农田生成嵌入、聚类相似作物剖面以及识别表示GT错误的异常值。我们通过假彩色合成（FCC）检查验证了聚类，并使用基于距离的度量来扩展和自动化此验证过程。当模型在干净和不干净的数据上进行训练时，清洗GT数据的重要性变得显而易见。例如，当我们使用干净的GT数据训练随机森林模型时，F1分数指标提高了高达70%的绝对百分点。这种方法推进了作物分类方法，并有望应用于改善贷款承保和农业决策。", "summary": "本研究提出了一种多级地面真实（GT）数据清洗框架，旨在解决基于监督机器学习的作物分类中GT数据不准确的问题。该框架利用多时相Sentinel-2图像，通过生成农田嵌入、聚类作物剖面和识别异常值来清洗数据。实验结果表明，经过清洗的GT数据能显著提升模型性能，例如随机森林模型的F1分数提高了高达70%的绝对百分点，这对于改进农业决策具有重要应用潜力。", "keywords": "作物分类, 地面真实数据, 机器学习, Sentinel-2, 数据清洗", "comments": "该论文的创新点在于提出了一个利用多时相Sentinel-2数据进行多级地面真实数据清洗的框架，有效解决了监督机器学习中数据质量对模型性能的限制。其重要性体现在显著提升了作物分类的准确性，并为农业贷款和决策提供了更可靠的数据基础。"}}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "AI": {"title_translation": "迈向统一的哥白尼地球视觉基础模型", "tldr": "本文提出了一个名为Copernicus-FM的统一地球观测基础模型，它利用大规模预训练数据集、灵活的模型架构和全面的基准测试，旨在解决现有地球观测基础模型在传感器限制、数据范围和元数据利用方面的不足。", "motivation": "现有的地球观测（EO）基础模型大多局限于固定光谱传感器，仅关注地球表面，并忽视图像以外的宝贵元数据，这限制了其潜力和应用范围。", "method": "本文提出了下一代地球观测基础模型的三个关键组成部分：1）Copernicus-Pretrain，一个大规模预训练数据集，整合了来自所有主要哥白尼哨兵任务的1870万张对齐图像，涵盖地球表面到大气层；2）Copernicus-FM，一个统一的基础模型，能够使用扩展动态超网络和灵活的元数据编码处理任何光谱或非光谱传感器模态；3）Copernicus-Bench，一个系统的评估基准，包含15个分层下游任务，涵盖从预处理到每个哨兵任务的专业应用。", "result": "本文的数据集、模型和基准测试显著提高了地球观测基础模型的可扩展性、多功能性和多模态适应性，并为连接地球观测、天气和气候研究创造了新的机会。", "conclusion": "通过引入Copernicus-Pretrain数据集、Copernicus-FM统一模型和Copernicus-Bench基准测试，本文成功地向构建更通用、更强大的地球观测基础模型迈进了一步，为未来的地球科学研究提供了坚实的基础。", "translation": "地球观测（EO）基础模型的进步释放了利用大型卫星数据从太空学习通用表示的潜力，这有益于对我们星球至关重要的广泛下游应用。然而，现有的大多数努力仍然局限于固定的光谱传感器，只关注地球表面，并忽视了图像之外的宝贵元数据。在这项工作中，我们通过三个关键组件向下一代EO基础模型迈进：1）Copernicus-Pretrain，一个大规模的预训练数据集，整合了来自所有主要哥白尼哨兵任务的1870万张对齐图像，涵盖从地球表面到大气层；2）Copernicus-FM，一个统一的基础模型，能够使用扩展动态超网络和灵活的元数据编码处理任何光谱或非光谱传感器模态；3）Copernicus-Bench，一个系统的评估基准，包含15个分层下游任务，涵盖从预处理到每个哨兵任务的专业应用。我们的数据集、模型和基准测试极大地提高了EO基础模型的可扩展性、多功能性和多模态适应性，同时也为连接EO、天气和气候研究创造了新的机会。代码、数据集和模型可在https://github.com/zhu-xlab/Copernicus-FM获取。", "summary": "本文提出了一个名为Copernicus-FM的统一地球观测基础模型，旨在克服现有模型在传感器类型、数据覆盖范围和元数据利用方面的局限。该模型由三个核心部分构成：一个包含1870万张图像的大规模预训练数据集Copernicus-Pretrain，一个能处理多种传感器模态的统一基础模型Copernicus-FM，以及一个包含15个下游任务的系统评估基准Copernicus-Bench。研究表明，该方法显著提升了地球观测基础模型的可扩展性、多功能性和多模态适应性，并为地球观测、天气和气候研究的融合开辟了新途径。", "keywords": "地球观测, 基础模型, 哥白尼, 多模态, 遥感", "comments": "这项工作在地球观测领域具有重要的创新性，它通过构建一个统一的基础模型、大规模的跨任务数据集和全面的评估基准，解决了现有地球观测基础模型在数据模态、覆盖范围和元数据利用上的核心痛点。特别是其多模态处理能力和对大气层数据的整合，有望推动地球科学领域的研究范式转变，连接不同学科。其提供的代码、数据集和模型也大大降低了研究门槛，促进了社区发展。"}}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign.", "AI": {"title_translation": "DecAlign: 面向解耦多模态表示学习的层次化跨模态对齐", "tldr": "DecAlign提出了一种分层跨模态对齐框架，将多模态表示解耦为模态特有和模态通用特征，并在多模态基准测试中超越了现有SOTA方法。", "motivation": "多模态表示学习旨在捕获跨模态的共享和互补语义信息，但不同模态的内在异质性对有效的跨模态协作和集成提出了巨大挑战。", "method": "本文提出了DecAlign，一个新颖的层次化跨模态对齐框架，旨在将多模态表示解耦为模态特有（异质）和模态通用（同质）特征。为处理异质性，采用原型引导的最优传输对齐策略，利用高斯混合模型和多边际传输计划。为增强同质性，通过潜在分布匹配与最大均值差异（MMD）正则化来确保语义一致性。此外，引入多模态Transformer以增强高级语义特征融合。", "result": "在四个广泛使用的多模态基准测试上进行的广泛实验表明，DecAlign在五个指标上始终优于现有的最先进方法。", "conclusion": "DecAlign在增强卓越的跨模态对齐和语义一致性同时保留模态特有特征方面表现出有效性，标志着多模态表示学习场景的重大进步。", "translation": "多模态表示学习旨在捕获跨模态的共享和互补语义信息。然而，不同模态固有的异质性对实现有效的跨模态协作和集成提出了巨大挑战。为了解决这个问题，我们引入了DecAlign，一个新颖的层次化跨模态对齐框架，旨在将多模态表示解耦为模态特有（异质）和模态通用（同质）特征。为了处理异质性，我们采用了一种原型引导的最优传输对齐策略，利用高斯混合模型和多边际传输计划，从而在保留模态特有特征的同时减轻分布差异。为了增强同质性，我们通过将潜在分布匹配与最大均值差异正则化对齐来确保跨模态的语义一致性。此外，我们引入了一个多模态Transformer来增强高级语义特征融合，从而进一步减少跨模态不一致性。我们在四个广泛使用的多模态基准测试上进行的广泛实验表明，DecAlign在五个指标上始终优于现有的最先进方法。这些结果突显了DecAlign在增强卓越的跨模态对齐和语义一致性同时保留模态特有特征方面的有效性，标志着多模态表示学习场景的重大进步。我们的项目页面位于https://taco-group.github.io/DecAlign，代码可在https://github.com/taco-group/DecAlign获取。", "summary": "DecAlign是一种新颖的层次化跨模态对齐框架，旨在解决多模态表示学习中模态异质性带来的挑战。它通过将多模态表示解耦为模态特有和模态通用特征来实现这一点。该方法结合了原型引导的最优传输对齐策略来处理异质性，利用MMD正则化进行潜在分布匹配以增强同质性，并集成多模态Transformer进行高级特征融合。实验结果表明，DecAlign在多个多模态基准测试上显著优于现有SOTA方法。", "keywords": "多模态表示学习, 跨模态对齐, 特征解耦, 最优传输, 语义一致性", "comments": "DecAlign的创新之处在于其分层解耦策略，将多模态表示分解为模态特有和模态通用部分，并通过不同的机制（最优传输和MMD正则化）分别进行处理，有效解决了多模态数据固有的异质性问题，同时保持了语义一致性。"}}
{"id": "2503.11893", "pdf": "https://arxiv.org/pdf/2503.11893", "abs": "https://arxiv.org/abs/2503.11893", "authors": ["Md Abu Bakr Siddique", "Junliang Liu", "Piyush Singh", "Md Jahidul Islam"], "title": "UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The concept of waterbody style transfer remains largely unexplored in the\nunderwater imaging and vision literature. Traditional image style transfer\n(STx) methods primarily focus on artistic and photorealistic blending, often\nfailing to preserve object and scene geometry in images captured in\nhigh-scattering mediums such as underwater. The wavelength-dependent nonlinear\nattenuation and depth-dependent backscattering artifacts further complicate\nlearning underwater image STx from unpaired data. This paper introduces UStyle,\nthe first data-driven learning framework for transferring waterbody styles\nacross underwater images without requiring prior reference images or scene\ninformation. We propose a novel depth-aware whitening and coloring transform\n(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure\nperceptually consistent stylization while preserving scene structure. To\nenhance style transfer quality, we incorporate carefully designed loss\nfunctions that guide UStyle to maintain colorfulness, lightness, structural\nintegrity, and frequency-domain characteristics, as well as high-level content\nin VGG and CLIP (contrastive language-image pretraining) feature spaces. By\naddressing domain-specific challenges, UStyle provides a robust framework for\nno-reference underwater image STx, surpassing state-of-the-art (SOTA) methods\nthat rely solely on end-to-end reconstruction loss. Furthermore, we introduce\nthe UF7D dataset, a curated collection of high-resolution underwater images\nspanning seven distinct waterbody styles, establishing a benchmark to support\nfuture research in underwater image STx. The UStyle inference pipeline and UF7D\ndataset are released at: https://github.com/uf-robopi/UStyle.", "AI": {"title_translation": "UStyle：基于深度引导特征合成的水下场景水体风格迁移", "tldr": "UStyle是首个无需参考图的水下图像水体风格迁移框架，通过深度感知变换和特定损失函数，实现了感知一致且结构保留的风格迁移，并发布了UF7D数据集。", "motivation": "传统图像风格迁移方法在水下高散射介质中难以保持物体和场景几何；波长依赖的非线性衰减和深度依赖的反向散射伪影使从非配对数据中学习水下图像风格迁移复杂；水体风格迁移的概念在水下成像和视觉文献中仍未被充分探索。", "method": "本文提出了UStyle，首个数据驱动的无参考水下图像水体风格迁移框架。核心是新颖的深度感知白化和着色变换（DA-WCT）机制，该机制集成了基于物理的水体合成，以确保感知上一致的风格化同时保留场景结构。通过精心设计的损失函数（保持色彩、亮度、结构完整性、频域特性以及VGG和CLIP特征空间中的高级内容）来指导风格迁移质量。此外，还引入了UF7D数据集，一个包含七种不同水体风格的高分辨率水下图像集合，作为基准。", "result": "UStyle为无参考水下图像风格迁移提供了一个鲁棒的框架，超越了仅依赖端到端重建损失的最新（SOTA）方法。引入了UF7D数据集，为未来水下图像风格迁移研究建立了基准。", "conclusion": "UStyle成功解决了水下图像风格迁移的领域特定挑战，提供了一个无需参考图像的鲁棒框架。通过引入UF7D数据集，为水下图像风格迁移领域的研究奠定了基础。", "translation": "水体风格迁移的概念在水下成像和视觉文献中仍未被充分探索。传统的图像风格迁移（STx）方法主要侧重于艺术性和真实感融合，但往往无法在高散射介质（如水下）中捕获的图像中保留物体和场景几何。波长依赖的非线性衰减和深度依赖的反向散射伪影进一步使从非配对数据中学习水下图像STx变得复杂。本文介绍了UStyle，这是第一个数据驱动的学习框架，用于在不需要先验参考图像或场景信息的情况下，在水下图像之间进行水体风格迁移。我们提出了一种新颖的深度感知白化和着色变换（DA-WCT）机制，该机制集成了基于物理的水体合成，以确保感知上一致的风格化，同时保留场景结构。为了提高风格迁移质量，我们结合了精心设计的损失函数，引导UStyle在VGG和CLIP（对比语言-图像预训练）特征空间中保持色彩、亮度、结构完整性、频域特性以及高级内容。通过解决领域特定挑战，UStyle为无参考水下图像STx提供了一个鲁棒的框架，超越了仅依赖端到端重建损失的最新（SOTA）方法。此外，我们引入了UF7D数据集，这是一个精心策划的高分辨率水下图像集合，涵盖七种不同的水体风格，为支持未来水下图像STx研究建立了基准。UStyle推理管道和UF7D数据集已在https://github.com/uf-robopi/UStyle发布。", "summary": "本文提出了UStyle，首个用于水下图像水体风格迁移的数据驱动框架。针对传统方法在水下场景中难以保持结构和受复杂光学效应影响的问题，UStyle引入了深度感知白化和着色变换（DA-WCT）机制，结合物理合成，确保感知一致性和结构保留。通过多重损失函数指导，UStyle在色彩、亮度、结构和特征空间表现优异，超越了现有SOTA方法。此外，论文还发布了UF7D数据集，为水下图像风格迁移研究提供了新基准。", "keywords": "水体风格迁移, 水下图像, 深度感知, 特征合成, UF7D数据集", "comments": "该论文的创新之处在于首次提出了一种无需参考图像的水下水体风格迁移框架，有效解决了水下图像特有的复杂光学效应导致的结构保持难题。通过结合深度感知信息和物理模型（DA-WCT），UStyle在保证感知一致性的同时，显著提升了水下风格迁移的质量。此外，引入新的UF7D数据集，为该领域未来的研究提供了宝贵的基准。"}}
{"id": "2503.11905", "pdf": "https://arxiv.org/pdf/2503.11905", "abs": "https://arxiv.org/abs/2503.11905", "authors": ["Ruchika Chavhan", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Luca Morreale", "Mehdi Noroozi", "Sourav Bhattacharya"], "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Text-to-image synthesis has witnessed remarkable advancements in recent\nyears. Many attempts have been made to adopt text-to-image models to support\nmultiple tasks. However, existing approaches typically require\nresource-intensive re-training or additional parameters to accommodate for the\nnew tasks, which makes the model inefficient for on-device deployment. We\npropose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends\nthe capabilities of a pre-trained text-to-image diffusion model to support a\nvariety of image-to-image generation tasks. MTU replaces Feed-Forward Network\n(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,\nand combines them with a dynamic routing mechanism. To the best of our\nknowledge, MTU is the first multi-task diffusion modeling approach that\nseamlessly blends multi-tasking with on-device compatibility, by mitigating the\nissue of parameter inflation. We show that the performance of MTU is on par\nwith the single-task fine-tuned diffusion models across several tasks including\nimage editing, super-resolution, and inpainting, while maintaining similar\nlatency and computational load (GFLOPs) as the single-task fine-tuned models.", "AI": {"title_translation": "升级改造文本到图像扩散模型以实现多任务能力", "tldr": "MTU通过替换FFN层和动态路由，使预训练的文本到图像扩散模型在不增加参数和计算量的情况下支持多任务，性能与单任务模型相当，适合设备部署。", "motivation": "现有方法在使文本到图像模型支持多任务时，通常需要资源密集型再训练或额外参数，导致模型效率低下，不适合设备部署。", "method": "提出多任务升级改造（MTU）方法，通过将扩散模型中的前馈网络（FFN）层替换为更小的FFN（称为专家），并结合动态路由机制，将预训练的文本到图像扩散模型扩展到多种图像到图像生成任务。", "result": "MTU在图像编辑、超分辨率和图像修复等任务上的性能与单任务微调扩散模型相当，同时保持相似的延迟和计算负载（GFLOPs）。", "conclusion": "MTU是首个将多任务处理与设备兼容性无缝结合的多任务扩散建模方法，通过缓解参数膨胀问题，实现了高效的多任务图像生成。", "translation": "文本到图像合成近年来取得了显著进展。许多尝试都致力于使文本到图像模型支持多任务。然而，现有方法通常需要资源密集型再训练或额外参数来适应新任务，这使得模型在设备部署时效率低下。我们提出了多任务升级改造（MTU），一个简单而有效的方法，它扩展了预训练的文本到图像扩散模型的能力，以支持各种图像到图像生成任务。MTU用更小的FFN（称为专家）替换扩散模型中的前馈网络（FFN）层，并将其与动态路由机制结合。据我们所知，MTU是第一个将多任务处理与设备兼容性无缝融合的多任务扩散建模方法，通过缓解参数膨胀问题。我们表明，MTU的性能在图像编辑、超分辨率和图像修复等多个任务上与单任务微调扩散模型相当，同时保持与单任务微调模型相似的延迟和计算负载（GFLOPs）。", "summary": "本文提出了一种名为多任务升级改造（MTU）的新方法，旨在使预训练的文本到图像扩散模型能够高效地支持多种图像到图像生成任务，同时解决现有方法中参数膨胀和资源消耗大的问题。MTU通过将扩散模型中的前馈网络（FFN）层替换为更小的“专家”FFN，并结合动态路由机制来实现。实验证明，MTU在图像编辑、超分辨率和图像修复等任务上表现出与单任务微调模型相当的性能，并保持了相似的计算效率和低延迟，使其适用于设备部署。", "keywords": "扩散模型, 多任务学习, 文本到图像, 模型升级, 设备部署", "comments": "MTU的创新之处在于其通过替换FFN层和引入动态路由机制，在不显著增加模型参数和计算量的情况下，成功地将文本到图像扩散模型“升级”为多任务模型。这对于实现模型在资源受限设备上的部署具有重要意义，解决了现有方法中效率低下的痛点。其核心贡献在于提供了一种高效且兼容设备的多任务扩散模型范式。"}}
{"id": "2503.11906", "pdf": "https://arxiv.org/pdf/2503.11906", "abs": "https://arxiv.org/abs/2503.11906", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Emanuele Salerno"], "title": "A Survey on SAR ship classification using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to JSTARS journal", "summary": "Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture\nRadar (SAR) ship classification. This survey comprehensively analyzes the\ndiverse DL techniques employed in this domain. We identify critical trends and\nchallenges, highlighting the importance of integrating handcrafted features,\nutilizing public datasets, data augmentation, fine-tuning, explainability\ntechniques, and fostering interdisciplinary collaborations to improve DL model\nperformance. This survey establishes a first-of-its-kind taxonomy for\ncategorizing relevant research based on DL models, handcrafted feature use, SAR\nattribute utilization, and the impact of fine-tuning. We discuss the\nmethodologies used in SAR ship classification tasks and the impact of different\ntechniques. Finally, the survey explores potential avenues for future research,\nincluding addressing data scarcity, exploring novel DL architectures,\nincorporating interpretability techniques, and establishing standardized\nperformance metrics. By addressing these challenges and leveraging advancements\nin DL, researchers can contribute to developing more accurate and efficient\nship classification systems, ultimately enhancing maritime surveillance and\nrelated applications.", "AI": {"title_translation": "基于深度学习的SAR舰船分类综述", "tldr": "该综述全面分析了深度学习在SAR舰船分类中的应用，识别了关键趋势、挑战，并提出了未来研究方向。", "motivation": "深度学习已成为合成孔径雷达（SAR）舰船分类的强大工具，本综述旨在全面分析该领域中使用的各种深度学习技术。", "method": "本综述全面分析了SAR舰船分类中采用的各种深度学习技术，识别了关键趋势和挑战，并建立了一个首次提出的分类法，用于根据深度学习模型、手工特征使用、SAR属性利用和微调影响来对相关研究进行分类。此外，还讨论了所使用的方法和不同技术的影响。", "result": "综述识别了提升深度学习模型性能的关键趋势和挑战，包括整合手工特征、利用公共数据集、数据增强、微调、可解释性技术以及促进跨学科合作的重要性。同时，建立了一个首次提出的分类法。", "conclusion": "未来研究方向包括解决数据稀缺问题、探索新颖的深度学习架构、整合可解释性技术以及建立标准化的性能指标。通过应对这些挑战并利用深度学习的进步，研究人员可以开发更准确、高效的舰船分类系统，从而加强海上监视及相关应用。", "translation": "深度学习（DL）已成为合成孔径雷达（SAR）舰船分类的强大工具。本综述全面分析了该领域中采用的各种深度学习技术。我们识别了关键趋势和挑战，强调了整合手工特征、利用公共数据集、数据增强、微调、可解释性技术以及促进跨学科合作对于提高深度学习模型性能的重要性。本综述建立了一个首次提出的分类法，用于根据深度学习模型、手工特征使用、SAR属性利用和微调影响来对相关研究进行分类。我们讨论了SAR舰船分类任务中使用的方法以及不同技术的影响。最后，本综述探讨了未来研究的潜在途径，包括解决数据稀缺问题、探索新颖的深度学习架构、整合可解释性技术以及建立标准化的性能指标。通过应对这些挑战并利用深度学习的进步，研究人员可以开发更准确、高效的舰船分类系统，最终增强海上监视及相关应用。", "summary": "本综述全面分析了深度学习在合成孔径雷达（SAR）舰船分类中的应用。文章识别了关键趋势和挑战，强调了整合手工特征、利用公共数据集、数据增强、微调和可解释性技术的重要性。它还建立了一个基于深度学习模型、特征使用、SAR属性和微调影响的分类法。最后，综述探讨了未来研究方向，如解决数据稀缺、探索新架构和整合可解释性技术，旨在推动更准确高效的舰船分类系统发展。", "keywords": "深度学习, SAR, 舰船分类, 综述, 分类法", "comments": "这是一篇有价值的综述性论文，它系统地梳理了深度学习在SAR舰船分类领域的现状。其创新点在于首次提出了一个全面的分类法，有助于研究人员更好地理解和组织现有工作。论文对关键趋势、挑战和未来研究方向的识别，为该领域的研究人员提供了清晰的指导，对于促进SAR舰船分类技术的发展具有重要意义。"}}
{"id": "2503.11919", "pdf": "https://arxiv.org/pdf/2503.11919", "abs": "https://arxiv.org/abs/2503.11919", "authors": ["Jeonghwan Park", "Kang Li", "Huiyu Zhou"], "title": "k-fold Subsampling based Sequential Backward Feature Elimination", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy.", "AI": {"title_translation": "基于k折子采样序贯反向特征消除", "tldr": "提出了一种新的混合特征选择算法，用于人体检测，通过k折子采样和序贯反向消除，显著提高了SVM分类器的检测速度和准确性。", "motivation": "该研究旨在为人脸检测选择一个最优的特征向量，以更好地表示图像中主体的形状，并提高检测性能。", "method": "该算法是一种混合特征选择方法，结合了滤波器和包装器方法的优点。它采用k折子采样和序贯反向消除方法，并使用标准线性支持向量机（SVM）作为人体检测的分类器。", "result": "所提出的算法在INRIA和ETH行人数据集上进行了应用。与现有最先进算法相比，该方法可以将SVM分类器的检测速度提高50%以上，检测精度提高2%；与可变形部件模型方法相比，检测精度提高了约9%。", "conclusion": "该研究提出的基于k折子采样的序贯反向特征消除算法，能够显著提高人体检测的速度和准确性，优于现有的先进算法和可变形部件模型方法。", "translation": "我们提出了一种用于人体检测的新的包装器特征选择算法。该算法是一种混合特征选择方法，结合了滤波器和包装器方法的优点。它允许选择一个最优的特征向量，该向量能够很好地表示图像中主体的形状。具体而言，所提出的特征选择算法采用k折子采样和序贯反向消除方法，同时使用标准线性支持向量机（SVM）作为人体检测的分类器。我们将所提出的算法应用于公开可访问的INRIA和ETH行人完整图像数据集，并采用PASCAL VOC评估标准。与其他最先进的算法相比，我们基于特征选择的方法可以将SVM分类器的检测速度提高50%以上，检测精度提高2%。我们的算法在检测精度方面也优于可变形部件模型方法中引入的等效系统，提高了约9%。", "summary": "本文提出了一种名为“基于k折子采样序贯反向特征消除”的新型混合特征选择算法，专为人脸检测设计。该算法结合了滤波器和包装器方法的优势，旨在选择最优特征向量来有效表示图像中人体的形状。它采用k折子采样和序贯反向消除策略，并以标准线性支持向量机（SVM）作为分类器。在公开的INRIA和ETH行人数据集上，该算法显著提升了SVM分类器的检测速度（超过50%）和准确性（最高2%），同时在检测精度上比可变形部件模型方法提高了约9%。", "keywords": "特征选择, k折子采样, 序贯反向消除, 人体检测, SVM", "comments": "该论文提出了一种创新的混合特征选择算法，通过结合k折子采样和序贯反向消除，并在SVM分类器上应用，显著提升了人体检测的性能。其主要创新在于将两种特征选择方法的优点结合，并在实际数据集上展示了优异的速度和精度提升，这对于实时人体检测系统具有重要意义。"}}
{"id": "2503.11930", "pdf": "https://arxiv.org/pdf/2503.11930", "abs": "https://arxiv.org/abs/2503.11930", "authors": ["Jingxuan Zhang", "Robert J. Hart", "Ziqian Bi", "Shiaofen Fang", "Susan Walsh"], "title": "Generating a Biometrically Unique and Realistic Iris Database", "categories": ["cs.CV", "cs.LG"], "comment": "for associated iris database, see\n  https://huggingface.co/datasets/fatdove/Iris_Database", "summary": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity.", "AI": {"title_translation": "生成一个生物识别唯一且真实的虹膜数据库", "tldr": "该研究通过训练扩散模型，生成了生物识别唯一且真实的虹膜图像数据库，以解决虹膜数据获取中的隐私和伦理问题。", "motivation": "在虹膜作为生物识别标识符的使用日益增加的背景下，研究人员在获取虹膜图像数据库时面临隐私和伦理方面的担忧和困难，这阻碍了生物识别研究的进行。", "method": "通过在开源扩散框架内训练一个扩散模型，创建了一个包含逼真、生物识别不可识别的彩色虹膜图像的数据库。", "result": "验证了模型能够创建与训练数据生物识别唯一的虹膜纹理，并且模型输出能够生成完整分布的逼真虹膜色素沉着。", "conclusion": "扩散网络能够相对容易地实现这些标准，因此值得在虹膜数据库生成和演示攻击安全领域进行更多研究。", "translation": "在过去的30年里，虹膜作为生物识别标识符的使用急剧增加，这引发了对研究中使用虹膜图像的隐私和安全担忧。由于伦理问题，获取虹膜图像数据库可能很困难，这可能成为进行生物识别研究的障碍。在本文中，我们描述并展示了如何通过在开源扩散框架内训练扩散模型，创建一个包含逼真、生物识别不可识别的彩色虹膜图像数据库。我们不仅能够验证我们的模型能够创建与训练数据生物识别唯一的虹膜纹理，而且还能够验证我们的模型输出能够创建完整分布的逼真虹膜色素沉着。我们强调，扩散网络能够相对容易地实现这些标准，这保证了在其在虹膜数据库生成和演示攻击安全背景下的使用进行额外研究。", "summary": "本研究旨在解决虹膜生物识别研究中由于隐私和伦理问题导致的虹膜图像数据库获取困难。通过在开源扩散框架中训练一个扩散模型，成功生成了一个生物识别唯一且逼真的彩色虹膜图像数据库。研究结果表明，该模型不仅能生成与训练数据生物特征上独特的虹膜纹理，还能创建完整分布的逼真虹膜色素沉着。这突显了扩散网络在虹膜数据库生成和演示攻击安全方面的巨大潜力，并呼吁进行进一步研究。", "keywords": "虹膜数据库, 扩散模型, 生物识别, 数据生成, 隐私", "comments": "该论文通过使用扩散模型生成合成的生物识别虹膜数据库，有效地解决了虹膜数据获取中存在的隐私和伦理问题，这在生物识别研究领域具有重要意义。其创新之处在于利用扩散模型生成生物识别唯一且逼真的虹膜图像，为未来虹膜识别算法的开发和测试提供了丰富的、无隐私风险的数据集。这种方法有望加速生物识别技术的发展，同时维护个人隐私。"}}
{"id": "2503.11932", "pdf": "https://arxiv.org/pdf/2503.11932", "abs": "https://arxiv.org/abs/2503.11932", "authors": ["Dhruv Kudale", "Badri Vishal Kasuba", "Venkatapathy Subramanian", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "title": "SPRINT: Script-agnostic Structure Recognition in Tables", "categories": ["cs.CV"], "comment": "Accepted at ICDAR 2024", "summary": "Table Structure Recognition (TSR) is vital for various downstream tasks like\ninformation retrieval, table reconstruction, and document understanding. While\nmost state-of-the-art (SOTA) research predominantly focuses on TSR in English\ndocuments, the need for similar capabilities in other languages is evident,\nconsidering the global diversity of data. Moreover, creating substantial\nlabeled data in non-English languages and training these SOTA models from\nscratch is costly and time-consuming. We propose TSR as a language-agnostic\ncell arrangement prediction and introduce SPRINT, Script-agnostic Structure\nRecognition in Tables. SPRINT uses recently introduced Optimized Table\nStructure Language (OTSL) sequences to predict table structures. We show that\nwhen coupled with a pre-trained table grid estimator, SPRINT can improve the\noverall tree edit distance-based similarity structure scores of tables even for\nnon-English documents. We experimentally evaluate our performance across\nbenchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our\nfindings reveal that SPRINT not only matches SOTA models in performance on\nstandard datasets but also demonstrates lower latency. Additionally, SPRINT\nexcels in accurately identifying table structures in non-English documents,\nsurpassing current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predictions into\na widely used HTML-based table representation. To encourage further research,\nwe release our code and Multilingual Scanned and Scene Table Structure\nRecognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in\nthirteen languages encompassing several scripts at\nhttps://github.com/IITB-LEAP-OCR/SPRINT", "AI": {"title_translation": "SPRINT：表格中的脚本无关结构识别", "tldr": "SPRINT是一种新的表格结构识别模型，对多种语言都有效，性能媲美甚至超越现有最佳模型，且延迟更低。", "motivation": "现有的表格结构识别（TSR）研究主要集中在英文文档，但考虑到全球数据的多样性，其他语言也迫切需要TSR能力。在非英文语言中创建大量标注数据并从头训练现有最佳模型成本高昂且耗时。", "method": "本文提出将表格结构识别（TSR）视为一种语言无关的单元格排列预测任务，并引入了名为SPRINT的模型。SPRINT利用最近引入的优化表格结构语言（OTSL）序列来预测表格结构。当与预训练的表格网格估计器结合使用时，SPRINT可以提高表格的整体基于树编辑距离的相似性结构分数，即使对于非英文文档也是如此。", "result": "SPRINT在标准数据集上的性能与现有最佳模型相匹配，并且显示出更低的延迟。此外，SPRINT在准确识别非英文文档中的表格结构方面表现出色，比当前领先模型平均绝对提升了11.12%。", "conclusion": "SPRINT模型在多语言表格结构识别方面表现出卓越的性能，超越了现有领先模型，并且具有较低的延迟。研究团队还发布了代码和多语言数据集MUSTARD，以促进进一步的研究。", "translation": "表格结构识别（TSR）对于信息检索、表格重建和文档理解等各种下游任务至关重要。虽然大多数最先进（SOTA）的研究主要集中在英文文档中的TSR，但考虑到全球数据的多样性，其他语言对类似能力的需求显而易见。此外，在非英文语言中创建大量标注数据并从头训练这些SOTA模型成本高昂且耗时。我们提出将TSR视为一种语言无关的单元格排列预测，并引入了SPRINT，即表格中的脚本无关结构识别。SPRINT使用最近引入的优化表格结构语言（OTSL）序列来预测表格结构。我们表明，当与预训练的表格网格估计器结合使用时，SPRINT可以提高表格的整体基于树编辑距离的相似性结构分数，即使对于非英文文档也是如此。我们在包括PubTabNet、FinTabNet和PubTables-1M在内的基准TSR数据集上实验评估了我们的性能。我们的发现表明，SPRINT不仅在标准数据集上的性能与SOTA模型相匹配，而且还表现出更低的延迟。此外，SPRINT在准确识别非英文文档中的表格结构方面表现出色，超越了当前领先模型，显示出平均绝对增加了11.12%。我们还提出了一种将有效OTSL预测转换为广泛使用的基于HTML的表格表示的算法。为了鼓励进一步的研究，我们发布了我们的代码和多语言扫描和场景表格结构识别数据集MUSTARD，其中包含1428个表格，涵盖13种语言和多种脚本，并标注了OTSL序列，可在https://github.com/IITB-LEAP-OCR/SPRINT获取。", "summary": "本文提出了SPRINT，一个用于表格中脚本无关结构识别的模型，旨在解决现有表格结构识别（TSR）模型主要关注英文文档的问题。SPRINT将TSR视为语言无关的单元格排列预测任务，并利用优化表格结构语言（OTSL）序列进行结构预测。实验证明，SPRINT在标准数据集上性能与现有最佳模型相当，且延迟更低，同时在非英文文档的表格结构识别方面表现出显著优势，平均性能提升11.12%。研究还提供了一种将OTSL预测转换为HTML表格表示的算法，并开源了代码和多语言数据集MUSTARD以促进研究。", "keywords": "表格结构识别, 脚本无关, 多语言, OTSL, SPRINT", "comments": "SPRINT的创新之处在于其“脚本无关”的方法，将TSR任务重新定义为语言无关的单元格排列预测，并通过引入OTSL序列解决了多语言表格结构识别的挑战。这对于处理全球多样化数据具有重要意义，因为它降低了为非英文语言创建大量标注数据的成本和时间。其在非英文文档上的显著性能提升以及代码和数据集的发布，将极大地推动多语言TSR领域的研究进展。"}}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "AI": {"title_translation": "表情识别解决方案设计：采用全局通道-空间注意力机制", "tldr": "本文提出一种结合全局通道-空间注意力机制和多模态融合的表情识别方法，并在ABAW竞赛中取得了优异成绩。", "motivation": "解决视频中面部表情识别因细微表情变化和多尺度问题导致的识别准确率低的问题，并为即将到来的ABAW竞赛提供解决方案。", "method": "该方法首先提出全局通道-空间注意力机制和中值增强空间-通道注意力机制，分别用于加强语音和图像的特征处理。其次，采用语音-面部表情关键帧对齐技术来计算语音和面部表情的权重，并将这些权重输入到特征融合层进行多尺度扩张融合，以充分利用模态互补性。", "result": "在第6届ABAW竞赛的面部表情识别任务中，该方法在官方验证集上取得了优异成绩。", "conclusion": "所提出的方法在面部表情识别方面有效且具有竞争力。", "translation": "面部表情识别是一项具有挑战性的分类任务，在人机交互领域具有广阔的应用前景。本文旨在介绍我们即将参加的第八届野外情感行为分析（ABAW）竞赛的方法，该竞赛将于CVPR2025举行。为了解决视频中面部表情识别因细微表情变化和多尺度问题导致的识别准确率低等问题，我们提出了全局通道-空间注意力机制和中值增强空间-通道注意力机制，分别用于加强语音和图像的特征处理。其次，为了充分利用语音和面部表情模态之间的互补性，本文采用了语音-面部表情关键帧对齐技术来计算语音和面部表情的权重。这些权重被输入到特征融合层进行多尺度扩张融合，从而有效提高了面部表情识别的识别率。在第六届ABAW竞赛的面部表情识别任务中，我们的方法在官方验证集上取得了优异成绩，充分证明了所提出方法的有效性和竞争力。", "summary": "本文针对视频中面部表情识别（FER）面临的低准确率和多尺度问题，提出了一种新的解决方案。该方案结合了全局通道-空间注意力机制和中值增强空间-通道注意力机制，分别用于强化语音和图像特征。同时，通过语音-面部表情关键帧对齐技术计算模态权重，并进行多尺度扩张融合，以充分利用多模态信息。该方法在第6届ABAW竞赛的FER任务中表现出色，证明了其有效性和竞争力。", "keywords": "面部表情识别, 注意力机制, 多模态融合, ABAW竞赛, 人机交互", "comments": "该论文创新性地结合了全局通道-空间注意力机制和多模态（语音和面部表情）融合，并通过关键帧对齐和多尺度扩张融合来提升视频表情识别的准确性。其在ABAW竞赛中的优异表现证明了方法的有效性和实用性，对人机交互领域具有重要意义。"}}
{"id": "2503.11937", "pdf": "https://arxiv.org/pdf/2503.11937", "abs": "https://arxiv.org/abs/2503.11937", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.", "AI": {"title_translation": "Att-Adapter：一种通过条件变分自编码器实现的鲁棒精确领域特定多属性T2I扩散适配器", "tldr": "Att-Adapter是一个新的即插即用模块，用于在预训练的T2I扩散模型中实现精确的多属性控制，解决了现有方法在连续多属性控制上的挑战。", "motivation": "现有T2I扩散模型在新的领域中，通过纯文本指导同时精确控制多个连续属性（如眼睛开合度、汽车宽度）仍然是一个重大挑战。", "method": "引入了属性（Att）适配器，一个即插即用模块。它通过一组未配对且包含多个视觉属性的样本图像学习一个单一的控制适配器。Att-Adapter利用解耦的交叉注意力模块将多个领域属性与文本条件自然协调。此外，引入条件变分自编码器（CVAE）以减轻过拟合。", "result": "Att-Adapter在两个公共数据集上优于所有基于LoRA的基线，在控制连续属性方面表现更好。它还实现了更宽的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。该方法不需要配对的合成数据进行训练，并且易于扩展到单个模型中的多个属性。", "conclusion": "Att-Adapter是一个有效且灵活的解决方案，能够对预训练的T2I扩散模型进行精确、多属性的连续控制，且无需配对数据。", "translation": "文本到图像（T2I）扩散模型在生成高质量图像方面取得了卓越的性能。然而，在新的领域（例如，眼睛开合度或汽车宽度等数值）中，仅通过文本指导实现对连续属性的精确控制，尤其是同时控制多个属性，仍然是一个重大挑战。为了解决这个问题，我们引入了属性（Att）适配器，这是一种新颖的即插即用模块，旨在使预训练的扩散模型能够进行细粒度的多属性控制。我们的方法从一组可以是非配对且包含多个视觉属性的样本图像中学习一个单一的控制适配器。Att-Adapter利用解耦的交叉注意力模块自然地协调多个领域属性与文本条件。我们进一步在Att-Adapter中引入了条件变分自编码器（CVAE）以减轻过拟合，以匹配视觉世界的丰富多样性。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线。此外，我们的方法实现了更宽的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter非常灵活，训练不需要配对的合成数据，并且易于扩展到单个模型中的多个属性。", "summary": "本文提出了一种名为Att-Adapter的新型即插即用模块，用于解决文本到图像（T2I）扩散模型在新的领域中同时精确控制多个连续属性的挑战。Att-Adapter从非配对图像样本中学习，通过解耦的交叉注意力模块和条件变分自编码器（CVAE）实现多属性与文本条件的协调，并减轻过拟合。实验证明，Att-Adapter在连续属性控制方面优于基于LoRA和StyleGAN的基线，提供更宽的控制范围和更好的属性解耦，且无需配对合成数据。", "keywords": "文本到图像, 扩散模型, 属性控制, 适配器, 条件变分自编码器", "comments": "Att-Adapter的创新性在于其即插即用设计和处理未配对多属性数据的能力，以及通过CVAE缓解过拟合。其灵活性（无需配对数据）和可扩展性是其重要优势，使其在现实世界应用中更具潜力。"}}
{"id": "2503.11945", "pdf": "https://arxiv.org/pdf/2503.11945", "abs": "https://arxiv.org/abs/2503.11945", "authors": ["Naresh Kumar Devulapally", "Mingzhen Huang", "Vishal Asnani", "Shruti Agarwal", "Siwei Lyu", "Vishnu Suresh Lokhande"], "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Invisible watermarking of AI-generated images can help with copyright\nprotection, enabling detection and identification of AI-generated media. In\nthis work, we present a novel approach to watermark images of T2I Latent\nDiffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we\nenable watermarking in selected objects or parts of the image, offering greater\nflexibility compared to traditional full-image watermarking. Our method\nleverages the text encoder's compatibility across various LDMs, allowing\nplug-and-play integration for different LDMs. Moreover, introducing the\nwatermark early in the encoding stage improves robustness to adversarial\nperturbations in later stages of the pipeline. Our approach achieves $99\\%$ bit\naccuracy ($48$ bits) with a $10^5 \\times$ reduction in model parameters,\nenabling efficient watermarking.", "AI": {"title_translation": "您的文本编码器可以成为对象级水印控制器", "tldr": "提出一种针对T2I LDM的文本编码器水印方法，通过微调文本嵌入实现对象级、高精度、高鲁棒性且参数高效的水印。", "motivation": "隐形水印对于AI生成图像的版权保护、检测和识别至关重要。", "method": "该方法通过仅微调文本编码器的文本token嵌入$W_*$，在T2I潜在扩散模型（LDMs）中实现图像水印。它支持在图像的特定对象或部分进行水印（对象级水印），相比传统全图像水印更灵活。该方法利用文本编码器在不同LDM间的兼容性，并早期在编码阶段引入水印以提高鲁棒性。", "result": "实现了99%的比特精度（48比特），同时模型参数减少了10^5倍，证明了其高效性。", "conclusion": "该方法提供了一种高效、灵活且鲁棒的对象级AI生成图像水印解决方案。", "translation": "AI生成图像的隐形水印有助于版权保护，从而实现AI生成媒体的检测和识别。在这项工作中，我们提出了一种对T2I潜在扩散模型（LDMs）图像进行水印的新方法。通过仅微调文本token嵌入$W_*$，我们可以在图像的选定对象或部分中实现水印，与传统的全图像水印相比，提供了更大的灵活性。我们的方法利用了文本编码器在各种LDM中的兼容性，允许即插即用集成到不同的LDM中。此外，在编码阶段早期引入水印提高了对管道后期对抗性扰动的鲁棒性。我们的方法以模型参数减少10^5倍的代价实现了99%的比特精度（48比特），从而实现了高效水印。", "summary": "本文提出一种新颖的AI生成图像隐形水印方法，专注于T2I潜在扩散模型（LDMs）。该方法通过仅微调文本编码器的文本token嵌入，实现了对象级水印，提供了比传统方法更高的灵活性。它还利用了文本编码器的跨模型兼容性，并在早期编码阶段嵌入水印以增强对抗性鲁棒性。实验结果表明，该方法在大幅减少参数的同时，仍能实现高精度水印。", "keywords": "文本编码器, 水印, AI生成图像, 潜在扩散模型, 对象级水印", "comments": "该论文的创新点在于利用文本编码器实现图像的对象级水印，这比传统的全图像水印更具灵活性和实用性。其即插即用特性和显著减少模型参数的能力，使其在实际应用中更具吸引力。此外，在编码早期阶段引入水印以提高鲁棒性的策略也很有价值。"}}
{"id": "2503.11953", "pdf": "https://arxiv.org/pdf/2503.11953", "abs": "https://arxiv.org/abs/2503.11953", "authors": ["Priyanka Mandikal", "Tushar Nagarajan", "Alex Stoken", "Zihui Xue", "Kristen Grauman"], "title": "SPOC: Spatially-Progressing Object State Change Segmentation in Video", "categories": ["cs.CV"], "comment": null, "summary": "Object state changes in video reveal critical information about human and\nagent activity. However, existing methods are limited to temporal localization\nof when the object is in its initial state (e.g., the unchopped avocado) versus\nwhen it has completed a state change (e.g., the chopped avocado), which limits\napplicability for any task requiring detailed information about the progress of\nthe actions and its spatial localization. We propose to deepen the problem by\nintroducing the spatially-progressing object state change segmentation task.\nThe goal is to segment at the pixel-level those regions of an object that are\nactionable and those that are transformed. We introduce the first model to\naddress this task, designing a VLM-based pseudo-labeling approach, state-change\ndynamics constraints, and a novel WhereToChange benchmark built on in-the-wild\nInternet videos. Experiments on two datasets validate both the challenge of the\nnew task as well as the promise of our model for localizing exactly where and\nhow fast objects are changing in video. We further demonstrate useful\nimplications for tracking activity progress to benefit robotic agents. Project\npage: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc", "AI": {"title_translation": "SPOC：视频中空间渐进式对象状态变化分割", "tldr": "本文提出了一个新任务——空间渐进式对象状态变化分割，旨在像素级别识别对象中正在发生变化和已变化的部分，并提出了首个基于VLM伪标签、状态变化动态约束的模型，以及新的WhereToChange基准。", "motivation": "现有方法仅限于时间上定位对象状态变化的初始和完成时刻，这限制了其在需要详细动作进程和空间定位信息的任务中的适用性。", "method": "本文提出了空间渐进式对象状态变化分割任务，目标是在像素级别分割对象中可操作和已转换的区域。引入了首个解决该任务的模型，该模型设计了基于VLM的伪标签方法、状态变化动态约束，以及一个基于野外互联网视频构建的WhereToChange基准。", "result": "在两个数据集上的实验验证了新任务的挑战性以及模型在视频中精确定位对象变化位置和速度的潜力。此外，还展示了其在跟踪活动进程以造福机器人代理方面的有用影响。", "conclusion": "本文提出的SPOC模型和空间渐进式对象状态变化分割任务，能够精细地识别视频中对象状态变化的空间和时间进程，对于需要详细动作信息（如机器人代理）的应用具有重要意义。", "translation": "视频中对象状态的变化揭示了关于人类和代理活动的关键信息。然而，现有方法仅限于时间上定位对象处于初始状态（例如，未切的牛油果）与完成状态变化（例如，切好的牛油果）的时间点，这限制了其在需要关于动作进程及其空间定位的详细信息的任何任务中的适用性。我们通过引入空间渐进式对象状态变化分割任务来深化这个问题。目标是在像素级别分割对象中那些可操作的和已转换的区域。我们引入了第一个解决此任务的模型，设计了一种基于VLM的伪标签方法、状态变化动态约束，以及一个建立在野外互联网视频上的新型WhereToChange基准。在两个数据集上的实验验证了新任务的挑战性以及我们模型在视频中精确定位对象变化位置和速度的潜力。我们进一步证明了其对跟踪活动进程以造福机器人代理的有用影响。项目页面：https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc", "summary": "本文针对现有对象状态变化检测方法仅限于时间定位的局限性，提出了一个新颖的“空间渐进式对象状态变化分割”任务。该任务旨在像素级别识别视频中对象正在变化和已完成变化的区域。为此，作者开发了首个解决此任务的模型SPOC，该模型结合了基于VLM的伪标签技术、状态变化动态约束，并构建了新的WhereToChange基准。实验证明了该任务的挑战性和模型在精确识别对象变化位置和速度方面的有效性，对机器人活动跟踪等应用具有潜在价值。", "keywords": "对象状态变化, 视频分割, 空间渐进, 像素级定位, 机器人活动跟踪", "comments": "该论文的创新点在于提出了一个全新的、更细粒度的“空间渐进式对象状态变化分割”任务，突破了传统方法仅限于时间定位的局限性。通过像素级别的分割，可以更详细地理解动作的进程和空间演变，这对于需要精确交互和理解环境的机器人等应用至关重要。提出的VLM伪标签方法和状态变化动态约束是解决这一新挑战的关键技术，而WhereToChange基准的构建则为未来的研究提供了标准化的评估平台。这项工作显著推动了视频理解领域的发展。"}}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz Öztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.", "AI": {"title_translation": "CHOrD：用于3D室内场景的无碰撞、房屋级、有组织数字孪生生成，具有可控的平面图和最佳布局", "tldr": "CHOrD是一个新的框架，用于生成无碰撞、房屋级、有组织的3D室内场景数字孪生，通过2D图像中间表示和多模态控制实现。", "motivation": "现有方法直接合成场景布局（场景图或对象列表）容易产生碰撞伪影，且难以生成符合复杂平面图的场景布局。", "method": "CHOrD引入了基于2D图像的中间布局表示来有效防止碰撞伪影；通过多模态控制生成符合复杂平面图的场景布局；提出了一个覆盖范围更广、数据质量更高的新数据集。", "result": "CHOrD在3D-FRONT和其提出的数据集上均表现出最先进的性能，能够生成逼真、空间连贯的室内场景，并适应任意平面图变化。", "conclusion": "CHOrD通过其创新的中间布局表示和对平面图的精细控制，显著提升了3D室内场景数字孪生生成在质量和适应性方面的能力。", "translation": "我们引入CHOrD，一个用于3D室内场景可扩展合成的新颖框架，旨在创建房屋级、无碰撞和分层结构的室内数字孪生。与现有直接将场景布局合成为场景图或对象列表的方法不同，CHOrD结合了基于2D图像的中间布局表示，通过在生成过程中成功地将碰撞伪影捕获为分布外（OOD）情景，从而有效防止了碰撞伪影。此外，与现有方法不同，CHOrD能够生成符合具有多模态控制的复杂平面图的场景布局，从而能够创建对房间结构中的几何和语义变化都具有鲁棒性的连贯、全屋布局。此外，我们提出了一个新颖的数据集，其涵盖了更广泛的家庭物品和房间配置，并显著提高了数据质量。CHOrD在3D-FRONT和我们提出的数据集上均表现出最先进的性能，提供了逼真、空间连贯的室内场景合成，可适应任意平面图变化。", "summary": "CHOrD是一个用于生成房屋级、无碰撞、分层3D室内数字孪生的新颖框架。它通过引入2D图像中间布局表示来有效避免碰撞，并能通过多模态控制生成符合复杂平面图的场景布局。该框架还提出了一个改进的新数据集。CHOrD在多个数据集上展示了最先进的性能，能够生成逼真且空间连贯的室内场景，并适应各种平面图。", "keywords": "3D室内场景, 数字孪生, 碰撞检测, 平面图控制, 场景合成", "comments": "CHOrD的创新之处在于其采用2D图像作为中间布局表示，有效解决了3D场景生成中的碰撞问题，并引入了对复杂平面图的多模态控制，大大提高了生成场景的实用性和适应性。其提出的新数据集也对领域发展有贡献。"}}
{"id": "2503.11969", "pdf": "https://arxiv.org/pdf/2503.11969", "abs": "https://arxiv.org/abs/2503.11969", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "The registration between the pre-operative model and the intra-operative\nsurface is crucial in image-guided liver surgery, as it facilitates the\neffective use of pre-operative information during the procedure. However, the\nintra-operative surface, usually represented as a point cloud, often has\nlimited coverage, especially in laparoscopic surgery, and is prone to holes and\nnoise, posing significant challenges for registration methods. Point cloud\ncompletion methods have the potential to alleviate these issues. Thus, we\nexplore six state-of-the-art point cloud completion methods to identify the\noptimal completion method for liver surgery applications. We focus on a\npatient-specific approach for liver point cloud completion from a partial liver\nsurface under three cases: canonical pose, non-canonical pose, and canonical\npose with noise. The transformer-based method, AdaPoinTr, outperforms all other\nmethods to generate a complete point cloud from the given partial liver point\ncloud under the canonical pose. On the other hand, our findings reveal\nsubstantial performance degradation of these methods under non-canonical poses\nand noisy settings, highlighting the limitations of these methods, which\nsuggests the need for a robust point completion method for its application in\nimage-guided liver surgery.", "AI": {"title_translation": "微创肝脏介入中术中患者特异性点云补全方法的评估", "tldr": "评估了六种最先进的点云补全方法在肝脏手术中的应用，发现AdaPoinTr在标准姿态下表现最佳，但在非标准姿态和噪声环境下性能显著下降。", "motivation": "在图像引导的肝脏手术中，术前模型与术中表面配准至关重要，但术中表面（通常为点云）覆盖有限、存在孔洞和噪声，给配准方法带来了挑战。点云补全方法有望缓解这些问题。", "method": "本研究探索了六种最先进的点云补全方法，以识别适用于肝脏手术的最佳补全方法。研究侧重于患者特异性的肝脏点云补全，在三种情况下进行评估：标准姿态、非标准姿态和带噪声的标准姿态。", "result": "在标准姿态下，基于Transformer的AdaPoinTr方法在从给定部分肝脏点云生成完整点云方面优于所有其他方法。然而，在非标准姿态和噪声设置下，这些方法的性能出现显著下降。", "conclusion": "研究结果揭示了现有方法在非标准姿态和噪声环境下存在局限性，这表明需要开发一种更鲁棒的点云补全方法，以应用于图像引导的肝脏手术。", "translation": "术前模型与术中表面的配准在图像引导的肝脏手术中至关重要，因为它有助于在手术过程中有效利用术前信息。然而，术中表面（通常表示为点云）的覆盖范围通常有限，尤其是在腹腔镜手术中，并且容易出现孔洞和噪声，这给配准方法带来了重大挑战。点云补全方法有潜力缓解这些问题。因此，我们探索了六种最先进的点云补全方法，以确定肝脏手术应用的最佳补全方法。我们侧重于在三种情况下从部分肝脏表面进行肝脏点云补全的患者特异性方法：标准姿态、非标准姿态和带噪声的标准姿态。基于Transformer的AdaPoinTr方法在标准姿态下从给定部分肝脏点云生成完整点云方面优于所有其他方法。另一方面，我们的发现揭示了这些方法在非标准姿态和噪声设置下性能的显著下降，突出了这些方法的局限性，这表明需要一种鲁棒的点云补全方法以应用于图像引导的肝脏手术。", "summary": "本研究评估了六种最先进的点云补全方法在微创肝脏介入中的应用，旨在解决术中肝脏点云覆盖有限、存在孔洞和噪声的问题。研究采用患者特异性方法，在标准姿态、非标准姿态和带噪声的标准姿态下进行测试。结果显示，AdaPoinTr在标准姿态下表现最佳，但所有方法在非标准姿态和噪声环境下性能均显著下降。这强调了开发更鲁棒点云补全方法的重要性，以提高图像引导肝脏手术的准确性。", "keywords": "点云补全, 肝脏手术, 图像引导, 配准, AdaPoinTr", "comments": "该研究通过评估多种先进点云补全方法在肝脏手术中的表现，指出了现有技术在实际应用（特别是面对非标准姿态和噪声）中的局限性。其创新点在于对患者特异性点云补全方法的系统性比较，并明确提出了未来研究应关注的鲁棒性问题，对图像引导肝脏手术领域具有指导意义。"}}
{"id": "2503.11979", "pdf": "https://arxiv.org/pdf/2503.11979", "abs": "https://arxiv.org/abs/2503.11979", "authors": ["Runfa Blark Li", "Mahdi Shaghaghi", "Keito Suzuki", "Xinshuang Liu", "Varun Moparthi", "Bang Du", "Walker Curtis", "Martin Renschler", "Ki Myung Brian Lee", "Nikolay Atanasov", "Truong Nguyen"], "title": "DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) is one of the most important\nenvironment-perception and navigation algorithms for computer vision, robotics,\nand autonomous cars/drones. Hence, high quality and fast mapping becomes a\nfundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an\nexplicit representation with excellent rendering quality and speed,\nstate-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical\npointcloud-SLAM, GS-SLAM generates photometric information by learning from\ninput camera views and synthesize unseen views with high-quality textures.\nHowever, these GS-SLAM fail when moving objects occupy the scene that violate\nthe static assumption of bundle adjustment. The failed updates of moving GS\naffects the static GS and contaminates the full map over long frames. Although\nsome efforts have been made by concurrent works to consider moving objects for\nGS-SLAM, they simply detect and remove the moving regions from GS rendering\n(\"anti'' dynamic GS-SLAM), where only the static background could benefit from\nGS. To this end, we propose the first real-time GS-SLAM, \"DynaGSLAM'', that\nachieves high-quality online GS rendering, tracking, motion predictions of\nmoving objects in dynamic scenes while jointly estimating accurate ego motion.\nOur DynaGSLAM outperforms SOTA static & \"Anti'' dynamic GS-SLAM on three\ndynamic real datasets, while keeping speed and memory efficiency in practice.", "AI": {"title_translation": "DynaGSLAM：用于动态场景中移动物体在线渲染、跟踪、运动预测的实时高斯泼溅SLAM", "tldr": "DynaGSLAM是首个实时高斯泼溅SLAM系统，专门处理动态场景中的移动物体，实现高质量的在线渲染、跟踪和运动预测，同时精确估计自我运动，并优于现有静态和“反”动态GS-SLAM。", "motivation": "现有的高斯泼溅SLAM（GS-SLAM）在场景中存在移动物体时会失效，因为它们违反了捆绑调整的静态假设，导致移动高斯更新失败并污染整个地图。尽管有些工作尝试检测并移除移动区域，但这只让静态背景受益，无法处理移动物体本身。", "method": "我们提出了首个实时GS-SLAM系统“DynaGSLAM”，它能够对动态场景中的移动物体进行高质量的在线GS渲染、跟踪和运动预测，同时联合估计精确的自我运动。", "result": "DynaGSLAM在三个动态真实数据集上优于最先进的静态和“反”动态GS-SLAM，同时保持了实际的速度和内存效率。", "conclusion": "DynaGSLAM成功地解决了传统GS-SLAM在动态场景中处理移动物体时的局限性，提供了一种高效、高质量的实时解决方案，适用于复杂的动态环境感知和导航任务。", "translation": "同步定位与建图（SLAM）是计算机视觉、机器人和自动驾驶汽车/无人机最重要的环境感知和导航算法之一。因此，高质量和快速的建图成为一个基本问题。随着3D高斯泼溅（3DGS）作为一种具有出色渲染质量和速度的显式表示的出现，最先进（SOTA）的工作将GS引入SLAM。与经典点云SLAM相比，GS-SLAM通过从输入相机视图中学习来生成光度信息，并合成具有高质量纹理的未见视图。然而，当移动物体占据场景并违反捆绑调整的静态假设时，这些GS-SLAM就会失败。移动GS的失败更新会影响静态GS并长期污染整个地图。尽管一些同期工作已努力考虑GS-SLAM的移动物体，但它们只是简单地检测并从GS渲染中移除移动区域（“反”动态GS-SLAM），其中只有静态背景可以受益于GS。为此，我们提出了首个实时GS-SLAM，“DynaGSLAM”，它实现了动态场景中移动物体的高质量在线GS渲染、跟踪、运动预测，同时联合估计精确的自我运动。我们的DynaGSLAM在三个动态真实数据集上优于SOTA静态和“反”动态GS-SLAM，同时在实践中保持了速度和内存效率。", "summary": "本文提出DynaGSLAM，这是首个实时高斯泼溅SLAM系统，旨在解决动态场景中移动物体带来的挑战。现有GS-SLAM因静态假设而失效，而DynaGSLAM能够同时实现高质量的在线GS渲染、跟踪和移动物体的运动预测，并精确估计自我运动。实验结果表明，DynaGSLAM在动态数据集上优于现有静态和“反”动态GS-SLAM，并保持了高效的性能。", "keywords": "SLAM, 高斯泼溅, 动态场景, 实时, 运动预测", "comments": "DynaGSLAM的创新之处在于它是首个能够真正处理动态场景中移动物体的实时GS-SLAM，而不仅仅是移除移动区域。这对于自动驾驶和机器人等需要精确环境感知和导航的应用至关重要。其重要性在于将3DGS的优势扩展到更复杂的动态环境，克服了现有方法的局限性。"}}
{"id": "2503.11981", "pdf": "https://arxiv.org/pdf/2503.11981", "abs": "https://arxiv.org/abs/2503.11981", "authors": ["Utkarsh Nath", "Rajeev Goel", "Rahul Khurana", "Kyle Min", "Mark Ollila", "Pavan Turaga", "Varun Jampani", "Tejaswi Gowda"], "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation saw dramatic advances in recent years by leveraging\nText-to-Image models. However, most existing techniques struggle with\ncompositional prompts, which describe multiple objects and their spatial\nrelationships. They often fail to capture fine-grained inter-object\ninteractions. We introduce DecompDreamer, a Gaussian splatting-based training\nroutine designed to generate high-quality 3D compositions from such complex\nprompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose\nscenes into structured components and their relationships. We propose a\nprogressive optimization strategy that first prioritizes joint relationship\nmodeling before gradually shifting toward targeted object refinement. Our\nqualitative and quantitative evaluations against state-of-the-art text-to-3D\nmodels demonstrate that DecompDreamer effectively generates intricate 3D\ncompositions with superior object disentanglement, offering enhanced control\nand flexibility in 3D generation. Project page :\nhttps://decompdreamer3d.github.io", "AI": {"title_translation": "DecompDreamer：通过多对象分解和高斯泼溅推进结构化3D资产生成", "tldr": "DecompDreamer 是一种基于高斯泼溅的训练方法，利用视觉-语言模型分解场景并采用渐进式优化策略，解决了现有文本到3D生成模型在处理复杂多对象提示时遇到的困难，能够生成具有更好对象解耦和更高质量的复杂3D组合。", "motivation": "现有的文本到3D生成技术在处理描述多个对象及其空间关系的组合提示时遇到困难，难以捕捉细粒度的对象间交互。", "method": "引入了DecompDreamer，一个基于高斯泼溅的训练流程。它利用视觉-语言模型（VLMs）将场景分解为结构化组件及其关系，并提出了一种渐进式优化策略，首先优先处理联合关系建模，然后逐步转向目标对象精修。", "result": "DecompDreamer 有效地生成了复杂的3D组合，并实现了卓越的对象解耦。", "conclusion": "DecompDreamer 在3D生成中提供了增强的控制和灵活性。", "translation": "文本到3D生成在近年来通过利用文本到图像模型取得了显著进展。然而，大多数现有技术在处理描述多个对象及其空间关系的组合提示时遇到困难。它们往往无法捕捉细粒度的对象间交互。我们引入了DecompDreamer，一个基于高斯泼溅的训练流程，旨在从此类复杂提示中生成高质量的3D组合。DecompDreamer 利用视觉-语言模型（VLMs）将场景分解为结构化组件及其关系。我们提出了一种渐进式优化策略，首先优先处理联合关系建模，然后逐步转向目标对象精修。我们与最先进的文本到3D模型进行的定性和定量评估表明，DecompDreamer 有效地生成了复杂的3D组合，并具有卓越的对象解耦能力，在3D生成中提供了增强的控制和灵活性。项目页面：https://decompdreamer3d.github.io", "summary": "DecompDreamer 提出了一种新的基于高斯泼溅的文本到3D生成方法，旨在解决现有模型在处理多对象组合提示时遇到的困难。该方法利用视觉-语言模型进行场景分解，并采用渐进式优化策略，从而能够生成高质量、具有良好对象解耦的复杂3D组合，增强了3D生成的控制和灵活性。", "keywords": "文本到3D生成, 高斯泼溅, 多对象分解, 视觉-语言模型, 3D组合", "comments": "本文的创新点在于其针对文本到3D生成中处理复杂、多对象组合提示的挑战，提出了一种结合多对象分解和渐进式优化的新方法。通过利用视觉-语言模型进行场景分解和高斯泼溅进行渲染，DecompDreamer 显著提升了生成3D资产时对象之间的解耦能力和整体质量，为更精细的3D内容创作提供了新的可能。"}}
{"id": "2503.11995", "pdf": "https://arxiv.org/pdf/2503.11995", "abs": "https://arxiv.org/abs/2503.11995", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Zhihao Chen", "Guangwei Gao"], "title": "Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "In recent years, Transformer has witnessed significant progress in food\nrecognition. However, most existing approaches still face two critical\nchallenges in lightweight food recognition: (1) the quadratic complexity and\nredundant feature representation from interactions with irrelevant tokens; (2)\nstatic feature recognition and single-scale representation, which overlook the\nunstructured, non-fixed nature of food images and the need for multi-scale\nfeatures. To address these, we propose an adaptive and efficient sparse\nTransformer architecture (Fraesormer) with two core designs: Adaptive Top-k\nSparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature\nGating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator\n(GDTKO) to retain critical attention scores, filtering low query-key matches\nthat hinder feature aggregation. It also introduces a partial channel mechanism\nto reduce redundancy and promote expert information flow, enabling local-global\ncollaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale\nfeature representation, enhancing contextual semantic information. Extensive\nexperiments show that Fraesormer outperforms state-of-the-art methods. code is\navailable at https://zs1314.github.io/Fraesormer.", "AI": {"title_translation": "Fraesormer：学习自适应稀疏Transformer以实现高效食物识别", "tldr": "Fraesormer提出了一种自适应稀疏Transformer架构，通过自适应Top-k稀疏部分注意力（ATK-SPA）和分层尺度敏感特征门控网络（HSSFGN）解决了食物识别中Transformer的二次复杂度和冗余特征表示以及静态单尺度表示的挑战，并取得了最先进的性能。", "motivation": "现有Transformer在轻量级食物识别中面临两大挑战：1) 二次复杂度和与不相关token交互导致的冗余特征表示；2) 静态特征识别和单尺度表示，这忽略了食物图像的非结构化、非固定性质以及对多尺度特征的需求。", "method": "提出了一种自适应高效的稀疏Transformer架构（Fraesormer），包含两个核心设计：自适应Top-k稀疏部分注意力（ATK-SPA）和分层尺度敏感特征门控网络（HSSFGN）。ATK-SPA使用可学习的门控动态Top-K操作符（GDTKO）保留关键注意力分数，过滤低查询-键匹配，并引入部分通道机制以减少冗余，促进专家信息流，实现局部-全局协作建模。HSSFGN采用门控机制实现多尺度特征表示，增强上下文语义信息。", "result": "广泛的实验表明，Fraesormer优于最先进的方法。", "conclusion": "Fraesormer通过其创新的自适应稀疏注意力和多尺度特征门控机制，有效解决了轻量级食物识别中Transformer的效率和表示能力问题，达到了卓越的性能。", "translation": "近年来，Transformer在食物识别领域取得了显著进展。然而，大多数现有方法在轻量级食物识别中仍面临两个关键挑战：(1) 二次复杂度和与不相关token交互产生的冗余特征表示；(2) 静态特征识别和单尺度表示，这忽略了食物图像的非结构化、非固定性质以及对多尺度特征的需求。为了解决这些问题，我们提出了一种自适应高效的稀疏Transformer架构（Fraesormer），其包含两个核心设计：自适应Top-k稀疏部分注意力（ATK-SPA）和分层尺度敏感特征门控网络（HSSFGN）。ATK-SPA使用可学习的门控动态Top-K操作符（GDTKO）来保留关键注意力分数，过滤阻碍特征聚合的低查询-键匹配。它还引入了部分通道机制，以减少冗余并促进专家信息流，从而实现局部-全局协作建模。HSSFGN采用门控机制实现多尺度特征表示，增强上下文语义信息。广泛的实验表明，Fraesormer优于最先进的方法。代码可在https://zs1314.github.io/Fraesormer 获取。", "summary": "该论文提出了一种名为Fraesormer的自适应高效稀疏Transformer架构，旨在解决轻量级食物识别中Transformer面临的二次复杂性、冗余特征表示以及静态单尺度特征表示问题。Fraesormer的核心设计包括自适应Top-k稀疏部分注意力（ATK-SPA）和分层尺度敏感特征门控网络（HSSFGN）。ATK-SPA通过动态Top-K操作符和部分通道机制优化注意力机制，减少冗余并促进局部-全局建模。HSSFGN则通过门控机制实现多尺度特征表示。实验证明Fraesormer在食物识别任务上超越了现有最先进方法。", "keywords": "食物识别, 稀疏Transformer, 自适应注意力, 多尺度特征, 轻量级模型", "comments": "Fraesormer的创新之处在于其结合了稀疏注意力机制和多尺度特征表示，有效解决了Transformer在处理非结构化食物图像时的效率和表示能力问题。ATK-SPA的自适应性及其局部-全局协作建模能力是关键亮点。HSSFGN对多尺度特征的关注也增强了模型的鲁棒性。该方法对于推动轻量级和高效的视觉Transformer在实际应用中的发展具有重要意义。"}}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs", "AI": {"title_translation": "针对运动物体的3D高斯泼溅用于高保真街景重建", "tldr": "本文提出了一种新的3D高斯泼溅方法，用于动态街景重建，能够处理运动物体并实现高保真和高效性。", "motivation": "动态街景的精确重建对于自动驾驶、增强现实和虚拟现实等应用至关重要。传统方法在处理运动物体、遮挡、实时处理限制、计算效率和场景动态方面存在困难。", "method": "本文提出了一种新颖的3D高斯点分布方法。该方法引入了自适应透明度机制来消除运动物体同时保留静态场景细节；通过迭代细化高斯点分布来增强几何精度和纹理表示；并结合方向编码与空间位置优化以优化存储和渲染效率。", "result": "实验结果表明，该方法实现了高重建质量、改进的渲染性能以及在大规模动态环境中的适应性。", "conclusion": "本文为实时、高精度3D重建建立了一个强大的框架，推动了动态场景建模在多个应用中的实用性。", "translation": "动态街景的精确重建对于自动驾驶、增强现实和虚拟现实等应用至关重要。依赖密集点云和三角网格的传统方法在处理运动物体、遮挡和实时处理限制方面存在困难，限制了它们在复杂城市环境中的有效性。虽然多视图立体和神经辐射场在3D重建方面取得了进展，但它们在计算效率和处理场景动态方面面临挑战。本文提出了一种新颖的用于动态街景重建的3D高斯点分布方法。我们的方法引入了一种自适应透明度机制，可以在消除运动物体的同时保留高保真静态场景细节。此外，高斯点分布的迭代细化增强了几何精度和纹理表示。我们结合方向编码和空间位置优化，以优化存储和渲染效率，减少冗余同时保持场景完整性。实验结果表明，我们的方法实现了高重建质量、改进的渲染性能以及在大规模动态环境中的适应性。这些贡献为实时、高精度3D重建建立了一个强大的框架，推动了动态场景建模在多个应用中的实用性。本工作的源代码可在https://github.com/deepcoxcom/3dgs公开获取。", "summary": "本文提出了一种新颖的3D高斯点分布方法，用于高保真动态街景重建。该方法采用自适应透明度机制来消除运动物体，通过迭代细化提高几何和纹理精度，并结合方向编码与空间位置优化以提高效率。实验证明，该方法在大型动态环境中实现了高重建质量、改进的渲染性能和良好的适应性，为实时、高精度3D重建提供了一个鲁棒的框架。", "keywords": "3D高斯泼溅, 动态场景重建, 街景, 运动物体, 实时3D", "comments": "该论文通过在3D高斯泼溅框架内有效处理运动物体，解决了动态场景重建中的一个重要挑战，这是一项相对较新且高效的3D表示方法。自适应透明度机制是其关键创新点。该方法对实时性能和高保真度的关注使其与自动驾驶等实际应用高度相关。"}}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM.", "AI": {"title_translation": "ROS-SAM：高精度遥感运动目标交互式分割", "tldr": "ROS-SAM是一种针对遥感运动目标的高质量交互式分割方法，通过LoRA微调、深度网络层增强和上下文整合，显著提高了分割精度和泛化能力，并展现了出色的零样本性能。", "motivation": "大规模遥感视频数据对高质量交互式分割提出了需求，但现有方法面临小目标尺寸、模糊特征和泛化能力有限等挑战，难以实现高精度分割。", "method": "本文提出了ROS-SAM方法，其核心创新包括：1) 基于LoRA的微调，实现高效域适应并保持SAM的泛化能力；2) 增强深度网络层以提高特征判别力，减少误分类；3) 在掩码解码器中整合全局上下文与局部边界细节，生成高质量分割掩码。此外，还设计了数据管道，以更好地处理不同尺度的目标并专注于高质量预测。", "result": "在遥感视频数据集上的实验表明，重新设计的数据管道使IoU提高了6%，而ROS-SAM使IoU提高了13%。在现有遥感目标跟踪数据集上进行评估时，ROS-SAM展现了令人印象深刻的零样本能力，生成的掩码与手动标注非常接近。", "conclusion": "ROS-SAM被证实是遥感应用中进行细粒度分割的强大工具。", "translation": "大规模遥感视频数据的可用性凸显了高质量交互式分割的重要性。然而，小目标尺寸、模糊特征和有限的泛化能力等挑战使得现有方法难以实现这一目标。在这项工作中，我们提出了ROS-SAM，一种旨在实现高质量交互式分割，同时保持在不同遥感数据上泛化能力的方法。ROS-SAM建立在三个关键创新之上：1) 基于LoRA的微调，能够实现高效的领域适应，同时保持SAM的泛化能力；2) 增强深度网络层，以提高提取特征的判别力，从而减少误分类；以及3) 在掩码解码器中整合全局上下文与局部边界细节，以生成高质量的分割掩码。此外，我们设计了数据管道，以确保模型在训练期间学习更好地处理不同尺度的目标，同时在推理期间专注于高质量预测。在遥感视频数据集上的实验表明，重新设计的数据管道将IoU提高了6%，而ROS-SAM将IoU提高了13%。最后，在现有遥感目标跟踪数据集上进行评估时，ROS-SAM展示了令人印象深刻的零样本能力，生成的掩码与手动标注非常接近。这些结果证实ROS-SAM是遥感应用中进行细粒度分割的强大工具。代码可在https://github.com/ShanZard/ROS-SAM获取。", "summary": "ROS-SAM是一种专为遥感运动目标设计的高质量交互式分割方法。针对现有方法在小目标、模糊特征和泛化能力上的不足，ROS-SAM通过LoRA微调实现高效域适应，增强深度网络层以提高特征判别力，并在掩码解码器中整合全局与局部细节。此外，优化的数据管道确保模型能处理多尺度目标并进行高质量预测。实验证明，ROS-SAM显著提升了IoU，并展现了出色的零样本分割能力，是遥感细粒度分割的有力工具。", "keywords": "遥感, 交互式分割, ROS-SAM, LoRA, 零样本", "comments": "该论文通过结合LoRA微调、深度特征增强和上下文感知解码器，有效解决了遥感图像中运动目标分割的挑战，特别是在小目标和泛化能力方面。其零样本能力的展示尤其具有创新性，表明模型在未见过的数据上也能表现良好，这对于实际遥感应用非常重要。"}}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.", "AI": {"title_translation": "UniMamba：用于基于LiDAR的3D目标检测的统一空间-通道表示学习与组高效Mamba", "tldr": "UniMamba通过结合3D卷积和SSM，解决了Transformer在LiDAR 3D检测中空间结构破坏和计算效率低的问题，实现了高效的局部和全局空间上下文聚合。", "motivation": "现有的基于Transformer的LiDAR 3D检测方法在将3D体素序列化为1D序列时会破坏空间结构，并且由于二次复杂度，分组处理导致感受野受限。", "method": "本文提出一种新颖的统一Mamba（UniMamba），它以简洁的多头方式无缝集成了3D卷积和状态空间模型（SSM）的优点，旨在高效且同时地执行“局部和全局”空间上下文聚合。UniMamba块主要由空间局部性建模、互补Z-order序列化和局部-全局序列聚合器组成。空间局部性建模模块集成了3D子流形卷积。高效的Z-order曲线用于水平和垂直序列化。局部-全局序列聚合器采用通道分组策略，利用多头SSM高效编码“局部和全局”空间相互依赖性。此外，还形成了带有堆叠UniMamba块的编解码器架构，以分层促进多尺度空间学习。", "result": "在nuScenes、Waymo和Argoverse 2三个流行数据集上进行了广泛实验。特别地，UniMamba在nuScenes数据集上实现了70.2 mAP。", "conclusion": "UniMamba通过有效结合3D卷积和SSM，克服了传统Transformer在3D点云处理中的局限性，在LiDAR 3D目标检测任务上取得了优异性能。", "translation": "近期LiDAR 3D检测的进展表明，基于Transformer的框架在捕获点云空间中的全局依赖性方面表现出有效性，它将3D体素序列化为扁平的1D序列进行迭代自注意力。然而，在序列化过程中，3D体素的空间结构将不可避免地被破坏。此外，由于3D体素的数量相当可观以及Transformer的二次复杂度，在馈送给Transformer之前，多个序列被分组，导致感受野有限。受状态空间模型（SSM）在2D视觉任务中取得的卓越性能启发，本文提出了一种新颖的统一Mamba（UniMamba），它以简洁的多头方式无缝集成了3D卷积和SSM的优点，旨在高效且同时地执行“局部和全局”空间上下文聚合。具体而言，UniMamba块主要由空间局部性建模、互补Z-order序列化和局部-全局序列聚合器组成。空间局部性建模模块集成了3D子流形卷积，以在序列化之前捕获动态空间位置嵌入。然后采用高效的Z-order曲线进行水平和垂直序列化。此外，局部-全局序列聚合器采用通道分组策略，利用多头SSM高效编码“局部和全局”空间相互依赖性。此外，还形成了带有堆叠UniMamba块的编解码器架构，以分层促进多尺度空间学习。在nuScenes、Waymo和Argoverse 2三个流行数据集上进行了广泛实验。特别是，我们的UniMamba在nuScenes数据集上达到了70.2 mAP。", "summary": "本文提出UniMamba，一种用于LiDAR 3D目标检测的新型架构，旨在克服传统Transformer在处理3D体素时空间结构破坏和感受野受限的问题。UniMamba巧妙地结合了3D卷积和状态空间模型（SSM），通过设计包含空间局部性建模、Z-order序列化和局部-全局序列聚合器的UniMamba块，实现了局部和全局空间上下文的高效同步聚合。实验结果表明，UniMamba在多个主流数据集上，特别是nuScenes上取得了SOTA性能，证明了其在3D点云表示学习中的有效性。", "keywords": "LiDAR 3D目标检测, 统一Mamba, 状态空间模型, 3D卷积, 空间-通道表示学习", "comments": "UniMamba的创新点在于将3D卷积与SSM结合，解决了Transformer在3D点云处理中的固有缺陷。通过Z-order序列化和多头SSM的通道分组策略，实现了对局部和全局空间依赖的有效捕获，提高了效率和准确性。这种统一的架构为LiDAR 3D目标检测提供了一个有前景的新方向。"}}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets.", "AI": {"title_translation": "单图像去雨的双域多尺度表示学习", "tldr": "提出DMSR网络，利用空间和频率域的双域多尺度表示进行图像去雨，在多个基准数据集上达到SOTA性能。", "motivation": "现有图像去雨方法通常依赖于单输入、单输出、单尺度架构，忽略了外部和内部特征之间的联合多尺度信息，且单域表示过于受限，难以处理真实世界的复杂雨场景。", "method": "提出双域多尺度表示网络（DMSR），其核心思想是并行利用外部和内部域的联合多尺度表示，并结合空间域和频率域的优势。DMSR包含两个主要组件：多尺度渐进空间细化模块（MPSRM）和频域尺度混合器（FDSM）。MPSRM在内部域内使用分层调制和融合策略实现多尺度专家信息的交互和耦合。FDSM提取空间域的多尺度局部信息，同时建模频域的全局依赖。", "result": "模型在六个基准数据集上实现了最先进的性能。", "conclusion": "通过利用双域多尺度表示，DMSR网络能够有效处理复杂雨场景并取得优异的去雨效果。", "translation": "现有图像去雨方法通常依赖于单输入、单输出和单尺度架构，这忽略了外部和内部特征之间的联合多尺度信息。此外，单域表示往往过于受限，限制了它们处理真实世界雨场景复杂性的能力。为了解决这些挑战，我们提出了一种新颖的双域多尺度表示网络（DMSR）。其核心思想是并行利用外部和内部域的联合多尺度表示，同时利用空间域和频率域的优势来捕获更全面的属性。具体而言，我们的方法包含两个主要组件：多尺度渐进空间细化模块（MPSRM）和频域尺度混合器（FDSM）。MPSRM使用分层调制和融合策略在内部域内实现多尺度专家信息的交互和耦合。FDSM在空间域中提取多尺度局部信息，同时在频率域中建模全局依赖。大量实验表明，我们的模型在六个基准数据集上实现了最先进的性能。", "summary": "本文提出了一种新颖的双域多尺度表示网络（DMSR）用于单图像去雨，旨在解决现有方法在处理多尺度信息和复杂雨场景时的局限性。DMSR通过并行利用空间域和频率域中的外部与内部联合多尺度表示来捕获更全面的图像属性。该网络包含多尺度渐进空间细化模块（MPSRM）和频域尺度混合器（FDSM）两大核心组件。实验结果表明，DMSR在多个基准数据集上均达到了最先进的性能。", "keywords": "图像去雨, 双域, 多尺度, 深度学习, 频率域", "comments": "这篇论文的创新点在于提出了双域（空间域和频率域）和多尺度表示相结合的方法，以更全面地处理图像去雨问题。传统的单域单尺度方法存在局限性，而DMSR通过MPSRM和FDSM两个模块，有效地融合了多尺度局部信息和全局依赖，显著提升了去雨效果，并在多个基准数据集上验证了其先进性。"}}
{"id": "2503.12015", "pdf": "https://arxiv.org/pdf/2503.12015", "abs": "https://arxiv.org/abs/2503.12015", "authors": ["Donglin Yang", "Paul Vicol", "Xiaojuan Qi", "Renjie Liao", "Xiaofan Zhang"], "title": "QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for Efficient Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based super-resolution (SR) methods often perform pixel-wise\ncomputations uniformly across entire images, even in homogeneous regions where\nhigh-resolution refinement is redundant. We propose the Quadtree Diffusion\nModel (QDM), a region-adaptive diffusion framework that leverages a quadtree\nstructure to selectively enhance detail-rich regions while reducing\ncomputations in homogeneous areas. By guiding the diffusion with a quadtree\nderived from the low-quality input, QDM identifies key regions-represented by\nleaf nodes-where fine detail is essential and applies minimal refinement\nelsewhere. This mask-guided, two-stream architecture adaptively balances\nquality and efficiency, producing high-fidelity outputs with low computational\nredundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR\ntasks across diverse image types, particularly in medical imaging (e.g., CT\nscans), where large homogeneous regions are prevalent. Furthermore, QDM\noutperforms or is comparable to state-of-the-art SR methods on standard\nbenchmarks while significantly reducing computational costs, highlighting its\nefficiency and suitability for resource-limited environments. Our code is\navailable at https://github.com/linYDTHU/QDM.", "AI": {"title_translation": "QDM：基于四叉树的区域自适应稀疏扩散模型，用于高效图像超分辨率", "tldr": "QDM是一个基于四叉树的区域自适应扩散模型，通过在细节丰富区域进行精细计算，而在同质区域减少计算，实现了高效的图像超分辨率，同时保持高质量并显著降低计算成本。", "motivation": "现有的深度学习超分辨率方法在整个图像上进行统一的像素级计算，即使在不需要高分辨率细化的同质区域也是如此，导致计算冗余。", "method": "提出QDM（四叉树扩散模型），一个区域自适应的扩散框架。它利用四叉树结构选择性地增强细节丰富的区域，同时减少同质区域的计算。QDM通过低质量输入导出的四叉树来引导扩散，识别关键区域（由叶节点表示）进行精细细节处理，并在其他地方应用最小程度的细化。这是一个掩码引导的双流架构，自适应地平衡质量和效率。", "result": "QDM在多种图像类型（特别是医疗成像，如CT扫描）的高分辨率超分辨率任务中表现出有效性。在标准基准测试中，QDM的性能优于或与最先进的超分辨率方法相当，同时显著降低了计算成本。", "conclusion": "QDM通过其区域自适应方法，实现了高效且高质量的图像超分辨率，特别适用于资源受限的环境。", "translation": "标题: QDM：基于四叉树的区域自适应稀疏扩散模型，用于高效图像超分辨率\n摘要: 基于深度学习的超分辨率（SR）方法通常在整个图像上统一执行像素级计算，即使在同质区域中，高分辨率细化是冗余的。我们提出了四叉树扩散模型（QDM），一个区域自适应扩散框架，它利用四叉树结构选择性地增强细节丰富的区域，同时减少同质区域的计算。通过使用从低质量输入导出的四叉树引导扩散，QDM识别出需要精细细节的关键区域（由叶节点表示），并在其他地方应用最小程度的细化。这种掩码引导的双流架构自适应地平衡质量和效率，以低计算冗余生成高保真输出。实验证明了QDM在各种图像类型（特别是医疗成像，例如CT扫描）的高分辨率SR任务中的有效性，这些区域普遍存在大型同质区域。此外，QDM在标准基准测试中优于或与最先进的SR方法相当，同时显著降低了计算成本，突出了其效率和对资源有限环境的适用性。我们的代码可在https://github.com/linYDTHU/QDM获取。", "summary": "QDM是一种基于四叉树的区域自适应稀疏扩散模型，旨在解决传统深度学习超分辨率方法在同质区域计算冗余的问题。该模型利用四叉树结构，根据输入图像的细节丰富程度，智能地分配计算资源，在细节区域进行精细处理，而在同质区域减少计算。QDM采用掩码引导的双流架构，有效平衡了图像质量和计算效率。实验证明，QDM在多种图像，特别是医疗图像的超分辨率任务中表现出色，并在保持或超越SOTA性能的同时，显著降低了计算成本，使其适用于资源受限的环境。", "keywords": "四叉树, 扩散模型, 超分辨率, 区域自适应, 效率", "comments": "这篇论文的创新点在于引入了四叉树结构来指导扩散模型进行区域自适应的计算，从而解决了传统超分辨率方法在同质区域的计算冗余问题。这种方法显著提高了效率，尤其适用于包含大量同质区域的图像类型，如医疗影像。其重要性在于为资源受限环境下的高质量图像超分辨率提供了新的解决方案。"}}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "AI": {"title_translation": "塑造你的美学：以艺术原则赋能文本到图像模型", "tldr": "文本到图像（T2I）扩散模型常生成不美观的图像，现有方法忽略个性化美学。本文提出“美学对齐”任务，利用艺术原则（PoA）对视觉美学进行编码，并构建了大型组合艺术数据集CompArt。通过训练轻量级适配器，使T2I模型能通过用户指定的PoA条件提供10种构图控制，并设计了相应的评估框架。", "motivation": "文本到图像（T2I）扩散模型虽然广泛应用且能生成高保真输出，但往往倾向于生成不美观的图像，这与它们训练所用的互联网随机图像类似。现有解决此问题的方法基于视觉美学是普遍的隐含前提，这具有局限性。论文认为T2I背景下的美学应是个性化的。", "method": "本文提出了一种新颖的“美学对齐”任务，旨在将用户指定的美学与T2I生成输出对齐。受艺术品如何提供宝贵美学视角的启发，研究人员利用艺术家所采用的构图框架，即“艺术原则”（PoA），对视觉美学进行了编码。为了促进这项研究，他们构建了一个名为CompArt的大规模组合艺术数据集，该数据集基于WikiArt并由多模态大型语言模型（LLM）进行了PoA分析注释。通过利用LLM的表达能力并训练一个轻量级且可迁移的适配器，他们实现了构图控制。此外，还设计了一个适当的评估框架来评估该方法的有效性。", "result": "研究表明，T2I扩散模型能够通过用户指定的艺术原则（PoA）条件有效提供10种构图控制。此外，还设计了一个适当的评估框架来评估该方法的有效性。", "conclusion": "本文成功地通过整合艺术原则，使得文本到图像模型能够实现个性化的美学控制，为生成更具吸引力和更符合用户偏好的图像开辟了新的方向。", "translation": "文本到图像（T2I）扩散模型（DM）因其生成高保真输出的能力以及对任何能够将想象力转化为文字的人的可访问性而获得了广泛采用。然而，DM通常倾向于生成不美观的输出，就像它们训练所用的互联网随机图像一样。现有解决此问题的方法基于视觉美学是普遍的隐含前提，这具有局限性。T2I背景下的美学应该是关于个性化的，我们提出了美学对齐的新任务，旨在将用户指定的美学与T2I生成输出对齐。受艺术品如何提供宝贵视角来处理美学的启发，我们使用艺术家所采用的构图框架，即艺术原则（PoA），对视觉美学进行编码。为了促进这项研究，我们引入了CompArt，一个基于WikiArt构建的大规模组合艺术数据集，其中包含由一个有能力的多模态LLM注释的PoA分析。利用LLM的表达能力并训练一个轻量级且可迁移的适配器，我们证明了T2I DM可以通过用户指定的PoA条件有效提供10种构图控制。此外，我们设计了一个适当的评估框架来评估我们方法的有效性。", "summary": "文本到图像（T2I）扩散模型常因缺乏个性化美学控制而生成不美观的图像。本文提出“美学对齐”这一新任务，旨在通过整合用户指定的艺术原则（PoA）来增强T2I模型。研究人员将视觉美学编码为PoA，并构建了由多模态LLM注释的大型数据集CompArt。通过训练轻量级适配器，该方法使T2I模型能够提供10种构图控制，从而允许用户根据艺术原则指导图像生成，显著提升输出的吸引力和个性化程度。", "keywords": "文本到图像, 美学, 扩散模型, 艺术原则, 构图控制", "comments": "本文的创新之处在于将T2I模型的图像生成从普遍美学推向个性化控制，并创造性地将艺术领域的“艺术原则”（PoA）作为结构化的美学定义。CompArt数据集的构建和利用LLM进行注释及适配器训练是其技术亮点。这项工作解决了当前T2I模型在美学输出方面的一个关键局限性，为未来生成更具艺术指导性的图像提供了新的方向。"}}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation.", "AI": {"title_translation": "SteerX：使用几何引导创建任何无相机3D和4D场景", "tldr": "SteerX是一种零样本推理时引导方法，它将场景重建统一到3D/4D场景生成过程中，通过引入几何奖励函数来改善几何对齐。", "motivation": "现有的3D/4D场景生成方法在不同阶段分别改进对齐，难以管理其他阶段产生的细微错位。", "method": "SteerX是一种零样本推理时引导方法，它将场景重建统一到生成过程，并引入了两个使用无姿态前馈场景重建模型的几何奖励函数，以使数据分布倾向于更好的几何对齐。", "result": "通过广泛的实验，我们证明了SteerX在改进3D/4D场景生成方面的有效性。", "conclusion": "SteerX通过在生成过程中统一场景重建并引入几何奖励函数，有效改善了3D/4D场景生成的几何对齐问题。", "translation": "3D/4D场景生成的最新进展强调了视频生成和场景重建过程中物理对齐的重要性。然而，现有方法在每个阶段分别改进对齐，使得难以管理由另一阶段产生的细微错位。在此，我们提出了SteerX，一种零样本推理时引导方法，它将场景重建统一到生成过程中，使数据分布倾向于更好的几何对齐。为此，我们通过使用无姿态前馈场景重建模型，为3D/4D场景生成引入了两个几何奖励函数。通过广泛的实验，我们证明了SteerX在改进3D/4D场景生成方面的有效性。", "summary": "SteerX是一种创新的零样本推理时引导方法，旨在解决现有3D/4D场景生成中几何对齐的挑战。它通过将场景重建整合到生成过程中，并引入基于无姿态前馈场景重建模型的几何奖励函数，来优化数据分布以实现更好的几何对齐。实验证明SteerX能有效提升3D/4D场景生成的质量。", "keywords": "3D/4D场景生成, 几何引导, 场景重建, 零样本推理, 几何对齐", "comments": "SteerX的创新之处在于它将场景重建与生成过程统一起来，并通过引入几何奖励函数来解决现有方法中难以处理的细微错位问题。这种零样本推理时引导方法提供了一种新颖的视角，有望显著提升3D/4D场景生成的物理对齐精度和整体质量。"}}
{"id": "2503.12026", "pdf": "https://arxiv.org/pdf/2503.12026", "abs": "https://arxiv.org/abs/2503.12026", "authors": ["Zihan Zhoua", "Changrui Daia", "Aibo Songa", "Xiaolin Fang"], "title": "Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised video correspondence learning depends on the ability to\naccurately associate pixels between video frames that correspond to the same\nvisual object. However, achieving reliable pixel matching without supervision\nremains a major challenge. To address this issue, recent research has focused\non feature learning techniques that aim to encode unique pixel representations\nfor matching. Despite these advances, existing methods still struggle to\nachieve exact pixel correspondences and often suffer from false matches,\nlimiting their effectiveness in self-supervised settings.\n  To this end, we explore an efficient self-supervised Video Correspondence\nLearning framework (MER) that aims to accurately extract object details from\nunlabeled videos. First, we design a dedicated Motion Enhancement Engine that\nemphasizes capturing the dynamic motion of objects in videos. In addition, we\nintroduce a flexible sampling strategy for inter-pixel correspondence\ninformation (Multi-Cluster Sampler) that enables the model to pay more\nattention to the pixel changes of important objects in motion. Through\nexperiments, our algorithm outperforms the state-of-the-art competitors on\nvideo correspondence learning tasks such as video object segmentation and video\nobject keypoint tracking.", "AI": {"title_translation": "利用运动信息改进自监督视频对应学习", "tldr": "本文提出了一个名为MER的自监督视频对应学习框架，通过运动增强引擎和多簇采样器，有效提高了无监督视频中像素匹配的准确性，并在视频目标分割和关键点跟踪任务上超越了现有技术。", "motivation": "自监督视频对应学习在无监督像素匹配方面面临挑战，现有方法难以实现精确的像素对应，并常出现错误匹配，限制了其有效性。", "method": "本文提出了一个名为MER的自监督视频对应学习框架，旨在从无标签视频中准确提取物体细节。该框架包含一个专门的运动增强引擎，用于捕捉视频中物体的动态运动；以及一个灵活的像素间对应信息采样策略（多簇采样器），使模型更关注运动中重要物体的像素变化。", "result": "MER算法在视频对应学习任务（如视频目标分割和视频目标关键点跟踪）上，表现优于现有最先进的竞争算法。", "conclusion": "本文提出的MER框架通过有效利用运动信息和改进采样策略，成功提高了自监督视频对应学习的准确性和鲁棒性，解决了现有方法在像素匹配上的不足。", "translation": "自监督视频对应学习依赖于在视频帧之间准确关联对应相同视觉对象的像素的能力。然而，在没有监督的情况下实现可靠的像素匹配仍然是一个重大挑战。为了解决这个问题，最近的研究集中于特征学习技术，旨在编码独特的像素表示以进行匹配。尽管取得了这些进展，现有方法仍然难以实现精确的像素对应，并且经常出现错误匹配，限制了它们在自监督设置中的有效性。\n为此，我们探索了一个高效的自监督视频对应学习框架（MER），旨在从无标签视频中准确提取对象细节。首先，我们设计了一个专门的运动增强引擎，强调捕捉视频中对象的动态运动。此外，我们引入了一种灵活的像素间对应信息采样策略（多簇采样器），使模型能够更关注运动中重要对象的像素变化。通过实验，我们的算法在视频对应学习任务（如视频对象分割和视频对象关键点跟踪）上超越了最先进的竞争算法。", "summary": "本文针对自监督视频对应学习中像素匹配准确性低和错误匹配多的问题，提出了一个名为MER的自监督视频对应学习框架。该框架通过设计运动增强引擎来捕捉物体动态运动，并引入多簇采样器策略来关注重要物体的像素变化。实验结果表明，MER在视频目标分割和视频目标关键点跟踪等任务上，性能优于当前最先进的方法。", "keywords": "自监督学习, 视频对应, 运动信息, 像素匹配, 视频目标分割", "comments": "本文的创新点在于引入了运动信息来提升自监督视频对应学习的准确性，特别是通过运动增强引擎和多簇采样器，使得模型能够更有效地捕捉和利用视频中的动态变化。这对于在无监督环境下实现精确的像素级对应具有重要意义，并有望推动视频理解领域的发展。"}}
{"id": "2503.12028", "pdf": "https://arxiv.org/pdf/2503.12028", "abs": "https://arxiv.org/abs/2503.12028", "authors": ["F. Çengel", "V. Adanova", "S. Tari"], "title": "Challenges in Plane Symmetry: From Theory to Perception", "categories": ["cs.CV"], "comment": null, "summary": "The planar ornaments are created by repeating a base unit using a combination\nof four primitive geometric operations: translation, rotation, reflection, and\nglide reflection. According to group theory, different combinations of these\nfour geometric operations lead to different symmetry groups. In this work, we\nselect a single challenging ornament, and analyze it both from the theoretical\npoint of view and perceptual point of view. We present the perceptual\nexperiment results, where one can see that the symmetries that the participants\nperceived from the ornaments do not match to what the theory dictates.", "AI": {"title_translation": "平面对称的挑战：从理论到感知", "tldr": "论文发现，人们对平面装饰的对称性感知与群论的理论规定不符。", "motivation": "探索平面装饰中理论对称性与感知对称性之间的差异，特别是当两者不匹配时。", "method": "选择一个具有挑战性的装饰图案，并从理论和感知的角度进行分析，通过感知实验验证参与者的感知。", "result": "感知实验结果显示，参与者从装饰中感知到的对称性与理论所规定的不符。", "conclusion": "感知到的平面对称性可能与基于群论的理论对称性存在显著差异，这表明理解对称性不仅需要理论知识，还需要考虑人类的感知。", "translation": "平面装饰是通过结合四种基本几何操作（平移、旋转、反射和滑移反射）重复一个基本单元而创建的。根据群论，这四种几何操作的不同组合会产生不同的对称群。在这项工作中，我们选择了一个具有挑战性的装饰，并从理论和感知的角度对其进行了分析。我们展示了感知实验结果，从中可以看出参与者从装饰中感知到的对称性与理论所规定的不符。", "summary": "本文探讨了平面装饰中理论对称性与感知对称性之间的差异。研究选取一个复杂的装饰图案，从群论角度进行理论分析，并开展感知实验。结果表明，参与者实际感知到的对称性与理论定义的对称性存在显著不符，揭示了平面对称感知中的挑战性问题。", "keywords": "平面对称, 感知, 群论, 几何操作, 装饰图案", "comments": "这项工作强调了理论与人类感知之间存在的有趣差异，对于理解视觉感知、图案设计以及人机交互中的对称性应用具有重要启发意义。"}}
{"id": "2503.12034", "pdf": "https://arxiv.org/pdf/2503.12034", "abs": "https://arxiv.org/abs/2503.12034", "authors": ["Enes Erdogan", "Eren Erdal Aksoy", "Sanem Sariel"], "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures, 7 tables", "summary": "Recognition of human manipulation actions in real-time is essential for safe\nand effective human-robot interaction and collaboration. The challenge lies in\ndeveloping a model that is both lightweight enough for real-time execution and\ncapable of generalization. While some existing methods in the literature can\nrun in real-time, they struggle with temporal scalability, i.e., they fail to\nadapt to long-duration manipulations effectively. To address this, leveraging\nthe generalizable scene graph representations, we propose a new Factorized\nGraph Sequence Encoder network that not only runs in real-time but also scales\neffectively in the temporal dimension, thanks to its factorized encoder\narchitecture. Additionally, we introduce Hand Pooling operation, a simple\npooling operation for more focused extraction of the graph-level embeddings.\nOur model outperforms the previous state-of-the-art real-time approach,\nachieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual\nAction (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.\nMoreover, we conduct an extensive ablation study to validate our network design\nchoices. Finally, we compare our model with its architecturally similar\nRGB-based model on the Bimacs dataset and show the limitations of this model in\ncontrast to ours on such an object-centric manipulation dataset.", "AI": {"title_translation": "实时操作行为识别与分解图序列编码器", "tldr": "提出了一种分解图序列编码器网络，用于实时、可伸缩的人类操作行为识别，在两个数据集上超越了现有SOTA方法。", "motivation": "实时识别人类操作行为对于安全有效的人机交互与协作至关重要。现有实时方法在时间可伸缩性上存在挑战，无法有效适应长时间操作。", "method": "提出了一种新的分解图序列编码器（Factorized Graph Sequence Encoder）网络，利用可泛化的场景图表示。该网络具有分解编码器架构，实现了实时性和时间维度的有效伸缩性。此外，引入了手部池化（Hand Pooling）操作，用于更聚焦地提取图级嵌入。", "result": "模型在KIT Bimanual Action (Bimacs) 数据集和Collaborative Action (CoAx) 数据集上的F1-macro分数分别比以前的最先进实时方法提高了14.3%和5.6%。还进行了广泛的消融研究，并与基于RGB的模型进行了比较，展示了其在以对象为中心的操作数据集上的局限性。", "conclusion": "提出的分解图序列编码器网络能够有效解决实时人类操作行为识别中的时间可伸缩性问题，并在性能上超越现有技术，为安全有效的人机交互提供了更好的解决方案。", "translation": "实时识别人类操作行为对于安全有效的人机交互与协作至关重要。挑战在于开发一个既轻量级足以实时执行又能够泛化的模型。虽然文献中一些现有方法可以实时运行，但它们在时间可伸缩性方面存在困难，即无法有效适应长时间的操作。为了解决这个问题，我们利用可泛化的场景图表示，提出了一种新的分解图序列编码器网络，它不仅可以实时运行，而且由于其分解编码器架构，在时间维度上也能有效伸缩。此外，我们引入了手部池化操作，这是一种简单的池化操作，用于更聚焦地提取图级嵌入。我们的模型在KIT双手动操作（Bimacs）数据集和协作操作（CoAx）数据集上的F1-macro分数分别比以前的最先进实时方法提高了14.3%和5.6%。此外，我们进行了广泛的消融研究以验证我们的网络设计选择。最后，我们将我们的模型与在Bimacs数据集上与其架构相似的基于RGB的模型进行了比较，并展示了该模型在以对象为中心的操作数据集上相对于我们的模型的局限性。", "summary": "这篇论文提出了一种名为“分解图序列编码器”（Factorized Graph Sequence Encoder）的新型网络，用于解决实时人类操作行为识别中现有方法在时间可伸缩性上的局限性。该模型利用可泛化的场景图表示和分解编码器架构，实现了实时高效且在时间维度上可伸缩的性能。通过引入手部池化操作，进一步优化了图级嵌入的提取。实验结果表明，该模型在两个公开数据集上显著超越了现有最先进的实时方法，并在消融研究中验证了其设计选择的有效性，同时指出了基于RGB模型在特定操作数据集上的局限性。", "keywords": "操作行为识别, 实时, 图神经网络, 时间可伸缩性, 人机交互", "comments": "这篇论文的创新点在于提出了分解图序列编码器和手部池化操作，有效地解决了实时操作行为识别中时间可伸缩性的关键问题。其轻量级和高泛化能力的特点使其在人机协作等领域具有重要的实际应用价值。性能上的显著提升也证明了其方法的有效性。"}}
{"id": "2503.12035", "pdf": "https://arxiv.org/pdf/2503.12035", "abs": "https://arxiv.org/abs/2503.12035", "authors": ["Zhengyuan Peng", "Jinpeng Ma", "Zhimin Sun", "Ran Yi", "Haichuan Song", "Xin Tan", "Lizhuang Ma"], "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is a classification task that aims to\nclassify both base and novel classes in unlabeled images, using knowledge from\na labeled dataset. In GCD, previous research overlooks scene information or\ntreats it as noise, reducing its impact during model training. However, in this\npaper, we argue that scene information should be viewed as a strong prior for\ninferring novel classes. We attribute the misinterpretation of scene\ninformation to a key factor: the Ambiguity Challenge inherent in GCD.\nSpecifically, novel objects in base scenes might be wrongly classified into\nbase categories, while base objects in novel scenes might be mistakenly\nrecognized as novel categories. Once the ambiguity challenge is addressed,\nscene information can reach its full potential, significantly enhancing the\nperformance of GCD models. To more effectively leverage scene information, we\npropose the Modeling Object-Scene Associations (MOS) framework, which utilizes\na simple MLP-based scene-awareness module to enhance GCD performance. It\nachieves an exceptional average accuracy improvement of 4% on the challenging\nfine-grained datasets compared to state-of-the-art methods, emphasizing its\nsuperior performance in fine-grained GCD. The code is publicly available at\nhttps://github.com/JethroPeng/MOS.", "AI": {"title_translation": "MOS: 广义类别发现中目标-场景关联建模", "tldr": "本论文提出MOS框架，通过利用场景信息解决广义类别发现中的歧义挑战，显著提升了模型性能，尤其在细粒度数据集上表现出色。", "motivation": "在广义类别发现 (GCD) 任务中，现有研究忽略场景信息或将其视为噪声，从而降低了其在模型训练中的影响。然而，本文认为场景信息是推断新类别的强先验，而其被误解的原因在于GCD固有的“歧义挑战”。解决这一挑战能充分发挥场景信息的潜力，显著提升GCD模型的性能。", "method": "本文提出了建模目标-场景关联 (MOS) 框架，该框架利用一个简单的基于MLP的场景感知模块来增强GCD性能，从而更有效地利用场景信息。", "result": "与现有最先进的方法相比，MOS框架在具有挑战性的细粒度数据集上实现了4%的平均准确率提高，突出了其在细粒度GCD中的卓越性能。", "conclusion": "通过解决歧义挑战并有效利用场景信息，MOS框架能够显著提升广义类别发现模型的性能，尤其是在细粒度GCD任务中。", "translation": "广义类别发现（GCD）是一种分类任务，旨在利用标记数据集中的知识，对未标记图像中的基础类别和新类别进行分类。在GCD中，以前的研究忽略场景信息或将其视为噪声，从而降低了其在模型训练中的影响。然而，在本文中，我们认为场景信息应被视为推断新类别的强先验。我们将场景信息被误解归因于一个关键因素：GCD固有的歧义挑战。具体来说，基础场景中的新对象可能被错误地分类到基础类别中，而新场景中的基础对象可能被错误地识别为新类别。一旦解决了歧义挑战，场景信息就能充分发挥其潜力，显著提高GCD模型的性能。为了更有效地利用场景信息，我们提出了建模目标-场景关联（MOS）框架，该框架利用一个简单的基于MLP的场景感知模块来增强GCD性能。与现有最先进的方法相比，它在具有挑战性的细粒度数据集上实现了4%的平均准确率提高，突出了其在细粒度GCD中的卓越性能。代码已在https://github.com/JethroPeng/MOS 公开。", "summary": "本文针对广义类别发现（GCD）任务，提出了建模目标-场景关联（MOS）框架。该框架旨在解决GCD中“歧义挑战”导致的场景信息被忽视或误解的问题，强调场景信息作为推断新类别的强先验的重要性。MOS通过一个简单的基于MLP的场景感知模块，有效利用目标-场景关联，显著提升了GCD模型的性能，尤其在细粒度数据集上实现了比现有方法高4%的平均准确率。", "keywords": "广义类别发现, 目标-场景关联, 场景信息, 细粒度分类, 歧义挑战", "comments": "该论文的创新之处在于识别并解决了广义类别发现任务中场景信息被忽视或误解的关键问题。通过提出MOS框架和引入场景感知模块，它提供了一种有效利用上下文信息来克服“歧义挑战”的方法，从而显著提升了模型在细粒度分类任务上的性能，为GCD领域带来了新的视角和改进。"}}
{"id": "2503.12047", "pdf": "https://arxiv.org/pdf/2503.12047", "abs": "https://arxiv.org/abs/2503.12047", "authors": ["Hangrui Xu", "Chuanrui Zhang", "Zhengxian Wu", "Peng Jiao", "Haoqian Wang"], "title": "PSGait: Multimodal Gait Recognition using Parsing Skeleton", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the parsing skeleton\nrepresentation, we propose a novel parsing skeleton-based gait recognition\nframework, named PSGait, which takes parsing skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods. Furthermore,\nas a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in\nRank-1 accuracy across various gait recognition models. These results\ndemonstrate the effectiveness and versatility of parsing skeletons for gait\nrecognition in the wild, establishing PSGait as a new state-of-the-art approach\nfor multimodal gait recognition.", "AI": {"title_translation": "PSGait：基于解析骨架的多模态步态识别", "tldr": "本文提出了一种名为解析骨架的新型步态表示方法，并开发了PSGait框架，通过融合解析骨架和轮廓信息，显著提高了野外场景下的步态识别准确率，超越了现有最先进的多模态方法。", "motivation": "传统的步态识别方法（依赖轮廓或骨架）在受控实验室环境中表现良好，但在真实世界场景中由于信息熵有限而效果不佳。为了在野外场景中实现准确的步态识别，需要一种新的步态表示方法。", "method": "提出了一种名为“解析骨架”的新型步态表示，该表示引入骨架引导的人体解析方法，以捕获细粒度身体动态，从而具有更高的信息熵。在此基础上，提出了一个名为PSGait的基于解析骨架的步态识别框架，该框架将解析骨架和轮廓作为输入，并通过融合这两种模态的图像序列来增强个体区分度。", "result": "PSGait在各种数据集上进行了全面基准测试，性能优于现有最先进的多模态方法。作为一种即插即用方法，PSGait使各种步态识别模型的Rank-1准确率最高提高了10.9%。", "conclusion": "解析骨架在野外步态识别中表现出有效性和多功能性。PSGait被确立为多模态步态识别领域新的最先进方法。", "translation": "步态识别因其非侵入性和对遮挡的弹性而成为一种强大的生物识别模式。传统的步态识别方法通常依赖于轮廓或骨架。尽管它们在受控实验室环境下的步态识别中取得了成功，但由于其步态表示的信息熵有限，它们通常在真实世界场景中失败。为了在野外实现准确的步态识别，我们提出了一种新颖的步态表示，名为解析骨架。这种表示创新性地引入了骨架引导的人体解析方法来捕获细粒度身体动态，因此它们具有更高的信息熵来编码行走过程中细粒度人体部位的形状和动态。此外，为了有效地探索解析骨架表示的能力，我们提出了一种新颖的基于解析骨架的步态识别框架，名为PSGait，它以解析骨架和轮廓作为输入。通过融合这两种模态，得到的图像序列被输入到步态识别模型中，以增强个体区分度。我们在各种数据集上进行了全面的基准测试来评估我们的模型。PSGait优于现有最先进的多模态方法。此外，作为一种即插即用方法，PSGait使各种步态识别模型的Rank-1准确率最高提高了10.9%。这些结果证明了解析骨架在野外步态识别中的有效性和多功能性，确立了PSGait作为多模态步态识别领域新的最先进方法。", "summary": "本研究提出了一种名为“解析骨架”的新型步态表示方法，旨在解决传统步态识别方法在真实世界场景中信息熵有限的问题。解析骨架通过骨架引导的人体解析捕获细粒度身体动态，提高了信息熵。在此基础上，开发了PSGait框架，该框架融合了解析骨架和轮廓两种模态，以增强个体区分度。实验结果表明，PSGait在多个数据集上优于现有最先进的多模态方法，并且作为即插即用模块可使模型精度最高提升10.9%，证明了其在野外步态识别中的有效性和领先性。", "keywords": "步态识别, 解析骨架, 多模态, PSGait, 生物识别", "comments": "该论文的创新点在于提出了“解析骨架”这一新型步态表示，通过引入骨架引导的人体解析来捕获更细粒度的身体动态，显著增加了步态信息熵，从而克服了传统方法在复杂真实场景下的局限性。PSGait框架的提出以及多模态融合策略进一步提升了识别性能。其即插即用的特性也增加了其实用价值和潜在影响力。"}}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.", "AI": {"title_translation": "TACO：驯服扩散模型实现野外视频非模态补全", "tldr": "TACO是一个条件扩散模型，利用预训练视频扩散模型的丰富流形，并通过渐进式微调在大型合成数据集上训练，以解决野外视频非模态补全（VAC）的挑战，并在各种视频和下游任务中表现出多功能性。", "motivation": "人类可以从有限的视觉线索中推断出物体的完整形状和外观，但现有模型在完成部分可观测物体并确保视频帧间一致性方面仍面临挑战，尤其是在非结构化的野外视频中。本文旨在解决视频非模态补全（VAC）任务。", "method": "本文提出了一个条件扩散模型TACO，它利用预训练视频扩散模型学习到的丰富、一致的流形，并将其重新用于VAC任务。为了实现对野外场景的有效和鲁棒泛化，研究者策划了一个大型合成数据集，通过系统地对未遮挡视频施加遮挡来设置多个难度级别。在此基础上，他们设计了一种渐进式微调范式，从简单的恢复任务开始，逐渐推进到更复杂的任务。", "result": "TACO在来自互联网的各种野外视频以及自动驾驶、机器人操作和场景理解中常用的多样化、未见数据集上展示了其多功能性。此外，研究表明TACO可以有效地应用于物体重建和姿态估计等各种下游任务。", "conclusion": "TACO模型通过解决视频非模态补全任务，展示了其促进物理世界理解和推理的潜力。", "translation": "人类可以从有限的视觉线索中推断出物体的完整形状和外观，这依赖于对物理世界的广泛先验知识。然而，对于现有模型而言，完成部分可观测物体并确保视频帧间一致性仍然具有挑战性，尤其是对于非结构化的野外视频。本文旨在解决视频非模态补全（VAC）任务，其目标是给定一个指定感兴趣物体的视觉提示，在整个视频中一致地生成完整的物体。我们利用预训练视频扩散模型学习到的丰富、一致的流形，提出了一个条件扩散模型TACO，它将这些流形重新用于VAC。为了使其能够有效且鲁棒地泛化到具有挑战性的野外场景，我们通过系统地对未遮挡视频施加遮挡，策划了一个具有多个难度级别的大型合成数据集。在此基础上，我们设计了一种渐进式微调范式，从简单的恢复任务开始，逐渐推进到更复杂的任务。我们展示了TACO在来自互联网的各种野外视频以及自动驾驶、机器人操作和场景理解中常用的多样化、未见数据集上的多功能性。此外，我们还展示了TACO可以有效地应用于物体重建和姿态估计等各种下游任务，突显了其促进物理世界理解和推理的潜力。我们的项目页面可在https://jason-aplp.github.io/TACO获取。", "summary": "本文提出了一种名为TACO的条件扩散模型，用于解决野外视频非模态补全（VAC）的挑战。TACO利用预训练视频扩散模型学习到的丰富流形，并通过构建大型合成数据集和采用渐进式微调策略来增强其泛化能力。实验结果表明，TACO在各种野外视频和未见数据集上表现出卓越的多功能性，并且能够有效应用于物体重建和姿态估计等下游任务，从而促进对物理世界的理解和推理。", "keywords": "视频非模态补全, 扩散模型, 视频补全, 野外视频, 渐进式微调", "comments": "该论文的创新点在于将预训练的视频扩散模型重新利用于视频非模态补全任务，并引入了渐进式微调范式以应对野外场景的复杂性。其构建大规模合成数据集的方法也为该领域的研究提供了宝贵的资源。TACO不仅解决了核心的VAC问题，还展示了其在多个下游任务中的应用潜力，这突显了其在推动物理世界理解和推理方面的重要性。"}}
{"id": "2503.12052", "pdf": "https://arxiv.org/pdf/2503.12052", "abs": "https://arxiv.org/abs/2503.12052", "authors": ["Zhiyao Sun", "Yu-Hui Wen", "Matthieu Lin", "Ho-Jui Fang", "Sheng Ye", "Tian Lv", "Yong-Jin Liu"], "title": "Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://human-tailor.github.com", "summary": "Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use.", "AI": {"title_translation": "Tailor：一个集成的文本驱动CG就绪人体和服装生成系统", "tldr": "Tailor是一个文本到3D头像系统，能生成高保真、可定制的3D人体及可模拟的服装，解决了现有方法在生成CG就绪虚拟形象方面的不足。", "motivation": "创建详细的3D人体化身及其服装通常需要专业知识和劳动密集型过程。尽管生成式AI在文本到3D人体/服装生成方面取得了进展，但现有方法缺乏提供可访问、集成管道来生产即用型着装化身的能力。", "method": "Tailor系统包含一个三阶段管道：1. 使用大型语言模型将文本描述解释为参数化身体形状和语义匹配的服装模板。2. 开发具有新颖几何损失的拓扑保持变形，以精确地使服装适应身体几何形状。3. 采用增强的纹理扩散模块和对称局部注意力机制，确保视图一致性和照片级真实细节。", "result": "定量和定性评估表明，Tailor在保真度、可用性和多样性方面优于现有最先进的方法。", "conclusion": "Tailor系统成功地提供了一个集成且高效的解决方案，用于从文本描述生成高保真、CG就绪的3D人体和服装，克服了现有方法的局限性。", "translation": "创建带有服装的详细3D人体化身通常需要专业知识和劳动密集型过程。尽管生成式AI在文本到3D人体/服装生成方面取得了最新进展，但现有方法在提供可访问、集成的管道以生产即用型着装化身方面存在不足。为了解决这个问题，我们引入了Tailor，一个集成的文本到化身系统，可以生成高保真、可定制的3D人体和可模拟的服装。我们的系统包括一个三阶段管道。我们首先使用大型语言模型将文本描述解释为参数化身体形状和语义匹配的服装模板。接下来，我们开发了具有新颖几何损失的拓扑保持变形，以精确地使服装适应身体几何形状。此外，一个带有对称局部注意力机制的增强纹理扩散模块确保了视图一致性和照片级真实细节。定量和定性评估表明，Tailor在保真度、可用性和多样性方面优于现有最先进的方法。代码将提供给学术使用。", "summary": "Tailor是一个创新的集成系统，旨在解决当前文本到3D人体/服装生成方法在生成CG就绪虚拟形象方面的不足。该系统通过一个三阶段管道，利用大型语言模型解析文本描述，实现服装与身体的精确适应，并通过增强的纹理扩散模块确保高保真细节。实验证明，Tailor在保真度、可用性和多样性方面均优于现有技术。", "keywords": "3D人体生成, 服装生成, 文本到3D, 虚拟化身, 生成式AI", "comments": "Tailor的创新之处在于其集成的三阶段管道，特别是结合LLM进行语义理解和几何变形以实现服装与身体的精确匹配，以及对称局部注意力机制在纹理生成中的应用。这显著降低了创建高质量3D虚拟形象的门槛，对于CG、游戏和虚拟现实领域具有重要意义。"}}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet.", "AI": {"title_translation": "EHNet: 一种用于人群计数和定位的高效混合网络", "tldr": "EHNet是一个高效的混合网络，通过新颖的模块解决了人群计数和定位中的多尺度挑战，并在多个基准数据集上取得了领先性能。", "motivation": "人群计数和定位是计算机视觉中的关键技术，但单幅图像中多尺度人群分布的存在仍然是人群计数任务中的一个基本挑战。", "method": "引入了高效混合网络（EHNet），将人群计数重新定义为点回归框架。EHNet利用空间位置注意力模块（SPAM）捕获全面的空间上下文和长程依赖，并开发了自适应特征聚合模块（AFAM）以有效融合和协调多尺度特征表示。在此基础上，引入了多尺度注意力解码器（MSAD）。", "result": "在ShanghaiTech Part A、ShanghaiTech Part B、UCF-CC-50和UCF-QNRF四个基准数据集上的实验结果表明，EHNet在降低计算开销的情况下实现了有竞争力的性能，并优于现有方法。", "conclusion": "EHNet通过其新颖的模块化设计，有效解决了人群计数和定位中的多尺度挑战，并在性能和效率之间取得了良好平衡。", "translation": "近年来，人群计数和定位已成为计算机视觉中至关重要的技术，其应用涵盖各个领域。单幅图像中多尺度人群分布的存在仍然是人群计数任务中的一个基本挑战。为了解决这些挑战，我们引入了高效混合网络（EHNet），这是一种用于高效人群计数和定位的新颖框架。通过将人群计数重新表述为点回归框架，EHNet利用空间位置注意力模块（SPAM）来捕获全面的空间上下文和长程依赖。此外，我们开发了一个自适应特征聚合模块（AFAM），以有效地融合和协调多尺度特征表示。在此基础上，我们引入了多尺度注意力解码器（MSAD）。在四个基准数据集上的实验结果表明，EHNet在降低计算开销的情况下实现了有竞争力的性能，在ShanghaiTech Part A、ShanghaiTech Part B、UCF-CC-50和UCF-QNRF上优于现有方法。我们的代码位于https://anonymous.4open.science/r/EHNet。", "summary": "EHNet是一种新颖的高效混合网络，专为解决人群计数和定位中的多尺度挑战而设计。它将人群计数重构为点回归问题，并引入了空间位置注意力模块（SPAM）和自适应特征聚合模块（AFAM）来处理空间上下文和多尺度特征融合。此外，还提出了多尺度注意力解码器（MSAD）。实验证明，EHNet在多个主流数据集上以更低的计算成本取得了优于现有方法的性能。", "keywords": "人群计数, 人群定位, 高效混合网络, 多尺度, 注意力机制", "comments": "EHNet的创新之处在于其针对人群计数和定位任务中多尺度挑战的模块化设计，特别是SPAM和AFAM的使用，这些模块有助于有效地捕获上下文信息和融合多尺度特征。其在保持竞争性性能的同时降低计算开销，表明了其在实际应用中的潜力。"}}
{"id": "2503.12063", "pdf": "https://arxiv.org/pdf/2503.12063", "abs": "https://arxiv.org/abs/2503.12063", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting", "categories": ["cs.CV"], "comment": null, "summary": "Cell counting remains a fundamental yet challenging task in medical and\nbiological research due to the diverse morphology of cells, their dense\ndistribution, and variations in image quality. We present DLA-Count, a\nbreakthrough approach to cell counting that introduces three key innovations:\n(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell\nmatching in dense regions, (2) Multi-scale Deformable Gaussian Convolution\n(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced\nFeature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive\nexperiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)\ndemonstrate that our method outperforms previous methods across diverse\ndatasets, with improvements in Mean Absolute Error of up to 46.7\\% on ADI and\n42.5\\% on MBM datasets. Our code is available at\nhttps://anonymous.4open.science/r/DLA-Count.", "AI": {"title_translation": "DLA-Count：用于密集细胞分布计数的动态标签分配网络", "tldr": "DLA-Count是一种用于密集细胞计数的突破性方法，通过引入KHM、MDGC和GFD显著提高了计数精度。", "motivation": "由于细胞形态多样、分布密集以及图像质量变化，细胞计数在医学和生物学研究中仍然是一项基础但具有挑战性的任务。", "method": "本文提出了DLA-Count，一种细胞计数方法，包含三项关键创新：1) K-邻近匈牙利匹配（KHM），显著改善密集区域的细胞匹配；2) 多尺度可变形高斯卷积（MDGC），适应不同细胞形态；3) 高斯增强特征解码器（GFD），用于高效的多尺度特征融合。", "result": "在四个具有挑战性的细胞计数数据集（ADI、MBM、VGG和DCC）上的大量实验表明，该方法在不同数据集上均优于现有方法，在ADI数据集上的平均绝对误差提高了46.7%，在MBM数据集上提高了42.5%。", "conclusion": "DLA-Count通过其创新的组件，在密集细胞分布计数方面取得了显著的性能提升，超越了现有方法。", "translation": "细胞计数在医学和生物学研究中仍然是一项基础但具有挑战性的任务，这归因于细胞形态的多样性、其密集分布以及图像质量的变化。我们提出了DLA-Count，一种细胞计数方面的突破性方法，引入了三项关键创新：(1) K-邻近匈牙利匹配（KHM），显著改善了密集区域的细胞匹配；(2) 多尺度可变形高斯卷积（MDGC），适应不同的细胞形态；以及(3) 高斯增强特征解码器（GFD），用于高效的多尺度特征融合。我们在四个具有挑战性的细胞计数数据集（ADI、MBM、VGG和DCC）上进行了广泛的实验，结果表明我们的方法在不同数据集上均优于现有方法，在ADI数据集上的平均绝对误差提高了46.7%，在MBM数据集上提高了42.5%。我们的代码可在https://anonymous.4open.science/r/DLA-Count获取。", "summary": "DLA-Count是一种针对密集细胞计数任务的新型动态标签分配网络。它通过引入K-邻近匈牙利匹配（KHM）改善密集区域匹配，多尺度可变形高斯卷积（MDGC）适应细胞形态，以及高斯增强特征解码器（GFD）进行高效特征融合。实验证明，DLA-Count在多个挑战性数据集上显著优于现有方法，在ADI和MBM数据集上分别实现了高达46.7%和42.5%的平均绝对误差改进。", "keywords": "细胞计数, 动态标签分配, 深度学习, 图像分析, DLA-Count", "comments": "DLA-Count通过其独特的三项创新（KHM、MDGC、GFD）解决了细胞计数中密集分布和形态多样性的核心挑战，展示了显著的性能提升，尤其是在平均绝对误差方面，表明其在医学和生物学图像分析领域具有重要应用潜力。"}}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey", "AI": {"title_translation": "知识蒸馏的全面综述", "tldr": "这篇论文是对知识蒸馏方法的一项全面综述，旨在解决大型深度学习模型在边缘设备上部署的挑战，并涵盖了最新的方法和未来方向。", "motivation": "大型深度神经网络（DNNs）和Transformer模型参数量巨大，导致在边缘设备上部署时存在高运行时和内存消耗等问题，尤其是对于大型基础模型、视觉-语言模型（VLMs）和大型语言模型（LLMs）。知识蒸馏（KD）作为一种解决方案被提出。", "method": "本文提出了一项关于知识蒸馏方法的全面综述，从蒸馏源、蒸馏方案、蒸馏算法、模态蒸馏、蒸馏应用以及现有方法比较等不同方面进行了回顾。本工作以新的视角和表示结构对知识蒸馏的最新方法进行分类和研究，并涵盖了扩散模型、3D输入、基础模型、Transformer和LLMs的知识蒸馏等重要子类别。", "result": "本综述提供了一个新的视角和表示结构，对知识蒸馏的最新方法进行了分类和研究，并涵盖了多个重要的子类别，如扩散模型、3D输入、基础模型、Transformer和LLMs的知识蒸馏。", "conclusion": "论文讨论了知识蒸馏中存在的挑战以及未来可能的研究方向。", "translation": "深度神经网络（DNNs）在计算机视觉和自然语言处理领域取得了显著的性能，并在学术界和工业界有广泛应用。然而，随着DNNs和具有大量参数的Transformer模型的最新进展，在边缘设备上部署这些大型模型会导致诸如高运行时和内存消耗等严重问题。这对于最近的大规模基础模型、视觉-语言模型（VLMs）和大型语言模型（LLMs）尤其令人担忧。知识蒸馏（KD）是解决上述问题的突出技术之一，它采用教师-学生架构。更具体地说，一个轻量级的学生模型通过从繁重的教师模型中获取额外知识进行训练。在这项工作中，提出了一项关于知识蒸馏方法的全面综述。这包括从不同方面回顾KD：蒸馏源、蒸馏方案、蒸馏算法、模态蒸馏、蒸馏应用以及现有方法之间的比较。与大多数现有综述相比，这些综述要么过时，要么只是更新了以前的综述，这项工作提出了一项具有新观点和表示结构的全面综述，对知识蒸馏中最新的方法进行了分类和研究。本综述考虑了各种至关重要的子类别，包括扩散模型、3D输入、基础模型、Transformer和LLMs的KD。此外，还讨论了KD中存在的挑战和未来可能的研究方向。项目Github页面：https://github.com/IPL-Sharif/KD_Survey", "summary": "这篇论文提供了一项关于知识蒸馏（KD）方法的全面综述，旨在解决大型深度神经网络和Transformer模型在边缘设备上部署时面临的挑战。该综述从蒸馏源、方案、算法、模态、应用以及方法比较等多个角度对KD进行了深入分析，并引入了新的分类结构和视角，重点关注了最新的KD方法，包括针对扩散模型、3D输入、基础模型、Transformer和LLMs的KD。文章还讨论了KD领域的现有挑战和未来的研究方向。", "keywords": "知识蒸馏, 模型压缩, 深度学习, 教师-学生架构, 综述", "comments": "这篇综述的创新之处在于其提供了新的视角和表示结构来分类和研究知识蒸馏的最新方法，并且涵盖了更广泛的子类别，特别是针对当前热门的基础模型、Transformer和LLMs的KD，使其区别于过时或简单的更新。这对于研究人员理解和应用知识蒸馏技术具有重要价值。"}}
{"id": "2503.12068", "pdf": "https://arxiv.org/pdf/2503.12068", "abs": "https://arxiv.org/abs/2503.12068", "authors": ["Qingchen Tang", "Lei Fan", "Maurice Pagnucco", "Yang Song"], "title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised image segmentation with image-level labels has drawn\nattention due to the high cost of pixel-level annotations. Traditional methods\nusing Class Activation Maps (CAMs) often highlight only the most discriminative\nregions, leading to incomplete masks. Recent approaches that introduce textual\ninformation struggle with histopathological images due to inter-class\nhomogeneity and intra-class heterogeneity. In this paper, we propose a\nprototype-based image prompting framework for histopathological image\nsegmentation. It constructs an image bank from the training set using\nclustering, extracting multiple prototype features per class to capture\nintra-class heterogeneity. By designing a matching loss between input features\nand class-specific prototypes using contrastive learning, our method addresses\ninter-class homogeneity and guides the model to generate more accurate CAMs.\nExperiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show\nthat our method outperforms existing weakly supervised segmentation approaches,\nsetting new benchmarks in histopathological image segmentation.", "AI": {"title_translation": "基于原型的图像提示用于弱监督组织病理学图像分割", "tldr": "本文提出了一种基于原型的图像提示框架，通过聚类和对比学习解决组织病理学图像分割中类内异质性和类间同质性问题，从而生成更准确的CAMs，并在多个数据集上优于现有方法。", "motivation": "像素级标注成本高昂，导致弱监督图像分割受到关注。现有方法如CAMs常导致掩膜不完整。引入文本信息的方法难以处理组织病理学图像，原因是类间同质性高和类内异质性强。", "method": "本文提出一个基于原型的图像提示框架。它通过聚类从训练集中构建图像库，为每个类别提取多个原型特征以捕捉类内异质性。通过设计输入特征与类特定原型之间的匹配损失（使用对比学习），该方法解决了类间同质性问题并引导模型生成更准确的CAMs。", "result": "在四个数据集（LUAD-HistoSeg、BCSS-WSSS、GCSS和BCSS）上的实验表明，该方法优于现有的弱监督分割方法，并在组织病理学图像分割领域设立了新的基准。", "conclusion": "该研究成功提出了一个有效的原型基图像提示框架，显著提升了弱监督组织病理学图像分割的性能，解决了现有方法在处理类内异质性和类间同质性方面的不足。", "translation": "由于像素级标注成本高昂，带有图像级标签的弱监督图像分割受到了关注。使用类激活图（CAMs）的传统方法通常只突出最具判别性的区域，导致掩膜不完整。最近引入文本信息的方法在组织病理学图像中表现不佳，原因是类间同质性高和类内异质性强。在本文中，我们提出了一种用于组织病理学图像分割的基于原型的图像提示框架。它通过聚类从训练集中构建图像库，为每个类别提取多个原型特征以捕捉类内异质性。通过设计输入特征与类特定原型之间的匹配损失（使用对比学习），我们的方法解决了类间同质性问题并引导模型生成更准确的CAMs。在四个数据集（LUAD-HistoSeg、BCSS-WSSS、GCSS和BCSS）上的实验表明，我们的方法优于现有的弱监督分割方法，并在组织病理学图像分割领域设立了新的基准。", "summary": "本文针对弱监督组织病理学图像分割中像素级标注成本高、现有方法掩膜不完整以及处理类内异质性和类间同质性困难的问题，提出了一种基于原型的图像提示框架。该框架通过聚类构建图像库以提取多原型特征捕捉类内异质性，并利用对比学习设计匹配损失以解决类间同质性问题，从而生成更准确的类激活图。实验结果显示，该方法在多个数据集上显著优于现有弱监督分割方法，并设立了新的性能基准。", "keywords": "弱监督分割, 组织病理学图像, 原型学习, 图像提示, 对比学习", "comments": "该论文的创新点在于将原型学习和对比学习引入到弱监督组织病理学图像分割中，有效地解决了传统CAM方法掩膜不完整以及现有方法在处理组织病理图像复杂性时的局限性。通过捕捉类内异质性和缓解类间同质性，该方法显著提升了分割精度，为医学图像分析领域提供了一个有前景的新方向。"}}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness.", "AI": {"title_translation": "鲁棒数据集蒸馏通过匹配对抗轨迹", "tldr": "提出了一种名为MAT的方法，通过在蒸馏过程中引入对抗性训练，实现了鲁棒数据集蒸馏，使得模型在蒸馏数据上训练后能抵抗对抗性攻击并保持高精度。", "motivation": "现有数据集蒸馏方法忽略了模型的鲁棒性，导致使用蒸馏数据训练的模型容易受到对抗性攻击。", "method": "提出“鲁棒数据集蒸馏”任务，并引入匹配对抗轨迹（MAT）方法。MAT将对抗训练整合到基于轨迹的数据集蒸馏中，通过在轨迹生成过程中纳入对抗样本，获得鲁棒的训练轨迹来指导蒸馏过程。", "result": "实验证明，即使通过在蒸馏数据集上进行自然训练，模型也能获得增强的对抗鲁棒性，同时与现有蒸馏方法相比保持有竞争力的精度。", "conclusion": "本工作将鲁棒数据集蒸馏作为一个新的重要研究方向，并为未来弥合高效训练和对抗鲁棒性之间差距的研究提供了强大的基线。", "translation": "数据集蒸馏能够合成紧凑的数据集，使模型在训练后达到与原始大规模数据集相当的性能。然而，现有的蒸馏方法忽略了模型的鲁棒性，导致使用蒸馏数据训练的模型容易受到对抗性攻击。为了解决这一限制，我们引入了“鲁棒数据集蒸馏”任务，这是一个在蒸馏过程中将对抗鲁棒性嵌入到合成数据集中的新范式。我们提出了匹配对抗轨迹（MAT），一种将对抗训练集成到基于轨迹的数据集蒸馏中的方法。MAT在轨迹生成过程中融入对抗样本，以获得鲁棒的训练轨迹，然后用这些轨迹来指导蒸馏过程。实验证明，即使通过在我们蒸馏的数据集上进行自然训练，模型也能获得增强的对抗鲁棒性，同时与现有蒸馏方法相比保持有竞争力的精度。我们的工作强调了鲁棒数据集蒸馏作为一个新的重要研究方向，并为未来弥合高效训练和对抗鲁棒性之间差距的研究提供了强大的基线。", "summary": "本文针对现有数据集蒸馏方法缺乏模型鲁棒性的问题，提出了“鲁棒数据集蒸馏”的新任务。为此，作者提出了匹配对抗轨迹（MAT）方法，该方法通过在基于轨迹的蒸馏过程中整合对抗训练，利用对抗样本生成鲁棒的训练轨迹，从而指导合成数据集的生成。实验结果表明，使用MAT蒸馏出的数据集进行训练，模型在保持高精度的同时，显著增强了对抗鲁棒性。这项工作为高效训练与对抗鲁棒性之间的结合开辟了新方向。", "keywords": "数据集蒸馏, 对抗鲁棒性, 对抗训练, 轨迹匹配, 鲁棒性", "comments": "这篇论文的创新点在于首次明确提出了“鲁棒数据集蒸馏”这一新任务，并提供了一个有效的解决方案MAT。它解决了现有数据集蒸馏方法在模型鲁棒性方面的短板，对于在资源受限或需要快速部署的场景下，提升模型对对抗性攻击的防御能力具有重要意义。MAT通过将对抗训练融入蒸馏过程，使得合成数据本身就具备了鲁棒性特征，这是一个巧妙且实用的方法。"}}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization.", "AI": {"title_translation": "V-Stylist：基于多模态大语言模型智能体协作与反思的视频风格化系统", "tldr": "V-Stylist是一个多智能体系统，利用多模态大语言模型的协作与反思范式，实现了对复杂视频和开放式用户查询的视频风格化，并达到了最先进的性能。", "motivation": "尽管视频风格化取得了进展，但现有方法难以根据开放式用户查询，对包含复杂转场的视频进行风格化处理。本文旨在填补这一空白。", "method": "本文提出了一个通用的多智能体视频风格化系统V-Stylist，该系统采用多模态大语言模型（MLLM）的新颖协作与反思范式。V-Stylist包含三个关键角色：1) 视频解析器：将输入视频分解为多个镜头，并生成关键镜头内容的文本提示，有效处理复杂转场。2) 风格解析器：识别用户查询中的风格，并通过鲁棒的思维树搜索范式，从风格树中逐步搜索匹配的风格模型，精确指定模糊的风格偏好。3) 风格艺术家：利用匹配的模型将所有视频镜头渲染成所需风格，并通过多轮自我反思范式自适应调整细节控制。此外，本文还构建了一个新的基准测试TVSBench，用于评估复杂视频在开放用户查询下的风格化效果。", "result": "V-Stylist在有效和自动视频风格化方面取得了重大突破，解决了主要挑战。广泛的实验表明，V-Stylist达到了最先进的水平，例如，在整体平均指标上，V-Stylist分别超越FRESCO和ControlVideo 6.05%和4.51%，标志着视频风格化领域的重大进展。", "conclusion": "V-Stylist通过模拟人类专业人士的独特设计，成功应对了复杂视频转场和开放式风格描述的挑战，实现了高效自动的视频风格化，并达到了当前最先进的性能。", "translation": "尽管视频风格化近期取得了进展，但大多数现有方法难以根据用户查询的开放式风格描述来渲染具有复杂转场的任何视频。为了填补这一空白，我们引入了一个通用的多智能体视频风格化系统V-Stylist，它采用了一种新颖的多模态大语言模型协作与反思范式。具体来说，我们的V-Stylist是一个系统化的工作流程，包含三个关键角色：(1) 视频解析器：将输入视频分解为多个镜头，并生成其关键镜头内容的文本提示。通过简洁的视频到镜头提示范式，它使我们的V-Stylist能够有效处理具有复杂转场的视频。(2) 风格解析器：识别用户查询中的风格，并通过鲁棒的思维树搜索范式，逐步从风格树中搜索匹配的风格模型。它使我们的V-Stylist能够精确指定开放用户查询中模糊的风格偏好。(3) 风格艺术家：利用匹配的模型将所有视频镜头渲染成所需风格。通过新颖的多轮自我反思范式，它使我们的V-Stylist能够根据风格要求自适应调整细节控制。凭借这种模仿人类专业人士的独特设计，我们的V-Stylist在有效和自动视频风格化的主要挑战上取得了重大突破。此外，我们还构建了一个新的基准测试——文本驱动视频风格化基准（TVSBench），它填补了评估复杂视频在开放用户查询下风格化的空白。广泛的实验表明，V-Stylist达到了最先进的水平，例如，V-Stylist在整体平均指标上分别超越FRESCO和ControlVideo 6.05%和4.51%，标志着视频风格化领域的重大进展。", "summary": "本文提出了V-Stylist，一个基于多模态大语言模型（MLLM）协作与反思的多智能体系统，旨在解决现有视频风格化方法在处理复杂视频转场和开放式用户风格描述方面的不足。V-Stylist包含视频解析器、风格解析器和风格艺术家三个核心模块，分别负责视频分解、风格识别与匹配、以及风格渲染与细节调整。该系统通过模仿人类专业流程，有效应对了自动视频风格化的主要挑战。此外，研究团队还构建了新的文本驱动视频风格化基准（TVSBench）。实验结果表明，V-Stylist在各项指标上均超越了现有SOTA方法，实现了显著的性能提升。", "keywords": "视频风格化, 多智能体系统, 多模态大语言模型, 协作反思, 文本驱动", "comments": "V-Stylist通过引入多模态大语言模型的协作与反思范式，为视频风格化领域带来了创新。其多智能体架构模仿了人类专业工作流，有效解决了复杂视频转场和开放式风格描述的难题，展现了MLLM在复杂任务协调中的潜力。同时，构建新的基准测试TVSBench也填补了该领域评估工具的空白，对后续研究具有重要意义。"}}
{"id": "2503.12086", "pdf": "https://arxiv.org/pdf/2503.12086", "abs": "https://arxiv.org/abs/2503.12086", "authors": ["Rui Qian", "Chenyangguang Zhang", "Yan Di", "Guangyao Zhai", "Ruida Zhang", "Jiayu Guo", "Benjamin Busam", "Jian Pu"], "title": "FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have exhibited highly effective performance for\nphotorealistic novel view synthesis recently. However, the key limitation it\nmeets is the reliance on a hand-crafted frequency annealing strategy to recover\n3D scenes with imperfect camera poses. The strategy exploits a temporal\nlow-pass filter to guarantee convergence while decelerating the joint\noptimization of implicit scene reconstruction and camera registration. In this\nwork, we introduce the Frequency Adapted Bundle Adjusting Radiance Field\n(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted\nspatial low-pass filter to address the decelerating problem. We establish a\ntheoretical framework to interpret the relationship between position encoding\nof NeRF and camera registration and show that our frequency-adapted filter can\nmitigate frequency fluctuation caused by the temporal filter. Furthermore, we\nshow that applying a spatial low-pass filter in NeRF can optimize camera poses\nproductively through radial uncertainty overlaps among various views. Extensive\nexperiments show that FA-BARF can accelerate the joint optimization process\nunder little perturbations in object-centric scenes and recover real-world\nscenes with unknown camera poses. This implies wider possibilities for NeRF\napplied in dense 3D mapping and reconstruction under real-time requirements.\nThe code will be released upon paper acceptance.", "AI": {"title_translation": "FA-BARF：频率自适应束调整神经辐射场", "tldr": "FA-BARF通过引入频率自适应空间低通滤波器来解决NeRF在不完美相机姿态下进行场景重建和相机配准联合优化时的速度减慢问题，并提高了其在密集3D建图和重建中的应用潜力。", "motivation": "NeRF在照片级真实感新视角合成方面表现出色，但其主要限制在于依赖手工设计的频率退火策略来恢复具有不完美相机姿态的3D场景。这种策略使用时间低通滤波器，在保证收敛的同时减慢了隐式场景重建和相机配准的联合优化过程。", "method": "本文引入了频率自适应束调整辐射场（FA-BARF），用频率自适应空间低通滤波器替代了时间低通滤波器，以解决速度减慢问题。研究建立了理论框架来解释NeRF的位置编码与相机配准之间的关系，并表明频率自适应滤波器可以减轻时间滤波器引起的频率波动。此外，通过径向不确定性在不同视图间的重叠，证明了在NeRF中应用空间低通滤波器可以有效地优化相机姿态。", "result": "实验结果表明，FA-BARF可以在以物体为中心的场景中，在微小扰动下加速联合优化过程，并且能够恢复相机姿态未知的真实世界场景。", "conclusion": "FA-BARF通过解决NeRF在不完美相机姿态下的联合优化速度问题，为NeRF在实时密集3D建图和重建中的应用提供了更广阔的可能性。", "translation": "神经辐射场（NeRF）最近在照片级真实感新视角合成方面表现出高效的性能。然而，其主要限制在于依赖手工设计的频率退火策略来恢复具有不完美相机姿态的3D场景。该策略利用时间低通滤波器来保证收敛，同时减缓了隐式场景重建和相机配准的联合优化。在这项工作中，我们引入了频率自适应束调整辐射场（FA-BARF），用频率自适应空间低通滤波器替代了时间低通滤波器，以解决减速问题。我们建立了一个理论框架来解释NeRF的位置编码与相机配准之间的关系，并表明我们的频率自适应滤波器可以减轻时间滤波器引起的频率波动。此外，我们表明在NeRF中应用空间低通滤波器可以通过各种视图之间的径向不确定性重叠有效地优化相机姿态。大量的实验表明，FA-BARF可以在以物体为中心的场景中，在微小扰动下加速联合优化过程，并恢复相机姿态未知的真实世界场景。这意味着NeRF在满足实时要求的密集3D建图和重建方面具有更广阔的可能性。代码将在论文接收后发布。", "summary": "本文提出了FA-BARF，旨在解决NeRF在不完美相机姿态下进行3D场景重建和相机配准时因依赖时间低通滤波器而导致的联合优化速度减慢问题。FA-BARF用频率自适应空间低通滤波器取代了传统的时间滤波器，并通过理论分析证明了其能减轻频率波动并有效优化相机姿态。实验证明，FA-BARF能加速优化过程，并在未知相机姿态的真实世界场景中表现良好，拓宽了NeRF在实时3D建图和重建领域的应用。", "keywords": "NeRF, 神经辐射场, 相机姿态估计, 频率适应, 束调整", "comments": "FA-BARF的创新之处在于用频率自适应的空间低通滤波器替代了传统的时间低通滤波器，巧妙地解决了NeRF在不完美相机姿态下联合优化速度慢的问题。通过理论分析和实验验证，该方法不仅提高了效率，也增强了NeRF在实际应用中的鲁棒性，特别是在需要实时3D建图和重建的场景中，具有重要的实际意义。"}}
{"id": "2503.12087", "pdf": "https://arxiv.org/pdf/2503.12087", "abs": "https://arxiv.org/abs/2503.12087", "authors": ["Gino E. Jansen", "Mark J. Schuuring", "Berto J. Bouma", "Ivana Išgum"], "title": "Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel approach to achieving temporally consistent mitral\nannulus landmark localization in echocardiography videos using sparse\nannotations. Our method introduces a self-supervised loss term that enforces\ntemporal consistency between neighboring frames, which smooths the position of\nlandmarks and enhances measurement accuracy over time. Additionally, we\nincorporate realistic field-of-view augmentations to improve the recognition of\nmissing anatomical landmarks. We evaluate our approach on both a public and\nprivate dataset, and demonstrate significant improvements in Mitral Annular\nPlane Systolic Excursion (MAPSE) calculations and overall landmark tracking\nstability. The method achieves a mean absolute MAPSE error of 1.81 $\\pm$ 0.14\nmm, an annulus size error of 2.46 $\\pm$ 0.31 mm, and a landmark localization\nerror of 2.48 $\\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for\nrecognition of missing landmarks.", "AI": {"title_translation": "超声心动图视频中稀疏标注的心脏二尖瓣环时间一致性测量", "tldr": "本文提出了一种新方法，利用自监督损失和视野增强，从稀疏标注中实现超声心动图视频中二尖瓣环地标的时间一致性定位，显著提高了测量精度和跟踪稳定性。", "motivation": "在超声心动图视频中，需要实现时间一致性的二尖瓣环地标定位，以提高测量精度和可靠性。", "method": "本方法引入了一个自监督损失项，用于在相邻帧之间强制执行时间一致性，从而平滑地标位置并提高测量精度。此外，还结合了逼真的视野增强，以改善对缺失解剖地标的识别。", "result": "该方法在公共和私人数据集上进行了评估，结果显示在二尖瓣环平面收缩期位移（MAPSE）计算和整体地标跟踪稳定性方面有显著改进。实现了1.81 ± 0.14毫米的平均绝对MAPSE误差，2.46 ± 0.31毫米的环形尺寸误差，以及2.48 ± 0.07毫米的地标定位误差。最后，在识别缺失地标方面实现了0.99的ROC-AUC。", "conclusion": "本文提出的方法通过引入自监督损失和视野增强，成功实现了超声心动图视频中二尖瓣环地标的时间一致性定位，显著提高了关键心脏测量的准确性和稳定性。", "translation": "这项工作提出了一种新颖的方法，利用稀疏标注在超声心动图视频中实现时间一致性的二尖瓣环地标定位。我们的方法引入了一个自监督损失项，该项强制执行相邻帧之间的时间一致性，从而平滑地标位置并提高随时间推移的测量精度。此外，我们结合了逼真的视野增强，以改善对缺失解剖地标的识别。我们在公共和私人数据集上评估了我们的方法，并证明了在二尖瓣环平面收缩期位移（MAPSE）计算和整体地标跟踪稳定性方面的显著改进。该方法实现了1.81 ± 0.14毫米的平均绝对MAPSE误差，2.46 ± 0.31毫米的环形尺寸误差，以及2.48 ± 0.07毫米的地标定位误差。最后，它在识别缺失地标方面实现了0.99的ROC-AUC。", "summary": "本文提出了一种用于超声心动图视频中二尖瓣环地标定位的新方法，旨在解决时间一致性问题。通过引入自监督损失项来确保相邻帧间的时间一致性，并结合视野增强技术以提高对缺失地标的识别能力。该方法在两个数据集上均表现出色，显著提高了MAPSE计算和地标跟踪的稳定性，并给出了具体的测量误差数据，证明了其在心脏测量方面的准确性和有效性。", "keywords": "二尖瓣环, 超声心动图, 时间一致性, 自监督学习, 地标定位", "comments": "本文的创新点在于引入了自监督损失项来强制执行时间一致性，以及结合了实际视野增强来处理缺失地标识别。这对于医学影像分析，特别是超声心动图的动态测量，具有重要意义。通过提高时间一致性和测量精度，可以为临床诊断提供更可靠的数据。"}}
{"id": "2503.12093", "pdf": "https://arxiv.org/pdf/2503.12093", "abs": "https://arxiv.org/abs/2503.12093", "authors": ["Oren Shrout", "Ayellet Tal"], "title": "SFMNet: Sparse Focal Modulation for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We propose SFMNet, a novel 3D sparse detector that combines the efficiency of\nsparse convolutions with the ability to model long-range dependencies. While\ntraditional sparse convolution techniques efficiently capture local structures,\nthey struggle with modeling long-range relationships. However, capturing\nlong-range dependencies is fundamental for 3D object detection. In contrast,\ntransformers are designed to capture these long-range dependencies through\nattention mechanisms. But, they come with high computational costs, due to\ntheir quadratic query-key-value interactions. Furthermore, directly applying\nattention to non-empty voxels is inefficient due to the sparse nature of 3D\nscenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,\nwhich integrates short- and long-range contexts with linear complexity by\nleveraging a new hierarchical sparse convolution design. This approach enables\nSFMNet to achieve high detection performance with improved efficiency, making\nit well-suited for large-scale LiDAR scenes. We show that our detector achieves\nstate-of-the-art performance on autonomous driving datasets.", "AI": {"title_translation": "SFMNet: 稀疏焦点调制用于三维目标检测", "tldr": "SFMNet提出了一种结合稀疏卷积效率和建模长程依赖能力的新型三维稀疏检测器，通过稀疏焦点调制模块在线性复杂度下实现了SOTA性能。", "motivation": "传统稀疏卷积难以建模长程依赖关系，而这对于三维目标检测至关重要。Transformer虽然能捕获长程依赖，但计算成本高昂，且直接应用于稀疏三维场景效率低下。", "method": "SFMNet基于新型的稀疏焦点调制（SFM）模块构建，该模块通过利用新的分层稀疏卷积设计，以线性复杂度集成短程和长程上下文。", "result": "SFMNet在自动驾驶数据集上实现了最先进的检测性能，并提高了效率。", "conclusion": "SFMNet成功地结合了稀疏卷积的效率和建模长程依赖的能力，为大规模LiDAR场景中的三维目标检测提供了一种高性能且高效的解决方案。", "translation": "我们提出了SFMNet，一种新型的三维稀疏检测器，它结合了稀疏卷积的效率和建模长程依赖的能力。传统的稀疏卷积技术能有效捕获局部结构，但在建模长程关系方面存在困难。然而，捕获长程依赖对于三维目标检测至关重要。相比之下，Transformer通过注意力机制旨在捕获这些长程依赖。但是，由于其二次方的查询-键-值交互，它们带来了高计算成本。此外，由于三维场景的稀疏性，直接将注意力应用于非空体素效率低下。我们的SFMNet建立在一个新型的稀疏焦点调制（SFM）模块上，该模块通过利用一种新的分层稀疏卷积设计，以线性复杂度集成短程和长程上下文。这种方法使SFMNet能够以更高的效率实现高检测性能，使其非常适合大规模LiDAR场景。我们表明，我们的检测器在自动驾驶数据集上实现了最先进的性能。", "summary": "SFMNet是一种新型的三维稀疏检测器，旨在解决传统稀疏卷积在长程依赖建模上的不足以及Transformer在稀疏三维场景中计算成本高昂的问题。它引入了稀疏焦点调制（SFM）模块，结合分层稀疏卷积，以线性复杂度有效地集成短程和长程上下文。SFMNet在提高效率的同时，在自动驾驶数据集上实现了最先进的三维目标检测性能，特别适用于大规模LiDAR场景。", "keywords": "3D目标检测, 稀疏卷积, 长程依赖, SFMNet, 稀疏焦点调制", "comments": "SFMNet的创新之处在于其稀疏焦点调制（SFM）模块，它有效地结合了稀疏卷积的计算效率和Transformer的长程依赖建模能力，同时避免了Transformer的高昂计算成本和在稀疏数据上的低效性。通过引入分层稀疏卷积设计，它以线性复杂度处理上下文信息，使其在大规模LiDAR场景中具有重要应用价值。该方法提供了一种有效平衡性能和效率的解决方案。"}}
{"id": "2503.12094", "pdf": "https://arxiv.org/pdf/2503.12094", "abs": "https://arxiv.org/abs/2503.12094", "authors": ["Weiming Zhang", "Dingwen Xiao", "Lei Chen", "Lin Wang"], "title": "E-SAM: Training-Free Segment Every Entity Model", "categories": ["cs.CV"], "comment": "Under review", "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities\nwithin an image without the need for predefined class labels. This\ncharacteristic makes ES well-suited to open-world applications with adaptation\nto diverse and dynamically changing environments, where new and previously\nunseen entities may appear frequently. Existing ES methods either require large\nannotated datasets or high training costs, limiting their scalability and\nadaptability. Recently, the Segment Anything Model (SAM), especially in its\nAutomatic Mask Generation (AMG) mode, has shown potential for holistic image\nsegmentation. However, it struggles with over-segmentation and\nunder-segmentation, making it less effective for ES. In this paper, we\nintroduce E-SAM, a novel training-free framework that exhibits exceptional ES\ncapability. Specifically, we first propose Multi-level Mask Generation (MMG)\nthat hierarchically processes SAM's AMG outputs to generate reliable\nobject-level masks while preserving fine details at other levels. Entity-level\nMask Refinement (EMR) then refines these object-level masks into accurate\nentity-level masks. That is, it separates overlapping masks to address the\nredundancy issues inherent in SAM's outputs and merges similar masks by\nevaluating entity-level consistency. Lastly, Under-Segmentation Refinement\n(USR) addresses under-segmentation by generating additional high-confidence\nmasks fused with EMR outputs to produce the final ES map. These three modules\nare seamlessly optimized to achieve the best ES without additional training\noverhead. Extensive experiments demonstrate that E-SAM achieves\nstate-of-the-art performance compared to prior ES methods, demonstrating a\nsignificant improvement by +30.1 on benchmark metrics.", "AI": {"title_translation": "E-SAM：免训练的逐实体分割模型", "tldr": "E-SAM是一个免训练的实体分割框架，通过多级掩码生成、实体级掩码细化和欠分割细化，显著提升了SAM在实体分割上的性能，无需额外训练。", "motivation": "现有实体分割（ES）方法需要大量标注数据或高昂的训练成本，限制了其可扩展性和适应性。Segment Anything Model（SAM）在整体图像分割方面有潜力，但在ES中存在过分割和欠分割问题，效果不佳。", "method": "本文提出了E-SAM，一个新颖的免训练框架，包含三个核心模块：1. 多级掩码生成（MMG）：分层处理SAM的自动掩码生成（AMG）输出，生成可靠的对象级掩码并保留精细细节。2. 实体级掩码细化（EMR）：将对象级掩码细化为准确的实体级掩码，通过分离重叠掩码和合并相似掩码来解决冗余问题。3. 欠分割细化（USR）：生成额外的高置信度掩码，并与EMR输出融合以解决欠分割问题。这三个模块无缝优化，无需额外训练开销。", "result": "E-SAM实现了最先进的性能，在基准指标上相比现有实体分割方法显著提升了+30.1。", "conclusion": "E-SAM通过其创新的免训练多模块设计，有效克服了现有实体分割方法的局限性以及SAM在实体分割中固有的过分割和欠分割问题，在开放世界实体分割应用中展现出卓越的性能和潜力。", "translation": "实体分割（ES）旨在识别和分割图像中的不同实体，而无需预定义的类别标签。这一特性使得ES非常适合开放世界应用，能够适应多样化和动态变化的环境，其中新的和以前未见的实体可能频繁出现。现有的ES方法要么需要大量的标注数据集，要么需要高昂的训练成本，这限制了它们的可扩展性和适应性。最近，Segment Anything Model（SAM），尤其是在其自动掩码生成（AMG）模式下，在整体图像分割方面显示出潜力。然而，它在过分割和欠分割方面存在问题，这使其在ES方面效果不佳。在本文中，我们引入了E-SAM，一个新颖的免训练框架，它展现出卓越的ES能力。具体而言，我们首先提出了多级掩码生成（MMG），它分层处理SAM的AMG输出，以生成可靠的对象级掩码，同时在其他级别保留精细细节。然后，实体级掩码细化（EMR）将这些对象级掩码细化为准确的实体级掩码。也就是说，它通过评估实体级一致性来分离重叠掩码以解决SAM输出中固有的冗余问题，并合并相似掩码。最后，欠分割细化（USR）通过生成额外的与EMR输出融合的高置信度掩码来解决欠分割问题，以生成最终的ES图。这三个模块无缝优化，无需额外的训练开销即可实现最佳ES。广泛的实验表明，与先前的ES方法相比，E-SAM实现了最先进的性能，在基准指标上显著提高了+30.1。", "summary": "本文介绍了E-SAM，一个创新的免训练框架，旨在解决现有实体分割（ES）方法对大量标注数据和高训练成本的依赖，以及Segment Anything Model（SAM）在ES中面临的过分割和欠分割挑战。E-SAM由三个关键模块组成：多级掩码生成（MMG）用于从SAM的输出中提取可靠的对象级掩码；实体级掩码细化（EMR）负责将这些掩码精炼为准确的实体级掩码，处理重叠和相似区域；以及欠分割细化（USR）用于补充遗漏的区域。这些模块协同工作，无需额外训练即可提升ES性能。实验结果证明，E-SAM在ES任务上达到了最先进的水平，并在基准测试中取得了显著的性能提升。", "keywords": "实体分割, 免训练, Segment Anything Model, 多级掩码生成, 掩码细化", "comments": "E-SAM的创新之处在于它是一个完全“免训练”的框架，这极大地降低了实体分割的门槛和成本。它巧妙地利用了SAM的强大生成能力，并通过MMG、EMR和USR三个模块系统地解决了SAM在实体分割中固有的过分割和欠分割问题，使其更适用于开放世界场景。其显著的性能提升证明了这种后处理和精炼策略的有效性。"}}
{"id": "2503.12095", "pdf": "https://arxiv.org/pdf/2503.12095", "abs": "https://arxiv.org/abs/2503.12095", "authors": ["Walter Zimmer", "Ross Greer", "Daniel Lehmberg", "Marc Pavel", "Holger Caesar", "Xingcheng Zhou", "Ahmed Ghita", "Mohan Trivedi", "Rui Song", "Hu Cao", "Akshay Gopalkrishnan", "Alois C. Knoll"], "title": "Towards Vision Zero: The Accid3nD Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Even though a significant amount of work has been done to increase the safety\nof transportation networks, accidents still occur regularly. They must be\nunderstood as unavoidable and sporadic outcomes of traffic networks. No public\ndataset contains 3D annotations of real-world accidents recorded from roadside\nsensors. We present the Accid3nD dataset, a collection of real-world highway\naccidents in different weather and lighting conditions. It contains vehicle\ncrashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,\ninstance masks, and 3D bounding boxes with track IDs. In total, the dataset\ncontains 111,945 labeled frames recorded from four roadside cameras and LiDARs\nat 25 Hz. The dataset contains six object classes and is provided in the\nOpenLABEL format. We propose an accident detection model that combines a\nrule-based approach with a learning-based one. Experiments and ablation studies\non our dataset show the robustness of our proposed method. The dataset, model,\nand code are available on our website: https://accident-dataset.github.io.", "AI": {"title_translation": "迈向零愿景：Accid3nD数据集", "tldr": "该论文介绍了Accid3nD，一个包含路边传感器记录的真实世界高速公路事故3D标注的新公共数据集，并提出了一个事故检测模型。", "motivation": "尽管在提高交通网络安全性方面做了大量工作，事故仍然经常发生。目前缺乏包含路边传感器记录的真实世界事故3D标注的公共数据集。", "method": "研究人员提出了Accid3nD数据集，这是一个收集了在不同天气和光照条件下真实世界高速公路事故的数据集。它包含高速驾驶下的车辆碰撞数据，拥有2,634,233个标注的2D边界框、实例掩码和带跟踪ID的3D边界框。该数据集总共包含111,945帧标注数据，这些数据由四个路边摄像头和激光雷达以25 Hz的频率记录。数据集包含六个对象类别，并以OpenLABEL格式提供。研究人员还提出了一种结合了基于规则方法和基于学习方法的事故检测模型。", "result": "创建了Accid3nD数据集。对该数据集进行的实验和消融研究表明，所提出的事故检测方法具有鲁棒性。", "conclusion": "该论文成功构建了一个包含路边传感器记录的真实世界事故3D标注的Accid3nD数据集，并提出的事故检测模型表现出鲁棒性，有助于推动交通安全研究。", "translation": "尽管在提高交通网络安全性方面做了大量工作，事故仍然经常发生。它们必须被理解为交通网络中不可避免的、零星的结果。目前没有公共数据集包含路边传感器记录的真实世界事故的3D标注。我们提出了Accid3nD数据集，这是一个在不同天气和光照条件下收集的真实世界高速公路事故的集合。它包含高速驾驶下的车辆碰撞数据，拥有2,634,233个标注的2D边界框、实例掩码和带跟踪ID的3D边界框。总的来说，该数据集包含111,945帧标注数据，这些数据由四个路边摄像头和激光雷达以25 Hz的频率记录。数据集包含六个对象类别，并以OpenLABEL格式提供。我们提出了一种结合了基于规则方法和基于学习方法的事故检测模型。对我们的数据集进行的实验和消融研究表明了我们所提出方法的鲁棒性。数据集、模型和代码可在我们的网站上获取：https://accident-dataset.github.io。", "summary": "本论文介绍了Accid3nD数据集，这是首个包含路边传感器记录的真实世界高速公路事故3D标注的公共数据集，旨在填补当前数据空白。该数据集包含在不同天气和光照条件下高速车辆碰撞的详细2D和3D标注，总计超过11万帧。此外，论文还提出了一种结合规则和学习方法的事故检测模型，并通过实验验证了其鲁棒性。数据集、模型和代码均已公开。", "keywords": "Accid3nD, 事故检测, 数据集, 3D标注, 路边传感器", "comments": "该论文的创新之处在于提供了首个包含路边传感器记录的真实世界事故3D标注的公共数据集，这对于开发和测试鲁棒的事故检测系统至关重要。其重要性在于填补了关键的数据空白，为实现“零愿景”的交通安全目标提供了宝贵的资源和研究基础。"}}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks.", "AI": {"title_translation": "O-TPT：视觉-语言模型中测试时提示调优校准的正交性约束", "tldr": "O-TPT通过对文本特征施加正交性约束，有效提升了视觉-语言模型（VLMs）中测试时提示调优的校准性能，显著降低了校准误差并超越了现有方法。", "motivation": "尽管视觉-语言模型（VLMs）的测试时提示调优方法可以提高准确性，但其模型校准性较差，这引发了对模型可靠性和可信度的质疑。因此，需要更多关注来校准VLMs中的测试时提示调优。", "method": "我们提出了一种名为O-TPT的新方法，该方法通过对可学习提示对应的文本特征引入正交性约束来校准VLMs中的测试时提示调优。研究表明，对文本特征施加简单的正交化是获得文本分散的更有效方法。", "result": "我们的方法在各种数据集上，无论使用何种骨干网络和基线，都始终优于现有最先进技术，显著降低了整体平均校准误差。此外，我们的方法在细粒度分类任务中超越了零样本校准性能。", "conclusion": "O-TPT通过引入正交性约束有效地解决了视觉-语言模型中测试时提示调优的校准问题，显著提高了模型的可靠性和性能。", "translation": "视觉-语言模型（VLMs）的测试时提示调优因其在无需微调的情况下利用未标记数据学习的能力而受到关注。尽管VLMs的测试时提示调优方法可以提高准确性，但由此产生的模型往往表现出较差的校准性，这使得这些模型的可靠性和可信度受到质疑。值得注意的是，需要更多关注来校准视觉-语言模型中的测试时提示调优。为此，我们提出了一种名为O-TPT的新方法，该方法对可学习提示对应的文本特征引入正交性约束，以校准VLMs中的测试时提示调优。为了引入正交性约束，我们做出了以下贡献。首先，我们揭示了现有方法依赖于文本特征分散导致次优校准性能背后的新见解。其次，我们表明对文本特征施加简单的正交化是获得文本分散的更有效方法。我们在不同骨干网络和基线的各种数据集上进行了广泛实验。结果表明，我们的方法始终优于现有最先进技术，显著降低了整体平均校准误差。此外，我们的方法在细粒度分类任务中超越了零样本校准性能。", "summary": "O-TPT是一种新颖的方法，通过对视觉-语言模型（VLMs）中可学习提示的文本特征施加正交性约束，显著改善了测试时提示调优的校准问题。该方法解决了现有技术校准性能不佳、影响模型可靠性的问题。实验证明，O-TPT在各种数据集上均能一致地降低平均校准误差，并超越了最先进的校准方法以及在细粒度分类任务中的零样本性能。", "keywords": "O-TPT, 视觉-语言模型, 测试时提示调优, 校准, 正交性约束", "comments": "该论文的创新点在于将正交性约束引入到视觉-语言模型的测试时提示调优中，以解决模型校准性差的问题。通过简单的正交化处理，有效改善了文本特征的分散性，从而提高了模型的可靠性和可信度。这是一个重要且实用的贡献，因为它直接解决了现有方法在实际应用中面临的信任危机。"}}
{"id": "2503.12102", "pdf": "https://arxiv.org/pdf/2503.12102", "abs": "https://arxiv.org/abs/2503.12102", "authors": ["Paula Andrea Pérez-Toro", "Tomás Arias-Vergara", "Fangxu Xing", "Xiaofeng Liu", "Maureen Stone", "Jiachen Zhuo", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "Jana Hutter", "Jerry L. Prince", "Andreas Maier", "Jonghye Woo"], "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the relationship between vocal tract motion during speech and\nthe resulting acoustic signal is crucial for aided clinical assessment and\ndeveloping personalized treatment and rehabilitation strategies. Toward this\ngoal, we introduce an audio-to-video generation framework for creating Real\nTime/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract\nfrom speech signals. Our framework first preprocesses RT-/cine-MRI sequences\nand speech samples to achieve temporal alignment, ensuring synchronization\nbetween visual and audio data. We then employ a modified stable diffusion\nmodel, integrating structural and temporal blocks, to effectively capture\nmovement characteristics and temporal dynamics in the synchronized data. This\nprocess enables the generation of MRI sequences from new speech inputs,\nimproving the conversion of audio into visual data. We evaluated our framework\non healthy controls and tongue cancer patients by analyzing and comparing the\nvocal tract movements in synthesized videos. Our framework demonstrated\nadaptability to new speech inputs and effective generalization. In addition,\npositive human evaluations confirmed its effectiveness, with realistic and\naccurate visualizations, suggesting its potential for outpatient therapy and\npersonalized simulation of vocal tract visualizations.", "AI": {"title_translation": "一种用于声门磁共振成像的基于时空扩散的语音到视频合成方法", "tldr": "本文提出了一种基于时空扩散的语音到视频合成方法，用于从语音信号生成声门磁共振成像（MRI）视频，对临床评估和个性化治疗具有潜在应用价值。", "motivation": "理解言语过程中声门运动与声学信号之间的关系对于辅助临床评估以及制定个性化治疗和康复策略至关重要。目前缺乏有效的工具来实现这一目标。", "method": "该框架首先对实时/电影磁共振成像（RT-/cine-MRI）序列和语音样本进行预处理以实现时间对齐。然后，采用修改后的稳定扩散模型，该模型集成了结构和时间块，以有效地捕获同步数据中的运动特征和时间动态。此过程能够从新的语音输入生成MRI序列。", "result": "该框架在健康受试者和舌癌患者身上进行了评估，通过分析和比较合成视频中的声门运动，证明了其对新语音输入的适应性和有效的泛化能力。此外，积极的人工评估证实了其有效性，生成了真实准确的可视化。", "conclusion": "该语音到视频合成方法能够从语音生成真实准确的声门可视化，具有在门诊治疗和声门可视化个性化模拟方面的巨大潜力。", "translation": "了解言语过程中声门运动与由此产生的声学信号之间的关系对于辅助临床评估以及制定个性化治疗和康复策略至关重要。为此，我们引入了一种音频到视频生成框架，用于从语音信号创建声门的实时/电影磁共振成像（RT-/cine-MRI）视觉效果。我们的框架首先预处理RT-/cine-MRI序列和语音样本以实现时间对齐，确保视觉和音频数据之间的同步。然后，我们采用修改后的稳定扩散模型，该模型集成了结构和时间块，以有效地捕获同步数据中的运动特征和时间动态。此过程能够从新的语音输入生成MRI序列，从而改善音频到视觉数据的转换。我们通过分析和比较合成视频中的声门运动，在健康对照组和舌癌患者身上评估了我们的框架。我们的框架展示了对新语音输入的适应性和有效的泛化能力。此外，积极的人工评估证实了其有效性，生成了真实准确的可视化，表明其在门诊治疗和声门可视化个性化模拟方面的潜力。", "summary": "本文提出了一种创新的音频到视频生成框架，该框架利用修改后的时空扩散模型，能够从语音信号合成声门的实时磁共振成像（MRI）视频。该方法首先对RT-/cine-MRI序列和语音样本进行时间对齐预处理，随后通过集成结构和时间块的稳定扩散模型，捕捉并生成新的语音输入对应的MRI序列。该框架在健康受试者和舌癌患者中进行了评估，结果显示其对新语音输入具有良好的适应性和有效的泛化能力。此外，人工评估也证实了其生成的可视化效果真实且准确，这表明该技术在临床评估、个性化治疗和康复策略中具有重要的应用潜力。", "keywords": "语音到视频合成, 声门MRI, 时空扩散, 语音分析, 磁共振成像", "comments": "该论文的创新之处在于将修改后的稳定扩散模型应用于语音到声门MRI视频的合成，这在医学成像和语音分析领域是一个新颖的应用。它为理解声门运动与语音的关系提供了一种非侵入性且动态的可视化工具。其在个性化治疗和临床评估方面的潜力是其重要性所在，特别是对于舌癌患者的评估和康复。"}}
{"id": "2503.12124", "pdf": "https://arxiv.org/pdf/2503.12124", "abs": "https://arxiv.org/abs/2503.12124", "authors": ["Yingying Deng", "Xiangyu He", "Fan Tang", "Weiming Dong"], "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "The customization of multiple attributes has gained popularity with the\nrising demand for personalized content creation. Despite promising empirical\nresults, the contextual coherence between different attributes has been largely\noverlooked. In this paper, we argue that subsequent attributes should follow\nthe multivariable conditional distribution introduced by former attribute\ncreation. In light of this, we reformulate multi-attribute creation from a\nconditional probability theory perspective and tackle the challenging zero-shot\nsetting. By explicitly modeling the dependencies between attributes, we further\nenhance the coherence of generated images across diverse attribute\ncombinations. Furthermore, we identify connections between multi-attribute\ncustomization and multi-task learning, effectively addressing the high\ncomputing cost encountered in multi-attribute synthesis. Extensive experiments\ndemonstrate that Z-Magic outperforms existing models in zero-shot image\ngeneration, with broad implications for AI-driven design and creative\napplications.", "AI": {"title_translation": "Z-Magic：零样本多属性引导图像生成器", "tldr": "Z-Magic是一种新的零样本多属性引导图像生成器，通过建模属性间的依赖关系并结合多任务学习，解决了现有模型中属性间上下文连贯性不足和计算成本高的问题，在零样本图像生成方面表现优异。", "motivation": "随着个性化内容创作需求的增长，多属性定制变得流行，但现有方法在不同属性之间的上下文连贯性方面被很大程度地忽视了。", "method": "本文从条件概率论的角度重新构建了多属性创建问题，并处理具有挑战性的零样本设置。通过明确建模属性之间的依赖关系，增强了生成图像在不同属性组合下的连贯性。此外，将多属性定制与多任务学习联系起来，有效解决了多属性合成中遇到的高计算成本问题。", "result": "Z-Magic在零样本图像生成方面超越了现有模型。", "conclusion": "Z-Magic在零样本图像生成方面的卓越表现，对AI驱动的设计和创意应用具有广泛的影响。", "translation": "Z-Magic：零样本多属性引导图像生成器\n\n随着个性化内容创作需求的不断增长，多属性定制变得越来越受欢迎。尽管取得了有希望的经验结果，但不同属性之间的上下文连贯性却在很大程度上被忽视了。在本文中，我们认为后续属性应遵循由前一个属性创建引入的多变量条件分布。鉴于此，我们从条件概率论的角度重新构建了多属性创建问题，并解决了具有挑战性的零样本设置。通过明确建模属性之间的依赖关系，我们进一步增强了生成图像在不同属性组合下的连贯性。此外，我们还确定了多属性定制与多任务学习之间的联系，有效地解决了多属性合成中遇到的高计算成本问题。大量的实验表明，Z-Magic在零样本图像生成方面优于现有模型，对AI驱动的设计和创意应用具有广泛的影响。", "summary": "Z-Magic是一种用于零样本多属性引导图像生成的新模型。它解决了现有方法中多属性图像生成时属性间上下文连贯性不足的问题，通过将多属性创建重新定义为基于条件概率论的问题，并明确建模属性间的依赖关系。同时，该模型通过与多任务学习的结合，有效降低了计算成本。实验证明Z-Magic在零样本图像生成方面超越了现有模型，具有广泛的应用前景。", "keywords": "零样本图像生成, 多属性定制, 条件概率, 多任务学习, 图像生成器", "comments": "该论文的创新点在于提出了Z-Magic模型，它不仅关注多属性图像生成中的零样本设置，更重要的是通过引入条件概率理论来解决属性间的上下文连贯性问题，这是一个此前被忽视的关键点。同时，将多属性定制与多任务学习相结合，有效优化了计算效率，提升了模型的实用性。这对于AI驱动的设计和创意应用具有重要意义。"}}
{"id": "2503.12127", "pdf": "https://arxiv.org/pdf/2503.12127", "abs": "https://arxiv.org/abs/2503.12127", "authors": ["Tobia Poppi", "Tejaswi Kasarla", "Pascal Mettes", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Hyperbolic Safety-Aware Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "CVPR 2025", "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.", "AI": {"title_translation": "双曲安全感知视觉-语言模型", "tldr": "本研究提出了一种名为HySAC的新方法，利用双曲空间对视觉-语言模型（如CLIP）进行安全内容感知，通过将安全和不安全内容编码为蕴涵层次结构，并使用蕴涵损失函数，从而在不牺牲模型识别能力的情况下，提升了不安全内容的识别和内容审核的灵活性。", "motivation": "当前视觉-语言模型（如CLIP）在检索不安全内容时，主要依赖“遗忘”技术来擦除模型对不安全概念的知识。然而，这种方法虽然能减少不良输出，却限制了模型区分安全与不安全内容的能力，阻碍了其在现实世界中的整合。", "method": "本研究提出了一种新颖的“感知”范式，而非传统的“遗忘”技术。该方法利用双曲空间固有的层次特性，将安全和不安全内容编码为蕴涵层次结构，并将它们置于双曲空间的不同区域。所提出的HySAC（Hyperbolic Safety-Aware CLIP）模型采用蕴涵损失函数来建模安全与不安全图像-文本对之间的层次和不对称关系，克服了标准视觉-语言模型中欧几里得嵌入的局限性。", "result": "广泛的实验表明，本方法不仅增强了安全性识别能力，还为视觉-语言模型中的内容审核建立了一个更具适应性和可解释性的框架。该模型能够充当多模态不安全内容分类器和灵活的内容检索器，并能动态地将不安全查询重定向到更安全的替代方案或保留原始输出。", "conclusion": "通过利用双曲空间和蕴涵层次结构，HySAC成功地将视觉-语言模型从简单的“遗忘”模式提升到“安全感知”模式，有效解决了不安全内容检索问题，并提供了一个更灵活、可解释的内容审核框架，显著提升了模型区分安全与不安全内容的能力。", "translation": "解决从视觉-语言模型（如CLIP）中检索不安全内容的问题是实现其在现实世界中整合的重要一步。目前的工作依赖于“遗忘”技术，试图擦除模型对不安全概念的知识。虽然这种方法在减少不必要输出方面有效，但它限制了模型区分安全和不安全内容的能力。在这项工作中，我们引入了一种新颖的方法，通过利用双曲空间固有的层次特性，将范式从“遗忘”转向“感知”。我们建议将安全和不安全内容编码为蕴涵层次结构，两者都被放置在双曲空间的不同区域。我们的HySAC（Hyperbolic Safety-Aware CLIP）采用蕴涵损失函数来建模安全和不安全图像-文本对之间的层次和不对称关系。这种建模在标准视觉-语言模型中由于其依赖欧几里得嵌入而无效，但它赋予了模型对不安全内容的感知能力，使其能够作为多模态不安全分类器和灵活的内容检索器，并可以选择动态地将不安全查询重定向到更安全的替代方案或保留原始输出。广泛的实验表明，我们的方法不仅增强了安全性识别，还为视觉-语言模型中的内容审核建立了一个更具适应性和可解释性的框架。我们的源代码可在https://github.com/aimagelab/HySAC获取。", "summary": "本文提出了一种名为HySAC（Hyperbolic Safety-Aware CLIP）的新型视觉-语言模型安全内容感知方法。针对当前“遗忘”技术在处理不安全内容时限制模型区分能力的不足，HySAC利用双曲空间的层次特性，将安全与不安全内容编码为蕴涵层次结构，并通过蕴涵损失函数来建模它们之间的不对称关系。实验证明，该方法不仅提高了安全性识别能力，还提供了一个更灵活、可解释的内容审核框架，使模型能够有效识别和处理不安全内容，同时保持内容检索的灵活性。", "keywords": "双曲空间, 安全感知, 视觉-语言模型, 内容审核, CLIP", "comments": "本文提出了一种创新性的方法，将传统的“遗忘”范式转变为“感知”范式，以处理视觉-语言模型中的不安全内容。其核心创新在于利用双曲空间来建模安全与不安全内容之间的层次关系，这克服了欧几里得嵌入的局限性，使得模型能够更精细地识别和区分内容。这种方法不仅提升了安全性识别的准确性，还增强了模型在内容审核方面的灵活性和可解释性，为构建更安全、更负责任的AI系统提供了有价值的思路。"}}
{"id": "2503.12131", "pdf": "https://arxiv.org/pdf/2503.12131", "abs": "https://arxiv.org/abs/2503.12131", "authors": ["Shentong Mo", "Zehua Chen", "Fan Bao", "Jun Zhu"], "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities.", "AI": {"title_translation": "DiffGAP：一种在对比空间中弥合跨模态鸿沟的轻量级扩散模块", "tldr": "DiffGAP引入了一个轻量级扩散模块，通过双向去噪过程，在对比空间中有效弥合跨模态鸿沟，显著提升了跨模态理解和生成性能。", "motivation": "现有跨模态理解和生成方法（如CLAP和CAVP）虽通过单一对比损失增强了文本、视频和音频嵌入的对齐，但它们忽视了各模态中固有的双向交互和噪声，这些因素会严重影响跨模态集成的质量和效率。", "method": "我们提出了DiffGAP，一种在对比空间中包含轻量级生成模块的新方法。DiffGAP采用双向扩散过程，对文本和视频嵌入进行基于音频嵌入的去噪，反之亦然，从而促进更细致和鲁棒的跨模态交互。", "result": "在VGGSound和AudioCaps数据集上的实验结果表明，DiffGAP显著提高了视频/文本-音频生成和检索任务的性能。", "conclusion": "DiffGAP通过其新颖的双向扩散机制，有效增强了跨模态理解和生成能力，弥合了现有方法中的模态间隙。", "translation": "在跨模态理解和生成方面的最新工作，特别是通过CLAP（对比语言-音频预训练）和CAVP（对比音频-视觉预训练）等模型，通过单一对比损失显著增强了文本、视频和音频嵌入的对齐。然而，这些方法常常忽视了每种模态中存在的双向交互和固有噪声，这些因素可能严重影响跨模态集成的质量和效率。为了解决这一限制，我们引入了DiffGAP，这是一种在对比空间中结合轻量级生成模块的新方法。具体而言，我们的DiffGAP采用量身定制的双向扩散过程，以更有效地弥合跨模态鸿沟。这涉及对文本和视频嵌入进行基于音频嵌入的去噪过程，反之亦然，从而促进更细致和鲁棒的跨模态交互。我们在VGGSound和AudioCaps数据集上的实验结果表明，DiffGAP显著提高了视频/文本-音频生成和检索任务的性能，证实了其在增强跨模态理解和生成能力方面的有效性。", "summary": "本文提出了DiffGAP，一种在对比空间中引入轻量级扩散模块的新方法，旨在解决现有跨模态模型（如CLAP和CAVP）忽视模态间双向交互和噪声的问题。DiffGAP采用双向扩散过程，对多模态嵌入进行条件去噪，从而实现更精细和鲁棒的跨模态集成。实验证明，DiffGAP在VGGSound和AudioCaps数据集上的视频/文本-音频生成和检索任务中表现出显著的性能提升。", "keywords": "跨模态, 扩散模型, 对比学习, 去噪, 多模态集成", "comments": "DiffGAP的创新之处在于将轻量级扩散模块引入对比空间，并通过双向去噪过程处理模态间的噪声和交互，这对于提升跨模态任务的鲁棒性和细致性具有重要意义。其方法的通用性也值得关注。"}}
{"id": "2503.12150", "pdf": "https://arxiv.org/pdf/2503.12150", "abs": "https://arxiv.org/abs/2503.12150", "authors": ["Hongyu Sun", "Qiuhong Ke", "Ming Cheng", "Yongcai Wang", "Deying Li", "Chenhui Gou", "Jianfei Cai"], "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables", "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.", "AI": {"title_translation": "Point-Cache：用于鲁棒和泛化点云分析的测试时动态分层缓存", "tldr": "本文提出了Point-Cache，一个训练无关的动态分层缓存模型，用于在测试时处理点云识别模型的分布偏移，并支持识别已知和未知类别的点云，实现了高效且显著的性能提升。", "motivation": "为了解决现有方法在处理点云识别模型测试时分布偏移的局限性（依赖训练数据、无法识别新类别），并探索一种更实用、更具挑战性的场景：仅基于在线测试数据来适应模型，以识别之前见过的类别和新的、未见过的类别。", "method": "本文开发了Point-Cache，一个分层缓存模型，它捕获在线测试样本的关键线索，特别是点云的全局结构及其局部细节。Point-Cache作为一个丰富的3D知识库，被动态管理以优先包含高质量样本。它被设计成一个即插即用模块，可以灵活地集成到大型多模态3D模型中，以支持开放词汇点云识别。", "result": "Point-Cache在8个具有挑战性的基准测试和4个代表性的大型3D模型上表现出显著的性能提升。其效率与零样本推理相当，因为它完全是训练无关的。", "conclusion": "Point-Cache提供了一个通用且训练无关的解决方案，能够使点云识别模型在测试时有效处理分布偏移，并支持对已知及未知类别的识别，展现出强大的鲁棒性和泛化能力。", "translation": "本文提出了一种通用解决方案，旨在使点云识别模型在测试时能够处理分布偏移。与以往严重依赖训练数据（在线推理期间通常无法获取）且仅限于识别训练期间预定义的固定点云类别的方法不同，我们探索了一种更实用、更具挑战性的场景：仅基于在线测试数据来适应模型，以在测试时识别之前见过的类别和新的、未见过的类别。为此，我们开发了Point-Cache，一个分层缓存模型，它捕获在线测试样本的关键线索，特别关注点云的全局结构及其局部细节。Point-Cache作为一个丰富的3D知识库，被动态管理以优先包含高质量样本。我们的方法被设计为一个即插即用模块，可以灵活地集成到大型多模态3D模型中，以支持开放词汇点云识别。值得注意的是，我们的解决方案操作效率与零样本推理相当，因为它完全是训练无关的。Point-Cache在8个具有挑战性的基准测试和4个代表性的大型3D模型上表现出显著的性能提升，突出了其有效性。代码可在 https://github.com/auniquesun/Point-Cache 获取。", "summary": "本文提出Point-Cache，一个训练无关的动态分层缓存模型，旨在解决点云识别模型在测试时面临的分布偏移问题。它通过捕获在线测试样本的全局结构和局部细节，构建一个动态管理的3D知识库，并以即插即用模块的形式集成到大型3D模型中，支持开放词汇识别。该方法无需训练，效率高，并在多个基准测试和模型上展现出显著的性能提升。", "keywords": "点云分析, 分布偏移, 动态缓存, 开放词汇识别, 训练无关", "comments": "Point-Cache的创新点在于其完全训练无关的设计，使其能够高效地适应在线测试数据并处理分布偏移，同时支持开放词汇识别。其即插即用的模块化设计也增强了其实用性。这对于实际应用中数据分布变化和新类别识别具有重要意义。"}}
{"id": "2503.12165", "pdf": "https://arxiv.org/pdf/2503.12165", "abs": "https://arxiv.org/abs/2503.12165", "authors": ["Zijian He", "Yuwei Ning", "Yipeng Qin", "Wangrun Wang", "Sibei Yang", "Liang Lin", "Guanbin Li"], "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Virtual Try-On (VTON) is a transformative technology in e-commerce and\nfashion design, enabling realistic digital visualization of clothing on\nindividuals. In this work, we propose VTON 360, a novel 3D VTON method that\naddresses the open challenge of achieving high-fidelity VTON that supports\nany-view rendering. Specifically, we leverage the equivalence between a 3D\nmodel and its rendered multi-view 2D images, and reformulate 3D VTON as an\nextension of 2D VTON that ensures 3D consistent results across multiple views.\nTo achieve this, we extend 2D VTON models to include multi-view garments and\nclothing-agnostic human body images as input, and propose several novel\ntechniques to enhance them, including: i) a pseudo-3D pose representation using\nnormal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial\nattention mechanism that models the correlations between features from\ndifferent viewing angles, and iii) a multi-view CLIP embedding that enhances\nthe garment CLIP features used in 2D VTON with camera information. Extensive\nexperiments on large-scale real datasets and clothing images from e-commerce\nplatforms demonstrate the effectiveness of our approach. Project page:\nhttps://scnuhealthy.github.io/VTON360.", "AI": {"title_translation": "VTON 360：任意视角下的高保真虚拟试穿", "tldr": "VTON 360是一种新颖的3D虚拟试穿方法，通过将3D VTON重新定义为2D VTON的扩展，并引入多视角输入和新颖技术，实现了任意视角下高保真的虚拟试穿。", "motivation": "虚拟试穿（VTON）是电子商务和时尚设计中的一项变革性技术，但现有方法在实现支持任意视角渲染的高保真VTON方面仍面临挑战。", "method": "VTON 360将3D VTON重新定义为2D VTON的扩展，确保多视角结果的3D一致性。该方法将多视角服装和服装无关的人体图像作为输入，并引入了多项新颖技术：i) 使用SMPL-X 3D人体模型导出的法线图进行伪3D姿态表示；ii) 多视角空间注意力机制，建模不同视角特征之间的相关性；iii) 多视角CLIP嵌入，通过相机信息增强服装CLIP特征。", "result": "在大型真实数据集和电子商务平台的服装图像上进行的广泛实验证明了该方法的有效性。", "conclusion": "VTON 360通过其创新的3D VTON方法和多视角技术，成功解决了任意视角下高保真虚拟试穿的挑战，并取得了显著效果。", "translation": "虚拟试穿（VTON）是电子商务和时尚设计中的一项变革性技术，能够实现服装在个体身上的逼真数字可视化。在这项工作中，我们提出了VTON 360，一种新颖的3D VTON方法，解决了实现支持任意视角渲染的高保真VTON这一开放性挑战。具体来说，我们利用3D模型及其渲染的多视角2D图像之间的等效性，并将3D VTON重新定义为2D VTON的扩展，从而确保多视角结果的3D一致性。为了实现这一点，我们扩展了2D VTON模型，使其包含多视角服装和服装无关的人体图像作为输入，并提出了几项新颖的技术来增强它们，包括：i）使用从SMPL-X 3D人体模型导出的法线图进行伪3D姿态表示；ii）多视角空间注意力机制，建模来自不同视角的特征之间的相关性；以及iii）多视角CLIP嵌入，通过相机信息增强2D VTON中使用的服装CLIP特征。在大型真实数据集和电子商务平台服装图像上的广泛实验证明了我们方法的有效性。项目页面：https://scnuhealthy.github.io/VTON360。", "summary": "本文提出了VTON 360，一种新颖的3D虚拟试穿（VTON）方法，旨在解决任意视角下高保真VTON的开放性挑战。VTON 360将3D VTON重新定义为2D VTON的扩展，确保多视角结果的3D一致性。它将多视角服装和服装无关的人体图像作为输入，并引入了伪3D姿态表示、多视角空间注意力机制和多视角CLIP嵌入等创新技术。在大型真实数据集上的实验证明了该方法的有效性。", "keywords": "虚拟试穿, 3D VTON, 多视角渲染, 高保真, 深度学习", "comments": "VTON 360的创新之处在于将3D VTON问题巧妙地转化为2D VTON的扩展，并通过引入伪3D姿态、多视角注意力及CLIP嵌入等技术，有效地解决了多视角一致性和高保真渲染的难题，对于电子商务和时尚行业的虚拟试穿应用具有重要意义。"}}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettré", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.", "AI": {"title_translation": "将极高密度人群学习为活性物质", "tldr": "提出一种将高密度人群建模为活性物质并结合神经网络的神经随机微分方程系统，以解决视频中极高密度人群分析和预测的难题，并显示出优越的性能和可解释性。", "motivation": "视频中高密度人群的分析和预测是一个长期存在的计算机视觉难题，由于高质量数据缺乏和复杂的人群动态，该领域研究相对不足。", "method": "提出一种新方法，旨在从低质量的野外视频中学习。核心创新是将高密度人群建模为“活性物质”（一种受随机力作用的连续体，命名为“人群材料”），并将其与神经网络结合，形成一个神经随机微分方程系统，以模拟复杂的人群动态。由于缺乏类似研究，作者调整了一系列现有方法进行比较。", "result": "通过详尽的评估，该模型在分析和预测极高密度人群方面优于现有方法。", "conclusion": "该模型作为连续时间物理模型，具有强大的可解释性，可用于模拟和分析，这与大多数离散时间、黑盒式的深度学习方法截然不同。", "translation": "基于视频的高密度人群分析和预测一直是计算机视觉领域的一个长期课题。由于高质量数据的缺乏和复杂的人群动态等原因，它出了名的困难，因此研究相对不足。在本文中，我们提出了一种新方法，旨在从野外视频中学习，这些视频通常质量较低，难以跟踪个体或计数人头。其关键新颖之处在于引入了一种新的物理先验来建模人群动态。我们将高密度人群建模为活性物质，一个带有受随机力作用的活性粒子的连续体，命名为“人群材料”。我们的物理模型与神经网络相结合，形成了一个神经随机微分方程系统，可以模仿复杂的人群动态。由于缺乏类似的研究，我们调整了一系列与我们方法接近的现有方法进行比较。通过详尽的评估，我们表明我们的模型在分析和预测极高密度人群方面优于现有方法。此外，由于我们的模型是一个连续时间物理模型，它可以用于模拟和分析，提供强大的可解释性。这与大多数离散时间模型和黑盒式的深度学习方法截然不同。", "summary": "本文提出一种新颖的方法来解决视频中极高密度人群的分析和预测难题。通过将高密度人群建模为“活性物质”并结合神经网络，构建了一个神经随机微分方程系统，能够有效模拟复杂的人群动态。该模型在低质量野外视频上表现出色，并在分析和预测极高密度人群方面优于现有方法。其连续时间物理模型的特性还赋予了强大的可解释性，可用于模拟和分析。", "keywords": "高密度人群, 活性物质, 神经随机微分方程, 人群动态, 视频分析", "comments": "这篇论文的创新点在于将物理学中的“活性物质”概念引入到高密度人群建模中，并与神经网络相结合，形成一个具有可解释性的神经随机微分方程系统。这提供了一种与传统深度学习黑盒模型不同的解决方案，尤其是在数据质量不高的情况下，物理先验的引入有助于更好地理解和预测复杂的人群行为。这种结合物理模型和深度学习的方法为解决计算机视觉中的复杂动态问题开辟了新的途径。"}}
{"id": "2503.12173", "pdf": "https://arxiv.org/pdf/2503.12173", "abs": "https://arxiv.org/abs/2503.12173", "authors": ["Yuchen Deng", "Haibin Ling", "Bingyao Huang"], "title": "LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization", "categories": ["cs.CV", "cs.MM"], "comment": "12 pages, 9 figures", "summary": "We propose LAPIG, a language guided projector image generation method with\nsurface adaptation and stylization. LAPIG consists of a projector-camera system\nand a target textured projection surface. LAPIG takes the user text prompt as\ninput and aims to transform the surface style using the projector. LAPIG's key\nchallenge is that due to the projector's physical brightness limitation and the\nsurface texture, the viewer's perceived projection may suffer from color\nsaturation and artifacts in both dark and bright regions, such that even with\nthe state-of-the-art projector compensation techniques, the viewer may see\nclear surface texture-related artifacts. Therefore, how to generate a projector\nimage that follows the user's instruction while also displaying minimum surface\nartifacts is an open problem. To address this issue, we propose projection\nsurface adaptation (PSA) that can generate compensable surface stylization. We\nfirst train two networks to simulate the projector compensation and\nproject-and-capture processes, this allows us to find a satisfactory projector\nimage without real project-and-capture and utilize gradient descent for fast\nconvergence. Then, we design content and saturation losses to guide the\nprojector image generation, such that the generated image shows no clearly\nperceivable artifacts when projected. Finally, the generated image is projected\nfor visually pleasing surface style morphing effects. The source code and video\nare available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.", "AI": {"title_translation": "LAPIG：语言引导的投影图像生成，具有表面适应和风格化", "tldr": "LAPIG是一种语言引导的投影图像生成方法，通过表面适应和风格化解决投影伪影问题，实现视觉上令人愉悦的表面风格转换。", "motivation": "由于投影仪的物理亮度限制和表面纹理，在投影时会出现色彩饱和度问题和伪影，即使使用最先进的补偿技术也无法完全消除，因此如何在遵循用户指令的同时生成显示最少表面伪影的投影图像是一个开放问题。", "method": "LAPIG是一种语言引导的投影图像生成方法，包含一个投影仪-摄像机系统和目标纹理投影表面。它接收用户文本提示作为输入，并提出投影表面适应（PSA）技术来生成可补偿的表面风格化。该方法首先训练两个网络模拟投影仪补偿和投影-捕获过程，从而无需实际投影即可找到满意的图像，并利用梯度下降加速收敛。随后，设计内容和饱和度损失来指导图像生成，确保投影图像无明显伪影。最终，生成的图像被投影以实现视觉上令人愉悦的表面风格变形效果。", "result": "LAPIG能够生成可补偿的表面风格化，在投影时不会显示任何清晰可见的伪影，并实现了视觉上令人愉悦的表面风格变形效果。", "conclusion": "LAPIG成功解决了在纹理表面上生成具有最小伪影的投影图像的挑战，通过新颖的表面适应技术和基于学习的方法，实现了由语言提示引导的视觉上令人愉悦的风格转换。", "translation": "我们提出了LAPIG，一种语言引导的投影图像生成方法，具有表面适应和风格化功能。LAPIG由一个投影仪-摄像机系统和一个目标纹理投影表面组成。LAPIG接收用户文本提示作为输入，旨在利用投影仪改变表面风格。LAPIG的关键挑战在于，由于投影仪的物理亮度限制和表面纹理，观看者感知的投影可能会在暗区和亮区都出现色彩饱和度问题和伪影，即使采用最先进的投影仪补偿技术，观看者也可能看到明显的与表面纹理相关的伪影。因此，如何生成一个既遵循用户指令又显示最少表面伪影的投影图像是一个开放问题。为了解决这个问题，我们提出了投影表面适应（PSA），它可以生成可补偿的表面风格化。我们首先训练两个网络来模拟投影仪补偿和投影-捕获过程，这使我们能够在没有实际投影-捕获的情况下找到令人满意的投影图像，并利用梯度下降实现快速收敛。然后，我们设计了内容损失和饱和度损失来指导投影图像生成，使得生成的图像在投影时不会显示任何清晰可见的伪影。最后，生成的图像被投影以实现视觉上令人愉悦的表面风格变形效果。源代码和视频可在项目页面获取：https://Yu-chen-Deng.github.io/LAPIG/。", "summary": "LAPIG是一种新颖的语言引导投影图像生成方法，旨在解决在纹理表面上投影时因亮度限制和表面纹理导致的色彩饱和度及伪影问题。该方法通过引入投影表面适应（PSA）技术，并训练两个网络模拟投影补偿和投影-捕获过程，从而在不进行实际投影的情况下优化投影图像。通过设计内容和饱和度损失，LAPIG能够生成在投影时几乎没有可见伪影的图像，最终实现由用户文本提示驱动的、视觉上令人愉悦的表面风格转换效果。", "keywords": "语言引导, 投影图像生成, 表面适应, 风格化, 伪影补偿", "comments": "LAPIG的创新之处在于其通过模拟投影-捕获过程和引入投影表面适应（PSA）来解决传统投影补偿技术无法克服的纹理表面伪影问题。这种方法避免了反复的实际投影测试，显著提高了效率。结合语言引导，它为交互式表面风格转换提供了一个强大的工具，具有广泛的应用潜力，例如在增强现实、艺术创作和智能家居等领域。"}}
{"id": "2503.12191", "pdf": "https://arxiv.org/pdf/2503.12191", "abs": "https://arxiv.org/abs/2503.12191", "authors": ["Ying Zang", "Yuncan Gao", "Jiangi Zhang", "Yuangi Hu", "Runlong Cao", "Lanyun Zhu", "Qi Zhu", "Deyi Ji", "Renjun Xu", "Tianrun Chen"], "title": "Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches", "categories": ["cs.CV"], "comment": null, "summary": "This work advances zero-shot interactive segmentation for remote sensing\nimagery through three key contributions. First, we propose a novel sketch-based\nprompting method, enabling users to intuitively outline objects, surpassing\ntraditional point or box prompts. Second, we introduce LTL-Sensing, the first\ndataset pairing human sketches with remote sensing imagery, setting a benchmark\nfor future research. Third, we present LTL-Net, a model featuring a multi-input\nprompting transport module tailored for freehand sketches. Extensive\nexperiments show our approach significantly improves segmentation accuracy and\nrobustness over state-of-the-art methods like SAM, fostering more intuitive\nhuman-AI collaboration in remote sensing analysis and enhancing its\napplications.", "AI": {"title_translation": "打破边界：利用手绘草图增强遥感图像分割", "tldr": "该论文提出了一种基于草图的新方法、一个新的数据集（LTL-Sensing）和一个模型（LTL-Net），以改进遥感图像的零样本交互式分割，并超越了现有方法。", "motivation": "为了通过克服传统点或框提示的局限性，并实现更直观的用户交互，来推进遥感图像的零样本交互式分割。", "method": "1. 提出了一种新颖的基于草图的提示方法，用于直观地勾勒对象。2. 引入了LTL-Sensing，这是第一个将人类草图与遥感图像配对的数据集。3. 提出了LTL-Net，一个带有专为手绘草图定制的多输入提示传输模块的模型。", "result": "广泛的实验表明，与SAM等最先进的方法相比，我们的方法显著提高了分割精度和鲁棒性。", "conclusion": "该方法促进了遥感分析中更直观的人机协作，并增强了其应用。", "translation": "这项工作通过三项关键贡献推进了遥感图像的零样本交互式分割。首先，我们提出了一种新颖的基于草图的提示方法，使用户能够直观地勾勒出对象轮廓，超越了传统的点或框提示。其次，我们引入了LTL-Sensing，这是第一个将人类草图与遥感图像配对的数据集，为未来的研究树立了基准。第三，我们提出了LTL-Net，一个具有专为手绘草图定制的多输入提示传输模块的模型。广泛的实验表明，我们的方法显著提高了分割精度和鲁棒性，超越了SAM等最先进的方法，促进了遥感分析中更直观的人机协作并增强了其应用。", "summary": "该论文提出了一种新颖的基于草图的提示方法，用于遥感图像的零样本交互式分割。它还介绍了LTL-Sensing，这是第一个将人类草图与遥感图像配对的数据集，以及LTL-Net，一个专为手绘草图设计的模型。实验表明，与最先进的方法相比，该方法显著提高了分割精度和鲁棒性，从而增强了遥感领域的人机协作。", "keywords": "遥感, 图像分割, 手绘草图, 零样本, 交互式分割", "comments": "该论文的创新之处在于超越了传统的边界框/点提示，转向更直观的徒手草图进行交互式分割，并辅以新的数据集和专用模型。这可能显著改善遥感分析中的用户体验和效率。"}}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks.", "AI": {"title_translation": "S2IL：结构稳定的增量学习", "tldr": "提出S2IL，一种新的特征蒸馏方法，通过保留特征的整体空间模式来缓解灾难性遗忘，在增量学习任务中表现优于现有方法，尤其在多任务场景下。", "motivation": "现有特征蒸馏（FD）方法在类增量学习（CIL）中存在灾难性遗忘（CF）问题，并且强制特征幅度和方向的严格对齐，限制了模型适应新知识的能力。", "method": "提出结构稳定的增量学习（S2IL），这是一种用于类增量学习（CIL）的特征蒸馏（FD）方法。它通过关注保留特征的整体空间模式来减轻灾难性遗忘，从而促进灵活且稳定的表示，以保留旧知识。", "result": "S2IL在CIFAR-100、ImageNet-100和ImageNet-1K等SOTA基准数据集上实现了强大的增量准确性，并优于其他FD方法。特别是在增量任务数量较多的场景中，S2IL显著优于其他方法。", "conclusion": "S2IL通过结构性地稳定特征表示，有效缓解了类增量学习中的灾难性遗忘问题，并在多任务增量学习中表现出卓越的性能。", "translation": "特征蒸馏（FD）策略被证明在缓解类增量学习（CIL）中出现的灾难性遗忘（CF）方面是有效的。然而，当前的FD方法在增量步骤中强制特征幅度与方向的严格对齐，限制了模型适应新知识的能力。本文提出结构稳定的增量学习（S2IL），这是一种用于CIL的FD方法，它通过关注保留特征的整体空间模式来减轻CF，从而促进灵活（可塑性）但稳定的表示，以保留旧知识（稳定性）。我们还证明了我们提出的S2IL方法在SOTA基准数据集CIFAR-100、ImageNet-100和ImageNet-1K上实现了强大的增量准确性，并且优于其他FD方法。值得注意的是，在增量任务数量较多的场景中，S2IL显著优于其他方法。", "summary": "本文提出了一种名为S2IL（结构稳定的增量学习）的特征蒸馏方法，旨在解决类增量学习中灾难性遗忘问题。与现有方法不同，S2IL通过保留特征的整体空间模式来促进模型在学习新知识的同时保持对旧知识的稳定表示。实验结果表明，S2IL在多个基准数据集上取得了优异的增量准确性，尤其在面对大量增量任务时，其性能显著超越了其他特征蒸馏方法。", "keywords": "增量学习, 灾难性遗忘, 特征蒸馏, 结构稳定性, 空间模式", "comments": "S2IL的创新点在于其不同于传统FD方法，通过关注特征的整体空间模式而非严格对齐来平衡模型的适应性和稳定性，这为增量学习提供了一个新的视角。其在多任务场景下的显著优势表明了该方法在实际应用中的潜力。"}}
{"id": "2503.12206", "pdf": "https://arxiv.org/pdf/2503.12206", "abs": "https://arxiv.org/abs/2503.12206", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Muhammad Haris Khan", "Mohsen Ali"], "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. We evaluated our\nmodels on 11 base-to-novel datasets and they achieved superior accuracy on 9 of\nthese, including benchmarks like ImageNet, SUN397 and Caltech101, while\nmaintaining a strictly training-free paradigm. Our overall accuracy of 83.44%\nsurpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.\nOur method achieved 83.6% average accuracy across 13 datasets, a 9.7%\nimprovement over the previous 73.9% state-of-the-art for training-free\napproaches. Our method improves domain generalization, with a 3.6% gain on\nImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot\nmethods.", "AI": {"title_translation": "TLAC: 两阶段LMM增强CLIP用于零样本分类", "tldr": "TLAC和SLAC是无需训练的方法，利用大型多模态模型（LMM）增强CLIP，在零样本图像分类上超越了现有方法，解决了微调的限制。", "motivation": "现有的CLIP方法在图像分类中表现出色，但依赖于微调技术（如提示学习和适配器调优），这限制了CLIP对新数据集和领域的适应性，并需要大量时间和计算资源。本研究旨在克服这一限制，开发无需训练的方法。", "method": "提出SLAC（单阶段LMM增强CLIP）和TLAC（两阶段LMM增强CLIP）两种无需训练的方法。这些方法利用预训练的大型多模态模型（LMMs，如Gemini）进行图像分类。具体步骤包括：首先，提示LMM识别图像中的对象；随后，CLIP文本编码器通过识别与LMM预测对象语义相似度最高的类来确定图像类别。", "result": "模型在11个从基础到新颖的数据集中，有9个实现了更高的准确性，包括ImageNet、SUN397和Caltech101，且严格遵循无需训练范式。整体准确率达到83.44%，超越了之前最先进的少样本方法6.75%。在13个数据集上的平均准确率为83.6%，比之前无需训练方法的73.9%提高了9.7%。在领域泛化方面，ImageNetV2上提高了3.6%，ImageNet-S上提高了16.96%，ImageNet-R上提高了12.59%，均优于先前的少样本方法。", "conclusion": "本研究提出了TLAC和SLAC两种无需训练的LMM增强CLIP方法，有效克服了传统CLIP微调的限制，在零样本图像分类和领域泛化方面取得了显著的SOTA性能提升，展现了LMM在无需额外训练情况下对CLIP性能的强大增强能力。", "translation": "对比语言-图像预训练（CLIP）在图像分类方面展现出令人印象深刻的零样本性能。然而，最先进的方法通常依赖于微调技术，如提示学习和基于适配器的调优，以优化CLIP的性能。微调的必要性显著限制了CLIP对新数据集和领域的适应性。这一要求使得每个新数据集都需要大量时间和计算资源。为了克服这一限制，我们引入了简单而有效的无需训练方法：单阶段LMM增强CLIP（SLAC）和两阶段LMM增强CLIP（TLAC），它们利用强大的大型多模态模型（LMMs），例如Gemini，进行图像分类。所提出的方法利用了预训练LMMs的能力，无需额外训练即可无缝适应各种数据集和领域。我们的方法涉及提示LMM识别图像中的对象。随后，CLIP文本编码器通过识别与LMM预测对象语义相似度最高的类别来确定图像类别。我们在11个从基础到新颖的数据集上评估了我们的模型，它们在其中9个数据集上实现了更高的准确性，包括ImageNet、SUN397和Caltech101等基准测试，同时保持了严格的无需训练范式。我们的整体准确率达到83.44%，超过了之前最先进的少样本方法6.75%。我们的方法在13个数据集上平均准确率达到83.6%，比之前无需训练方法的73.9%提高了9.7%。我们的方法改善了领域泛化能力，在ImageNetV2上比之前的少样本方法提高了3.6%，在ImageNet-S上提高了16.96%，在ImageNet-R上提高了12.59%。", "summary": "该论文提出了TLAC（两阶段LMM增强CLIP）和SLAC（单阶段LMM增强CLIP）两种无需训练的零样本图像分类方法，旨在克服传统CLIP需要微调的局限性。这些方法利用大型多模态模型（LMMs）识别图像对象，再结合CLIP文本编码器确定最终类别。实验结果表明，TLAC和SLAC在多个基准数据集上显著提升了零样本分类准确率，并改善了领域泛化能力，超越了现有最先进的少样本和无需训练方法。", "keywords": "零样本分类, CLIP, 大型多模态模型, 无需训练, 领域泛化", "comments": "这项工作的主要创新在于提出了无需训练的LMM增强CLIP方法，有效解决了CLIP在面对新数据集时需要大量微调的痛点。通过利用LMM的强大理解能力，该方法实现了更强的零样本泛化和领域适应性，显著降低了计算资源和时间成本。其在多个基准测试中超越SOTA的表现，证明了其重要性和有效性。"}}
{"id": "2503.12213", "pdf": "https://arxiv.org/pdf/2503.12213", "abs": "https://arxiv.org/abs/2503.12213", "authors": ["Ruyu Wang", "Xuefeng Hou", "Sabrina Schmedding", "Marco F. Huber"], "title": "STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted by WACV2025", "summary": "In layout-to-image (L2I) synthesis, controlled complex scenes are generated\nfrom coarse information like bounding boxes. Such a task is exciting to many\ndownstream applications because the input layouts offer strong guidance to the\ngeneration process while remaining easily reconfigurable by humans. In this\npaper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based\nmodel that produces photo-realistic images and provides fine-grained control of\nstylized objects in scenes. Our approach learns a global condition for each\nlayout, and a self-supervised semantic map for weight modulation using a novel\nEdge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)\nis also introduced to cross-condition the global condition and image feature\nfor capturing the objects' relationships. These measures provide consistent\nguidance through the model, enabling more accurate and controllable image\ngeneration. Extensive benchmarking demonstrates that our STAY Diffusion\npresents high-quality images while surpassing previous state-of-the-art methods\nin generation diversity, accuracy, and controllability.", "AI": {"title_translation": "STAY Diffusion：用于多样化布局到图像生成的风格化布局扩散模型", "tldr": "STAY Diffusion是一种基于扩散的模型，用于布局到图像的合成，它能生成逼真的图像并对风格化对象进行精细控制，在多样性、准确性和可控性方面超越了现有技术。", "motivation": "布局到图像（L2I）合成任务因其能够从粗略信息（如边界框）生成受控的复杂场景，并为生成过程提供强大的指导，同时易于人类重新配置，因此对许多下游应用具有重要意义。", "method": "本文提出了STAY Diffusion（风格化布局扩散）模型，这是一个基于扩散的模型，用于生成逼真的图像，并对场景中的风格化对象进行细粒度控制。该方法为每个布局学习一个全局条件，并使用新颖的边缘感知归一化（EA Norm）来自监督语义图进行权重调制。此外，还引入了新的风格化掩码注意力（SM Attention），用于交叉调节全局条件和图像特征，以捕捉对象关系。这些措施为模型提供了持续的指导，实现了更准确和可控的图像生成。", "result": "广泛的基准测试表明，STAY Diffusion生成了高质量图像，并在生成多样性、准确性和可控性方面超越了之前的最先进方法。", "conclusion": "STAY Diffusion模型在布局到图像的合成任务中，通过其创新的组件，实现了高质量、多样化、准确且可控的图像生成，超越了现有技术水平。", "translation": "在布局到图像（L2I）合成中，受控的复杂场景是从诸如边界框之类的粗略信息生成的。这样的任务对于许多下游应用来说是令人兴奋的，因为输入布局为生成过程提供了强大的指导，同时易于人类重新配置。在本文中，我们提出了风格化布局扩散（STAY Diffusion），这是一种基于扩散的模型，可以生成逼真的图像，并对场景中的风格化对象提供细粒度控制。我们的方法为每个布局学习一个全局条件，并使用一种新颖的边缘感知归一化（EA Norm）来自监督语义图进行权重调制。此外，还引入了新的风格化掩码注意力（SM Attention），用于交叉调节全局条件和图像特征，以捕捉对象关系。这些措施为模型提供了持续的指导，实现了更准确和可控的图像生成。广泛的基准测试表明，我们的STAY Diffusion呈现出高质量的图像，同时在生成多样性、准确性和可控性方面超越了之前的最先进方法。", "summary": "STAY Diffusion是一种新颖的基于扩散的布局到图像（L2I）模型，旨在从粗略布局信息生成逼真且可控的图像。该模型通过学习全局条件、引入边缘感知归一化（EA Norm）进行权重调制，以及采用风格化掩码注意力（SM Attention）来捕捉对象关系，实现了对风格化对象的细粒度控制。实验结果表明，STAY Diffusion在图像质量、生成多样性、准确性和可控性方面均优于现有最先进方法。", "keywords": "布局到图像, 扩散模型, 风格化对象, 图像生成, 细粒度控制", "comments": "该论文提出了一种创新的扩散模型STAY Diffusion，它在布局到图像生成领域取得了显著进展。其创新点在于引入了EA Norm和SM Attention，这两者协同作用，增强了模型对风格化对象的细粒度控制和对象关系的捕捉能力。这对于需要高度可控和多样化图像输出的下游应用具有重要意义。该模型在性能上超越了SOTA，表明其在图像合成领域的强大潜力。"}}
{"id": "2503.12215", "pdf": "https://arxiv.org/pdf/2503.12215", "abs": "https://arxiv.org/abs/2503.12215", "authors": ["Amulya Reddy Maligireddy", "Manohar Reddy Uppula", "Nidhi Rastogi", "Yaswanth Reddy Parla"], "title": "Gun Detection Using Combined Human Pose and Weapon Appearance", "categories": ["cs.CV"], "comment": null, "summary": "The increasing frequency of firearm-related incidents has necessitated\nadvancements in security and surveillance systems, particularly in firearm\ndetection within public spaces. Traditional gun detection methods rely on\nmanual inspections and continuous human monitoring of CCTV footage, which are\nlabor-intensive and prone to high false positive and negative rates. To address\nthese limitations, we propose a novel approach that integrates human pose\nestimation with weapon appearance recognition using deep learning techniques.\nUnlike prior studies that focus on either body pose estimation or firearm\ndetection in isolation, our method jointly analyzes posture and weapon presence\nto enhance detection accuracy in real-world, dynamic environments. To train our\nmodel, we curated a diverse dataset comprising images from open-source\nrepositories such as IMFDB and Monash Guns, supplemented with AI-generated and\nmanually collected images from web sources. This dataset ensures robust\ngeneralization and realistic performance evaluation under various surveillance\nconditions. Our research aims to improve the precision and reliability of\nfirearm detection systems, contributing to enhanced public safety and threat\nmitigation in high-risk areas.", "AI": {"title_translation": "结合人体姿态与武器外观的枪支检测", "tldr": "提出一种结合人体姿态估计和武器外观识别的深度学习方法，以提高公共场所枪支检测的准确性和可靠性，克服传统方法和单一检测方法的局限性。", "motivation": "鉴于枪支相关事件的日益频繁，公共场所的枪支检测系统需求迫切。传统方法依赖人工检查和监控，效率低且错误率高，因此需要更先进的自动化解决方案。", "method": "本文提出一种新颖的深度学习方法，将人体姿态估计与武器外观识别相结合，共同分析姿态和武器存在以提高检测精度。为了训练模型，作者从IMFDB、Monash Guns等开源库以及AI生成和手动收集的图像中整理了一个多样化的数据集。", "result": "Not mentioned in abstract", "conclusion": "本研究旨在提高枪支检测系统的精度和可靠性，从而增强公共安全并减轻高风险区域的威胁。", "translation": "枪支相关事件的日益频繁使得安全和监控系统，特别是在公共场所的枪支检测方面，有必要取得进展。传统的枪支检测方法依赖于人工检查和对闭路电视录像的持续人工监控，这既费力又容易产生高误报率和漏报率。为了解决这些局限性，我们提出了一种新颖的方法，该方法使用深度学习技术将人体姿态估计与武器外观识别相结合。与以往仅关注身体姿态估计或单独枪支检测的研究不同，我们的方法共同分析姿态和武器存在，以提高在真实动态环境中的检测精度。为了训练我们的模型，我们策划了一个多样化的数据集，该数据集包含来自IMFDB和Monash Guns等开源存储库的图像，并辅以AI生成和从网络源手动收集的图像。该数据集确保了在各种监控条件下的鲁棒泛化和实际性能评估。我们的研究旨在提高枪支检测系统的精度和可靠性，从而有助于增强公共安全并减轻高风险区域的威胁。", "summary": "本文提出一种结合深度学习技术的新型枪支检测方法，该方法整合了人体姿态估计和武器外观识别，旨在克服传统人工检测和单一检测方法的局限性。通过共同分析人体姿态和武器存在，该方法提高了在动态真实世界环境中的检测精度。研究人员为此构建了一个包含开源、AI生成和手动收集图像的多样化数据集，以确保模型的泛化能力和性能评估的现实性。该研究旨在提升枪支检测系统的精度和可靠性，从而增强公共安全。", "keywords": "枪支检测, 深度学习, 人体姿态估计, 武器外观识别, 公共安全", "comments": "这篇论文的创新点在于将人体姿态估计和武器外观识别相结合，突破了以往单一检测方法的局限，有望显著提高枪支检测的准确性。其构建多样化数据集的方法也值得关注，有助于模型在复杂真实环境中的泛化能力。这项研究对于提升公共安全和威胁缓解具有重要意义。"}}
{"id": "2503.12218", "pdf": "https://arxiv.org/pdf/2503.12218", "abs": "https://arxiv.org/abs/2503.12218", "authors": ["Chengxuan Qian", "Kai Han", "Siqi Ma", "Chongwen Lyu", "Zhenlong Yuan", "Jun Chen", "Zhe Liu"], "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods.", "AI": {"title_translation": "用于噪声标签下鲁棒医学图像分割的自适应标签校正", "tldr": "本文提出了一种基于Mean Teacher的自适应标签校正（ALC）框架，用于在存在噪声标签的情况下进行鲁棒的医学图像分割，通过自适应标签细化和不确定性选择来提高模型性能。", "motivation": "深度学习在医学图像分析中取得了显著成功，但其对大量高质量标注数据的依赖限制了其应用。虽然带噪声的标注数据更容易获取，但直接将其用于训练会降低模型性能。", "method": "本文提出了一种基于Mean Teacher的自适应标签校正（ALC）自集成框架，用于在噪声标签下进行鲁棒的医学图像分割。该框架利用Mean Teacher架构确保在噪声扰动下的一致性学习，并包含一个自适应标签细化机制，动态捕获和加权多个扰动版本之间的差异，以提高噪声标签的质量。此外，还引入了基于样本级不确定性的标签选择算法，优先处理高置信度样本进行网络更新，从而减轻噪声标注的影响。一致性学习被整合以对齐学生和教师网络的预测，进一步增强模型鲁棒性。", "result": "在两个公共数据集上进行的广泛实验表明，所提出的框架是有效的，并在分割性能方面显示出显著改进。与最先进的方法相比，ALC框架取得了有竞争力的结果。", "conclusion": "ALC框架通过充分利用Mean Teacher结构的优势，有效处理噪声标签，适应具有挑战性的场景，并取得了与现有最先进方法相当的竞争性结果，显著提高了医学图像分割的鲁棒性。", "translation": "深度学习在医学图像分析中取得了显著成功，但其对大量高质量标注数据的依赖限制了其应用。虽然带噪声的标注数据更容易获取，但直接将其用于训练会降低模型性能。为了解决这一挑战，我们提出了一种基于Mean Teacher的自适应标签校正（ALC）自集成框架，用于在噪声标签下进行鲁棒的医学图像分割。该框架利用Mean Teacher架构确保在噪声扰动下的一致性学习。它包括一个自适应标签细化机制，动态捕获和加权多个扰动版本之间的差异，以提高噪声标签的质量。此外，还引入了一个基于样本级不确定性的标签选择算法，优先处理高置信度样本进行网络更新，从而减轻噪声标注的影响。一致性学习被整合以对齐学生和教师网络的预测，进一步增强模型鲁棒性。在两个公共数据集上进行的广泛实验表明，所提出的框架是有效的，并在分割性能方面显示出显著改进。通过充分利用Mean Teacher结构的优势，ALC框架有效处理噪声标签，适应具有挑战性的场景，并取得了与现有最先进方法相当的竞争性结果。", "summary": "本文提出了一种名为自适应标签校正（ALC）的基于Mean Teacher的自集成框架，旨在解决医学图像分割中噪声标签导致模型性能下降的问题。该框架通过自适应标签细化机制提高噪声标签质量，并利用基于样本级不确定性的标签选择算法优先处理高置信度样本。此外，结合一致性学习进一步增强模型鲁棒性。实验证明，ALC框架在处理噪声标签和提高分割性能方面表现出色，达到了与现有最佳方法相当的水平。", "keywords": "医学图像分割, 噪声标签, 自适应标签校正, Mean Teacher, 鲁棒性", "comments": "该论文通过结合Mean Teacher架构、自适应标签细化和不确定性驱动的样本选择，为噪声标签下的医学图像分割提供了一个创新且鲁棒的解决方案。其优势在于能够有效利用易于获取但质量不高的标注数据，显著降低了对昂贵高质量标注的需求，对实际应用具有重要意义。"}}
{"id": "2503.12230", "pdf": "https://arxiv.org/pdf/2503.12230", "abs": "https://arxiv.org/abs/2503.12230", "authors": ["Yihao Wang", "Raphael Memmesheimer", "Sven Behnke"], "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps.", "AI": {"title_translation": "LIAM：用于语言指令、图像、动作和语义地图的多模态Transformer", "tldr": "LIAM是一个端到端的多模态Transformer模型，它结合语言、图像、动作和语义地图输入，预测家政服务机器人的动作序列。", "motivation": "现有方法在处理家政服务机器人任务的多样性方面可能缺乏灵活性，通常需要为每个任务单独实现。利用大型语言模型和开放词汇对象感知方法可以为机器人提供更大的灵活性以应对复杂多变的任务。", "method": "本文提出了LIAM，一个端到端模型，它基于语言、图像、动作和地图输入预测动作序列。语言和图像输入通过CLIP主干进行编码，并设计了两个预训练任务来微调其权重并预对齐潜在空间。该方法在ALFRED数据集上进行了评估。", "result": "实验结果表明，预对齐不同模态的嵌入空间以及结合语义地图对于模型性能至关重要且有效。", "conclusion": "结合多模态输入（语言、图像、动作、语义地图）并通过预对齐嵌入空间，能够有效地提高家政服务机器人处理多样化任务的能力。", "translation": "大型语言模型和开放词汇对象感知方法的可用性为家用服务机器人提供了更大的灵活性。通过向机器人提供任务描述和适当的环境信息，可以解决家用任务的巨大多样性，而无需单独实现每个任务。在这项工作中，我们提出了LIAM——一个端到端模型，它根据语言、图像、动作和地图输入预测动作序列。语言和图像输入通过CLIP主干进行编码，为此我们设计了两个预训练任务来微调其权重并预对齐潜在空间。我们在ALFRED数据集（一个用于家用任务的模拟器生成基准）上评估了我们的方法。我们的结果证明了预对齐不同模态嵌入空间的重要性以及结合语义地图的有效性。", "summary": "本文提出了LIAM，一个面向家用服务机器人的多模态Transformer模型，能够根据语言指令、图像、动作和语义地图输入预测动作序列。LIAM利用CLIP主干编码语言和图像，并通过专门的预训练任务对齐不同模态的潜在空间。在ALFRED数据集上的评估结果表明，预对齐嵌入空间和整合语义地图对于提升模型性能至关重要。", "keywords": "多模态Transformer, 家用服务机器人, 语言指令, 语义地图, 动作预测", "comments": "LIAM的创新之处在于其端到端的多模态集成能力，特别是在预对齐不同模态的嵌入空间以及有效利用语义地图方面。这对于提高家政服务机器人在复杂、多样化环境中的泛化能力和任务执行效率具有重要意义。"}}
{"id": "2503.12232", "pdf": "https://arxiv.org/pdf/2503.12232", "abs": "https://arxiv.org/abs/2503.12232", "authors": ["Yan Jiang", "Hao Yu", "Xu Cheng", "Haoyu Chen", "Zhaodong Sun", "Guoying Zhao"], "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community.", "AI": {"title_translation": "从实验室到现实世界：一个面向隐私保护的可见光-红外行人重识别新基准", "tldr": "本文提出了L2RW，一个面向隐私保护的可见光-红外行人重识别（VI-ReID）新基准，通过去中心化训练实现，性能可与集中式方法媲美。", "motivation": "现有的可见光-红外行人重识别（VI-ReID）方法采用集中式训练，但在现实世界的监控环境中，数据分散在多个设备/实体中，引发隐私和数据所有权问题，使得集中式训练不切实际。", "method": "提出了L2RW基准，旨在将去中心化训练融入VI-ReID以解决隐私问题。该基准为不同的隐私敏感度级别设计了协议和相应算法，确保数据完全隔离或选择性共享。通过对各种服务器端联邦算法进行大量实验，模拟了严格隐私约束下的训练场景。", "result": "实验证明去中心化VI-ReID训练是可行的。在未见过的域中评估时，使用隔离数据（隐私保护）训练的L2RW，其性能与使用共享数据（无隐私限制）训练的最先进方法相当。", "conclusion": "这项工作为部署符合现实世界场景的可见光-红外行人重识别（VI-ReID）提供了一个新的研究入口，通过去中心化训练解决了隐私问题，有望造福社区。", "translation": "可见光-红外行人重识别（VI-ReID）旨在匹配在不同光照条件下捕获的行人图像，已引起广泛研究关注并取得了可喜的成果。然而，在现实世界的监控环境中，数据分布在多个设备/实体中，引发了隐私和所有权问题，这使得现有的集中式训练对于VI-ReID来说不切实际。为了解决这些挑战，我们提出了L2RW，一个使VI-ReID更接近现实世界应用的基准。L2RW的基本原理是将去中心化训练整合到VI-ReID中，以解决数据共享法规有限场景中的隐私问题。具体来说，我们为不同的隐私敏感度级别设计了协议和相应的算法。在我们新的基准中，我们确保模型训练在以下条件下完成：1）来自每个摄像头的数据完全隔离，或2）不同的数据实体（例如，某个区域的数据控制者）可以选择性地共享数据。通过这种方式，我们模拟了具有严格隐私约束的场景，这更接近现实世界的条件。我们对各种服务器端联邦算法进行了大量实验，显示了去中心化VI-ReID训练的可行性。值得注意的是，当在未见过的域（即新的数据实体）中进行评估时，我们使用隔离数据（隐私保护）训练的L2RW，其性能与使用共享数据（无隐私限制）训练的SOTA（最先进）方法相当。我们希望这项工作能为部署符合现实世界场景的VI-ReID提供一个新的研究入口，并能造福社区。", "summary": "本文引入了L2RW，一个针对可见光-红外行人重识别（VI-ReID）的新基准，通过整合去中心化训练来解决现实世界监控中的隐私问题。L2RW通过数据隔离或选择性共享协议模拟严格的隐私约束。实验证明了去中心化VI-ReID的可行性，L2RW即使在未见过的域中也能实现与最先进的集中式方法相当的性能，从而促进了VI-ReID在现实世界中的部署。", "keywords": "可见光-红外行人重识别, 隐私保护, 去中心化训练, 基准, 联邦学习", "comments": "本文提出了一个重要的基准（L2RW），通过去中心化训练解决隐私问题，弥合了实验室VI-ReID研究与现实世界部署之间的差距。其创新之处在于模拟严格的隐私约束，并证明了隐私保护训练可以获得与无隐私限制方法相当的性能，这是在监控领域实现实用、道德AI系统的重大一步。"}}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos.", "AI": {"title_translation": "RePerformer：从播放到照片级真实再表演的沉浸式以人为中心的体积视频", "tldr": "RePerformer 提出了一种新颖的基于高斯的方法，用于高质量以人为中心的体积视频，能够统一播放和照片级真实再表演，通过解耦运动和外观高斯，并利用2D CNN和语义感知对齐实现新动作下的渲染。", "motivation": "现有的人体中心体积视频方法主要侧重于重放通用动态场景或动画化人体化身，这限制了它们再表演通用动态场景的能力。", "method": "本文提出了 RePerformer，一种新颖的基于高斯表示的方法。它将动态场景分层解耦为运动高斯和外观高斯，并在规范空间中进行关联。采用基于 Morton 的参数化来高效编码外观高斯到2D位置和属性图。为了增强泛化能力，采用2D CNNs 将位置图映射到属性图，然后组装成外观高斯进行高保真渲染。对于再表演，开发了一个语义感知对齐模块，并对运动高斯应用形变迁移，从而实现新动作下的照片级真实渲染。", "result": "广泛的实验验证了 RePerformer 的鲁棒性和有效性。", "conclusion": "RePerformer 为以人为中心的体积视频中的“播放-然后-再表演”范式设定了一个新的基准。", "translation": "以人为中心的体积视频提供了沉浸式的自由视角体验，然而现有方法要么专注于重放通用动态场景，要么专注于动画化人体化身，这限制了它们再表演通用动态场景的能力。在本文中，我们提出了 RePerformer，一种新颖的基于高斯表示的方法，它统一了高保真以人为中心的体积视频的播放和再表演。具体来说，我们将动态场景分层解耦为运动高斯和外观高斯，并在规范空间中进行关联。我们进一步采用基于 Morton 的参数化方法，将外观高斯高效编码为2D位置和属性图。为了增强泛化能力，我们采用2D CNNs 将位置图映射到属性图，这些属性图可以组装成外观高斯，用于动态场景的高保真渲染。对于再表演，我们开发了一个语义感知对齐模块，并对运动高斯应用形变迁移，从而实现在新动作下的照片级真实渲染。广泛的实验验证了 RePerformer 的鲁棒性和有效性，为以人为中心的体积视频中的“播放-然后-再表演”范式设定了一个新的基准。", "summary": "RePerformer 是一种新颖的基于高斯表示的方法，用于创建高质量的以人为中心的体积视频。它通过将动态场景解耦为运动和外观高斯来统一视频的播放和再表演。该方法利用基于Morton的参数化和2D CNNs进行高效编码和泛化渲染，并通过语义感知对齐和形变迁移实现新动作下的照片级真实再表演。实验证明了其鲁棒性和有效性，为该领域树立了新标杆。", "keywords": "以人为中心的体积视频, 高斯表示, 再表演, 播放, 形变迁移", "comments": "RePerformer的创新之处在于其统一播放和再表演的框架，以及基于高斯的解耦表示。通过将运动和外观信息分离，并结合高效的编码（Morton参数化）和学习（2D CNNs），它显著提升了以人为中心体积视频的保真度和泛化能力。特别是其在再表演方面的能力，通过语义感知对齐和形变迁移，使得在全新动作下也能实现照片级真实渲染，这对于虚拟现实、内容创作等领域具有重要意义。该方法为未来的体积视频研究提供了新的方向。"}}
{"id": "2503.12249", "pdf": "https://arxiv.org/pdf/2503.12249", "abs": "https://arxiv.org/abs/2503.12249", "authors": ["Boyu Chen", "Ameenat L. Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "categories": ["cs.CV"], "comment": null, "summary": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging\ntechnique with great potential for diagnosing anterior uveitis, a\nvision-threatening ocular inflammatory condition. A hallmark of this condition\nis the presence of inflammatory cells in the eye's anterior chamber, and\ndetecting these cells using AS-OCT images has attracted research interest.\nWhile recent efforts aim to replace manual cell detection with automated\ncomputer vision approaches, detecting extremely small (minuscule) objects in\nhigh-resolution images, such as AS-OCT, poses substantial challenges: (1) each\ncell appears as a minuscule particle, representing less than 0.005\\% of the\nimage, making the detection difficult, and (2) OCT imaging introduces\npixel-level noise that can be mistaken for cells, leading to false positive\ndetections. To overcome these challenges, we propose a minuscule cell detection\nframework through a progressive field-of-view focusing strategy. This strategy\nsystematically refines the detection scope from the whole image to a target\nregion where cells are likely to be present, and further to minuscule regions\npotentially containing individual cells. Our framework consists of two modules.\nFirst, a Field-of-Focus module uses a vision foundation model to segment the\ntarget region. Subsequently, a Fine-grained Object Detection module introduces\na specialized Minuscule Region Proposal followed by a Spatial Attention Network\nto distinguish individual cells from noise within the segmented region.\nExperimental results demonstrate that our framework outperforms\nstate-of-the-art methods for cell detection, providing enhanced efficacy for\nclinical applications. Our code is publicly available at:\nhttps://github.com/joeybyc/MCD.", "AI": {"title_translation": "渐进式视野聚焦的AS-OCT图像微小细胞检测", "tldr": "该论文提出了一种渐进式视野聚焦框架，用于在AS-OCT图像中检测微小炎症细胞，性能优于现有方法。", "motivation": "前节OCT (AS-OCT) 图像中微小炎症细胞的检测对前葡萄膜炎的诊断至关重要。然而，检测极小物体（每个细胞小于图像的0.005%）和区分像素级噪声（易导致假阳性）是巨大挑战。", "method": "本文提出了一种通过渐进式视野聚焦策略的微小细胞检测框架。该框架包含两个模块：1) 视野聚焦模块：使用视觉基础模型分割目标区域。2) 细粒度目标检测模块：引入微小区域提议和空间注意力网络，以区分分割区域内的单个细胞和噪声。", "result": "实验结果表明，该框架在细胞检测方面优于最先进的方法，为临床应用提供了更高的效率。", "conclusion": "所提出的渐进式视野聚焦框架成功克服了AS-OCT图像中微小细胞检测的挑战，提高了临床应用的有效性。", "translation": "前节光学相干断层扫描 (AS-OCT) 是一种新兴的成像技术，在诊断前葡萄膜炎（一种威胁视力的眼部炎症疾病）方面具有巨大潜力。这种疾病的一个标志是眼球前房内存在炎症细胞，使用AS-OCT图像检测这些细胞引起了研究兴趣。虽然最近的努力旨在用自动化计算机视觉方法取代手动细胞检测，但在高分辨率图像（如AS-OCT）中检测极小（微小）物体带来了巨大挑战：(1) 每个细胞都表现为微小颗粒，占图像的比例不到0.005%，使得检测困难；(2) OCT成像引入的像素级噪声可能被误认为是细胞，导致假阳性检测。为了克服这些挑战，我们提出了一种通过渐进式视野聚焦策略的微小细胞检测框架。该策略系统地将检测范围从整个图像细化到细胞可能存在的T目标区域，并进一步细化到可能包含单个细胞的微小区域。我们的框架由两个模块组成。首先，视野聚焦模块使用视觉基础模型来分割目标区域。随后，细粒度目标检测模块引入专门的微小区域提议，然后是空间注意力网络，以区分分割区域内的单个细胞和噪声。实验结果表明，我们的框架在细胞检测方面优于最先进的方法，为临床应用提供了更高的效率。我们的代码已公开可用：https://github.com/joeybyc/MCD。", "summary": "为了解决AS-OCT图像中微小炎症细胞检测的难题，本文提出了一种渐进式视野聚焦框架。该框架通过视野聚焦模块初步定位目标区域，再通过细粒度目标检测模块（包含微小区域提议和空间注意力网络）精确区分细胞与噪声。实验证明，该方法在细胞检测方面超越了现有技术，显著提升了临床诊断的效率。", "keywords": "AS-OCT, 细胞检测, 渐进式聚焦, 前葡萄膜炎, 计算机视觉", "comments": "该论文的创新点在于其渐进式视野聚焦策略和专门为处理高分辨率医学图像中微小、噪声干扰目标而设计的双模块框架。这对于提高前葡萄膜炎的自动化诊断精度具有重要意义，克服了传统方法在微小目标识别和噪声抑制方面的局限性。"}}
{"id": "2503.12260", "pdf": "https://arxiv.org/pdf/2503.12260", "abs": "https://arxiv.org/abs/2503.12260", "authors": ["Josep Cabacas-Maso", "Elena Ortega-Beltrán", "Ismael Benito-Altamirano", "Carles Ventura"], "title": "Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge", "categories": ["cs.CV", "I.4"], "comment": null, "summary": "We present our contribution to the 8th ABAW challenge at CVPR 2025, where we\ntackle valence-arousal estimation, emotion recognition, and facial action unit\ndetection as three independent challenges. Our approach leverages the\nwell-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all\nthree tasks, achieving results that surpass the proposed baselines.\nAdditionally, we explore the use of CLIP for the emotion recognition challenge\nas an additional experiment. We provide insights into the architectural choices\nthat contribute to the strong performance of our methods.", "AI": {"title_translation": "通过双向注意力混合特征网络和CLIP增强面部表情识别：应用于第8届ABAW挑战", "tldr": "本文介绍了作者在第8届ABAW挑战中，利用双向注意力混合特征网络（DDAMFN）和CLIP，在效价-唤醒估计、情感识别和面部动作单元检测任务上超越基线的贡献。", "motivation": "论文旨在解决第8届ABAW挑战中的效价-唤醒估计、情感识别和面部动作单元检测三个独立挑战。", "method": "本文主要利用双向注意力混合特征网络 (DDAMFN) 来处理所有三个任务。此外，还探索了使用 CLIP 进行情感识别挑战。", "result": "该方法在所有三个任务上都取得了超越基线的结果。", "conclusion": "该方法（DDAMFN结合CLIP）能有效提升面部表情识别、效价-唤醒估计和面部动作单元检测的性能，并在第8届ABAW挑战中表现出色。", "translation": "我们介绍了我们对CVPR 2025第8届ABAW挑战的贡献，在该挑战中，我们将效价-唤醒估计、情感识别和面部动作单元检测作为三个独立的挑战来解决。我们的方法利用了著名的双向注意力混合特征网络（DDAMFN）来完成所有这三个任务，取得了超越所提出基线的结果。此外，我们探索了将CLIP用于情感识别挑战作为一项额外实验。我们提供了对促使我们方法取得强大性能的架构选择的见解。", "summary": "本文介绍了在第8届ABAW挑战中，针对效价-唤醒估计、情感识别和面部动作单元检测三项任务的解决方案。研究团队采用双向注意力混合特征网络（DDAMFN），并在情感识别任务中额外探索了CLIP的应用。实验结果表明，该方法在所有任务上均超越了现有基线，证明了其架构选择的有效性。", "keywords": "面部表情识别, DDAMFN, CLIP, ABAW挑战, 情感识别", "comments": "本文的创新点在于将DDAMFN应用于多个面部表情相关任务，并结合CLIP进行情感识别，展示了多任务学习和跨模态预训练模型在面部分析领域的潜力。其超越基线的表现证明了所提方法的有效性。"}}
{"id": "2503.12261", "pdf": "https://arxiv.org/pdf/2503.12261", "abs": "https://arxiv.org/abs/2503.12261", "authors": ["R. Gnana Praveen", "Jahangir Alam"], "title": "Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Submission to valence arousal track of 8th ABAW competition. arXiv\n  admin note: substantial text overlap with arXiv:2403.13659", "summary": "Multimodal emotion recognition has recently drawn a lot of interest in\naffective computing as it has immense potential to outperform isolated unimodal\napproaches. Audio and visual modalities are two predominant contact-free\nchannels in videos, which are often expected to carry a complementary\nrelationship with each other. However, audio and visual channels may not always\nbe complementary with each other, resulting in poor audio-visual feature\nrepresentations, thereby degrading the performance of the system. In this\npaper, we propose a flexible audio-visual fusion model that can adapt to weak\ncomplementary relationships using a gated attention mechanism. Specifically, we\nextend the recursive joint cross-attention model by introducing gating\nmechanism in every iteration to control the flow of information between the\ninput features and the attended features depending on the strength of their\ncomplementary relationship. For instance, if the modalities exhibit strong\ncomplementary relationships, the gating mechanism chooses cross-attended\nfeatures, otherwise non-attended features. To further improve the performance\nof the system, we further introduce stage gating mechanism, which is used to\ncontrol the flow of information across the gated outputs of each iteration.\nTherefore, the proposed model improves the performance of the system even when\nthe audio and visual modalities do not have a strong complementary relationship\nwith each other by adding more flexibility to the recursive joint cross\nattention mechanism. The proposed model has been evaluated on the challenging\nAffwild2 dataset and significantly outperforms the state-of-the-art fusion\napproaches.", "AI": {"title_translation": "处理音视频情感识别中的弱互补关系", "tldr": "提出一种带有门控注意力机制的灵活音视频融合模型，以应对模态间弱互补关系，在情感识别任务中表现优异。", "motivation": "现有的多模态情感识别方法在音视频模态之间存在弱互补关系时，会导致特征表示不佳，从而降低系统性能。", "method": "提出一种灵活的音视频融合模型，采用门控注意力机制来适应弱互补关系。该模型通过在每次迭代中引入门控机制来扩展递归联合交叉注意力模型，以根据互补关系的强度控制信息流。此外，还引入了阶段门控机制来控制每次迭代的门控输出之间的信息流。", "result": "该模型在具有挑战性的Affwild2数据集上进行了评估，并显著优于最先进的融合方法。", "conclusion": "提出的模型通过增加递归联合交叉注意力机制的灵活性，即使在音视频模态之间没有强互补关系的情况下，也能提高系统性能。", "translation": "多模态情感识别最近在情感计算领域引起了广泛关注，因为它具有超越孤立单模态方法的巨大潜力。音频和视觉模态是视频中两种主要的非接触式通道，它们通常被期望彼此之间具有互补关系。然而，音频和视觉通道可能并非总是彼此互补，导致音视频特征表示不佳，从而降低系统性能。在本文中，我们提出了一种灵活的音视频融合模型，该模型可以使用门控注意力机制适应弱互补关系。具体而言，我们通过在每次迭代中引入门控机制来扩展递归联合交叉注意力模型，以根据其互补关系的强度控制输入特征和注意力特征之间的信息流。例如，如果模态表现出强互补关系，门控机制会选择交叉注意力特征，否则选择非注意力特征。为了进一步提高系统性能，我们进一步引入了阶段门控机制，该机制用于控制每次迭代的门控输出之间的信息流。因此，即使当音频和视觉模态之间没有强互补关系时，所提出的模型通过增加递归联合交叉注意力机制的灵活性，也能提高系统性能。所提出的模型已在具有挑战性的Affwild2数据集上进行了评估，并显著优于最先进的融合方法。", "summary": "本文针对多模态情感识别中音视频模态可能存在弱互补关系导致性能下降的问题，提出了一种灵活的音视频融合模型。该模型通过在递归联合交叉注意力机制中引入门控注意力机制和阶段门控机制，能够根据模态互补关系的强弱自适应地控制信息流，从而在模态互补性较弱时也能有效提升系统性能。实验结果表明，该模型在Affwild2数据集上显著优于现有最先进的融合方法。", "keywords": "多模态情感识别, 音视频融合, 门控注意力, 弱互补关系, 递归联合交叉注意力", "comments": "这项工作通过引入门控机制来处理多模态情感识别中模态间可能存在的弱互补关系，具有创新性。它解决了传统融合方法在模态互补性不强时性能受限的问题，提高了模型的鲁棒性和适应性。该方法对跨模态信息融合领域有借鉴意义，特别是在处理模态间复杂关系方面。"}}
{"id": "2503.12267", "pdf": "https://arxiv.org/pdf/2503.12267", "abs": "https://arxiv.org/abs/2503.12267", "authors": ["Aziz Amari", "Mariem Makni", "Wissal Fnaich", "Akram Lahmar", "Fedi Koubaa", "Oumayma Charrad", "Mohamed Ali Zormati", "Rabaa Youssef Douss"], "title": "An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation", "categories": ["cs.CV"], "comment": null, "summary": "In large organizations, the number of financial transactions can grow\nrapidly, driving the need for fast and accurate multi-criteria invoice\nvalidation. Manual processing remains error-prone and time-consuming, while\ncurrent automated solutions are limited by their inability to support a variety\nof constraints, such as documents that are partially handwritten or\nphotographed with a mobile phone. In this paper, we propose to automate the\nvalidation of machine written invoices using document layout analysis and\nobject detection techniques based on recent deep learning (DL) models. We\nintroduce a novel dataset consisting of manually annotated real-world invoices\nand a multi-criteria validation process. We fine-tune and benchmark the most\nrelevant DL models on our dataset. Experimental results show the effectiveness\nof the proposed pipeline and selected DL models in terms of achieving fast and\naccurate validation of invoices.", "AI": {"title_translation": "一种高效的基于深度学习的票据文档验证自动化方法", "tldr": "本文提出一种基于深度学习的票据自动化验证方法，通过新建数据集和模型微调，实现了快速准确的票据验证。", "motivation": "在大型组织中，金融交易数量迅速增长，需要快速准确的多标准发票验证。然而，人工处理容易出错且耗时，且现有自动化解决方案无法支持如部分手写或手机拍摄的文档等多种约束。", "method": "本文提出使用基于最新深度学习（DL）模型的文档布局分析和目标检测技术来自动化机打发票的验证。研究引入了一个包含手动标注真实世界发票的新颖数据集和一个多标准验证过程，并对最相关的DL模型在该数据集上进行了微调和基准测试。", "result": "实验结果表明，所提出的流程和选定的深度学习模型在实现发票的快速准确验证方面是有效的。", "conclusion": "研究表明，所提出的基于深度学习的自动化票据验证方法能够有效提升验证的速度和准确性，解决了现有方法的局限性。", "translation": "在大型组织中，金融交易的数量可能迅速增长，这推动了对快速准确的多标准发票验证的需求。人工处理仍然容易出错且耗时，而当前的自动化解决方案则受限于其无法支持各种约束，例如部分手写或通过手机拍摄的文档。在本文中，我们提出使用基于最新深度学习（DL）模型的文档布局分析和目标检测技术来自动化机打发票的验证。我们引入了一个包含手动标注的真实世界发票的新颖数据集和一个多标准验证过程。我们对最相关的DL模型在我们的数据集上进行了微调和基准测试。实验结果表明，所提出的管道和选定的DL模型在实现发票快速准确验证方面的有效性。", "summary": "针对大型组织中发票验证效率低、易出错及现有自动化方案局限性，本文提出一种基于深度学习的自动化发票验证方法。该方法结合文档布局分析和目标检测技术，并构建了一个新的真实世界发票数据集进行模型训练与评估。实验证明，该方法能有效实现发票的快速准确验证。", "keywords": "发票验证, 深度学习, 文档自动化, 目标检测, 布局分析", "comments": "该研究通过引入新颖的真实世界发票数据集和结合深度学习的文档布局分析与目标检测技术，有效解决了传统人工验证的低效和现有自动化方案的局限性。其创新点在于针对发票验证的特定需求，优化了深度学习模型的应用，并验证了其在实际场景中的高效性和准确性，对企业财务自动化具有重要意义。"}}
{"id": "2503.12269", "pdf": "https://arxiv.org/pdf/2503.12269", "abs": "https://arxiv.org/abs/2503.12269", "authors": ["Negar Shahamiri", "Moritz Rempe", "Lukas Heine", "Jens Kleesiek", "Fabian Hörst"], "title": "Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net", "categories": ["cs.CV"], "comment": null, "summary": "Automatic tissue segmentation and nuclei detection is an important task in\npathology, aiding in biomarker extraction and discovery. The panoptic\nsegmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to\nimprove tissue segmentation and nuclei detection in melanoma histopathology.\nUnlike many challenge submissions focusing on extensive model tuning, our\napproach emphasizes delivering a deployable solution within a 24-hour\ndevelopment timeframe, using out-of-the-box frameworks. The pipeline combines\ntwo models, namely CellViT++ for nuclei detection and nnU-Net for tissue\nsegmentation. Our results demonstrate a significant improvement in tissue\nsegmentation, achieving a Dice score of 0.750, surpassing the baseline score of\n0.629. For nuclei detection, we obtained results comparable to the baseline in\nboth challenge tracks. The code is publicly available at\nhttps://github.com/TIO-IKIM/PUMA.", "AI": {"title_translation": "使用CellViT++和nnU-Net在24小时内攻克PUMA挑战", "tldr": "本文提出一种结合CellViT++和nnU-Net的快速部署方案，在24小时内完成PUMA挑战，并在组织分割上显著优于基线，核检测结果与基线相当。", "motivation": "自动组织分割和细胞核检测在病理学中对于生物标志物提取和发现至关重要。PUMA挑战旨在改进晚期黑色素瘤组织病理学中的细胞核和组织全景分割。", "method": "该方法强调在24小时内使用开箱即用的框架提供可部署的解决方案。该管道结合了CellViT++用于细胞核检测和nnU-Net用于组织分割。", "result": "组织分割的Dice分数达到0.750，显著超过基线分数0.629。细胞核检测结果在两个挑战轨道上与基线相当。", "conclusion": "该研究表明，即使在24小时的开发时间内使用现成框架，也能在组织分割方面取得显著进展，并在核检测方面达到基线水平，证明了其方法的有效性和实用性。", "translation": "自动组织分割和细胞核检测是病理学中的一项重要任务，有助于生物标志物的提取和发现。晚期黑色素瘤细胞核和组织全景分割（PUMA）挑战旨在改进黑色素瘤组织病理学中的组织分割和细胞核检测。与许多侧重于大量模型调优的挑战提交不同，我们的方法强调在24小时的开发时间内，使用开箱即用的框架提供可部署的解决方案。该管道结合了CellViT++（用于细胞核检测）和nnU-Net（用于组织分割）两个模型。我们的结果表明，组织分割取得了显著改进，Dice分数达到0.750，超过了基线分数0.629。对于细胞核检测，我们在两个挑战轨道上都取得了与基线相当的结果。代码已公开在https://github.com/TIO-IKIM/PUMA。", "summary": "本文提出了一种在24小时内解决PUMA挑战的实用方法，该方法将CellViT++用于细胞核检测，nnU-Net用于组织分割。与传统侧重模型调优的方法不同，本研究专注于使用开箱即用框架快速部署解决方案。结果显示，组织分割的Dice分数显著提高至0.750，超越基线0.629，而细胞核检测结果与基线相当，证明了其在时间和效果上的有效性。", "keywords": "组织分割, 细胞核检测, PUMA挑战, CellViT++, nnU-Net", "comments": "这篇论文的创新之处在于其对“24小时内提供可部署解决方案”的强调，这与通常挑战中追求极致性能但耗时耗力的模型调优形成了对比。它展示了在实际应用场景中，如何通过结合成熟的“开箱即用”框架（CellViT++和nnU-Net）在有限时间内快速取得有竞争力的结果，尤其是在组织分割方面取得了显著提升。这对于需要快速原型开发或资源受限的环境具有重要意义。"}}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.", "AI": {"title_translation": "Reflect-DiT：通过上下文反射实现文本到图像扩散Transformer的推理时缩放", "tldr": "Reflect-DiT 引入上下文反射，使文本到图像扩散模型能在推理时通过反馈改进生成，提高了性能和效率。", "motivation": "当前文本到图像生成主要依赖计算成本高的训练时缩放。推理时缩放（如best-of-N采样）效果有限且效率不高，因此需要更有效的方法来提升推理时性能。", "method": "提出 Reflect-DiT 方法，为文本到图像扩散Transformer配备上下文反射能力，使其能够利用之前生成的图像和文本反馈（描述所需改进）来细化生成，而非被动依赖随机采样。", "result": "在 GenEval 基准上，使用 SANA-1.0-1.6B 作为基础模型，性能提升 +0.19。在 GenEval 上以每提示仅生成 20 个样本的情况下，达到 0.81 的新 SOTA 分数，超越了使用更大模型（SANA-1.5-4.8B）和 2048 个样本的 best-of-N 方法获得的 0.80 分数。", "conclusion": "Reflect-DiT 通过引入上下文反射，显著提升了文本到图像扩散模型的推理时性能和效率，超越了传统的 best-of-N 采样方法。", "translation": "推进文本到图像生成的主流方法一直是训练时缩放，即在更多数据上使用更大计算资源训练更大的模型。尽管这种方法有效，但计算成本高昂，导致人们对通过推理时缩放来提高性能的兴趣日益增长。目前，文本到图像扩散模型的推理时缩放主要限于 best-of-N 采样，即每个提示生成多张图像，并由选择模型选择最佳输出。受语言领域中 DeepSeek-R1 等推理模型近期成功的启发，我们通过为文本到图像扩散Transformer配备上下文反射能力，引入了一种替代朴素 best-of-N 采样的方法。我们提出了 Reflect-DiT，该方法使扩散Transformer能够利用先前生成的图像的上下文示例以及描述必要改进的文本反馈来完善其生成。Reflect-DiT 不再被动地依赖随机采样并期望在未来的生成中获得更好的结果，而是明确地调整其生成以解决需要增强的特定方面。实验结果表明，Reflect-DiT 使用 SANA-1.0-1.6B 作为基础模型，在 GenEval 基准测试中提高了性能（+0.19）。此外，它在 GenEval 上以每提示仅生成 20 个样本的情况下，达到了 0.81 的新最先进分数，超越了之前使用明显更大的模型（SANA-1.5-4.8B）在 best-of-N 方法下使用 2048 个样本获得的 0.80 分数。", "summary": "Reflect-DiT 是一种新的文本到图像扩散Transformer推理时缩放方法，它引入了上下文反射能力，允许模型根据先前生成的图像和文本反馈来迭代优化其输出。该方法克服了传统 best-of-N 采样的局限性，实现了更精确的生成调整。实验证明，Reflect-DiT 在 GenEval 基准测试中显著提高了性能，并在更少样本数下达到了新的技术水平，展示了其在提高生成质量和效率方面的优势。", "keywords": "文本到图像生成, 扩散模型, 推理时缩放, 上下文反射, Reflect-DiT", "comments": "该论文创新性地将语言模型中的“反射”概念引入文本到图像生成领域，通过上下文反馈循环实现了推理时的性能提升，而非仅仅依赖训练时模型规模的扩大。Reflect-DiT 在效率和效果上都超越了传统的 best-of-N 采样，特别是在减少生成样本数的同时达到SOTA，具有重要的实际应用价值。"}}
{"id": "2503.12281", "pdf": "https://arxiv.org/pdf/2503.12281", "abs": "https://arxiv.org/abs/2503.12281", "authors": ["Paola Natalia Cañas", "Marcos Nieto", "Oihana Otaegui", "Igor Rodríguez"], "title": "Exploration of VLMs for Driver Monitoring Systems Applications", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 16th ITS European Congress, Seville, Spain, 19-21 May\n  2025", "summary": "In recent years, we have witnessed significant progress in emerging deep\nlearning models, particularly Large Language Models (LLMs) and Vision-Language\nModels (VLMs). These models have demonstrated promising results, indicating a\nnew era of Artificial Intelligence (AI) that surpasses previous methodologies.\nTheir extensive knowledge and zero-shot capabilities suggest a paradigm shift\nin developing deep learning solutions, moving from data capturing and algorithm\ntraining to just writing appropriate prompts. While the application of these\ntechnologies has been explored across various industries, including automotive,\nthere is a notable gap in the scientific literature regarding their use in\nDriver Monitoring Systems (DMS). This paper presents our initial approach to\nimplementing VLMs in this domain, utilising the Driver Monitoring Dataset to\nevaluate their performance and discussing their advantages and challenges when\nimplemented in real-world scenarios.", "AI": {"title_translation": "VLMs在驾驶员监控系统应用中的探索", "tldr": "本文探讨了视觉语言模型(VLMs)在驾驶员监控系统(DMS)中的初步应用，填补了该领域研究空白，并讨论了其优势和挑战。", "motivation": "近年来，大型语言模型(LLMs)和视觉语言模型(VLMs)取得了显著进展，预示着人工智能的新时代。尽管这些技术已在包括汽车在内的各个行业得到探索，但在驾驶员监控系统(DMS)中应用它们的研究存在显著空白。本文旨在填补这一空白。", "method": "本文提出了在驾驶员监控领域实现VLMs的初步方法，并利用驾驶员监控数据集评估了它们的性能，同时讨论了在实际场景中实施它们的优势和挑战。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "近年来，我们见证了新兴深度学习模型，特别是大型语言模型（LLMs）和视觉语言模型（VLMs）的显著进展。这些模型展示了可喜的成果，预示着一个超越以往方法的人工智能新时代。它们广泛的知识和零样本能力预示着深度学习解决方案开发中的范式转变，从数据捕获和算法训练转向仅仅编写合适的提示。尽管这些技术的应用已在包括汽车在内的各个行业中得到探索，但在驾驶员监控系统（DMS）中应用它们方面的科学文献存在显著空白。本文提出了我们在此领域实施VLMs的初步方法，利用驾驶员监控数据集评估它们的性能，并讨论了在实际场景中实施它们的优势和挑战。", "summary": "本文探讨了视觉语言模型（VLMs）在驾驶员监控系统（DMS）应用中的潜力，旨在弥补现有研究空白。研究提出了在DMS中应用VLMs的初步方法，并使用驾驶员监控数据集进行了性能评估，同时讨论了VLMs在实际应用中的优势与挑战。", "keywords": "VLMs, 驾驶员监控系统, 深度学习, 汽车AI", "comments": "该论文的创新之处在于首次将VLMs应用于驾驶员监控系统领域，填补了现有研究的空白。其重要性在于探索了新一代AI模型在汽车安全领域的应用潜力。然而，论文仅提出了初步方法，并未提供具体的实验结果和详细的性能数据，这可能是一个局限性。"}}
{"id": "2503.12284", "pdf": "https://arxiv.org/pdf/2503.12284", "abs": "https://arxiv.org/abs/2503.12284", "authors": ["Krzysztof Byrski", "Grzegorz Wilczyński", "Weronika Smolak-Dyżewska", "Piotr Borycki", "Dawid Baran", "Sławomir Tadeja", "Przemysław Spurek"], "title": "REdiSplats: Ray Tracing for Editable Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has become one of the most important neural rendering\nalgorithms. GS represents 3D scenes using Gaussian components with trainable\ncolor and opacity. This representation achieves high-quality renderings with\nfast inference. Regrettably, it is challenging to integrate such a solution\nwith varying light conditions, including shadows and light reflections, manual\nadjustments, and a physical engine. Recently, a few approaches have appeared\nthat incorporate ray-tracing or mesh primitives into GS to address some of\nthese caveats. However, no such solution can simultaneously solve all the\nexisting limitations of the classical GS. Consequently, we introduce\nREdiSplats, which employs ray tracing and a mesh-based representation of flat\n3D Gaussians. In practice, we model the scene using flat Gaussian distributions\nparameterized by the mesh. We can leverage fast ray tracing and control\nGaussian modification by adjusting the mesh vertices. Moreover, REdiSplats\nallows modeling of light conditions, manual adjustments, and physical\nsimulation. Furthermore, we can render our models using 3D tools such as\nBlender or Nvdiffrast, which opens the possibility of integrating them with all\nexisting 3D graphics techniques dedicated to mesh representations.", "AI": {"title_translation": "REdiSplats：可编辑高斯泼溅的射线追踪", "tldr": "REdiSplats通过结合射线追踪和基于网格的平面高斯表示，解决了传统高斯泼溅在光照条件、手动调整和物理模拟方面的局限性。", "motivation": "传统高斯泼溅（GS）难以集成可变光照条件（包括阴影和光线反射）、手动调整以及物理引擎。现有方法未能同时解决这些限制，因此需要一种能克服所有现有局限性的解决方案。", "method": "REdiSplats采用射线追踪和基于网格的平面3D高斯表示。它使用由网格参数化的平面高斯分布来建模场景，通过调整网格顶点来控制高斯修改，并利用快速射线追踪。", "result": "REdiSplats能够模拟光照条件、进行手动调整和物理模拟。此外，它可以使用Blender或Nvdiffrast等3D工具渲染模型，从而可以与所有现有的针对网格表示的3D图形技术集成。", "conclusion": "REdiSplats通过引入射线追踪和基于网格的平面高斯表示，成功克服了经典高斯泼溅在光照、编辑和物理模拟方面的挑战，并与现有3D图形生态系统兼容。", "translation": "高斯泼溅（GS）已成为最重要的神经渲染算法之一。GS使用具有可训练颜色和不透明度的高斯分量来表示3D场景。这种表示实现了高质量的渲染和快速推理。遗憾的是，将此类解决方案与变化的光照条件（包括阴影和光线反射）、手动调整以及物理引擎集成起来具有挑战性。最近，出现了一些将射线追踪或网格基元整合到GS中以解决其中一些问题的方法。然而，目前还没有任何解决方案能够同时解决经典GS的所有现有局限性。因此，我们引入了REdiSplats，它采用射线追踪和基于网格的平面3D高斯表示。在实践中，我们使用由网格参数化的平面高斯分布来建模场景。我们可以利用快速射线追踪并通过调整网格顶点来控制高斯修改。此外，REdiSplats允许对光照条件、手动调整和物理模拟进行建模。此外，我们可以使用Blender或Nvdiffrast等3D工具渲染我们的模型，这为将它们与所有现有的专用于网格表示的3D图形技术集成提供了可能性。", "summary": "REdiSplats提出了一种新的神经渲染方法，结合射线追踪和基于网格的平面3D高斯表示，以解决传统高斯泼溅在处理复杂光照、手动编辑和物理模拟方面的限制。通过将场景建模为由网格参数化的平面高斯分布，REdiSplats实现了快速射线追踪和灵活的几何控制，并能与现有3D图形工具链无缝集成。", "keywords": "高斯泼溅, 射线追踪, 可编辑性, 神经渲染, 3D图形", "comments": "REdiSplats的创新之处在于其结合了射线追踪和独特的基于网格的平面高斯表示，这有效地解决了传统高斯泼溅在动态光照、可编辑性和物理交互方面的关键局限性。其与标准3D工具（如Blender）的兼容性显著增强了其实用性和在现有图形工作流中的集成潜力，为神经渲染的实际应用开辟了新途径。"}}
{"id": "2503.12303", "pdf": "https://arxiv.org/pdf/2503.12303", "abs": "https://arxiv.org/abs/2503.12303", "authors": ["Xiaoying Zhang", "Da Peng", "Yipeng Zhang", "Zonghao Guo", "Chengyue Wu", "Chi Chen", "Wei Ke", "Helen Meng", "Maosong Sun"], "title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs", "categories": ["cs.CV"], "comment": "38 pages", "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent pre-training approaches focus on enhancing perception by training on\nhigh-quality image captions due to the extremely high cost of collecting\nchain-of-thought (CoT) reasoning data for improving reasoning. While leveraging\nadvanced MLLMs for caption generation enhances scalability, the outputs often\nlack comprehensiveness and accuracy. In this paper, we introduce Self-Improving\nCognition (SIcog), a self-learning framework designed to construct\nnext-generation foundation MLLMs by enhancing their systematic cognitive\ncapabilities through multimodal pre-training with self-generated data.\nSpecifically, we propose chain-of-description, an approach that improves an\nMLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used to\nrefine the MLLM during multimodal pre-training, facilitating next-generation\nfoundation MLLM construction. Extensive experiments on both low- and\nhigh-resolution MLLMs across diverse benchmarks demonstrate that, with merely\n213K self-generated pre-training samples, SIcog produces next-generation\nfoundation MLLMs with significantly improved cognition, achieving\nbenchmark-leading performance compared to prevalent pre-training approaches.", "AI": {"title_translation": "面向下一代基础多模态大语言模型自改进系统认知", "tldr": "SIcog是一个自学习框架，通过自生成数据进行多模态预训练，显著提升了多模态大语言模型（MLLMs）的系统认知能力，在各种基准测试中取得了领先性能。", "motivation": "尽管多模态大语言模型（MLLMs）能力强大，但在细粒度感知和复杂推理方面仍面临挑战。当前主流的预训练方法侧重于通过高质量图像字幕增强感知，但用于改善推理的思维链（CoT）推理数据收集成本极高。利用先进的MLLM生成字幕虽然提高了可扩展性，但输出常常缺乏全面性和准确性。", "method": "本文提出了自改进认知（SIcog）框架，这是一个自学习框架，通过自生成数据进行多模态预训练，增强MLLM的系统认知能力。具体而言，它提出了“描述链”（chain-of-description）方法，通过逐步视觉理解提高MLLM的系统感知能力，确保更高的全面性和准确性。此外，它采用结构化思维链（CoT）推理技术，使MLLM能够整合深度多模态推理。SIcog首先使用最少的外部标注为MLLM配备系统感知和推理能力。增强后的模型随后生成详细的字幕和CoT推理数据，这些数据通过自洽性进一步筛选。最终，这些筛选过的数据用于在多模态预训练期间优化MLLM。", "result": "通过仅213K个自生成的预训练样本，SIcog能够生产出具有显著改进认知能力的下一代基础多模态大语言模型，与主流预训练方法相比，在各种基准测试中（包括低分辨率和高分辨率MLLM）实现了领先的性能。", "conclusion": "SIcog框架通过自生成数据和多模态预训练，成功提升了多模态大语言模型的系统认知能力，有效解决了细粒度感知和复杂推理的挑战，并实现了卓越的性能。", "translation": "尽管多模态大语言模型（MLLMs）能力强大，但在细粒度感知和复杂推理方面仍面临挑战。当前主流的预训练方法侧重于通过高质量图像字幕增强感知，因为收集用于改善推理的思维链（CoT）推理数据成本极高。虽然利用先进的MLLM生成字幕增强了可扩展性，但输出常常缺乏全面性和准确性。在本文中，我们引入了自改进认知（SIcog），这是一个自学习框架，旨在通过使用自生成数据进行多模态预训练，增强其系统认知能力，从而构建下一代基础MLLM。具体而言，我们提出了“描述链”方法，通过实现逐步视觉理解来提高MLLM的系统感知能力，确保更高的全面性和准确性。此外，我们采用结构化思维链推理技术，使MLLM能够整合深度多模态推理。为了构建一个具有自改进认知的下一代基础MLLM，SIcog首先使用最少的外部标注为MLLM配备系统感知和推理能力。增强后的模型随后生成详细的字幕和CoT推理数据，这些数据通过自洽性进一步筛选。最终，这些筛选过的数据用于在多模态预训练期间优化MLLM，从而促进下一代基础MLLM的构建。在低分辨率和高分辨率MLLM上针对各种基准进行的广泛实验表明，仅使用213K个自生成的预训练样本，SIcog就能生产出具有显著改进认知能力的下一代基础MLLM，与主流预训练方法相比，实现了领先的基准性能。", "summary": "本文提出了一种名为SIcog的自学习框架，旨在构建下一代基础多模态大语言模型（MLLMs），以解决当前MLLMs在细粒度感知和复杂推理方面的不足。SIcog通过引入“描述链”实现逐步视觉理解，并结合结构化思维链推理，来增强MLLMs的系统认知能力。该框架首先利用少量外部标注赋予模型初步能力，然后由模型自生成详细的字幕和CoT推理数据，并通过自洽性进行筛选。最终，这些自生成且筛选过的数据用于多模态预训练，从而显著提升MLLMs的认知性能。实验证明，SIcog仅使用少量自生成数据即可在多个基准测试中达到领先水平。", "keywords": "多模态大语言模型, 自改进学习, 系统认知, 思维链, 数据生成", "comments": "SIcog的创新之处在于其自学习和自改进机制，通过自生成高质量的感知和推理数据来克服传统方法对昂贵人工标注数据的依赖。这种方法极大地提高了数据收集的可扩展性，并有效提升了MLLM的系统认知能力，为下一代基础模型的发展提供了新的方向。其在仅使用少量自生成数据的情况下达到领先性能，证明了其高效性和潜力。"}}
{"id": "2503.12307", "pdf": "https://arxiv.org/pdf/2503.12307", "abs": "https://arxiv.org/abs/2503.12307", "authors": ["Jiahao Wu", "Rui Peng", "Zhiyan Wang", "Lu Xiao", "Luyang Tang", "Jinbo Yan", "Kaiqiang Xiong", "Ronggang Wang"], "title": "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps://github.com/WuJH2001/swift4d.", "AI": {"title_translation": "Swift4D：用于动态场景紧凑高效重建的自适应分而治之高斯泼溅", "tldr": "Swift4D提出了一种分而治之的3D高斯泼溅方法，通过分离静态和动态基元，显著提高了动态场景重建的效率和质量，同时大幅减少了存储和训练时间。", "motivation": "现有的新视角合成方法，包括结合3D高斯泼溅的方法，在恢复高质量结果方面仍面临挑战，并且通常消耗过多的存储内存和训练时间。", "method": "Swift4D是一种分而治之的3D高斯泼溅方法，能够分别处理静态和动态基元。它采用可学习的分解策略，通过一个额外参数将基元分类为静态或动态。对于动态基元，使用紧凑的多分辨率4D哈希映射器将其从规范空间转换到变形空间，然后将静态和动态基元混合以生成最终输出。", "result": "Swift4D不仅实现了最先进的渲染质量，而且训练速度比之前的SOTA方法快20倍，在真实世界数据集上的最低存储要求仅为30MB。", "conclusion": "通过将场景基元分为静态和动态部分并分别处理，Swift4D有效地解决了动态场景重建中质量、效率和存储消耗之间的平衡问题，实现了显著的性能提升。", "translation": "新视角合成长期以来一直是一项实用但具有挑战性的任务，尽管引入了许多方法来解决这个问题，甚至结合了像3D高斯泼溅这样的先进表示，它们仍然难以恢复高质量的结果，并且通常消耗过多的存储内存和训练时间。在本文中，我们提出了Swift4D，一种分而治之的3D高斯泼溅方法，能够分别处理静态和动态基元，在渲染质量和效率之间取得了良好的平衡，其动机是场景的大部分是静态基元，不需要额外的动态特性。具体来说，我们只关注动态基元的动态变换建模，这同时有利于效率和质量。我们首先采用可学习的分解策略来分离基元，该策略依赖于一个额外参数将基元分类为静态或动态。对于动态基元，我们采用紧凑的多分辨率4D哈希映射器在每个时间戳将这些基元从规范空间转换到变形空间，然后混合静态和动态基元以生成最终输出。这种分而治之的方法有助于高效训练并减少存储冗余。我们的方法不仅实现了最先进的渲染质量，而且训练速度比以前的SOTA方法快20倍，在真实世界数据集上的最低存储要求仅为30MB。代码可在https://github.com/WuJH2001/swift4d 获得。", "summary": "Swift4D提出了一种新颖的分而治之的3D高斯泼溅方法，用于高效且紧凑地重建动态场景。该方法通过可学习的分解策略将场景基元分为静态和动态部分，并仅对动态部分进行变形建模。这种策略显著提升了渲染质量和训练效率，同时大幅降低了存储需求，实现了高质量、高效率和低存储的动态场景新视角合成。", "keywords": "3D高斯泼溅, 动态场景重建, 新视角合成, 分而治之, 效率", "comments": "Swift4D的创新点在于其“分而治之”的策略，将动态场景中的静态和动态部分分离处理，这有效地利用了场景中静态基元占主导的特性。这种方法不仅优化了动态变换的建模效率，还显著减少了存储冗余和训练时间，是3D高斯泼溅在动态场景重建领域的一个重要进步。"}}
{"id": "2503.12326", "pdf": "https://arxiv.org/pdf/2503.12326", "abs": "https://arxiv.org/abs/2503.12326", "authors": ["Maciej P. Polak", "Dane Morgan"], "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Automated data extraction from research texts has been steadily improving,\nwith the emergence of large language models (LLMs) accelerating progress even\nfurther. Extracting data from plots in research papers, however, has been such\na complex task that it has predominantly been confined to manual data\nextraction. We show that current multimodal large language models, with proper\ninstructions and engineered workflows, are capable of accurately extracting\ndata from plots. This capability is inherent to the pretrained models and can\nbe achieved with a chain-of-thought sequence of zero-shot engineered prompts we\ncall PlotExtract, without the need to fine-tune. We demonstrate PlotExtract\nhere and assess its performance on synthetic and published plots. We consider\nonly plots with two axes in this analysis. For plots identified as extractable,\nPlotExtract finds points with over 90% precision (and around 90% recall) and\nerrors in x and y position of around 5% or lower. These results prove that\nmultimodal LLMs are a viable path for high-throughput data extraction for plots\nand in many circumstances can replace the current manual methods of data\nextraction.", "AI": {"title_translation": "利用多模态大型语言模型的视觉能力实现图表中数据的自动化提取", "tldr": "该研究展示了多模态大型语言模型（MLLMs）在无需微调的情况下，通过精心设计的提示（PlotExtract）能够准确地从图表中自动化提取数据，且性能良好。", "motivation": "尽管大型语言模型（LLMs）加速了研究文本中数据提取的进展，但从研究论文的图表中提取数据仍然是一项复杂且主要依赖手动完成的任务。", "method": "该研究提出并展示了PlotExtract，一种利用当前多模态大型语言模型（MLLMs）结合适当指令和工程化工作流的方法。该方法通过一系列零样本（zero-shot）的思维链（chain-of-thought）工程化提示来实现，无需微调预训练模型。", "result": "对于被识别为可提取的图表（仅考虑双轴图），PlotExtract在点查找上实现了超过90%的精度和约90%的召回率，并且x和y位置的误差约为5%或更低。这些结果在合成和已发表的图表上得到了验证。", "conclusion": "多模态大型语言模型为图表的高通量数据提取提供了一条可行的路径，并且在许多情况下可以取代当前手动的数据提取方法。", "translation": "研究文本中的自动化数据提取一直在稳步改进，大型语言模型（LLMs）的出现进一步加速了这一进程。然而，从研究论文的图表中提取数据一直是一项复杂的任务，主要局限于手动数据提取。我们展示了当前的多模态大型语言模型，通过适当的指令和工程化工作流程，能够准确地从图表中提取数据。这种能力是预训练模型固有的，并且可以通过我们称之为PlotExtract的零样本工程化提示的思维链序列来实现，而无需进行微调。我们在此演示了PlotExtract，并评估了其在合成图和已发表图表上的性能。在此分析中，我们仅考虑具有两个轴的图表。对于被识别为可提取的图表，PlotExtract在点查找上实现了超过90%的精度（和约90%的召回率），x和y位置的误差约为5%或更低。这些结果证明，多模态LLMs是实现图表高通量数据提取的可行途径，并且在许多情况下可以取代当前手动的数据提取方法。", "summary": "该论文探讨了利用多模态大型语言模型（MLLMs）实现图表数据自动化提取的可行性。研究表明，通过名为PlotExtract的零样本思维链提示序列，无需微调，MLLMs即可准确地从图表中提取数据。该方法在双轴图表上表现出色，点查找精度和召回率均超过90%，位置误差低于5%。这证明了MLLMs在图表数据高通量提取方面的潜力，有望替代传统手动方法。", "keywords": "多模态大型语言模型, 数据提取, 图表分析, 零样本学习, PlotExtract", "comments": "这项研究的创新之处在于，它展示了现有预训练的多模态大型语言模型，在没有进行任何微调的情况下，仅通过巧妙设计的提示工程（零样本思维链）就能解决一个长期以来依赖手动且复杂的图表数据提取问题。这突显了MLLMs在处理视觉-语言任务方面的强大泛化能力和潜力，对于加速科学数据分析具有重要意义。"}}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.", "AI": {"title_translation": "CapArena：LLM时代详细图像字幕的基准测试与分析", "tldr": "CapArena是一个用于评估详细图像字幕质量的平台。研究发现，GPT-4o等领先模型在图像字幕方面已达到或超越人类水平，而大多数开源模型仍有差距。同时，传统的自动评估指标存在偏差，而VLM-as-a-Judge表现出色。基于此，CapArena-Auto被开发为一个准确高效的自动化基准测试工具。", "motivation": "随着大型语言模型（LLMs）的兴起，现代视觉语言模型（VLMs）能够生成详细全面的图像描述。然而，如何对这些详细图像字幕的质量进行基准测试仍然是一个未解决的挑战。本文旨在解决两个关键问题：当前VLM在图像字幕方面的表现如何（特别是与人类相比）？以及自动化指标能否可靠地评估详细字幕的质量？", "method": "研究构建了CapArena平台，该平台包含超过6000个人工标注的成对字幕比较和高质量的人类偏好投票，用于评估VLM的图像字幕性能。在此基础上，研究评估了传统和最新的字幕评估指标，以及“VLM作为评判者”（VLM-as-a-Judge）的能力。最后，基于这些洞察，开发并发布了CapArena-Auto，一个自动化详细字幕基准测试工具。", "result": "1. GPT-4o等领先模型在图像字幕方面达到或超越了人类表现，而大多数开源模型则落后。2. 传统评估指标（如METEOR）在字幕层面与人类有一定一致性，但在模型排名上存在系统性偏差和不一致性。3. “VLM作为评判者”在字幕和模型层面都表现出强大的辨别能力。4. CapArena-Auto实现了与人类排名94.3%的相关性，且成本仅为每次测试4美元。", "conclusion": "本研究通过构建CapArena平台揭示了当前VLM在详细图像字幕方面的表现，并验证了“VLM作为评判者”在评估详细字幕质量方面的可靠性。基于此，CapArena-Auto为详细图像字幕提供了一个准确、高效的自动化基准测试解决方案。", "translation": "图像字幕一直是视觉语言研究中的一个长期挑战。随着大型语言模型（LLMs）的兴起，现代视觉语言模型（VLMs）能够生成详细而全面的图像描述。然而，如何对这些字幕的质量进行基准测试仍然是一个未解决的问题。本文解决了两个关键问题：(1) 当前VLM在图像字幕方面的表现如何，特别是与人类相比？我们构建了CapArena，一个拥有超过6000个成对字幕比较和高质量人类偏好投票的平台。我们的竞技场式评估标志着一个里程碑，表明GPT-4o等领先模型达到了甚至超越了人类表现，而大多数开源模型则落后。(2) 自动化指标能否可靠地评估详细字幕质量？利用CapArena的人工标注数据，我们评估了传统和最新的字幕评估指标，以及“VLM作为评判者”（VLM-as-a-Judge）。我们的分析表明，虽然某些指标（例如METEOR）在字幕层面与人类表现出良好的一致性，但它们的系统性偏差导致模型排名不一致。相比之下，“VLM作为评判者”在字幕和模型层面都表现出强大的辨别能力。基于这些见解，我们发布了CapArena-Auto，一个准确高效的详细字幕自动化基准测试工具，它以每次测试仅4美元的成本实现了与人类排名94.3%的相关性。数据和资源将在https://caparena.github.io开源。", "summary": "本研究介绍了CapArena，一个用于基准测试和分析详细图像字幕的平台。通过大规模的人类偏好评估，发现GPT-4o等顶级VLM在图像字幕方面已达到或超越人类水平，而多数开源模型仍有差距。研究同时评估了自动化指标，发现传统指标存在局限性，而“VLM作为评判者”表现出卓越的评估能力。基于此，论文提出了CapArena-Auto，一个高效且准确的自动化详细字幕评估工具，实现了高人类相关性。", "keywords": "图像字幕, VLM, 基准测试, 人类评估, 自动化指标", "comments": "CapArena通过构建大规模人类偏好数据集，解决了详细图像字幕质量评估的难题，这是其主要创新点。其发现GPT-4o等模型达到人类水平，对领域发展具有重要指导意义。此外，提出VLM-as-a-Judge并开发CapArena-Auto，为未来自动化评估提供了有效且经济的解决方案，极大地推动了详细图像字幕的基准测试和研究。"}}
{"id": "2503.12332", "pdf": "https://arxiv.org/pdf/2503.12332", "abs": "https://arxiv.org/abs/2503.12332", "authors": ["Yunze Liu", "Peiran Wu", "Cheng Liang", "Junxiao Shen", "Limin Wang", "Li Yi"], "title": "VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Recent Mamba-based architectures for video understanding demonstrate\npromising computational efficiency and competitive performance, yet struggle\nwith overfitting issues that hinder their scalability. To overcome this\nchallenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework\nfeaturing a novel pre-training approach. VideoMAP uses a 4:1\nMamba-to-Transformer ratio, effectively balancing computational cost and model\ncapacity. This architecture, combined with our proposed frame-wise masked\nautoregressive pre-training strategy, delivers significant performance gains\nwhen scaling to larger models. Additionally, VideoMAP exhibits impressive\nsample efficiency, significantly outperforming existing methods with less\ntraining data. Experiments show that VideoMAP outperforms existing models\nacross various datasets, including Kinetics-400, Something-Something V2,\nBreakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a\nvisual encoder for multimodal large language models, highlighting its ability\nto reduce memory usage and enable the processing of longer video sequences. The\ncode is open-source at https://github.com/yunzeliu/MAP", "AI": {"title_translation": "VideoMAP：迈向可扩展的基于Mamba的视频自回归预训练", "tldr": "VideoMAP是一个混合Mamba-Transformer框架，通过新的预训练方法解决了Mamba模型在视频理解中过拟合和可扩展性问题，实现了更高的性能和样本效率。", "motivation": "近期用于视频理解的基于Mamba的架构展现出有前景的计算效率和有竞争力的性能，但却受困于过拟合问题，这阻碍了它们的可扩展性。", "method": "引入VideoMAP，一个混合Mamba-Transformer框架，采用4:1的Mamba-Transformer比例，有效平衡计算成本和模型容量。结合了帧级掩蔽自回归预训练策略。", "result": "1. 在扩展到更大模型时，性能显著提升。\n2. 样本效率高，使用更少训练数据优于现有方法。\n3. 在Kinetics-400, Something-Something V2, Breakfast, COIN等数据集上超越现有模型。\n4. 作为多模态大型语言模型的视觉编码器，能减少内存使用并处理更长的视频序列。", "conclusion": "VideoMAP通过其混合架构和创新的预训练方法，有效克服了Mamba模型在视频理解中的可扩展性挑战，并展示了其在多种视频理解任务和作为多模态视觉编码器方面的优越性。", "translation": "近期用于视频理解的基于Mamba的架构展现出有前景的计算效率和有竞争力的性能，但却受困于过拟合问题，这阻碍了它们的可扩展性。为了克服这一挑战，我们引入了VideoMAP，一个具有新颖预训练方法的混合Mamba-Transformer框架。VideoMAP采用4:1的Mamba-Transformer比例，有效平衡了计算成本和模型容量。这种架构，结合我们提出的帧级掩蔽自回归预训练策略，在扩展到更大模型时带来了显著的性能提升。此外，VideoMAP展现出令人印象深刻的样本效率，使用更少的训练数据显著优于现有方法。实验表明，VideoMAP在包括Kinetics-400、Something-Something V2、Breakfast和COIN在内的各种数据集上都超越了现有模型。此外，我们还展示了VideoMAP作为多模态大型语言模型视觉编码器的潜力，突出了其减少内存使用和处理更长视频序列的能力。代码已在https://github.com/yunzeliu/MAP开源。", "summary": "VideoMAP是一个混合Mamba-Transformer框架，旨在解决现有Mamba模型在视频理解中面临的过拟合和可扩展性问题。该模型采用4:1的Mamba-Transformer比例和帧级掩蔽自回归预训练策略，不仅在扩展时展现出显著的性能提升，还具有出色的样本效率。实验证明，VideoMAP在多个视频数据集上超越了现有模型，并显示出作为多模态大语言模型视觉编码器的潜力，能有效减少内存并处理长视频序列。", "keywords": "VideoMAP, Mamba, Transformer, 视频理解, 自回归预训练", "comments": "这项工作通过引入混合Mamba-Transformer架构和创新的预训练方法，有效解决了Mamba模型在视频理解中的可扩展性瓶颈。其在样本效率和处理长视频序列方面的优势，以及作为多模态LLM视觉编码器的潜力，使其在视频理解领域具有重要意义。"}}
{"id": "2503.12335", "pdf": "https://arxiv.org/pdf/2503.12335", "abs": "https://arxiv.org/abs/2503.12335", "authors": ["Tengfei Wang", "Yongmao Hou", "Zhaoning Zhang", "Yiwei Xu", "Zongqian Zhan", "Xin Wang"], "title": "GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images", "categories": ["cs.CV"], "comment": "This paper has been submitted to IROS 2025", "summary": "Accurate geometric surface reconstruction, providing essential environmental\ninformation for navigation and manipulation tasks, is critical for enabling\nrobotic self-exploration and interaction. Recently, 3D Gaussian Splatting\n(3DGS) has gained significant attention in the field of surface reconstruction\ndue to its impressive geometric quality and computational efficiency. While\nrecent relevant advancements in novel view synthesis under inconsistent\nillumination using 3DGS have shown promise, the challenge of robust surface\nreconstruction under such conditions is still being explored. To address this\nchallenge, we propose a method called GS-3I. Specifically, to mitigate 3D\nGaussian optimization bias caused by underexposed regions in single-view\nimages, based on Convolutional Neural Network (CNN), a tone mapping correction\nframework is introduced. Furthermore, inconsistent lighting across multi-view\nimages, resulting from variations in camera settings and complex scene\nillumination, often leads to geometric constraint mismatches and deviations in\nthe reconstructed surface. To overcome this, we propose a normal compensation\nmechanism that integrates reference normals extracted from single-view image\nwith normals computed from multi-view observations to effectively constrain\ngeometric inconsistencies. Extensive experimental evaluations demonstrate that\nGS-3I can achieve robust and accurate surface reconstruction across complex\nillumination scenarios, highlighting its effectiveness and versatility in this\ncritical challenge. https://github.com/TFwang-9527/GS-3I", "AI": {"title_translation": "GS-3I：用于光照不一致图像表面重建的高斯泼溅", "tldr": "GS-3I提出了一种基于3D高斯泼溅的方法，通过引入色调映射校正框架和法线补偿机制，解决了光照不一致图像下的鲁棒表面重建问题，并实现了准确的重建。", "motivation": "准确的几何表面重建对于机器人自主探索和操作至关重要。尽管3D高斯泼溅在光照不一致条件下的新视图合成方面取得了进展，但在这些条件下进行鲁棒的表面重建仍然是一个挑战。", "method": "本文提出了一种名为GS-3I的方法。为了减轻单视图图像中曝光不足区域导致的3D高斯优化偏差，引入了一个基于卷积神经网络（CNN）的色调映射校正框架。此外，针对多视图图像中光照不一致导致几何约束不匹配和重建表面偏差的问题，提出了一种法线补偿机制，该机制整合了从单视图图像中提取的参考法线与从多视图观测中计算的法线，以有效约束几何不一致性。", "result": "广泛的实验评估表明，GS-3I能够在复杂光照场景下实现鲁棒且准确的表面重建。", "conclusion": "GS-3I方法通过其提出的色调映射校正框架和法线补偿机制，成功解决了光照不一致图像下的表面重建挑战，实现了高鲁棒性和准确性。", "translation": "准确的几何表面重建为导航和操作任务提供必要的环境信息，对于实现机器人自主探索和交互至关重要。最近，3D高斯泼溅（3DGS）因其令人印象深刻的几何质量和计算效率在表面重建领域受到了广泛关注。尽管最近在利用3DGS在光照不一致条件下进行新视图合成的相关进展显示出前景，但在这种条件下进行鲁棒表面重建的挑战仍在探索中。为了解决这一挑战，我们提出了一种名为GS-3I的方法。具体而言，为了减轻单视图图像中曝光不足区域导致的3D高斯优化偏差，引入了一个基于卷积神经网络（CNN）的色调映射校正框架。此外，由于相机设置和复杂场景光照的变化导致多视图图像中光照不一致，通常会导致几何约束不匹配和重建表面的偏差。为了克服这个问题，我们提出了一种法线补偿机制，该机制整合了从单视图图像中提取的参考法线与从多视图观测中计算的法线，以有效约束几何不一致性。广泛的实验评估表明，GS-3I能够在复杂光照场景下实现鲁棒且准确的表面重建，突显了其在这一关键挑战中的有效性和多功能性。https://github.com/TFwang-9527/GS-3I", "summary": "GS-3I是一种针对光照不一致图像进行表面重建的新方法。该方法通过引入基于CNN的色调映射校正框架来处理单视图图像中的曝光不足问题，并提出法线补偿机制来解决多视图图像中光照不一致导致的几何偏差。实验证明GS-3I在复杂光照条件下能实现鲁棒准确的表面重建。", "keywords": "高斯泼溅, 表面重建, 光照不一致, 色调映射, 法线补偿", "comments": "该论文创新性地将3D高斯泼溅技术应用于光照不一致环境下的表面重建，通过引入色调映射和法线补偿机制，有效解决了传统方法在此类复杂场景中面临的挑战。其提出的解决方案具有很强的实用性，对于机器人导航、操作等领域具有重要意义。"}}
{"id": "2503.12343", "pdf": "https://arxiv.org/pdf/2503.12343", "abs": "https://arxiv.org/abs/2503.12343", "authors": ["Xiaoyu Xiong", "Changyu Hu", "Chunru Lin", "Pingchuan Ma", "Chuang Gan", "Tao Du"], "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues", "categories": ["cs.CV"], "comment": null, "summary": "We present TopoGaussian, a holistic, particle-based pipeline for inferring\nthe interior structure of an opaque object from easily accessible photos and\nvideos as input. Traditional mesh-based approaches require tedious and\nerror-prone mesh filling and fixing process, while typically output rough\nboundary surface. Our pipeline combines Gaussian Splatting with a novel,\nversatile particle-based differentiable simulator that simultaneously\naccommodates constitutive model, actuator, and collision, without interference\nwith mesh. Based on the gradients from this simulator, we provide flexible\nchoice of topology representation for optimization, including particle, neural\nimplicit surface, and quadratic surface. The resultant pipeline takes easily\naccessible photos and videos as input and outputs the topology that matches the\nphysical characteristics of the input. We demonstrate the efficacy of our\npipeline on a synthetic dataset and four real-world tasks with 3D-printed\nprototypes. Compared with existing mesh-based method, our pipeline is 5.26x\nfaster on average with improved shape quality. These results highlight the\npotential of our pipeline in 3D vision, soft robotics, and manufacturing\napplications.", "AI": {"title_translation": "TopoGaussian：从视觉线索推断内部拓扑结构", "tldr": "TopoGaussian利用照片视频推断不透明物体内部结构，比传统网格方法更快更好，解决了传统网格方法效率低、质量差的问题。", "motivation": "传统基于网格的方法在推断不透明物体内部结构时，需要繁琐且易出错的网格填充和修复过程，并且通常只输出粗糙的边界表面。", "method": "本研究提出了TopoGaussian，一个整体的、基于粒子的管道，用于从易于获取的照片和视频中推断不透明物体的内部结构。该管道将高斯泼溅（Gaussian Splatting）与一种新颖、通用的基于粒子的可微分模拟器相结合，该模拟器能同时适应本构模型、执行器和碰撞，且不干扰网格。基于模拟器的梯度，该方法为优化提供了灵活的拓扑表示选择，包括粒子、神经隐式表面和二次曲面。", "result": "所提出的管道以易于获取的照片和视频作为输入，并输出与输入物理特性匹配的拓扑结构。该方法在合成数据集和四个使用3D打印原型的真实世界任务中展示了其有效性。与现有基于网格的方法相比，TopoGaussian管道平均快5.26倍，并提高了形状质量。", "conclusion": "TopoGaussian管道在3D视觉、软机器人和制造应用中具有巨大潜力，能够高效且高质量地推断不透明物体的内部拓扑结构。", "translation": "我们提出了TopoGaussian，一个整体的、基于粒子的管道，用于从易于获取的照片和视频输入中推断不透明物体的内部结构。传统的基于网格的方法需要繁琐且易出错的网格填充和修复过程，并且通常只输出粗糙的边界表面。我们的管道将高斯泼溅（Gaussian Splatting）与一种新颖、通用的基于粒子的可微分模拟器相结合，该模拟器能同时适应本构模型、执行器和碰撞，且不干扰网格。基于该模拟器的梯度，我们为优化提供了灵活的拓扑表示选择，包括粒子、神经隐式表面和二次曲面。由此产生的管道以易于获取的照片和视频作为输入，并输出与输入物理特性匹配的拓扑结构。我们在一个合成数据集和四个使用3D打印原型的真实世界任务中展示了我们管道的有效性。与现有基于网格的方法相比，我们的管道平均快5.26倍，并提高了形状质量。这些结果突出了我们管道在3D视觉、软机器人和制造应用中的潜力。", "summary": "TopoGaussian是一种新型的基于粒子的方法，用于从照片和视频中推断不透明物体的内部拓扑结构。它克服了传统网格方法的不足，通过结合高斯泼溅和可微分粒子模拟器，实现了更快速（平均快5.26倍）和更高质量的形状重建。该方法支持多种拓扑表示，并在合成数据和真实世界任务中展现了其在3D视觉、软机器人和制造领域的应用潜力。", "keywords": "TopoGaussian, 内部拓扑, 高斯泼溅, 粒子模拟, 3D重建", "comments": "该论文提出了一种创新的基于粒子而非传统网格的方法来推断物体内部拓扑结构，解决了现有方法的效率和质量问题。其核心创新在于结合了高斯泼溅和通用的可微分粒子模拟器，这使得它能够处理复杂的物理特性而无需依赖繁琐的网格操作。速度和质量的显著提升是其重要性体现，预示着在3D视觉、软机器人和制造等多个应用领域具有广阔前景。"}}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines.", "AI": {"title_translation": "ProbDiffFlow：一种高效的免学习概率单图像光流估计框架", "tldr": "ProbDiffFlow是一种免训练的框架，通过合成未来帧并聚合光流估计，从单张图像中估计概率光流分布，解决了现有单图像光流方法对标注数据依赖和无法捕捉运动不确定性的问题，并取得了优越的性能。", "motivation": "传统光流方法需要连续帧，但数据获取或实际场景中断常导致帧不可用。现有单图像光流方法存在两大局限性：1) 依赖标注训练数据，导致其任务特异性；2) 产生确定性预测，无法捕捉运动不确定性。", "method": "本文提出了ProbDiffFlow，一个免训练的框架，用于从单张图像估计光流分布。它遵循“通过合成进行估计”的范式：首先使用基于扩散的模型生成多样且合理的未来帧，然后使用预训练的光流模型从这些合成样本中估计运动，最后将结果聚合成概率流分布。", "result": "在合成和真实世界数据集上的实验表明，ProbDiffFlow 在准确性、多样性和效率方面均表现出色，优于现有的单图像和双帧基线方法。", "conclusion": "ProbDiffFlow提供了一种高效的免学习框架，能够从单张图像估计概率光流，有效解决了现有方法的局限性，并在性能上超越了现有基线。", "translation": "本文研究了光流估计，这是运动分析中的一项关键任务，应用于自主导航、动作识别和电影制作。传统光流方法需要连续帧，但由于数据采集限制或真实世界场景中断，这些帧通常不可用。因此，单帧光流估计正在文献中兴起。然而，现有单帧方法存在两个主要局限性：(1) 它们依赖于标注训练数据，使其具有任务特异性，以及 (2) 它们产生确定性预测，未能捕捉运动不确定性。为了克服这些挑战，我们提出了ProbDiffFlow，一个免训练的框架，可以从单张图像估计光流分布。ProbDiffFlow不直接预测运动，而是遵循“通过合成进行估计”的范式：它首先使用基于扩散的模型生成多样且合理的未来帧，然后使用预训练的光流模型从这些合成样本中估计运动，最后将结果聚合成概率流分布。这种设计消除了对任务特定训练的需求，同时捕捉了多种可能的运动。在合成和真实世界数据集上的实验表明，ProbDiffFlow 在准确性、多样性和效率方面均表现出色，优于现有的单图像和双帧基线。", "summary": "ProbDiffFlow是一个创新的免训练框架，旨在解决单图像光流估计中现有方法对标注数据依赖和无法捕捉运动不确定性的问题。它采用“通过合成进行估计”的策略，利用扩散模型生成未来帧，再通过预训练模型估计运动，最终聚合成概率光流分布。该方法在准确性、多样性和效率上均优于传统基线，为单图像运动分析提供了高效且鲁棒的解决方案。", "keywords": "光流估计, 单图像, 概率性, 免训练, 扩散模型", "comments": "ProbDiffFlow的创新之处在于其“免学习”和“通过合成进行估计”的范式，避免了对大量标注训练数据的依赖，同时能够捕捉运动的不确定性，这对于实际应用中数据获取困难和需要量化不确定性的场景非常重要。其结合扩散模型和预训练光流模型的设计思路巧妙且高效。"}}
{"id": "2503.12350", "pdf": "https://arxiv.org/pdf/2503.12350", "abs": "https://arxiv.org/abs/2503.12350", "authors": ["Wenqing Kuang", "Xiongwei Zhao", "Yehui Shen", "Congcong Wen", "Huimin Lu", "Zongtan Zhou", "Xieyuanli Chen"], "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR.", "AI": {"title_translation": "ResLPR：一个用于抵抗天气腐蚀的鲁棒地点识别的LiDAR数据恢复网络和基准", "tldr": "ResLPRNet通过恢复受损LiDAR数据提高恶劣天气下的地点识别性能，并发布了新的恶劣天气LPR基准ResLPR。", "motivation": "现有的LiDAR地点识别（LPR）方法在恶劣天气下表现不佳，而LPR的抗环境腐蚀能力对自动驾驶的安全性至关重要。", "method": "提出ResLPRNet，一个基于小波变换的LiDAR数据恢复网络，用于恢复受损LiDAR扫描，并可即插即用地与预训练LPR模型集成。同时，引入ResLPR基准，包含WeatherKITTI和WeatherNCLT数据集，用于评估恶劣天气下的LPR方法。", "result": "在WeatherKITTI和WeatherNCLT数据集上的实验表明，所提出的恢复方法与多种LPR方法结合使用时，在挑战性天气场景中表现出显著的鲁棒性和性能提升。", "conclusion": "ResLPRNet有效提升了恶劣天气下LiDAR地点识别的性能，并且ResLPR基准为评估LPR方法在恶劣天气下的鲁棒性提供了重要资源。", "translation": "基于LiDAR的地点识别（LPR）是自动驾驶的关键组成部分，其对环境腐蚀的抵抗能力对于高风险应用中的安全性至关重要。尽管最先进（SOTA）的LPR方法在清洁天气下表现良好，但它们仍然难以应对驾驶场景中常见的由天气引起的腐蚀。为了解决这个问题，我们提出了ResLPRNet，一种新颖的LiDAR数据恢复网络，它通过使用基于小波变换的网络恢复受损的LiDAR扫描，从而大大增强了LPR在恶劣天气下的性能。ResLPRNet高效、轻量，可以即插即用地与预训练的LPR模型集成，而无需大量的额外计算成本。鉴于缺乏恶劣天气下的LPR数据集，我们引入了ResLPR，一个新颖的基准，用于在严酷的雪、雾和雨条件下由LiDAR畸变引起的各种LiDAR失真下检查SOTA LPR方法。我们在提出的WeatherKITTI和WeatherNCLT数据集上进行的实验表明，在挑战性天气场景中，将我们的恢复方法与多种LPR方法结合使用时，可以实现显着的弹性和显著的增益。我们的代码和基准可在此处公开获取：https://github.com/nubot-nudt/ResLPR。", "summary": "本论文提出了ResLPRNet，一个轻量高效的LiDAR数据恢复网络，旨在通过小波变换恢复受损LiDAR扫描，显著提升恶劣天气下LiDAR地点识别（LPR）的性能。鉴于恶劣天气LPR数据集的稀缺性，论文还引入了ResLPR基准，包含WeatherKITTI和WeatherNCLT数据集，用于评估不同天气条件下的SOTA LPR方法。实验证明，ResLPRNet能有效增强LPR在挑战性天气中的鲁棒性和表现。", "keywords": "LiDAR, 地点识别, 数据恢复, 恶劣天气, 基准", "comments": "该论文创新性地提出了一个用于LiDAR数据恢复的网络ResLPRNet，以解决LPR在恶劣天气下的性能下降问题。其即插即用的特性和轻量级设计增强了实用性。更重要的是，论文构建并公开了恶劣天气下的LPR基准（ResLPR），填补了该领域数据集的空白，对未来研究具有重要贡献。"}}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.", "AI": {"title_translation": "Atlas：多尺度注意力机制改进长上下文图像建模", "tldr": "Atlas引入了多尺度注意力机制（MSA）来高效建模大型图像，相比现有方法显著提升了速度并保持了竞争力。", "motivation": "高效建模海量图像是机器学习领域一个长期存在的挑战。", "method": "论文提出了多尺度注意力机制（MSA），其核心思想是多尺度表示和双向跨尺度通信。MSA创建O(log N)个尺度来表示图像，并通过交叉注意力在尺度间传播信息。在此基础上，论文引入了Atlas，一个基于MSA的新型神经网络架构。", "result": "Atlas显著改善了长上下文图像建模的计算-性能权衡。在1024像素分辨率下，Atlas-B的准确率为91.04%，与ConvNext-B（91.92%）相当但速度快4.3倍。Atlas比FasterViT快2.95倍且性能好7.38%，比LongViT快2.25倍且性能好4.96%。与MambaVision-S相比，Atlas-S在1024px、2048px和4096px下分别实现了5%、16%和32%更高的准确率，同时运行时长相似。", "conclusion": "Atlas基于多尺度注意力机制，通过显著提升速度并在保持或提高准确率的同时，有效解决了海量图像建模的挑战。", "translation": "高效建模海量图像是机器学习领域一个长期存在的挑战。为此，我们引入了多尺度注意力机制（Multi-Scale Attention, MSA）。MSA依赖于两个关键思想：（i）多尺度表示和（ii）双向跨尺度通信。MSA创建O(log N)个尺度来表示图像，通过逐步粗化的特征，并利用交叉注意力在尺度间传播信息。然后，我们引入了Atlas，一个基于MSA的新型神经网络架构。我们证明Atlas显著改善了在ImageNet 100高分辨率变体中长上下文图像建模的计算-性能权衡。在1024像素分辨率下，Atlas-B达到了91.04%的准确率，与ConvNext-B（91.92%）相当，但速度快4.3倍。Atlas比FasterViT快2.95倍，性能好7.38%；比LongViT快2.25倍，性能好4.96%。与MambaVision-S的比较中，我们发现Atlas-S在1024像素、2048像素和4096像素下分别实现了5%、16%和32%更高的准确率，同时运行时长相似。重现实验和预训练模型的代码可在https://github.com/yalalab/atlas获取。", "summary": "本文介绍了Atlas，一种利用多尺度注意力机制（MSA）来高效建模海量图像的新型神经网络架构。MSA采用多尺度表示和双向跨尺度通信进行信息传播。Atlas在长上下文图像建模中显著改善了计算-性能权衡，在高分辨率ImageNet 100上，其准确率具有竞争力，同时实现了显著的速度提升（例如，比ConvNext-B快4.3倍，且比FasterViT、LongViT和MambaVision-S拥有更好的准确率/速度）。", "keywords": "多尺度注意力, 长上下文图像建模, Atlas, ImageNet, 神经网络架构", "comments": "该论文提出了一种处理大型图像的创新方法，引入了多尺度注意力机制，有效地结合了多尺度表示和跨尺度通信。与现有最先进模型相比，其显著的性能提升（在保持或提高准确性的同时实现加速）突显了其在高分辨率图像处理方面的实际重要性。"}}
{"id": "2503.12356", "pdf": "https://arxiv.org/pdf/2503.12356", "abs": "https://arxiv.org/abs/2503.12356", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Se Young Chun"], "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025", "summary": "Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure.", "AI": {"title_translation": "文本到图像扩散模型中基于免训练门控低秩适应的局部概念擦除", "tldr": "该论文提出了一种名为GLoCE的免训练方法，利用门控低秩适应技术，选择性地从文本到图像扩散模型中擦除有害概念，同时不影响图像的其他区域，其性能优于现有方法。", "motivation": "现有的文本到图像扩散模型概念擦除方法在擦除局部目标概念时，往往会损害图像其他区域的保真度，从而降低整体图像生成性能。本研究的动机是开发一种仅删除目标概念所在图像区域，同时保持其他区域完整的方法。", "method": "本研究首先引入了一个名为局部概念擦除的框架，允许仅删除图像中包含目标概念的特定区域。作为解决方案，论文提出了一种名为“用于概念擦除的门控低秩适应”（GLoCE）的免训练方法。GLoCE通过向扩散模型注入一个轻量级模块（包含低秩矩阵和一个简单门）来实现。该门仅通过概念的几个生成步骤确定，无需训练。GLoCE直接应用于图像嵌入，并设计门仅针对目标概念激活，从而选择性地仅删除目标概念的区域，即使目标概念和其余概念在图像中同时存在。", "result": "实验结果表明，GLoCE在擦除局部目标概念后，不仅提高了图像对文本提示的保真度，而且在有效性、特异性和鲁棒性方面都大大优于现有技术，并且可以扩展到大规模概念擦除。", "conclusion": "该论文成功引入了局部概念擦除框架，并提出了一种名为GLoCE的免训练方法。GLoCE能够有效且选择性地删除目标概念，同时保持图像保真度，与现有方法相比，表现出卓越的性能和可扩展性。", "translation": "基于微调的概念擦除在防止文本到图像扩散模型生成有害内容方面表现出有希望的结果，它通过删除目标概念同时保留其余概念。为了在概念擦除后保持扩散模型的生成能力，当目标概念局部出现在图像中时，仅需删除包含目标概念的图像区域，而保持其他区域不变。然而，现有技术为了擦除特定区域中出现的局部目标概念，常常损害图像其他区域的保真度，从而降低了图像生成的整体性能。为了解决这些限制，我们首先引入了一个名为局部概念擦除的框架，它允许仅删除图像中包含目标概念的特定区域，同时保留其他区域。作为局部概念擦除的解决方案，我们提出了一种名为“用于概念擦除的门控低秩适应”（GLoCE）的免训练方法，该方法将一个轻量级模块注入到扩散模型中。GLoCE由低秩矩阵和一个简单门组成，仅通过概念的几个生成步骤确定，无需训练。通过将GLoCE直接应用于图像嵌入并设计门仅针对目标概念激活，GLoCE可以有选择地仅删除目标概念的区域，即使目标概念和其余概念在图像中同时存在。大量实验表明，GLoCE不仅在擦除局部目标概念后提高了图像对文本提示的保真度，而且在有效性、特异性和鲁棒性方面都大大优于现有技术，并且可以扩展到大规模概念擦除。", "summary": "该论文引入了“局部概念擦除”框架，旨在选择性地从文本到图像扩散模型中移除有害概念，而不损害图像的其他区域。为此，论文提出了一种新颖的免训练方法GLoCE，它通过向扩散模型注入一个轻量级的门控低秩适应模块。GLoCE通过基于生成步骤激活门来有效定位并擦除特定概念区域，与现有微调方法相比，显著提高了图像保真度、有效性、特异性和鲁棒性，并且可以扩展到大规模概念擦除。", "keywords": "局部概念擦除, 扩散模型, 免训练, 低秩适应, GLoCE", "comments": "该论文提出了一种创新的免训练方法（GLoCE）用于局部概念擦除，这相对于以往的微调方法是一个重大改进，因为后者常常会降低整体图像质量。在不进行大量再训练或不影响其他图像区域的情况下选择性地移除概念的能力，对于生成式AI的安全性和控制至关重要。其可扩展到大规模概念擦除的特性进一步增强了其实际应用价值。"}}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "AI": {"title_translation": "L2COcc：通过激光雷达模型蒸馏实现的轻量级以相机为中心的语义场景补全", "tldr": "本文提出L2COcc，一个轻量级相机中心语义场景补全框架，通过高效体素变换器和跨模态知识蒸馏（如FSD、TPVD、PAD）显著降低计算负担，同时在SemanticKITTI和SSCBench-KITTI-360基准上超越现有视觉方法，并减少超过23%的内存和推理时间。", "motivation": "语义场景补全（SSC）是自动驾驶感知系统的关键组成部分，但现有方法通常采用计算和内存密集型的3D操作，对训练和测试平台造成显著的计算负担。", "method": "本文提出了L2COcc，一个轻量级的以相机为中心的语义场景补全框架，该框架也支持激光雷达输入。方法中包含高效体素变换器（EVT）和跨模态知识模块，如特征相似度蒸馏（FSD）、TPV蒸馏（TPVD）和预测对齐蒸馏（PAD），以在保持高精度的同时显著降低计算负担。", "result": "实验评估表明，所提出的方法在SemanticKITTI和SSCBench-KITTI-360基准上，分别在精度方面超越了当前最先进的基于视觉的SSC方法。此外，该方法更轻量化，与现有最先进方法相比，内存消耗和推理时间均减少了23%以上。", "conclusion": "L2COcc成功地在保持高精度的前提下，显著降低了语义场景补全任务的计算负担和资源消耗，并在多个基准上表现优于现有方法，为自动驾驶感知系统提供了一种更高效的解决方案。", "translation": "语义场景补全（SSC）是自动驾驶感知系统中的关键组成部分，其任务是从传感器数据中推断场景的3D语义占据信息。为了提高精度，以往的研究采用了各种计算量大、内存密集型的3D操作，这在训练和测试期间对平台施加了显著的计算要求。本文提出了L2COcc，一个轻量级的以相机为中心的SSC框架，该框架也支持激光雷达输入。凭借我们提出的高效体素变换器（EVT）和跨模态知识模块，包括特征相似度蒸馏（FSD）、TPV蒸馏（TPVD）和预测对齐蒸馏（PAD），我们的方法在保持高精度的同时显著降低了计算负担。实验评估表明，我们提出的方法在SemanticKITTI和SSCBench-KITTI-360基准上，分别在精度方面超越了当前最先进的基于视觉的SSC方法。此外，我们的方法更轻量化，与现有最先进方法相比，内存消耗和推理时间均减少了23%以上。代码可在我们的项目页面获取：https://studyingfufu.github.io/L2COcc/。", "summary": "本文提出L2COcc，一个轻量级且以相机为中心的语义场景补全（SSC）框架，同时支持激光雷达输入。为解决现有SSC方法计算和内存开销大的问题，L2COcc引入了高效体素变换器（EVT）和多项跨模态知识蒸馏技术（FSD、TPVD、PAD）。实验结果表明，L2COcc在SemanticKITTI和SSCBench-KITTI-360基准上，精度超越了当前最先进的视觉SSC方法，并在内存和推理时间上实现了超过23%的显著优化。", "keywords": "语义场景补全, 轻量级, 知识蒸馏, 相机中心, 激光雷达", "comments": "这篇论文的创新点在于提出了一个轻量级的相机中心SSC框架，并通过结合高效体素变换器和多种知识蒸馏技术（特别是跨模态蒸馏，利用LiDAR模型知识）来解决现有方法计算资源消耗大的问题。其重要性在于为自动驾驶等实时应用提供了更高效、更实用的语义场景补全方案，在保证高精度的同时大幅降低了资源开销。"}}
{"id": "2503.12381", "pdf": "https://arxiv.org/pdf/2503.12381", "abs": "https://arxiv.org/abs/2503.12381", "authors": ["Ruchika Sharma", "Rudresh Dwivedi"], "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN", "categories": ["cs.CV", "cs.MM"], "comment": "Submiited to journal", "summary": "Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision.", "AI": {"title_translation": "基于优化混合模型的深度伪造检测：通过改进RCNN的耳部生物特征描述符", "tldr": "提出一种基于耳部生物特征和优化混合模型（DBN+Bi-GRU+改进RCNN）的深度伪造检测方法，该方法在准确性、特异性和精确度方面表现优异，超越了传统模型。", "motivation": "深度伪造技术被广泛用于创建虚假内容，如假新闻、电影和谣言，且持续演进，使得区分深度伪造和人工篡改图像变得具有挑战性，因此对持续识别和预防深度伪造的调查至关重要。", "method": "该方法引入通过检测细微耳部运动和形状变化来生成耳部描述符。提出一种新颖的优化混合深度伪造检测模型，该模型通过增强RCNN考虑耳部生物特征描述符。初始步骤包括将输入视频转换为帧，进行预处理（调整大小、归一化、灰度转换和过滤），然后使用Viola-Jones技术进行人脸检测。接下来，利用包含DBN（深度信念网络）和Bi-GRU（双向门控循环单元）的混合模型，基于耳部描述符进行深度伪造检测。检测阶段的输出通过改进的分数级融合确定。为提高性能，使用SU-JFO（自升级水母优化方法）优化调整了两个检测模型的权重。", "result": "实验在压缩、噪声、旋转、姿态和光照四种场景下，于三个不同数据集上进行。性能结果表明，所提出的方法在准确性、特异性和精确度等各项性能指标上优于传统的CNN、SqueezeNet、LeNet、LinkNet、LSTM、DFP和ResNext+CNN+LSTM模型。", "conclusion": "性能结果证实我们提出的方法在各项性能指标上优于传统模型。", "translation": "近年来，深度伪造技术被广泛用于通过改变和替换来自各种来源的面部信息来创建有害内容，例如假新闻、电影和谣言。鉴于深度伪造的持续演进，对持续识别和预防的研究至关重要。由于人工智能（AI）的最新技术进步，区分深度伪造和人工篡改图像变得具有挑战性。本方法引入了对细微耳部运动和形状变化的鲁棒检测，以生成耳部描述符。此外，我们还提出了一种新颖的优化混合深度伪造检测模型，该模型通过增强RCNN（基于区域的卷积神经网络）考虑耳部生物特征描述符。最初，输入视频被转换为帧，并通过调整大小、归一化、灰度转换和过滤过程进行预处理，然后使用Viola-Jones技术进行人脸检测。接下来，利用包含DBN（深度信念网络）和Bi-GRU（双向门控循环单元）的混合模型，基于耳部描述符进行深度伪造检测。检测阶段的输出通过改进的分数级融合确定。为了提高性能，两个检测模型的权重使用SU-JFO（自升级水母优化方法）进行了优化调整。实验基于压缩、噪声、旋转、姿态和光照四种场景，在三个不同数据集上进行。性能结果证实，我们提出的方法在准确性、特异性和精确度等各项性能指标上优于传统的模型，如CNN（卷积神经网络）、SqueezeNet、LeNet、LinkNet、LSTM（长短期记忆）、DFP（深度伪造预测器）[1]和ResNext+CNN+LSTM [2]。", "summary": "该论文提出一种新颖的基于耳部生物特征的优化混合模型，用于深度伪造检测。该方法通过分析耳部细微运动和形状变化生成描述符，并结合DBN和Bi-GRU的混合模型进行检测，同时利用改进RCNN增强特征提取。通过SU-JFO优化模型权重，并在多种场景和数据集上进行实验，结果显示该方法在准确性、特异性和精确度方面显著优于现有传统模型。", "keywords": "深度伪造检测, 耳部生物特征, 混合模型, RCNN, 优化算法", "comments": "该论文的创新点在于引入耳部生物特征作为深度伪造检测的独特且潜在的有效线索，这可能为应对日益复杂的深度伪造技术提供了一个新的视角，尤其是在面部信息被高度篡改的情况下。同时，结合多种深度学习技术（DBN、Bi-GRU、改进RCNN）与优化算法（SU-JFO）构建混合模型，增强了检测的鲁棒性和准确性。这项研究为深度伪造检测领域带来了新的思路和方法。"}}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO.", "AI": {"title_translation": "RENO：用于3D LiDAR点云的实时神经压缩", "tldr": "RENO是首个用于3D LiDAR点云的实时神经编解码器，实现了卓越的性能和轻量级模型。", "motivation": "尽管基于学习的神经模型在LiDAR点云压缩（LPCC）任务中取得了显著进展，但实现实时压缩仍然是一个艰巨的挑战，而实时压缩是众多工业应用不可或缺的标准。", "method": "本文提出了RENO，它跳过八叉树构建，直接基于多尺度稀疏张量表示。RENO设计了稀疏占用码，利用跨尺度关联并以一次性方式推导体素的占用，从而大大节省了处理时间。", "result": "RENO实现了实时编码速度，在桌面平台（例如一台RTX 3090 GPU）上，编码和解码过程均达到10 fps（14位深度）。在相似质量下，RENO比G-PCCv23和Draco分别节省了12.25%和48.34%的比特率。RENO模型大小仅为1MB。", "conclusion": "RENO是第一个用于3D LiDAR点云的实时神经编解码器，通过其创新的稀疏占用码和直接构建在多尺度稀疏张量上的方法，实现了卓越的实时性能和显著的比特率节省，同时保持了轻量级模型，使其非常适合实际应用。", "translation": "尽管基于学习的神经模型在LiDAR点云压缩（LPCC）任务中取得了显著进展，但实现实时压缩——众多工业应用不可或缺的标准——仍然是一个艰巨的挑战。本文提出了RENO，这是首个用于3D LiDAR点云的实时神经编解码器，以轻量级模型实现了卓越的性能。RENO跳过八叉树构建，直接基于多尺度稀疏张量表示。RENO没有采用多阶段推断，而是设计了稀疏占用码，利用跨尺度关联并以一次性方式推导体素的占用，大大节省了处理时间。实验结果表明，所提出的RENO在桌面平台（例如一台RTX 3090 GPU）上，编码和解码过程均实现了实时编码速度，即14位深度下10 fps，同时在相似质量下，比G-PCCv23和Draco分别节省了12.25%和48.34%的比特率。RENO模型大小仅为1MB，使其对实际应用极具吸引力。源代码可在https://github.com/NJUVISION/RENO获取。", "summary": "本文介绍了RENO，一种开创性的实时神经编解码器，专为3D LiDAR点云压缩设计。它通过跳过八叉树构建并直接利用多尺度稀疏张量表示，以及引入独特的稀疏占用码来实现高效的单次体素占用推断，从而显著缩短处理时间。RENO模型轻量级（1MB），在实现实时编码速度（10 fps）的同时，展现出优于现有方法的比特率节省（相比G-PCCv23节省12.25%，相比Draco节省48.34%）。", "keywords": "LiDAR点云压缩, 实时, 神经编解码器, RENO, 稀疏占用码", "comments": "RENO的创新之处在于其直接构建于多尺度稀疏张量表示，并引入了稀疏占用码，从而实现了单次推断和显著的时间节省。其在保持高压缩效率的同时实现了实时性能，且模型尺寸极小，这对于需要实时处理和部署的工业应用具有重要意义。它是LiDAR点云压缩领域的一个重要突破。"}}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "AI": {"title_translation": "VRsketch2Gaussian：VR草图引导的3D高斯泼溅三维物体生成", "tldr": "本文提出了VRSketch2Gaussian，一个首个VR草图引导的多模态原生3D对象生成框架，结合3D高斯泼溅表示，并引入了首个大规模配对数据集VRSS，实现了高质量的VR草图引导3D生成。", "motivation": "现有研究在多模态VR草图引导的3D对象生成方面存在空白，缺乏大规模配对数据集，导致VR草图嵌入与丰富CLIP嵌入之间存在域差距，且难以实现细粒度的几何和外观控制。", "method": "本文提出了VRSketch2Gaussian框架，该框架是一个VR草图引导、多模态、原生3D对象生成框架，并结合3D高斯泼溅表示。同时，引入了首个大规模配对数据集VRSS，包含VR草图、文本、图像和3DGS。关键创新包括：1) Sketch-CLIP特征对齐：提出两阶段对齐策略，弥合稀疏VR草图嵌入和丰富CLIP嵌入之间的域差距。2) 细粒度多模态条件化：使用VR草图进行几何条件化，文本描述进行外观控制，并提出可泛化VR草图编码器来对齐不同模态。3) 高效高保真3D原生生成：利用3D原生生成方法实现快速且纹理丰富的3D对象合成。", "result": "在VRSS数据集上进行的实验表明，该方法实现了高质量、多模态的VR草图引导3D生成。", "conclusion": "VRSketch2Gaussian方法和VRSS数据集将对3D生成社区非常有益。", "translation": "我们提出了VRSketch2Gaussian，这是一个首个VR草图引导的、多模态的、原生3D对象生成框架，它结合了3D高斯泼溅表示。作为我们工作的一部分，我们引入了VRSS，这是第一个包含VR草图、文本、图像和3DGS的大规模配对数据集，弥合了多模态VR草图生成中的空白。我们的方法具有以下关键创新：1) Sketch-CLIP特征对齐。我们提出了一个两阶段对齐策略，弥合了稀疏VR草图嵌入和丰富CLIP嵌入之间的域差距，促进了VR草图检索和生成任务。2) 细粒度多模态条件化。我们通过使用显式VR草图进行几何条件化和文本描述进行外观控制来解耦3D生成过程。为了实现这一点，我们提出了一个可泛化的VR草图编码器，可以有效地对齐不同的模态。3) 高效高保真3D原生生成。我们的方法利用3D原生生成方法，实现了快速且纹理丰富的3D对象合成。在我们的VRSS数据集上进行的实验表明，我们的方法实现了高质量、多模态的VR草图引导3D生成。我们相信我们的VRSS数据集和VRSketch2Gaussian方法将对3D生成社区有益。", "summary": "本文提出了VRSketch2Gaussian，一个首个VR草图引导的多模态原生3D对象生成框架，该框架集成了3D高斯泼溅表示。为弥合多模态VR草图生成中的空白，作者引入了VRSS，一个包含VR草图、文本、图像和3DGS的大规模配对数据集。该方法具有三项关键创新：Sketch-CLIP特征对齐，通过两阶段策略弥合VR草图与CLIP嵌入的域差距；细粒度多模态条件化，利用VR草图进行几何控制，文本描述进行外观控制；以及高效高保真3D原生生成。实验证明该方法能够实现高质量的VR草图引导3D生成。", "keywords": "VR草图生成, 3D高斯泼溅, 多模态, 3D对象生成, 数据集", "comments": "这篇论文的创新点在于提出了首个结合VR草图和3D高斯泼溅的多模态3D对象生成框架VRSketch2Gaussian，并构建了首个大规模多模态配对数据集VRSS，有效弥合了VR草图与多模态数据之间的鸿沟。其细粒度控制几何和外观的能力以及高效的3D原生生成方法，对3D内容创作领域具有重要意义。"}}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000.", "AI": {"title_translation": "Car-1000：一个新型大规模细粒度视觉分类数据集", "tldr": "本文介绍了Car-1000，一个针对汽车模型细粒度识别的新型大规模数据集，旨在解决现有数据集的不足。", "motivation": "细粒度视觉分类（FGVC）在计算机视觉中是一项重要任务，尤其在汽车模型识别方面具有显著的应用价值。然而，现有的Stanford-Car数据集类别数量有限（196种），且仅包含2013年之前的车型，无法满足汽车行业快速发展带来的日益复杂和精致的车型识别需求。", "method": "为了解决现有数据集的局限性，本文引入了Car-1000数据集。该数据集专门为各种汽车模型的细粒度视觉分类而设计，涵盖了来自165个不同汽车制造商的1000种独特汽车模型。此外，作者还在Car-1000数据集上复现了几种最先进的FGVC方法，以建立该领域的新基准。", "result": "引入了Car-1000数据集，包含165个汽车制造商的1000种独特汽车模型。在Car-1000数据集上建立了新的细粒度视觉分类基准，复现了多个最先进的FGVC方法。", "conclusion": "Car-1000数据集为未来的细粒度视觉分类研究提供了新的视角和资源，旨在推动汽车模型识别领域的发展。", "translation": "细粒度视觉分类（FGVC）是计算机视觉中一项具有挑战性但意义重大的任务，旨在识别鸟类、汽车、飞机等不同子类别。其中，识别不同汽车模型在自动驾驶、交通监控和场景理解方面具有显著的应用价值，在过去几年中受到了相当大的关注。然而，作为最广泛使用的汽车识别细粒度数据集，Stanford-Car仅有196个不同类别，且只包含2013年之前生产的车型。由于近年来汽车行业的快速发展，各种汽车模型的外观变得日益复杂和精致。因此，之前的Stanford-Car数据集未能捕捉到这种不断演变的变化，无法满足汽车行业的需求。为了应对这些挑战，在我们的论文中，我们引入了Car-1000，一个专门为各种汽车模型的细粒度视觉分类设计的大规模数据集。Car-1000包含来自165个不同汽车制造商的车辆，涵盖了1000种独特的汽车模型。此外，我们还在Car-1000数据集上复现了几种最先进的FGVC方法，为该领域的研究建立了新的基准。我们希望我们的工作能为未来的FGVC研究人员提供一个全新的视角。我们的数据集可在https://github.com/toggle1995/Car-1000获取。", "summary": "本文介绍了Car-1000，一个新型大规模细粒度视觉分类数据集，旨在解决现有Stanford-Car数据集在汽车模型识别方面类别过少和数据过时的问题。Car-1000包含来自165个汽车制造商的1000种独特汽车模型，并在此数据集上建立了新的FGVC基准，复现了多种先进方法，以期为该领域的研究提供新的视角。", "keywords": "细粒度视觉分类, 汽车识别, 大规模数据集, Car-1000, 计算机视觉", "comments": "Car-1000数据集的创新之处在于其大规模和对最新车型的覆盖，这直接解决了现有数据集的局限性。其重要性体现在为细粒度汽车识别提供了一个更具挑战性和相关性的基准，有望推动自动驾驶、交通监控和场景理解等领域的发展。通过提供一个包含1000种独特车型的丰富数据集，该工作显著促进了细粒度视觉分类研究的进步。"}}
{"id": "2503.12399", "pdf": "https://arxiv.org/pdf/2503.12399", "abs": "https://arxiv.org/abs/2503.12399", "authors": ["Jiangdong Cai", "Yan Chen", "Zhenrong Shen", "Haotian Jiang", "Honglin Xiong", "Kai Xuan", "Lichi Zhang", "Qian Wang"], "title": "Pathology Image Restoration via Mixture of Prompts", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In digital pathology, acquiring all-in-focus images is essential to\nhigh-quality imaging and high-efficient clinical workflow. Traditional scanners\nachieve this by scanning at multiple focal planes of varying depths and then\nmerging them, which is relatively slow and often struggles with complex tissue\ndefocus. Recent prevailing image restoration technique provides a means to\nrestore high-quality pathology images from scans of single focal planes.\nHowever, existing image restoration methods are inadequate, due to intricate\ndefocus patterns in pathology images and their domain-specific semantic\ncomplexities. In this work, we devise a two-stage restoration solution\ncascading a transformer and a diffusion model, to benefit from their powers in\npreserving image fidelity and perceptual quality, respectively. We particularly\npropose a novel mixture of prompts for the two-stage solution. Given initial\nprompt that models defocus in microscopic imaging, we design two prompts that\ndescribe the high-level image semantics from pathology foundation model and the\nfine-grained tissue structures via edge extraction. We demonstrate that, by\nfeeding the prompt mixture to our method, we can restore high-quality pathology\nimages from single-focal-plane scans, implying high potentials of the mixture\nof prompts to clinical usage. Code will be publicly available at\nhttps://github.com/caijd2000/MoP.", "AI": {"title_translation": "基于混合提示的病理图像修复", "tldr": "该论文提出了一种通过结合Transformer和扩散模型的两阶段修复方案，并引入新颖的“混合提示”来从单焦平面扫描中恢复高质量病理图像，解决了现有方法的不足。", "motivation": "在数字病理学中，获取全聚焦图像对于高质量成像和高效临床工作流程至关重要。传统扫描方法速度慢，且难以处理复杂的组织散焦。现有的图像修复方法由于病理图像中复杂的散焦模式及其领域特定的语义复杂性而显得不足。", "method": "本文设计了一个两阶段修复解决方案，该方案将Transformer和扩散模型串联起来，分别利用它们在保持图像保真度和感知质量方面的优势。特别地，提出了一种新颖的“混合提示”用于此两阶段解决方案。给定模拟显微成像中散焦的初始提示，设计了两个提示：一个描述来自病理学基础模型的高级图像语义，另一个通过边缘提取描述细粒度组织结构。将这些混合提示输入到所提出的方法中。", "result": "通过将混合提示输入到所提出的方法中，可以从单焦平面扫描中恢复高质量的病理图像。", "conclusion": "混合提示在临床使用中具有很高的潜力。", "translation": "在数字病理学中，获取全聚焦图像对于高质量成像和高效临床工作流程至关重要。传统扫描仪通过在不同深度的多个焦平面进行扫描然后合并来实现这一点，这种方法相对较慢，并且经常难以处理复杂的组织散焦。最近流行的图像修复技术提供了一种从单焦平面扫描中恢复高质量病理图像的方法。然而，由于病理图像中复杂的散焦模式及其领域特定的语义复杂性，现有的图像修复方法不足。在这项工作中，我们设计了一个两阶段修复解决方案，该方案将Transformer和扩散模型串联起来，分别利用它们在保持图像保真度和感知质量方面的优势。我们特别提出了一种新颖的“混合提示”用于此两阶段解决方案。给定模拟显微成像中散焦的初始提示，我们设计了两个提示，分别描述来自病理学基础模型的高级图像语义和通过边缘提取的细粒度组织结构。我们证明，通过将混合提示输入到我们的方法中，我们可以从单焦平面扫描中恢复高质量的病理图像，这意味着混合提示在临床使用中具有很高的潜力。代码将在 https://github.com/caijd2000/MoP 公开。", "summary": "针对数字病理图像中复杂的散焦和语义问题，本文提出了一种新颖的两阶段图像修复方案。该方案结合了Transformer和扩散模型，并通过引入“混合提示”来增强修复效果。混合提示包含初始散焦模型、来自病理学基础模型的高级语义信息以及通过边缘提取获得的细粒度结构信息。实验证明，该方法能有效从单焦平面扫描中恢复高质量病理图像，显示出其在临床应用中的巨大潜力。", "keywords": "病理图像修复, 散焦, 混合提示, Transformer, 扩散模型", "comments": "该论文的创新点在于提出了一个结合Transformer和扩散模型的两阶段架构，并特别引入了“混合提示”机制来处理病理图像的复杂性。这种混合提示通过整合领域特定的高级语义（来自基础模型）和细粒度结构（边缘提取），有效地解决了传统方法在处理复杂散焦和语义细节方面的不足。这项工作对于提高数字病理图像的质量和效率，加速临床诊断流程具有重要意义，尤其是在单焦平面扫描场景下，有望显著优化工作流程。"}}
{"id": "2503.12401", "pdf": "https://arxiv.org/pdf/2503.12401", "abs": "https://arxiv.org/abs/2503.12401", "authors": ["Jianwei Zhao", "Xin Li", "Fan Yang", "Qiang Zhai", "Ao Luo", "Yang Zhao", "Hong Cheng", "Huazhu Fu"], "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks.", "AI": {"title_translation": "MExD：一种用于全玻片图像分类的专家注入扩散模型", "tldr": "MExD是一个结合MoE和扩散模型的生成式全玻片图像（WSI）分类方法，旨在解决WSI尺寸大、噪声和数据不平衡问题，并在多个基准测试中达到了最先进的性能。", "motivation": "全玻片图像（WSI）分类面临挑战，包括图像尺寸巨大、存在大量非信息区域引入噪声，以及特征聚合时导致数据不平衡。", "method": "本文提出MExD（专家注入扩散模型），结合了专家混合（MoE）机制和扩散模型。MExD通过一种新颖的基于MoE的聚合器平衡补丁特征分布，选择性地强调相关信息，有效过滤噪声，解决数据不平衡，并提取关键特征。然后，这些特征通过基于扩散的生成过程集成，直接生成WSI的类别分布。MExD是WSI分类中首个生成式策略。", "result": "MExD在三个广泛使用的基准测试（Camelyon16、TCGA-NSCLC和BRACS）上进行了验证，在二分类和多分类任务中均持续实现了最先进的性能。", "conclusion": "MExD作为首个在WSI分类中采用生成式策略的模型，能有效处理WSI的挑战，并取得了卓越的分类性能。", "translation": "全玻片图像（WSI）分类由于图像尺寸巨大和存在大量非信息区域而带来独特的挑战，这些问题在特征聚合过程中引入噪声并导致数据不平衡。为了解决这些问题，我们提出了MExD，一种专家注入扩散模型，它结合了专家混合（MoE）机制和扩散模型的优势，以增强分类能力。MExD通过一种新颖的基于MoE的聚合器来平衡补丁特征分布，该聚合器选择性地强调相关信息，有效过滤噪声，解决数据不平衡，并提取基本特征。然后，这些特征通过基于扩散的生成过程进行整合，直接产生WSI的类别分布。MExD超越了传统的判别式方法，代表了WSI分类中的第一个生成式策略，能够捕获细粒度细节，从而获得鲁棒和精确的结果。我们的MExD在三个广泛使用的基准测试——Camelyon16、TCGA-NSCLC和BRACS上进行了验证，在二分类和多分类任务中均持续实现了最先进的性能。", "summary": "本文提出MExD，一种用于全玻片图像（WSI）分类的专家注入扩散模型，旨在解决WSI分类中图像尺寸大、噪声多和数据不平衡等问题。MExD结合了MoE机制和扩散模型，通过MoE聚合器有效过滤噪声、平衡特征分布并提取关键特征，再通过扩散过程生成WSI的类别分布。作为WSI分类中的首个生成式策略，MExD在多个基准测试中展现出卓越的SOTA性能。", "keywords": "全玻片图像分类, 扩散模型, 专家混合, 生成式策略, 癌症诊断", "comments": "MExD的创新之处在于它是WSI分类领域中首次引入生成式策略的模型，而非传统的判别式方法。它通过结合MoE机制和扩散模型，有效地解决了WSI固有的巨大尺寸、非信息区域噪声和数据不平衡等挑战，显著提升了分类的鲁棒性和精确度，并在多个基准测试中取得了SOTA成果，具有重要的研究价值和潜在的临床应用前景。"}}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "AI": {"title_translation": "SAM2-ELNet：遥感图像分割的标签增强与自动标注", "tldr": "SAM2-ELNet通过结合标签增强、边缘注意力机制和SAM2骨干网络，解决了遥感图像分割中手动标注数据质量差的问题，提升了分割性能，并展现了自动标注的潜力。", "motivation": "现有遥感图像分割模型高度依赖高质量标注数据，但手动标注成本高、耗时、易受主观干扰，导致标签边界失真和细节丢失。此外，遥感标注数据稀缺，限制了传统神经网络的特征提取能力。", "method": "提出SAM2-ELNet，一个边缘增强的标注网络，包含标注模块和边缘注意力机制，以解决标签细节丢失、碎片化和边界不准确问题。该方法使用预训练的自监督大模型SAM2的Hiera骨干网络作为编码器，通过微调在小样本下实现高质量和高效的特征提取。", "result": "在Deep-SAR Oil Spill (SOS) 数据集上的实验表明，使用增强标签训练的模型表现更好，最终损失更低，更接近真实数据分布。", "conclusion": "SAM2-ELNet通过标签增强显著提升了遥感图像分割的性能，并展现了将其扩展为高效自动标注框架的潜力，有助于大规模遥感图像解译和智能识别。", "translation": "遥感图像分割对于环境监测、灾害评估和资源管理至关重要，直接影响地表信息提取的准确性和效率。现有监督模型在遥感图像分割任务中的性能高度依赖于标签数据的质量。然而，目前的标签数据主要依靠人工标注，这带来了高昂的时间成本并受到主观干扰，导致标签边界失真并常常丢失细节。为了解决上述问题，我们的工作提出了一个边缘增强的标注网络，称为SAM2-ELNet，它结合了标注模块和边缘注意力机制。该模型有效解决了标签细节丢失、碎片化和边界不准确等问题。由于手动标注的遥感数据稀缺，传统神经网络的特征提取能力受到限制。我们的方法使用预训练的自监督大模型Segment Anything Model 2 (SAM2) 的Hiera骨干网络作为编码器，即使在小样本情况下也能通过在下游任务上进行微调实现高质量和高效的特征提取。本研究比较了原始标签和增强标签在手动标注的Deep-SAR Oil Spill (SOS) 数据集上的训练效果。结果显示，使用增强标签训练的模型表现更好，最终损失更低，表明与真实数据分布更接近。我们的工作还通过泛化实验探索了将模型扩展为高效自动标注框架的潜力，从而促进大规模遥感图像解译和智能识别。", "summary": "本文提出SAM2-ELNet，一个用于遥感图像分割的边缘增强标注网络，旨在解决手动标注数据质量差、细节丢失和边界不准确等问题。该模型结合了标注模块和边缘注意力机制，并利用预训练SAM2的Hiera骨干网络作为编码器，以在小样本情况下实现高效特征提取。实验结果表明，使用SAM2-ELNet增强的标签训练的模型在性能上优于使用原始标签训练的模型，并且最终损失更低。此外，该研究还探索了将该模型扩展为高效自动标注框架的潜力，以促进大规模遥感图像解译和智能识别。", "keywords": "遥感图像分割, 标签增强, 自动标注, SAM2, 边缘注意力", "comments": "该研究的创新点在于结合了标签增强和边缘注意力机制来优化遥感图像分割中的标签质量问题，并巧妙地利用了预训练的SAM2模型作为强大的特征提取器，解决了小样本数据下的特征提取限制。这对于减少手动标注成本、提高分割精度和推动大规模遥感应用具有重要意义。其提出的自动标注框架具有广阔的应用前景。"}}
{"id": "2503.12418", "pdf": "https://arxiv.org/pdf/2503.12418", "abs": "https://arxiv.org/abs/2503.12418", "authors": ["Shuo Gao", "Jingyang Zhang", "Jun Xue", "Meng Yang", "Yang Chen", "Guangquan Zhou"], "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, conference", "summary": "Carotid atherosclerosis represents a significant health risk, with its early\ndiagnosis primarily dependent on ultrasound-based assessments of carotid\nintima-media thickening. However, during carotid ultrasound screening,\nsignificant view variations cause style shifts, impairing content cues related\nto thickening, such as lumen anatomy, which introduces spurious correlations\nthat hinder assessment. Therefore, we propose a novel causal-inspired method\nfor assessing carotid intima-media thickening in frame-wise ultrasound videos,\nwhich focuses on two aspects: eliminating spurious correlations caused by style\nand enhancing causal content correlations. Specifically, we introduce a novel\nSpurious Correlation Elimination (SCE) module to remove non-causal style\neffects by enforcing prediction invariance with style perturbations.\nSimultaneously, we propose a Causal Equivalence Consolidation (CEC) module to\nstrengthen causal content correlation through adversarial optimization during\ncontent randomization. Simultaneously, we design a Causal Transition\nAugmentation (CTA) module to ensure smooth causal flow by integrating an\nauxiliary pathway with text prompts and connecting it through contrastive\nlearning. The experimental results on our in-house carotid ultrasound video\ndataset achieved an accuracy of 86.93\\%, demonstrating the superior performance\nof the proposed method. Code is available at\n\\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}.", "AI": {"title_translation": "一种因果启发的超声视频颈动脉内膜中层厚度评估模型", "tldr": "该研究提出了一种新的因果启发方法，用于评估超声视频中的颈动脉内膜中层厚度，通过消除风格引起的虚假相关性并增强因果内容相关性，在内部数据集上实现了86.93%的准确率。", "motivation": "颈动脉粥样硬化是一种严重的健康风险，早期诊断主要依赖超声评估颈动脉内膜中层厚度。然而，超声筛查中显著的视角变化会导致风格偏移，损害与增厚相关的内容线索（如管腔解剖），从而引入阻碍评估的虚假相关性。", "method": "本研究提出了一种新颖的因果启发方法，用于评估逐帧超声视频中的颈动脉内膜中层厚度。该方法侧重于两个方面：消除由风格引起的虚假相关性，以及增强因果内容相关性。具体而言，引入了虚假相关消除（SCE）模块，通过强制预测对风格扰动的不变性来消除非因果风格效应。同时，提出了因果等效整合（CEC）模块，通过内容随机化期间的对抗性优化来增强因果内容相关性。此外，设计了因果转换增强（CTA）模块，通过整合带有文本提示的辅助路径并通过对比学习连接，确保平滑的因果流。", "result": "在内部颈动脉超声视频数据集上的实验结果显示，所提出的方法实现了86.93%的准确率，证明了其卓越的性能。", "conclusion": "该研究提出的因果启发模型在颈动脉内膜中层厚度评估中表现出卓越的性能，有效克服了超声视频中风格变化引起的虚假相关性问题。", "translation": "颈动脉粥样硬化代表着重大的健康风险，其早期诊断主要依赖于基于超声的颈动脉内膜中层厚度评估。然而，在颈动脉超声筛查过程中，显著的视角变化会导致风格偏移，损害与增厚相关的内容线索，例如管腔解剖，这引入了阻碍评估的虚假相关性。因此，我们提出了一种新颖的因果启发方法，用于评估逐帧超声视频中的颈动脉内膜中层厚度，该方法侧重于两个方面：消除由风格引起的虚假相关性并增强因果内容相关性。具体而言，我们引入了一种新颖的虚假相关消除（SCE）模块，通过强制预测对风格扰动的不变性来消除非因果风格效应。同时，我们提出了因果等效整合（CEC）模块，通过内容随机化期间的对抗性优化来加强因果内容相关性。同时，我们设计了一个因果转换增强（CTA）模块，通过整合带有文本提示的辅助路径并通过对比学习连接，以确保平滑的因果流。在我们的内部颈动脉超声视频数据集上的实验结果显示，准确率达到了86.93%，证明了所提出方法的卓越性能。代码可在https://github.com/xielaobanyy/causal-imt 获取。", "summary": "本研究针对超声视频中颈动脉内膜中层厚度评估受风格变化导致虚假相关性影响的问题，提出了一种新型因果启发模型。该模型包含虚假相关消除（SCE）、因果等效整合（CEC）和因果转换增强（CTA）三个模块，分别用于去除非因果风格效应、增强因果内容相关性以及确保因果流的平滑性。实验结果表明，该方法在颈动脉超声视频数据集上取得了86.93%的准确率，验证了其在复杂医学影像评估中的优越性能。", "keywords": "颈动脉内膜中层厚度, 超声视频, 因果推理, 虚假相关性, 深度学习", "comments": "该论文的创新点在于提出了一个因果启发的模型来解决超声视频中内膜中层厚度评估中风格变化导致的虚假相关性问题。通过引入SCE、CEC和CTA模块，系统性地处理了风格干扰和因果内容增强，为医学影像分析中的鲁棒性评估提供了新的思路。其方法论具有通用性，可能对其他受风格或背景因素影响的医学图像分析任务产生积极影响。"}}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture.", "AI": {"title_translation": "EgoEvGesture：基于第一人称事件相机的手势识别", "tldr": "提出了一种名为EgoEvGesture的新型网络架构，专门用于处理事件相机数据，以实现第一人称手势识别，解决了传统RGB方案和现有事件处理方法的局限性，并在新数据集上取得了SOTA性能。", "motivation": "传统的基于RGB的方案在动态场景下存在运动模糊和光照变化的问题。事件相机虽然在处理高动态范围和超低功耗方面有优势，但现有的基于RGB的架构无法有效处理异步事件流，且第一人称视角下的事件数据包含头部运动和手势，增加了识别复杂性。", "method": "提出了一种新的网络架构EgoEvGesture，包含：1) 带有非对称深度可分离卷积的轻量级CNN，以减少参数并保留时空特征；2) 即插即用的状态空间模型作为上下文块，解耦头部运动噪声和手势动态；3) 无参数的Bins-Temporal Shift Module (BSTM)，沿bin和时间维度移动特征以有效融合稀疏事件。此外，构建了EgoEvGesture数据集，这是首个用于第一人称事件相机手势识别的大规模数据集。", "result": "在异构测试中，该方法以7M参数实现了62.7%的准确率，比现有最先进方法高3.1%。在DVS128 Gesture数据集上，准确率达到96.97%，展示了强大的跨数据集泛化能力。自由式运动中的误分类主要源于高个体间变异性和与训练数据不同的未见测试模式。", "conclusion": "该研究提出了一种有效处理第一人称事件相机数据的手势识别网络架构EgoEvGesture，并在新的大规模数据集和现有数据集上验证了其优越的性能和泛化能力，克服了传统方法在动态场景下的局限性以及事件数据处理的复杂性。", "translation": "第一人称手势识别是增强自然人机交互的关键技术，但传统的基于RGB的解决方案在动态场景中存在运动模糊和光照变化的问题。虽然事件相机在处理高动态范围和超低功耗方面表现出独特的优势，但现有的基于RGB的架构由于其同步基于帧的特性，在处理异步事件流时面临固有的局限性。此外，从第一人称视角来看，事件相机记录的数据既包括头部运动产生的事件，也包括手势产生的事件，从而增加了手势识别的复杂性。为了解决这个问题，我们提出了一种专门为事件数据处理设计的新型网络架构，该架构包含：(1) 一个带有非对称深度可分离卷积的轻量级CNN，以减少参数同时保留时空特征；(2) 一个即插即用的状态空间模型作为上下文块，将头部运动噪声与手势动态解耦；以及(3) 一个无参数的Bins-Temporal Shift Module (BSTM)，该模块沿bin和时间维度移动特征以有效融合稀疏事件。我们进一步构建了EgoEvGesture数据集，这是第一个使用事件相机进行第一人称手势识别的大规模数据集。实验结果表明，我们的方法在异构测试中仅用7M参数就达到了62.7%的准确率，比最先进的方法高出3.1%。自由式运动中显著的错误分类源于高个体间变异性和与训练数据不同的未见测试模式。此外，我们的方法在DVS128 Gesture数据集上取得了96.97%的显著准确率，展示了强大的跨数据集泛化能力。数据集和模型已公开，网址为https://github.com/3190105222/EgoEv_Gesture。", "summary": "该论文提出了一种名为EgoEvGesture的新型网络架构，专门用于第一人称事件相机手势识别。针对传统RGB方案的局限性以及事件数据处理的复杂性，EgoEvGesture集成了轻量级CNN、解耦头部运动的状态空间模型和高效融合稀疏事件的BSTM。研究还构建了首个大规模第一人称事件手势数据集。实验证明，该方法在异构测试中以低参数量超越SOTA，并在跨数据集泛化能力上表现出色。", "keywords": "事件相机, 手势识别, 第一人称, 神经网络, 数据集", "comments": "该论文的创新点在于提出了一个专门处理第一人称事件相机数据的网络架构，有效解决了传统方法的局限性和事件数据处理的复杂性。特别是通过引入状态空间模型解耦头部运动噪声和构建大规模事件手势数据集，对该领域的发展具有重要意义。其低参数量和高准确率的结合，预示着其在实际应用中的潜力。"}}
{"id": "2503.12441", "pdf": "https://arxiv.org/pdf/2503.12441", "abs": "https://arxiv.org/abs/2503.12441", "authors": ["Yuda Zou", "Zelong Liu", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "title": "Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "Crowd counting and localization are important in applications such as public\nsecurity and traffic management. Existing methods have achieved impressive\nresults thanks to extensive laborious annotations. This paper propose a novel\npoint-localization-based semi-supervised crowd counting and localization method\ntermed Consistent-Point. We identify and address two inconsistencies of\npseudo-points, which have not been adequately explored. To enhance their\nposition consistency, we aggregate the positions of neighboring auxiliary\nproposal-points. Additionally, an instance-wise uncertainty calibration is\nproposed to improve the class consistency of pseudo-points. By generating more\nconsistent pseudo-points, Consistent-Point provides more stable supervision to\nthe training process, yielding improved results. Extensive experiments across\nfive widely used datasets and three different labeled ratio settings\ndemonstrate that our method achieves state-of-the-art performance in crowd\nlocalization while also attaining impressive crowd counting results. The code\nwill be available.", "AI": {"title_translation": "Consistent-Point：半监督人群计数与定位中的一致伪点", "tldr": "本文提出了一种名为Consistent-Point的半监督人群计数和定位方法，通过解决伪点的不一致性问题，实现了最先进的性能。", "motivation": "人群计数和定位在公共安全和交通管理等应用中至关重要。现有方法虽然效果显著，但高度依赖耗时耗力的大量标注。本文旨在通过半监督学习减少对标注数据的依赖，并解决伪点在半监督学习中存在的未充分探索的不一致性问题。", "method": "本文提出了一种新颖的基于点定位的半监督人群计数和定位方法，名为Consistent-Point。该方法识别并解决了伪点的两种不一致性问题。为增强位置一致性，作者聚合了相邻辅助提议点的位置。此外，还提出了一种实例级不确定性校准来提高伪点的类别一致性。通过生成更一致的伪点，Consistent-Point为训练过程提供了更稳定的监督。", "result": "在五个广泛使用的数据集和三种不同的标注比例设置下进行了广泛实验，结果表明Consistent-Point在人群定位方面取得了最先进的性能，同时在人群计数方面也取得了令人印象深刻的结果。", "conclusion": "Consistent-Point通过解决伪点的一致性问题，为半监督人群计数和定位提供了一种有效且高性能的解决方案，显著减少了对大量标注数据的依赖，并在多个数据集上达到了最先进的性能。", "translation": "人群计数和定位在公共安全和交通管理等应用中至关重要。现有方法由于大量费力的标注而取得了令人印象深刻的结果。本文提出了一种名为Consistent-Point的新型基于点定位的半监督人群计数和定位方法。我们识别并解决了伪点的两种不一致性问题，这些问题尚未得到充分探索。为了增强它们的位置一致性，我们聚合了相邻辅助提议点的位置。此外，还提出了一种实例级不确定性校准来提高伪点的类别一致性。通过生成更一致的伪点，Consistent-Point为训练过程提供了更稳定的监督，从而产生了改进的结果。在五个广泛使用的数据集和三种不同标注比例设置下进行的广泛实验表明，我们的方法在人群定位方面取得了最先进的性能，同时在人群计数方面也取得了令人印象深刻的结果。代码将可用。", "summary": "Consistent-Point是一种新颖的半监督人群计数和定位方法，旨在解决现有方法对大量标注的依赖。该方法通过聚合相邻提议点和引入实例级不确定性校准，解决了伪点的位置和类别一致性问题，从而生成更稳定的监督信号。实验证明，Consistent-Point在人群定位上达到了最先进水平，并在人群计数上也表现出色。", "keywords": "人群计数, 人群定位, 半监督学习, 伪点, 一致性", "comments": "本文的创新点在于系统地识别并解决了半监督学习中伪点存在的两种关键不一致性问题，并通过具体的策略（聚合相邻提议点和实例级不确定性校准）有效提升了伪点的质量。这使得模型能在更少的标注数据下获得更稳定的训练和更好的性能，对于减少数据标注成本具有重要意义。提供代码也增加了其可复现性和实用价值。"}}
{"id": "2503.12446", "pdf": "https://arxiv.org/pdf/2503.12446", "abs": "https://arxiv.org/abs/2503.12446", "authors": ["Tianle Li", "Yongming Rao", "Winston Hu", "Yu Cheng"], "title": "BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Encoder-free multimodal large language models(MLLMs) eliminate the need for a\nwell-trained vision encoder by directly processing image tokens before the\nlanguage model. While this approach reduces computational overhead and model\ncomplexity, it often requires large amounts of training data to effectively\ncapture the visual knowledge typically encoded by vision models like CLIP. The\nabsence of a vision encoder implies that the model is likely to rely on\nsubstantial data to learn the necessary visual-semantic alignments. In this\nwork, we present BREEN, a data-efficient encoder-free multimodal architecture\nthat mitigates this issue. BREEN leverages a learnable query and image experts\nto achieve comparable performance with significantly less training data. The\nlearnable query, positioned between image and text tokens, is supervised by the\noutput of a pretrained CLIP model to distill visual knowledge, bridging the gap\nbetween visual and textual modalities. Additionally, the image expert processes\nimage tokens and learnable queries independently, improving efficiency and\nreducing interference with the LLM's textual capabilities. BREEN achieves\ncomparable performance to prior encoder-free state-of-the-art models like\nMono-InternVL, using only 13 million text-image pairs in training about one\npercent of the data required by existing methods. Our work highlights a\npromising direction for data-efficient encoder-free multimodal learning,\noffering an alternative to traditional encoder-based approaches.", "AI": {"title_translation": "BREEN：利用可学习查询桥接数据高效的无编码器多模态学习", "tldr": "无编码器多模态大语言模型（MLLMs）通常需要大量训练数据。BREEN通过引入可学习查询和图像专家，实现了数据高效的无编码器多模态学习，仅用现有方法约1%的数据量就达到了可比的性能。", "motivation": "无编码器多模态大语言模型（MLLMs）虽然减少了计算开销和模型复杂性，但由于缺少视觉编码器，通常需要大量的训练数据来有效地捕获视觉知识，以实现视觉-语义对齐。", "method": "本文提出了BREEN，一种数据高效的无编码器多模态架构。BREEN利用一个可学习查询，该查询位于图像和文本标记之间，并由预训练CLIP模型的输出进行监督，以蒸馏视觉知识，从而弥合视觉和文本模态之间的差距。此外，图像专家独立处理图像标记和可学习查询，提高了效率并减少了对LLM文本能力的干扰。", "result": "BREEN在训练中仅使用了1300万个文本-图像对，达到了与现有最先进的无编码器模型（如Mono-InternVL）相当的性能，而这仅仅是现有方法所需数据量的约百分之一。", "conclusion": "BREEN的工作展示了数据高效的无编码器多模态学习一个有前景的方向，为传统的基于编码器的方法提供了一种替代方案。", "translation": "无编码器多模态大语言模型（MLLMs）通过在语言模型之前直接处理图像标记，消除了对训练有素的视觉编码器的需求。虽然这种方法减少了计算开销和模型复杂性，但它通常需要大量的训练数据才能有效地捕获通常由CLIP等视觉模型编码的视觉知识。视觉编码器的缺失意味着模型可能需要依赖大量数据来学习必要的视觉-语义对齐。在这项工作中，我们提出了BREEN，一种数据高效的无编码器多模态架构，它缓解了这个问题。BREEN利用可学习查询和图像专家，以显著更少的训练数据实现可比的性能。可学习查询位于图像和文本标记之间，由预训练CLIP模型的输出进行监督，以蒸馏视觉知识，弥合视觉和文本模态之间的差距。此外，图像专家独立处理图像标记和可学习查询，提高了效率并减少了对LLM文本能力的干扰。BREEN仅使用1300万个文本-图像对进行训练，就达到了与Mono-InternVL等现有无编码器最先进模型相当的性能，这大约是现有方法所需数据量的百分之一。我们的工作突出了数据高效无编码器多模态学习的一个有前景的方向，为传统的基于编码器的方法提供了一种替代方案。", "summary": "本文介绍了BREEN，一种数据高效的无编码器多模态大语言模型架构，旨在解决传统无编码器MLLMs对大量训练数据依赖的问题。BREEN通过引入一个由预训练CLIP模型监督的可学习查询来蒸馏视觉知识，并利用图像专家独立处理图像和查询，从而提高了效率并减少了对语言模型的影响。实验结果表明，BREEN仅使用现有方法所需数据量的约1%（1300万对文本-图像数据）即可达到与现有最先进无编码器模型相当的性能，为数据高效的多模态学习提供了一条新途径。", "keywords": "无编码器多模态学习, 数据高效, 可学习查询, 图像专家, CLIP蒸馏", "comments": "BREEN的创新点在于其在无编码器多模态学习中实现了显著的数据效率，通过引入可学习查询和图像专家，有效地弥补了缺少专用视觉编码器带来的数据依赖问题。这种方法为构建更轻量级、更易于训练的多模态模型提供了有价值的思路，尤其是在数据资源有限的场景下具有重要意义。它提供了一个传统基于编码器方法的有前景的替代方案。"}}
{"id": "2503.12447", "pdf": "https://arxiv.org/pdf/2503.12447", "abs": "https://arxiv.org/abs/2503.12447", "authors": ["Li Yicong"], "title": "Causality Model for Semantic Understanding on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "PhD Thesis", "summary": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA).", "AI": {"title_translation": "视频语义理解的因果模型", "tldr": "本论文探讨了在视频理解中，通过引入因果模型来解决数据不平衡和分布偏移导致的深度神经网络性能下降问题，并将其应用于视频关系检测和视频问答任务。", "motivation": "经过十年的发展，视频理解领域面临瓶颈，单纯依赖海量数据和复杂架构不再是万能解决方案。普遍存在的数据不平衡阻碍了深度神经网络有效学习潜在的因果机制，导致在遇到如长尾不平衡和扰动不平衡等分布偏移时性能显著下降。这促使研究人员寻求捕捉视频数据中因果模式的替代方法，以提高深度神经网络的鲁棒性。", "method": "本论文聚焦于语义视频理解领域，探索因果建模的潜力，将其作为一种原则来发现观察到的相关性背后的真实因果模式，以解决上述挑战并提高深度神经网络的鲁棒性。具体应用于两个基本任务：视频关系检测（VidVRD）和视频问答（VideoQA）。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "经过十年的繁荣发展，视频理解的发展已达到一个关键的十字路口，仅仅依赖海量数据和复杂架构不再是适用于所有情况的通用解决方案。普遍存在的数据不平衡阻碍了深度神经网络有效学习潜在的因果机制，导致在遇到如长尾不平衡和扰动不平衡等分布偏移时性能显著下降。这一认识促使研究人员寻求替代方法来捕捉视频数据中的因果模式。为了应对这些挑战并提高深度神经网络的鲁棒性，因果建模作为一种原则应运而生，旨在发现观察到的相关性背后的真实因果模式。本论文专注于语义视频理解领域，探讨了因果建模在推进两个基本任务方面的潜力：视频关系检测（VidVRD）和视频问答（VideoQA）。", "summary": "当前视频理解领域面临深度神经网络因数据不平衡和分布偏移导致性能下降的挑战。为解决这一问题并提高模型鲁棒性，本论文提出引入因果建模来发现视频数据中潜在的因果模式。研究将因果模型应用于语义视频理解，并具体探讨其在视频关系检测（VidVRD）和视频问答（VideoQA）这两个核心任务上的应用潜力。", "keywords": "因果模型, 视频理解, 语义理解, 数据不平衡, 鲁棒性", "comments": "这篇论文的创新点在于将因果建模引入视频理解领域，以解决当前深度学习方法在处理数据不平衡和分布偏移时的局限性。通过关注因果关系而非仅仅是相关性，有望显著提升模型在复杂真实世界场景中的鲁棒性和泛化能力。这是一个重要的研究方向，尤其是在视频数据日益复杂和多样化的背景下。"}}
{"id": "2503.12450", "pdf": "https://arxiv.org/pdf/2503.12450", "abs": "https://arxiv.org/abs/2503.12450", "authors": ["Feihong Yan", "Qingyan Wei", "Jiayi Tang", "Jiajun Li", "Yulin Wang", "Xuming Hu", "Huiqi Li", "Linfeng Zhang"], "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.", "AI": {"title_translation": "惰性MAR：通过特征缓存加速掩码自回归模型", "tldr": "LazyMAR通过利用令牌和条件冗余引入新的缓存机制，显著加速了掩码自回归（MAR）模型，实现了近3倍的加速且不损失生成质量。", "motivation": "掩码自回归（MAR）模型在图像生成中具有潜力，但其双向自注意力与传统KV缓存机制冲突，导致计算瓶颈，未能实现预期的效率。", "method": "本文提出了LazyMAR，通过利用两种新型冗余（令牌冗余和条件冗余）来解决MAR模型的计算瓶颈。令牌冗余指相邻解码步骤中大量令牌表示相似，可缓存重用；条件冗余指分类器自由引导中条件与无条件输出的差异在相邻步骤中相似。LazyMAR引入两种相应的缓存机制，该方法是免训练且即插即用的。", "result": "LazyMAR实现了2.83倍的加速，并且生成质量几乎没有下降。", "conclusion": "LazyMAR通过引入新的缓存机制，有效解决了MAR模型的计算瓶颈，显著提高了其效率，同时保持了生成质量，为MAR模型的实际应用提供了重要的优化。", "translation": "掩码自回归（MAR）模型作为图像生成中一种有前景的方法，通过利用并行解码能力，有望在计算效率上超越传统的自回归模型。然而，它们对双向自注意力的依赖与传统的KV缓存机制固有冲突，从而产生了意想不到的计算瓶颈，削弱了其预期的效率。为了解决这个问题，本文通过利用两种冗余来研究MAR的缓存机制：令牌冗余表明，在相邻的解码步骤中，大部分令牌具有非常相似的表示，这使得我们能够首先在之前的步骤中缓存它们，然后在后续步骤中重用它们。条件冗余表明，在分类器自由引导中，条件输出和无条件输出之间的差异在相邻步骤中表现出非常相似的值。基于这两种冗余，我们提出了LazyMAR，它引入了两种缓存机制来逐一处理它们。LazyMAR对于所有MAR模型都是免训练和即插即用的。实验结果表明，我们的方法在几乎没有生成质量下降的情况下，实现了2.83倍的加速。我们的代码将在https://github.com/feihongyan1/LazyMAR发布。", "summary": "本文提出了LazyMAR，一个针对掩码自回归（MAR）模型的加速框架，旨在解决其因双向自注意力与传统KV缓存冲突而导致的计算效率瓶颈。LazyMAR通过识别并利用令牌冗余和条件冗余这两种新型冗余，设计了相应的缓存机制。该方法无需训练，即插即用，实验证明能将MAR模型的速度提升2.83倍，同时几乎不影响生成质量。", "keywords": "掩码自回归模型, 特征缓存, 计算加速, 令牌冗余, 条件冗余", "comments": "这篇论文通过识别并利用MAR模型中特有的两种冗余（令牌冗余和条件冗余），巧妙地解决了其双向自注意力与传统KV缓存机制的冲突，从而显著提升了计算效率。其“免训练”和“即插即用”的特性极大地降低了应用门槛，对于MAR模型在实际应用中的推广具有重要意义。这是一个非常实用的优化工作。"}}
{"id": "2503.12451", "pdf": "https://arxiv.org/pdf/2503.12451", "abs": "https://arxiv.org/abs/2503.12451", "authors": ["Hossein Ranjbar", "Alireza Taheri"], "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons.", "AI": {"title_translation": "ISLR101：一个伊朗词级手语识别数据集", "tldr": "ISLR101是首个公开的伊朗手语数据集，包含4614个视频，用于孤立手语识别。研究者构建了基于视觉和骨骼的基线模型，分别达到了97.01%和94.02%的准确率。", "motivation": "手语识别领域面临数据资源不足的挑战，尤其对于伊朗手语，缺乏公开可用的数据集，这阻碍了相关研究和开发。", "method": "研究者创建了ISLR101数据集，包含4614个视频，涵盖101个不同手语词汇，由10名不同手语者（3名聋人、2名手语翻译、5名二语学习者）在不同背景下录制，分辨率为800x600，帧率为25fps。数据还包括使用OpenPose提取的骨骼姿态信息。在此基础上，建立了基于视觉外观和基于骨骼的基线模型，并在ISLR101上进行训练和评估。", "result": "ISLR101数据集包含4614个视频，涵盖101个独立手语词汇。基于视觉外观的基线模型在测试集上达到97.01%的准确率，而基于骨骼的基线模型达到94.02%的准确率。此外，还发布了训练集、验证集和测试集划分，以便进行公平比较。", "conclusion": "ISLR101数据集是首个公开可用的伊朗手语数据集，填补了该领域的数据空白，并为未来的伊朗手语识别研究提供了重要的基准和资源。", "translation": "手语识别涉及对复杂多通道信息的建模，例如手形和动作，同时依赖于足够的手语特定数据。然而，手语通常资源不足，给该领域的研究和开发带来了重大挑战。为了解决这一差距，我们引入了ISLR101，这是第一个公开可用的用于孤立手语识别的伊朗手语数据集。这个综合数据集包括4614个视频，涵盖101个不同的手语词汇，由10名不同的手语者（3名聋人、2名手语翻译和5名二语学习者）在不同背景下录制，分辨率为800x600像素，帧率为25帧/秒。它还包括使用OpenPose提取的骨骼姿态信息。我们建立了基于视觉外观和基于骨骼的框架作为基线模型，并在ISLR101上对其进行了彻底的训练和评估。这些模型在测试集上分别达到了97.01%和94.02%的准确率。此外，我们发布了训练、验证和测试集划分，以促进公平比较。", "summary": "该论文介绍了ISLR101，这是首个公开的伊朗词级手语识别数据集，旨在解决手语资源匮乏的问题。该数据集包含4614个视频，涵盖101个独立手语，由10名不同手语者录制，并提供了骨骼姿态信息。研究者在此数据集上构建并评估了基于视觉和骨骼的基线模型，分别取得了97.01%和94.02%的准确率，并公开了数据集划分以促进公平研究。", "keywords": "伊朗手语, 手语识别, 数据集, ISLR101, 骨骼姿态", "comments": "ISLR101数据集的发布对伊朗手语识别领域具有重要意义，它填补了该领域的数据空白，为研究人员提供了急需的资源。数据集的多样性（不同手语者、背景）和包含骨骼信息是其亮点。提供的基线模型和数据集划分将极大促进后续研究的开展和比较。"}}
{"id": "2503.12453", "pdf": "https://arxiv.org/pdf/2503.12453", "abs": "https://arxiv.org/abs/2503.12453", "authors": ["Edgar Heinert", "Thomas Gottwald", "Annika Mütze", "Matthias Rottmann"], "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs.", "AI": {"title_translation": "通过线索分解评估图像分类和分割中的形状偏差和鲁棒性", "tldr": "本文提出了一种新的评估程序，包括一种无AI的线索分解方法和一种新的线索分解形状偏差评估指标，并引入了一个鲁棒性指标。实验结果表明，在图像分类中与现有方法一致，但在鲁棒性评估方面表现优越，并首次揭示了语义分割DNN的偏差。", "motivation": "以往研究深度神经网络（DNNs）对图像内容（如纹理和形状）的感知偏差的方法通常基于风格迁移，且仅限于图像分类DNN。本文旨在提供一种新的评估程序来克服这些限制。", "method": "本文提出了一种新的评估程序，包括：1) 一种线索分解方法，包含两种无AI的数据预处理方法，分别提取形状和纹理线索；2) 一种利用线索分解数据的新型线索分解形状偏差评估指标。此外，还引入了一个相应的线索分解鲁棒性指标，用于估计DNN对图像损坏的鲁棒性。", "result": "在数值实验中，本文在图像分类DNNs偏差方面的发现与以前的评估指标一致。然而，本文的线索分解鲁棒性指标在估计DNN鲁棒性方面显示出优越的结果。此外，本文首次揭示了语义分割数据集Cityscapes和ADE20k上DNNs的偏差。", "conclusion": "本文提出的线索分解方法和评估指标在图像分类偏差评估方面与现有方法一致，但在鲁棒性评估方面表现更优，并且首次成功地揭示了语义分割DNN的偏差。", "translation": "以前的工作研究了深度神经网络（DNNs）如何根据它们对不同图像线索（如纹理和形状）的偏见来感知图像内容。以前测量形状和纹理偏见的方法通常基于风格迁移，并且仅限于图像分类的DNN。在这项工作中，我们提供了一种新的评估程序，包括：1）一种线索分解方法，其中包含两种无AI的数据预处理方法，分别提取形状和纹理线索；2）一种新颖的线索分解形状偏差评估指标，该指标利用了线索分解数据。为了应用目的，我们引入了一个相应的线索分解鲁棒性指标，该指标允许估计DNN对图像损坏的鲁棒性。在我们的数值实验中，我们在图像分类DNNs偏差方面的发现与以前的评估指标一致。然而，我们的线索分解鲁棒性指标在估计DNN鲁棒性方面显示出优越的结果。此外，我们首次在语义分割数据集Cityscapes和ADE20k上对DNNs进行的结果揭示了语义分割DNNs的偏差。", "summary": "本文提出了一种新的深度神经网络（DNN）形状偏差和鲁棒性评估方法，以克服现有风格迁移方法的局限性及其对图像分类的限制。该方法包含两种无AI的数据预处理技术，用于提取形状和纹理线索，以及一种新的线索分解形状偏差评估指标。此外，还引入了线索分解鲁棒性指标，用于评估DNN对图像损坏的鲁棒性。实验结果表明，该方法在图像分类偏差评估方面与现有方法一致，但在鲁棒性评估方面表现更优，并且首次成功地揭示了语义分割DNN的偏差。", "keywords": "形状偏差, 鲁棒性评估, 线索分解, 图像分类, 语义分割", "comments": "本文的创新之处在于提出了无AI的线索分解方法，摆脱了以往风格迁移方法的限制，并首次将形状偏差和鲁棒性评估扩展到语义分割任务，为理解DNN的感知机制提供了新的视角和更广泛的应用。"}}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD.", "AI": {"title_translation": "探索指代表达计数中的上下文属性密度", "tldr": "现有的指代表达计数（REC）算法在理解细粒度属性方面存在挑战。本文提出了“上下文属性密度”（CAD）的概念，并设计了一个U形CAD估计器，显著提升了REC的性能。", "motivation": "先前的指代表达计数（REC）算法在细粒度属性理解方面面临挑战，难以准确对齐属性信息与视觉模式，其局限性源于对“上下文属性密度”（CAD）的探索不足。", "method": "本文将“上下文属性密度”（CAD）定义为视觉区域中特定细粒度属性的信息强度。为此，提出了一种U形CAD估计器，其中指代表达和来自GroundingDINO的多尺度视觉特征可以相互作用。通过额外的密度监督，模型可以有效地编码CAD，并通过一个带有CAD精炼查询的新颖注意力过程进行解码。", "result": "本文提出的框架显著优于最先进的REC方法，在计数指标上实现了30%的错误减少，在定位精度上实现了10%的提升。", "conclusion": "上下文属性密度对于指代表达计数（REC）具有重要意义，本文提出的方法有效地利用了这一概念来提升性能。", "translation": "指代表达计数（REC）算法旨在为各种细粒度文本表达提供更灵活、更具交互性的计数能力。然而，细粒度属性理解的要求对现有技术构成了挑战，因为它们难以准确地将属性信息与正确的视觉模式对齐。鉴于“视觉密度”已被证实的重要性，本文推测当前REC方法的局限性源于对“上下文属性密度”（CAD）的探索不足。在REC的范畴内，我们将CAD定义为视觉区域中某一特定细粒度属性的信息强度衡量标准。为了建模CAD，我们提出了一个U形CAD估计器，其中指代表达和来自GroundingDINO的多尺度视觉特征可以相互作用。通过额外的密度监督，我们可以有效地编码CAD，随后通过一个带有CAD精炼查询的新颖注意力过程进行解码。整合所有这些贡献，我们的框架显著优于最先进的REC方法，在计数指标上实现了30%的错误减少，在定位精度上实现了10%的提升。这些令人惊喜的结果揭示了上下文属性密度对于REC的重要性。代码将在github.com/Xu3XiWang/CAD-GD。", "summary": "本论文旨在解决现有指代表达计数（REC）算法在细粒度属性理解方面的局限性。文章引入了“上下文属性密度”（CAD）的概念，将其定义为视觉区域中细粒度属性的信息强度。为建模CAD，研究者提出了一个U形CAD估计器，该估计器结合了指代表达和GroundingDINO的多尺度视觉特征，并通过密度监督和CAD精炼的注意力机制进行解码。实验结果表明，该框架显著优于现有最先进的REC方法，在计数误差上减少了30%，定位精度提升了10%，证明了上下文属性密度在REC中的重要性。", "keywords": "指代表达计数, 上下文属性密度, 细粒度理解, 视觉密度, GroundingDINO", "comments": "本文的创新点在于引入并成功建模了“上下文属性密度”（CAD）这一新概念，有效解决了指代表达计数中细粒度属性理解的关键瓶颈。显著的性能提升证明了CAD对REC任务的重要性。"}}
{"id": "2503.12461", "pdf": "https://arxiv.org/pdf/2503.12461", "abs": "https://arxiv.org/abs/2503.12461", "authors": ["Fanhu Zeng", "Hao Tang", "Yihua Shao", "Siyu Chen", "Ling Shao", "Yan Wang"], "title": "MambaIC: State Space Models for High-Performance Learned Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to CVPR 2025", "summary": "A high-performance image compression algorithm is crucial for real-time\ninformation transmission across numerous fields. Despite rapid progress in\nimage compression, computational inefficiency and poor redundancy modeling\nstill pose significant bottlenecks, limiting practical applications. Inspired\nby the effectiveness of state space models (SSMs) in capturing long-range\ndependencies, we leverage SSMs to address computational inefficiency in\nexisting methods and improve image compression from multiple perspectives. In\nthis paper, we integrate the advantages of SSMs for better\nefficiency-performance trade-off and propose an enhanced image compression\napproach through refined context modeling, which we term MambaIC. Specifically,\nwe explore context modeling to adaptively refine the representation of hidden\nstates. Additionally, we introduce window-based local attention into\nchannel-spatial entropy modeling to reduce potential spatial redundancy during\ncompression, thereby increasing efficiency. Comprehensive qualitative and\nquantitative results validate the effectiveness and efficiency of our approach,\nparticularly for high-resolution image compression. Code is released at\nhttps://github.com/AuroraZengfh/MambaIC.", "AI": {"title_translation": "MambaIC：用于高性能学习图像压缩的状态空间模型", "tldr": "MambaIC利用状态空间模型改进图像压缩，通过优化上下文建模和引入局部注意力来提高效率和性能，尤其适用于高分辨率图像。", "motivation": "现有的图像压缩算法存在计算效率低下和冗余建模不佳的问题，限制了其实际应用。", "method": "论文提出了MambaIC，一种增强的图像压缩方法，它整合了状态空间模型（SSMs）的优势，以改善效率-性能权衡。具体来说，MambaIC探索了上下文建模以自适应地细化隐藏状态表示，并引入了基于窗口的局部注意力到通道-空间熵建模中，以减少潜在的空间冗余，从而提高效率。", "result": "全面的定性和定量结果验证了该方法的有效性和效率，特别是对于高分辨率图像压缩。", "conclusion": "MambaIC通过结合状态空间模型和改进的上下文建模，显著提升了图像压缩的性能和效率，尤其在高分辨率场景下表现突出。", "translation": "高性能图像压缩算法对于众多领域的实时信息传输至关重要。尽管图像压缩取得了快速进展，但计算效率低下和冗余建模不佳仍然是重要的瓶颈，限制了实际应用。受状态空间模型（SSMs）在捕获长程依赖方面有效性的启发，我们利用SSMs来解决现有方法中的计算效率低下问题，并从多个角度改进图像压缩。在本文中，我们整合了SSMs的优势，以实现更好的效率-性能权衡，并通过精细的上下文建模提出了一种增强的图像压缩方法，我们称之为MambaIC。具体而言，我们探索了上下文建模以自适应地细化隐藏状态的表示。此外，我们将基于窗口的局部注意力引入到通道-空间熵建模中，以减少压缩过程中潜在的空间冗余，从而提高效率。全面的定性和定量结果验证了我们方法的有效性和效率，特别是对于高分辨率图像压缩。代码已在https://github.com/AuroraZengfh/MambaIC 发布。", "summary": "论文提出了MambaIC，一种基于状态空间模型（SSMs）的高性能学习图像压缩算法，旨在解决现有方法中计算效率低下和冗余建模不足的问题。MambaIC通过利用SSMs的优势，改进上下文建模以自适应地细化隐藏状态，并引入基于窗口的局部注意力进行通道-空间熵建模，从而提高效率并优化性能。实验结果表明，MambaIC在图像压缩，特别是高分辨率图像压缩方面，具有显著的有效性和效率。", "keywords": "图像压缩, 状态空间模型, MambaIC, 上下文建模, 高分辨率图像", "comments": "该论文创新性地将状态空间模型应用于图像压缩领域，有效解决了传统方法在计算效率和长程依赖建模上的不足。通过精细的上下文建模和引入局部注意力，MambaIC在提高压缩性能的同时，显著提升了效率，尤其对高分辨率图像压缩具有重要意义，展现了SSMs在图像处理中的巨大潜力。"}}
{"id": "2503.12464", "pdf": "https://arxiv.org/pdf/2503.12464", "abs": "https://arxiv.org/abs/2503.12464", "authors": ["Alessio Xompero", "Andrea Cavallaro"], "title": "Learning Privacy from Visual Entities", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages (13 for the main article, 8 for bibliography, acks,\n  appendixes), 9 figures, 12 tables. Article accepted and to appear in the\n  Proceedings on Privacy Enhancing Technologies, 2025 (3):\n  https://petsymposium.org/popets/2025/. To be presented at the Privacy\n  Enhancing Technologies Symposium 2025. Artifact (source code) under review:\n  https://github.com/graphnex/privacy-from-visual-entities", "summary": "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.", "AI": {"title_translation": "从视觉实体中学习隐私", "tldr": "本文提出一种结合迁移学习和CNN的简单方法，仅用732个参数就能在预测图像隐私性方面达到与复杂图神经网络相当的性能，并发现高维特征和图组件并非必要。", "motivation": "预测图像是私密的还是公开的是一项具有挑战性的任务，因为主观解释和内容多样性。", "method": "本文采用迁移学习和卷积神经网络（CNN）的简单组合，将隐私与场景类型关联起来，并对CNN进行微调以优化图像特征。", "result": "该方法仅优化732个参数，却能实现与基于图的方法相当的性能；发现高维特征向量是不必要的，并使模型复杂化；图组件对性能的影响可以忽略不计，性能主要由微调CNN驱动。", "conclusion": "结合迁移学习和微调CNN的简单模型足以有效预测图像隐私性，并且比复杂的图神经网络更高效，高维特征和图组件并非性能的关键因素。", "translation": "主观解释和内容多样性使得预测图像是私密的还是公开的是一项具有挑战性的任务。图神经网络与卷积神经网络（CNN）结合，包含14,000到5亿个参数，用于生成视觉实体（例如，场景和对象类型）的特征，并识别有助于决策的实体。在本文中，我们展示了使用迁移学习和CNN的更简单组合来关联隐私与场景类型，仅优化732个参数，同时实现了与基于图的方法相当的性能。相反，基于图的方法的端到端训练可能会掩盖各个组件对分类性能的贡献。此外，我们表明，为每个视觉实体提取的高维特征向量是不必要的，并使模型复杂化。图组件对性能的影响也可以忽略不计，性能主要由微调CNN以优化隐私节点的图像特征驱动。", "summary": "本文探讨了图像隐私性预测的挑战，并提出一种新方法。研究表明，通过结合迁移学习和卷积神经网络（CNN）来关联隐私与场景类型，仅需优化732个参数，即可达到与参数量庞大（14,000至5亿）的图神经网络相当的性能。论文指出，现有图方法的端到端训练可能掩盖了各组件的贡献，并且高维特征向量和图组件对性能的影响微乎其微，核心驱动力在于对CNN的微调。", "keywords": "图像隐私, 卷积神经网络, 迁移学习, 图神经网络, 视觉实体", "comments": "这篇论文的创新点在于证明了在图像隐私性预测任务中，一个参数量极小（732个）的简单模型（迁移学习+CNN）可以达到与复杂图神经网络（数百万参数）相当的性能。这挑战了“模型越复杂越好”的普遍认知，强调了模型简洁性和关键特征微调的重要性，对于资源受限或需要高效部署的应用具有重要意义。"}}
{"id": "2503.12470", "pdf": "https://arxiv.org/pdf/2503.12470", "abs": "https://arxiv.org/abs/2503.12470", "authors": ["Han Mei", "Kunqian Li", "Shuaixin Liu", "Chengzhi Ma", "Qianli Jiang"], "title": "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https://github.com/OUCVisionGroup/DPF-Net.", "AI": {"title_translation": "DPF-Net：嵌入物理成像模型的以数据驱动的水下图像增强", "tldr": "DPF-Net是一个结合了物理模型和数据驱动方法的水下图像增强网络，通过估计物理参数并引入新的损失函数，实现了卓越的增强效果。", "motivation": "水下图像由于光线吸收和散射的复杂相互作用而严重退化，导致图像质量显著下降，因此需要有效的图像增强方法。", "method": "本研究提出了一个名为DPF-Net的两阶段水下图像增强网络。首先，使用合成数据集训练一个物理参数估计模块，以确保参数的可靠性。然后，该模块与增强网络协同训练，将估计的物理参数嵌入到数据驱动模型中。为保持恢复过程的一致性，提出了基于物理的退化一致性损失。此外，还引入了一种利用整个数据集的弱参考损失项，以减少模型对单个参考图像质量的依赖。", "result": "所提出的DPF-Net在多个测试集上表现出优于其他基准方法的性能，并取得了最先进的结果。", "conclusion": "DPF-Net通过结合物理成像模型的鲁棒性和数据驱动方法的效率，以及创新的损失函数，成功解决了水下图像退化问题，实现了卓越的图像增强效果。", "translation": "由于水下环境中光线吸收和散射的复杂相互作用，水下图像会经历显著的退化。本研究提出了一个名为数据驱动和物理参数融合网络（DPF-Net）的两阶段水下图像增强网络，它利用了物理成像模型的鲁棒性以及数据驱动方法的通用性和效率。我们首先使用合成数据集训练一个物理参数估计模块，以保证物理参数的可靠性，而不是像以往研究中常见的那样，仅仅通过应用成像方程来学习原始图像和参考图像之间的拟合关系。该模块随后与增强网络协同训练，其中估计的物理参数被整合到嵌入空间中的数据驱动模型中。为了在水下成像退化中保持恢复过程的一致性，我们提出了一种基于物理的退化一致性损失。此外，我们还提出了一种利用整个数据集的创新性弱参考损失项，这减轻了我们模型对单个参考图像质量的依赖。我们提出的DPF-Net在多个测试集上表现出优于其他基准方法的性能，取得了最先进的结果。源代码和预训练模型可在项目主页获取：https://github.com/OUCVisionGroup/DPF-Net。", "summary": "DPF-Net是一个用于水下图像增强的两阶段网络，它创新性地结合了物理成像模型和数据驱动方法。该网络首先训练一个物理参数估计模块以确保参数可信度，然后将这些参数嵌入到数据驱动的增强模型中。为了优化恢复过程，DPF-Net引入了基于物理的退化一致性损失和弱参考损失。实验结果表明，DPF-Net在多个测试集上均表现出卓越的性能，达到了最先进的水平。", "keywords": "水下图像增强, 物理成像模型, 数据驱动, DPF-Net, 图像复原", "comments": "该论文的创新点在于将物理成像模型的鲁棒性与数据驱动方法的通用性及效率相结合，克服了传统方法仅依赖拟合关系的局限性。通过引入物理参数估计模块和创新的损失函数（物理退化一致性损失和弱参考损失），显著提升了水下图像增强的可靠性和性能。其提出的两阶段训练策略和损失函数设计对于解决水下图像的复杂退化问题具有重要意义，并实现了SOTA结果，表明了该方法的有效性和先进性。"}}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE", "AI": {"title_translation": "基于扩散模型的可见光-红外行人再识别合成数据生成", "tldr": "提出DiVE框架，利用扩散模型生成可见光-红外配对的行人再识别合成数据，解决数据收集难题并提升模型性能。", "motivation": "可见光-红外行人再识别（VI-ReID）任务中，收集和标注大规模多模态行人图像成本高昂、耗时且受数据保护法律限制，导致数据集需求难以满足。现有合成数据方法缺乏针对VI-ReID的特定技术。", "method": "本文提出DiVE（Diffusion-based VI-ReID data Expansion）框架，通过解耦身份和模态来自动获取大规模身份保留的RGB-IR配对图像。具体而言，身份表示从共享相同ID的样本中获取，图像模态通过在模态特定数据上微调Stable Diffusion来学习。DiVE将文本驱动图像合成扩展到身份保留的RGB-IR多模态图像合成。", "result": "实验表明，使用DiVE生成的合成数据训练的VI-ReID模型性能持续显著提升。特别是，在LLCM数据集上，使用合成图像训练的SOTA方法CAJ，其mAP比基线提高了约9%。", "conclusion": "DiVE框架通过生成大规模身份保留的RGB-IR配对图像，有效解决了VI-ReID任务中数据收集和标注的难题，显著提升了模型的性能。", "translation": "模型的性能与训练数据的丰富程度密切相关。在可见光-红外行人再识别（VI-ReID）任务中，收集和标注每个个体在各种摄像头和模态下的大规模图像是繁琐、耗时、昂贵的，并且必须遵守数据保护法律，这给满足数据集需求带来了严峻挑战。当前研究探索将合成数据生成作为一种高效且保护隐私的替代方案，以替代实地收集真实数据。然而，尚未探索出一种专门为VI-ReID模型量身定制的数据合成技术。在本文中，我们提出了一种新颖的数据生成框架，命名为基于扩散的VI-ReID数据扩展（DiVE），通过解耦身份和模态，自动获取大规模身份保留的RGB-IR配对图像，以提高VI-ReID模型的性能。具体而言，身份表示是从一组共享相同ID的样本中获取的，而图像的模态则通过在模态特定数据上微调Stable Diffusion（SD）来学习。DiVE将文本驱动的图像合成扩展到身份保留的RGB-IR多模态图像合成。这种方法通过直接将合成数据整合到ReID模型训练中，显著降低了数据收集和标注成本。实验证明，使用DiVE生成的合成数据训练的VI-ReID模型始终表现出显著的性能提升。特别是，在LLCM数据集上，使用合成图像训练的最新方法CAJ，其mAP比基线提高了约9%。代码：https://github.com/BorgDiven/DiVE", "summary": "本文提出DiVE（Diffusion-based VI-ReID data Expansion）框架，旨在解决可见光-红外行人再识别（VI-ReID）任务中数据收集和标注的挑战。DiVE利用扩散模型，通过解耦身份和模态来自动生成大规模、身份保留的RGB-IR配对图像。该方法将文本驱动图像合成扩展到多模态图像合成，显著降低了数据成本。实验证明，DiVE生成的合成数据能有效提升VI-ReID模型的性能，使最先进的方法在特定数据集上获得显著提升。", "keywords": "合成数据生成, 可见光-红外行人再识别, 扩散模型, 数据增强, 多模态", "comments": "该论文的创新点在于提出了一个专门针对VI-ReID任务的扩散模型数据生成框架DiVE，通过解耦身份和模态，实现了身份保留的多模态图像合成。这为解决VI-ReID领域数据稀缺和隐私保护问题提供了一个高效且实用的解决方案，对于推动该领域的发展具有重要意义。"}}
{"id": "2503.12485", "pdf": "https://arxiv.org/pdf/2503.12485", "abs": "https://arxiv.org/abs/2503.12485", "authors": ["Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Hezhen Hu", "Wengang Zhou", "Houqiang Li"], "title": "Cross-Modal Consistency Learning for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminate background perturbation but\ninevitably suffer from insufficient semantic cues compared to raw RGB videos.\nNevertheless, direct representation learning only from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public.", "AI": {"title_translation": "跨模态一致性学习用于手语识别", "tldr": "本文提出了一种名为CCL-SLR的跨模态一致性学习框架，通过自监督预训练利用RGB和姿态模态的跨模态一致性，以解决现有手语识别预训练方法在语义信息不足或存在无关视觉特征方面的挑战，并在ISLR基准测试中取得了显著性能。", "motivation": "现有的孤立手语识别（ISLR）预训练方法仅关注紧凑的姿态数据，导致语义线索不足；而直接从原始RGB视频学习表示又因存在与手语无关的视觉特征而具有挑战性。", "method": "本文提出了一个跨模态一致性学习框架（CCL-SLR）。首先，CCL-SLR采用对比学习进行模态内和跨模态的实例判别，逐步对齐RGB和姿态模态的特征空间，从而提取一致的手语表示。其次，引入了运动保留掩蔽（MPM）和语义正样本挖掘（SPM）技术，分别从数据增强和样本相似性的角度提高跨模态一致性。", "result": "在四个ISLR基准测试上进行了广泛的实验，结果表明CCL-SLR取得了令人印象深刻的性能。", "conclusion": "CCL-SLR通过利用RGB和姿态模态的跨模态一致性，有效提升了孤立手语识别的性能。", "translation": "预训练已被证明能有效提升孤立手语识别（ISLR）的性能。现有的预训练方法仅关注紧凑的姿态数据，这虽然消除了背景扰动，但与原始RGB视频相比，不可避免地遭受语义线索不足的问题。然而，由于存在与手语无关的视觉特征，直接从RGB视频中学习表示仍然具有挑战性。为了解决这一困境，我们提出了一种跨模态一致性学习框架（CCL-SLR），该框架基于自监督预训练，利用了来自RGB和姿态模态的跨模态一致性。首先，CCL-SLR采用对比学习进行模态内和跨模态的实例判别。通过单模态和跨模态对比学习，CCL-SLR逐渐对齐RGB和姿态模态的特征空间，从而提取一致的手语表示。其次，我们进一步引入了运动保留掩蔽（MPM）和语义正样本挖掘（SPM）技术，分别从数据增强和样本相似性的角度改善跨模态一致性。在四个ISLR基准测试上进行的广泛实验表明，CCL-SLR取得了令人印象深刻的性能，证明了其有效性。代码将公开发布。", "summary": "本文提出了一种名为跨模态一致性学习（CCL-SLR）的新框架，用于解决孤立手语识别（ISLR）中现有预训练方法仅依赖姿态数据导致语义不足或直接RGB学习受无关特征干扰的问题。CCL-SLR通过自监督预训练，结合模态内和跨模态对比学习来对齐RGB和姿态特征空间，以提取一致的手语表示。此外，引入了运动保留掩蔽（MPM）和语义正样本挖掘（SPM）技术以增强跨模态一致性。实验结果表明，该方法在多个ISLR基准上表现出色。", "keywords": "跨模态学习, 一致性学习, 手语识别, 对比学习, 自监督预训练", "comments": "该研究通过结合RGB和姿态两种模态的优势，有效解决了手语识别中单一模态预训练的局限性。其提出的跨模态一致性学习框架（CCL-SLR）以及运动保留掩蔽（MPM）和语义正样本挖掘（SPM）等技术，为手语识别领域提供了一种新颖且有效的方法，具有重要的研究价值和应用潜力。"}}
{"id": "2503.12490", "pdf": "https://arxiv.org/pdf/2503.12490", "abs": "https://arxiv.org/abs/2503.12490", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Bin Chen", "Zian Guan", "Yuhao Wang", "Xu Jia", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing.", "AI": {"title_translation": "GeoRSMLLM：地球科学和遥感领域的多模态大语言模型", "tldr": "当前视觉-语言模型在遥感复杂任务中表现不足，本文提出了GeoRSMLLM模型，该模型采用统一数据表示和自增强策略，旨在处理地球科学和遥感领域更广泛的视觉-语言任务。", "motivation": "当前遥感领域的视觉-语言模型（VLM）在处理涉及复杂指令（如多条件）或像素级操作（如分割和变化检测）的任务时表现不佳，尽管它们在传统任务和指代表达理解方面有潜力。", "method": "本文对遥感领域的视觉-语言任务进行了全面的分层总结，并引入了遥感视觉-语言任务集（RSVLTS），其中包含开放词汇任务（OVT）、指代表达任务（RET）、描述对象任务（DOT）和视觉问答（VQA），难度递增。此外，提出了一种针对RSVLTS的基于点集的新颖统一数据表示方法，并结合了条件解析器和基于循环指代的自增强策略。这些功能被整合到GeoRSMLLM模型中。", "result": "GeoRSMLLM增强模型旨在处理RSVLTS的广泛任务，为地球科学和遥感领域的视觉-语言任务提供了一种更通用的解决方案。", "conclusion": "GeoRSMLLM模型通过其统一数据表示和自增强策略，为解决现有视觉-语言模型在地球科学和遥感领域复杂任务上的局限性提供了更通用的解决方案。", "translation": "遥感（RS）中视觉-语言模型（VLM）的应用在场景分类、目标检测和图像标注等传统任务中展现出巨大潜力。然而，当前模型在指代表达理解（REC）方面表现出色，但在涉及复杂指令（例如，存在多个条件）或像素级操作（如分割和变化检测）的任务中却表现不佳。在这份白皮书中，我们根据所需的认知能力水平，提供了遥感领域视觉-语言任务的全面分层总结。我们引入了遥感视觉-语言任务集（RSVLTS），其中包括难度递增的开放词汇任务（OVT）、指代表达任务（RET）和描述对象任务（DOT），以及单独的视觉问答（VQA）。此外，我们提出了一种新颖的统一数据表示方法，即针对RSVLTS的点集方法，以及一个条件解析器和基于循环指代的自增强策略。这些功能被整合到GeoRSMLLM模型中，这个增强模型旨在处理RSVLTS的广泛任务，为地球科学和遥感领域的视觉-语言任务提供更通用的解决方案铺平道路。", "summary": "本白皮书针对现有遥感视觉-语言模型（VLM）在处理复杂指令和像素级任务方面的不足，提出了GeoRSMLLM模型。该模型通过引入遥感视觉-语言任务集（RSVLTS）并采用统一的点集数据表示、条件解析器和循环指代自增强策略，旨在提供一个更通用的解决方案，以应对地球科学和遥感领域中广泛的视觉-语言任务。", "keywords": "多模态大语言模型, 地球科学, 遥感, 视觉-语言任务, GeoRSMLLM", "comments": "该论文提出了一种解决遥感领域VLM局限性的创新方法，特别是针对复杂指令和像素级任务。通过引入GeoRSMLLM模型、RSVLTS任务集、统一数据表示和自增强策略，它为构建更通用、更强大的地球科学和遥感视觉-语言模型奠定了基础，具有重要的研究价值。"}}
{"id": "2503.12492", "pdf": "https://arxiv.org/pdf/2503.12492", "abs": "https://arxiv.org/abs/2503.12492", "authors": ["Dapeng Zhao"], "title": "Geometry-Aware Face Reconstruction Under Occluded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods.", "AI": {"title_translation": "遮挡场景下的几何感知人脸重建", "tldr": "该研究提出了一种受GAN和凹凸贴图启发的几何感知人脸重建方法，即使在遮挡场景下也能进行全面且详细的3D人脸重建，并在实验中证明其优于传统方法和手动遮挡去除方法。", "motivation": "现有的深度学习3D人脸重建方法在处理遮挡场景时面临挑战，并且未能捕捉到复杂的人脸几何细节。", "method": "该方法受GANs和凹凸贴图原理启发，通过对基本结构引入中级形状细化来保持整体形状的鲁棒性，并能为被遮挡的面部区域生成合理的细节。", "result": "该框架在传统方法难以处理的遮挡场景中产生了逼真的重建结果。广泛的实验证明了其在通用3D人脸重建任务中相比手动遮挡去除方法具有卓越的适应性和调控能力。", "conclusion": "该研究提出的方法有效解决了遮挡场景下的3D人脸重建问题，通过结合GANs和凹凸贴图的原理，实现了对遮挡人脸的全面且细节丰富的重建，并表现出优于传统方法的性能。", "translation": "最近，基于深度学习的3D人脸重建方法在质量和效率方面取得了可喜的进展。然而，这些技术在有效处理遮挡场景方面面临挑战，并且未能捕捉到复杂的人脸几何细节。受GANs和凹凸贴图原理的启发，我们成功解决了这些问题。我们的方法旨在提供全面的3D人脸重建，即使在存在遮挡的情况下。在保持整体形状鲁棒性的同时，我们对基本结构引入了中级形状细化。此外，我们展示了我们的方法如何巧妙地扩展以生成被遮挡面部区域的合理细节。我们提供了大量示例，展示了我们框架在产生逼真结果方面的有效性，而传统方法往往在这方面表现不佳。为了证实我们方法卓越的适应性，我们针对通用3D人脸重建任务进行了广泛的实验，作为其与手动遮挡去除方法相比具有监管能力的具体证据。", "summary": "本研究提出了一种新颖的深度学习方法，用于解决现有3D人脸重建技术在处理遮挡场景和捕捉精细几何细节方面的局限性。受生成对抗网络（GANs）和凹凸贴图的启发，所提出的框架通过引入中级形状细化，实现了在遮挡条件下的全面且鲁棒的3D人脸重建，并能为被遮挡区域生成逼真的细节。广泛的实验证明了该方法在性能和适应性上均优于传统方法和手动遮挡去除技术。", "keywords": "3D人脸重建, 遮挡, GANs, 凹凸贴图, 几何感知", "comments": "该研究的创新之处在于将GANs和凹凸贴图相结合，用于在遮挡场景下进行几何感知的人脸重建，并能生成被遮挡区域的细节，这在3D人脸重建领域是一个重要的挑战。该论文有效解决了当前方法的一个实际局限性。"}}
{"id": "2503.12494", "pdf": "https://arxiv.org/pdf/2503.12494", "abs": "https://arxiv.org/abs/2503.12494", "authors": ["Dapeng Zhao"], "title": "Learning Contour-Guided 3D Face Reconstruction with Occlusions", "categories": ["cs.CV"], "comment": null, "summary": "Recently, deep learning-based 3D face reconstruction methods have\ndemonstrated promising advancements in terms of quality and efficiency.\nNevertheless, these techniques face challenges in effectively handling occluded\nscenes and fail to capture intricate geometric facial details. Inspired by the\nprinciples of GANs and bump mapping, we have successfully addressed these\nissues. Our approach aims to deliver comprehensive 3D facial reconstructions,\neven in the presence of occlusions.While maintaining the overall shape's\nrobustness, we introduce a mid-level shape refinement to the fundamental\nstructure. Furthermore, we illustrate how our method adeptly extends to\ngenerate plausible details for obscured facial regions. We offer numerous\nexamples that showcase the effectiveness of our framework in producing\nrealistic results, where traditional methods often struggle. To substantiate\nthe superior adaptability of our approach, we have conducted extensive\nexperiments in the context of general 3D face reconstruction tasks, serving as\nconcrete evidence of its regulatory prowess compared to manual occlusion\nremoval methods.", "AI": {"title_translation": "学习轮廓引导的带遮挡三维人脸重建", "tldr": "本文提出了一种结合GANs和凹凸贴图的新方法，用于在存在遮挡的情况下进行全面的三维人脸重建，并能捕捉复杂的几何细节，优于传统方法。", "motivation": "现有的深度学习三维人脸重建方法在处理遮挡场景和捕捉复杂几何面部细节方面面临挑战。", "method": "本方法受GANs和凹凸贴图的启发，引入了中级形状细化以增强基本结构，并能为被遮挡的面部区域生成合理的细节，从而实现全面的三维人脸重建。", "result": "本框架在生成逼真结果方面表现出有效性，在传统方法难以处理的情况下尤为突出。通过广泛的实验证明，该方法在通用三维人脸重建任务中具有卓越的适应性和调控能力，优于手动遮挡移除方法。", "conclusion": "本文提出的方法能够有效地在存在遮挡的情况下进行全面的三维人脸重建，并能捕捉复杂的面部细节，其适应性和性能优于现有技术。", "translation": "最近，基于深度学习的三维人脸重建方法在质量和效率方面取得了可喜的进展。然而，这些技术在有效处理遮挡场景方面面临挑战，并且未能捕捉到复杂的几何面部细节。受GANs和凹凸贴图原理的启发，我们成功解决了这些问题。我们的方法旨在提供全面的三维人脸重建，即使在存在遮挡的情况下也是如此。在保持整体形状鲁棒性的同时，我们对基本结构引入了中级形状细化。此外，我们阐述了我们的方法如何巧妙地扩展以生成被遮挡面部区域的合理细节。我们提供了大量示例，展示了我们的框架在生成逼真结果方面的有效性，而传统方法往往难以做到这一点。为了证实我们方法卓越的适应性，我们针对通用三维人脸重建任务进行了广泛的实验，作为其与手动遮挡移除方法相比具有调控能力的具体证据。", "summary": "本文提出了一种新颖的深度学习方法，旨在解决现有三维人脸重建技术在处理遮挡和捕捉精细面部细节方面的不足。该方法结合了生成对抗网络（GANs）和凹凸贴图的原理，引入了中级形状细化，并能有效生成被遮挡区域的合理细节。实验结果表明，与传统方法相比，该框架在生成逼真三维人脸重建方面表现出卓越的有效性和适应性，尤其是在存在遮挡的情况下。", "keywords": "三维人脸重建, 遮挡处理, GANs, 凹凸贴图, 形状细化", "comments": "该论文的创新点在于结合了GANs和凹凸贴图来解决三维人脸重建中长期存在的遮挡问题，并能捕捉到精细的几何细节。其引入的中级形状细化和为遮挡区域生成细节的能力，是提高重建质量的关键。这对于需要鲁棒三维人脸模型的应用（如AR/VR、人机交互等）具有重要意义。"}}
{"id": "2503.12495", "pdf": "https://arxiv.org/pdf/2503.12495", "abs": "https://arxiv.org/abs/2503.12495", "authors": ["Xuan Ma", "Zewen Lv", "Chengcai Ma", "Tao Zhang", "Yuelan Xin", "Kun Zhan"], "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau", "categories": ["cs.CV"], "comment": "Journal of Applied Remote Sensing, 2025", "summary": "Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a\nsignificant environmental challenge due to overgrazing, climate change, and\nrodent activity, which degrade vegetation cover and soil quality. These\nextremely degraded grassland on QTP, commonly referred to as black-soil area,\nrequire accurate assessment to guide effective restoration efforts. In this\npaper, we present a newly created QTP black-soil dataset, annotated under\nexpert guidance. We introduce a novel neural network model, BS-Mamba,\nspecifically designed for the black-soil area detection using UAV remote\nsensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying\nblack-soil area across two independent test datasets than the state-of-the-art\nmodels. This research contributes to grassland restoration by providing an\nefficient method for assessing the extent of black-soil area on the QTP.", "AI": {"title_translation": "青藏高原黑土滩区域检测的BS-Mamba模型", "tldr": "针对青藏高原黑土滩问题，本文提出了一个新数据集和BS-Mamba模型，用于无人机遥感影像的黑土滩检测，并取得了优于现有模型的精度。", "motivation": "青藏高原（QTP）的极端退化草地（黑土滩）由于过度放牧、气候变化和啮齿动物活动，导致植被覆盖和土壤质量下降，构成严重的环境挑战。需要准确评估这些区域以指导有效的恢复工作。", "method": "本文提出了一个新的青藏高原黑土滩数据集，并在专家指导下进行了标注。同时，引入了一种名为BS-Mamba的新型神经网络模型，专门用于利用无人机遥感影像进行黑土滩区域检测。", "result": "BS-Mamba模型在两个独立的测试数据集中，识别黑土滩区域的准确性高于现有最先进模型。", "conclusion": "这项研究通过提供一种评估青藏高原黑土滩区域范围的有效方法，为草地恢复做出了贡献。", "translation": "青藏高原（QTP）上极端退化的草地由于过度放牧、气候变化和啮齿动物活动，导致植被覆盖和土壤质量下降，构成了严峻的环境挑战。这些青藏高原上常见的极端退化草地，通常被称为黑土滩区域，需要准确评估以指导有效的恢复工作。本文提出了一个新创建的青藏高原黑土滩数据集，该数据集在专家指导下进行了标注。我们引入了一种新型神经网络模型BS-Mamba，专门设计用于利用无人机遥感影像进行黑土滩区域检测。BS-Mamba模型在两个独立的测试数据集中，识别黑土滩区域的准确性高于现有最先进模型。这项研究通过提供一种评估青藏高原黑土滩区域范围的有效方法，为草地恢复做出了贡献。", "summary": "本文针对青藏高原极端退化草地（黑土滩）的准确评估需求，构建了一个新的黑土滩数据集，并提出了一种基于无人机遥感影像的神经网络模型BS-Mamba。实验结果表明，BS-Mamba模型在黑土滩区域检测方面优于现有最先进模型，为草地恢复提供了高效的评估方法。", "keywords": "青藏高原, 黑土滩, BS-Mamba, 无人机遥感, 草地恢复", "comments": "该研究通过创建专门的数据集和开发新型神经网络模型BS-Mamba，为青藏高原黑土滩这一特定且重要的环境问题提供了创新的解决方案。其创新点在于模型针对性强，结合了无人机遥感技术，且在精度上超越了现有模型，对于退化草地修复具有重要的实际应用价值。"}}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain.", "AI": {"title_translation": "您的视觉语言模型是否迷失在长视频采样困境中？", "tldr": "本文提出了LSDBench基准和RHS框架，以解决长视频中视觉语言模型面临的“采样困境”，提高其在处理高必要采样密度（NSD）任务时的效率和性能。", "motivation": "大型视觉语言模型（LVLMs）在视频理解方面取得了显著进展，但高效处理长视频仍面临“采样困境”：低密度采样可能丢失关键信息，而高密度采样则引入冗余。", "method": "为解决长视频采样问题，本文提出了LSDBench，这是首个通过构建高必要采样密度（NSD）问题来评估LVLMs在长视频任务上的基准。同时，为应对高NSD问题，提出了一种新颖的推理驱动分层采样（RHS）框架，该框架结合了问题相关线索的全局定位和局部密集采样以进行精确推理。此外，开发了一个轻量级语义引导帧选择器来优先选择信息丰富的帧。", "result": "RHS框架能够以显著更少的采样帧实现可比或更优的性能。", "conclusion": "LSDBench基准和RHS框架共同解决了高NSD长视频任务的独特挑战，为评估和改进LVLMs在该领域树立了新标准。", "translation": "大型视觉语言模型（LVLMs）的兴起显著推动了视频理解的发展。然而，由于“采样困境”——低密度采样可能丢失关键信息，而高密度采样则引入冗余——高效处理长视频仍然是一个挑战。为解决这个问题，我们引入了LSDBench，这是第一个旨在通过构建高必要采样密度（NSD）问题来评估LVLMs在长视频任务上的基准，其中NSD代表准确回答给定问题所需的最小采样密度。LSDBench侧重于密集的、短时长的动作，以严格评估LVLMs采用的采样策略。为应对高NSD问题带来的挑战，我们提出了一种新颖的推理驱动分层采样（RHS）框架，该框架结合了问题相关线索的全局定位和局部密集采样以进行精确推理。此外，我们开发了一个轻量级语义引导帧选择器来优先选择信息丰富的帧，使RHS能够以显著更少的采样帧实现可比或更优的性能。总而言之，我们的LSDBench和RHS框架解决了高NSD长视频任务的独特挑战，为评估和改进该领域的LVLMs树立了新标准。", "summary": "本文针对大型视觉语言模型（LVLMs）在处理长视频时面临的“采样困境”——即在信息丢失与冗余之间取得平衡的挑战——提出了两项创新。首先，引入了LSDBench，一个用于评估LVLMs处理高必要采样密度（NSD）长视频任务的基准。其次，提出了一种推理驱动分层采样（RHS）框架，通过结合全局定位和局部密集采样，并辅以语义引导帧选择器，RHS能够在显著减少采样帧数的情况下，依然保持或超越现有性能，从而有效解决了高NSD长视频任务的难题。", "keywords": "视觉语言模型, 长视频, 采样困境, LSDBench, RHS框架", "comments": "本文识别并解决了一个LVLMs在视频理解领域面临的关键挑战——长视频的采样困境。LSDBench作为首个针对高NSD长视频任务的评估基准，填补了现有评估体系的空白。RHS框架结合了全局与局部采样策略，并通过轻量级帧选择器优化效率，显示出在保持性能的同时大幅减少计算资源的潜力，具有重要的实践意义和创新性。"}}
{"id": "2503.12507", "pdf": "https://arxiv.org/pdf/2503.12507", "abs": "https://arxiv.org/abs/2503.12507", "authors": ["Guangqian Guo", "Yoong Guo", "Xuehui Yu", "Wenbo Li", "Yaoxing Wang", "Shan Gao"], "title": "Segment Any-Quality Images with Generative Latent Space Enhancement", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset.", "AI": {"title_translation": "使用生成式潜在空间增强分割任意质量图像", "tldr": "本文提出GleSAM，通过生成式潜在空间增强技术，解决了Segment Anything Models (SAMs)在低质量图像上性能显著下降的问题，显著提高了其在复杂退化图像上的分割鲁棒性和泛化能力。", "motivation": "Segment Anything Models (SAMs) 在严重退化、低质量图像上的性能显著下降，限制了它们在现实世界场景中的有效性。", "method": "本文提出了GleSAM模型，通过在SAM的潜在空间中应用生成式潜在扩散过程来重建高质量图像表示，从而改善分割。该方法将潜在扩散概念应用于SAM框架，并引入了两种技术以提高预训练扩散模型与分割框架的兼容性。GleSAM可应用于预训练的SAM和SAM2，且仅需少量额外可学习参数。此外，作者还构建了LQSeg数据集，用于训练和评估模型，该数据集包含更多样化的退化类型和级别。", "result": "GleSAM显著提高了在复杂退化图像上的分割鲁棒性，同时保持了对清晰图像的泛化能力。GleSAM在未见过的退化类型上也表现良好，证明了其方法和数据集的通用性。", "conclusion": "GleSAM通过生成式潜在空间增强有效解决了SAM在低质量图像上的性能下降问题，显著提高了分割鲁棒性和泛化能力，其方法和数据集具有通用性和高效性。", "translation": "尽管Segment Anything Models (SAMs)取得了成功，但它们在严重退化、低质量图像上的性能显著下降，限制了其在现实世界场景中的有效性。为了解决这个问题，我们提出了GleSAM，它利用生成式潜在空间增强来提高在低质量图像上的鲁棒性，从而实现对各种图像质量的泛化。具体来说，我们将潜在扩散的概念应用于基于SAM的分割框架，并在SAM的潜在空间中执行生成扩散过程以重建高质量表示，从而改善分割。此外，我们引入了两种技术来提高预训练扩散模型与分割框架之间的兼容性。我们的方法可以应用于预训练的SAM和SAM2，仅需最少的额外可学习参数，从而实现高效优化。我们还构建了LQSeg数据集，其中包含更多样化的退化类型和级别，用于训练和评估模型。大量实验表明，GleSAM显著提高了复杂退化图像上的分割鲁棒性，同时保持了对清晰图像的泛化能力。此外，GleSAM在未见过的退化类型上也表现良好，这突显了我们方法和数据集的通用性。", "summary": "本文提出GleSAM，旨在解决Segment Anything Models (SAMs)在低质量图像上性能下降的问题。GleSAM通过在SAM的潜在空间中应用生成式潜在扩散过程来重建高质量图像表示，从而增强分割鲁棒性。该方法兼容预训练的SAM和SAM2，并引入了提高扩散模型与分割框架兼容性的技术。为训练和评估，作者还构建了多样化的LQSeg数据集。实验证明GleSAM在复杂和未见过的退化图像上显著提高了分割性能，同时保持了对清晰图像的泛化能力。", "keywords": "Segment Anything Model, 图像分割, 潜在扩散, 低质量图像, 鲁棒性", "comments": "该论文的创新点在于将生成式潜在扩散技术引入到SAM的分割框架中，以提升其对低质量图像的处理能力。这种方法有效地解决了SAM在实际应用中遇到的一个重要限制，即对图像质量的敏感性。通过在潜在空间进行增强，GleSAM能够以高效的方式提高模型的鲁棒性和泛化能力，并且可以应用于现有的预训练SAM模型。此外，构建新的LQSeg数据集也为该领域的研究提供了有价值的资源。"}}
{"id": "2503.12515", "pdf": "https://arxiv.org/pdf/2503.12515", "abs": "https://arxiv.org/abs/2503.12515", "authors": ["Pan Du", "Delin An", "Chaoli Wang", "Jian-Xun Wang"], "title": "AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "comment": "42 pages, 8 figures", "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings.", "AI": {"title_translation": "AI驱动的自动化模型构建，用于主动脉血流的患者特异性CFD模拟", "tldr": "该研究提出了一种深度学习框架，可自动化从医学图像创建患者特异性血管模型，显著减少手动工作并提高效率，从而促进计算流体动力学（CFD）模拟的临床应用。", "motivation": "构建患者特异性血管模型仍然劳动密集、容易出错且耗时，这限制了其在临床上的应用，阻碍了对心血管血流动力学的理解以及心血管疾病的诊断和治疗进展。", "method": "本研究引入了一个深度学习框架，该框架将体素分割模块用于精确的血管描绘与表面变形模块相结合，后者在医学图像数据指导下执行解剖学一致的无监督表面细化。该框架将体素分割和表面变形统一为一个内聚的管道，以自动化从医学图像创建可用于模拟的血管模型。", "result": "在公开数据集上进行评估，所提出的方法在分割和网格质量方面表现出最先进的性能，同时显著减少了手动工作和处理时间。", "conclusion": "这项工作提升了基于图像的计算建模的可扩展性和可靠性，促进了其在临床和研究环境中的更广泛应用。", "translation": "基于图像的建模对于理解心血管血流动力学和推进心血管疾病的诊断与治疗至关重要。构建患者特异性血管模型仍然劳动密集、容易出错且耗时，这限制了其临床应用。本研究引入了一个深度学习框架，可自动化从医学图像创建可用于模拟的血管模型。该框架将用于精确体素血管描绘的分割模块与执行解剖学一致且由医学图像数据引导的无监督表面精修的表面变形模块集成。通过将体素分割和表面变形统一为一个内聚的管道，该框架解决了现有方法的关键局限性，增强了几何精度和计算效率。在公开数据集上进行评估，所提出的方法在分割和网格质量方面表现出最先进的性能，同时显著减少了手动工作和处理时间。这项工作提升了基于图像的计算建模的可扩展性和可靠性，促进了其在临床和研究环境中的更广泛应用。", "summary": "本研究提出了一种创新的深度学习框架，旨在自动化从医学图像构建患者特异性血管模型，以用于计算流体动力学（CFD）模拟。该框架通过整合体素分割和无监督表面变形模块，形成一个统一的管道，解决了当前手动建模耗时且易出错的局限性。实验结果表明，该方法在分割准确性和网格质量上达到了最先进水平，同时显著降低了人工干预和处理时间，从而提升了基于图像的计算建模在临床和研究中的可扩展性和可靠性。", "keywords": "深度学习, 血管建模, CFD模拟, 自动化, 患者特异性", "comments": "这项研究的创新之处在于其提出了一种统一的深度学习框架，将血管分割与表面变形相结合，实现了患者特异性血管模型构建的自动化。这显著解决了传统方法耗时、易错的痛点，对于推动计算流体动力学在心血管疾病诊断和治疗中的应用具有重要意义。其在准确性和效率上的提升，有望大大加速临床和研究转化。"}}
{"id": "2503.12519", "pdf": "https://arxiv.org/pdf/2503.12519", "abs": "https://arxiv.org/abs/2503.12519", "authors": ["Taein Kwon", "Zador Pataki", "Mahdi Rad", "Marc Pollefeys"], "title": "Multi Activity Sequence Alignment via Implicit Clustering", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Self-supervised temporal sequence alignment can provide rich and effective\nrepresentations for a wide range of applications. However, existing methods for\nachieving optimal performance are mostly limited to aligning sequences of the\nsame activity only and require separate models to be trained for each activity.\nWe propose a novel framework that overcomes these limitations using sequence\nalignment via implicit clustering. Specifically, our key idea is to perform\nimplicit clip-level clustering while aligning frames in sequences. This coupled\nwith our proposed dual augmentation technique enhances the network's ability to\nlearn generalizable and discriminative representations. Our experiments show\nthat our proposed method outperforms state-of-the-art results and highlight the\ngeneralization capability of our framework with multi activity and different\nmodalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will\nrelease our code upon acceptance.", "AI": {"title_translation": "多活动序列通过隐式聚类对齐", "tldr": "本文提出一种新的隐式聚类序列对齐框架，解决了现有方法只能对齐单一活动序列的局限性，并在多活动和多模态数据集上超越SOTA。", "motivation": "现有的自监督时间序列对齐方法在实现最佳性能时，大多仅限于对齐相同活动的序列，并且需要为每种活动训练单独的模型。", "method": "本文提出一个通过隐式聚类进行序列对齐的新颖框架。核心思想是在对齐序列中的帧时执行隐式剪辑级别聚类，并结合双重增强技术来学习可泛化和判别性的表示。", "result": "实验表明，所提出的方法优于现有最先进的结果，并在H2O、PennAction和IKEA ASM这三个不同数据集上，展示了该框架在多活动和不同模态下的泛化能力。", "conclusion": "该框架通过隐式聚类和双重增强，有效解决了多活动序列对齐的挑战，并取得了优于SOTA的泛化性能。", "translation": "自监督时间序列对齐可以为广泛的应用提供丰富而有效的表示。然而，现有实现最佳性能的方法大多仅限于对齐相同活动的序列，并且需要为每种活动训练单独的模型。\n我们提出了一个新颖的框架，通过隐式聚类实现序列对齐，从而克服了这些限制。具体来说，我们的核心思想是在对齐序列中的帧时执行隐式剪辑级别聚类。这与我们提出的双重增强技术相结合，增强了网络学习可泛化和判别性表示的能力。我们的实验表明，我们提出的方法优于现有最先进的结果，并突出了我们框架在H2O、PennAction和IKEA ASM这三个不同数据集上，在多活动和不同模态下的泛化能力。我们将在论文被接受后发布代码。", "summary": "本文提出了一种新颖的自监督序列对齐框架，通过引入隐式剪辑级聚类和双重增强技术，解决了现有方法在多活动序列对齐上的局限性，并实现了更强的泛化能力。实验结果表明，该方法在多个数据集上超越了现有最先进的性能。", "keywords": "序列对齐, 隐式聚类, 自监督学习, 多活动, 泛化能力", "comments": "这项工作通过引入隐式聚类和双重增强，有效地解决了多活动序列对齐的挑战，提高了模型的泛化能力，避免了为每种活动单独训练模型的需要，具有重要的创新性和实用价值。"}}
{"id": "2503.12526", "pdf": "https://arxiv.org/pdf/2503.12526", "abs": "https://arxiv.org/abs/2503.12526", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditID, a training-free approach based on the DiT architecture,\nwhich achieves highly editable customized IDs for text to image generation.\nExisting text-to-image models for customized IDs typically focus more on ID\nconsistency while neglecting editability. It is challenging to alter facial\norientation, character attributes, and other features through prompts. EditID\naddresses this by deconstructing the text-to-image model for customized IDs\ninto an image generation branch and a character feature branch. The character\nfeature branch is further decoupled into three modules: feature extraction,\nfeature fusion, and feature integration. By introducing a combination of\nmapping features and shift features, along with controlling the intensity of ID\nfeature integration, EditID achieves semantic compression of local features\nacross network depths, forming an editable feature space. This enables the\nsuccessful generation of high-quality images with editable IDs while\nmaintaining ID consistency, achieving excellent results in the IBench\nevaluation, which is an editability evaluation framework for the field of\ncustomized ID text-to-image generation that quantitatively demonstrates the\nsuperior performance of EditID. EditID is the first text-to-image solution to\npropose customizable ID editability on the DiT architecture, meeting the\ndemands of long prompts and high quality image generation.", "AI": {"title_translation": "EditID：面向文本到图像生成的免训练可编辑ID定制", "tldr": "EditID是一种基于DiT的免训练方法，为文本到图像生成提供了高度可编辑的定制ID，解决了现有模型在保持ID一致性的同时难以修改特征的问题。", "motivation": "现有定制ID文本到图像模型更侧重于ID一致性而忽略了可编辑性，通过提示词改变面部方向、角色属性和其他特征具有挑战性。", "method": "EditID将定制ID的文本到图像模型分解为图像生成分支和角色特征分支。角色特征分支进一步解耦为特征提取、特征融合和特征集成三个模块。通过引入映射特征和移位特征的组合，并控制ID特征集成的强度，EditID实现了跨网络深度的局部特征语义压缩，形成了可编辑的特征空间。", "result": "这使得能够生成具有可编辑ID的高质量图像，同时保持ID一致性，并在IBench评估中取得了优异结果，IBench是一个针对定制ID文本到图像生成领域的可编辑性评估框架，定量证明了EditID的卓越性能。", "conclusion": "EditID是首个在DiT架构上提出可定制ID可编辑性的文本到图像解决方案，满足了长提示和高质量图像生成的需求。", "translation": "我们提出了EditID，一种基于DiT架构的免训练方法，实现了文本到图像生成中高度可编辑的定制ID。现有定制ID的文本到图像模型通常更侧重于ID一致性而忽略了可编辑性。通过提示词改变面部方向、角色属性和其他特征具有挑战性。EditID通过将定制ID的文本到图像模型分解为图像生成分支和角色特征分支来解决这个问题。角色特征分支进一步解耦为三个模块：特征提取、特征融合和特征集成。通过引入映射特征和移位特征的组合，以及控制ID特征集成的强度，EditID实现了跨网络深度的局部特征语义压缩，形成了一个可编辑的特征空间。这使得能够成功生成具有可编辑ID的高质量图像，同时保持ID一致性，在IBench评估中取得了优异结果，IBench是一个针对定制ID文本到图像生成领域的可编辑性评估框架，定量证明了EditID的卓越性能。EditID是首个在DiT架构上提出可定制ID可编辑性的文本到图像解决方案，满足了长提示和高质量图像生成的需求。", "summary": "EditID是一种新颖的免训练方法，基于DiT架构，用于具有定制ID的文本到图像生成。它通过将模型分解为图像生成分支和角色特征分支（包含特征提取、融合和集成模块）来解决现有模型在ID一致性上优先于可编辑性的局限性。通过映射特征和移位特征的智能组合以及受控的ID特征集成，EditID创建了一个可编辑的特征空间。这使得能够生成具有可编辑ID的高质量图像，同时保持一致性，其在IBench评估中的优异表现也定量证明了这一点。EditID被认为是首个将可定制ID可编辑性引入DiT架构的解决方案，满足了长提示和高质量输出的需求。", "keywords": "EditID, 文本到图像生成, ID定制, 可编辑性, DiT", "comments": "EditID的创新之处在于其免训练方法以及将文本到图像模型分解为图像生成和角色特征独立分支的新颖架构。这种架构，特别是三模块的角色特征分支以及映射/移位特征的使用，有效解决了定制ID生成中长期存在的编辑性挑战，同时保持了ID一致性。其在DiT架构上的适用性以及处理长提示和高质量图像生成的能力，标志着该领域的一个重大进步。"}}
{"id": "2503.12527", "pdf": "https://arxiv.org/pdf/2503.12527", "abs": "https://arxiv.org/abs/2503.12527", "authors": ["Yang Yi", "Kunqing Wang", "Jinpu Zhang", "Zhen Tan", "Xiangke Wang", "Hui Shen", "Dewen Hu"], "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.", "AI": {"title_translation": "一种即插即用的基于学习的IMU偏差因子，用于鲁棒的视觉惯性里程计", "tldr": "本文提出了一种名为IPNet的即插即用学习框架，通过直接利用原始IMU数据估计IMU偏差，显著提高了视觉惯性里程计（VIO）的定位精度和鲁棒性，平均ATE-RMSE指标提升了46%。", "motivation": "低成本惯性测量单元（IMU）的偏差是影响视觉惯性里程计（VIO）性能的关键因素。当视觉跟踪出现错误时，优化后的偏差结果可能严重偏离真实值，从而对系统的稳定性和定位精度产生不利影响。此外，低成本惯性设备中初始偏差误差对系统性能有重大影响。", "method": "本文提出了一种新颖的即插即用框架，其核心是惯性先验网络（IPNet），旨在精确估计IMU偏差。该网络直接利用原始IMU数据估计平均偏差，消除了传统递归预测对历史估计的依赖，有效防止了误差传播。此外，引入了一种迭代方法来计算偏差的平均值以进行网络训练，解决了许多视觉惯性数据集中缺乏偏差标签的问题。", "result": "该框架在两个公共数据集和一个自收集数据集上进行了评估。大量实验表明，该方法显著提高了定位精度和鲁棒性，ATE-RMSE指标平均提高了46%。", "conclusion": "本文提出的IPNet框架能够有效估计IMU偏差，显著提高了视觉惯性里程计的定位精度和鲁棒性，成功应对了低成本IMU偏差带来的挑战。", "translation": "低成本惯性测量单元（IMU）的偏差是影响视觉惯性里程计（VIO）性能的关键因素。特别是当视觉跟踪遇到错误时，优化后的偏差结果可能严重偏离真实值，从而对系统的稳定性与定位精度产生不利影响。在本文中，我们提出了一种新颖的即插即用框架，其核心是惯性先验网络（IPNet），旨在精确估计IMU偏差。鉴于低成本惯性设备中初始偏差误差对系统性能的重大影响，我们的网络直接利用原始IMU数据来估计平均偏差，消除了传统递归预测对历史估计的依赖，并有效防止了误差传播。此外，我们引入了一种迭代方法来计算偏差的平均值以进行网络训练，解决了许多视觉惯性数据集中缺乏偏差标签的问题。该框架在两个公共数据集和一个自收集数据集上进行了评估。大量实验表明，我们的方法显著提高了定位精度和鲁棒性，ATE-RMSE指标平均提高了46%。源代码和视频将在\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git} 提供。", "summary": "本文提出了一种名为惯性先验网络（IPNet）的即插即用学习框架，旨在解决低成本IMU偏差对视觉惯性里程计（VIO）性能的关键影响。针对传统方法中偏差误差传播和标签缺乏的问题，IPNet直接从原始IMU数据估计平均偏差，并引入迭代训练方法。实验结果表明，该方法显著提升了VIO的定位精度和鲁棒性，平均ATE-RMSE指标改善了46%。", "keywords": "IMU偏差, 视觉惯性里程计, 深度学习, 鲁棒性, 定位精度", "comments": "本文的创新点在于提出了一个基于学习的即插即用网络（IPNet），能够直接从原始IMU数据估计偏差，有效避免了传统递归预测中固有的误差传播问题。此外，针对缺乏偏差标签的问题，提出的迭代训练方法提供了一个巧妙的解决方案。这项工作对于提升使用低成本IMU的VIO系统的鲁棒性和精度具有重要意义。"}}
{"id": "2503.12531", "pdf": "https://arxiv.org/pdf/2503.12531", "abs": "https://arxiv.org/abs/2503.12531", "authors": ["Mehmet Kerem Turkcan", "Mattia Ballo", "Filippo Filicori", "Zoran Kostic"], "title": "Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks", "categories": ["cs.CV"], "comment": null, "summary": "We introduce specialized diffusion-based generative models that capture the\nspatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions\nthrough supervised learning on annotated laparoscopic surgery footage. The\nproposed models form a foundation for data-driven world models capable of\nsimulating the biomechanical interactions and procedural dynamics of surgical\nsuturing with high temporal fidelity. Annotating a dataset of $\\sim2K$ clips\nextracted from simulation videos, we categorize surgical actions into\nfine-grained sub-stitch classes including ideal and non-ideal executions of\nneedle positioning, targeting, driving, and withdrawal. We fine-tune two\nstate-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to\ngenerate high-fidelity surgical action sequences at $\\ge$768x512 resolution and\n$\\ge$49 frames. For training our models, we explore both Low-Rank Adaptation\n(LoRA) and full-model fine-tuning approaches. Our experimental results\ndemonstrate that these world models can effectively capture the dynamics of\nsuturing, potentially enabling improved training simulators, surgical skill\nassessment tools, and autonomous surgical systems. The models also display the\ncapability to differentiate between ideal and non-ideal technique execution,\nproviding a foundation for building surgical training and evaluation systems.\nWe release our models for testing and as a foundation for future research.\nProject Page: https://mkturkcan.github.io/suturingmodels/", "AI": {"title_translation": "迈向缝合世界模型：学习机器人手术任务的预测模型", "tldr": "本文介绍了基于扩散的生成模型，用于捕获机器人手术缝合子动作的时空动态，能够模拟生物力学交互和程序动态，并区分理想和非理想技术执行。", "motivation": "开发能够模拟机器人手术缝合的生物力学交互和程序动态的数据驱动世界模型，以改进训练模拟器、手术技能评估工具和自主手术系统。", "method": "引入了专门的基于扩散的生成模型，通过对带注释的腹腔镜手术录像进行监督学习，捕获精细的机器人手术子缝合动作的时空动态。构建了一个包含约2K剪辑的数据集，将手术动作分为精细的子缝合类别，包括针定位、目标、驱动和拔出的理想和非理想执行。微调了两个最先进的视频扩散模型LTX-Video和HunyuanVideo，以生成高保真手术动作序列。训练探索了LoRA和全模型微调方法。", "result": "所提出的世界模型能够有效捕获缝合动态，生成高保真手术动作序列（≥768x512分辨率，≥49帧）。模型还能够区分理想和非理想的技术执行。", "conclusion": "本文提出的世界模型能够有效捕获缝合动态，并区分理想和非理想技术执行，为构建改进的训练模拟器、手术技能评估工具和自主手术系统奠定了基础。", "translation": "我们引入了专门的基于扩散的生成模型，通过对带注释的腹腔镜手术录像进行监督学习，捕获精细机器人手术子缝合动作的时空动态。所提出的模型为数据驱动的世界模型奠定了基础，这些模型能够以高时间保真度模拟手术缝合的生物力学交互和程序动态。我们注释了一个从模拟视频中提取的约2K剪辑的数据集，将手术动作分为精细的子缝合类别，包括针定位、目标、驱动和拔出的理想和非理想执行。我们微调了两个最先进的视频扩散模型LTX-Video和HunyuanVideo，以生成分辨率≥768x512和帧数≥49的高保真手术动作序列。为了训练我们的模型，我们探索了低秩适应（LoRA）和全模型微调方法。我们的实验结果表明，这些世界模型能够有效捕获缝合动态，可能有助于改进训练模拟器、手术技能评估工具和自主手术系统。这些模型还显示出区分理想和非理想技术执行的能力，为构建手术训练和评估系统提供了基础。我们发布了我们的模型，以供测试和作为未来研究的基础。项目页面：https://mkturkcan.github.io/suturingmodels/", "summary": "本研究提出了基于扩散的生成模型，用于学习机器人手术中精细缝合子动作的时空动态。通过对带注释的腹腔镜手术视频进行监督学习，模型能够模拟手术缝合的生物力学和程序动态，并生成高保真度的手术动作序列。模型在区分理想与非理想技术执行方面表现出色，为开发更先进的训练模拟器、技能评估工具和自主手术系统奠定了基础。", "keywords": "机器人手术, 世界模型, 扩散模型, 缝合, 技能评估", "comments": "这项研究的创新之处在于利用扩散模型来捕捉机器人手术中精细动作的时空动态，并能区分操作的理想与非理想执行。这对于提高手术训练的真实性、实现更精准的技能评估以及推动自主手术系统的发展具有重要意义。数据集的构建和对现有先进视频扩散模型的微调也体现了其工程实现能力。"}}
{"id": "2503.12532", "pdf": "https://arxiv.org/pdf/2503.12532", "abs": "https://arxiv.org/abs/2503.12532", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Ziqin Wei", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.", "AI": {"title_translation": "STEVE：一种用于计算机使用代理训练的步骤验证管道", "tldr": "STEVE是一个步骤验证管道，它利用GPT-4o验证轨迹步骤并使用Kahneman和Tversky优化来高效训练计算机使用代理，超越了监督微调，并在WinAgentArena中表现出色。", "motivation": "开发能够自主操作图形用户界面的AI代理是一项长期挑战，行为克隆训练需要大量高质量轨迹，而现有方法难以满足可扩展性需求。", "method": "论文设计了STEVE，一个步骤验证管道。首先建立大型指令集并收集次优代理的轨迹数据。然后使用GPT-4o根据动作执行前后的屏幕来验证轨迹中每个步骤的正确性，并分配二元标签。最后，采用卡尼曼-特沃斯基优化来优化代理。", "result": "实验表明，该代理通过利用轨迹中的正向和负向动作，优于监督微调。STEVE还能够训练一个7B的视觉-语言模型作为计算机使用代理，在WinAgentArena实时桌面环境中实现了领先性能，并且效率高、成本低。", "conclusion": "STEVE提供了一种高效且可扩展的方法来训练计算机使用代理，通过步骤验证和优化，显著提升了代理在复杂环境中的性能，并降低了训练成本。", "translation": "开发能够自主操作图形用户界面的AI代理是一项长期且具有挑战性的任务。最近数据缩放定律的进展启发我们使用缩放指令集来训练计算机使用代理，但使用行为克隆训练代理仍然需要大量高质量的轨迹。为了满足可扩展性需求，我们设计了STEVE，一个用于计算机使用代理训练的步骤验证管道。首先，我们为计算机使用代理建立了一个大型指令集，并收集了一些次优代理的轨迹数据。GPT-4o用于根据动作执行前后的屏幕验证轨迹中每个步骤的正确性，为每个步骤分配一个二进制标签。最后，我们采用卡尼曼-特沃斯基优化来优化基于二进制步骤标签的代理。大量实验表明，我们的代理通过利用轨迹中的正向和负向动作，超越了监督微调。此外，STEVE使我们能够训练一个7B的视觉-语言模型作为计算机使用代理，在具有挑战性的实时桌面环境WinAgentArena中以极高的效率和更低的成本实现了领先性能。代码和数据：https://github.com/FanbinLu/STEVE。", "summary": "本文提出了STEVE，一个用于计算机使用代理训练的步骤验证管道，旨在解决行为克隆训练中对大量高质量轨迹的需求。STEVE通过构建大规模指令集，利用GPT-4o对次优代理生成的轨迹进行步骤级验证和二元标签分配，并采用卡尼曼-特沃斯基优化来训练代理。实验证明，STEVE训练的代理在性能上优于监督微调，并能高效训练7B视觉-语言模型在WinAgentArena等复杂桌面环境中取得领先表现。", "keywords": "计算机使用代理, 步骤验证, GPT-4o, 卡尼曼-特沃斯基优化, 行为克隆", "comments": "STEVE的创新之处在于其独特的步骤验证管道，利用大模型（GPT-4o）对次优轨迹进行细粒度标注，并结合卡尼曼-特沃斯基优化，有效解决了高质量数据稀缺的问题，提高了训练效率和模型性能。这对于开发更强大的通用计算机使用AI代理具有重要意义。"}}
{"id": "2503.12535", "pdf": "https://arxiv.org/pdf/2503.12535", "abs": "https://arxiv.org/abs/2503.12535", "authors": ["Guibiao Liao", "Qing Li", "Zhenyu Bao", "Guoping Qiu", "Kanglin Liu"], "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. The project page is available at\n  https://gbliao.github.io/SPC-GS.github.io/", "summary": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches\nhave shown significant performance with dense input images. However, they\nexhibit poor performance when confronted with sparse inputs, primarily due to\nthe sparse distribution of Gaussian points and insufficient view supervision.\nTo relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based\nGaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)\nRegularization for open-world free view synthesis with sparse inputs.\nSpecifically, SGI provides a dense, scene-layout-based Gaussian distribution by\nutilizing view-changed images generated from the video generation model and\nview-constraint Gaussian points densification. Additionally, SPC mitigates\nlimited view supervision by employing semantic-prompt-based consistency\nconstraints developed by SAM2. This approach leverages available semantics from\ntraining views, serving as instructive prompts, to optimize visually\noverlapping regions in novel views with 2D and 3D consistency constraints.\nExtensive experiments demonstrate the superior performance of SPC-GS across\nReplica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in\nPSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world\nsemantic segmentation.", "AI": {"title_translation": "SPC-GS：基于语义提示一致性的高斯泼溅，用于稀疏输入下的室内开放世界自由视角合成", "tldr": "SPC-GS提出了一种新的高斯泼溅方法，通过场景布局初始化和语义提示一致性正则化，解决了稀疏输入下室内开放世界自由视角合成性能差的问题。", "motivation": "现有的基于3D高斯泼溅的室内开放世界自由视角合成方法在面对稀疏输入时表现不佳，主要原因是高斯点分布稀疏和视角监督不足。", "method": "本文提出了SPC-GS，它利用了场景布局高斯初始化（SGI）和语义提示一致性（SPC）正则化。SGI通过使用视频生成模型生成的视角变化图像和视角约束高斯点稠密化来提供稠密的、基于场景布局的高斯分布。SPC则通过使用SAM2开发的基于语义提示的一致性约束来缓解有限的视角监督，利用训练视角中可用的语义作为指导性提示，通过2D和3D一致性约束优化新颖视角中的视觉重叠区域。", "result": "在Replica和ScanNet基准测试中，SPC-GS表现出卓越的性能。在重建质量方面，PSNR提高了3.06 dB；在开放世界语义分割方面，mIoU提高了7.3%。", "conclusion": "SPC-GS通过解决稀疏输入下的高斯点分布和视角监督不足问题，显著提升了室内开放世界自由视角合成的性能和语义分割能力。", "translation": "基于3D高斯泼溅的室内开放世界自由视角合成方法在密集输入图像下表现出显著的性能。然而，当面临稀疏输入时，它们表现不佳，这主要是由于高斯点分布稀疏和视角监督不足。为了缓解这些挑战，我们提出了SPC-GS，它利用基于场景布局的高斯初始化（SGI）和语义提示一致性（SPC）正则化，用于稀疏输入下的开放世界自由视角合成。具体来说，SGI通过利用视频生成模型生成的视角变化图像和视角约束高斯点稠密化，提供了密集、基于场景布局的高斯分布。此外，SPC通过采用SAM2开发的基于语义提示的一致性约束来缓解有限的视角监督。这种方法利用训练视角中可用的语义作为指导性提示，通过2D和3D一致性约束优化新颖视角中的视觉重叠区域。广泛的实验表明，SPC-GS在Replica和ScanNet基准测试中表现出卓越的性能。值得注意的是，我们的SPC-GS在重建质量方面实现了3.06 dB的PSNR增益，在开放世界语义分割方面实现了7.3%的mIoU改进。", "summary": "本文提出SPC-GS，一种针对稀疏输入下室内开放世界自由视角合成的3D高斯泼溅方法。为解决稀疏输入导致的高斯点分布稀疏和视角监督不足问题，SPC-GS引入了场景布局高斯初始化（SGI）和语义提示一致性（SPC）正则化。SGI通过生成视角变化图像和高斯点稠密化实现密集高斯分布，而SPC则利用SAM2的语义提示来增强视角监督。实验证明，SPC-GS在重建质量和语义分割方面均优于现有方法。", "keywords": "高斯泼溅, 自由视角合成, 稀疏输入, 语义提示, 3D重建", "comments": "SPC-GS的创新点在于结合了场景布局信息进行高斯初始化和利用语义提示进行一致性约束，有效解决了稀疏输入下3D高斯泼溅的性能瓶颈。尤其利用SAM2的语义能力来弥补视角监督的不足，是一个新颖且有效的方法，对于稀疏数据条件下的3D重建和渲染具有重要意义。"}}
{"id": "2503.12539", "pdf": "https://arxiv.org/pdf/2503.12539", "abs": "https://arxiv.org/abs/2503.12539", "authors": ["Weiguang Zhao", "Rui Zhang", "Qiufeng Wang", "Guangliang Cheng", "Kaizhu Huang"], "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps://github.com/weiguangzhao/BFANet.", "AI": {"title_translation": "BFANet: 基于边界特征分析的3D语义分割再探索", "tldr": "BFANet通过分析边界特征并引入新的错误分类及度量，提升了3D语义分割在挑战性区域的表现，其伪标签计算速度显著加快。", "motivation": "当前最先进的3D语义分割技术主要关注整体性能提升，却忽视了对难以分割的挑战性区域的探索。本文旨在通过更细致的视角重新审视3D语义分割，解决这些被忽略的复杂性。", "method": "本文将3D语义分割错误划分为四类，并为每类设计了相应的评估指标。在此基础上，提出了一种创新的3D语义分割网络BFANet，该网络整合了对语义边界特征的详细分析。具体方法包括：1) 设计边界-语义模块，将点云特征解耦为语义特征和边界特征，并融合它们的查询队列以通过注意力机制增强语义特征；2) 引入一种更简洁、更快速的边界伪标签计算算法，其速度比现有技术快3.9倍，且兼容数据增强。", "result": "在基准数据集上的广泛实验表明，BFANet模型具有优越性，证实了强调本文独特设计的四种度量标准的重要性。其边界伪标签计算算法比现有技术快3.9倍。", "conclusion": "通过强调本文独特设计的四种度量标准，并引入BFANet模型进行边界特征分析，显著提升了3D语义分割在挑战性区域的表现。", "translation": "3D语义分割在理解3D场景中扮演着基础且关键的角色。尽管当前最先进的技术主要集中于提升基于通用指标（例如mIoU、mAcc和oAcc）的3D语义分割整体性能，但它们不幸地大多忽略了对分割挑战性区域的探索。在本文中，我们通过更细致的视角重新审视3D语义分割，揭示了通常被更广泛的性能指标所掩盖的细微复杂性。具体而言，我们将3D语义分割错误划分为四个综合类别，并为每个类别量身定制了相应的评估指标。基于这个分类框架，我们引入了一种创新的3D语义分割网络——BFANet，它结合了对语义边界特征的详细分析。首先，我们设计了边界-语义模块，将点云特征解耦为语义特征和边界特征，并融合它们的查询队列以通过注意力机制增强语义特征。其次，我们引入了一种更简洁、更快速的边界伪标签计算算法，其速度比现有技术快3.9倍，兼容数据增强，并在训练中实现高效计算。在基准数据上进行的广泛实验表明，我们的BFANet模型具有优越性，证实了强调这四个独特设计的指标的重要性。代码可在https://github.com/weiguangzhao/BFANet获取。", "summary": "本文针对3D语义分割中挑战性区域被忽视的问题，提出了BFANet网络。研究将3D语义分割错误细分为四类并设计了相应评估指标。BFANet通过边界-语义模块解耦并增强语义特征，并引入了一种加速3.9倍的边界伪标签计算算法。实验证明BFANet在性能上优于现有模型，并强调了新度量的重要性。", "keywords": "3D语义分割, 边界特征分析, BFANet, 错误分类, 伪标签计算", "comments": "本文的创新点在于其对3D语义分割错误进行了细致的分类，并提出了针对这些挑战性区域的评估指标，这为该领域的研究提供了新的视角。BFANet通过边界特征分析，特别是边界-语义模块和高效的伪标签计算，有效提升了分割精度，尤其是在边界区域。该工作对提升3D场景理解的鲁棒性具有重要意义。"}}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "AI": {"title_translation": "ST-Think：多模态大型语言模型如何从自我中心视角对4D世界进行推理", "tldr": "本文探讨了多模态大型语言模型（MLLM）如何从自我中心视角对4D世界进行时空推理。为此，作者提出了一个新基准Ego-ST Bench和一个视频推理模型ST-R1，该模型结合了逆向思维和强化学习，显著提升了性能。", "motivation": "人类擅长从自我中心视角理解动态视觉事件，进行时空推理。然而，多模态大型语言模型（MLLM）是否能同样理解4D世界尚不确定。本文旨在赋予MLLM类人推理能力。", "method": "本文引入了Ego-ST Bench，一个包含5000多个问答对的新基准，用于系统评估空间、时间和综合时空推理。此外，还提出了ST-R1视频模型，一个将逆向思维融入强化学习过程的视频推理模型。该模型结合了长链思维（long-CoT）监督微调和群组相对策略优化（GRPO）强化学习。", "result": "ST-R1模型显著提升了性能，并使用有限的高质量数据取得了显著改进。", "conclusion": "Ego-ST Bench和ST-R1模型为推进基于视频的时空推理研究提供了宝贵的见解和资源。", "translation": "人类擅长时空推理，能够毫不费力地从自我中心视角解读动态视觉事件。然而，多模态大型语言模型（MLLMs）是否也能同样理解4D世界仍不确定。本文探讨了从自我中心视角进行多模态时空推理，旨在赋予MLLMs类人推理能力。为了支持这一目标，我们引入了Ego-ST Bench，一个新颖的基准，包含超过5000个跨越四类别的问答对，系统地评估空间、时间以及综合时空推理能力。此外，我们提出了ST-R1视频模型，一个基于视频的推理模型，它将逆向思维融入其强化学习过程中，显著提升了性能。我们将长链思维（long-CoT）监督微调与群组相对策略优化（GRPO）强化学习相结合，在有限的高质量数据下取得了显著改进。Ego-ST Bench和ST-R1为推进基于视频的时空推理研究提供了宝贵的见解和资源。", "summary": "本文探讨了多模态大型语言模型（MLLM）在从自我中心视角进行4D世界时空推理方面的能力。为解决MLLM在此方面的不足，研究提出了一个名为Ego-ST Bench的新型基准，包含超过5000个问答对，用于全面评估时空推理能力。同时，开发了ST-R1视频模型，该模型通过将逆向思维整合到其强化学习过程中，并结合长链思维（long-CoT）监督微调和群组相对策略优化（GRPO）强化学习，显著提高了性能。Ego-ST Bench和ST-R1共同为推进视频时空推理研究提供了重要资源和见解。", "keywords": "多模态大语言模型, 时空推理, 自我中心视频, 基准, 强化学习", "comments": "本文通过引入一个专门的基准Ego-ST Bench和一个结合了逆向思维和先进强化学习（GRPO）的创新模型ST-R1，显著推动了多模态大型语言模型在自我中心视角4D时空推理领域的研究。其在有限高质量数据下取得显著改进的能力，突显了该方法的效率和潜力。"}}
{"id": "2503.12545", "pdf": "https://arxiv.org/pdf/2503.12545", "abs": "https://arxiv.org/abs/2503.12545", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Xiaojiang Peng", "Kai Wang", "Yang You", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.", "AI": {"title_translation": "PEBench：一个用于基准测试多模态大型语言模型机器遗忘的虚构数据集", "tldr": "PEBench是一个新基准，旨在解决多模态大语言模型（MLLMs）中机器遗忘（MU）评估不完整的问题，通过提供一个包含个人实体和事件场景的数据集，以促进安全和隐私保护模型的研究。", "motivation": "多模态大型语言模型（MLLMs）的进步依赖于大量数据，引发了隐私和安全问题。机器遗忘（MU）是解决这些问题的方案，但目前对MLLMs中MU的评估不完整，且底层问题定义不清，阻碍了更安全、更可信系统的发展。", "method": "引入了一个名为PEBench的基准，其中包括一个包含个人实体和相应通用事件场景的数据集，旨在全面评估MLLMs的机器遗忘性能。通过PEBench，旨在提供一个标准化和稳健的框架来推进安全和隐私保护多模态模型的研究。", "result": "基准测试了6种机器遗忘方法，揭示了它们的优点和局限性，并阐明了MLLMs中机器遗忘的关键挑战和机遇。", "conclusion": "PEBench提供了一个标准化且鲁棒的框架，以推进安全和隐私保护多模态模型的研究，并帮助揭示了机器遗忘在MLLMs中的关键挑战和机遇。", "translation": "近年来，多模态大型语言模型（MLLMs）在视觉问答、视觉理解和推理等任务中展现出显著进步。然而，这种令人印象深刻的进展依赖于从互联网收集的大量数据，引发了对隐私和安全的重大担忧。为了解决这些问题，机器遗忘（MU）已成为一种有前景的解决方案，它能够从已训练的模型中移除特定知识，而无需从头开始重新训练。尽管MLLMs的机器遗忘已受到关注，但目前对其有效性的评估仍不完整，并且底层问题常常定义不清，这阻碍了创建更安全、更可信系统的策略开发。为了弥补这一差距，我们引入了一个名为PEBench的基准，其中包括一个个人实体和相应通用事件场景的数据集，旨在全面评估MLLMs的机器遗忘性能。通过PEBench，我们旨在提供一个标准化和稳健的框架，以推进安全和隐私保护多模态模型的研究。我们对6种机器遗忘方法进行了基准测试，揭示了它们的优点和局限性，并阐明了MLLMs中机器遗忘的关键挑战和机遇。", "summary": "该论文介绍了PEBench，这是一个针对多模态大型语言模型（MLLMs）机器遗忘（MU）的新基准。鉴于MLLMs对大量数据的依赖引发的隐私和安全担忧，以及当前MU评估的不足，PEBench提供了一个包含个人实体和事件场景的虚构数据集，旨在全面评估MU方法。研究人员利用PEBench对六种MU方法进行了基准测试，揭示了它们的优缺点，并指出了MLLMs中MU面临的关键挑战和机遇，从而为安全和隐私保护的MLLMs研究提供了一个标准化框架。", "keywords": "机器遗忘, 多模态大语言模型, 基准测试, 数据集, 隐私保护", "comments": "该论文通过引入PEBench基准，创新性地解决了多模态大型语言模型（MLLMs）中机器遗忘（MU）评估标准不足的问题。其重要性在于为未来MU方法的发展提供了一个急需的标准化和系统化的评估框架，有助于推动隐私保护和安全AI系统的研究。数据集的“虚构性”可能是一个有趣的方面，因为它可能简化了数据隐私合规性问题，同时仍能模拟真实世界的遗忘场景。"}}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindström", "Peng Su", "Matthias Nießner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.", "AI": {"title_translation": "MTGS: 多次遍历高斯泼溅", "tldr": "MTGS是一种处理多次遍历数据的新方法，通过分离静态和动态元素来重建高质量的驾驶场景，显著提升了新视角合成和几何精度。", "motivation": "多遍历数据在场景重建中具有巨大潜力，但其固有的挑战，如外观变化和动态物体，常导致重建质量不佳。", "method": "我们提出了多遍历高斯泼溅（MTGS），一种新颖的方法，通过建模共享的静态几何，同时单独处理动态元素和外观变化，从任意收集的多遍历数据中重建高质量驾驶场景。该方法采用多遍历动态场景图，包含共享静态节点和遍历特定的动态节点，并辅以带有可学习球谐系数残差的颜色校正节点。", "result": "与单次遍历基线相比，MTGS在LPIPS上提高了23.5%，在几何精度上提高了46.3%。", "conclusion": "MTGS通过有效处理多遍历数据中的静态和动态元素及外观变化，实现了高保真度的新视角合成和任意视角的导航灵活性，显著提升了重建质量。", "translation": "多遍历数据，通常通过日常通勤或自动驾驶车队收集，为道路块内的场景重建提供了多个视点。这些数据为高质量的新视角合成提供了巨大潜力，这对于自动驾驶模拟器等应用至关重要。然而，多遍历数据固有的挑战常常导致重建质量不佳，包括外观变化和动态物体的存在。为了解决这些问题，我们提出了多遍历高斯泼溅（MTGS），一种新颖的方法，通过建模共享的静态几何，同时单独处理动态元素和外观变化，从任意收集的多遍历数据中重建高质量的驾驶场景。我们的方法采用多遍历动态场景图，包含共享静态节点和遍历特定的动态节点，并辅以带有可学习球谐系数残差的颜色校正节点。这种方法能够实现高保真度的新视角合成，并提供导航任何视点的灵活性。我们对大规模驾驶数据集nuPlan（包含多遍历数据）进行了广泛实验。我们的结果表明，与单遍历基线相比，MTGS在LPIPS上提高了23.5%，在几何精度上提高了46.3%。代码和数据将公开可用。", "summary": "MTGS是一种新颖的多遍历高斯泼溅方法，旨在解决自动驾驶场景重建中多遍历数据固有的挑战，如外观变化和动态物体。该方法通过构建一个包含共享静态几何和独立动态元素、外观变化处理的多遍历动态场景图，显著提升了高质量驾驶场景的重建能力。实验结果显示，MTGS在LPIPS和几何精度上均优于现有单遍历基线，实现了高保真度的新视角合成。", "keywords": "多遍历数据, 高斯泼溅, 场景重建, 新视角合成, 动态场景图", "comments": "该论文提出了一种创新的方法来处理多遍历数据在场景重建中的复杂性，特别是通过分离静态和动态元素以及处理外观变化。这种方法对于自动驾驶模拟器等需要高精度新视角合成的应用具有重要意义。其核心创新在于多遍历动态场景图的设计，以及颜色校正节点的引入，有效解决了实际数据中的挑战。成果显著，并在大规模数据集上进行了验证。"}}
{"id": "2503.12559", "pdf": "https://arxiv.org/pdf/2503.12559", "abs": "https://arxiv.org/abs/2503.12559", "authors": ["Xiao Wang", "Qingyi Si", "Jianlong Wu", "Shiyu Zhu", "Li Cao", "Liqiang Nie"], "title": "AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized video\nunderstanding, yet are still limited by context length when processing long\nvideos. Recent methods compress videos by leveraging visual redundancy\nuniformly, yielding promising results. Nevertheless, our quantitative analysis\nshows that redundancy varies significantly across time and model layers,\nnecessitating a more flexible compression strategy. We propose AdaReTaKe, a\ntraining-free method that flexibly reduces visual redundancy by allocating\ncompression ratios among time and layers with theoretical guarantees.\nIntegrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity\nfrom 256 to 2048 frames while preserving critical information. Experiments on\nVideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe\noutperforms existing methods by 2.3% and 2.8% for 7B and 72B models,\nrespectively, with even greater improvements of 5.9% and 6.0% on the longest\nLVBench. Our code is available at\nhttps://github.com/SCZwangxiao/video-FlexReduc.git.", "AI": {"title_translation": "AdaReTaKe：视频语言理解中自适应冗余减少以感知更长内容", "tldr": "AdaReTaKe是一种无需训练的方法，通过自适应地减少视频冗余，使多模态大语言模型能处理更长的视频，并显著提升性能。", "motivation": "多模态大语言模型（MLLMs）在处理长视频时受限于上下文长度，现有方法均匀压缩视频冗余，但定量分析显示冗余在时间和模型层之间差异显著，需要更灵活的压缩策略。", "method": "提出AdaReTaKe，一种无需训练的方法，通过在时间和模型层之间分配压缩比来灵活减少视觉冗余，并提供理论保证。", "result": "AdaReTaKe将MLLMs的处理能力从256帧提高到2048帧，同时保留关键信息。在VideoMME、MLVU、LongVideoBench和LVBench数据集上的实验表明，AdaReTaKe在7B和72B模型上分别优于现有方法2.3%和2.8%，在最长的LVBench上提升更大，分别为5.9%和6.0%。", "conclusion": "AdaReTaKe通过自适应冗余减少，有效解决了多模态大语言模型处理长视频时的上下文长度限制，并显著提升了性能。", "translation": "多模态大语言模型（MLLMs）彻底改变了视频理解，但在处理长视频时仍受限于上下文长度。最近的方法通过统一利用视觉冗余来压缩视频，取得了有希望的结果。然而，我们的定量分析表明，冗余在时间和模型层之间差异显著，需要更灵活的压缩策略。我们提出了AdaReTaKe，一种无需训练的方法，通过在时间和层之间分配压缩比来灵活减少视觉冗余，并提供理论保证。集成到最先进的MLLMs中，AdaReTaKe将处理能力从256帧提高到2048帧，同时保留关键信息。在VideoMME、MLVU、LongVideoBench和LVBench数据集上的实验表明，AdaReTaKe在7B和72B模型上分别优于现有方法2.3%和2.8%，在最长的LVBench上甚至有更大的改进，分别为5.9%和6.0%。我们的代码可在https://github.com/SCZwangxiao/video-FlexReduc.git获取。", "summary": "本文提出了AdaReTaKe，一种无需训练的自适应冗余减少方法，旨在解决多模态大语言模型处理长视频时的上下文长度限制。通过定量分析发现视频冗余在时间和模型层间差异显著，AdaReTaKe能够灵活地在时间和层间分配压缩比，从而有效压缩视频并保留关键信息。实验证明，AdaReTaKe显著提升了MLLMs对长视频的处理能力，并在多个基准数据集上超越了现有方法，尤其在处理极长视频时表现出更显著的性能提升。", "keywords": "视频语言理解, 多模态大语言模型, 冗余减少, 长视频处理, 自适应压缩", "comments": "AdaReTaKe的创新之处在于其无需训练的自适应冗余减少策略，能够根据视频内容和模型层级的不同动态调整压缩比，有效突破了MLLMs在处理长视频时的上下文长度瓶颈。其理论保证和显著的实验性能提升，特别是对超长视频的处理能力，使其成为视频-语言理解领域的重要进展。"}}
{"id": "2503.12562", "pdf": "https://arxiv.org/pdf/2503.12562", "abs": "https://arxiv.org/abs/2503.12562", "authors": ["Ruopeng Gao", "Yuyao Wang", "Chunxu Liu", "Limin Wang"], "title": "History-Aware Transformation of ReID Features for Multiple Object Tracking", "categories": ["cs.CV"], "comment": "Tech report. Without bells and whistles, achieving 80.8 HOTA on\n  SportsMOT", "summary": "The aim of multiple object tracking (MOT) is to detect all objects in a video\nand bind them into multiple trajectories. Generally, this process is carried\nout in two steps: detecting objects and associating them across frames based on\nvarious cues and metrics. Many studies and applications adopt object\nappearance, also known as re-identification (ReID) features, for target\nmatching through straightforward similarity calculation. However, we argue that\nthis practice is overly naive and thus overlooks the unique characteristics of\nMOT tasks. Unlike regular re-identification tasks that strive to distinguish\nall potential targets in a general representation, multi-object tracking\ntypically immerses itself in differentiating similar targets within the same\nvideo sequence. Therefore, we believe that seeking a more suitable feature\nrepresentation space based on the different sample distributions of each\nsequence will enhance tracking performance. In this paper, we propose using\nhistory-aware transformations on ReID features to achieve more discriminative\nappearance representations. Specifically, we treat historical trajectory\nfeatures as conditions and employ a tailored Fisher Linear Discriminant (FLD)\nto find a spatial projection matrix that maximizes the differentiation between\ndifferent trajectories. Our extensive experiments reveal that this\ntraining-free projection can significantly boost feature-only trackers to\nachieve competitive, even superior tracking performance compared to\nstate-of-the-art methods while also demonstrating impressive zero-shot transfer\ncapabilities. This demonstrates the effectiveness of our proposal and further\nencourages future investigation into the importance and customization of ReID\nmodels in multiple object tracking. The code will be released at\nhttps://github.com/HELLORPG/HATReID-MOT.", "AI": {"title_translation": "用于多目标跟踪的ReID特征历史感知变换", "tldr": "本文提出了一种历史感知的ReID特征变换方法，以提高多目标跟踪（MOT）的性能。通过定制的Fisher线性判别（FLD）实现无训练的投影，该方法显著提升了跟踪器的表现，并展现出强大的零样本迁移能力。", "motivation": "多目标跟踪（MOT）中，将Re-ID特征直接用于目标匹配过于简单，忽略了MOT任务中区分同一视频序列中相似目标的独特需求。因此，需要寻找更合适的特征表示空间来提升跟踪性能。", "method": "提出对ReID特征进行历史感知变换，以获得更具判别力的外观表示。具体而言，将历史轨迹特征作为条件，并采用定制的Fisher线性判别（FLD）来寻找一个空间投影矩阵，以最大化不同轨迹之间的区分度。此投影是无需训练的。", "result": "实验表明，这种无需训练的投影可以显著提升仅依赖特征的跟踪器，使其达到与最先进方法相当甚至更优的跟踪性能，同时还表现出令人印象深刻的零样本迁移能力。", "conclusion": "该提案被证明是有效的，并进一步鼓励未来深入研究ReID模型在多目标跟踪中的重要性和定制化。", "translation": "多目标跟踪（MOT）的目标是检测视频中的所有对象并将它们绑定成多条轨迹。通常，这个过程分两步进行：检测对象，并根据各种线索和度量在帧之间关联它们。许多研究和应用采用对象外观，也称为重识别（ReID）特征，通过直接的相似性计算进行目标匹配。然而，我们认为这种做法过于简单，因此忽略了MOT任务的独特特征。与旨在区分一般表示中所有潜在目标的常规重识别任务不同，多目标跟踪通常专注于区分同一视频序列中的相似目标。因此，我们认为根据每个序列的不同样本分布寻找更合适的特征表示空间将增强跟踪性能。在本文中，我们提出使用历史感知变换对ReID特征进行处理，以实现更具判别力的外观表示。具体来说，我们将历史轨迹特征视为条件，并采用定制的Fisher线性判别（FLD）来寻找一个空间投影矩阵，该矩阵可以最大化不同轨迹之间的区分度。我们的大量实验表明，这种无需训练的投影可以显著提升仅依赖特征的跟踪器，使其达到与最先进方法相当甚至更优的跟踪性能，同时还表现出令人印象深刻的零样本迁移能力。这证明了我们提案的有效性，并进一步鼓励未来深入研究ReID模型在多目标跟踪中的重要性和定制化。代码将发布在 https://github.com/HELLORPG/HATReID-MOT。", "summary": "多目标跟踪（MOT）通常采用ReID特征进行目标关联，但本文认为这种做法过于简单，忽略了MOT区分同一视频序列中相似目标的特性。为解决此问题，论文提出了一种历史感知的ReID特征变换方法，通过将历史轨迹特征作为条件，利用定制的Fisher线性判别（FLD）来学习一个无需训练的空间投影矩阵，以最大化不同轨迹间的区分度。实验结果显示，该方法显著提升了跟踪性能，甚至超越了现有先进方法，并展现出优秀的零样本迁移能力，证明了其有效性及ReID模型在MOT中定制化的潜力。", "keywords": "多目标跟踪, Re-ID, 特征变换, Fisher线性判别, 历史感知", "comments": "该论文的创新点在于提出了历史感知的ReID特征变换，并利用定制的FLD进行无训练的特征投影，有效地解决了传统ReID特征在MOT中区分度不足的问题。其无需训练的特性和出色的零样本迁移能力是其重要优势，为未来MOT中ReID特征的应用提供了新的思路。"}}
{"id": "2503.12567", "pdf": "https://arxiv.org/pdf/2503.12567", "abs": "https://arxiv.org/abs/2503.12567", "authors": ["Abyad Enan", "Mashrur Chowdhury"], "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) for possible publication", "summary": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel.", "AI": {"title_translation": "基于GAN的交通标志分类单阶段防御对抗补丁攻击", "tldr": "本研究提出了一种基于GAN的单阶段防御策略，用于保护自动驾驶车辆的交通标志分类系统免受对抗性补丁攻击，该策略无需预先了解补丁设计，对不同尺寸的补丁均有效，并显著提高了分类准确性。", "motivation": "自动驾驶车辆（AVs）的感知模块容易受到对抗性攻击，特别是对抗性补丁攻击（APA），这可能导致交通标志的错误分类，从而引发灾难性事故。因此，需要增强AV感知系统的安全性。", "method": "本研究开发了一种基于生成对抗网络（GAN）的单阶段防御策略，用于交通标志分类，以防御对抗性补丁攻击。该方法无需预先了解补丁的设计，并且对不同尺寸的补丁均有效。该防御策略是模型无关的。", "result": "实验分析表明，与没有防御机制的分类器相比，所提出的防御策略在APA条件下将分类器的准确性提高了80.8%，并将所考虑的所有交通标志的整体分类准确性提高了58%。该策略对不同尺寸的补丁均有效。", "conclusion": "本研究提出的基于GAN的单阶段防御策略能有效增强自动驾驶车辆交通标志分类系统抵御对抗性补丁攻击的能力，显著提高了分类准确性，并且具有模型无关的优点。", "translation": "计算机视觉在确保自动驾驶车辆（AVs）安全导航方面发挥着关键作用。AV感知模块负责捕获和解释周围环境，以促进安全导航。该模块使AV能够识别交通标志、交通灯和各种道路使用者。然而，感知模块容易受到对抗性攻击，这会损害其准确性和可靠性。其中一种攻击是对抗性补丁攻击（APA），这是一种物理攻击，攻击者策略性地在物体上放置一个特殊制作的贴纸，以欺骗物体分类器。在APA中，对抗性补丁被放置在目标物体上，导致分类器错误识别。这种APA可能导致AV错误分类交通标志，从而引发灾难性事故。为了增强AV感知系统抵御APA的安全性，本研究开发了一种基于生成对抗网络（GAN）的交通标志分类单阶段防御策略。这种方法旨在防御针对不同类别交通标志的APA，而无需预先了解补丁的设计。本研究发现这种方法对不同尺寸的补丁均有效。我们的实验分析表明，本文提出的防御策略在APA条件下将分类器的准确性提高了80.8%，并且与没有任何防御机制的分类器相比，将本研究中考虑的所有交通标志的整体分类准确性提高了58%。我们的防御策略是模型无关的，这使其适用于任何交通标志分类器，无论其底层分类模型如何。", "summary": "本研究提出了一种基于生成对抗网络（GAN）的单阶段防御策略，旨在保护自动驾驶车辆的交通标志分类系统免受对抗性补丁攻击（APA）。该方法无需预先了解攻击补丁的设计，且对不同尺寸的补丁均有效。实验结果显示，与无防御机制的分类器相比，该策略在APA条件下将分类准确性提升了高达80.8%，整体分类准确性提升了58%。此外，该防御策略是模型无关的，可广泛应用于各类交通标志分类器。", "keywords": "GAN, 对抗性补丁攻击, 交通标志分类, 自动驾驶, 单阶段防御", "comments": "本文提出了一种新颖的基于GAN的单阶段防御策略，用于对抗物理世界的对抗性补丁攻击，这对于自动驾驶车辆的安全至关重要。其创新点在于无需预先了解补丁设计即可进行防御，并且是模型无关的，大大增加了其适用性。实验结果显示出显著的性能提升，表明该方法在实际应用中具有巨大潜力，为自动驾驶系统的鲁棒性提供了重要保障。"}}
{"id": "2503.12572", "pdf": "https://arxiv.org/pdf/2503.12572", "abs": "https://arxiv.org/abs/2503.12572", "authors": ["Francesco Girlanda", "Denys Rozumnyi", "Marc Pollefeys", "Martin R. Oswald"], "title": "Deblur Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp\nreconstructions from motion-blurred inputs. The proposed method bridges the\nstrengths of both frame-to-frame and frame-to-model approaches to model\nsub-frame camera trajectories that lead to high-fidelity reconstructions in\nmotion-blurred settings. Moreover, our pipeline incorporates techniques such as\nonline loop closure and global bundle adjustment to achieve a dense and precise\nglobal trajectory. We model the physical image formation process of\nmotion-blurred images and minimize the error between the observed blurry images\nand rendered blurry images obtained by averaging sharp virtual sub-frame\nimages. Additionally, by utilizing a monocular depth estimator alongside the\nonline deformation of Gaussians, we ensure precise mapping and enhanced image\ndeblurring. The proposed SLAM pipeline integrates all these components to\nimprove the results. We achieve state-of-the-art results for sharp map\nestimation and sub-frame trajectory recovery both on synthetic and real-world\nblurry input data.", "AI": {"title_translation": "去模糊高斯泼溅SLAM", "tldr": "Deblur-SLAM是一种RGB SLAM管道，旨在从运动模糊的输入中恢复清晰的重建和精确的轨迹，通过建模子帧相机运动和集成去模糊技术实现最先进的性能。", "motivation": "该论文旨在解决从运动模糊的RGB输入中恢复清晰三维重建和准确相机轨迹的挑战，因为运动模糊会严重影响传统SLAM系统的性能。", "method": "Deblur-SLAM结合了帧到帧和帧到模型方法来建模子帧相机轨迹。它集成了在线闭环和全局捆绑调整以实现精确的全局轨迹。该方法对运动模糊图像的物理形成过程进行建模，并最小化观察到的模糊图像与通过平均清晰虚拟子帧图像获得的渲染模糊图像之间的误差。此外，它还利用单目深度估计器和高斯在线变形来确保精确映射和增强图像去模糊。", "result": "该方法在合成和真实世界的模糊输入数据上，在清晰地图估计和子帧轨迹恢复方面都达到了最先进的结果。", "conclusion": "Deblur-SLAM通过有效地整合子帧轨迹建模、物理图像形成过程模拟、在线闭环、全局捆绑调整、单目深度估计和高斯在线变形等组件，显著提高了从运动模糊输入中恢复清晰地图和准确轨迹的能力，并达到了最先进的性能。", "translation": "我们提出了Deblur-SLAM，这是一个鲁棒的RGB SLAM管道，旨在从运动模糊的输入中恢复清晰的重建。所提出的方法结合了帧到帧和帧到模型方法的优点，以建模导致运动模糊设置中高保真重建的子帧相机轨迹。此外，我们的管道结合了在线闭环和全局捆绑调整等技术，以实现密集和精确的全局轨迹。我们对运动模糊图像的物理图像形成过程进行建模，并最小化观察到的模糊图像与通过平均清晰的虚拟子帧图像获得的渲染模糊图像之间的误差。此外，通过利用单目深度估计器以及高斯在线变形，我们确保了精确的映射和增强的图像去模糊。所提出的SLAM管道集成了所有这些组件以改善结果。我们在合成和真实世界的模糊输入数据上，在清晰地图估计和子帧轨迹恢复方面都达到了最先进的结果。", "summary": "Deblur-SLAM是一种鲁棒的RGB SLAM管道，专门用于从运动模糊的输入中恢复清晰的重建和精确的轨迹。它结合了帧到帧和帧到模型方法的优点，通过建模子帧相机轨迹来处理模糊，并利用在线闭环和全局捆绑调整实现密集的全局轨迹。该方法通过模拟模糊图像的物理形成过程，并结合单目深度估计和高斯在线变形来优化去模糊和映射。实验证明，Deblur-SLAM在合成和真实世界的模糊数据上，在清晰地图估计和子帧轨迹恢复方面均达到了最先进的水平。", "keywords": "运动模糊, SLAM, 高斯泼溅, 轨迹恢复, 图像去模糊", "comments": "该论文解决了SLAM领域中一个重要的实际挑战：运动模糊。其创新之处在于通过建模子帧轨迹并将物理模糊模型与高斯泼溅和深度估计相结合，为运动模糊常见的真实世界应用提供了鲁棒的解决方案。"}}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.", "AI": {"title_translation": "BalancedDPO：自适应多指标对齐", "tldr": "BalancedDPO通过同时对齐文本到图像扩散模型与人类偏好、CLIP分数和美学质量等多个指标，解决了现有方法过拟合和泛化能力有限的问题，并在多个数据集上取得了最先进的成果。", "motivation": "尽管文本到图像（T2I）扩散模型取得了显著进展，但使其与多样化偏好对齐仍然是一个持续的挑战。现有方法通常优化单一指标或依赖于狭窄策划的数据集，这导致过拟合以及在关键视觉质量指标上的泛化能力受限。", "method": "本文提出了BalancedDPO，这是直接偏好优化（DPO）的一个新颖扩展，通过同时将T2I扩散模型与人类偏好、CLIP分数和美学质量等多个指标对齐来解决上述限制。其主要新颖之处在于在偏好分布空间中聚合来自不同指标的共识标签，而非现有奖励混合方法，从而实现了鲁棒和可扩展的多指标对齐，同时保持了标准DPO管道的简单性。", "result": "在Pick-a-Pic、PartiPrompt和HPD数据集上的评估表明，BalancedDPO取得了最先进的成果，在所有主要指标上均优于现有方法。与DiffusionDPO相比，BalancedDPO在Pick-a-pic、PartiPrompt和HPD上的平均胜率分别提高了15%、7.1%和10.3%。", "conclusion": "BalancedDPO通过其新颖的多指标对齐方法，有效解决了文本到图像扩散模型与多样化偏好对齐的挑战，显著提升了模型的泛化能力和性能。", "translation": "文本到图像（T2I）扩散模型取得了显著进展，但使其与多样化偏好对齐仍然是一个持续的挑战。现有方法通常优化单一指标或依赖于狭窄策划的数据集，这导致过拟合以及在关键视觉质量指标上的泛化能力受限。我们提出了BalancedDPO，这是直接偏好优化（DPO）的一个新颖扩展，通过同时将T2I扩散模型与人类偏好、CLIP分数和美学质量等多个指标对齐来解决这些限制。我们的主要新颖之处在于在偏好分布空间中聚合来自不同指标的共识标签，而非现有奖励混合方法，从而实现了鲁棒和可扩展的多指标对齐，同时保持了标准DPO管道的简单性，我们将其称为BalancedDPO。我们在Pick-a-Pic、PartiPrompt和HPD数据集上的评估表明，BalancedDPO取得了最先进的成果，在所有主要指标上均优于现有方法。与DiffusionDPO相比，BalancedDPO在Pick-a-pic、PartiPrompt和HPD上的平均胜率分别提高了15%、7.1%和10.3%。", "summary": "BalancedDPO是一种新颖的DPO扩展，旨在解决文本到图像（T2I）扩散模型在与多样化偏好对齐时遇到的过拟合和泛化能力受限问题。它通过在偏好分布空间中聚合来自人类偏好、CLIP分数和美学质量等多个指标的共识标签，实现鲁棒且可扩展的多指标对齐，同时保持DPO的简单性。实验结果表明，BalancedDPO在多个数据集上取得了超越现有方法的领先性能，显著提高了平均胜率。", "keywords": "BalancedDPO, 文本到图像, 扩散模型, 多指标对齐, 直接偏好优化", "comments": "本文的创新点在于提出了一种在偏好分布空间中聚合多指标共识标签的方法，而非传统的奖励混合，这使得多指标对齐更加鲁棒和可扩展。其重要性在于有效解决了T2I模型在多指标对齐方面的泛化和过拟合问题，为生成高质量、多样化图像提供了新的途径。"}}
{"id": "2503.12588", "pdf": "https://arxiv.org/pdf/2503.12588", "abs": "https://arxiv.org/abs/2503.12588", "authors": ["Xiaoyu Han", "Shengping Zhang", "Qinglin Liu", "Zonglin Li", "Chenyang Wang"], "title": "Progressive Limb-Aware Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2022. The code is available at\n  https://github.com/xyhanHIT/PL-VTON", "summary": "Existing image-based virtual try-on methods directly transfer specific\nclothing to a human image without utilizing clothing attributes to refine the\ntransferred clothing geometry and textures, which causes incomplete and blurred\nclothing appearances. In addition, these methods usually mask the limb textures\nof the input for the clothing-agnostic person representation, which results in\ninaccurate predictions for human limb regions (i.e., the exposed arm skin),\nespecially when transforming between long-sleeved and short-sleeved garments.\nTo address these problems, we present a progressive virtual try-on framework,\nnamed PL-VTON, which performs pixel-level clothing warping based on multiple\nattributes of clothing and embeds explicit limb-aware features to generate\nphoto-realistic try-on results. Specifically, we design a Multi-attribute\nClothing Warping (MCW) module that adopts a two-stage alignment strategy based\non multiple attributes to progressively estimate pixel-level clothing\ndisplacements. A Human Parsing Estimator (HPE) is then introduced to\nsemantically divide the person into various regions, which provides structural\nconstraints on the human body and therefore alleviates texture bleeding between\nclothing and limb regions. Finally, we propose a Limb-aware Texture Fusion\n(LTF) module to estimate high-quality details in limb regions by fusing\ntextures of the clothing and the human body with the guidance of explicit\nlimb-aware features. Extensive experiments demonstrate that our proposed method\noutperforms the state-of-the-art virtual try-on methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON.", "AI": {"title_translation": "渐进式肢体感知虚拟试穿", "tldr": "PL-VTON是一个渐进式虚拟试穿框架，通过多属性服装形变和肢体感知特征融合，解决了现有方法中服装几何和纹理不完整以及肢体区域预测不准确的问题，生成了逼真的试穿结果。", "motivation": "现有图像虚拟试穿方法未能利用服装属性细化服装几何和纹理，导致服装外观不完整和模糊。此外，这些方法通常会掩盖输入图像的肢体纹理以生成与服装无关的人体表示，导致人体肢体区域（如暴露的手臂皮肤）预测不准确，尤其是在长袖和短袖服装之间转换时。", "method": "本文提出了一种渐进式虚拟试穿框架PL-VTON。该框架通过多属性服装形变进行像素级服装扭曲，并嵌入显式肢体感知特征来生成逼真的试穿结果。具体而言，PL-VTON设计了一个多属性服装扭曲（MCW）模块，采用基于多属性的两阶段对齐策略来逐步估计像素级服装位移。然后引入人体解析估计器（HPE）将人体语义地划分为不同区域，为人体提供结构约束，从而减轻服装和肢体区域之间的纹理渗色。最后，提出肢体感知纹理融合（LTF）模块，在显式肢体感知特征的指导下，融合服装和人体的纹理，以估计肢体区域的高质量细节。", "result": "大量的实验表明，所提出的方法在定性和定量上均优于现有的最先进的虚拟试穿方法。", "conclusion": "PL-VTON通过引入多属性服装形变和肢体感知纹理融合，有效地解决了现有虚拟试穿方法中服装外观不完整、模糊以及肢体区域预测不准确的问题，显著提升了虚拟试穿的真实感和准确性。", "translation": "现有基于图像的虚拟试穿方法直接将特定服装转移到人体图像上，而没有利用服装属性来细化转移服装的几何形状和纹理，这导致服装外观不完整和模糊。此外，这些方法通常会掩盖输入图像的肢体纹理以生成与服装无关的人体表示，从而导致人体肢体区域（即暴露的手臂皮肤）预测不准确，尤其是在长袖和短袖服装之间转换时。为了解决这些问题，我们提出了一个渐进式虚拟试穿框架，名为PL-VTON，它根据服装的多个属性执行像素级服装扭曲，并嵌入显式肢体感知特征以生成逼真的试穿结果。具体来说，我们设计了一个多属性服装扭曲（MCW）模块，该模块采用基于多属性的两阶段对齐策略，逐步估计像素级服装位移。然后引入人体解析估计器（HPE）将人体语义地划分为各种区域，这为人体提供了结构约束，从而减轻了服装和肢体区域之间的纹理渗色。最后，我们提出了一个肢体感知纹理融合（LTF）模块，通过在显式肢体感知特征的指导下融合服装和人体的纹理，以估计肢体区域的高质量细节。大量的实验表明，我们提出的方法在定性和定量上均优于最先进的虚拟试穿方法。代码可在https://github.com/xyhanHIT/PL-VTON获取。", "summary": "PL-VTON是一个创新的渐进式虚拟试穿框架，旨在解决现有方法中服装几何和纹理不完整以及肢体区域预测不准确的问题。它通过引入多属性服装扭曲（MCW）模块进行像素级对齐，结合人体解析估计器（HPE）提供结构约束，并利用肢体感知纹理融合（LTF）模块细化肢体细节，从而生成高质量、逼真的虚拟试穿效果，并在实验中表现出优于现有技术。", "keywords": "虚拟试穿, 服装形变, 肢体感知, 图像生成, 深度学习", "comments": "该论文通过引入多属性服装扭曲和显式肢体感知特征，有效地解决了虚拟试穿中长期存在的服装变形不准确和肢体细节丢失问题，尤其是在处理不同袖长的服装时。其模块化的设计（MCW, HPE, LTF）思路清晰，针对性强，显著提升了试穿结果的真实感和视觉质量，具有较高的实用价值和创新性。"}}
{"id": "2503.12590", "pdf": "https://arxiv.org/pdf/2503.12590", "abs": "https://arxiv.org/abs/2503.12590", "authors": ["Haoran Feng", "Zehuan Huang", "Lin Li", "Hairong Lv", "Lu Sheng"], "title": "Personalize Anything for Free with Diffusion Transformer", "categories": ["cs.CV"], "comment": "https://fenghora.github.io/Personalize-Anything-Page/", "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose \\textbf{Personalize\nAnything}, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.", "AI": {"title_translation": "使用扩散Transformer免费个性化任何事物", "tldr": "提出“个性化一切”，一个无需训练的DiT框架，通过替换去噪令牌实现个性化图像生成，具有最先进的身份保留和多功能性。", "motivation": "现有的无需训练的个性化图像生成方法在身份保留、适用性以及与扩散Transformer（DiT）的兼容性方面存在困难。", "method": "本文提出了“个性化一切”，一个无需训练的DiT框架，用于个性化图像生成。该方法通过发现简单地用参考主体的去噪令牌替换现有令牌即可实现零样本主体重建。该框架包含：1）时间步自适应令牌替换，通过早期注入强制主体一致性，并通过后期正则化增强灵活性；2）补丁扰动策略，以提高结构多样性。该方法无缝支持布局引导生成、多主体个性化和掩码控制编辑。", "result": "该方法在身份保留和多功能性方面表现出最先进的性能。它无缝支持布局引导生成、多主体个性化和掩码控制编辑等多种场景。", "conclusion": "这项工作为DiT提供了新的见解，并为高效个性化提供了一个实用的范例。", "translation": "个性化图像生成旨在产生用户指定概念的图像，同时实现灵活编辑。最近的无需训练方法虽然比基于训练的方法具有更高的计算效率，但在身份保留、适用性和与扩散Transformer（DiT）的兼容性方面存在困难。在本文中，我们揭示了DiT未被发掘的潜力，即简单地用参考主体的去噪令牌替换现有令牌即可实现零样本主体重建。这种简单而有效的特征注入技术解锁了从个性化到图像编辑的各种场景。基于这一观察，我们提出了“个性化一切”，一个无需训练的框架，通过以下方式在DiT中实现个性化图像生成：1）时间步自适应令牌替换，通过早期注入强制主体一致性，并通过后期正则化增强灵活性；2）补丁扰动策略，以提高结构多样性。我们的方法无缝支持布局引导生成、多主体个性化和掩码控制编辑。评估表明，在身份保留和多功能性方面达到了最先进的性能。我们的工作为DiT建立了新的见解，同时为高效个性化提供了一个实用的范例。", "summary": "“个性化一切”是一个新颖的无需训练框架，利用扩散Transformer（DiT）进行个性化图像生成。通过发现简单地替换去噪令牌即可实现零样本主体重建，该框架引入了时间步自适应令牌替换和补丁扰动策略。这种方法克服了现有无需训练方法的局限性，在身份保留、多功能性和与各种编辑任务的兼容性方面达到了最先进的性能，并为DiT提供了新的见解。", "keywords": "个性化图像生成, 扩散Transformer, 无需训练, 身份保留, 令牌替换", "comments": "这篇论文提出了一种创新性的无需训练的个性化图像生成方法，利用扩散Transformer，解决了现有方法在身份保留和DiT兼容性方面的关键局限。其“简单地替换去噪令牌”的简洁而有效的方法是一个重要的贡献，为高效和多功能的图像个性化及编辑开辟了新的可能性。"}}
{"id": "2503.12595", "pdf": "https://arxiv.org/pdf/2503.12595", "abs": "https://arxiv.org/abs/2503.12595", "authors": ["Dan Halperin", "Niklas Eisl"], "title": "Point Cloud Based Scene Segmentation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency.", "AI": {"title_translation": "基于点云的场景分割：一项综述", "tldr": "这篇综述回顾了自动驾驶领域点云语义分割的最新方法，将其分为基于投影、3D和混合方法，并讨论了数据集和结果。", "motivation": "自动驾驶是安全关键应用，需要精确的环境信息。3D目标检测提供的信息不足，而3D语义分割通过为每个点分配标签，提供更丰富、更密集的理解，这对自动驾驶任务至关重要。", "method": "本文是一篇综述，全面概述了自动驾驶领域点云语义分割的最新方法。作者将方法分为基于投影、3D和混合方法，讨论了最重要和常用的数据集，并强调了合成数据的重要性。此外，还展示了不同方法的结果，并比较了它们的分割精度和效率。", "result": "综述展示了不同方法的分割精度和效率结果，并进行了比较。", "conclusion": "本综述旨在为未来的研究提供启发。", "translation": "自动驾驶是一个安全关键型应用，因此，配套的辅助系统能够提供车辆周围环境的精确信息是重中之重。诸如3D目标检测之类的任务，由于仅预测前景对象的包围盒，因此对周围场景的理解不够详细。相比之下，3D语义分割通过为每个独立点分配标签，提供了关于环境更丰富和密集的信息，这对于自动驾驶任务，如导航或车道变换，至关重要。为了启发未来的研究，在本综述论文中，我们对自动驾驶领域点云语义分割的最新方法进行了全面概述。我们将这些方法分为基于投影、3D和混合方法。此外，我们讨论了这项任务最重要和常用的数据集，并强调了在真实世界数据有限时合成数据对支持研究的重要性。我们还进一步展示了不同方法的结果，并比较了它们的分割精度和效率。", "summary": "本综述论文旨在全面概述自动驾驶领域中基于点云的场景语义分割的最新方法。文章指出，与3D目标检测相比，3D语义分割能提供更详细和密集的车辆周围环境信息，这对于自动驾驶至关重要。论文将现有方法分为基于投影、3D和混合三类，讨论了常用的数据集（包括合成数据的重要性），并比较了不同方法的分割精度和效率，以期启发未来的研究。", "keywords": "点云语义分割, 自动驾驶, 场景分割, 综述, 3D视觉", "comments": "作为一篇综述论文，本文的创新性在于系统地整理和分类了点云语义分割在自动驾驶领域的现有研究，为研究人员提供了全面的概览。其重要性在于明确了该领域的研究现状、挑战和未来方向，特别是强调了3D语义分割对自动驾驶的关键作用以及合成数据的重要性，有助于推动该领域的发展。"}}
{"id": "2503.12605", "pdf": "https://arxiv.org/pdf/2503.12605", "abs": "https://arxiv.org/abs/2503.12605", "authors": ["Yaoting Wang", "Shengqiong Wu", "Yuecheng Zhang", "William Wang", "Ziwei Liu", "Jiebo Luo", "Hao Fei"], "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "survey resource at\n  https://github.com/yaotingwangofficial/Awesome-MCoT; 12 figures, 4 tables, 44\n  pages", "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.", "AI": {"title_translation": "多模态思维链推理：一项综合调查", "tldr": "本文首次对多模态思维链推理（MCoT）进行了系统性综述，涵盖其概念、方法、应用、挑战和未来方向，旨在促进多模态AGI的发展。", "motivation": "多模态思维链（MCoT）推理领域缺乏最新的系统性综述，需要填补这一空白，以促进该领域的持续发展并迈向多模态通用人工智能（AGI）。", "method": "该论文通过提供MCoT推理的首次系统性综述，阐明了相关的基础概念和定义，提出了全面的分类法，深入分析了各种应用场景下的当前方法，并探讨了现有挑战和未来研究方向。", "result": "本文提供了MCoT推理的系统性综述、全面的分类法、对当前方法的深入分析、以及对现有挑战和未来研究方向的见解。", "conclusion": "该综述旨在促进多模态通用人工智能（AGI）领域的创新。", "translation": "多模态思维链（MCoT）推理通过将人类般的逐步推理过程中的思维链（CoT）优势扩展到多模态语境，最近获得了显著的研究关注，尤其是在与多模态大型语言模型（MLLMs）的结合方面。现有的MCoT研究设计了各种方法和创新的推理范式，以解决图像、视频、语音、音频、3D和结构化数据等不同模态的独特挑战，并在机器人、医疗保健、自动驾驶和多模态生成等应用中取得了广泛成功。然而，MCoT仍然面临独特的挑战和机遇，需要进一步关注以确保该领域的持续繁荣，不幸的是，目前缺乏对该领域的最新综述。为了弥补这一空白，我们首次对MCoT推理进行了系统性调查，阐明了相关的基本概念和定义。我们提供了一个全面的分类法，并从不同视角对各种应用场景下的当前方法进行了深入分析。此外，我们对现有挑战和未来研究方向提供了见解，旨在促进多模态AGI的创新。", "summary": "本文首次对多模态思维链（MCoT）推理进行了系统性综述，旨在弥补该领域缺乏最新回顾的空白。文章详细阐述了MCoT的基本概念、定义、现有方法、在图像、视频、语音等多种模态应用中的成功案例，并对当前挑战及未来研究方向进行了深入分析，以期推动多模态通用人工智能的发展。", "keywords": "多模态思维链推理, MCoT, 综述, 多模态大型语言模型, 多模态AGI", "comments": "这篇综述论文的重要性在于它填补了多模态思维链推理领域缺乏系统性总结的空白，为研究人员提供了全面的概览、分类法和未来方向指导，对于推动多模态AI，特别是多模态AGI的发展具有重要意义。"}}
{"id": "2503.12615", "pdf": "https://arxiv.org/pdf/2503.12615", "abs": "https://arxiv.org/abs/2503.12615", "authors": ["Alessio Spagnoletti", "Jean Prost", "Andrés Almansa", "Nicolas Papadakis", "Marcelo Pereyra"], "title": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization", "categories": ["cs.CV", "cs.LG"], "comment": "27 pages, 20 figures", "summary": "Text-to-image latent diffusion models (LDMs) have recently emerged as\npowerful generative models with great potential for solving inverse problems in\nimaging. However, leveraging such models in a Plug & Play (PnP), zero-shot\nmanner remains challenging because it requires identifying a suitable text\nprompt for the unknown image of interest. Also, existing text-to-image PnP\napproaches are highly computationally expensive. We herein address these\nchallenges by proposing a novel PnP inference paradigm specifically designed\nfor embedding generative models within stochastic inverse solvers, with special\nattention to Latent Consistency Models (LCMs), which distill LDMs into fast\ngenerators. We leverage our framework to propose LAtent consisTency INverse\nsOlver (LATINO), the first zero-shot PnP framework to solve inverse problems\nwith priors encoded by LCMs. Our conditioning mechanism avoids automatic\ndifferentiation and reaches SOTA quality in as little as 8 neural function\nevaluations. As a result, LATINO delivers remarkably accurate solutions and is\nsignificantly more memory and computationally efficient than previous\napproaches. We then embed LATINO within an empirical Bayesian framework that\nautomatically calibrates the text prompt from the observed measurements by\nmarginal maximum likelihood estimation. Extensive experiments show that prompt\nself-calibration greatly improves estimation, allowing LATINO with PRompt\nOptimization to define new SOTAs in image reconstruction quality and\ncomputational efficiency.", "AI": {"title_translation": "LATINO-PRO: 潜在一致性逆向求解器与提示优化", "tldr": "LATINO-PRO是一种新的即插即用框架，利用潜在一致性模型（LCMs）高效解决图像逆问题，并通过提示优化进一步提升性能。", "motivation": "现有文本到图像潜在扩散模型（LDMs）在即插即用（PnP）和零样本（zero-shot）方式解决图像逆问题时面临挑战：需要识别合适的文本提示，且计算成本高昂。", "method": "本文提出了一种新颖的PnP推理范式，专门设计用于将生成模型嵌入到随机逆向求解器中，并特别关注将LDMs提炼成快速生成器的潜在一致性模型（LCMs）。在此框架下，提出了LAtent consisTency INverse sOlver (LATINO)，这是第一个使用LCMs编码先验来解决逆问题的零样本PnP框架，其条件机制避免自动微分，并在少至8次神经函数评估中达到SOTA质量。随后，将LATINO嵌入到一个经验贝叶斯框架中，通过边际最大似然估计从观测测量中自动校准文本提示，从而形成LATINO-PRO。", "result": "LATINO在图像重建中提供了非常准确的解决方案，并且比以前的方法在内存和计算上显著更高效。提示自校准极大地改善了估计。LATINO-PRO在图像重建质量和计算效率方面定义了新的SOTA。", "conclusion": "LATINO-PRO通过结合潜在一致性模型和提示优化，克服了传统文本到图像LDMs在解决逆问题时的挑战，实现了卓越的性能和效率，为图像重建领域带来了新的突破。", "translation": "文本到图像的潜在扩散模型（LDMs）最近已成为强大的生成模型，在解决图像逆问题方面具有巨大潜力。然而，以即插即用（PnP）、零样本的方式利用此类模型仍然具有挑战性，因为它需要为未知的感兴趣图像识别一个合适的文本提示。此外，现有的文本到图像PnP方法计算成本很高。我们在此通过提出一种新颖的PnP推理范式来解决这些挑战，该范式专门设计用于将生成模型嵌入到随机逆向求解器中，并特别关注将LDMs提炼成快速生成器的潜在一致性模型（LCMs）。我们利用我们的框架提出了潜在一致性逆向求解器（LATINO），这是第一个使用LCMs编码先验来解决逆问题的零样本PnP框架。我们的条件机制避免了自动微分，并在少至8次神经函数评估中达到了SOTA质量。因此，LATINO提供了非常准确的解决方案，并且比以前的方法在内存和计算上显著更高效。然后，我们将LATINO嵌入到一个经验贝叶斯框架中，该框架通过边际最大似然估计从观测测量中自动校准文本提示。广泛的实验表明，提示自校准极大地改善了估计，使得带有提示优化的LATINO（LATINO-PRO）在图像重建质量和计算效率方面定义了新的SOTA。", "summary": "本文提出了LATINO-PRO，一个基于潜在一致性模型（LCMs）的即插即用（PnP）零样本框架，用于高效解决图像逆问题。针对现有方法计算成本高和提示识别困难的挑战，LATINO首先利用LCMs作为先验，实现了卓越的重建质量和计算效率。在此基础上，通过引入一个经验贝叶斯框架进行自动提示校准，LATINO-PRO进一步提升了性能，并在图像重建质量和效率上达到了新的最先进水平。", "keywords": "图像逆问题, 潜在一致性模型, 即插即用, 提示优化, 零样本", "comments": "这篇论文的创新点在于首次将潜在一致性模型（LCMs）引入到零样本即插即用（PnP）框架中解决图像逆问题，显著提升了计算效率和重建速度。此外，引入经验贝叶斯框架进行文本提示的自动校准，有效解决了手动提示选择的难题，并进一步优化了重建质量。这对于实际应用中图像逆问题的解决具有重要意义，尤其是在计算资源受限的场景下。"}}
{"id": "2503.12617", "pdf": "https://arxiv.org/pdf/2503.12617", "abs": "https://arxiv.org/abs/2503.12617", "authors": ["Anthony Lamelas", "Harrison Muchnic"], "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)", "summary": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.", "AI": {"title_translation": "语义类别扩展：探究对视觉Transformer标注性能的影响", "tldr": "研究发现，增加语义类别数量可以提升视觉Transformer的图像分类性能，但超过某个阈值后效果会减弱甚至反转。", "motivation": "本研究旨在探究语义类别扩展对视觉Transformer（ViTs）图像分类性能的影响，并假设随着语义等效类别数量的增加，ViTs的标注准确性会提高直到达到理论上限。", "method": "研究使用Jina AI提供的CLIP服务器进行实验，通过自定义Python函数处理多种图像数据集，评估模型准确性。实验通过指数级引入新的冗余类别，观察准确性趋势直至其稳定、下降或波动。", "result": "研究发现，语义扩展初期能提高模型性能，但超过关键阈值后，其益处会减弱或反转。", "conclusion": "研究结果揭示了视觉Transformer类别标注策略的局限性和可能的优化方向。", "translation": "本研究探讨了扩展语义类别对视觉Transformer (ViT) 图像分类性能的影响。在本特定案例中，使用了Jina AI提供的CLIP服务器进行实验。研究假设，随着地面实况和人工引入的语义等效类别数量的增加，ViT的标注准确性会提高，直到达到理论最大值或极限。选择了多种图像数据集来验证这一假设。这些数据集通过一个定制的Python函数进行处理，该函数旨在评估模型的准确性，并对数据集之间的格式差异进行了调整。通过指数级引入新的冗余类别，实验评估了准确性趋势，直到它们趋于平稳、下降或出现不一致的波动。研究结果表明，虽然语义扩展最初会提高模型性能，但超过关键阈值后，其益处会减弱或反转，这为ViT类别标注策略的局限性和可能的优化提供了见解。", "summary": "本研究探讨了语义类别扩展对视觉Transformer（ViT）图像分类性能的影响。实验利用Jina AI的CLIP服务器和多种图像数据集，通过指数级引入冗余类别来评估ViT的标注准确性。结果显示，语义扩展初期能提升模型性能，但超过特定阈值后效果会减弱甚至反转，这为ViT的类别标注策略优化提供了重要见解。", "keywords": "视觉Transformer, 语义类别, 图像分类, 标注性能, CLIP服务器", "comments": "这项研究创新性地探讨了语义类别扩展对视觉Transformer性能的影响，揭示了性能提升的临界点。其发现对于优化基于ViT的图像分类系统的类别标注策略具有重要指导意义，尤其是在实际应用中如何平衡类别数量与模型表现。"}}
{"id": "2503.12627", "pdf": "https://arxiv.org/pdf/2503.12627", "abs": "https://arxiv.org/abs/2503.12627", "authors": ["Rui Cao"], "title": "Online Misinformation Detection in Live Streaming Videos", "categories": ["cs.CV", "cs.CL"], "comment": "First prize winner in the Smart City Challenge in the 16th ACM\n  international WSDM conference(WSDM), 2023", "summary": "Online misinformation detection is an important issue and methods are\nproposed to detect and curb misinformation in various forms. However, previous\nstudies are conducted in an offline manner. We claim a realistic misinformation\ndetection setting that has not been studied yet is online misinformation\ndetection in live streaming videos (MDLS). In the proposal, we formulate the\nproblem of MDLS and illustrate the importance and the challenge of the task.\nBesides, we propose feasible ways of developing the problem into AI challenges\nas well as potential solutions to the problem.", "AI": {"title_translation": "实时流媒体视频中的在线虚假信息检测", "tldr": "本文提出了实时流媒体视频中的在线虚假信息检测（MDLS）问题，指出其重要性和挑战，并探讨了将其发展为AI挑战和潜在解决方案的途径。", "motivation": "现有的虚假信息检测研究主要集中在离线场景，而实时流媒体视频中的在线虚假信息检测（MDLS）这一现实且尚未被充分研究的设置，是当前研究的空白。", "method": "本文提出了MDLS问题，阐述了其重要性和挑战，并提出了将该问题发展为AI挑战的可行方法以及潜在的解决方案。", "result": "Not mentioned in abstract", "conclusion": "本文识别并提出了一个新颖且重要的研究方向——实时流媒体视频中的在线虚假信息检测，并为未来的研究奠定了基础，包括问题定义和潜在的解决思路。", "translation": "在线虚假信息检测是一个重要问题，已有多种方法被提出用于检测和遏制各种形式的虚假信息。然而，以往的研究都是以离线方式进行的。我们提出一个尚未被研究的现实虚假信息检测设置，即实时流媒体视频中的在线虚假信息检测（MDLS）。在本文中，我们阐述了MDLS问题，并说明了这项任务的重要性和挑战。此外，我们还提出了将该问题发展为AI挑战的可行方法以及潜在的解决方案。", "summary": "本文指出现有虚假信息检测研究主要为离线方式，而忽视了实时流媒体视频中的在线检测需求。作者提出了“实时流媒体视频中的在线虚假信息检测（MDLS）”这一新问题，并详细阐述了其重要性与面临的挑战。文章还探讨了如何将MDLS问题转化为人工智能挑战，并提出了潜在的解决方案，旨在为未来在该领域的研究铺平道路。", "keywords": "在线虚假信息检测, 实时流媒体, 虚假信息, AI挑战, 问题定义", "comments": "这篇论文的创新点在于识别并提出了一个未被充分研究但具有现实意义的问题——实时流媒体视频中的在线虚假信息检测。它为该领域的研究开辟了新的方向，并鼓励AI社区关注这一挑战。其局限性在于，它主要停留在问题提出和方法探讨阶段，并未提供具体的实现方案或实验结果。"}}
{"id": "2503.12652", "pdf": "https://arxiv.org/pdf/2503.12652", "abs": "https://arxiv.org/abs/2503.12652", "authors": ["Tsu-Jui Fu", "Yusu Qian", "Chen Chen", "Wenze Hu", "Zhe Gan", "Yinfei Yang"], "title": "UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models have shown impressive results in\ngenerating visually compelling images following user prompts. Building on this,\nvarious methods further fine-tune the pre-trained T2I model for specific tasks.\nHowever, this requires separate model architectures, training designs, and\nmultiple parameter sets to handle different tasks. In this paper, we introduce\nUniVG, a generalist diffusion model capable of supporting a diverse range of\nimage generation tasks with a single set of weights. UniVG treats multi-modal\ninputs as unified conditions to enable various downstream applications, ranging\nfrom T2I generation, inpainting, instruction-based editing, identity-preserving\ngeneration, and layout-guided generation, to depth estimation and referring\nsegmentation. Through comprehensive empirical studies on data mixing and\nmulti-task training, we provide detailed insights into the training processes\nand decisions that inform our final designs. For example, we show that T2I\ngeneration and other tasks, such as instruction-based editing, can coexist\nwithout performance trade-offs, while auxiliary tasks like depth estimation and\nreferring segmentation enhance image editing. Notably, our model can even\noutperform some task-specific models on their respective benchmarks, marking a\nsignificant step towards a unified image generation model.", "AI": {"title_translation": "UniVG：统一图像生成与编辑的通用扩散模型", "tldr": "UniVG是一个通用扩散模型，仅用一套权重即可处理多种图像生成和编辑任务，甚至在某些特定任务上超越专用模型。", "motivation": "现有文本到图像（T2I）扩散模型在生成图像方面表现出色，但处理不同任务（如生成、编辑）需要独立的模型架构、训练设计和多套参数，导致效率低下且复杂。", "method": "本文引入了UniVG，一个通用扩散模型，通过将多模态输入视为统一条件来支持多种图像生成和编辑任务。通过对数据混合和多任务训练进行全面的实证研究，提供了训练过程和设计决策的详细见解。", "result": "UniVG能够支持T2I生成、图像修复、基于指令的编辑、身份保留生成、布局引导生成，以及深度估计和指代分割等多种任务。研究表明，T2I生成与其他任务（如基于指令的编辑）可以共存而没有性能权衡，辅助任务（如深度估计和指代分割）能增强图像编辑。值得注意的是，UniVG在某些特定任务的基准测试中甚至优于专用模型。", "conclusion": "UniVG代表着朝着统一图像生成模型迈出的重要一步，它能够用一套权重处理多种图像生成和编辑任务，并展现出优异的性能。", "translation": "文本到图像（T2I）扩散模型在生成符合用户提示的视觉引人注目的图像方面显示出令人印象深刻的结果。在此基础上，各种方法进一步微调预训练的T2I模型以执行特定任务。然而，这需要独立的模型架构、训练设计和多套参数来处理不同的任务。在本文中，我们引入了UniVG，一个通用扩散模型，能够用一套权重支持各种图像生成任务。UniVG将多模态输入视为统一条件，以实现各种下游应用，包括T2I生成、图像修复、基于指令的编辑、身份保留生成和布局引导生成，以及深度估计和指代分割。通过对数据混合和多任务训练进行全面的实证研究，我们提供了对训练过程和指导我们最终设计的决策的详细见解。例如，我们展示了T2I生成和其他任务（如基于指令的编辑）可以共存而没有性能权衡，而辅助任务（如深度估计和指代分割）则增强了图像编辑。值得注意的是，我们的模型甚至可以在各自的基准测试中超越一些任务专用模型，标志着朝着统一图像生成模型迈出了重要一步。", "summary": "UniVG是一个创新的通用扩散模型，旨在解决当前图像生成和编辑领域中任务专用模型分散的问题。它通过将多模态输入统一处理，实现了一套权重支持文本到图像生成、图像修复、指令编辑、身份保留生成、布局引导生成、深度估计和指代分割等多种任务。研究表明，UniVG在多任务共存时性能无损，辅助任务还能提升编辑效果，并在部分任务上超越了专用模型，这标志着向统一图像生成范式迈出了重要一步。", "keywords": "扩散模型, 统一图像生成, 图像编辑, 通用模型, 多任务学习", "comments": "UniVG的创新之处在于其“通用性”，通过一套模型权重实现了多种图像生成和编辑任务的统一处理，极大地简化了模型管理和部署。其在多任务训练中展现出的性能无损甚至超越专用模型的能力，预示了未来AI模型发展的一个重要方向：从专才到通才，这对于降低AI应用门槛和提高效率具有重要意义。"}}
{"id": "2503.12663", "pdf": "https://arxiv.org/pdf/2503.12663", "abs": "https://arxiv.org/abs/2503.12663", "authors": ["Imran Kabir", "Md Alimoor Reza", "Syed Billah"], "title": "Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "Large multimodal models (LMMs) are increasingly integrated into autonomous\ndriving systems for user interaction. However, their limitations in\nfine-grained spatial reasoning pose challenges for system interpretability and\nuser trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation\n(RAG) framework that improves LMMs' spatial understanding in driving scenarios.\nLogic-RAG constructs a dynamic knowledge base (KB) about object-object\nrelationships in first-order logic (FOL) using a perception module, a\nquery-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG\non visual-spatial queries using both synthetic and real-world driving videos.\nWhen using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous\ndriving system, these models achieved only 55% accuracy on synthetic driving\nscenes and under 75% on real-world driving scenes. Augmenting them with\nLogic-RAG increased their accuracies to over 80% and 90%, respectively. An\nablation study showed that even without logical inference, the fact-based\ncontext constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is\nextensible: it allows seamless replacement of individual components with\nimproved versions and enables domain experts to compose new knowledge in both\nFOL and natural language. In sum, Logic-RAG addresses critical spatial\nreasoning deficiencies in LMMs for autonomous driving applications. Code and\ndata are available at https://github.com/Imran2205/LogicRAG.", "AI": {"title_translation": "Logic-RAG：利用视觉空间知识增强大型多模态模型以理解道路场景", "tldr": "Logic-RAG是一个检索增强生成框架，通过构建基于一阶逻辑的动态知识库，显著提升了大型多模态模型在自动驾驶场景中的视觉空间理解和推理能力。", "motivation": "大型多模态模型（LMMs）在自动驾驶系统中日益普及，但它们在细粒度空间推理方面的局限性给系统可解释性和用户信任带来了挑战。", "method": "本文提出了Logic-RAG，一个新颖的检索增强生成（RAG）框架，用于改善LMMs在驾驶场景中的空间理解。Logic-RAG利用感知模块、查询到逻辑嵌入器和逻辑推理引擎，构建了一个关于物体之间关系的一阶逻辑（FOL）动态知识库。", "result": "在合成驾驶场景中，流行LMMs的准确率仅为55%，但在Logic-RAG增强后提高到80%以上。在真实世界驾驶场景中，流行LMMs的准确率低于75%，Logic-RAG将其提高到90%以上。消融研究表明，即使没有逻辑推理，Logic-RAG构建的基于事实的上下文也能将准确率提高15%。", "conclusion": "Logic-RAG解决了大型多模态模型在自动驾驶应用中关键的空间推理缺陷。", "translation": "大型多模态模型（LMMs）正越来越多地被整合到自动驾驶系统中以实现用户交互。然而，它们在细粒度空间推理方面的局限性给系统可解释性和用户信任带来了挑战。我们引入了Logic-RAG，一个新颖的检索增强生成（RAG）框架，旨在改善LMMs在驾驶场景中的空间理解。Logic-RAG利用感知模块、查询到逻辑嵌入器和逻辑推理引擎，构建了一个关于物体之间关系的一阶逻辑（FOL）动态知识库。我们使用合成和真实世界的驾驶视频对Logic-RAG进行了视觉空间查询评估。当使用流行的LMMs（GPT-4V、Claude 3.5）作为自动驾驶系统的代理时，这些模型在合成驾驶场景中仅达到55%的准确率，在真实世界驾驶场景中低于75%。通过Logic-RAG增强后，它们的准确率分别提高到80%以上和90%以上。一项消融研究表明，即使没有逻辑推理，仅由Logic-RAG构建的基于事实的上下文也能将准确率提高15%。Logic-RAG具有可扩展性：它允许无缝替换单个组件为改进版本，并使领域专家能够以FOL和自然语言组合新知识。总之，Logic-RAG解决了LMMs在自动驾驶应用中关键的空间推理缺陷。代码和数据可在https://github.com/Imran2205/LogicRAG获取。", "summary": "Logic-RAG是一个为大型多模态模型（LMMs）设计的检索增强生成（RAG）框架，旨在提升其在自动驾驶场景中的视觉空间理解能力。该框架通过一个感知模块、查询到逻辑嵌入器和逻辑推理引擎，构建一个基于一阶逻辑的动态知识库，以捕捉物体间的空间关系。实验结果表明，Logic-RAG显著提高了LMMs在合成和真实驾驶视频上的空间查询准确率，分别从55%和低于75%提升至80%以上和90%以上。研究还发现，即使没有逻辑推理，仅基于事实的上下文也能带来显著的准确率提升。Logic-RAG具有良好的可扩展性，能够有效解决LMMs在自动驾驶应用中空间推理的不足。", "keywords": "Logic-RAG, 大型多模态模型, 空间推理, 自动驾驶, 检索增强生成", "comments": "Logic-RAG的创新点在于将一阶逻辑和动态知识库引入检索增强生成框架，以解决大型多模态模型在自动驾驶场景中细粒度空间推理的局限性。其通过构建结构化的视觉空间知识，显著提升了模型的准确率和可解释性。该框架的可扩展性也为其未来的发展和应用提供了灵活性，允许集成更先进的组件和专家知识。这项工作对提升自动驾驶系统的安全性和可靠性具有重要意义。"}}
{"id": "2503.12678", "pdf": "https://arxiv.org/pdf/2503.12678", "abs": "https://arxiv.org/abs/2503.12678", "authors": ["Partho Ghosh", "Raisa Bentay Hossain", "Mohammad Zunaed", "Taufiq Hasan"], "title": "Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing", "categories": ["cs.CV"], "comment": null, "summary": "Automatic video activity recognition is crucial across numerous domains like\nsurveillance, healthcare, and robotics. However, recognizing human activities\nfrom video data becomes challenging when training and test data stem from\ndiverse domains. Domain generalization, adapting to unforeseen domains, is thus\nessential. This paper focuses on office activity recognition amidst\nenvironmental variability. We propose three pre-processing techniques\napplicable to any video encoder, enhancing robustness against environmental\nvariations. Our study showcases the efficacy of MViT, a leading\nstate-of-the-art video classification model, and other video encoders combined\nwith our techniques, outperforming state-of-the-art domain adaptation methods.\nOur approach significantly boosts accuracy, precision, recall and F1 score on\nunseen domains, emphasizing its adaptability in real-world scenarios with\ndiverse video data sources. This method lays a foundation for more reliable\nvideo activity recognition systems across heterogeneous data domains.", "AI": {"title_translation": "用于办公空间视频中人体活动识别的域泛化与自适应预处理", "tldr": "本文提出了一种结合自适应预处理技术的方法，以提高在不同域视频数据下人体活动识别的鲁棒性和准确性，优于现有最先进方法。", "motivation": "自动视频活动识别在监控、医疗和机器人等领域至关重要，但当训练和测试数据来自不同域时，识别人体活动变得具有挑战性，因此需要域泛化能力。", "method": "本文提出了三种可应用于任何视频编码器的预处理技术，以增强对环境变化的鲁棒性。研究结合了MViT等领先的视频分类模型和所提出的预处理技术。", "result": "所提出的方法在未见过的域上显著提高了准确率、精确率、召回率和F1分数，并且优于最先进的域适应方法。", "conclusion": "该方法为在异构数据域中建立更可靠的视频活动识别系统奠定了基础。", "translation": "自动视频活动识别在监控、医疗保健和机器人等众多领域至关重要。然而，当训练和测试数据源于不同领域时，从视频数据中识别人体活动变得具有挑战性。因此，域泛化，即适应不可预见的领域，至关重要。本文侧重于环境多变下的办公室活动识别。我们提出了三种可应用于任何视频编码器的预处理技术，以增强对环境变化的鲁棒性。我们的研究展示了MViT（一种领先的最先进视频分类模型）以及结合我们技术的其他视频编码器的功效，它们优于最先进的域适应方法。我们的方法显著提高了在未见过的域上的准确率、精确率、召回率和F1分数，强调了其在具有多样化视频数据源的真实场景中的适应性。该方法为在异构数据域中建立更可靠的视频活动识别系统奠定了基础。", "summary": "本文针对办公空间视频中人体活动识别面临的跨域挑战，提出了一种基于自适应预处理的域泛化方法。通过引入三种可与现有视频编码器（如MViT）结合使用的预处理技术，显著提升了模型在未见过域上的性能，各项指标（准确率、精确率、召回率、F1分数）均超越了现有最先进的域适应方法。该研究为构建在多样化视频数据源下更可靠的活动识别系统奠定了基础。", "keywords": "域泛化, 人体活动识别, 视频分析, 预处理, 办公室视频", "comments": "本文的创新点在于提出了通用的自适应预处理技术，而非依赖复杂的模型架构或域适应算法，这使得其方法具有更强的普适性和实用性。其在真实世界场景中的适应性以及对各项性能指标的显著提升，突显了其重要性。该方法为解决视频活动识别中的域泛化问题提供了一个有效且简洁的途径。"}}
{"id": "2503.12688", "pdf": "https://arxiv.org/pdf/2503.12688", "abs": "https://arxiv.org/abs/2503.12688", "authors": ["Tianyuan Wang"], "title": "Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In industrial X-ray Computed Tomography (CT), the need for rapid in-line\ninspection is critical. Sparse-angle tomography plays a significant role in\nthis by reducing the required number of projections, thereby accelerating\nprocessing and conserving resources. Most existing methods aim to balance\nreconstruction quality and scanning time, typically relying on fixed scan\ndurations. Adaptive adjustment of the number of angles is essential; for\ninstance, more angles may be required for objects with complex geometries or\nnoisier projections. The concept of optimal stopping, which dynamically adjusts\nthis balance according to varying industrial needs, remains underutilized.\nBuilding on our previous work, we integrate optimal stopping into sequential\nOptimal Experimental Design (OED). We propose a novel method for computing the\npolicy gradient within the Actor-Critic framework, enabling the development of\nadaptive policies for informative angle selection and scan termination.\nAdditionally, we investigated the gap between simulation and real-world\napplications in the context of the developed learning-based method. Our trained\nmodel, developed using synthetic data, demonstrates reliable performance when\napplied to real-world data. This approach enhances the flexibility of CT\noperations and expands the applicability of sparse-angle tomography in\nindustrial settings.", "AI": {"title_translation": "X射线CT中的动态角度选择：一种基于强化学习的最优停止方法", "tldr": "该研究提出了一种基于强化学习（Actor-Critic框架）的动态角度选择方法，用于工业X射线CT中的稀疏角度层析成像，以实现扫描的自适应终止和优化，并在真实数据上表现良好。", "motivation": "在工业X射线CT中，快速在线检测至关重要，稀疏角度层析成像有助于此。然而，现有方法通常依赖固定的扫描时间，未能充分利用最优停止概念来动态调整角度数量，以适应复杂几何或噪声投影等不同需求。", "method": "作者将最优停止概念整合到顺序最优实验设计（OED）中。他们提出了一种在Actor-Critic框架内计算策略梯度的新方法，从而能够开发用于信息角度选择和扫描终止的自适应策略。此外，他们还研究了所开发的基于学习的方法在模拟与实际应用之间的差距。", "result": "使用合成数据训练的模型在应用于真实世界数据时表现出可靠的性能。", "conclusion": "该方法增强了CT操作的灵活性，并扩展了稀疏角度层析成像在工业环境中的适用性。", "translation": "在工业X射线计算机断层扫描（CT）中，快速在线检测的需求至关重要。稀疏角度层析成像通过减少所需的投影数量，从而加速处理并节省资源，在此方面发挥着重要作用。大多数现有方法旨在平衡重建质量和扫描时间，通常依赖于固定的扫描持续时间。角度数量的自适应调整至关重要；例如，对于具有复杂几何形状或噪声投影的对象，可能需要更多的角度。最优停止的概念，即根据不同的工业需求动态调整这种平衡，仍未得到充分利用。基于我们之前的工作，我们将最优停止集成到顺序最优实验设计（OED）中。我们提出了一种在Actor-Critic框架内计算策略梯度的新方法，从而能够开发用于信息角度选择和扫描终止的自适应策略。此外，我们还研究了所开发的基于学习的方法在模拟与实际应用之间的差距。我们使用合成数据训练的模型在应用于真实世界数据时表现出可靠的性能。这种方法增强了CT操作的灵活性，并扩展了稀疏角度层析成像在工业环境中的适用性。", "summary": "本文针对工业X射线CT中稀疏角度层析成像的效率问题，提出了一种基于强化学习（Actor-Critic框架）的动态角度选择和扫描终止方法。该方法将最优停止概念融入顺序最优实验设计中，通过计算策略梯度实现自适应策略。研究表明，该模型在合成数据上训练后，在真实数据上仍能展现可靠性能，从而提升了CT操作的灵活性和稀疏角度层析成像在工业应用中的普适性。", "keywords": "稀疏角度CT, 强化学习, 最优停止, 动态角度选择, Actor-Critic", "comments": "该论文的创新点在于将强化学习中的最优停止概念应用于X射线CT的动态角度选择，解决了传统方法固定扫描时间的局限性。通过Actor-Critic框架实现自适应策略，提高了工业CT检测的效率和灵活性。其在合成数据训练后能应用于真实数据的表现，证明了方法的实用性。"}}
{"id": "2503.12689", "pdf": "https://arxiv.org/pdf/2503.12689", "abs": "https://arxiv.org/abs/2503.12689", "authors": ["Hengjia Li", "Lifan Jiang", "Xi Xiao", "Tianyang Wang", "Hongwei Yi", "Boxi Wu", "Deng Cai"], "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization", "categories": ["cs.CV"], "comment": null, "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\n$\\textbf{MagicID}$, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.", "AI": {"title_translation": "MagicID：用于ID一致性和动态保留视频定制的混合偏好优化", "tldr": "MagicID引入了一种新颖的混合偏好优化框架，通过解决身份退化和动态性降低的问题，生成身份一致且动态丰富的定制视频，超越了现有方法。", "motivation": "现有视频身份定制方法面临两大挑战：视频长度延长时身份退化，以及训练期间动态性降低，这主要是由于它们依赖于传统的静态图像自重建训练。", "method": "MagicID框架通过构建具有明确身份和动态奖励的成对偏好视频数据进行偏好学习，取代了传统自重建方法。它引入了一种混合采样策略，首先利用参考图像的静态视频优先保留身份，然后使用基于Frontier的采样方法增强生成视频的动态运动质量。模型通过这些混合偏好对进行优化，以与定制偏好对之间的奖励差异对齐。", "result": "大量实验表明，MagicID成功实现了身份一致性和自然动态，在各种指标上超越了现有方法。", "conclusion": "MagicID有效解决了视频定制中身份退化和动态性降低的挑战，从而在生成高保真、身份一致且动态丰富的视频方面表现出卓越的性能。", "translation": "视频身份定制旨在根据用户的参考图像生成保持一致身份并展现显著动态的高保真视频。然而，现有方法面临两个关键挑战：视频长度延长时身份退化，以及训练期间动态性降低，这主要是由于它们依赖于传统的静态图像自重建训练。为了解决这些问题，我们引入了 $\\textbf{MagicID}$，一个新颖的框架，旨在直接促进生成符合用户偏好的身份一致且动态丰富的视频。具体来说，我们建议构建具有明确身份和动态奖励的成对偏好视频数据进行偏好学习，而不是坚持传统的自重建。为了解决定制偏好数据的限制，我们引入了一种混合采样策略。该方法首先通过利用源自参考图像的静态视频来优先保留身份，然后使用基于Frontier的采样方法增强生成视频中的动态运动质量。通过利用这些混合偏好对，我们优化模型以与定制偏好对之间的奖励差异对齐。大量实验表明，MagicID成功实现了身份一致性和自然动态，在各种指标上超越了现有方法。", "summary": "MagicID是一个新颖的视频身份定制框架，旨在解决现有方法中身份退化和动态性降低的问题。它通过引入混合偏好优化，利用带有明确身份和动态奖励的成对偏好视频数据进行学习，并采用混合采样策略来优先保留身份和增强动态性。实验证明MagicID在生成身份一致且动态自然的视频方面优于现有方法。", "keywords": "视频定制, 身份保留, 动态生成, 偏好学习, 混合采样", "comments": "MagicID的创新之处在于其摒弃了传统的自重建训练，转而采用混合偏好优化，并引入了独特的混合采样策略来平衡身份一致性和动态性。这种方法有效地解决了视频定制中的核心挑战，为生成高质量、个性化视频提供了新的思路。"}}
{"id": "2503.12701", "pdf": "https://arxiv.org/pdf/2503.12701", "abs": "https://arxiv.org/abs/2503.12701", "authors": ["Javier Tirado-Garín", "Javier Civera"], "title": "AnyCalib: On-Manifold Learning for Model-Agnostic Single-View Camera Calibration", "categories": ["cs.CV"], "comment": null, "summary": "We present AnyCalib, a method for calibrating the intrinsic parameters of a\ncamera from a single in-the-wild image, that is agnostic to the camera model.\nCurrent methods are predominantly tailored to specific camera models and/or\nrequire extrinsic cues, such as the direction of gravity, to be visible in the\nimage. In contrast, we argue that the perspective and distortion cues inherent\nin images are sufficient for model-agnostic camera calibration. To demonstrate\nthis, we frame the calibration process as the regression of the rays\ncorresponding to each pixel. We show, for the first time, that this\nintermediate representation allows for a closed-form recovery of the intrinsics\nfor a wide range of camera models, including but not limited to: pinhole,\nBrown-Conrady and Kannala-Brandt. Our approach also applies to edited --\ncropped and stretched -- images. Experimentally, we demonstrate that AnyCalib\nconsistently outperforms alternative methods, including 3D foundation models,\ndespite being trained on orders of magnitude less data. Code is available at\nhttps://github.com/javrtg/AnyCalib.", "AI": {"title_translation": "AnyCalib：基于流形学习的与模型无关的单视图相机标定", "tldr": "AnyCalib是一种与相机模型无关的单视图相机内参标定方法，通过回归像素对应的光线，实现对多种相机模型的闭式恢复，且性能优于现有方法，所需数据量更少。", "motivation": "现有相机标定方法通常针对特定相机模型或需要外部线索（如重力方向），限制了其通用性。本文旨在证明图像固有的透视和畸变线索足以进行与模型无关的相机标定。", "method": "提出AnyCalib方法，将相机标定过程视为对每个像素对应光线的回归。该中间表示首次实现了对包括针孔、Brown-Conrady和Kannala-Brandt在内的多种相机模型的内参闭式恢复。该方法也适用于编辑过的图像（裁剪和拉伸）。", "result": "实验证明，AnyCalib持续优于包括3D基础模型在内的其他方法，尽管其训练数据量少几个数量级。", "conclusion": "AnyCalib证明了仅凭图像固有的透视和畸变线索，通过光线回归的中间表示，可以实现与模型无关且高效的单视图相机内参标定，并在多种相机模型上表现出优越性能。", "translation": "我们提出了AnyCalib，一种从单张野外图像校准相机内参的方法，该方法与相机模型无关。现有方法主要针对特定相机模型和/或需要图像中可见的外部线索，例如重力方向。相比之下，我们认为图像中固有的透视和畸变线索足以进行与模型无关的相机校准。为了证明这一点，我们将校准过程框定为对每个像素对应光线的回归。我们首次表明，这种中间表示允许对各种相机模型进行内参的闭式恢复，包括但不限于：针孔、Brown-Conrady和Kannala-Brandt。我们的方法也适用于经过编辑的图像——裁剪和拉伸。实验表明，尽管AnyCalib的训练数据量少几个数量级，但它持续优于替代方法，包括3D基础模型。代码可在https://github.com/javrtg/AnyCalib获取。", "summary": "AnyCalib是一种新型的与相机模型无关的单视图相机内参标定方法。它通过将标定过程重构为像素光线回归，首次实现了对多种主流相机模型（如针孔、Brown-Conrady和Kannala-Brandt）的内参闭式恢复。该方法仅依赖图像固有的透视和畸变线索，无需外部信息，且对编辑过的图像也有效。实验结果表明，AnyCalib在数据量显著减少的情况下，性能优于现有方法，包括3D基础模型。", "keywords": "相机标定, 单视图, 模型无关, 内参, 光线回归", "comments": "这篇论文的创新点在于提出了一种模型无关的单视图相机标定方法，通过将标定问题转化为光线回归，实现了对多种相机模型的内参闭式恢复，极大地提升了通用性和实用性。其在仅使用少量数据训练的情况下超越现有方法的表现，凸显了其高效性和潜力，对实际应用具有重要意义。"}}
{"id": "2503.12706", "pdf": "https://arxiv.org/pdf/2503.12706", "abs": "https://arxiv.org/abs/2503.12706", "authors": ["Rahul Deshmukh", "Avinash Kak"], "title": "SatDepth: A Novel Dataset for Satellite Image Matching", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep-learning based methods for image matching have\ndemonstrated their superiority over traditional algorithms, enabling\ncorrespondence estimation in challenging scenes with significant differences in\nviewing angles, illumination and weather conditions. However, the existing\ndatasets, learning frameworks, and evaluation metrics for the deep-learning\nbased methods are limited to ground-based images recorded with pinhole cameras\nand have not been explored for satellite images. In this paper, we present\n``SatDepth'', a novel dataset that provides dense ground-truth correspondences\nfor training image matching frameworks meant specifically for satellite images.\nSatellites capture images from various viewing angles and tracks through\nmultiple revisits over a region. To manage this variability, we propose a\ndataset balancing strategy through a novel image rotation augmentation\nprocedure. This procedure allows for the discovery of corresponding pixels even\nin the presence of large rotational differences between the images. We\nbenchmark four existing image matching frameworks using our dataset and carry\nout an ablation study that confirms that the models trained with our dataset\nwith rotation augmentation outperform (up to 40% increase in precision) the\nmodels trained with other datasets, especially when there exist large\nrotational differences between the images.", "AI": {"title_translation": "SatDepth：一种用于卫星图像匹配的新型数据集", "tldr": "提出了SatDepth数据集，用于解决深度学习卫星图像匹配中现有数据集的局限性，并通过旋转增强提高了匹配精度。", "motivation": "现有用于图像匹配的深度学习数据集仅限于地面图像，未能解决卫星图像（例如，视角、光照、天气条件和特别是大旋转差异）所带来的挑战。", "method": "本文提出了“SatDepth”数据集，为卫星图像匹配框架提供密集的真实对应点。为应对图像之间的大旋转差异，该研究提出了一种新颖的图像旋转增强程序作为数据集平衡策略。作者还使用该数据集对四个现有图像匹配框架进行了基准测试，并进行了消融研究。", "result": "使用SatDepth数据集（特别是结合旋转增强）训练的模型，在精度上比使用其他数据集训练的模型高出高达40%，尤其是在图像之间存在较大旋转差异的情况下。", "conclusion": "SatDepth数据集及其提出的旋转增强策略显著提高了基于深度学习的卫星图像匹配性能，尤其是在处理具有挑战性的旋转场景时。", "translation": "最近，基于深度学习的图像匹配方法已证明其优于传统算法，能够在视角、光照和天气条件差异显著的挑战性场景中进行对应点估计。然而，现有用于深度学习方法的图像匹配数据集、学习框架和评估指标仅限于使用针孔相机记录的地面图像，尚未针对卫星图像进行探索。在本文中，我们提出了“SatDepth”，这是一个新颖的数据集，为专门用于卫星图像的图像匹配框架提供密集的真实对应点。卫星通过对一个区域的多次重访，从不同的视角和轨道捕获图像。为了管理这种可变性，我们通过一种新颖的图像旋转增强程序提出了一种数据集平衡策略。此程序即使在图像之间存在大旋转差异的情况下，也能发现对应的像素。我们使用我们的数据集对四个现有图像匹配框架进行了基准测试，并进行了一项消融研究，证实了使用我们的数据集并进行旋转增强训练的模型优于使用其他数据集训练的模型（精度提高高达40%），尤其是在图像之间存在大旋转差异时。", "summary": "本文介绍了SatDepth，一个专为基于深度学习的卫星图像匹配设计的新数据集。它通过为卫星图像提供密集的真实对应点，解决了现有地面数据集的局限性，这些卫星图像通常表现出显著的旋转差异。作者在SatDepth中提出了一种新颖的图像旋转增强程序来管理这种可变性。基准测试和消融研究表明，使用SatDepth（特别是结合旋转增强）训练的模型，与使用其他数据集训练的模型相比，精度可提高高达40%，尤其是在存在大旋转差异的情况下。", "keywords": "卫星图像匹配, 数据集, 深度学习, 旋转增强, 遥感", "comments": "这篇论文通过提供首个专门用于卫星图像匹配的深度学习数据集，弥补了遥感领域的一个关键空白。其创新的旋转增强策略对于处理卫星图像固有的变异性至关重要，使得该数据集在实际应用中具有高度实用性和影响力，有助于提高该领域模型的鲁棒性和准确性。"}}
{"id": "2503.12720", "pdf": "https://arxiv.org/pdf/2503.12720", "abs": "https://arxiv.org/abs/2503.12720", "authors": ["Feng Qiao", "Zhexiao Xiong", "Eric Xing", "Nathan Jacobs"], "title": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching", "categories": ["cs.CV"], "comment": "Project page is available at https://qjizhi.github.io/genstereo", "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo", "AI": {"title_translation": "GenStereo：迈向开放世界立体图像生成与无监督匹配", "tldr": "GenStereo是一种基于扩散的方法，用于生成高质量立体图像并进行无监督立体匹配。它通过创新的扩散过程和自适应融合机制，克服了现有方法的局限性，在生成和匹配任务中均达到最先进水平。", "motivation": "获取高质量立体图像具有挑战性，因为需要精确校准双摄像头设置和获取准确的密集视差图。现有立体图像生成方法通常只关注视觉质量或几何精度，而不能兼顾。因此，需要一种能够同时实现高视觉质量和几何精度的立体图像生成方法。", "method": "GenStereo是一种基于扩散的方法，包含两项主要创新：1) 将扩散过程条件化于视差感知坐标嵌入和扭曲输入图像，从而实现比以往方法更精确的立体对齐；2) 一种自适应融合机制，智能地将扩散生成的图像与扭曲图像结合，从而提高真实感和视差一致性。该方法在11个多样化的立体数据集上进行了广泛训练。", "result": "GenStereo在立体图像生成和无监督立体匹配任务中均达到了最先进的性能。它展示了强大的泛化能力，并消除了对复杂硬件设置的需求。", "conclusion": "GenStereo框架能够实现高质量立体图像生成，并可用于无监督学习场景，对现实世界应用具有重要价值。它解决了现有方法在兼顾视觉质量和几何精度方面的不足。", "translation": "立体图像是许多应用的基础，包括扩展现实（XR）设备、自动驾驶和机器人技术。不幸的是，由于双摄像头设置的精确校准要求以及获取准确、密集视差图的复杂性，获取高质量立体图像仍然具有挑战性。现有的立体图像生成方法通常只关注用于观看的视觉质量或用于匹配的几何精度，而不能兼顾。我们引入了GenStereo，一种基于扩散的方法，旨在弥补这一空白。该方法包括两项主要创新：(1) 将扩散过程条件化于视差感知坐标嵌入和扭曲输入图像，从而实现比以往方法更精确的立体对齐；(2) 一种自适应融合机制，智能地将扩散生成的图像与扭曲图像结合，从而提高真实感和视差一致性。通过在11个多样化的立体数据集上进行广泛训练，GenStereo展示了强大的泛化能力。GenStereo在立体图像生成和无监督立体匹配任务中均达到了最先进的性能。我们的框架消除了对复杂硬件设置的需求，同时实现了高质量立体图像生成，使其对现实世界应用和无监督学习场景都具有重要价值。项目页面可在https://qjizhi.github.io/genstereo获取。", "summary": "GenStereo是一种新颖的基于扩散的立体图像生成方法，旨在解决现有方法在兼顾视觉质量和几何精度方面的不足。它通过创新的视差感知扩散过程和自适应融合机制，在保持高真实感的同时实现精确的立体对齐和视差一致性。该方法在多个数据集上进行了广泛训练，并在立体图像生成和无监督立体匹配任务中均取得了最先进的性能，为现实应用和无监督学习提供了无需复杂硬件的高质量立体图像生成能力。", "keywords": "立体图像生成, 无监督匹配, 扩散模型, 视差估计, 深度学习", "comments": "GenStereo的创新之处在于其将扩散模型应用于立体图像生成，并通过视差感知条件化和自适应融合机制，有效地平衡了图像的视觉质量和几何精度。这对于需要高精度立体数据的XR、自动驾驶和机器人等领域具有重要意义。该方法能够消除对复杂硬件的需求，降低了高质量立体数据获取的门槛，具有广泛的应用前景。"}}
{"id": "2503.12731", "pdf": "https://arxiv.org/pdf/2503.12731", "abs": "https://arxiv.org/abs/2503.12731", "authors": ["Haoran Ma", "Kaihan Zhang", "Jiannan Cai"], "title": "Navigating Heat Exposure: Simulation of Route Planning Based on Visual Language Model Agents", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Heat exposure significantly influences pedestrian routing behaviors. Existing\nmethods such as agent-based modeling (ABM) and empirical measurements fail to\naccount for individual physiological variations and environmental perception\nmechanisms under thermal stress. This results in a lack of human-centred,\nheat-adaptive routing suggestions. To address these limitations, we propose a\nnovel Vision Language Model (VLM)-driven Persona-Perception-Planning-Memory\n(PPPM) framework that integrating street view imagery and urban network\ntopology to simulate heat-adaptive pedestrian routing. Through structured\nprompt engineering on Gemini-2.0 model, eight distinct heat-sensitive personas\nwere created to model mobility behaviors during heat exposure, with empirical\nvalidation through questionnaire survey. Results demonstrate that simulation\noutputs effectively capture inter-persona variations, achieving high\nsignificant congruence with observed route preferences and highlighting\ndifferences in the factors driving agents decisions. Our framework is highly\ncost-effective, with simulations costing 0.006USD and taking 47.81s per route.\nThis Artificial Intelligence-Generated Content (AIGC) methodology advances\nurban climate adaptation research by enabling high-resolution simulation of\nthermal-responsive mobility patterns, providing actionable insights for\nclimate-resilient urban planning.", "AI": {"title_translation": "导航热暴露：基于视觉语言模型代理的路径规划模拟", "tldr": "该论文提出了一种基于视觉语言模型（VLM）的PPPM框架，用于模拟适应热的行人路径规划，通过创建热敏感人格并进行实证验证。该框架与观察到的路径偏好高度一致，且成本效益高，为城市规划提供了见解。", "motivation": "现有行人热暴露下的路径规划方法（如ABM和经验测量）未能考虑个体生理差异和环境感知机制，导致缺乏以人为本、适应热的路径建议。", "method": "本文提出了一种新颖的视觉语言模型（VLM）驱动的“人格-感知-规划-记忆”（PPPM）框架，该框架整合了街景图像和城市网络拓扑结构。通过对Gemini-2.0模型进行结构化提示工程，创建了八种不同的热敏感人格，并通过问卷调查对移动行为进行了实证验证。", "result": "模拟输出有效地捕捉了人格间的差异，与观察到的路径偏好高度一致，并突出了驱动代理决策因素的差异。该框架成本效益高，每次路径模拟成本为0.006美元，耗时47.81秒。", "conclusion": "这种人工智能生成内容（AIGC）方法通过实现热响应移动模式的高分辨率模拟，推进了城市气候适应研究，为气候适应型城市规划提供了可操作的见解。", "translation": "热暴露显著影响行人路径规划行为。现有方法，如基于代理的建模（ABM）和经验测量，未能考虑热应激下的个体生理差异和环境感知机制。这导致缺乏以人为本、适应热的路径建议。为了解决这些限制，我们提出了一种新颖的视觉语言模型（VLM）驱动的“人格-感知-规划-记忆”（PPPM）框架，该框架整合了街景图像和城市网络拓扑结构，以模拟适应热的行人路径规划。通过对Gemini-2.0模型进行结构化提示工程，创建了八种不同的热敏感人格，以模拟热暴露期间的移动行为，并通过问卷调查进行了实证验证。结果表明，模拟输出有效地捕捉了人格间的差异，与观察到的路径偏好高度一致，并突出了驱动代理决策因素的差异。我们的框架成本效益高，每次路径模拟成本为0.006美元，耗时47.81秒。这种人工智能生成内容（AIGC）方法通过实现热响应移动模式的高分辨率模拟，推进了城市气候适应研究，为气候适应型城市规划提供了可操作的见解。", "summary": "该论文提出了一种由视觉语言模型（VLM）驱动的“人格-感知-规划-记忆”（PPPM）框架，旨在模拟适应热的行人路径规划。为解决现有方法在考虑热应激下个体差异方面的不足，该框架整合了街景图像和城市网络拓扑结构。它利用Gemini-2.0上的结构化提示工程来创建并实证验证了八种热敏感人格。模拟结果表明，该框架能有效捕捉人格间差异，与观察到的路径偏好高度一致，且具有成本效益，为气候适应型城市规划提供了宝贵见解。", "keywords": "热暴露, 行人路径规划, 视觉语言模型, 人格-感知-规划-记忆, 城市规划, 气候适应", "comments": "该论文提出了一种创新方法，利用视觉语言模型（VLM）和提示工程（特别是Gemini-2.0）来创建逼真的、热敏感的人格，用于行人路径规划模拟。这种以人为本的方法解决了城市气候适应研究中的一个关键空白，为生成城市规划的可操作见解提供了一种可扩展且具有成本效益的方法。实证验证增加了其可信度。"}}
{"id": "2503.12732", "pdf": "https://arxiv.org/pdf/2503.12732", "abs": "https://arxiv.org/abs/2503.12732", "authors": ["Zibin Liu", "Banglei Guan", "Yang Shang", "Yifei Bian", "Pengju Sun", "Qifeng Yu"], "title": "Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Geoscience and Remote Sensing", "summary": "Pose tracking of uncooperative spacecraft is an essential technology for\nspace exploration and on-orbit servicing, which remains an open problem. Event\ncameras possess numerous advantages, such as high dynamic range, high temporal\nresolution, and low power consumption. These attributes hold the promise of\novercoming challenges encountered by conventional cameras, including motion\nblur and extreme illumination, among others. To address the standard on-orbit\nobservation missions, we propose a line-based pose tracking method for\nuncooperative spacecraft utilizing a stereo event camera. To begin with, we\nestimate the wireframe model of uncooperative spacecraft, leveraging the\nspatio-temporal consistency of stereo event streams for line-based\nreconstruction. Then, we develop an effective strategy to establish\ncorrespondences between events and projected lines of uncooperative spacecraft.\nUsing these correspondences, we formulate the pose tracking as a continuous\noptimization process over 6-DOF motion parameters, achieved by minimizing\nevent-line distances. Moreover, we construct a stereo event-based uncooperative\nspacecraft motion dataset, encompassing both simulated and real events. The\nproposed method is quantitatively evaluated through experiments conducted on\nour self-collected dataset, demonstrating an improvement in terms of\neffectiveness and accuracy over competing methods. The code will be\nopen-sourced at https://github.com/Zibin6/SE6PT.", "AI": {"title_translation": "基于立体事件的非合作航天器6自由度姿态跟踪", "tldr": "该论文提出了一种利用立体事件相机对非合作航天器进行6自由度姿态跟踪的方法，通过线特征重建和事件-线对应关系，实现连续优化，并在自建数据集上验证了其有效性和准确性。", "motivation": "非合作航天器的姿态跟踪是空间探索和在轨服务中的关键技术，但仍是一个开放问题。传统相机面临运动模糊和极端光照等挑战，而事件相机具有高动态范围、高时间分辨率和低功耗等优势，有望克服这些挑战。", "method": "本文提出了一种利用立体事件相机对非合作航天器进行基于线的姿态跟踪方法。首先，利用立体事件流的时空一致性进行线基重建，估计非合作航天器的线框模型。然后，开发策略建立事件与非合作航天器投影线之间的对应关系。最后，将姿态跟踪表述为6自由度运动参数的连续优化过程，通过最小化事件-线距离实现。此外，还构建了一个包含模拟和真实事件的立体事件基非合作航天器运动数据集。", "result": "提出的方法在自收集数据集上进行了定量评估，结果表明其在有效性和准确性方面优于竞争方法。", "conclusion": "本文提出了一种基于立体事件相机的非合作航天器线基姿态跟踪方法，通过线框模型估计、事件-线对应和连续优化实现了姿态跟踪，并在实验中证明了其有效性和准确性，有望解决传统相机在特定环境下的局限性。", "translation": "非合作航天器的姿态跟踪是空间探索和在轨服务中的一项基本技术，但仍是一个开放问题。事件相机具有高动态范围、高时间分辨率和低功耗等诸多优点。这些特性有望克服传统相机遇到的挑战，包括运动模糊和极端光照等。为了解决标准的在轨观测任务，我们提出了一种利用立体事件相机对非合作航天器进行基于线的姿态跟踪方法。首先，我们利用立体事件流的时空一致性进行基于线的重建，估计非合作航天器的线框模型。然后，我们开发了一种有效的策略来建立事件与非合作航天器投影线之间的对应关系。利用这些对应关系，我们将姿态跟踪表述为6自由度运动参数的连续优化过程，通过最小化事件-线距离来实现。此外，我们构建了一个立体事件基非合作航性器运动数据集，包括模拟事件和真实事件。通过在我们自己收集的数据集上进行的实验，对所提出的方法进行了定量评估，结果表明其在有效性和准确性方面优于竞争方法。代码将在 https://github.com/Zibin6/SE6PT 开源。", "summary": "本文针对非合作航天器姿态跟踪这一开放问题，提出了一种基于立体事件相机的线基姿态跟踪方法。该方法首先利用立体事件流进行线框模型重建，然后建立事件与投影线之间的对应关系，并通过最小化事件-线距离实现6自由度运动参数的连续优化。实验结果表明，该方法在自建数据集上比现有方法更有效和准确，并有望克服传统相机在极端环境下的局限性。", "keywords": "事件相机, 姿态跟踪, 非合作航天器, 6自由度, 立体视觉", "comments": "该论文的创新点在于首次将立体事件相机应用于非合作航天器的6自由度姿态跟踪，并提出了基于线特征的重建和优化方法。事件相机的引入有望解决传统相机在运动模糊和极端光照下的问题，对于空间探索和在轨服务具有重要意义。所构建的立体事件数据集也为后续研究提供了基础。"}}
{"id": "2503.12745", "pdf": "https://arxiv.org/pdf/2503.12745", "abs": "https://arxiv.org/abs/2503.12745", "authors": ["Patrick Rim", "Hyoungseob Park", "S. Gangopadhyay", "Ziyao Zeng", "Younjoon Chung", "Alex Wong"], "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We present ProtoDepth, a novel prototype-based approach for continual\nlearning of unsupervised depth completion, the multimodal 3D reconstruction\ntask of predicting dense depth maps from RGB images and sparse point clouds.\nThe unsupervised learning paradigm is well-suited for continual learning, as\nground truth is not needed. However, when training on new non-stationary\ndistributions, depth completion models will catastrophically forget previously\nlearned information. We address forgetting by learning prototype sets that\nadapt the latent features of a frozen pretrained model to new domains. Since\nthe original weights are not modified, ProtoDepth does not forget when\ntest-time domain identity is known. To extend ProtoDepth to the challenging\nsetting where the test-time domain identity is withheld, we propose to learn\ndomain descriptors that enable the model to select the appropriate prototype\nset for inference. We evaluate ProtoDepth on benchmark dataset sequences, where\nwe reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for\noutdoor to achieve the state of the art.", "AI": {"title_translation": "ProtoDepth：基于原型的无监督持续深度补全", "tldr": "ProtoDepth通过学习原型集来解决无监督持续深度补全中的灾难性遗忘问题，并在基准数据集上显著减少了遗忘。", "motivation": "现有的深度补全模型在新的非平稳分布上训练时，会发生灾难性遗忘，即忘记之前学习到的信息。无监督学习范式虽然适合持续学习，但仍面临此问题。", "method": "提出ProtoDepth，一种新颖的基于原型的方法，用于无监督持续深度补全。它通过学习原型集来适应冻结预训练模型的潜在特征到新领域，而不修改原始权重。为了应对测试时领域身份未知的情况，还提出了学习领域描述符，使模型能够选择合适的原型集进行推理。", "result": "ProtoDepth在基准数据集上进行了评估，与基线相比，室内遗忘率降低了52.2%，室外遗忘率降低了53.2%，达到了最先进的水平。当测试时领域身份已知时，ProtoDepth不会发生遗忘。", "conclusion": "ProtoDepth通过其原型学习和领域描述符机制，有效解决了无监督持续深度补全中的灾难性遗忘问题，并在多个领域实现了显著的性能提升。", "translation": "我们提出了ProtoDepth，一种新颖的基于原型的方法，用于无监督持续深度补全，这是一项从RGB图像和稀疏点云预测密集深度图的多模态3D重建任务。无监督学习范式非常适合持续学习，因为它不需要真值。然而，当在新非平稳分布上训练时，深度补全模型将灾难性地遗忘之前学习到的信息。我们通过学习原型集来解决遗忘问题，这些原型集将冻结预训练模型的潜在特征适应到新领域。由于原始权重未被修改，当测试时领域身份已知时，ProtoDepth不会遗忘。为了将ProtoDepth扩展到测试时领域身份被隐藏的挑战性设置，我们建议学习领域描述符，使模型能够选择合适的原型集进行推理。我们在基准数据集序列上评估了ProtoDepth，与基线相比，室内遗忘率降低了52.2%，室外遗忘率降低了53.2%，从而达到了最先进的水平。", "summary": "ProtoDepth是一种新颖的基于原型的方法，旨在解决无监督持续深度补全中的灾难性遗忘问题。该方法通过学习原型集来使冻结预训练模型的潜在特征适应新领域，从而避免修改原始权重。为应对测试时领域未知的情况，ProtoDepth还引入了领域描述符以自动选择合适的原型集。在基准数据集上的评估显示，ProtoDepth显著减少了遗忘，室内和室外遗忘率分别降低了52.2%和53.2%，达到了最先进的性能。", "keywords": "持续学习, 深度补全, 无监督学习, 原型学习, 灾难性遗忘", "comments": "这篇论文通过引入原型学习和领域描述符的创新机制，有效地解决了无监督持续学习中长期存在的灾难性遗忘问题。其核心在于不修改原始模型权重，而是通过适配潜在特征来实现领域适应，这在保持模型性能的同时避免了遗忘。该方法在实际应用中具有重要意义，尤其是在需要模型持续学习新环境而无需人工标注的场景。"}}
{"id": "2503.12751", "pdf": "https://arxiv.org/pdf/2503.12751", "abs": "https://arxiv.org/abs/2503.12751", "authors": ["Yifan Zhan", "Wangze Xu", "Qingtian Zhu", "Muyao Niu", "Mingze Ma", "Yifei Liu", "Zhihang Zhong", "Xiao Sun", "Yinqiang Zheng"], "title": "R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars", "categories": ["cs.CV"], "comment": null, "summary": "We present R3-Avatar, incorporating a temporal codebook, to overcome the\ninability of human avatars to be both animatable and of high-fidelity rendering\nquality. Existing video-based reconstruction of 3D human avatars either focuses\nsolely on rendering, lacking animation support, or learns a pose-appearance\nmapping for animating, which degrades under limited training poses or complex\nclothing. In this paper, we adopt a \"record-retrieve-reconstruct\" strategy that\nensures high-quality rendering from novel views while mitigating degradation in\nnovel poses. Specifically, disambiguating timestamps record temporal appearance\nvariations in a codebook, ensuring high-fidelity novel-view rendering, while\nnovel poses retrieve corresponding timestamps by matching the most similar\ntraining poses for augmented appearance. Our R3-Avatar outperforms cutting-edge\nvideo-based human avatar reconstruction, particularly in overcoming visual\nquality degradation in extreme scenarios with limited training human poses and\ncomplex clothing.", "AI": {"title_translation": "R3-Avatar: 记录和检索时间码本用于重建真实感人体化身", "tldr": "R3-Avatar提出了一种结合时间码本的方法，解决了现有3D人体化身在动画性和高保真渲染质量之间难以兼顾的问题，尤其在有限训练姿态和复杂衣物下表现出色。", "motivation": "现有基于视频的3D人体化身重建方法要么只关注渲染而缺乏动画支持，要么通过姿态-外观映射实现动画，但在训练姿态有限或衣物复杂时性能下降，无法同时实现可动画性和高保真渲染质量。", "method": "本文提出“记录-检索-重建”策略。通过时间戳在码本中记录时间外观变化，以确保高保真新视角渲染；新姿态通过匹配最相似的训练姿态来检索相应的时间戳，以增强外观。", "result": "R3-Avatar在克服极端场景（有限训练人体姿态和复杂衣物）下的视觉质量下降方面，超越了最先进的基于视频的人体化身重建方法。", "conclusion": "R3-Avatar通过其独特的“记录-检索-重建”策略和时间码本，成功地实现了高保真且可动画的人体化身重建，尤其在挑战性条件下表现优异。", "translation": "我们提出了R3-Avatar，它结合了时间码本，以克服人体化身无法同时实现可动画性和高保真渲染质量的问题。现有的基于视频的3D人体化身重建要么只专注于渲染而缺乏动画支持，要么学习姿态-外观映射以实现动画，但在有限的训练姿态或复杂衣物下会退化。在本文中，我们采用“记录-检索-重建”策略，确保从新视角进行高质量渲染，同时减轻新姿态下的退化。具体来说，通过消除歧义的时间戳在码本中记录时间外观变化，确保高保真新视角渲染，而新姿态通过匹配最相似的训练姿态来检索相应的时间戳，以增强外观。我们的R3-Avatar超越了最先进的基于视频的人体化身重建方法，特别是在克服有限训练人体姿态和复杂衣物等极端场景下的视觉质量下降。", "summary": "R3-Avatar提出了一种新颖的“记录-检索-重建”策略，通过引入时间码本，解决了现有3D人体化身在动画性和高保真渲染质量之间难以同时实现的问题。该方法通过记录时间外观变化并基于姿态相似性检索对应时间戳，有效提升了在有限训练姿态和复杂衣物下的渲染质量和动画效果，表现优于现有技术。", "keywords": "人体化身, 时间码本, 3D重建, 高保真渲染, 动画", "comments": "R3-Avatar的创新之处在于其“记录-检索-重建”策略和时间码本的使用，有效解决了高保真渲染与动画能力之间的矛盾，尤其是在数据受限和复杂场景下的鲁棒性表现突出，对人体化身重建领域具有重要意义。"}}
{"id": "2503.12758", "pdf": "https://arxiv.org/pdf/2503.12758", "abs": "https://arxiv.org/abs/2503.12758", "authors": ["Zhifeng Wang", "Renjiao Yi", "Xin Wen", "Chenyang Zhu", "Kai Xu"], "title": "VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for Angiography Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Angiography imaging is a medical imaging technique that enhances the\nvisibility of blood vessels within the body by using contrast agents.\nAngiographic images can effectively assist in the diagnosis of vascular\ndiseases. However, contrast agents may bring extra radiation exposure which is\nharmful to patients with health risks. To mitigate these concerns, in this\npaper, we aim to automatically generate angiography from non-angiographic\ninputs, by leveraging and enhancing the inherent physical properties of\nvascular structures. Previous methods relying on 2D slice-based angiography\nsynthesis struggle with maintaining continuity in 3D vascular structures and\nexhibit limited effectiveness across different imaging modalities. We propose\nVasTSD, a 3D vascular tree-state space diffusion model to synthesize\nangiography from 3D non-angiographic volumes, with a novel state space\nserialization approach that dynamically constructs vascular tree topologies,\nintegrating these with a diffusion-based generative model to ensure the\ngeneration of anatomically continuous vasculature in 3D volumes. A pre-trained\nvision embedder is employed to construct vascular state space representations,\nenabling consistent modeling of vascular structures across multiple modalities.\nExtensive experiments on various angiographic datasets demonstrate the\nsuperiority of VasTSD over prior works, achieving enhanced continuity of blood\nvessels in synthesized angiographic synthesis for multiple modalities and\nanatomical regions.", "AI": {"title_translation": "VasTSD：学习用于血管造影合成的3D血管树状状态空间扩散模型", "tldr": "提出VasTSD，一个3D血管树状状态空间扩散模型，可从非血管造影输入合成高连续性的3D血管造影图像，以避免造影剂的辐射暴露。", "motivation": "血管造影剂带来辐射暴露，对患者有害；现有2D方法难以保持3D血管结构的连续性，且跨模态效果有限。", "method": "提出VasTSD，一个3D血管树状状态空间扩散模型，用于从3D非血管造影体积合成血管造影。它采用新颖的状态空间序列化方法，动态构建血管树拓扑，并结合扩散生成模型，确保生成解剖学上连续的3D血管。使用预训练视觉嵌入器构建血管状态空间表示，实现多模态血管结构的一致建模。", "result": "在各种血管造影数据集上的大量实验表明，VasTSD优于现有工作，在多模态和解剖区域的合成血管造影中实现了血管连续性的增强。", "conclusion": "VasTSD通过其创新的3D血管树状状态空间扩散模型，成功地从非血管造影输入合成了高质量、连续的3D血管造影图像，有效解决了传统方法的局限性并避免了造影剂的副作用。", "translation": "血管造影是一种医学成像技术，通过使用造影剂增强体内血管的可见性。血管造影图像可以有效地辅助血管疾病的诊断。然而，造影剂可能带来额外的辐射暴露，这对有健康风险的患者有害。为了缓解这些担忧，本文旨在通过利用和增强血管结构的固有物理特性，自动从非血管造影输入生成血管造影。以前依赖于2D切片血管造影合成的方法在保持3D血管结构连续性方面存在困难，并且在不同成像模态之间表现出有限的有效性。我们提出了VasTSD，一个3D血管树状状态空间扩散模型，用于从3D非血管造影体积合成血管造影，其采用新颖的状态空间序列化方法，动态构建血管树拓扑，并将这些与基于扩散的生成模型相结合，以确保在3D体积中生成解剖学上连续的血管系统。采用预训练的视觉嵌入器来构建血管状态空间表示，从而实现多模态血管结构的一致建模。在各种血管造影数据集上进行的大量实验表明，VasTSD优于现有工作，在多模态和解剖区域的合成血管造影中实现了血管连续性的增强。", "summary": "本文提出VasTSD，一个3D血管树状状态空间扩散模型，旨在从非血管造影输入自动合成血管造影图像，以避免传统造影剂带来的辐射暴露。该模型通过新颖的状态空间序列化方法动态构建血管树拓扑，并结合扩散模型确保生成解剖学上连续的3D血管结构。实验证明，VasTSD在多模态和不同解剖区域的血管造影合成中，血管连续性优于现有方法。", "keywords": "血管造影合成, 3D血管, 扩散模型, 状态空间, 血管树拓扑", "comments": "该研究的创新点在于提出了3D血管树状状态空间扩散模型（VasTSD），并引入了新颖的状态空间序列化方法，有效解决了传统2D合成方法在3D血管结构连续性上的不足。其重要性在于提供了一种无需造影剂即可生成高质量血管造影图像的途径，从而降低了患者的辐射风险，并能跨多模态和解剖区域应用，具有较高的临床应用潜力。"}}
{"id": "2503.12763", "pdf": "https://arxiv.org/pdf/2503.12763", "abs": "https://arxiv.org/abs/2503.12763", "authors": ["Kewei Sui", "Anindita Ghosh", "Inwoo Hwang", "Jian Wang", "Chuan Guo"], "title": "A Survey on Human Interaction Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": "The repository listing relevant papers is accessible at:\n  https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation", "summary": "Humans inhabit a world defined by interactions -- with other humans, objects,\nand environments. These interactive movements not only convey our relationships\nwith our surroundings but also demonstrate how we perceive and communicate with\nthe real world. Therefore, replicating these interaction behaviors in digital\nsystems has emerged as an important topic for applications in robotics, virtual\nreality, and animation. While recent advances in deep generative models and new\ndatasets have accelerated progress in this field, significant challenges remain\nin modeling the intricate human dynamics and their interactions with entities\nin the external world. In this survey, we present, for the first time, a\ncomprehensive overview of the literature in human interaction motion\ngeneration. We begin by establishing foundational concepts essential for\nunderstanding the research background. We then systematically review existing\nsolutions and datasets across three primary interaction tasks -- human-human,\nhuman-object, and human-scene interactions -- followed by evaluation metrics.\nFinally, we discuss open research directions and future opportunities.", "AI": {"title_translation": "人类交互运动生成综述", "tldr": "本综述首次全面概述了人类交互运动生成领域，涵盖了基础概念、现有解决方案、数据集、评估指标以及人-人、人-物和人-场景交互中的开放研究方向和未来机遇。", "motivation": "在数字系统中复制人类交互行为是机器人、虚拟现实和动画等应用中的一个重要课题，但由于建模复杂的人类动力学及其与外部实体的交互存在挑战。", "method": "本综述首次全面概述了人类交互运动生成领域的文献，首先建立了理解研究背景所需的基础概念，然后系统地回顾了人-人、人-物和人-场景交互这三种主要交互任务的现有解决方案、数据集和评估指标，最后讨论了开放的研究方向和未来的机遇。", "result": "本综述首次提供了人类交互运动生成领域的全面文献概述，系统地分类了现有解决方案和数据集，并指出了开放的研究方向。", "conclusion": "本论文通过讨论人类交互运动生成领域的开放研究方向和未来的机遇来总结。", "translation": "人类生活在一个由与他人、物体和环境互动定义的世界中。这些交互式运动不仅传达了我们与周围环境的关系，还展示了我们如何感知真实世界并与之交流。因此，在数字系统中复制这些交互行为已成为机器人、虚拟现实和动画应用中的一个重要课题。尽管深度生成模型和新数据集的最新进展加速了该领域的进步，但在建模复杂的人类动力学及其与外部实体的交互方面仍存在重大挑战。在这篇综述中，我们首次对人类交互运动生成领域的文献进行了全面概述。我们首先建立了理解研究背景所必需的基础概念。然后，我们系统地回顾了人类-人类、人类-物体和人类-场景交互这三种主要交互任务的现有解决方案和数据集，并讨论了评估指标。最后，我们讨论了开放的研究方向和未来的机遇。", "summary": "本综述论文首次全面概述了人类交互运动生成领域。它涵盖了基础概念，系统地回顾了人-人、人-物和人-场景交互中的现有解决方案、数据集和评估指标，并讨论了未来的研究方向。尽管在建模复杂的人类动力学方面存在挑战，但该领域对机器人、虚拟现实和动画至关重要。", "keywords": "人类交互, 运动生成, 综述, 机器人, 虚拟现实", "comments": "这篇综述很重要，因为它首次全面概述了一个关键且具有挑战性的领域。它通过对交互进行分类和突出差距，有助于构建该领域的知识体系，这对未来的研究非常有价值。"}}
{"id": "2503.12764", "pdf": "https://arxiv.org/pdf/2503.12764", "abs": "https://arxiv.org/abs/2503.12764", "authors": ["Yidi Liu", "Dong Li", "Yuxin Ma", "Jie Huang", "Wenlong Zhang", "Xueyang Fu", "Zheng-jun Zha"], "title": "Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-definition (UHD) image restoration often faces computational\nbottlenecks and information loss due to its extremely high resolution. Existing\nstudies based on Variational Autoencoders (VAE) improve efficiency by\ntransferring the image restoration process from pixel space to latent space.\nHowever, degraded components are inherently coupled with background elements in\ndegraded images, both information loss during compression and information gain\nduring compensation remain uncontrollable. These lead to restored images often\nexhibiting image detail loss and incomplete degradation removal. To address\nthis issue, we propose a Controlled Differential Disentangled VAE, which\nutilizes Hierarchical Contrastive Disentanglement Learning and an Orthogonal\nGated Projection Module to guide the VAE to actively discard easily recoverable\nbackground information while encoding more difficult-to-recover degraded\ninformation into the latent space. Additionally, we design a Complex Invertible\nMultiscale Fusion Network to handle background features, ensuring their\nconsistency, and utilize a latent space restoration network to transform the\ndegraded latent features, leading to more accurate restoration results.\nExtensive experimental results demonstrate that our method effectively\nalleviates the information loss problem in VAE models while ensuring\ncomputational efficiency, significantly improving the quality of UHD image\nrestoration, and achieves state-of-the-art results in six UHD restoration tasks\nwith only 1M parameters.", "AI": {"title_translation": "解耦重建：通过主动特征解耦和可逆融合实现高质量超高清修复", "tldr": "本文提出一种受控微分解耦VAE，结合分层对比解耦学习和正交门控投影模块，以在潜在空间中主动分离可恢复背景信息和难恢复退化信息，并通过可逆多尺度融合网络和潜在空间修复网络实现高效高质量的超高清图像修复，在六项任务上达到SOTA性能。", "motivation": "超高清（UHD）图像修复面临计算瓶颈和信息损失问题，现有基于变分自编码器（VAE）的方法虽然提高了效率，但退化成分与背景元素耦合，压缩和补偿过程中的信息损失和增益难以控制，导致修复图像细节丢失和退化去除不完全。", "method": "提出一种受控微分解耦VAE（Controlled Differential Disentangled VAE），利用分层对比解耦学习（Hierarchical Contrastive Disentanglement Learning）和正交门控投影模块（Orthogonal Gated Projection Module）指导VAE主动丢弃易恢复的背景信息，同时将难以恢复的退化信息编码到潜在空间。此外，设计一个复杂可逆多尺度融合网络（Complex Invertible Multiscale Fusion Network）处理背景特征以确保一致性，并利用潜在空间修复网络转换退化潜在特征。", "result": "该方法有效缓解了VAE模型中的信息损失问题，同时确保了计算效率，显著提高了超高清图像修复的质量。在六项超高清修复任务中取得了最先进的（state-of-the-art）结果，且模型参数仅为1M。", "conclusion": "本文提出的方法通过主动特征解耦和可逆融合机制，有效解决了超高清图像修复中计算效率与信息损失的矛盾，实现了高质量的图像修复，并在多项任务上展现出卓越的性能。", "translation": "超高分辨率（UHD）图像修复由于其极高的分辨率，常面临计算瓶颈和信息损失问题。现有基于变分自编码器（VAE）的研究通过将图像修复过程从像素空间转移到潜在空间来提高效率。然而，退化图像中退化成分与背景元素固有地耦合在一起，压缩过程中的信息损失和补偿过程中的信息增益都难以控制。这些问题导致修复后的图像常常表现出细节丢失和退化去除不完全。为了解决这个问题，我们提出了一种受控微分解耦VAE（Controlled Differential Disentangled VAE），它利用分层对比解耦学习和正交门控投影模块来引导VAE在编码更多难以恢复的退化信息到潜在空间的同时，主动丢弃易于恢复的背景信息。此外，我们设计了一个复杂可逆多尺度融合网络来处理背景特征，确保其一致性，并利用一个潜在空间修复网络来转换退化潜在特征，从而获得更准确的修复结果。大量的实验结果表明，我们的方法在确保计算效率的同时，有效缓解了VAE模型中的信息损失问题，显著提高了超高清图像修复的质量，并在六项超高清修复任务中以仅1M的参数量达到了最先进的水平。", "summary": "本文针对超高清（UHD）图像修复中存在的计算瓶颈和信息损失问题，提出了一种名为“受控微分解耦VAE”的新方法。该方法通过引入分层对比解耦学习和正交门控投影模块，使VAE能够主动分离并编码难以恢复的退化信息，同时处理易恢复的背景信息。此外，设计了复杂可逆多尺度融合网络和潜在空间修复网络以确保特征一致性和提高修复精度。实验证明，该方法在保持计算效率的同时，显著提升了UHD图像修复质量，并在多个任务上达到了最先进的性能，且模型参数量极小。", "keywords": "超高清修复, 特征解耦, 变分自编码器, 可逆融合, 信息损失", "comments": "该论文的创新点在于提出了“受控微分解耦VAE”框架，通过主动分离背景和退化信息，有效解决了传统VAE在超高清修复中信息损失和耦合问题。其结合分层对比解耦学习和可逆融合网络的设计，既保证了计算效率又提升了修复质量，尤其是在仅1M参数量下达到SOTA性能，显示了其高效性和实用性。"}}
{"id": "2503.12769", "pdf": "https://arxiv.org/pdf/2503.12769", "abs": "https://arxiv.org/abs/2503.12769", "authors": ["Shenghao Fu", "Qize Yang", "Yuan-Ming Li", "Yi-Xing Peng", "Kun-Yu Lin", "Xihan Wei", "Jian-Fang Hu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.", "AI": {"title_translation": "ViSpeak：流媒体视频中的视觉指令反馈", "tldr": "本文提出了一个名为“视觉指令反馈”的新任务，并开发了ViSpeak模型和数据集，以增强流媒体视频中的用户-代理交互。", "motivation": "现有的大型多模态模型主要关注离线视频理解，而流媒体视频理解由于其时间敏感、全模态和交互性特点，对当前模型构成了巨大挑战。", "method": "1. 提出了“视觉指令反馈”的新任务，要求模型识别视觉内容中的指令。2. 定义了七个与视觉模态高度相关的关键子任务。3. 收集了用于训练的ViSpeak-Instruct数据集和用于评估的ViSpeak-Bench数据集。4. 提出了ViSpeak模型，一个最先进的流媒体视频理解LMM，并在ViSpeak-Instruct数据集上进行微调，使其具备视觉指令反馈能力。", "result": "ViSpeak模型在各种流媒体视频理解基准上达到了GPT-4o级别的性能。经过微调后，ViSpeak具备了基本的视觉指令反馈能力，并可作为未来研究的坚实基线。", "conclusion": "本文提出了视觉指令反馈的新任务、相关数据集和ViSpeak模型，为流媒体视频理解和用户-代理交互提供了新的视角和强大的基线。", "translation": "大型多模态模型（LMMs）的最新进展主要集中在离线视频理解。相反，流媒体视频理解由于其时间敏感、全模态和交互性特点，对当前模型构成了巨大挑战。在这项工作中，我们旨在从一个新的角度扩展流媒体视频理解，并提出一个名为“视觉指令反馈”的新任务，其中模型应该感知视觉内容并从中提取指令。例如，当用户向代理挥手时，代理应该识别该手势并开始以欢迎信息进行对话。因此，遵循视觉模态中的指令极大地增强了用户-代理交互。为了促进研究，我们定义了七个与视觉模态高度相关的关键子任务，并收集了用于训练的ViSpeak-Instruct数据集和用于评估的ViSpeak-Bench数据集。此外，我们提出了ViSpeak模型，它是一个最先进的流媒体视频理解LMM，在各种流媒体视频理解基准上达到了GPT-4o级别的性能。在我们的ViSpeak-Instruct数据集上进行微调后，ViSpeak具备了基本的视觉指令反馈能力，可作为未来研究的坚实基线。", "summary": "本文提出了一项名为“视觉指令反馈”的新任务，旨在解决流媒体视频理解中时间敏感和交互性带来的挑战。研究者为此定义了七个关键子任务，并构建了ViSpeak-Instruct训练数据集和ViSpeak-Bench评估数据集。在此基础上，他们开发了ViSpeak模型，这是一个先进的流媒体视频理解大型多模态模型，在相关基准测试中表现出卓越性能，并经过微调后获得了基本的视觉指令反馈能力，为未来的研究奠定了基础。", "keywords": "流媒体视频理解, 视觉指令反馈, 大型多模态模型, 用户-代理交互, ViSpeak", "comments": "本文的创新点在于提出了“视觉指令反馈”这一新颖任务，将流媒体视频理解从被动感知提升到主动交互层面。通过构建专门的数据集和提出高性能的ViSpeak模型，为解决实时、交互式视觉理解的挑战提供了重要的研究方向和SOTA基线，对未来人机交互领域具有重要意义。"}}
{"id": "2503.12772", "pdf": "https://arxiv.org/pdf/2503.12772", "abs": "https://arxiv.org/abs/2503.12772", "authors": ["Sung-Yeon Park", "Can Cui", "Yunsheng Ma", "Ahmadreza Moradipari", "Rohit Gupta", "Kyungtae Han", "Ziran Wang"], "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA.", "AI": {"title_translation": "NuPlanQA：多模态大型语言模型中多视角驾驶场景理解的大规模数据集与基准", "tldr": "本文介绍了NuPlanQA-1M，一个用于多模态大型语言模型（MLLMs）中多视角驾驶场景理解的大规模数据集和基PlanQA-Eval，并提出了BEV-LLM，通过整合鸟瞰图（BEV）特征来提升MLLMs在该领域的性能。", "motivation": "尽管多模态大型语言模型（MLLMs）在多个领域表现出色，但它们对复杂驾驶场景（特别是多视角信息）的理解能力仍有待验证，现有MLLMs面临显著挑战。", "method": "引入了NuPlanQA-Eval，一个用于驾驶场景理解的多视角、多模态评估基准。提出了NuPlanQA-1M，一个包含100万个真实世界视觉问答（VQA）对的大规模数据集，并将其分为9个子任务和3个核心技能。提出了BEV-LLM，将多视角图像中的鸟瞰图（BEV）特征整合到MLLMs中。", "result": "评估结果显示，现有MLLMs在驾驶场景特定的感知和自我中心空间推理方面面临关键挑战。相比之下，BEV-LLM在该领域表现出卓越的适应性，在9个子任务中的6个中优于其他模型。", "conclusion": "鸟瞰图（BEV）特征的整合显著增强了多视角多模态大型语言模型（MLLMs）的性能，但仍需进一步完善以有效适应驾驶场景。NuPlanQA数据集已公开发布以促进后续研究。", "translation": "多模态大型语言模型（MLLMs）的最新进展在各个领域都表现出强大的性能；然而，它们理解驾驶场景的能力仍有待验证。驾驶场景的复杂性，包括多视角信息，对现有MLLMs构成了重大挑战。在本文中，我们介绍了NuPlanQA-Eval，一个用于驾驶场景理解的多视角、多模态评估基准。为了进一步支持多视角驾驶场景的泛化，我们还提出了NuPlanQA-1M，一个包含100万个真实世界视觉问答（VQA）对的大规模数据集。为了对交通场景进行上下文感知分析，我们将数据集分为九个子任务，涵盖三个核心技能：道路环境感知、空间关系识别和自我中心推理。此外，我们提出了BEV-LLM，将多视角图像中的鸟瞰图（BEV）特征整合到MLLMs中。我们的评估结果揭示了现有MLLMs在驾驶场景特定感知和自我中心空间推理方面面临的关键挑战。相比之下，BEV-LLM在该领域表现出卓越的适应性，在九个子任务中的六个中优于其他模型。这些发现强调了BEV整合如何增强多视角MLLMs，同时也指出了需要进一步完善的关键领域，以有效适应驾驶场景。为了促进进一步研究，我们已在https://github.com/sungyeonparkk/NuPlanQA 公开NuPlanQA。", "summary": "本文旨在解决多模态大型语言模型（MLLMs）在理解复杂多视角驾驶场景方面的不足。为此，研究人员引入了NuPlanQA-1M，一个包含100万个VQA对的大规模数据集，以及NuPlanQA-Eval，一个专为多视角驾驶场景理解设计的评估基准。此外，论文提出了一种名为BEV-LLM的新模型，它通过将鸟瞰图（BEV）特征整合到MLLMs中来提升性能。实验结果表明，BEV-LLM在驾驶场景感知和空间推理方面优于现有模型，并在九个子任务中的六个中表现最佳，验证了BEV整合的有效性。该研究强调了BEV特征对于增强MLLMs在驾驶场景中性能的重要性，并指出了未来研究的方向。所有资源均已公开，以促进领域发展。", "keywords": "多模态大型语言模型, 驾驶场景理解, 多视角, 数据集, BEV", "comments": "该论文的创新之处在于创建了一个大规模、多视角驾驶场景理解数据集（NuPlanQA-1M）和专门的评估基准（NuPlanQA-Eval），填补了多模态大型语言模型（MLLMs）在处理复杂驾驶场景方面的空白。提出的BEV-LLM通过整合鸟瞰图（BEV）特征，为提升MLLMs在该领域的性能提供了一个实用的方法，这是对实现更鲁棒的自动驾驶系统的重要贡献。数据集的公开发布也将极大地促进未来的研究。"}}
{"id": "2503.12778", "pdf": "https://arxiv.org/pdf/2503.12778", "abs": "https://arxiv.org/abs/2503.12778", "authors": ["Gul Sheeraz", "Qun Chen", "Liu Feiyu", "Zhou Fengjin MD"], "title": "Adaptive Deep Learning for Multiclass Breast Cancer Classification via Misprediction Risk Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer remains one of the leading causes of cancer-related deaths\nworldwide. Early detection is crucial for improving patient outcomes, yet the\ndiagnostic process is often complex and prone to inconsistencies among\npathologists. Computer-aided diagnostic approaches have significantly enhanced\nbreast cancer detection, particularly in binary classification (benign vs.\nmalignant). However, these methods face challenges in multiclass\nclassification, leading to frequent mispredictions. In this work, we propose a\nnovel adaptive learning approach for multiclass breast cancer classification\nusing H&E-stained histopathology images. First, we introduce a misprediction\nrisk analysis framework that quantifies and ranks the likelihood of an image\nbeing mislabeled by a classifier. This framework leverages an interpretable\nrisk model that requires only a small number of labeled samples for training.\nNext, we present an adaptive learning strategy that fine-tunes classifiers\nbased on the specific characteristics of a given dataset. This approach\nminimizes misprediction risk, allowing the classifier to adapt effectively to\nthe target workload. We evaluate our proposed solutions on real benchmark\ndatasets, demonstrating that our risk analysis framework more accurately\nidentifies mispredictions compared to existing methods. Furthermore, our\nadaptive learning approach significantly improves the performance of\nstate-of-the-art deep neural network classifiers.", "AI": {"title_translation": "经误判风险分析的多类别乳腺癌分类自适应深度学习", "tldr": "本文提出一种新颖的自适应深度学习方法，通过误判风险分析框架和自适应学习策略，显著提高了多类别乳腺癌分类的准确性。", "motivation": "早期乳腺癌诊断过程复杂且病理学家之间存在不一致性，尤其在多类别分类中常出现误判。现有计算机辅助诊断方法在二分类上表现良好，但在多类别分类中面临挑战，导致频繁误判。因此，需要一种新方法来解决多类别乳腺癌分类中的误判问题。", "method": "提出了一种新颖的自适应学习方法，用于使用H&E染色组织病理学图像进行多类别乳腺癌分类。该方法包括：1. 引入误判风险分析框架，量化并排序分类器误标记图像的可能性，并利用一个只需少量标记样本即可训练的可解释风险模型。2. 提出自适应学习策略，根据给定数据集的特定特征对分类器进行微调，以最小化误判风险。", "result": "在真实基准数据集上进行评估，结果表明所提出的风险分析框架比现有方法更准确地识别误判。此外，所提出的自适应学习方法显著提高了最先进的深度神经网络分类器的性能。", "conclusion": "本文提出了一种新颖的自适应深度学习方法，结合了误判风险分析框架和自适应学习策略，有效解决了多类别乳腺癌分类中的误判问题，并显著提升了分类性能。", "translation": "乳腺癌仍然是全球癌症相关死亡的主要原因之一。早期检测对于改善患者预后至关重要，但诊断过程通常复杂且病理学家之间存在不一致性。计算机辅助诊断方法显著增强了乳腺癌检测，特别是在二分类（良性与恶性）方面。然而，这些方法在多类别分类中面临挑战，导致频繁的误判。在这项工作中，我们提出了一种新颖的自适应学习方法，用于使用H&E染色组织病理学图像进行多类别乳腺癌分类。首先，我们引入了一个误判风险分析框架，该框架量化并排序分类器误标记图像的可能性。该框架利用一个可解释的风险模型，仅需要少量标记样本进行训练。接下来，我们提出一个自适应学习策略，根据给定数据集的特定特征对分类器进行微调。这种方法最大限度地降低了误判风险，使分类器能够有效地适应目标工作负载。我们在真实基准数据集上评估了我们提出的解决方案，结果表明我们的风险分析框架比现有方法更准确地识别误判。此外，我们的自适应学习方法显著提高了最先进的深度神经网络分类器的性能。", "summary": "本文针对多类别乳腺癌分类中常见的误判问题，提出了一种基于H&E染色组织病理学图像的自适应深度学习方法。该方法包含一个误判风险分析框架，用于量化和识别图像被误判的可能性，以及一个自适应学习策略，通过微调分类器来最小化误判风险。实验证明，该框架能更准确地识别误判，并且自适应学习方法显著提升了现有深度神经网络分类器的性能。", "keywords": "乳腺癌分类, 深度学习, 误判风险分析, 自适应学习, 组织病理学图像", "comments": "这篇论文通过引入误判风险分析和自适应学习策略，创新性地解决了多类别乳腺癌分类中的关键挑战——误判问题。其方法不仅提高了误判识别的准确性，还显著提升了现有深度学习模型的性能，对于提高乳腺癌诊断的自动化和准确性具有重要意义。该方法对少量标记样本的依赖性也使其具有实际应用潜力。"}}
{"id": "2503.12779", "pdf": "https://arxiv.org/pdf/2503.12779", "abs": "https://arxiv.org/abs/2503.12779", "authors": ["Haoxiao Wang", "Kaichen Zhou", "Binrui Gu", "Zhiyuan Feng", "Weijie Wang", "Peilin Sun", "Yicheng Xiao", "Jianhua Zhang", "Hao Dong"], "title": "TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image", "categories": ["cs.CV"], "comment": "Accepted by ICRA 2025", "summary": "Manipulating transparent objects presents significant challenges due to the\ncomplexities introduced by their reflection and refraction properties, which\nconsiderably hinder the accurate estimation of their 3D shapes. To address\nthese challenges, we propose a single-view RGB-D-based depth completion\nframework, TransDiff, that leverages the Denoising Diffusion Probabilistic\nModels(DDPM) to achieve material-agnostic object grasping in desktop.\nSpecifically, we leverage features extracted from RGB images, including\nsemantic segmentation, edge maps, and normal maps, to condition the depth map\ngeneration process. Our method learns an iterative denoising process that\ntransforms a random depth distribution into a depth map, guided by initially\nrefined depth information, ensuring more accurate depth estimation in scenarios\ninvolving transparent objects. Additionally, we propose a novel training method\nto better align the noisy depth and RGB image features, which are used as\nconditions to refine depth estimation step by step. Finally, we utilized an\nimproved inference process to accelerate the denoising procedure. Through\ncomprehensive experimental validation, we demonstrate that our method\nsignificantly outperforms the baselines in both synthetic and real-world\nbenchmarks with acceptable inference time. The demo of our method can be found\non https://wang-haoxiao.github.io/TransDiff/", "AI": {"title_translation": "TransDiff：基于扩散模型利用单张RGB-D图像操作透明物体的方法", "tldr": "TransDiff提出了一种基于扩散模型的单视图RGB-D深度补全框架，用于解决透明物体操作中3D形状估计困难的问题，实现了材料无关的抓取。", "motivation": "透明物体的反射和折射特性使其3D形状难以准确估计，这给操作透明物体带来了巨大挑战。", "method": "TransDiff是一个单视图RGB-D深度补全框架，利用去噪扩散概率模型(DDPM)。它使用RGB图像特征（语义分割、边缘图、法线图）作为条件来生成深度图，学习一个迭代去噪过程。该方法通过初始细化的深度信息引导随机深度分布转换为深度图。此外，提出了一种新的训练方法来更好地对齐噪声深度和RGB图像特征，并改进了推理过程以加速去噪。", "result": "在合成和真实世界基准测试中，TransDiff显著优于现有基线方法，并具有可接受的推理时间。", "conclusion": "TransDiff通过其创新的深度补全框架和训练方法，有效解决了透明物体操作中深度估计的挑战，实现了材料无关的抓取。", "translation": "操作透明物体带来了巨大的挑战，因为其反射和折射特性使得其3D形状难以准确估计。为了解决这些挑战，我们提出了一个基于单视图RGB-D的深度补全框架TransDiff，该框架利用去噪扩散概率模型（DDPM）在桌面环境中实现与材料无关的物体抓取。具体来说，我们利用从RGB图像中提取的特征，包括语义分割、边缘图和法线图，来调节深度图生成过程。我们的方法学习了一个迭代去噪过程，该过程在初始细化深度信息的引导下，将随机深度分布转换为深度图，确保在涉及透明物体的情况下进行更准确的深度估计。此外，我们提出了一种新颖的训练方法，以更好地对齐作为条件用于逐步细化深度估计的噪声深度和RGB图像特征。最后，我们利用改进的推理过程来加速去噪过程。通过全面的实验验证，我们证明了我们的方法在合成和真实世界基准测试中均显著优于基线方法，且推理时间可接受。我们方法的演示可在https://wang-haoxiao.github.io/TransDiff/上找到。", "summary": "TransDiff是一个创新的单视图RGB-D深度补全框架，专门用于解决透明物体因其复杂的光学特性导致的3D形状估计难题。该方法利用去噪扩散概率模型（DDPM），并结合RGB图像的语义分割、边缘和法线图等特征，迭代地从随机分布中生成精确的深度图。通过新颖的训练和推理加速策略，TransDiff在透明物体抓取任务中表现出色，显著优于现有方法，实现了材料无关的桌面物体操作。", "keywords": "透明物体, 深度估计, 扩散模型, RGB-D, 机器人抓取", "comments": "该论文的创新点在于将扩散模型应用于透明物体的深度估计和操作，这在机器人抓取领域是一个具有挑战性的问题。通过结合RGB图像的多模态特征作为条件，并提出新的训练和推理策略，TransDiff有效地提升了透明物体3D形状估计的准确性，对于推进机器人处理复杂光学特性物体的能力具有重要意义。其材料无关的抓取能力也拓宽了应用场景。"}}
{"id": "2503.12780", "pdf": "https://arxiv.org/pdf/2503.12780", "abs": "https://arxiv.org/abs/2503.12780", "authors": ["Chang Liu", "Bavesh Balaji", "Saad Hossain", "C Thomas", "Kwei-Herng Lai", "Raviteja Vemulapalli", "Alexander Wong", "Sirisha Rambhatla"], "title": "LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML", "68Txx", "I.2.1"], "comment": null, "summary": "Unsupervised domain adaptation for semantic segmentation (DASS) aims to\ntransfer knowledge from a label-rich source domain to a target domain with no\nlabels. Two key approaches in DASS are (1) vision-only approaches using masking\nor multi-resolution crops, and (2) language-based approaches that use generic\nclass-wise prompts informed by target domain (e.g. \"a {snowy} photo of a\n{class}\"). However, the former is susceptible to noisy pseudo-labels that are\nbiased to the source domain. The latter does not fully capture the intricate\nspatial relationships of objects -- key for dense prediction tasks. To this\nend, we propose LangDA. LangDA addresses these challenges by, first, learning\ncontextual relationships between objects via VLM-generated scene descriptions\n(e.g. \"a pedestrian is on the sidewalk, and the street is lined with\nbuildings.\"). Second, LangDA aligns the entire image features with text\nrepresentation of this context-aware scene caption and learns generalized\nrepresentations via text. With this, LangDA sets the new state-of-the-art\nacross three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and\n3.9%.", "AI": {"title_translation": "LangDA: 基于语言构建上下文感知以实现域自适应语义分割", "tldr": "LangDA提出了一种新的方法，通过视觉-语言模型（VLM）生成的场景描述来学习上下文关系，并将图像特征与文本表示对齐，以解决域自适应语义分割中现有方法的局限性，并实现了新的SOTA性能。", "motivation": "现有的无监督域自适应语义分割（DASS）方法存在局限性：纯视觉方法易受源域偏置的噪声伪标签影响；基于语言的方法未能充分捕捉物体间复杂的空间关系。", "method": "LangDA首先通过VLM生成的场景描述来学习物体间的上下文关系，然后将整个图像特征与这些上下文感知的场景描述的文本表示对齐，并通过文本学习泛化表示。", "result": "LangDA在三个DASS基准测试中均达到了新的最先进水平，分别优于现有方法2.6%、1.4%和3.9%。", "conclusion": "LangDA通过引入上下文感知和语言驱动的特征对齐，有效解决了无监督域自适应语义分割中现有方法的挑战，并在多项基准测试中取得了显著的性能提升。", "translation": "无监督域自适应语义分割（DASS）旨在将知识从标签丰富的源域迁移到无标签的目标域。DASS中的两种关键方法是：(1) 使用掩码或多分辨率裁剪的纯视觉方法，以及(2) 使用由目标域信息（例如“一张{下雪的}{类别}照片”）生成的通用类别提示的基于语言的方法。然而，前者容易受到偏向源域的噪声伪标签的影响。后者未能充分捕捉物体间复杂的空间关系——这对于密集预测任务至关重要。为此，我们提出了LangDA。LangDA通过以下方式解决这些挑战：首先，通过VLM生成的场景描述（例如“一个行人在人行道上，街道两旁是建筑物。”）学习物体间的上下文关系。其次，LangDA将整个图像特征与这种上下文感知场景字幕的文本表示对齐，并通过文本学习泛化表示。凭借此，LangDA在三个DASS基准测试中均创造了新的最先进水平，分别优于现有方法2.6%、1.4%和3.9%。", "summary": "LangDA提出了一种新颖的无监督域自适应语义分割（DASS）方法，旨在解决现有纯视觉和基于语言方法的局限性。它通过利用视觉-语言模型（VLM）生成的场景描述来学习物体间的上下文关系，并将图像特征与这些上下文感知的文本表示对齐，从而学习泛化表示。实验结果表明，LangDA在三个DASS基准测试中均取得了显著的性能提升，超越了现有最先进的方法。", "keywords": "域自适应, 语义分割, 上下文感知, 视觉-语言模型, 知识迁移", "comments": "LangDA的创新之处在于其独特地利用VLM生成的场景描述来构建上下文感知，从而有效解决了传统DASS方法中伪标签噪声和空间关系捕捉不足的问题。这种将视觉特征与上下文文本信息深度融合的方法，为密集预测任务的域适应开辟了新路径，具有重要的研究价值和应用潜力。"}}
{"id": "2503.12781", "pdf": "https://arxiv.org/pdf/2503.12781", "abs": "https://arxiv.org/abs/2503.12781", "authors": ["Zhang Jiaxing", "Tang Hao"], "title": "SAM2 for Image and Video Segmentation: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 7 Tables", "summary": "Despite significant advances in deep learning for image and video\nsegmentation, existing models continue to face challenges in cross-domain\nadaptability and generalization. Image and video segmentation are fundamental\ntasks in computer vision with wide-ranging applications in healthcare,\nagriculture, industrial inspection, and autonomous driving. With the advent of\nlarge-scale foundation models, SAM2 - an improved version of SAM (Segment\nAnything Model)has been optimized for segmentation tasks, demonstrating\nenhanced performance in complex scenarios. However, SAM2's adaptability and\nlimitations in specific domains require further investigation. This paper\nsystematically analyzes the application of SAM2 in image and video segmentation\nand evaluates its performance in various fields. We begin by introducing the\nfoundational concepts of image segmentation, categorizing foundation models,\nand exploring the technical characteristics of SAM and SAM2. Subsequently, we\ndelve into SAM2's applications in static image and video segmentation,\nemphasizing its performance in specialized areas such as medical imaging and\nthe challenges of cross-domain adaptability. As part of our research, we\nreviewed over 200 related papers to provide a comprehensive analysis of the\ntopic. Finally, the paper highlights the strengths and weaknesses of SAM2 in\nsegmentation tasks, identifies the technical challenges it faces, and proposes\nfuture development directions. This review provides valuable insights and\npractical recommendations for optimizing and applying SAM2 in real-world\nscenarios.", "AI": {"title_translation": "SAM2在图像和视频分割中的应用：一项综合调查", "tldr": "本文对SAM2在图像和视频分割领域的应用进行了全面调查，分析了其性能、挑战和未来发展方向。", "motivation": "尽管深度学习在图像和视频分割方面取得了显著进展，但现有模型在跨域适应性和泛化能力方面仍面临挑战。SAM2作为SAM的改进版本，在分割任务中表现出增强的性能，但其在特定领域的适应性和局限性需要进一步研究。", "method": "本文系统分析了SAM2在图像和视频分割中的应用，评估了其在各种领域的性能。研究回顾了200多篇相关论文，介绍了图像分割的基础概念，对基础模型进行了分类，并探讨了SAM和SAM2的技术特性。随后深入探讨了SAM2在静态图像和视频分割中的应用，重点关注其在医学成像等专业领域的性能以及跨域适应性的挑战。", "result": "本文强调了SAM2在分割任务中的优点和缺点，识别了其面临的技术挑战。", "conclusion": "本综述为优化和在实际场景中应用SAM2提供了有价值的见解和实用建议。", "translation": "尽管深度学习在图像和视频分割方面取得了显著进展，但现有模型在跨域适应性和泛化能力方面仍面临挑战。图像和视频分割是计算机视觉中的基本任务，在医疗保健、农业、工业检测和自动驾驶等领域有广泛应用。随着大规模基础模型的出现，SAM2——SAM（Segment Anything Model）的改进版本——已针对分割任务进行了优化，在复杂场景中表现出增强的性能。然而，SAM2在特定领域的适应性和局限性需要进一步调查。本文系统分析了SAM2在图像和视频分割中的应用，并评估了其在各个领域的性能。我们首先介绍了图像分割的基本概念，对基础模型进行了分类，并探讨了SAM和SAM2的技术特征。随后，我们深入探讨了SAM2在静态图像和视频分割中的应用，强调了其在医学成像等专业领域的性能以及跨域适应性的挑战。作为我们研究的一部分，我们查阅了200多篇相关论文，以提供对该主题的全面分析。最后，本文强调了SAM2在分割任务中的优点和缺点，识别了其面临的技术挑战，并提出了未来的发展方向。本综述为优化和在实际场景中应用SAM2提供了有价值的见解和实用建议。", "summary": "本文对SAM2在图像和视频分割领域的应用进行了全面调查。文章首先介绍了图像分割的基础概念和基础模型，并详细探讨了SAM和SAM2的技术特性。接着，论文深入分析了SAM2在静态图像和视频分割中的应用，特别关注其在医学成像等专业领域的表现以及跨域适应性的挑战。通过回顾200多篇相关论文，本研究总结了SAM2在分割任务中的优缺点、面临的技术挑战，并提出了未来的发展方向，旨在为SAM2的优化和实际应用提供见解和建议。", "keywords": "SAM2, 图像分割, 视频分割, 综述, 基础模型", "comments": "本综述通过对SAM2在图像和视频分割领域进行全面、系统的分析，填补了当前对SAM2在特定领域适应性和局限性研究的空白。其创新之处在于对超过200篇相关论文的广泛回顾，为SAM2的实际应用和未来发展提供了宝贵的理论和实践指导。"}}
{"id": "2503.12783", "pdf": "https://arxiv.org/pdf/2503.12783", "abs": "https://arxiv.org/abs/2503.12783", "authors": ["Jianan Li", "Huan Chen", "Wangcai Zhao", "Rui Chen", "Tingfa Xu"], "title": "Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by TNNLS", "summary": "Hyperspectral Images (HSIs) are crucial across numerous fields but are\nhindered by the long acquisition times associated with traditional\nspectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system\nmitigates this issue through a compression technique that accelerates the\nacquisition process. However, reconstructing HSIs from compressed data presents\nchallenges due to fixed spatial and spectral resolution constraints. This study\nintroduces a novel method using implicit neural representation for continuous\nhyperspectral image reconstruction. We propose the Mixed Granularity Implicit\nRepresentation (MGIR) framework, which includes a Hierarchical Spectral-Spatial\nImplicit Encoder for efficient multi-scale implicit feature extraction. This is\ncomplemented by a Mixed-Granularity Local Feature Aggregator that adaptively\nintegrates local features across scales, combined with a decoder that merges\ncoordinate information for precise reconstruction. By leveraging implicit\nneural representations, the MGIR framework enables reconstruction at any\ndesired spatial-spectral resolution, significantly enhancing the flexibility\nand adaptability of the CASSI system. Extensive experimental evaluations\nconfirm that our model produces reconstructed images at arbitrary resolutions\nand matches state-of-the-art methods across varying spectral-spatial\ncompression ratios. The code will be released at https://github.com/chh11/MGIR.", "AI": {"title_translation": "混合粒度隐式表示用于连续高光谱压缩重建", "tldr": "本文提出了MGIR框架，一种基于隐式神经表示的方法，用于从压缩数据中连续重建高光谱图像，实现任意空间-光谱分辨率，并达到最先进的性能。", "motivation": "高光谱图像（HSIs）的传统采集时间长，而编码孔径快照光谱成像（CASSI）系统虽然通过压缩加速了采集过程，但从压缩数据重建HSI时，存在固定的空间和光谱分辨率限制。", "method": "本研究提出了一种名为混合粒度隐式表示（MGIR）的新型框架，用于连续高光谱图像重建。MGIR框架包含一个用于高效多尺度隐式特征提取的层次光谱-空间隐式编码器，一个自适应整合跨尺度局部特征的混合粒度局部特征聚合器，以及一个结合坐标信息进行精确重建的解码器。该方法利用隐式神经表示，实现了在任意所需空间-光谱分辨率下进行重建。", "result": "实验评估证实，我们的模型能够以任意分辨率生成重建图像，并且在不同的光谱-空间压缩比下，其性能与最先进的方法相当。", "conclusion": "MGIR框架通过利用隐式神经表示，实现了任意所需空间-光谱分辨率的重建，显著增强了CASSI系统的灵活性和适应性。", "translation": "高光谱图像（HSIs）在众多领域至关重要，但传统光谱仪相关的长采集时间限制了其应用。编码孔径快照光谱成像（CASSI）系统通过一种压缩技术缓解了这个问题，加速了采集过程。然而，由于固定的空间和光谱分辨率限制，从压缩数据重建HSIs带来了挑战。本研究引入了一种使用隐式神经表示的新方法，用于连续高光谱图像重建。我们提出了混合粒度隐式表示（MGIR）框架，其中包括一个用于高效多尺度隐式特征提取的层次光谱-空间隐式编码器。这辅以一个混合粒度局部特征聚合器，该聚合器自适应地整合跨尺度的局部特征，并结合一个解码器来合并坐标信息以进行精确重建。通过利用隐式神经表示，MGIR框架能够在任何所需的空间-光谱分辨率下进行重建，显著增强了CASSI系统的灵活性和适应性。广泛的实验评估证实，我们的模型能够以任意分辨率生成重建图像，并且在不同的光谱-空间压缩比下与最先进的方法相匹配。代码将在https://github.com/chh11/MGIR发布。", "summary": "该论文旨在解决从CASSI系统压缩数据重建高光谱图像时，固定空间和光谱分辨率的限制。为此，作者提出了混合粒度隐式表示（MGIR）框架，该框架利用隐式神经表示实现连续高光谱图像重建。MGIR包含一个层次光谱-空间隐式编码器用于特征提取和一个混合粒度局部特征聚合器用于特征整合，结合解码器实现精确重建。其核心优势在于能够以任意期望的空间-光谱分辨率进行重建，显著提升了CASSI系统的灵活性。实验结果验证了该模型在任意分辨率重建方面的能力，并达到了与现有先进方法相当的性能。", "keywords": "高光谱成像, 压缩重建, 隐式神经表示, 混合粒度, CASSI", "comments": "本文的创新点在于首次将隐式神经表示引入到高光谱压缩重建领域，从而突破了传统方法在空间和光谱分辨率上的固定限制，实现了连续任意分辨率的重建，极大地增强了高光谱成像系统的灵活性和实用性。这一方法对于需要高分辨率和灵活性的高光谱应用具有重要意义。"}}
{"id": "2503.12786", "pdf": "https://arxiv.org/pdf/2503.12786", "abs": "https://arxiv.org/abs/2503.12786", "authors": ["Peirong Zhang", "Yuliang Liu", "Songxuan Lai", "Hongliang Li", "Lianwen Jin"], "title": "Privacy-Preserving Biometric Verification with Handwritten Random Digit String", "categories": ["cs.CV"], "comment": null, "summary": "Handwriting verification has stood as a steadfast identity authentication\nmethod for decades. However, this technique risks potential privacy breaches\ndue to the inclusion of personal information in handwritten biometrics such as\nsignatures. To address this concern, we propose using the Random Digit String\n(RDS) for privacy-preserving handwriting verification. This approach allows\nusers to authenticate themselves by writing an arbitrary digit sequence,\neffectively ensuring privacy protection. To evaluate the effectiveness of RDS,\nwe construct a new HRDS4BV dataset composed of online naturally handwritten\nRDS. Unlike conventional handwriting, RDS encompasses unconstrained and\nvariable content, posing significant challenges for modeling consistent\npersonal writing style. To surmount this, we propose the Pattern Attentive\nVErification Network (PAVENet), along with a Discriminative Pattern Mining\n(DPM) module. DPM adaptively enhances the recognition of consistent and\ndiscriminative writing patterns, thus refining handwriting style\nrepresentation. Through comprehensive evaluations, we scrutinize the\napplicability of online RDS verification and showcase a pronounced\noutperformance of our model over existing methods. Furthermore, we discover a\nnoteworthy forgery phenomenon that deviates from prior findings and discuss its\npositive impact in countering malicious impostor attacks. Substantially, our\nwork underscores the feasibility of privacy-preserving biometric verification\nand propels the prospects of its broader acceptance and application.", "AI": {"title_translation": "手写随机数字串的隐私保护生物识别验证", "tldr": "本文提出了一种使用随机数字串进行隐私保护手写验证的方法，并引入了PAVENet模型和DPM模块来处理手写风格建模的挑战，实验证明其优于现有方法。", "motivation": "传统手写验证技术（如签名）存在个人信息泄露的隐私风险，因此需要一种新的方法来解决这一担忧，实现隐私保护的生物识别验证。", "method": "本文提出使用随机数字串（RDS）进行隐私保护手写验证。为了应对RDS中非受限和可变内容带来的挑战，研究者构建了一个新的HRDS4BV在线自然手写RDS数据集。同时，提出了模式注意力验证网络（PAVENet）和判别模式挖掘（DPM）模块。DPM模块旨在自适应地增强对一致且具有判别性的书写模式的识别，从而优化手写风格表示。", "result": "通过全面评估，验证了在线RDS验证的适用性，并展示了所提出模型显著优于现有方法。此外，发现了一种不同于以往发现的显著伪造现象，并讨论了其在对抗恶意冒充攻击中的积极影响。", "conclusion": "本文的工作强调了隐私保护生物识别验证的可行性，并推动了其更广泛接受和应用的前景。", "translation": "手写验证几十年来一直是一种坚实的身份认证方法。然而，由于手写生物识别信息（如签名）中包含个人信息，这项技术存在潜在的隐私泄露风险。为了解决这一问题，我们提出了使用随机数字串（RDS）进行隐私保护的手写验证。这种方法允许用户通过书写任意数字序列来验证自己，有效确保了隐私保护。为了评估RDS的有效性，我们构建了一个由在线自然手写RDS组成的新HRDS4BV数据集。与传统手写不同，RDS包含非受限和可变的内容，这对手写风格的一致性建模带来了重大挑战。为了克服这一点，我们提出了模式注意力验证网络（PAVENet）以及判别模式挖掘（DPM）模块。DPM自适应地增强了对一致且具有判别性的书写模式的识别，从而改进了手写风格表示。通过全面的评估，我们审视了在线RDS验证的适用性，并展示了我们的模型明显优于现有方法。此外，我们发现了一种值得注意的伪造现象，它偏离了先前的发现，并讨论了其在对抗恶意冒充攻击中的积极影响。从根本上说，我们的工作强调了隐私保护生物识别验证的可行性，并推动了其更广泛接受和应用的前景。", "summary": "本研究旨在解决传统手写生物识别验证（如签名）中存在的隐私泄露风险。为此，论文提出了一种基于随机数字串（RDS）的隐私保护手写验证方法，允许用户通过书写任意数字序列进行身份验证。为支持该方法，论文构建了新的HRDS4BV数据集，并针对RDS中非受限内容的挑战，提出了模式注意力验证网络（PAVENet）及其判别模式挖掘（DPM）模块，以提升手写风格的表示和识别。实验结果表明，所提出的模型在在线RDS验证中表现出色，显著优于现有方法，并揭示了新的伪造现象及其在防御恶意攻击中的潜在益处，证实了隐私保护生物识别验证的实用性和前景。", "keywords": "隐私保护, 生物识别验证, 手写识别, 随机数字串, PAVENet", "comments": "该论文通过引入随机数字串（RDS）和创新的PAVENet及DPM模块，有效解决了传统手写生物识别验证中的隐私泄露问题，具有重要的理论和实践意义。其提出的新数据集和对伪造现象的发现，也为未来研究提供了宝贵的资源和视角。"}}
{"id": "2503.12797", "pdf": "https://arxiv.org/pdf/2503.12797", "abs": "https://arxiv.org/abs/2503.12797", "authors": ["Xinyu Ma", "Ziyang Ding", "Zhicong Luo", "Chi Chen", "Zonghao Guo", "Derek F. Wong", "Xiaoyi Feng", "Maosong Sun"], "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.", "AI": {"title_translation": "DeepPerception：在多模态大语言模型中推进R1类认知视觉感知以实现知识密集型视觉定位", "tldr": "DeepPerception通过引入知识密集型视觉定位（KVG）任务，并采用数据合成和两阶段训练框架，显著提升了多模态大语言模型（MLLMs）的认知视觉感知能力，使其在细粒度视觉辨别和知识整合方面表现出接近人类专家的水平。", "motivation": "当前的多模态大语言模型（MLLMs）在细粒度视觉辨别和将推理整合到视觉感知方面能力不足，无法像人类专家那样利用领域知识来精炼感知特征并进行深度分析。为了弥补这一差距，本文提出了知识密集型视觉定位（KVG）任务。", "method": "本文提出了DeepPerception模型，这是一种增强了认知视觉感知能力的多模态大语言模型，旨在解决知识密集型视觉定位（KVG）任务的挑战。其方法包括：1) 一个自动数据合成管道，用于生成高质量、知识对齐的训练样本；2) 一个两阶段训练框架，结合了用于认知推理支架的监督微调和用于优化感知-认知协同作用的强化学习。此外，为了评估性能，本文引入了KVG-Bench，一个包含10个领域、1.3K个手动策划测试用例的综合数据集。", "result": "实验结果表明，DeepPerception显著优于直接微调方法，在KVG-Bench上实现了+8.08%的准确率提升，并且比基线方法表现出+4.60%的卓越跨领域泛化能力。", "conclusion": "研究结果强调了将认知过程整合到多模态大语言模型（MLLMs）中对于实现类人视觉感知的重要性，并为多模态推理研究开辟了新的方向。", "translation": "人类专家擅长利用领域知识来精炼感知特征，从而进行细粒度视觉辨别，而当前的多模态大语言模型（MLLMs）在这方面仍不成熟。尽管拥有海量的专家级知识，MLLMs在将推理整合到视觉感知方面仍面临困难，通常直接生成响应而缺乏更深层次的分析。为了弥补这一差距，我们引入了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，需要细粒度感知和领域特定知识的整合。为了应对KVG的挑战，我们提出了DeepPerception，一个增强了认知视觉感知能力的多模态大语言模型。我们的方法包括：(1) 一个自动数据合成管道，用于生成高质量、知识对齐的训练样本；(2) 一个两阶段训练框架，结合了用于认知推理支架的监督微调和用于优化感知-认知协同作用的强化学习。为了评估性能，我们引入了KVG-Bench，一个涵盖10个领域、包含1.3K个手动策划测试用例的综合数据集。实验结果表明，DeepPerception显著优于直接微调方法，在KVG-Bench上实现了+8.08%的准确率提升，并且比基线方法表现出+4.60%的卓越跨领域泛化能力。我们的研究结果强调了将认知过程整合到多模态大语言模型中对于实现类人视觉感知的重要性，并为多模态推理研究开辟了新的方向。数据、代码和模型已在https://github.com/thunlp/DeepPerception发布。", "summary": "本文针对当前多模态大语言模型（MLLMs）在细粒度视觉辨别和推理整合方面的不足，提出了知识密集型视觉定位（KVG）任务。为解决KVG的挑战，作者引入了DeepPerception模型，该模型通过自动数据合成管道和结合监督微调与强化学习的两阶段训练框架，显著提升了MLLMs的认知视觉感知能力。通过在新建的KVG-Bench数据集上进行测试，DeepPerception在准确性和跨领域泛化能力上均表现出显著优势，证明了将认知过程融入MLLMs对于实现类人视觉感知的重要性。", "keywords": "多模态大语言模型, 视觉定位, 认知感知, 知识整合, DeepPerception", "comments": "本文的创新点在于提出了知识密集型视觉定位（KVG）这一新颖任务，并为解决该任务设计了DeepPerception模型，该模型结合了数据合成和两阶段训练框架，有效地提升了MLLMs的认知视觉感知能力。同时，引入KVG-Bench作为新的基准数据集，为未来的研究提供了标准。其重要性在于推动了MLLMs在细粒度视觉感知和知识推理整合方面的发展，使其更接近人类专家的表现。"}}
{"id": "2503.12799", "pdf": "https://arxiv.org/pdf/2503.12799", "abs": "https://arxiv.org/abs/2503.12799", "authors": ["Qiong Wu", "Xiangcong Yang", "Yiyi Zhou", "Chenxin Fang", "Baiyang Song", "Xiaoshuai Sun", "Rongrong Ji"], "title": "Grounded Chain-of-Thought for Multimodal Large Language Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC.", "AI": {"title_translation": "多模态大型语言模型的接地式思维链", "tldr": "本文提出接地式思维链（GCoT）任务和MM-GCoT数据集以解决多模态大语言模型（MLLMs）的视觉幻觉问题。研究发现现有MLLMs存在严重幻觉且与模型大小无关，而GCoT能有效提升模型一致性并泛化到其他多模态任务。", "motivation": "现有多模态大型语言模型（MLLMs）容易产生视觉幻觉，这严重阻碍了它们在可信应用中的部署。", "method": "研究提出了一种新的学习任务——接地式思维链（GCoT），旨在帮助MLLMs逐步识别并定位相关视觉线索，以接地坐标为基础预测答案。为此，构建了一个包含24,022个GCoT示例的MM-GCoT数据集，并引入了一个全面的、包含答案准确性、接地准确性和答案-接地一致性指标的一致性评估系统。", "result": "实验表明：i. 大多数MLLMs在一致性评估中表现不佳，存在明显的视觉幻觉；ii. 视觉幻觉与参数大小和通用多模态性能没有直接关系，即更大更强的MLLM受此问题的影响不小。", "conclusion": "所提出的MM-GCoT数据集能够有效培养现有MLLMs的GCoT能力，显著减少不一致回答，并且GCoT可以泛化到开放世界问答和推荐等现有多模态任务。", "translation": "尽管取得了巨大进展，但现有的多模态大型语言模型（MLLMs）容易产生视觉幻觉，这极大地阻碍了它们的可信应用。在本文中，我们从视觉空间推理的角度研究了这个问题，并为MLLMs提出了一项新的学习任务，称之为接地式思维链（GCoT）。与最近专注于视觉知识推理的视觉CoT研究不同，GCoT致力于帮助MLLMs逐步识别并定位相关的视觉线索，从而以接地坐标作为直观基础来预测正确答案。为了促进这项任务，我们还精心设计并构建了一个名为多模态接地式思维链（MM-GCoT）的数据集，该数据集包含5,033张图像的24,022个GCoT示例。此外，还引入了一个全面的一致性评估系统，包括答案准确性、接地准确性和答案-接地一致性等指标。我们进一步设计并对12个先进的MLLMs进行了一系列实验，揭示了一些显著发现：i. 大多数MLLMs在一致性评估中表现不佳，表明存在明显的视觉幻觉；ii. 视觉幻觉与参数大小和通用多模态性能没有直接关系，即更大更强的MLLM受此问题的影响不小。最后，我们还证明了所提出的数据集可以帮助现有MLLMs很好地培养其GCoT能力，并显著减少不一致的回答。此外，它们的GCoT还可以泛化到现有的多模态任务，例如开放世界问答和推荐（REC）。", "summary": "本文针对多模态大型语言模型（MLLMs）普遍存在的视觉幻觉问题，提出了一种新的学习任务——接地式思维链（GCoT）。GCoT通过引导MLLMs逐步识别并定位视觉线索，以接地坐标为基础进行推理。为支持此任务，研究者构建了MM-GCoT数据集和一套综合一致性评估系统。实验发现，现有MLLMs存在严重视觉幻觉，且此问题与模型大小无关。研究同时证明，所提数据集能有效提升MLLMs的GCoT能力，减少不一致回答，并可泛化到其他多模态任务。", "keywords": "多模态大型语言模型, 视觉幻觉, 接地式思维链, MM-GCoT, 一致性评估", "comments": "这项工作创新性地提出了“接地式思维链”概念，将视觉空间推理引入多模态大模型，以直接解决视觉幻觉这一核心可信度问题。通过构建专门的数据集和评估体系，为MLLMs的接地能力提供了量化和提升的途径。特别指出视觉幻觉与模型大小无关的发现具有重要意义，这表明简单的模型扩展并不能解决根本问题，需要更深层次的机制改进。"}}
{"id": "2503.12800", "pdf": "https://arxiv.org/pdf/2503.12800", "abs": "https://arxiv.org/abs/2503.12800", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Chunping Qiu", "Luoxi Jing", "Mengzhu Wang"], "title": "Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "With fully leveraging the value of unlabeled data, semi-supervised medical\nimage segmentation algorithms significantly reduces the limitation of limited\nlabeled data, achieving a significant improvement in accuracy. However, the\ndistributional shift between labeled and unlabeled data weakens the utilization\nof information from the labeled data. To alleviate the problem, we propose a\ngraph network feature alignment method based on pairwise similarity\nregularization (PaSR) for semi-supervised medical image segmentation. PaSR\naligns the graph structure of images in different domains by maintaining\nconsistency in the pairwise structural similarity of feature graphs between the\ntarget domain and the source domain, reducing distribution shift issues in\nmedical images. Meanwhile, further improving the accuracy of pseudo-labels in\nthe teacher network by aligning graph clustering information to enhance the\nsemi-supervised efficiency of the model. The experimental part was verified on\nthree medical image segmentation benchmark datasets, with results showing\nimprovements over advanced methods in various metrics. On the ACDC dataset, it\nachieved an average improvement of more than 10.66%.", "AI": {"title_translation": "成对相似性正则化用于半监督图医学图像分割", "tldr": "本文提出了一种基于成对相似性正则化（PaSR）的图网络特征对齐方法，用于半监督医学图像分割，通过对齐不同域图像的特征图结构和图聚类信息，有效减少了分布偏移，并在多个医学图像数据集上取得了显著的性能提升。", "motivation": "半监督医学图像分割算法在利用未标记数据时，会受到标记数据和未标记数据之间分布偏移的影响，从而削弱了标记数据信息的利用效率。", "method": "本文提出了一种基于成对相似性正则化（PaSR）的图网络特征对齐方法。PaSR通过保持目标域和源域之间特征图的成对结构相似性一致性来对齐不同域图像的图结构，以减少医学图像中的分布偏移问题。同时，该方法通过对齐图聚类信息来提高教师网络中伪标签的准确性，从而增强模型的半监督效率。", "result": "实验在三个医学图像分割基准数据集上进行了验证，结果显示该方法在各种指标上均优于现有先进方法。在ACDC数据集上，它实现了超过10.66%的平均性能提升。", "conclusion": "本文提出的基于成对相似性正则化（PaSR）的图网络特征对齐方法，有效解决了半监督医学图像分割中的数据分布偏移问题，显著提升了分割精度，并在多个基准数据集上展现出优越的性能。", "translation": "充分利用未标记数据的价值，半监督医学图像分割算法显著减少了有限标记数据的限制，实现了精度的显著提高。然而，标记数据和未标记数据之间的分布偏移削弱了标记数据信息的利用。为了缓解这个问题，我们提出了一种基于成对相似性正则化（PaSR）的图网络特征对齐方法，用于半监督医学图像分割。PaSR通过保持目标域和源域之间特征图的成对结构相似性一致性来对齐不同域图像的图结构，从而减少医学图像中的分布偏移问题。同时，通过对齐图聚类信息进一步提高教师网络中伪标签的准确性，从而提高模型的半监督效率。实验部分在三个医学图像分割基准数据集上进行了验证，结果显示在各种指标上优于现有先进方法。在ACDC数据集上，它实现了超过10.66%的平均提升。", "summary": "本文提出了一种名为PaSR的图网络特征对齐方法，旨在解决半监督医学图像分割中标记数据与未标记数据之间的分布偏移问题。该方法通过在目标域和源域之间保持特征图的成对结构相似性一致性来对齐图像的图结构，并进一步通过对齐图聚类信息来提高教师网络伪标签的准确性，从而提升模型的半监督效率。实验结果表明，该方法在多个医学图像分割基准数据集上均优于现有先进方法，并在ACDC数据集上取得了超过10.66%的显著平均性能提升。", "keywords": "半监督学习, 医学图像分割, 图网络, 成对相似性正则化, 分布偏移", "comments": "该论文提出了一种创新的方法，通过结合图网络和成对相似性正则化来解决半监督医学图像分割中的数据分布偏移问题。其核心在于通过特征图结构和图聚类信息的对齐来增强模型利用未标记数据的能力，这对于医学图像领域有限标记数据的情况具有重要意义。实验结果的显著提升也证明了其有效性。"}}
{"id": "2503.12820", "pdf": "https://arxiv.org/pdf/2503.12820", "abs": "https://arxiv.org/abs/2503.12820", "authors": ["Kailin Li", "Zhenxin Li", "Shiyi Lan", "Yuan Xie", "Zhizhong Zhang", "Jiayi Liu", "Zuxuan Wu", "Zhiding Yu", "Jose M. Alvarez"], "title": "Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Hydra-MDP++ introduces a novel teacher-student knowledge distillation\nframework with a multi-head decoder that learns from human demonstrations and\nrule-based experts. Using a lightweight ResNet-34 network without complex\ncomponents, the framework incorporates expanded evaluation metrics, including\ntraffic light compliance (TL), lane-keeping ability (LK), and extended comfort\n(EC) to address unsafe behaviors not captured by traditional NAVSIM-derived\nteachers. Like other end-to-end autonomous driving approaches, \\hydra processes\nraw images directly without relying on privileged perception signals.\nHydra-MDP++ achieves state-of-the-art performance by integrating these\ncomponents with a 91.0% drive score on NAVSIM through scaling to a V2-99 image\nencoder, demonstrating its effectiveness in handling diverse driving scenarios\nwhile maintaining computational efficiency.", "AI": {"title_translation": "Hydra-MDP++：通过专家引导的Hydra蒸馏推进端到端驾驶", "tldr": "Hydra-MDP++是一个新的端到端自动驾驶框架，它使用多头解码器和师生知识蒸馏，从人类和规则专家学习，并通过轻量级网络和扩展评估指标实现了最先进的性能。", "motivation": "传统的NAVSIM衍生教师未能捕捉到不安全的行为。该研究旨在解决这一问题，并推进端到端自动驾驶的性能。", "method": "Hydra-MDP++引入了一个新颖的师生知识蒸馏框架，具有多头解码器，该解码器从人类演示和基于规则的专家那里学习。它使用轻量级ResNet-34网络，并结合了扩展的评估指标，包括交通灯合规性（TL）、车道保持能力（LK）和扩展舒适度（EC）。该方法直接处理原始图像，不依赖于特权感知信号。", "result": "Hydra-MDP++通过扩展到V2-99图像编码器，在NAVSIM上实现了91.0%的驾驶分数，达到了最先进的性能。", "conclusion": "Hydra-MDP++框架通过结合专家引导的蒸馏、多头解码器和扩展评估指标，有效提高了端到端自动驾驶的性能，同时保持了计算效率。", "translation": "Hydra-MDP++引入了一种新颖的师生知识蒸馏框架，该框架具有多头解码器，可从人类演示和基于规则的专家那里学习。该框架使用轻量级ResNet-34网络，不含复杂组件，并纳入了扩展的评估指标，包括交通灯合规性（TL）、车道保持能力（TL）和扩展舒适度（EC），以解决传统NAVSIM衍生教师未捕捉到的不安全行为。与其他端到端自动驾驶方法一样，Hydra直接处理原始图像，不依赖特权感知信号。Hydra-MDP++通过将这些组件与扩展到V2-99图像编码器相结合，在NAVSIM上实现了91.0%的驾驶分数，展示了其在处理多样化驾驶场景同时保持计算效率方面的有效性。", "summary": "Hydra-MDP++是一种创新的端到端自动驾驶框架，它利用师生知识蒸馏和多头解码器从人类及规则专家数据中学习。该系统采用轻量级ResNet-34网络，并引入了交通灯合规性、车道保持和扩展舒适度等新的评估指标，以解决传统方法中安全行为捕捉不足的问题。Hydra-MDP++直接处理原始图像，无需特权感知信号，并通过扩展图像编码器，在NAVSIM上取得了91.0%的驾驶分数，展现了其在复杂驾驶场景中的高效和先进性能。", "keywords": "端到端驾驶, 知识蒸馏, 多头解码器, 自动驾驶, ResNet-34", "comments": "Hydra-MDP++的创新之处在于其结合了师生知识蒸馏与多头解码器，并从人类和规则专家数据中学习，这有助于弥补单一专家数据源的不足。同时，引入扩展评估指标（TL, LK, EC）对于提升自动驾驶的安全性至关重要，因为它关注了传统指标可能忽略的不安全行为。该方法在保持计算效率的同时实现了最先进的性能，表明其在实际部署中具有潜力。"}}
{"id": "2503.12821", "pdf": "https://arxiv.org/pdf/2503.12821", "abs": "https://arxiv.org/abs/2503.12821", "authors": ["Mingyang Song", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an $\\textbf{A}$daptive\n$\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which\nconsists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$)\nand $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.", "AI": {"title_translation": "从头到尾：通过自适应数据校准实现大型视觉-语言模型中的平衡表示", "tldr": "本文提出了一种名为ADR的自适应数据校准框架，通过数据再平衡和数据合成来解决大型视觉-语言模型（LVLMs）训练数据中的长尾问题，使LLaVA 1.5的平均性能提升了4.36%。", "motivation": "尽管大型视觉-语言模型（LVLMs）在结合视觉理解和语言生成方面取得了显著进展，但其训练数据仍存在长尾问题，即数据分布高度不平衡。以往的工作主要集中在传统的VLM架构和特定任务上，而对LVLM（如LLaVA）和更通用任务（如视觉问答和视觉推理）的长尾问题探索不足。本文旨在深入分析并解决LVLMs中头部概念过表征和尾部概念欠表征的问题。", "method": "本文提出了一种名为自适应数据优化框架（ADR）的方法，该框架包含两个阶段：数据再平衡（DR）和数据合成（DS）。在DR阶段，根据实体分布自适应地重新平衡冗余数据。在DS阶段，利用去噪扩散概率模型（DDPMs）和稀缺图像来补充欠表征部分。", "result": "通过在十一个基准测试上的全面评估，我们提出的ADR框架有效缓解了训练数据中的长尾问题，使LLaVA 1.5的平均性能相对提升了4.36%，且没有增加训练数据量。", "conclusion": "本文提出的自适应数据优化框架（ADR）通过数据再平衡和数据合成有效解决了大型视觉-语言模型（LVLMs）训练数据中的长尾问题，显著提升了模型在通用任务上的性能，且无需增加训练数据量，为LVLM的平衡表示提供了有效途径。", "translation": "大型视觉-语言模型（LVLMs）在结合视觉理解和语言生成方面取得了显著进展。尽管取得了成功，但LVLMs的训练数据仍然存在长尾（LT）问题，即数据分布高度不平衡。以往的工作主要集中在传统的VLM架构，即CLIP或ViT，以及识别和分类等特定任务。然而，对LVLM（例如LLaVA）和更通用任务（例如视觉问答和视觉推理）的探索仍然不足。在本文中，我们首先对LVLMs中的长尾问题进行了深入分析，并确定了两个核心原因：头部概念的过度表征和尾部概念的欠表征。基于上述观察，我们提出了一种自适应数据优化框架（ADR），该框架由两个阶段组成：数据再平衡（DR）和数据合成（DS）。在DR阶段，我们根据实体分布自适应地重新平衡冗余数据，而在DS阶段，我们利用去噪扩散概率模型（DDPMs）和稀缺图像来补充欠表征部分。通过在十一个基准测试上的全面评估，我们提出的ADR有效缓解了训练数据中的长尾问题，使LLaVA 1.5的平均性能相对提升了4.36%，且没有增加训练数据量。", "summary": "本文针对大型视觉-语言模型（LVLMs）训练数据中存在的长尾问题，提出了一种名为“自适应数据优化框架”（ADR）的新方法。该框架通过深入分析长尾问题的根源（头部概念过表征和尾部概念欠表征），设计了数据再平衡（DR）和数据合成（DS）两个阶段。DR阶段负责自适应地平衡冗余数据，而DS阶段则利用DDPMs补充稀缺的尾部数据。实验结果表明，ADR框架在十一个基准测试上有效缓解了长尾问题，使LLaVA 1.5的平均性能相对提升了4.36%，且无需增加训练数据体积，为LVLMs的平衡表示提供了有效方案。", "keywords": "长尾问题, 大型视觉-语言模型, 数据校准, 数据再平衡, 数据合成", "comments": "该论文的创新点在于针对大型视觉-语言模型（LVLMs）的通用任务，首次深入分析并提出了解决长尾问题的系统性框架ADR。通过结合数据再平衡和基于DDPM的数据合成，该方法为提升模型在不平衡数据下的性能提供了一条新颖且高效的途径。其在不增加训练数据量的情况下实现显著性能提升（4.36%）的成果，表明了该方法的实用性和重要性，对于推动LVLMs在现实世界应用中的鲁棒性具有重要意义。"}}
{"id": "2503.12827", "pdf": "https://arxiv.org/pdf/2503.12827", "abs": "https://arxiv.org/abs/2503.12827", "authors": ["Md Farhamdur Reza", "Richeng Jin", "Tianfu Wu", "Huaiyu Dai"], "title": "GSBAK$^K$: $top$-$K$ Geometric Score-based Black-box Attack", "categories": ["cs.CV"], "comment": "This article has been accepted for publication at ICLR 2025", "summary": "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBAK$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBAK$^K$ can be used to attack against classifiers with\n$top$-$K$ multi-label learning. Extensive experimental results on ImageNet and\nPASCAL VOC datasets validate the effectiveness of GSBAK$^K$ in crafting\n$top$-$K$ adversarial examples.", "AI": {"title_translation": "GSBAK$^K$: 基于几何分数的黑盒Top-K攻击", "tldr": "提出GSBAK$^K$，一种新的基于分数的黑盒攻击方法，能在$top$-$K$设置下有效生成对抗样本，并解决多标签分类器的漏洞问题。", "motivation": "现有基于分数的对抗攻击主要集中于针对$top$-1单标签分类器，其攻击成功率和查询效率不尽如人意，特别是在小扰动要求下；此外，多标签学习分类器的漏洞尚未得到充分研究。", "method": "本文提出了一种名为GSBAK$^K$（基于几何分数的黑盒攻击）的全面无代理基于分数的攻击方法。该方法引入了新颖的基于梯度的方法来寻找良好的初始边界点，并采用迭代方法在决策边界上利用新型梯度估计技术，特别是在$top$-$K$设置中有效利用决策边界的几何形状。此外，GSBAK$^K$也可用于攻击具有$top$-$K$多标签学习的分类器。", "result": "在ImageNet和PASCAL VOC数据集上的大量实验结果验证了GSBAK$^K$在生成$top$-$K$对抗样本方面的有效性。", "conclusion": "GSBAK$^K$能够有效地在$top$-$K$设置下生成对抗样本，并成功攻击单标签和多标签分类器，解决了现有方法在攻击成功率、查询效率和多标签漏洞研究方面的不足。", "translation": "现有基于分数的对抗攻击主要集中于针对单标签分类器的$top$-1对抗样本的生成。它们的攻击成功率和查询效率往往不尽如人意，特别是在小扰动要求下；此外，多标签学习分类器的漏洞尚未得到研究。在本文中，我们提出了一种全面的无代理基于分数的攻击方法，名为GSBAK$^K$（基于几何分数的黑盒攻击），以在侵略性的$top$-$K$设置下生成对抗样本，用于无目标和有目标攻击，其目标是改变目标分类器的$top$-$K$预测。我们引入了新颖的基于梯度的方法来寻找一个良好的初始边界点进行攻击。我们的迭代方法在决策边界上采用了新颖的梯度估计技术，特别是在$top$-$K$设置中有效，以有效地利用决策边界的几何形状。此外，GSBAK$^K$可用于攻击具有$top$-$K$多标签学习的分类器。在ImageNet和PASCAL VOC数据集上进行的大量实验结果验证了GSBAK$^K$在生成$top$-$K$对抗样本方面的有效性。", "summary": "本文提出了一种名为GSBAK$^K$的全面无代理基于分数的黑盒攻击方法，旨在解决现有攻击在$top$-1设置下成功率和效率不足以及未研究多标签分类器漏洞的问题。GSBAK$^K$通过引入新颖的梯度方法和梯度估计技术，有效利用决策边界几何形状，在$top$-$K$设置下生成对抗样本，并可用于攻击多标签学习分类器。实验结果验证了其在ImageNet和PASCAL VOC数据集上的有效性。", "keywords": "对抗攻击, 黑盒攻击, $top$-$K$攻击, 几何分数, 多标签学习", "comments": "这篇论文的创新点在于将黑盒对抗攻击从传统的$top$-1设置扩展到更具挑战性的$top$-$K$设置，并首次系统地研究了多标签分类器的漏洞。其利用决策边界几何形状的新颖梯度估计方法提高了攻击的效率和成功率，具有重要的理论和实践意义。"}}
{"id": "2503.12834", "pdf": "https://arxiv.org/pdf/2503.12834", "abs": "https://arxiv.org/abs/2503.12834", "authors": ["Seunggwan Lee", "Hwanhee Jung", "Byoungsoo Koh", "Qixing Huang", "Sangho Yoon", "Sangpil Kim"], "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 18 figures", "summary": "A fundamental challenge in conditional 3D shape generation is to minimize the\ninformation loss and maximize the intention of user input. Existing approaches\nhave predominantly focused on two types of isolated conditional signals, i.e.,\nuser sketches and text descriptions, each of which does not offer flexible\ncontrol of the generated shape. In this paper, we introduce PASTA, the flexible\napproach that seamlessly integrates a user sketch and a text description for 3D\nshape generation. The key idea is to use text embeddings from a vision-language\nmodel to enrich the semantic representation of sketches. Specifically, these\ntext-derived priors specify the part components of the object, compensating for\nmissing visual cues from ambiguous sketches. In addition, we introduce ISG-Net\nwhich employs two types of graph convolutional networks: IndivGCN, which\nprocesses fine-grained details, and PartGCN, which aggregates these details\ninto parts and refines the structure of objects. Extensive experiments\ndemonstrate that PASTA outperforms existing methods in part-level editing and\nachieves state-of-the-art results in sketch-to-3D shape generation.", "AI": {"title_translation": "PASTA：基于部件感知和文本对齐先验的草图到三维形状生成", "tldr": "PASTA是一个将用户草图和文本描述结合起来生成3D形状的方法，通过文本嵌入丰富草图语义，并引入ISG-Net进行细节和结构处理，在草图到3D形状生成和部件级编辑方面表现出色。", "motivation": "现有条件3D形状生成方法主要关注孤立的条件信号（用户草图或文本描述），导致生成的形状缺乏灵活控制。这些方法在信息损失和用户输入意图最大化方面存在挑战。", "method": "本文提出了PASTA，一种将用户草图和文本描述无缝整合以生成3D形状的灵活方法。核心思想是利用视觉-语言模型的文本嵌入来丰富草图的语义表示，通过文本派生的先验指定对象的部件，以弥补模糊草图中缺失的视觉线索。此外，引入了ISG-Net，它包含两种图卷积网络：IndivGCN用于处理细粒度细节，PartGCN用于将这些细节聚合为部件并优化对象结构。", "result": "PASTA在部件级编辑方面优于现有方法，并在草图到3D形状生成方面取得了最先进的成果。", "conclusion": "PASTA通过整合草图和文本描述，并利用文本先验和分层图卷积网络，成功解决了条件3D形状生成中的挑战，实现了更灵活、更精确的形状生成和部件级编辑，达到了SOTA性能。", "translation": "在条件三维形状生成中，最大限度地减少信息损失和最大限度地利用用户输入的意图是一个根本性挑战。现有方法主要集中于两种孤立的条件信号，即用户草图和文本描述，每种都不能提供对生成形状的灵活控制。在本文中，我们引入了PASTA，这是一种灵活的方法，可以无缝整合用户草图和文本描述以进行三维形状生成。其关键思想是使用来自视觉-语言模型的文本嵌入来丰富草图的语义表示。具体来说，这些文本派生的先验指定了对象的部件组成，弥补了模糊草图中缺失的视觉线索。此外，我们引入了ISG-Net，它采用了两种类型的图卷积网络：IndivGCN，用于处理细粒度细节；PartGCN，用于将这些细节聚合为部件并优化对象的结构。大量的实验表明，PASTA在部件级编辑方面优于现有方法，并在草图到三维形状生成方面取得了最先进的成果。", "summary": "本文提出PASTA，一个创新的3D形状生成框架，旨在解决现有方法中草图或文本单独控制的局限性。PASTA通过将用户草图与文本描述无缝结合，利用视觉-语言模型的文本嵌入来增强草图的语义，尤其是在部件级信息上。它还引入了ISG-Net，包含IndivGCN和PartGCN两种图卷积网络，分别处理细粒度细节和部件结构。实验证明，PASTA在部件级编辑和草图到3D形状生成方面均超越现有方法，达到了最先进的性能。", "keywords": "3D形状生成, 草图到3D, 文本对齐, 部件感知, 图卷积网络", "comments": "PASTA的创新点在于其将草图和文本描述这两种模态的输入进行融合，并通过文本先验来弥补草图的模糊性，从而实现更精细和灵活的3D形状生成。特别是引入了分层图卷积网络ISG-Net，能够同时处理细粒度特征和整体部件结构，这是其取得SOTA结果的关键。该工作对于提升用户控制的3D内容生成具有重要意义。"}}
{"id": "2503.12836", "pdf": "https://arxiv.org/pdf/2503.12836", "abs": "https://arxiv.org/abs/2503.12836", "authors": ["Sumin In", "Youngdong Jang", "Utae Jeong", "MinHyuk Jang", "Hyeongcheol Park", "Eunbyung Park", "Sangpil Kim"], "title": "CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 17 figures", "summary": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.", "AI": {"title_translation": "CompMarkGS：针对压缩3D高斯泼溅的鲁棒水印技术", "tldr": "本文提出了一种新的水印方法CompMarkGS，通过在训练中模拟压缩失真并引入可学习的水印嵌入特征，解决了3D高斯泼溅模型在压缩后水印丢失的问题，同时保持了高渲染质量。", "motivation": "3D高斯泼溅（3DGS）因其快速可微分渲染能力而被广泛商用，因此版权保护变得至关重要。然而，3DGS模型庞大，需要压缩以实现高效传输和存储。现有3DGS水印方法易受基于量化的压缩攻击，导致水印丢失。", "method": "本文提出了一种新的水印方法，通过以下方式确保模型压缩后的水印鲁棒性并保持高渲染质量：1) 在训练期间引入量化失真层以模拟压缩过程，从而在基于量化的压缩下保留水印。2) 提出可学习的水印嵌入特征，将水印嵌入到锚点特征中，确保结构一致性和无缝集成。3) 提出频率感知锚点增长机制，通过识别高频区域中的高斯点来增强图像质量。", "result": "实验结果证实，该方法在高压缩下仍能保留水印并保持卓越的图像质量。", "conclusion": "本文提出的CompMarkGS是一种有前景的3DGS模型安全方法，能够在高压缩下保留水印并维持高质量渲染。", "translation": "3D高斯泼溅（3DGS）实现了3D重建和新颖视图合成的快速可微分渲染，从而导致其广泛的商业应用。因此，通过水印进行版权保护变得至关重要。然而，由于3DGS依赖于数百万个高斯点，需要千兆字节的存储空间，因此高效传输和存储需要压缩。现有的3DGS水印方法容易受到基于量化的压缩攻击，通常导致嵌入水印的丢失。为了解决这一挑战，我们提出了一种新颖的水印方法，该方法确保模型压缩后水印的鲁棒性，同时保持高渲染质量。具体而言，我们在训练期间引入了一个量化失真层来模拟压缩，从而在基于量化的压缩下保留水印。此外，我们提出了一种可学习的水印嵌入特征，将水印嵌入到锚点特征中，确保结构一致性和与3D场景的无缝集成。此外，我们提出了一种频率感知锚点增长机制，通过有效识别这些区域内的高斯点来增强高频区域的图像质量。实验结果证实，我们的方法在高压缩下仍能保留水印并保持卓越的图像质量，验证了其作为安全3DGS模型的一种有前景的方法。", "summary": "本文针对3D高斯泼溅（3DGS）模型在压缩后水印易丢失的问题，提出了一种名为CompMarkGS的新型鲁棒水印方法。该方法通过在训练中引入量化失真层模拟压缩，确保水印在量化压缩下得以保留。同时，它提出了可学习的水印嵌入特征，将水印无缝嵌入到锚点特征中，并采用频率感知锚点增长机制提升高频区域的图像质量。实验证明，CompMarkGS能在高压缩比下有效保留水印并维持卓越的渲染质量，为3DGS模型的版权保护提供了有效方案。", "keywords": "3D高斯泼溅, 水印, 压缩鲁棒性, 版权保护, 深度学习", "comments": "本文解决了3D高斯泼溅（3DGS）模型商业应用中一个关键但具有挑战性的问题：在模型压缩后如何保持水印的鲁棒性。其创新点在于引入了量化失真层模拟压缩过程，以及可学习的水印嵌入特征和频率感知锚点增长机制，这些设计都直接针对了现有方法在压缩下水印脆弱的痛点。该方法的重要性在于它能有效保护3DGS模型的知识产权，促进其更安全的商业化应用。"}}
{"id": "2503.12838", "pdf": "https://arxiv.org/pdf/2503.12838", "abs": "https://arxiv.org/abs/2503.12838", "authors": ["Junjia Huang", "Pengxiang Yan", "Jinhang Cai", "Jiyang Liu", "Zhao Wang", "Yitong Wang", "Xinglong Wu", "Guanbin Li"], "title": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode", "categories": ["cs.CV"], "comment": "Under submission", "summary": "Text-driven image generation using diffusion models has recently gained\nsignificant attention. To enable more flexible image manipulation and editing,\nrecent research has expanded from single image generation to transparent layer\ngeneration and multi-layer compositions. However, existing approaches often\nfail to provide a thorough exploration of multi-layer structures, leading to\ninconsistent inter-layer interactions, such as occlusion relationships, spatial\nlayout, and shadowing. In this paper, we introduce DreamLayer, a novel\nframework that enables coherent text-driven generation of multiple image\nlayers, by explicitly modeling the relationship between transparent foreground\nand background layers. DreamLayer incorporates three key components, i.e.,\nContext-Aware Cross-Attention (CACA) for global-local information exchange,\nLayer-Shared Self-Attention (LSSA) for establishing robust inter-layer\nconnections, and Information Retained Harmonization (IRH) for refining fusion\ndetails at the latent level. By leveraging a coherent full-image context,\nDreamLayer builds inter-layer connections through attention mechanisms and\napplies a harmonization step to achieve seamless layer fusion. To facilitate\nresearch in multi-layer generation, we construct a high-quality, diverse\nmulti-layer dataset including 400k samples. Extensive experiments and user\nstudies demonstrate that DreamLayer generates more coherent and well-aligned\nlayers, with broad applicability, including latent-space image editing and\nimage-to-layer decomposition.", "AI": {"title_translation": "DreamLayer：基于扩散模型的同步多层生成", "tldr": "DreamLayer是一个新的框架，通过明确建模层间关系，实现连贯的文本驱动多图像层生成。", "motivation": "现有方法在多层结构探索方面不足，导致层间交互不一致，例如遮挡关系、空间布局和阴影。", "method": "DreamLayer引入了三个关键组件：上下文感知交叉注意力 (CACA) 用于全局-局部信息交换，层共享自注意力 (LSSA) 用于建立鲁棒的层间连接，以及信息保留协调 (IRH) 用于在潜在层面细化融合细节。它利用连贯的完整图像上下文，通过注意力机制建立层间连接，并应用协调步骤实现无缝层融合。此外，作者还构建了一个包含40万样本的高质量多层数据集。", "result": "广泛的实验和用户研究表明，DreamLayer生成了更连贯、对齐更好的层，并具有广泛的适用性，包括潜在空间图像编辑和图像到层分解。", "conclusion": "DreamLayer通过明确建模层间关系，解决了现有方法中层间交互不一致的问题，实现了连贯的文本驱动多图像层生成，并展示了其有效性和广泛的应用潜力。", "translation": "扩散模型驱动的文本图像生成最近受到了广泛关注。为了实现更灵活的图像操作和编辑，最近的研究已从单一图像生成扩展到透明层生成和多层合成。然而，现有方法往往未能对多层结构进行彻底探索，导致层间交互不一致，例如遮挡关系、空间布局和阴影。在本文中，我们引入了DreamLayer，这是一个新颖的框架，通过明确建模透明前景和背景层之间的关系，实现了连贯的文本驱动多图像层生成。DreamLayer包含三个关键组件，即用于全局-局部信息交换的上下文感知交叉注意力（CACA）、用于建立鲁棒层间连接的层共享自注意力（LSSA）以及用于在潜在层面细化融合细节的信息保留协调（IRH）。通过利用连贯的完整图像上下文，DreamLayer通过注意力机制建立层间连接，并应用协调步骤实现无缝层融合。为了促进多层生成研究，我们构建了一个包含40万样本的高质量、多样化多层数据集。广泛的实验和用户研究表明，DreamLayer生成了更连贯、对齐更好的层，并具有广泛的适用性，包括潜在空间图像编辑和图像到层分解。", "summary": "本文提出了DreamLayer，一个用于文本驱动多层图像生成的新框架。它通过明确建模前景和背景层之间的关系来解决现有方法中层间交互不一致的问题。DreamLayer包含上下文感知交叉注意力、层共享自注意力和信息保留协调三个核心组件，以实现全局信息交换、鲁棒层连接和精细融合。该方法利用完整的图像上下文和注意力机制，并通过协调步骤确保层融合的无缝性。为支持研究，作者还构建了一个大型多层数据集。实验证明DreamLayer能生成更连贯、对齐的图像层，并支持图像编辑和分解等应用。", "keywords": "多层生成, 扩散模型, 文本驱动图像生成, 层间交互, DreamLayer", "comments": "DreamLayer的创新之处在于通过明确建模前景和背景层的关系，解决了现有扩散模型在多层生成中层间不一致的问题。其引入的CACA、LSSA和IRH组件有效提升了层间交互的连贯性和融合质量。此外，构建大规模多层数据集也为该领域的研究提供了宝贵的资源，具有重要的贡献。"}}
{"id": "2503.12843", "pdf": "https://arxiv.org/pdf/2503.12843", "abs": "https://arxiv.org/abs/2503.12843", "authors": ["Haozhe Si", "Yuxuan Wan", "Minh Do", "Deepak Vasisht", "Han Zhao", "Hendrik F. Hamann"], "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Geospatial raster (imagery) data, such as that collected by satellite-based\nimaging systems at different times and spectral bands, hold immense potential\nfor enabling a wide range of high-impact applications. This potential stems\nfrom the rich information that is spatially and temporally contextualized\nacross multiple channels and sensing modalities. Recent work has adapted\nexisting self-supervised learning approaches for such geospatial data. However,\nthey fall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer (LESS ViT) with three key innovations: i)\nthe LESS Attention Block that approximates high-dimensional spatial-spectral\nattention through Kronecker's product of the low-dimensional spatial and\nspectral attention components; ii) the Continuous Positional-Channel Embedding\nLayer that preserves both spatial and spectral continuity and physical\ncharacteristics of each patch; and iii) the Perception Field Mask that exploits\nlocal spatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct a benchmark, GFM-Bench, which\nserves as a comprehensive benchmark for such geospatial raster data. We\npretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with\nintegrated positional and channel masking strategies. Experimental results\ndemonstrate that our proposed method surpasses current state-of-the-art\nmulti-modal geospatial foundation models, achieving superior performance with\nless computation and fewer parameters. The flexibility and extensibility of our\nframework make it a promising direction for future geospatial data analysis\ntasks that involve a wide range of modalities and channels.", "AI": {"title_translation": "迈向多模态和高光谱地理空间数据的可扩展基础模型", "tldr": "本文提出了一种名为LESS ViT的新型可扩展视觉Transformer，用于处理多模态和高光谱地理空间数据，它在性能、计算效率和参数数量上超越了现有SOTA模型。", "motivation": "现有的自监督学习方法在处理地理空间数据时，缺乏可扩展的模型架构，导致在面对不断增加的通道和模态时出现灵活性不足和计算效率低下。", "method": "本文引入了低秩高效空间-光谱视觉Transformer (LESS ViT)，包含三项关键创新：1) LESS注意力块，通过低维空间和光谱注意力分量的克罗内克积近似高维空间-光谱注意力；2) 连续位置-通道嵌入层，保留每个补丁的空间和光谱连续性及物理特性；3) 感知场掩码，通过将注意力限制在相邻补丁来利用局部空间依赖性。此外，本文构建了GFM-Bench基准，并使用集成位置和通道掩码策略的高光谱掩码自编码器框架预训练LESS ViT。", "result": "实验结果表明，LESS ViT超越了当前最先进的多模态地理空间基础模型，以更少的计算量和更少的参数实现了卓越的性能。", "conclusion": "LESS ViT框架的灵活性和可扩展性使其成为未来涉及广泛模态和通道的地理空间数据分析任务的一个有前景的方向。", "translation": "地理空间栅格（图像）数据，例如由基于卫星的成像系统在不同时间段和光谱波段收集的数据，在实现广泛的高影响力应用方面具有巨大的潜力。这种潜力源于在多个通道和传感模态中进行空间和时间背景化处理的丰富信息。最近的工作已经调整了现有的自监督学习方法来处理此类地理空间数据。然而，它们缺乏可扩展的模型架构，导致在面对不断增加的通道和模态时出现灵活性不足和计算效率低下。为了解决这些限制，我们引入了低秩高效空间-光谱视觉Transformer (LESS ViT)，它具有三项关键创新：i) LESS注意力块，通过低维空间和光谱注意力分量的克罗内克积近似高维空间-光谱注意力；ii) 连续位置-通道嵌入层，保留每个补丁的空间和光谱连续性及物理特性；以及iii) 感知场掩码，通过将注意力限制在相邻补丁来利用局部空间依赖性。为了评估所提出的创新，我们构建了一个基准测试GFM-Bench，它作为此类地理空间栅格数据的综合基准。我们使用集成位置和通道掩码策略的高光谱掩码自编码器框架预训练LESS ViT。实验结果表明，我们提出的方法超越了当前最先进的多模态地理空间基础模型，以更少的计算量和更少的参数实现了卓越的性能。我们框架的灵活性和可扩展性使其成为未来涉及广泛模态和通道的地理空间数据分析任务的一个有前景的方向。", "summary": "本文针对现有地理空间数据自监督学习方法在可扩展性、灵活性和计算效率方面的不足，提出了一种名为低秩高效空间-光谱视觉Transformer (LESS ViT) 的新型基础模型。LESS ViT通过引入LESS注意力块、连续位置-通道嵌入层和感知场掩码三项创新来解决这些问题。研究人员构建了GFM-Bench基准并使用高光谱掩码自编码器框架预训练LESS ViT。实验结果表明，LESS ViT在性能上超越了现有最先进的模型，同时显著减少了计算量和参数。该框架的灵活性和可扩展性预示着其在未来多模态地理空间数据分析任务中的巨大潜力。", "keywords": "地理空间数据, 多模态, 高光谱, 基础模型, 视觉Transformer", "comments": "该论文通过引入LESS ViT及其独特的三项创新，有效解决了多模态和高光谱地理空间数据处理中现有模型的可扩展性、灵活性和计算效率问题。其创新点在于对高维空间-光谱注意力的低秩近似、对空间和光谱连续性的保留，以及利用局部依赖性。通过构建新的基准和SOTA性能，LESS ViT为地理空间基础模型的发展开辟了新方向，特别是在处理大规模、多源异构数据方面具有重要意义。"}}
{"id": "2503.12844", "pdf": "https://arxiv.org/pdf/2503.12844", "abs": "https://arxiv.org/abs/2503.12844", "authors": ["Junhyeok Kim", "Jaewoo Park", "Junhee Park", "Sangeyl Lee", "Jiwan Chung", "Jisung Kim", "Ji Hoon Joung", "Youngjae Yu"], "title": "GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and Low-Vision Accessibility-Aware Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Mobility remains a significant challenge for the 2.2 billion people worldwide\naffected by blindness and low vision (BLV), with 7% of visually impaired\nindividuals experiencing falls at least once a month. While recent advances in\nMultimodal Large Language Models (MLLMs) offer promising opportunities for BLV\nassistance, their development has been hindered by limited datasets. This\nlimitation stems from the fact that BLV-aware annotation requires specialized\ndomain knowledge and intensive labor. To address this gap, we introduce\nGuideDog, a novel accessibility-aware guide dataset containing 22K\nimage-description pairs (including 2K human-annotated pairs) that capture\ndiverse real-world scenes from a pedestrian's viewpoint. Our approach shifts\nthe annotation burden from generation to verification through a collaborative\nhuman-AI framework grounded in established accessibility standards,\nsignificantly improving efficiency while maintaining high-quality annotations.\nWe also develop GuideDogQA, a subset of 818 samples featuring multiple-choice\nquestions designed to evaluate fine-grained visual perception capabilities,\nspecifically object recognition and relative depth perception. Our experimental\nresults highlight the importance of accurate spatial understanding for\neffective BLV guidance. GuideDog and GuideDogQA will advance research in\nMLLM-based assistive technologies for BLV individuals while contributing to\nbroader applications in understanding egocentric scenes for robotics and\naugmented reality. The code and dataset will be publicly available.", "AI": {"title_translation": "GuideDog：一个用于盲人和低视力无障碍引导的真实世界以自我为中心的多模态数据集", "tldr": "GuideDog是一个新的以自我为中心的多模态数据集（包含2.2万个图像-描述对），旨在解决盲人和低视力（BLV）辅助领域的数据集限制，通过人机协作高效标注，并包含用于评估视觉感知的GuideDogQA子集。", "motivation": "全球22亿盲人和低视力（BLV）人群面临严重的行动挑战，而多模态大型语言模型（MLLMs）在BLV辅助方面的发展受限于缺乏专门的、劳动密集型标注数据集。", "method": "研究引入了GuideDog数据集，包含2.2万个图像-描述对（其中2千个为人工标注），从行人视角捕捉真实世界场景。通过基于无障碍标准的协作式人机框架，将标注负担从生成转移到验证，提高了效率和质量。同时开发了GuideDogQA子集（818个样本），包含多项选择题，用于评估细粒度视觉感知能力，如物体识别和相对深度感知。", "result": "实验结果强调了准确的空间理解对于有效的BLV引导的重要性。GuideDog和GuideDogQA将推动基于MLLM的BLV辅助技术研究，并有助于机器人和增强现实中以自我为中心场景的理解。代码和数据集将公开发布。", "conclusion": "GuideDog数据集和GuideDogQA子集解决了盲人和低视力（BLV）辅助研究中的数据限制问题，有望推动基于多模态大型语言模型（MLLM）的辅助技术以及更广泛的以自我为中心场景理解应用的发展。", "translation": "全球有22亿人受到盲症和低视力（BLV）的影响，其中7%的视力障碍者每月至少跌倒一次，行动能力仍然是一个重大挑战。尽管多模态大型语言模型（MLLMs）的最新进展为BLV辅助提供了有前景的机会，但其发展一直受到数据集有限的阻碍。这种限制源于BLV感知注释需要专业的领域知识和大量的劳动。为了解决这一空白，我们引入了GuideDog，一个新颖的无障碍感知引导数据集，包含2.2万个图像-描述对（包括2千个人工注释对），捕捉了行人视角的各种真实世界场景。我们的方法通过一个基于既定无障碍标准的协作式人机框架，将注释负担从生成转移到验证，显著提高了效率，同时保持了高质量的注释。我们还开发了GuideDogQA，一个包含818个样本的子集，其多项选择题旨在评估细粒度视觉感知能力，特别是物体识别和相对深度感知。我们的实验结果强调了准确的空间理解对于有效的BLV引导的重要性。GuideDog和GuideDogQA将推动基于MLLM的BLV个体辅助技术研究，同时也有助于机器人和增强现实中理解以自我为中心场景的更广泛应用。代码和数据集将公开发布。", "summary": "该论文介绍了GuideDog，一个包含2.2万个图像-描述对的新型多模态数据集，以及用于细粒度视觉感知评估的GuideDogQA子集。它通过采用高效的人机协作标注框架，解决了阻碍盲人和低视力（BLV）辅助领域多模态大型语言模型（MLLM）发展的数据稀缺问题。该数据集旨在通过强调准确的空间理解来改善BLV引导，并将促进基于MLLM的辅助技术、机器人和增强现实领域的研究。", "keywords": "盲人和低视力, 多模态数据集, 无障碍, 以自我为中心的视觉, 人机协作", "comments": "该论文通过解决AI发展中的一个根本性瓶颈——数据稀缺性，来应对盲人和低视力（BLV）个体面临的一个关键现实世界问题。其创新之处在于采用人机协作标注框架，显著提高了专业数据的标注效率和质量。GuideDogQA的创建进一步支持了对关键视觉感知能力的针对性评估。"}}
{"id": "2503.12852", "pdf": "https://arxiv.org/pdf/2503.12852", "abs": "https://arxiv.org/abs/2503.12852", "authors": ["Aditi Tiwari", "Klara Nahrstedt"], "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing", "categories": ["cs.CV", "cs.MM"], "comment": "9 pages, 8 figures", "summary": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.", "AI": {"title_translation": "ACT360：一种用于关键任务训练和汇报的高效360度动作检测和总结框架", "tldr": "ACT360是一个利用360度视频和机器学习进行自动化动作检测和总结的框架，旨在提高关键任务训练和汇报的效率和准确性，并通过模型优化使其适用于资源受限环境。", "motivation": "传统关键任务训练（如灾难响应、军事模拟、工业安全）的后期分析依赖手动审查2D视频，耗时且缺乏全面的态势感知。这限制了高风险环境中对精度和错误最小化的要求。", "method": "ACT360系统利用360度视频和机器学习。核心是360YOWO模型，它是You Only Watch Once (YOWO) 的增强版，加入了空间注意力机制和等距柱状投影感知卷积 (EAC) 来缓解全景视频失真。为适应资源受限环境，对模型进行了量化和剪枝。还集成了360AIE (Action Insight Explorer)，一个基于网络的界面，用于自动动作检测、检索和使用大型语言模型 (LLM) 进行文本总结。", "result": "模型量化和剪枝将模型大小减少了74%，同时保持了鲁棒的准确性（mAP仅下降1.5%，从0.865降至0.850），并提高了推理速度。该方法在包含55个标记的360度视频的公开数据集上进行了验证，这些视频涵盖了七种关键操作动作。通过集成360AIE，显著提高了事件后分析的效率。", "conclusion": "ACT360是一个通用的关键任务汇报框架，集成了EAC、空间注意力、总结和模型优化，适用于任何需要轻量级动作检测和结构化训练后分析的训练环境。", "translation": "高效的训练和汇报在灾难响应、军事模拟和工业安全等高风险、关键任务环境中至关重要，在这些环境中，精确性和错误最小化是首要考虑。传统的训练后分析依赖于手动审查2D视频，这是一个耗时的过程，并且缺乏全面的态势感知。为了解决这些限制，我们引入了ACT360，一个利用360度视频和机器学习进行自动化动作检测和结构化汇报的系统。ACT360集成了360YOWO，这是一个增强的“你只看一次”（YOWO）模型，具有空间注意力和等距柱状投影感知卷积（EAC），以减轻全景视频失真。为了在资源受限环境中部署，我们应用了量化和模型剪枝，将模型大小减少了74%，同时保持了稳健的准确性（mAP仅下降1.5%，从0.865到0.850），并提高了推理速度。我们在一个包含55个标记的360度视频的公开数据集上验证了我们的方法，这些视频涵盖了七个关键操作动作，记录于各种真实世界训练会话和环境条件下。此外，ACT360集成了360AIE（动作洞察探索器），一个基于网络的界面，用于自动动作检测、检索和使用大型语言模型（LLM）进行文本总结，显著提高了事件后分析效率。ACT360作为一个通用的关键任务汇报框架，结合了EAC、空间注意力、总结和模型优化。这些创新适用于任何需要轻量级动作检测和结构化训练后分析的训练环境。", "summary": "ACT360是一种为关键任务环境（如灾难响应、军事训练）设计的360度视频动作检测与总结框架，旨在解决传统2D视频手动分析耗时且缺乏全面态势感知的问题。该系统基于增强的360YOWO模型，结合了空间注意力和等距柱状投影感知卷积（EAC）以处理全景视频失真，并通过量化和剪枝优化模型，使其在资源受限设备上高效运行，同时保持高精度。ACT360还集成了360AIE，一个利用LLM进行文本总结的Web界面，以提升分析效率。该框架在公开数据集上得到了验证，证明了其在轻量级动作检测和结构化训练后分析方面的通用性和有效性。", "keywords": "360度视频, 动作检测, 训练汇报, 机器学习, 模型优化", "comments": "该论文提出了一种创新的360度视频分析框架ACT360，解决了传统2D视频分析的局限性。其创新点在于结合了360度视频、深度学习（增强的YOWO模型与EAC、空间注意力）和大型语言模型（LLM）进行自动化动作检测和智能总结。特别值得注意的是，该框架通过模型优化（量化和剪枝）显著降低了模型大小和提高了推理速度，使其适用于资源受限的关键任务环境，这增加了其实用性和部署潜力。该框架的通用性也使其可以应用于多种需要高效训练和汇报的场景。"}}
{"id": "2503.12853", "pdf": "https://arxiv.org/pdf/2503.12853", "abs": "https://arxiv.org/abs/2503.12853", "authors": ["Yanlin Xiang", "Qingyuan He", "Ting Xu", "Ran Hao", "Jiacheng Hu", "Hanchao Zhang"], "title": "Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.", "AI": {"title_translation": "自适应Transformer注意力与多尺度融合的脊柱3D分割", "tldr": "本研究提出一种基于改进SwinUNETR的脊柱3D语义分割方法，通过多尺度融合和自适应注意力机制显著提高了分割精度和鲁棒性。", "motivation": "提高脊柱3D语义分割的准确性和鲁棒性，应对脊柱图像复杂的解剖结构。", "method": "提出一种基于改进SwinUNETR的3D语义分割方法，引入多尺度融合机制以增强特征提取能力，并引入自适应注意力机制以优化边界分割效果。", "result": "与3D CNN、3D U-Net和3D U-Net + Transformer相比，该模型在mIoU、mDice和mAcc指标上取得了显著提升，并具有更好的分割性能。消融实验验证了多尺度融合和自适应注意力机制的有效性。可视化分析显示模型能更好地恢复脊柱图像的真实解剖结构。", "conclusion": "该研究为医学图像分割任务提供了一个高效的解决方案，对智能医疗图像分析具有重要意义。", "translation": "本研究提出一种基于改进SwinUNETR的脊柱3D语义分割方法，以提高分割精度和鲁棒性。针对脊柱图像复杂的解剖结构，本文引入多尺度融合机制，通过利用不同尺度的信息增强特征提取能力，从而提高模型对目标区域的识别精度。此外，自适应注意力机制的引入使模型能够动态调整对关键区域的关注，从而优化边界分割效果。实验结果表明，与3D CNN、3D U-Net和3D U-Net + Transformer相比，本研究的模型在mIoU、mDice和mAcc指标上取得了显著提升，并具有更好的分割性能。消融实验进一步验证了所提出改进方法的有效性，证明多尺度融合和自适应注意力机制对分割任务具有积极作用。通过推理结果的可视化分析，模型能够更好地恢复脊柱图像的真实解剖结构。未来的研究可以进一步优化Transformer结构并扩大数据规模以提高模型的泛化能力。本研究为医学图像分割任务提供了一个高效的解决方案，对智能医疗图像分析具有重要意义。", "summary": "本文提出一种改进的SwinUNETR模型，用于脊柱3D语义分割，旨在提高复杂脊柱解剖结构的分割精度和鲁棒性。该方法结合了多尺度融合机制以增强特征提取，并引入自适应注意力机制以优化边界分割。实验结果表明，与现有方法相比，该模型在mIoU、mDice和mAcc等指标上表现出显著的性能提升，并通过消融实验验证了所提机制的有效性。该研究为医学图像分割提供了一个高效的解决方案。", "keywords": "脊柱分割, 3D语义分割, Transformer, 多尺度融合, 自适应注意力", "comments": "该研究通过结合多尺度融合和自适应注意力机制改进了SwinUNETR，有效地解决了脊柱3D分割中复杂的解剖结构问题，提高了分割精度和边界效果。其创新点在于对Transformer注意力机制的自适应调整以及多尺度特征的有效融合。这对于智能医疗图像分析领域具有重要意义。"}}
{"id": "2503.12855", "pdf": "https://arxiv.org/pdf/2503.12855", "abs": "https://arxiv.org/abs/2503.12855", "authors": ["Yujie Lu", "Yale Song", "William Wang", "Lorenzo Torresani", "Tushar Nagarajan"], "title": "VITED: Video Temporal Evidence Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities.", "AI": {"title_translation": "VITED：视频时间证据蒸馏", "tldr": "现有视频问答（VideoQA）模型在多步推理方面存在困难，因为它们固定采样帧且缺乏时间证据定位能力。本文提出了VITED框架，通过构建和生成时间证据链来增强现有数据集，并训练模型直接生成这些链。VITED能有效定位证据窗口并进行多步推理，在长视频问答基准测试中超越了现有先进方法。", "motivation": "现有视频问答模型在需要证据链推理的复杂视频问答方面表现不佳，主要问题包括：1. 统一采样固定数量的帧，可能错过视频中非均匀分布的关键证据。2. 缺乏在完整视频更广阔背景下对证据进行时间定位的能力，而这对于回答复杂问题至关重要。", "method": "本文提出了VITED框架，旨在通过以下方式解决现有问题：1. 增强现有VideoQA数据集，自动构建证据推理链，这些链通过搜索视频中带有支持证据的最佳兴趣区间来最大化回答给定问题的可能性。2. 训练VITED模型直接生成这些证据链，使其能够定位证据窗口并在长视频内容中进行多步推理。", "result": "VITED模型在多项长视频问答基准测试中表现出色，其性能优于缺乏证据推理能力的现有最先进方法。", "conclusion": "VITED框架通过引入证据链推理，有效解决了现有视频问答模型在复杂多步推理和时间证据定位方面的不足，显著提升了在长视频问答任务上的性能。", "translation": "我们研究通过证据链推理解决复杂的视频问答问题——从视频的多个相关部分识别时间跨度序列，以及其中的视觉证据。现有模型在多步推理方面存在困难，因为它们统一采样固定数量的帧，这可能会错过视频中非均匀分布的关键证据。此外，它们缺乏在完整视频的更广阔背景下对这些证据进行时间定位的能力，而这对于回答复杂问题是必需的。我们提出了一个框架，通过搜索视频中具有支持证据的最佳兴趣区间来自动构建证据推理链，从而增强现有视频问答数据集，最大限度地提高回答给定问题的可能性。我们训练我们的模型（VITED）直接生成这些证据链，使其能够在长视频内容中定位证据窗口并对其进行多步推理。我们展示了我们的证据蒸馏模型在长视频问答基准测试中的价值，在这些测试中，我们优于缺乏证据推理能力的最新方法。", "summary": "VITED是一个用于复杂视频问答的框架，旨在通过引入“证据链推理”来解决现有模型在多步推理和证据定位方面的不足。它通过自动构建并训练模型生成视频中的时间证据链，从而更好地定位关键信息并进行跨时间点的推理。实验表明，VITED在长视频问答基准测试上显著优于缺乏证据推理能力的现有最先进方法。", "keywords": "视频问答, 时间推理, 证据蒸馏, 多步推理, 视频理解", "comments": "VITED的创新之处在于其“证据链推理”的概念，通过显式地定位和连接视频中的关键时间证据，克服了传统VideoQA模型固定帧采样和缺乏时间上下文的局限性。这对于处理需要多步逻辑推理的长视频问答任务具有重要意义，提供了一个更具解释性和鲁棒性的解决方案。"}}
{"id": "2503.12862", "pdf": "https://arxiv.org/pdf/2503.12862", "abs": "https://arxiv.org/abs/2503.12862", "authors": ["Yu-Ting Zhan", "He-bi Yang", "Cheng-Yuan Ho", "Jui-Chiu Chiang", "Wen-Hsiao Peng"], "title": "CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has shown immense potential for novel view\nsynthesis. However, achieving rate-distortion-optimized compression of 3DGS\nrepresentations for transmission and/or storage applications remains a\nchallenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for\nend-to-end optimized compression, delivering state-of-the-art coding\nperformance. Despite this, it requires prolonged training and decoding time. To\naddress these limitations, we propose CAT-3DGS Pro, an enhanced version of\nCAT-3DGS that improves both compression performance and computational\nefficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which\nreplaces the triplane-based hyperprior to reduce redundant parameters. To\nachieve a more balanced rate-distortion trade-off and faster encoding, we\npropose an alternate optimization strategy (A-RDO). Additionally, we refine the\nsampling rate optimization method in CAT-3DGS, leading to significant\nimprovements in rate-distortion performance. These enhancements result in a\n46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while\nachieving 5x acceleration in decoding speed for the Amsterdam scene compared to\nCAT-3DGS.", "AI": {"title_translation": "CAT-3DGS Pro：高效3DGS压缩的新基准", "tldr": "CAT-3DGS Pro通过引入PCA引导的超先验、交替优化策略和改进的采样率优化，显著提升了3DGS压缩的性能和计算效率，实现了更低的码率和更快的训练/解码速度。", "motivation": "3D高斯泼溅 (3DGS) 在新颖视图合成方面潜力巨大，但其表示的码率失真优化压缩在传输和存储应用中仍面临挑战。现有的CAT-3DGS虽然性能领先，但训练和解码时间过长，因此需要提出一种更高效的解决方案。", "method": "本文提出了CAT-3DGS Pro，对CAT-3DGS进行了增强。主要方法包括：1. 引入PCA引导的向量矩阵超先验，取代三平面超先验以减少冗余参数。2. 提出交替优化策略 (A-RDO)，以实现更平衡的码率失真权衡和更快的编码。3. 改进CAT-3DGS中的采样率优化方法，进一步提升码率失真性能。", "result": "CAT-3DGS Pro 相较于 CAT-3DGS，在 BungeeNeRF 数据集上实现了46.6%的BD-rate降低和3倍的训练速度提升，同时在阿姆斯特丹场景中实现了5倍的解码速度提升。", "conclusion": "CAT-3DGS Pro通过多项创新改进，显著提升了3DGS压缩的性能和计算效率，为高效3DGS压缩设立了新的基准。", "translation": "3D高斯泼溅（3DGS）在新颖视图合成方面展现出巨大潜力。然而，实现针对传输和/或存储应用的3DGS表示的码率失真优化压缩仍然是一个挑战。CAT-3DGS引入了上下文自适应三平面超先验，实现了端到端优化的压缩，并提供了最先进的编码性能。尽管如此，它需要长时间的训练和解码时间。为了解决这些限制，我们提出了CAT-3DGS Pro，它是CAT-3DGS的增强版本，旨在提高压缩性能和计算效率。首先，我们引入了PCA引导的向量矩阵超先验，它取代了基于三平面的超先验，以减少冗余参数。为了实现更平衡的码率失真权衡和更快的编码，我们提出了一种交替优化策略（A-RDO）。此外，我们改进了CAT-3DGS中的采样率优化方法，从而显著改善了码率失真性能。这些增强使得在BungeeNeRF上实现了46.6%的BD-rate降低和3倍的训练速度提升，同时与CAT-3DGS相比，在阿姆斯特丹场景中实现了5倍的解码速度加速。", "summary": "本文提出CAT-3DGS Pro，旨在解决3DGS压缩中CAT-3DGS训练和解码时间过长的问题。通过引入PCA引导的向量矩阵超先验、交替优化策略（A-RDO）和改进的采样率优化方法，CAT-3DGS Pro显著提高了3DGS的压缩性能和计算效率。实验结果表明，该方法在BD-rate上实现了大幅降低，并在训练和解码速度上获得了显著提升。", "keywords": "3DGS, 压缩, CAT-3DGS Pro, 码率失真优化, 计算效率", "comments": "CAT-3DGS Pro在保持甚至提升压缩性能的同时，大幅优化了计算效率，这对于3DGS在实际应用中的推广至关重要。其创新点在于对超先验结构和优化策略的改进，有效地解决了现有方法的瓶颈。该工作为3DGS压缩领域树立了新的性能和效率标准。"}}
{"id": "2503.12866", "pdf": "https://arxiv.org/pdf/2503.12866", "abs": "https://arxiv.org/abs/2503.12866", "authors": ["Chenyu Zhang", "Kunlun Xu", "Zichen Liu", "Yuxin Peng", "Jiahuan Zhou"], "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Vision-language models (VLMs) encounter considerable challenges when adapting\nto domain shifts stemming from changes in data distribution. Test-time\nadaptation (TTA) has emerged as a promising approach to enhance VLM performance\nunder such conditions. In practice, test data often arrives in batches, leading\nto increasing interest in the transductive TTA setting. However, existing TTA\nmethods primarily focus on individual test samples, overlooking crucial\ncross-sample correlations within a batch. While recent ViT-based TTA methods\nhave introduced batch-level adaptation, they remain suboptimal for VLMs due to\ninadequate integration of the text modality. To address these limitations, we\npropose a novel transductive TTA framework, Supportive Clique-based Attribute\nPrompting (SCAP), which effectively combines visual and textual information to\nenhance adaptation by generating fine-grained attribute prompts across test\nbatches. SCAP first forms supportive cliques of test samples in an unsupervised\nmanner based on visual similarity and learns an attribute prompt for each\nclique, capturing shared attributes critical for adaptation. For each test\nsample, SCAP aggregates attribute prompts from its associated cliques,\nproviding enriched contextual information. To ensure adaptability over time, we\nincorporate a retention module that dynamically updates attribute prompts and\ntheir associated attributes as new data arrives. Comprehensive experiments\nacross multiple benchmarks demonstrate that SCAP outperforms existing\nstate-of-the-art methods, significantly advancing VLM generalization under\ndomain shifts. Our code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-SCAP.", "AI": {"title_translation": "SCAP：通过支持性团簇属性提示进行转导式测试时间适应", "tldr": "SCAP是一种新颖的转导式测试时间适应框架，通过结合视觉和文本信息，利用基于团簇的属性提示，有效提升了视觉语言模型在领域偏移下的性能。", "motivation": "视觉语言模型（VLMs）在数据分布变化导致的领域偏移时面临挑战。现有的测试时间适应（TTA）方法主要关注单个测试样本，忽略了批次内重要的跨样本相关性，并且在整合文本模态方面不足。", "method": "SCAP是一种新颖的转导式TTA框架，它有效结合视觉和文本信息，通过在测试批次中生成细粒度属性提示来增强适应性。该方法首先基于视觉相似性无监督地形成测试样本的支持性团簇，并为每个团簇学习一个属性提示。然后，SCAP为每个测试样本聚合其关联团簇的属性提示，提供丰富的上下文信息。此外，框架包含一个保留模块，用于动态更新属性提示及其关联属性。", "result": "在多个基准测试上的综合实验表明，SCAP优于现有最先进的方法，显著提升了视觉语言模型在领域偏移下的泛化能力。", "conclusion": "SCAP通过提出一种新颖的转导式测试时间适应框架，有效地解决了视觉语言模型在领域偏移下的适应性问题，并通过创新的团簇属性提示机制显著提升了模型的泛化能力。", "translation": "视觉语言模型（VLMs）在适应源于数据分布变化的领域偏移时，面临相当大的挑战。测试时间适应（TTA）已成为在这种条件下提升VLM性能的一种有前景的方法。在实践中，测试数据通常以批次形式到达，导致对转导式TTA设置的兴趣日益增加。然而，现有的TTA方法主要关注单个测试样本，忽略了批次内关键的跨样本相关性。尽管最近基于ViT的TTA方法引入了批次级适应，但由于文本模态整合不足，它们对VLM来说仍然不尽理想。为解决这些局限性，我们提出了一种新颖的转导式TTA框架——支持性团簇属性提示（SCAP），它有效地结合了视觉和文本信息，通过在测试批次中生成细粒度属性提示来增强适应性。SCAP首先基于视觉相似性以无监督方式形成测试样本的支持性团簇，并为每个团簇学习一个属性提示，捕获对适应性至关重要的共享属性。对于每个测试样本，SCAP聚合来自其关联团簇的属性提示，提供丰富的上下文信息。为确保随时间推移的适应性，我们整合了一个保留模块，随着新数据的到来动态更新属性提示及其关联属性。在多个基准测试上的综合实验表明，SCAP优于现有最先进的方法，显著提升了VLM在领域偏移下的泛化能力。我们的代码可在https://github.com/zhoujiahuan1991/CVPR2025-SCAP获取。", "summary": "SCAP是一种新颖的转导式测试时间适应框架，旨在解决视觉语言模型在领域偏移下的适应性挑战。它通过结合视觉和文本信息，利用无监督形成的样本团簇来学习和聚合细粒度属性提示，从而增强模型适应性。SCAP还包含一个保留模块以实现动态更新。实验结果表明，SCAP显著优于现有方法，提升了VLM的泛化能力。", "keywords": "视觉语言模型, 测试时间适应, 领域偏移, 属性提示, 团簇", "comments": "SCAP的创新之处在于其引入了“支持性团簇”的概念，并利用跨样本的关联性生成细粒度属性提示，有效弥补了现有TTA方法在处理批次数据和整合文本模态方面的不足。其动态更新机制也增强了模型的长期适应能力。"}}
{"id": "2503.12868", "pdf": "https://arxiv.org/pdf/2503.12868", "abs": "https://arxiv.org/abs/2503.12868", "authors": ["Zi Li", "Jianpeng Zhang", "Tai Ma", "Tony C. W. Mok", "Yan-Jie Zhou", "Zeli Chen", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "UniReg: Foundation Model for Controllable Medical Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based medical image registration has achieved performance parity\nwith conventional methods while demonstrating a substantial advantage in\ncomputational efficiency. However, learning-based registration approaches lack\ngeneralizability across diverse clinical scenarios, requiring the laborious\ndevelopment of multiple isolated networks for specific registration tasks,\ne.g., inter-/intra-subject registration or organ-specific alignment. % To\novercome this limitation, we propose \\textbf{UniReg}, the first interactive\nfoundation model for medical image registration, which combines the precision\nadvantages of task-specific learning methods with the generalization of\ntraditional optimization methods. Our key innovation is a unified framework for\ndiverse registration scenarios, achieved through a conditional deformation\nfield estimation within a unified registration model. This is realized through\na dynamic learning paradigm that explicitly encodes: (1) anatomical structure\npriors, (2) registration type constraints (inter/intra-subject), and (3)\ninstance-specific features, enabling the generation of scenario-optimal\ndeformation fields. % Through comprehensive experiments encompassing $90$\nanatomical structures at different body regions, our UniReg model demonstrates\ncomparable performance with contemporary state-of-the-art methodologies while\nachieving ~50\\% reduction in required training iterations relative to the\nconventional learning-based paradigm. This optimization contributes to a\nsignificant reduction in computational resources, such as training time. Code\nand model will be available.", "AI": {"title_translation": "UniReg：可控医学图像配准的基础模型", "tldr": "UniReg是一个统一的医学图像配准基础模型，解决了现有学习方法泛化性差的问题，通过条件形变场实现了对多种场景的通用配准，并显著减少了训练迭代次数。", "motivation": "现有的基于学习的医学图像配准方法缺乏在不同临床场景下的泛化能力，需要为特定任务（如受试者内/间配准或器官特异性对齐）耗费大量精力开发多个独立的网络，这非常耗时费力。", "method": "提出了UniReg，一个用于医学图像配准的交互式基础模型，旨在结合任务特定学习方法的精度和传统优化方法的泛化能力。其核心创新在于通过在统一的配准模型中估计条件形变场，实现了一个适用于多种配准场景的统一框架。该模型采用动态学习范式，明确编码了：1) 解剖结构先验，2) 配准类型约束（受试者内/间），以及3) 实例特定特征，从而能够生成针对特定场景最优的形变场。", "result": "UniReg模型在涵盖90个不同身体区域解剖结构的综合实验中，表现出与现有最先进方法相当的性能。同时，相对于传统基于学习的范式，所需训练迭代次数减少了约50%，显著减少了计算资源和训练时间。", "conclusion": "UniReg成功地将任务特定学习方法的精度优势与传统优化方法的泛化能力相结合，为医学图像配准提供了一个高效且可泛化的基础模型，有效解决了现有学习方法在泛化性方面的局限性。", "translation": "基于学习的医学图像配准在计算效率方面表现出显著优势，同时性能已与传统方法持平。然而，基于学习的配准方法在多样化的临床场景中缺乏泛化能力，需要为特定的配准任务（例如，受试者间/受试者内配准或器官特异性对齐）耗费大量精力开发多个独立的网络。为了克服这一限制，我们提出了UniReg，这是首个用于医学图像配准的交互式基础模型，它结合了任务特定学习方法的精度优势和传统优化方法的泛化能力。我们的关键创新在于为不同配准场景提供了一个统一的框架，通过在统一配准模型中估计条件形变场来实现。这通过一种动态学习范式实现，该范式明确编码了：(1) 解剖结构先验，(2) 配准类型约束（受试者间/受试者内），以及(3) 实例特定特征，从而能够生成针对场景最优的形变场。通过涵盖不同身体区域90个解剖结构的综合实验，我们的UniReg模型表现出与当代最先进方法相当的性能，同时相对于传统基于学习的范式，所需训练迭代次数减少了约50%。这种优化有助于显著减少计算资源，例如训练时间。代码和模型将可用。", "summary": "本文提出了UniReg，一个用于医学图像配准的交互式基础模型，旨在解决现有基于学习方法泛化能力差的问题。UniReg通过一个统一框架和条件形变场估计，结合了解剖结构先验、配准类型约束和实例特征，实现了对多种配准场景的通用和精确配准。实验证明，UniReg性能与最先进方法相当，并能将训练迭代次数减少约50%，显著节省了计算资源。", "keywords": "医学图像配准, 基础模型, 泛化性, 条件形变场, 计算效率", "comments": "UniReg的创新之处在于提出了首个交互式医学图像配准基础模型，有效解决了现有基于学习方法在泛化能力上的局限性。通过统一框架和条件形变场估计，它提供了一个通用且高效的解决方案，能够适应多样化的临床配准场景。其在性能上与SOTA方法持平，并在训练效率上实现了显著提升，这对于实际应用具有重要意义。"}}
{"id": "2503.12874", "pdf": "https://arxiv.org/pdf/2503.12874", "abs": "https://arxiv.org/abs/2503.12874", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Ke Ma", "Xinfeng Li", "Yihao Huang", "Wei Dong", "Yang Liu", "Xiaochun Cao"], "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT.", "AI": {"title_translation": "视觉-语言模型中基于进化的区域对抗性提示学习用于鲁棒性增强", "tldr": "本文提出ER-APT，一种结合梯度方法和遗传进化来生成多样化对抗样本，并进行区域对抗性提示调优的方法，以显著增强视觉-语言模型的鲁棒性。", "motivation": "大型预训练视觉-语言模型（VLMs）容易受到对抗性样本（AEs）的攻击，而现有通过对抗性训练提升鲁棒性的方法主要依赖单一梯度方向扰动生成AEs，导致多样性不足，从而鲁棒性提升有限。", "method": "本文提出了一种名为ER-APT的基于进化的区域对抗性提示调优方法。该方法将梯度方法与遗传进化相结合，以生成更多样化和具有挑战性的AEs。在每次训练迭代中，首先使用传统梯度方法生成AEs，随后应用包含选择、变异和交叉的遗传进化机制来优化AEs，以确保更广泛和更具侵略性的扰动分布。最终进化的AEs用于提示调优，实现基于区域的对抗性优化，而非传统的单点对抗性提示调优。此外，还提出了一种动态损失加权方法来调整提示学习效率以兼顾准确性和鲁棒性。", "result": "在各种基准数据集上的实验评估表明，所提出的方法具有优越性，优于最先进的APT方法。", "conclusion": "通过结合梯度方法和遗传进化生成多样化对抗样本，并采用区域对抗性优化和动态损失加权，ER-APT方法显著提升了视觉-语言模型的鲁棒性，并超越了现有最先进的对抗性提示调优方法。", "translation": "大型预训练视觉-语言模型（VLMs），如CLIP，展现出令人印象深刻的泛化能力，但对对抗性样本（AEs）仍然高度脆弱。以往的工作通过对抗性训练探索了鲁棒的文本提示，在鲁棒性和泛化能力方面取得了一些改进。然而，它们主要依赖于单一梯度方向扰动（例如PGD）来生成AEs，这缺乏多样性，导致对抗性鲁棒性的改进有限。为了解决这些局限性，我们提出了一种名为ER-APT的基于进化的区域对抗性提示调优方法，该方法将梯度方法与遗传进化相结合，以生成更多样化和更具挑战性的AEs。在每次训练迭代中，我们首先使用传统的基于梯度的方法生成AEs。随后，应用包含选择、变异和交叉的遗传进化机制来优化AEs，确保更广泛和更具侵略性的扰动分布。最终进化的AEs用于提示调优，实现基于区域的对抗性优化，而非传统的单点对抗性提示调优。我们还提出了一种动态损失加权方法来调整提示学习效率，以兼顾准确性和鲁棒性。在各种基准数据集上的实验评估表明，我们提出的方法具有优越性，优于最先进的APT方法。代码已在https://github.com/jiaxiaojunQAQ/ER-APT发布。", "summary": "本文提出ER-APT，一种基于进化的区域对抗性提示调优方法，旨在增强视觉-语言模型对对抗样本的鲁棒性。该方法结合梯度和遗传进化机制生成更多样化、更具挑战性的对抗样本，并通过区域对抗性优化而非传统单点方式进行提示调优。此外，引入动态损失加权策略以平衡准确性和鲁棒性。实验证明，ER-APT在多个基准数据集上优于现有先进方法。", "keywords": "对抗性鲁棒性, 视觉-语言模型, 对抗性提示调优, 遗传进化, 对抗样本", "comments": "该论文的创新点在于将遗传进化算法引入对抗样本生成，解决了传统梯度方法生成对抗样本多样性不足的问题。结合区域对抗性优化和动态损失加权，有效提升了视觉-语言模型的鲁棒性，对VLM的安全性研究具有重要意义。"}}
{"id": "2503.12875", "pdf": "https://arxiv.org/pdf/2503.12875", "abs": "https://arxiv.org/abs/2503.12875", "authors": ["Evelyn J. Mannix", "Bartholomew A. Woodham"], "title": "An interpretable approach to automating the assessment of biofouling in video footage", "categories": ["cs.CV"], "comment": null, "summary": "Biofouling$\\unicode{x2013}$communities of organisms that grow on hard\nsurfaces immersed in water$\\unicode{x2013}$provides a pathway for the spread of\ninvasive marine species and diseases. To address this risk, international\nvessels are increasingly being obligated to provide evidence of their\nbiofouling management practices. Verification that these activities are\neffective requires underwater inspections, using divers or underwater remotely\noperated vehicles (ROVs), and the collection and analysis of large amounts of\nimagery and footage. Automated assessment using computer vision techniques can\nsignificantly streamline this process, and this work shows how this challenge\ncan be addressed efficiently and effectively using the interpretable Component\nFeatures (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation\nmodel. ComFe is able to obtain improved performance in comparison to previous\nnon-interpretable Convolutional Neural Network (CNN) methods, with\nsignificantly fewer weights and greater transparency$\\unicode{x2013}$through\nidentifying which regions of the image contribute to the classification, and\nwhich images in the training data lead to that conclusion. All code, data and\nmodel weights are publicly released.", "AI": {"title_translation": "一种可解释的视频画面生物污损自动化评估方法", "tldr": "本文提出了一种基于可解释组件特征（ComFe）和DINOv2视觉Transformer模型的方法，用于自动化评估视频中的生物污损，该方法比现有CNN方法性能更优，权重更少，并提供更高的透明度。", "motivation": "生物污损为入侵性海洋物种和疾病的传播提供了途径。国际船只越来越需要提供其生物污损管理实践的证据，而验证这些活动的有效性需要大量的水下检查和图像分析。自动化评估可以显著简化这一过程。", "method": "本研究使用可解释的组件特征（ComFe）方法结合DINOv2视觉Transformer（ViT）基础模型来自动化评估视频画面中的生物污损。ComFe方法能够识别图像中对分类有贡献的区域，以及训练数据中导致该结论的图像，从而提供更高的透明度。", "result": "与之前的非可解释卷积神经网络（CNN）方法相比，ComFe方法获得了更好的性能，且权重显著减少，透明度更高。", "conclusion": "本研究展示了如何使用可解释的ComFe方法结合DINOv2 ViT模型高效有效地解决生物污损自动化评估的挑战。该方法在性能、模型大小和可解释性方面均优于现有方法，并且所有代码、数据和模型权重均已公开发布。", "translation": "生物污损——生长在水中硬表面上的生物群落——为入侵性海洋物种和疾病的传播提供了途径。为了应对这一风险，国际船只越来越需要提供其生物污损管理实践的证据。验证这些活动的有效性需要使用潜水员或水下遥控车辆（ROV）进行水下检查，并收集和分析大量的图像和视频资料。使用计算机视觉技术进行自动化评估可以显著简化这一过程，而这项工作展示了如何使用可解释的组件特征（ComFe）方法结合DINOv2视觉Transformer（ViT）基础模型高效有效地解决这一挑战。与之前的非可解释卷积神经网络（CNN）方法相比，ComFe能够获得改进的性能，且权重显著减少，透明度更高——通过识别图像的哪些区域对分类有贡献，以及训练数据中的哪些图像导致了该结论。所有代码、数据和模型权重均已公开发布。", "summary": "本文提出了一种可解释的组件特征（ComFe）方法，结合DINOv2视觉Transformer（ViT）基础模型，以实现视频画面中生物污损的自动化评估。该方法旨在解决生物污损带来的入侵物种和疾病传播风险，并通过自动化水下检查过程来提高效率。实验结果表明，ComFe在性能上优于非可解释的卷积神经网络（CNN）方法，同时显著减少了模型权重并增强了透明度，能够明确指出图像中对分类贡献的区域以及训练数据中的关键图像。所有相关资源均已公开发布。", "keywords": "生物污损, 可解释AI, 计算机视觉, DINOv2, ComFe", "comments": "本文的创新点在于引入了可解释的组件特征（ComFe）方法，结合DINOv2 Vision Transformer模型，解决了生物污损自动化评估中的透明度问题。相较于传统的黑箱模型（如CNN），ComFe不仅提升了性能，更通过识别关键图像区域和训练样本，显著增强了模型的可解释性，这对于信任和应用AI系统至关重要。减少模型权重也意味着更高的效率和更低的部署成本。公开发布代码、数据和模型权重，将极大地促进该领域的研究和应用。"}}
{"id": "2503.12885", "pdf": "https://arxiv.org/pdf/2503.12885", "abs": "https://arxiv.org/abs/2503.12885", "authors": ["Dewei Zhou", "Mingwei Li", "Zongxin Yang", "Yi Yang"], "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.", "AI": {"title_translation": "DreamRenderer：驯服大规模文本到图像模型中的多实例属性控制", "tldr": "DreamRenderer是一种无需训练的方法，解决了大规模文本到图像模型中多实例内容控制不准确的问题，显著提高了图像成功率。", "motivation": "现有的图像条件生成方法，即使是FLUX和3DIS等最先进的模型，在准确控制多个实例（或区域）的内容时仍面临挑战，例如实例间的属性泄漏，这限制了用户控制。", "method": "我们引入了DreamRenderer，一种基于FLUX模型的无需训练的方法。它通过边界框或掩码控制每个实例的内容，同时确保整体视觉和谐。主要创新包括：1）“桥接图像令牌”用于硬文本属性绑定，利用复制的图像令牌作为桥接令牌，确保T5文本嵌入在联合注意力期间为每个实例绑定正确的视觉属性；2）“硬图像属性绑定”仅应用于关键层，通过分析FLUX，识别负责实例属性渲染的关键层，仅在这些层应用硬绑定，在其他层使用软绑定，以确保精确控制同时保持图像质量。", "result": "在COCO-POS和COCO-MIG基准测试中，DreamRenderer将图像成功率比FLUX提高了17.7%，并将GLIGEN和3DIS等布局到图像模型的性能提高了高达26.8%。", "conclusion": "DreamRenderer通过其创新的桥接图像令牌和分层属性绑定策略，有效解决了大规模文本到图像模型中多实例属性控制的挑战，显著提升了生成图像的质量和用户控制能力。", "translation": "图像条件生成方法，例如深度和Canny条件方法，已在精确图像合成方面展现出卓越能力。然而，现有模型在准确控制多个实例（或区域）的内容时仍然存在困难。即使是FLUX和3DIS等最先进的模型也面临挑战，例如实例间的属性泄漏，这限制了用户控制。为了解决这些问题，我们引入了DreamRenderer，一种基于FLUX模型的无需训练的方法。DreamRenderer使用户能够通过边界框或掩码控制每个实例的内容，同时确保整体视觉和谐。我们提出了两项关键创新：1）用于硬文本属性绑定的桥接图像令牌，它使用复制的图像令牌作为桥接令牌，以确保T5文本嵌入（仅在文本数据上预训练）在联合注意力期间为每个实例绑定正确的视觉属性；2）仅应用于关键层的硬图像属性绑定。通过我们对FLUX的分析，我们识别了负责实例属性渲染的关键层，并仅在这些层应用硬图像属性绑定，在其他层使用软绑定。这种方法确保了精确控制，同时保持了图像质量。在COCO-POS和COCO-MIG基准测试上的评估表明，DreamRenderer比FLUX的图像成功率提高了17.7%，并将GLIGEN和3DIS等布局到图像模型的性能提高了高达26.8%。项目页面：https://limuloo.github.io/DreamRenderer/。", "summary": "DreamRenderer提出了一种无需训练的方法，以解决大规模文本到图像模型中多实例内容控制不准确的问题。该方法基于FLUX模型，通过引入“桥接图像令牌”实现硬文本属性绑定，并对关键层应用“硬图像属性绑定”，从而允许用户通过边界框或掩码精确控制每个实例的属性，同时保持图像整体和谐。实验证明，DreamRenderer在多个基准测试中显著提升了图像生成质量和控制精度，超越了现有模型。", "keywords": "多实例控制, 文本到图像, 属性绑定, DreamRenderer, FLUX", "comments": "DreamRenderer的创新之处在于其无需训练的特性以及对FLUX模型深层机制的理解和利用。通过识别并针对性地处理关键层面的属性绑定，它有效地解决了现有模型在多实例属性控制上的痛点，尤其是属性泄漏问题。这种方法为文本到图像生成领域提供了一个新的、高效的解决方案，特别是在需要精细控制复杂场景时具有重要意义。其性能提升显著，显示出强大的实用价值。"}}
{"id": "2503.12886", "pdf": "https://arxiv.org/pdf/2503.12886", "abs": "https://arxiv.org/abs/2503.12886", "authors": ["Linzhou Li", "Yumeng Li", "Yanlin Weng", "Youyi Zheng", "Kun Zhou"], "title": "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for\nreconstructing photorealistic, animatable head avatars at speeds sufficient for\non-the-fly reconstruction. Unlike prior approaches that utilize linear bases\nfrom 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps\ntracked 3DMM parameters into reduced blendshape weights with an MLP, leading to\na compact set of blendshape bases. The learned compact base composition\neffectively captures essential facial details for specific individuals, and\ndoes not rely on the fixed base composition weights of 3DMM, leading to\nenhanced reconstruction quality and higher efficiency. To further expedite the\nreconstruction process, we develop a novel color initialization estimation\nmethod and a batch-parallel Gaussian rasterization process, achieving\nstate-of-the-art quality with training throughput of about 630 images per\nsecond. Moreover, we propose a local-global sampling strategy that enables\ndirect on-the-fly reconstruction, immediately reconstructing the model as video\nstreams in real time while achieving quality comparable to offline settings.\nOur source code is available at https://github.com/gapszju/RGBAvatar.", "AI": {"title_translation": "RGBAvatar：用于头部化身在线建模的简化高斯混合形状", "tldr": "RGBAvatar 提出了一种通过 MLP 将 3DMM 参数映射到简化混合形状权重的方法，实现了光真实、可动画头部化身的实时高效率重建。", "motivation": "现有方法使用 3DMM 的线性基来建模高斯混合形状，但效率和质量有待提高。本文旨在实现光真实、可动画头部化身的实时（即时）重建。", "method": "RGBAvatar 方法通过 MLP 将跟踪的 3DMM 参数映射到简化的混合形状权重，形成紧凑的混合形状基。为了加速重建过程，开发了一种新颖的颜色初始化估计方法和一个批处理并行高斯光栅化过程，并提出了一种局部-全局采样策略以实现直接的实时重建。", "result": "该方法实现了增强的重建质量和更高的效率，训练吞吐量约为每秒 630 张图像，并且在实时重建中达到了与离线设置相当的质量。", "conclusion": "RGBAvatar 成功实现了光真实、可动画头部化身的实时在线建模，其重建质量和效率均达到了先进水平。", "translation": "我们提出了简化高斯混合形状化身（RGBAvatar），这是一种以足以进行即时重建的速度重建光真实、可动画头部化身的方法。与之前利用 3D 可变形模型（3DMM）的线性基来建模高斯混合形状的方法不同，我们的方法通过多层感知器（MLP）将跟踪到的 3DMM 参数映射到简化的混合形状权重中，从而形成一组紧凑的混合形状基。学习到的紧凑基组成有效地捕捉了特定个体的基本面部细节，并且不依赖于 3DMM 的固定基组成权重，从而提高了重建质量和效率。为了进一步加快重建过程，我们开发了一种新颖的颜色初始化估计方法和一个批处理并行高斯光栅化过程，实现了每秒约 630 张图像的训练吞吐量，并达到了最先进的质量。此外，我们提出了一种局部-全局采样策略，可以直接进行即时重建，在视频流中实时重建模型，同时达到与离线设置相当的质量。我们的源代码可在 https://github.com/gapszju/RGBAvatar 获取。", "summary": "RGBAvatar 是一种用于实时重建光真实、可动画头部化身的方法。它通过 MLP 将 3DMM 参数映射到简化的、紧凑的高斯混合形状权重，有效捕捉面部细节，提高了重建质量和效率。该方法还引入了新颖的颜色初始化估计和批处理并行高斯光栅化，以及局部-全局采样策略，实现了高吞吐量和与离线设置相当的实时重建质量。", "keywords": "头部化身, 高斯混合形状, 实时重建, 3DMM, MLP", "comments": "RGBAvatar 的创新之处在于其通过 MLP 将 3DMM 参数映射到简化高斯混合形状，从而实现了更紧凑和高效的面部细节捕捉，摆脱了对传统 3DMM 固定基的依赖。其实现的实时重建能力和高吞吐量对于在线应用具有重要意义，是该领域的显著进步。"}}
{"id": "2503.12888", "pdf": "https://arxiv.org/pdf/2503.12888", "abs": "https://arxiv.org/abs/2503.12888", "authors": ["Siyuan Yao", "Yang Guo", "Yanyang Yan", "Wenqi Ren", "Xiaochun Cao"], "title": "UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network", "categories": ["cs.CV"], "comment": "14 pages,11 figures,references added", "summary": "Transformer-based trackers have achieved promising success and become the\ndominant tracking paradigm due to their accuracy and efficiency. Despite the\nsubstantial progress, most of the existing approaches tackle object tracking as\na deterministic coordinate regression problem, while the target localization\nuncertainty has been greatly overlooked, which hampers trackers' ability to\nmaintain reliable target state prediction in challenging scenarios. To address\nthis issue, we propose UncTrack, a novel uncertainty-aware transformer tracker\nthat predicts the target localization uncertainty and incorporates this\nuncertainty information for accurate target state inference. Specifically,\nUncTrack utilizes a transformer encoder to perform feature interaction between\ntemplate and search images. The output features are passed into an\nuncertainty-aware localization decoder (ULD) to coarsely predict the\ncorner-based localization and the corresponding localization uncertainty. Then\nthe localization uncertainty is sent into a prototype memory network (PMN) to\nexcavate valuable historical information to identify whether the target state\nprediction is reliable or not. To enhance the template representation, the\nsamples with high confidence are fed back into the prototype memory bank for\nmemory updating, making the tracker more robust to challenging appearance\nvariations. Extensive experiments demonstrate that our method outperforms other\nstate-of-the-art methods. Our code is available at\nhttps://github.com/ManOfStory/UncTrack.", "AI": {"title_translation": "UncTrack：基于不确定性感知的原型记忆网络实现可靠视觉目标跟踪", "tldr": "UncTrack是一个不确定性感知的Transformer跟踪器，通过预测和利用定位不确定性以及原型记忆网络来提高目标跟踪的可靠性，超越了现有SOTA方法。", "motivation": "现有的Transformer跟踪器将目标跟踪视为确定性坐标回归问题，忽视了目标定位不确定性，导致在复杂场景下难以维持可靠的目标状态预测。", "method": "本文提出了UncTrack，一个新颖的不确定性感知的Transformer跟踪器。它利用Transformer编码器进行模板和搜索图像的特征交互，并将输出特征传递给不确定性感知的定位解码器（ULD）以粗略预测基于角点的定位及其不确定性。随后，定位不确定性被送入原型记忆网络（PMN）以挖掘有价值的历史信息来判断目标状态预测的可靠性。同时，高置信度样本会被反馈到原型记忆库进行更新，以增强跟踪器对挑战性外观变化的鲁棒性。", "result": "大量实验表明，我们的方法UncTrack优于其他最先进的视觉目标跟踪方法。", "conclusion": "UncTrack通过显式预测和利用目标定位不确定性，并结合原型记忆网络来增强模板表示，有效提高了视觉目标跟踪的可靠性和鲁棒性，并在性能上超越了现有SOTA方法。", "translation": "标题：UncTrack：基于不确定性感知的原型记忆网络实现可靠视觉目标跟踪\n\n摘要：基于Transformer的跟踪器因其准确性和效率取得了可喜的成功，并成为主导的跟踪范式。尽管取得了实质性进展，但大多数现有方法将目标跟踪视为确定性坐标回归问题，而目标定位不确定性却被大大忽视，这阻碍了跟踪器在挑战性场景中保持可靠目标状态预测的能力。为了解决这个问题，我们提出了UncTrack，一种新颖的不确定性感知的Transformer跟踪器，它预测目标定位不确定性并将此不确定性信息用于准确的目标状态推断。具体来说，UncTrack利用Transformer编码器执行模板和搜索图像之间的特征交互。输出特征被传递到不确定性感知的定位解码器（ULD）以粗略预测基于角点的定位和相应的定位不确定性。然后将定位不确定性发送到原型记忆网络（PMN）以挖掘有价值的历史信息，以识别目标状态预测是否可靠。为了增强模板表示，高置信度样本被反馈到原型记忆库进行记忆更新，使跟踪器对挑战性的外观变化更具鲁棒性。大量实验表明，我们的方法优于其他最先进的方法。我们的代码可在 https://github.com/ManOfStory/UncTrack 获取。", "summary": "UncTrack是一种新颖的不确定性感知的Transformer跟踪器，旨在解决现有方法忽略目标定位不确定性的问题。它通过Transformer编码器进行特征交互，并使用不确定性感知的定位解码器预测目标位置及其不确定性。此外，原型记忆网络利用这些不确定性信息和历史数据来评估预测可靠性，并通过高置信度样本更新记忆库，从而增强跟踪器在复杂场景下的鲁棒性和准确性。实验证明UncTrack超越了现有SOTA方法。", "keywords": "视觉目标跟踪, 不确定性感知, Transformer, 原型记忆网络, 可靠性", "comments": "本文创新性地将不确定性预测引入Transformer跟踪器中，解决了传统确定性回归方法的局限性。通过引入不确定性感知的定位解码器和原型记忆网络，UncTrack不仅提高了定位精度，还增强了跟踪器在复杂多变环境下的可靠性和鲁棒性，对于提升视觉目标跟踪的实用性具有重要意义。"}}
{"id": "2503.12905", "pdf": "https://arxiv.org/pdf/2503.12905", "abs": "https://arxiv.org/abs/2503.12905", "authors": ["Yuanbin Qian", "Shuhan Ye", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian", "Jiafei Wu"], "title": "UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by AAAI 2025", "summary": "Video anomaly detection plays a significant role in intelligent surveillance\nsystems. To enhance model's anomaly recognition ability, previous works have\ntypically involved RGB, optical flow, and text features. Recently, dynamic\nvision sensors (DVS) have emerged as a promising technology, which capture\nvisual information as discrete events with a very high dynamic range and\ntemporal resolution. It reduces data redundancy and enhances the capture\ncapacity of moving objects compared to conventional camera. To introduce this\nrich dynamic information into the surveillance field, we created the first DVS\nvideo anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this\nnew data modality, a multi-scale spiking fusion network (MSF) is designed based\non spiking neural networks (SNNs). This work explores the potential application\nof dynamic information from event data in video anomaly detection. Our\nexperiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and\nits superior performance compared to other models, establishing a new baseline\nfor SNN-based weakly supervised video anomaly detection.", "AI": {"title_translation": "UCF-Crime-DVS：一种用于脉冲神经网络视频异常检测的新型基于事件的数据集", "tldr": "提出了首个DVS视频异常检测数据集UCF-Crime-DVS和一个基于SNN的多尺度脉冲融合网络（MSF），并在视频异常检测中取得了SNN基线的新突破。", "motivation": "现有的视频异常检测方法主要依赖RGB、光流和文本特征，但动态视觉传感器（DVS）作为一种新兴技术，能以高动态范围和时间分辨率捕获离散事件，减少数据冗余并增强移动对象捕获能力。为了将这种丰富的动态信息引入监控领域，需要一个专门的DVS视频异常检测基准数据集。", "method": "1. 创建了首个DVS视频异常检测基准数据集UCF-Crime-DVS。 2. 设计了一个基于脉冲神经网络（SNNs）的多尺度脉冲融合网络（MSF），以充分利用DVS事件数据的新颖模态。", "result": "实验证明了该框架在UCF-Crime-DVS数据集上的有效性，并且其性能优于其他模型，为基于SNN的弱监督视频异常检测建立了新的基线。", "conclusion": "该工作探索了事件数据中的动态信息在视频异常检测中的潜在应用，并成功创建了首个DVS视频异常检测基准数据集和相应的SNN模型，为该领域树立了新的性能标杆。", "translation": "视频异常检测在智能监控系统中扮演着重要角色。为了增强模型的异常识别能力，以往的工作通常涉及RGB、光流和文本特征。最近，动态视觉传感器（DVS）作为一种有前景的技术出现，它以非常高的动态范围和时间分辨率将视觉信息捕获为离散事件。与传统相机相比，它减少了数据冗余并增强了移动对象的捕获能力。为了将这种丰富的动态信息引入监控领域，我们创建了第一个DVS视频异常检测基准数据集，即UCF-Crime-DVS。为了充分利用这种新的数据模态，我们设计了一个基于脉冲神经网络（SNNs）的多尺度脉冲融合网络（MSF）。这项工作探索了事件数据中的动态信息在视频异常检测中的潜在应用。我们的实验证明了我们的框架在UCF-Crime-DVS上的有效性及其优于其他模型的性能，为基于SNN的弱监督视频异常检测建立了新的基线。", "summary": "本文针对智能监控中的视频异常检测，引入了动态视觉传感器（DVS）产生的事件数据，并创建了首个DVS视频异常检测基准数据集UCF-Crime-DVS。为有效利用DVS数据，提出了一种基于脉冲神经网络（SNNs）的多尺度脉冲融合网络（MSF）。实验结果表明，该框架在UCF-Crime-DVS数据集上表现出色，并为基于SNN的弱监督视频异常检测设定了新的基线。", "keywords": "视频异常检测, 动态视觉传感器, 事件数据, 脉冲神经网络, 数据集", "comments": "这篇论文通过引入DVS事件数据和SNN，为视频异常检测领域带来了创新。DVS的高时间分辨率和低数据冗余特性有望克服传统RGB数据的局限性。创建首个DVS视频异常检测数据集UCF-Crime-DVS是其重要贡献，填补了该领域的数据空白。结合SNN的设计也体现了对新兴计算范式的探索，为未来低功耗、高效率的异常检测系统提供了潜力。"}}
{"id": "2503.12910", "pdf": "https://arxiv.org/pdf/2503.12910", "abs": "https://arxiv.org/abs/2503.12910", "authors": ["Jingyi Yuan", "Pengyu Jie", "Junyin Zhang", "Ziao Li", "Chenqiang Gao"], "title": "MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal\nparadigm for identifying defects in unseen categories without requiring target\nsamples in training phase. However, existing ZSAD methods struggle with the\nboundary of small and complex defects due to insufficient representations. Most\nof them use the single manually designed prompts, failing to work for diverse\nobjects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based\nCLIP framework which explores the efficacy of multi-form prompts for zero-shot\nindustrial anomaly detection. We employ an image to text prompting(I2TP)\nmechanism to better represent the object in the image. MFP-CLIP enhances\nperception to multi-scale and complex anomalies by self prompting(SP) and a\nmulti-patch feature aggregation(MPFA) module. To precisely localize defects, we\nintroduce the mask prompting(MP) module to guide model to focus on potential\nanomaly regions. Extensive experiments are conducted on two wildly used\nindustrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating\nMFP-CLIP's superiority in ZSAD.", "AI": {"title_translation": "MFP-CLIP：探索多形式提示在零样本工业异常检测中的有效性", "tldr": "MFP-CLIP提出一种基于CLIP的多形式提示框架，通过多模态提示和特征聚合，解决了现有零样本工业异常检测方法对小而复杂缺陷表示不足和定位不精确的问题，并在MVTecAD和VisA基准上表现出优越性。", "motivation": "现有零样本异常检测（ZSAD）方法由于表示不足，难以处理小而复杂的缺陷边界，且大多使用单一手动设计的提示，无法适用于多样化的对象和异常。", "method": "本文提出了MFP-CLIP，一个新颖的基于提示的CLIP框架，探索多形式提示的有效性。它采用图像到文本提示（I2TP）机制以更好地表示图像中的对象。通过自提示（SP）和多补丁特征聚合（MPFA）模块增强对多尺度和复杂异常的感知。此外，引入掩码提示（MP）模块以精确地定位缺陷。", "result": "在MVTecAD和VisA两个广泛使用的工业异常检测基准上进行了大量实验，证明了MFP-CLIP在零样本异常检测（ZSAD）中的优越性。", "conclusion": "MFP-CLIP通过结合多形式提示（如I2TP、SP、MPFA和MP）有效地解决了零样本工业异常检测中对小而复杂缺陷的表示不足和精确缺陷定位的挑战，并在主要基准上展现了卓越的性能。", "translation": "近年来，零样本异常检测（ZSAD）已成为一种关键范式，用于识别未经训练的类别中的缺陷，而无需在训练阶段提供目标样本。然而，现有的ZSAD方法由于表示不足，难以处理小而复杂缺陷的边界。它们大多使用单一手动设计的提示，无法适用于多样化的对象和异常。在本文中，我们提出了MFP-CLIP，一个新颖的基于提示的CLIP框架，它探索了多形式提示在零样本工业异常检测中的有效性。我们采用图像到文本提示（I2TP）机制，以更好地表示图像中的对象。MFP-CLIP通过自提示（SP）和多补丁特征聚合（MPFA）模块增强对多尺度和复杂异常的感知。为了精确地定位缺陷，我们引入了掩码提示（MP）模块，以引导模型关注潜在的异常区域。在两个广泛使用的工业异常检测基准MVTecAD和VisA上进行了大量实验，证明了MFP-CLIP在ZSAD中的优越性。", "summary": "本文提出了MFP-CLIP，一个基于CLIP的多形式提示框架，旨在解决现有零样本工业异常检测方法在处理小而复杂缺陷时表示不足和定位不精确的问题。MFP-CLIP通过图像到文本提示（I2TP）、自提示（SP）和多补丁特征聚合（MPFA）增强对多样化异常的感知，并利用掩码提示（MP）进行精确缺陷定位。在MVTecAD和VisA数据集上的实验证明了其在零样本异常检测方面的优越性。", "keywords": "零样本异常检测, 工业异常检测, 多形式提示, CLIP, 缺陷定位", "comments": "本文通过引入多形式提示机制，特别是结合了图像到文本、自提示、多补丁特征聚合和掩码提示，有效提升了CLIP模型在零样本工业异常检测中对复杂和多尺度缺陷的识别与定位能力，是提示工程在视觉领域应用的一个创新。"}}
{"id": "2503.12912", "pdf": "https://arxiv.org/pdf/2503.12912", "abs": "https://arxiv.org/abs/2503.12912", "authors": ["Bin Tang", "Keqi Pan", "Miao Zheng", "Ning Zhou", "Jialu Sui", "Dandan Zhu", "Cheng-Long Deng", "Shu-Guang Kuai"], "title": "Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 6 figures, AAAI 2025 Oral", "summary": "In recent years, predicting Big Five personality traits from multimodal data\nhas received significant attention in artificial intelligence (AI). However,\nexisting computational models often fail to achieve satisfactory performance.\nPsychological research has shown a strong correlation between pose and\npersonality traits, yet previous research has largely ignored pose data in\ncomputational models. To address this gap, we develop a novel multimodal\ndataset that incorporates full-body pose data. The dataset includes video\nrecordings of 287 participants completing a virtual interview with 36\nquestions, along with self-reported Big Five personality scores as labels. To\neffectively utilize this multimodal data, we introduce the Psychology-Inspired\nNetwork (PINet), which consists of three key modules: Multimodal Feature\nAwareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed\nModality Correlation Loss (PIMC Loss). The MFA module leverages the Vision\nMamba Block to capture comprehensive visual features related to personality,\nwhile the MFI module efficiently fuses the multimodal features. The PIMC Loss,\ngrounded in psychological theory, guides the model to emphasize different\nmodalities for different personality dimensions. Experimental results show that\nthe PINet outperforms several state-of-the-art baseline models. Furthermore,\nthe three modules of PINet contribute almost equally to the model's overall\nperformance. Incorporating pose data significantly enhances the model's\nperformance, with the pose modality ranking mid-level in importance among the\nfive modalities. These findings address the existing gap in personality-related\ndatasets that lack full-body pose data and provide a new approach for improving\nthe accuracy of personality prediction models, highlighting the importance of\nintegrating psychological insights into AI frameworks.", "AI": {"title_translation": "姿态作为一种模态：一种心理学启发式网络，用于人格识别与新型多模态数据集", "tldr": "提出一个包含全身姿态数据的新多模态数据集和心理学启发式网络（PINet），用于提高大五人格特质预测的准确性，并证明姿态数据的重要性。", "motivation": "现有计算模型在预测大五人格特质方面表现不佳，且以往研究忽视了姿态数据与人格特质的强相关性。缺乏包含全身姿态数据的人格相关数据集。", "method": "开发了一个新的多模态数据集，包含287名参与者的虚拟面试视频和自报告的大五人格分数，以及全身姿态数据。提出了心理学启发式网络（PINet），包含多模态特征感知（MFA）、多模态特征交互（MFI）和心理学信息模态相关损失（PIMC Loss）三个模块。MFA利用Vision Mamba Block捕获视觉特征，MFI融合多模态特征，PIMC Loss根据心理学理论指导模型强调不同模态。", "result": "PINet优于多个最先进的基线模型。PINet的三个模块对整体性能贡献几乎相等。姿态数据显著增强了模型性能，其重要性在五种模态中居中。", "conclusion": "本研究解决了人格相关数据集中缺乏全身姿态数据的空白，并为提高人格预测模型准确性提供了新方法，强调了将心理学见解整合到AI框架中的重要性。", "translation": "近年来，从多模态数据预测大五人格特质在人工智能（AI）领域受到了广泛关注。然而，现有的计算模型往往未能达到令人满意的性能。心理学研究表明，姿态与人格特质之间存在很强的相关性，但以往的研究在计算模型中很大程度上忽略了姿态数据。为了弥补这一空白，我们开发了一个包含全身姿态数据的新型多模态数据集。该数据集包括287名参与者完成36个问题的虚拟面试的视频记录，以及自报告的大五人格分数作为标签。为了有效利用这些多模态数据，我们引入了心理学启发式网络（PINet），它由三个关键模块组成：多模态特征感知（MFA）、多模态特征交互（MFI）和心理学信息模态相关损失（PIMC Loss）。MFA模块利用Vision Mamba Block捕获与人格相关的综合视觉特征，而MFI模块有效地融合了多模态特征。PIMC Loss以心理学理论为基础，指导模型针对不同人格维度强调不同的模态。实验结果表明，PINet优于多个最先进的基线模型。此外，PINet的三个模块对模型的整体性能贡献几乎相等。结合姿态数据显著提高了模型的性能，其中姿态模态在五种模态中的重要性排名居中。这些发现解决了人格相关数据集中缺乏全身姿态数据的现有空白，并为提高人格预测模型的准确性提供了一种新方法，强调了将心理学见解整合到AI框架中的重要性。", "summary": "本文针对现有大五人格特质预测模型性能不佳且忽视姿态数据的问题，提出了一个包含全身姿态数据的新型多模态数据集和心理学启发式网络（PINet）。PINet包含多模态特征感知、特征交互和心理学信息模态相关损失模块，旨在有效融合多模态数据并利用心理学见解。实验证明，PINet显著优于现有模型，并证实了姿态数据在人格预测中的重要性，填补了数据集空白。", "keywords": "人格识别, 姿态, 多模态, 心理学启发式网络, 大五人格", "comments": "本文的创新点在于首次将全身姿态数据引入人格识别领域，并构建了相应的多模态数据集。其提出的PINet模型通过结合心理学理论指导损失函数，有效利用了多模态信息，提升了预测性能。这为未来人格识别研究提供了新的方向，并强调了跨学科融合的重要性。"}}
{"id": "2503.12914", "pdf": "https://arxiv.org/pdf/2503.12914", "abs": "https://arxiv.org/abs/2503.12914", "authors": ["Zhuoqun Su", "Huimin Lu", "Shuaifeng Jiao", "Junhao Xiao", "Yaonan Wang", "Xieyuanli Chen"], "title": "Efficient Multimodal 3D Object Detector via Instance-Level Contrastive Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal 3D object detectors leverage the strengths of both geometry-aware\nLiDAR point clouds and semantically rich RGB images to enhance detection\nperformance. However, the inherent heterogeneity between these modalities,\nincluding unbalanced convergence and modal misalignment, poses significant\nchallenges. Meanwhile, the large size of the detection-oriented feature also\nconstrains existing fusion strategies to capture long-range dependencies for\nthe 3D detection tasks. In this work, we introduce a fast yet effective\nmultimodal 3D object detector, incorporating our proposed Instance-level\nContrastive Distillation (ICD) framework and Cross Linear Attention Fusion\nModule (CLFM). ICD aligns instance-level image features with LiDAR\nrepresentations through object-aware contrastive distillation, ensuring\nfine-grained cross-modal consistency. Meanwhile, CLFM presents an efficient and\nscalable fusion strategy that enhances cross-modal global interactions within\nsizable multimodal BEV features. Extensive experiments on the KITTI and\nnuScenes 3D object detection benchmarks demonstrate the effectiveness of our\nmethods. Notably, our 3D object detector outperforms state-of-the-art (SOTA)\nmethods while achieving superior efficiency. The implementation of our method\nhas been released as open-source at: https://github.com/nubot-nudt/ICD-Fusion.", "AI": {"title_translation": "实例级对比蒸馏的高效多模态3D目标检测器", "tldr": "本文提出了一种名为ICD-Fusion的快速有效的多模态3D目标检测器，通过实例级对比蒸馏（ICD）解决模态异质性问题，并通过交叉线性注意力融合模块（CLFM）增强跨模态全局交互，在KITTI和nuScenes基准上实现了SOTA性能和卓越效率。", "motivation": "现有多模态3D目标检测器面临挑战，包括模态间的固有异质性（如不平衡收敛和模态未对齐）以及面向检测的特征尺寸过大限制了现有融合策略捕获3D检测任务的远程依赖。", "method": "本文引入了一种新的多模态3D目标检测器，包含两个核心组件：1. 实例级对比蒸馏（ICD）框架：通过面向对象的对比蒸馏，将实例级图像特征与LiDAR表示对齐，确保细粒度的跨模态一致性。2. 交叉线性注意力融合模块（CLFM）：一种高效且可扩展的融合策略，用于增强大型多模态BEV特征内的跨模态全局交互。", "result": "在KITTI和nuScenes 3D目标检测基准上的大量实验证明了所提方法的有效性。我们的3D目标检测器在实现卓越效率的同时，性能优于最先进（SOTA）的方法。", "conclusion": "该研究通过引入实例级对比蒸馏（ICD）框架和交叉线性注意力融合模块（CLFM），成功解决了多模态3D目标检测中的模态异质性和特征融合挑战，实现了高效且性能卓越的3D目标检测器。", "translation": "多模态3D目标检测器利用几何感知的LiDAR点云和语义丰富的RGB图像的优势来提高检测性能。然而，这些模态之间固有的异质性，包括不平衡的收敛和模态未对齐，带来了重大挑战。同时，面向检测的特征的巨大尺寸也限制了现有融合策略捕获3D检测任务的远程依赖。在这项工作中，我们引入了一种快速而有效的多模态3D目标检测器，其中包含我们提出的实例级对比蒸馏（ICD）框架和交叉线性注意力融合模块（CLFM）。ICD通过面向对象的对比蒸馏将实例级图像特征与LiDAR表示对齐，确保细粒度的跨模态一致性。同时，CLFM提出了一种高效且可扩展的融合策略，增强了大型多模态BEV特征中的跨模态全局交互。在KITTI和nuScenes 3D目标检测基准上的大量实验证明了我们方法的有效性。值得注意的是，我们的3D目标检测器在实现卓越效率的同时，性能优于最先进（SOTA）的方法。我们方法的实现已开源：https://github.com/nubot-nudt/ICD-Fusion。", "summary": "本文提出了一种高效的多模态3D目标检测器，旨在解决LiDAR点云和RGB图像融合中存在的模态异质性及大尺寸特征导致的长程依赖捕获受限问题。该检测器包含实例级对比蒸馏（ICD）框架，用于实现细粒度的跨模态一致性；以及交叉线性注意力融合模块（CLFM），以高效地增强大型多模态BEV特征中的全局交互。在KITTI和nuScenes基准上的实验结果表明，该方法在性能和效率上均超越了现有最先进的方法。", "keywords": "多模态3D目标检测, 对比蒸馏, LiDAR, RGB, 特征融合", "comments": "该论文的创新之处在于提出了ICD和CLFM两个特定的解决方案，分别用于解决多模态3D检测中模态异质性导致的精细对齐问题以及大尺寸特征处理中的高效全局融合问题。其在实现SOTA性能的同时显著提升了效率，这对于实际应用具有重要意义。此外，开源实现也促进了研究社区的进一步发展和应用。"}}
{"id": "2503.12927", "pdf": "https://arxiv.org/pdf/2503.12927", "abs": "https://arxiv.org/abs/2503.12927", "authors": ["Huangwei Chen", "Zhu Zhu", "Zhenyu Yan", "Yifei Chen", "Mingyang Ding", "Chenlei Li", "Feiwei Qin"], "title": "MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification Assisted with Textual Description Generation", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB.", "AI": {"title_translation": "MMLNB：多模态学习辅助文本描述生成用于神经母细胞瘤亚型分类", "tldr": "MMLNB是一个多模态学习模型，结合病理图像和生成的文本描述，提高了神经母细胞瘤亚型分类的准确性和可解释性。", "motivation": "神经母细胞瘤（NB）是儿童癌症死亡的主要原因之一，其显著的组织病理学变异性需要精确的亚型分类以实现准确的预后和治疗。然而，传统的诊断方法依赖于主观评估，效率低下且结果不一致。", "method": "本文提出了MMLNB，一个多模态学习（MML）模型，它整合了病理图像和生成的文本描述。该方法分为两个阶段：首先，微调一个视觉-语言模型（VLM）以生成病理学感知的文本；其次，利用经过微调的VLM生成文本描述，并通过双分支架构独立提取视觉和文本特征，这些特征通过渐进式鲁棒多模态融合（PRMF）模块进行融合，以确保训练的稳定性。", "result": "实验结果表明，MMLNB模型比单一模态模型更准确。消融研究进一步证明了多模态融合、微调和PRMF机制的重要性。", "conclusion": "这项研究为数字病理学创建了一个可扩展的AI驱动框架，显著提升了神经母细胞瘤亚型分类的可靠性和可解释性。", "translation": "神经母细胞瘤 (NB) 是导致儿童癌症死亡的主要原因，表现出显著的组织病理学变异性，因此需要精确的亚型分类以实现准确的预后和治疗。传统的诊断方法依赖于主观评估，耗时且不一致。为了解决这些挑战，我们引入了 MMLNB，一个多模态学习 (MML) 模型，它将病理图像与生成的文本描述相结合，以提高分类准确性和可解释性。该方法遵循两阶段过程。首先，我们对一个视觉-语言模型 (VLM) 进行微调，以增强病理学感知的文本生成。其次，经过微调的 VLM 生成文本描述，并使用双分支架构独立提取视觉和文本特征。这些特征通过渐进式鲁棒多模态融合 (PRMF) 模块进行融合，以实现稳定的训练。实验结果表明，MMLNB 模型比单模态模型更准确。消融研究证明了多模态融合、微调和 PRMF 机制的重要性。这项研究为数字病理学创建了一个可扩展的 AI 驱动框架，增强了 NB 亚型分类的可靠性和可解释性。我们的源代码可在 https://github.com/HovChen/MMLNB 获取。", "summary": "本文介绍了MMLNB，一个新颖的多模态学习模型，旨在提高神经母细胞瘤（NB）亚型分类的准确性和可解释性。为了克服传统诊断方法的主观性和不一致性，MMLNB模型创新性地将病理图像与AI生成的文本描述相结合。其核心是一个两阶段过程：首先，对一个视觉-语言模型（VLM）进行微调，使其能够生成与病理学相关的文本；随后，该微调后的VLM生成文本描述，并通过一个双分支架构独立提取视觉和文本特征，最后通过渐进式鲁棒多模态融合（PRMF）模块进行稳定融合。实验结果验证了MMLNB模型优于单一模态模型的准确性，并且消融研究强调了多模态融合、微调和PRMF机制的关键作用，为数字病理学提供了一个可扩展的AI框架。", "keywords": "神经母细胞瘤, 多模态学习, 亚型分类, 视觉-语言模型, 数字病理学", "comments": "该论文通过整合生成的文本描述与病理图像进行神经母细胞瘤亚型分类，提出了一种创新方法，有效解决了传统诊断方法的主观性问题。其两阶段过程，特别是VLM的微调和用于鲁棒融合的PRMF模块，是关键的创新点。这项工作通过提升准确性和可解释性，对数字病理学做出了重要贡献，提供了一个可扩展的AI框架。"}}
{"id": "2503.12929", "pdf": "https://arxiv.org/pdf/2503.12929", "abs": "https://arxiv.org/abs/2503.12929", "authors": ["Xuying Zhang", "Yupeng Zhou", "Kai Wang", "Yikai Wang", "Zhen Li", "Xiuli Shao", "Daquan Zhou", "Qibin Hou", "Ming-Ming Cheng"], "title": "AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.\nHowever, existing works still struggle to maintain consistency between the\ngenerated views and the input views, especially when there is a significant\ncamera pose difference, leading to poor-quality 3D geometries and textures. We\nattribute this issue to their treatment of all target views with equal priority\naccording to our empirical observation that the target views closer to the\ninput views exhibit higher fidelity. With this inspiration, we propose\nAR-1-to-3, a novel next-view prediction paradigm based on diffusion models that\nfirst generates views close to the input views, which are then utilized as\ncontextual information to progressively synthesize farther views. To encode the\ngenerated view subsequences as local and global conditions for the next-view\nprediction, we accordingly develop a stacked local feature encoding strategy\n(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).\nExtensive experiments demonstrate that our method significantly improves the\nconsistency between the generated views and the input views, producing\nhigh-fidelity 3D assets.", "AI": {"title_translation": "AR-1-to-3：通过下一视角预测实现单图到一致性三维物体生成", "tldr": "AR-1-to-3提出了一种新的下一视角预测扩散模型范式，通过逐步生成视图来解决单图到3D生成中视图一致性差的问题，从而生成高质量的3D资产。", "motivation": "现有视角合成（NVS）方法在生成视图与输入视图之间难以保持一致性，尤其是在相机姿态差异显著时，这导致了低质量的三维几何和纹理。作者认为这是因为现有方法平等对待所有目标视图，而实际上更接近输入视图的目标视图具有更高的保真度。", "method": "本文提出了AR-1-to-3，一种基于扩散模型的新型下一视角预测范式。它首先生成接近输入视图的视图，然后将这些视图作为上下文信息逐步合成更远的视图。为了将生成的视图子序列编码为下一视角预测的局部和全局条件，本文相应地开发了堆叠局部特征编码策略（Stacked-LE）和基于LSTM的全局特征编码策略（LSTM-GE）。", "result": "实验证明，该方法显著提高了生成视图与输入视图之间的一致性，生成了高保真度的三维资产。", "conclusion": "AR-1-to-3通过其独特的下一视角预测范式和特征编码策略，有效解决了单图到三维生成中视图一致性差的问题，从而能够生成高质量的三维模型。", "translation": "新颖视角合成（NVS）是图像到三维创建的基石。然而，现有工作在生成视图与输入视图之间保持一致性方面仍然存在困难，尤其是在相机姿态差异显著时，这导致了低质量的三维几何和纹理。根据我们的经验观察，我们认为这个问题源于它们平等对待所有目标视图，因为我们发现更接近输入视图的目标视图表现出更高的保真度。受此启发，我们提出了AR-1-to-3，一种基于扩散模型的新型下一视角预测范式，它首先生成接近输入视图的视图，然后将这些视图用作上下文信息，逐步合成更远的视图。为了将生成的视图子序列编码为下一视角预测的局部和全局条件，我们相应地开发了堆叠局部特征编码策略（Stacked-LE）和基于LSTM的全局特征编码策略（LSTM-GE）。大量实验表明，我们的方法显著提高了生成视图与输入视图之间的一致性，生成了高保真度的三维资产。", "summary": "本文提出了AR-1-to-3，一种基于扩散模型的新型下一视角预测范式，旨在解决单图到三维生成中现有方法在视图一致性上的不足。该方法通过优先生成接近输入视图的视图，并将其作为上下文逐步合成更远的视图，从而提高了生成视图与输入视图之间的一致性。为实现这一目标，作者开发了堆叠局部特征编码（Stacked-LE）和基于LSTM的全局特征编码（LSTM-GE）策略。实验结果表明，AR-1-to-3能够显著提升生成视图的一致性，并生成高质量的三维资产。", "keywords": "视角合成, 3D生成, 扩散模型, 视图一致性, 下一视角预测", "comments": "该论文通过提出一种新颖的渐进式下一视角预测范式，有效解决了单图到3D生成中视图一致性这一关键挑战。其创新点在于认识到不同距离视图的生成优先级，并设计了相应的局部和全局特征编码策略来利用这种渐进式信息，这对于提升生成3D资产的质量和保真度具有重要意义。"}}
{"id": "2503.12935", "pdf": "https://arxiv.org/pdf/2503.12935", "abs": "https://arxiv.org/abs/2503.12935", "authors": ["Guoliang Xu", "Jianqin Yin", "Ren Zhang", "Yonghao Dang", "Feng Zhou", "Bo Yu"], "title": "L2HCount:Generalizing Crowd Counting from Low to High Crowd Density via Density Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Since COVID-19, crowd-counting tasks have gained wide applications. While\nsupervised methods are reliable, annotation is more challenging in high-density\nscenes due to small head sizes and severe occlusion, whereas it's simpler in\nlow-density scenes. Interestingly, can we train the model in low-density scenes\nand generalize it to high-density scenes? Therefore, we propose a low- to\nhigh-density generalization framework (L2HCount) that learns the pattern\nrelated to high-density scenes from low-density ones, enabling it to generalize\nwell to high-density scenes. Specifically, we first introduce a High-Density\nSimulation Module and a Ground-Truth Generation Module to construct fake\nhigh-density images along with their corresponding ground-truth crowd\nannotations respectively by image-shifting technique, effectively simulating\nhigh-density crowd patterns. However, the simulated images have two issues:\nimage blurring and loss of low-density image characteristics. Therefore, we\nsecond propose a Head Feature Enhancement Module to extract clear features in\nthe simulated high-density scene. Third, we propose a Dual-Density Memory\nEncoding Module that uses two crowd memories to learn scene-specific patterns\nfrom low- and simulated high-density scenes, respectively. Extensive\nexperiments on four challenging datasets have shown the promising performance\nof L2HCount.", "AI": {"title_translation": "L2HCount：通过密度模拟将人群计数从低密度推广到高密度", "tldr": "L2HCount提出一种从低密度到高密度场景泛化人群计数的方法，通过模拟高密度图像和增强头部特征来解决高密度场景标注困难的问题，并在多个数据集上表现良好。", "motivation": "由于COVID-19，人群计数任务得到广泛应用。然而，高密度场景下由于头部尺寸小和严重遮挡，监督方法的标注更具挑战性，而低密度场景则相对简单。本文旨在解决在高密度场景下标注困难的问题，探索能否在低密度场景训练模型并泛化到高密度场景。", "method": "提出了一种从低密度到高密度泛化框架L2HCount。该框架包括：1. 高密度模拟模块和真值生成模块：通过图像平移技术构建模拟高密度图像及其对应的真值标注。2. 头部特征增强模块：解决模拟图像的模糊和低密度特征丢失问题，提取清晰特征。3. 双密度记忆编码模块：使用两个人群记忆分别学习低密度和模拟高密度场景的特定模式。", "result": "在四个具有挑战性的数据集上进行了广泛实验，结果显示L2HCount表现出良好的性能。", "conclusion": "L2HCount框架能够有效地将人群计数模型从低密度场景泛化到高密度场景，解决了高密度场景标注困难的问题，并取得了有前景的性能。", "translation": "自COVID-19以来，人群计数任务得到了广泛应用。尽管监督方法可靠，但由于头部尺寸小和严重遮挡，高密度场景中的标注更具挑战性，而低密度场景则更简单。有趣的是，我们能否在低密度场景中训练模型并将其泛化到高密度场景？因此，我们提出了一个从低密度到高密度泛化框架（L2HCount），该框架从低密度场景中学习与高密度场景相关的模式，使其能够很好地泛化到高密度场景。具体而言，我们首先引入了一个高密度模拟模块和一个真值生成模块，分别通过图像平移技术构建伪高密度图像及其相应的真值人群标注，有效地模拟高密度人群模式。然而，模拟图像存在两个问题：图像模糊和低密度图像特征的丢失。因此，我们其次提出了一个头部特征增强模块，用于在高密度模拟场景中提取清晰的特征。第三，我们提出了一个双密度记忆编码模块，该模块使用两个人群记忆分别学习来自低密度和模拟高密度场景的场景特定模式。在四个具有挑战性的数据集上进行的广泛实验表明L2HCount具有良好的性能。", "summary": "L2HCount是一个针对人群计数任务提出的从低密度到高密度泛化框架。它通过高密度模拟模块和真值生成模块创建模拟高密度图像及标注，并利用头部特征增强模块解决模拟图像的模糊问题。此外，双密度记忆编码模块用于学习不同密度场景的特定模式，从而使模型在低密度场景训练后能有效泛化到高密度场景，解决了高密度标注难题。", "keywords": "人群计数, 密度泛化, 密度模拟, 图像平移, 特征增强", "comments": "该论文的创新点在于提出了一个从低密度到高密度的人群计数泛化框架，通过密度模拟和特征增强解决了高密度场景下数据标注困难的关键问题。其通过生成合成数据和专门的模块来弥补数据稀缺性，具有重要的实际应用价值，尤其是在无法获取大量高密度标注数据的场景。方法设计巧妙，分步解决了模拟数据带来的挑战。"}}
{"id": "2503.12944", "pdf": "https://arxiv.org/pdf/2503.12944", "abs": "https://arxiv.org/abs/2503.12944", "authors": ["Jianzheng Huang", "Xianyu Mo", "Ziling Liu", "Jinyu Yang", "Feng Zheng"], "title": "GIFT: Generated Indoor video frames for Texture-less point tracking", "categories": ["cs.CV"], "comment": null, "summary": "Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking.", "AI": {"title_translation": "GIFT：用于无纹理点跟踪的生成式室内视频帧", "tldr": "现有视频点跟踪方法在无纹理区域表现不佳。本文提出了GIFT，一个带纹理强度评估和精确标注的合成基准数据集，用于评估和分析点跟踪方法在不同纹理条件下的性能。", "motivation": "现有的点跟踪方法难以在无纹理或弱纹理的视频帧区域进行点跟踪。", "method": "1. 引入评估3D对象纹理强度的指标。2. 使用这些指标将ShapeNet中的3D模型分为三级纹理强度。3. 创建了GIFT，一个包含1800个室内视频序列的合成基准数据集，其地面真实点精确锚定在分类的目标对象上，确保每个视频对应一个特定的纹理强度级别。4. 在GIFT上全面评估了现有方法，以评估它们在不同纹理强度级别下的性能并分析纹理对点跟踪的影响。", "result": "创建了GIFT基准数据集，并利用其对现有方法进行了全面评估，分析了纹理强度对点跟踪性能的影响。具体的评估结果未在摘要中提及。", "conclusion": "本文的主要贡献是创建了一个用于评估点跟踪方法在不同纹理强度下性能的合成基准数据集GIFT，并利用该数据集进行了全面的性能评估和纹理影响分析。", "translation": "点跟踪正成为运动估计和视频编辑的强大解决方案。与经典特征匹配相比，点跟踪方法的主要优势在于能够在复杂的相机运动轨迹下和较长时间内鲁棒地跟踪点。然而，尽管方法学上有所改进，当前的点击跟踪方法仍然难以跟踪视频帧中的任何位置，特别是在无纹理或弱纹理区域。在这项工作中，我们首先引入了评估3D对象纹理强度的指标。利用这些指标，我们将ShapeNet中的3D模型分为三级纹理强度，并创建了GIFT，一个具有丰富注释的挑战性合成基准，包含1800个室内视频序列。与现有任意分配地面真实点的数据集不同，GIFT将地面真实点精确锚定在分类的目标对象上，确保每个视频对应一个特定的纹理强度级别。此外，我们还在GIFT上全面评估了现有方法，以评估它们在不同纹理强度级别下的性能并分析纹理对点跟踪的影响。", "summary": "本文针对当前点跟踪方法在无纹理或弱纹理区域表现不佳的问题，提出了GIFT数据集。首先定义了3D对象纹理强度评估指标，并基于此将ShapeNet模型分类。GIFT是一个包含1800个室内视频序列的合成基准，其独特之处在于地面真实点精确锚定在分类的目标对象上，确保了纹理强度的对应性。最后，作者利用GIFT数据集全面评估了现有方法，并分析了纹理对点跟踪性能的影响。", "keywords": "点跟踪, 纹理强度, 合成数据集, GIFT, 性能评估", "comments": "本文的创新点在于首次引入了纹理强度评估指标，并基于此构建了一个针对无纹理区域点跟踪的合成基准数据集GIFT。该数据集通过精确锚定地面真实点并对应特定纹理强度级别，为评估和分析点跟踪方法在不同纹理条件下的性能提供了宝贵的资源，对于推动点跟踪技术在复杂环境下的发展具有重要意义。"}}
{"id": "2503.12947", "pdf": "https://arxiv.org/pdf/2503.12947", "abs": "https://arxiv.org/abs/2503.12947", "authors": ["Ingyun Lee", "Jae Won Jang", "Seunghyeon Seo", "Nojun Kwak"], "title": "DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency for Few-shot View Synthesis", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view\nsynthesis but requires many multiview images, making it impractical for\nfew-shot scenarios. Ray augmentation was proposed to prevent overfitting for\nsparse training data by generating additional rays. However, existing methods,\nwhich generate augmented rays only near the original rays, produce severe\nfloaters and appearance distortion due to limited viewpoints and inconsistent\nrays obstructed by nearby obstacles and complex surfaces. To address these\nproblems, we propose DivCon-NeRF, which significantly enhances both diversity\nand consistency. It employs surface-sphere augmentation, which preserves the\ndistance between the original camera and the predicted surface point. This\nallows the model to compare the order of high-probability surface points and\nfilter out inconsistent rays easily without requiring the exact depth. By\nintroducing inner-sphere augmentation, DivCon-NeRF randomizes angles and\ndistances for diverse viewpoints, further increasing diversity. Consequently,\nour method significantly reduces floaters and visual distortions, achieving\nstate-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code\nwill be publicly available.", "AI": {"title_translation": "DivCon-NeRF：为少样本视图合成生成具有多样性和一致性的增强射线", "tldr": "DivCon-NeRF通过引入表面球体和内球体增强，为少样本视图合成生成多样且一致的增强射线，显著减少浮点和失真，实现最先进性能。", "motivation": "现有NeRF在少样本场景中需要大量多视图图像，不实用。射线增强虽能防止过拟合，但现有方法仅在原射线附近生成，导致浮点和外观失真严重，因为视角有限且射线不一致。", "method": "提出DivCon-NeRF，它采用表面球体增强（保留原相机与预测表面点距离，并过滤不一致射线）和内球体增强（随机化角度和距离以增加多样性），显著增强了射线的多样性和一致性。", "result": "DivCon-NeRF显著减少了浮点和视觉失真，并在Blender、LLFF和DTU数据集上实现了最先进的性能。", "conclusion": "DivCon-NeRF通过增强射线的多样性和一致性，有效解决了少样本视图合成中的浮点和失真问题，达到了卓越的性能。", "translation": "神经辐射场（NeRF）在新型视图合成方面表现出色，但需要大量的多视图图像，这使得它在少样本场景中不切实际。射线增强被提出用于通过生成额外的射线来防止稀疏训练数据过拟合。然而，现有方法仅在原始射线附近生成增强射线，由于视角有限以及附近障碍物和复杂表面阻碍导致射线不一致，从而产生严重的浮点和外观失真。为了解决这些问题，我们提出了DivCon-NeRF，它显著增强了多样性和一致性。它采用表面球体增强，保留了原始相机与预测表面点之间的距离。这使得模型能够比较高概率表面点的顺序，并在不需要精确深度的情况下轻松过滤掉不一致的射线。通过引入内球体增强，DivCon-NeRF随机化角度和距离以实现多样化的视角，进一步增加了多样性。因此，我们的方法显著减少了浮点和视觉失真，并在Blender、LLFF和DTU数据集上实现了最先进的性能。我们的代码将公开可用。", "summary": "针对NeRF在少样本视图合成中存在的过拟合和现有射线增强方法导致的浮点、失真问题，本文提出了DivCon-NeRF。该方法通过表面球体增强确保射线一致性并过滤无效射线，同时通过内球体增强增加视角多样性。实验证明，DivCon-NeRF有效减少了浮点和视觉失真，并在多个标准数据集上达到了最先进的性能。", "keywords": "神经辐射场, 少样本视图合成, 射线增强, 多样性, 一致性", "comments": "DivCon-NeRF通过引入创新的表面球体和内球体增强机制，有效解决了少样本NeRF中射线增强的局限性，提升了生成图像的质量和一致性，是该领域的重要进展。"}}
{"id": "2503.12953", "pdf": "https://arxiv.org/pdf/2503.12953", "abs": "https://arxiv.org/abs/2503.12953", "authors": ["Zheyuan Liu", "Junyan Wang", "Zicheng Duan", "Cristian Rodriguez-Opazo", "Anton van den Hengel"], "title": "Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in Text-to-Video Prediction", "categories": ["cs.CV"], "comment": "20 pages, 15 figures", "summary": "Text-video prediction (TVP) is a downstream video generation task that\nrequires a model to produce subsequent video frames given a series of initial\nvideo frames and text describing the required motion. In practice TVP methods\nfocus on a particular category of videos depicting manipulations of objects\ncarried out by human beings or robot arms. Previous methods adapt models\npre-trained on text-to-image tasks, and thus tend to generate video that lacks\nthe required continuity. A natural progression would be to leverage more recent\npre-trained text-to-video (T2V) models. This approach is rendered more\nchallenging by the fact that the most common fine-tuning technique, low-rank\nadaptation (LoRA), yields undesirable results. In this work, we propose an\nadaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).\nWithin the module, we devise a sub-module that produces frame-wise text\nembeddings from the input text, which acts as an additional text condition to\naid generation. We use FCA to fine-tune the T2V model, which incorporates the\ninitial frame(s) as an extra condition. We compare and discuss the more\neffective strategy for injecting such embeddings into the T2V model. We conduct\nextensive ablation studies on our design choices with quantitative and\nqualitative performance analysis. Our approach establishes a new\nstate-of-the-art for the task of TVP. The project page is at\nhttps://github.com/Cuberick-Orion/FCA .", "AI": {"title_translation": "用于文本到视频预测中微调扩散模型的逐帧条件适应", "tldr": "提出了一种名为逐帧条件适应（FCA）的新策略，用于微调文本到视频扩散模型，以解决现有方法在文本-视频预测（TVP）中缺乏连续性的问题，并达到了新的SOTA。", "motivation": "文本-视频预测（TVP）方法通常专注于特定类别的视频，并且之前的方法依赖于文本到图像预训练模型，导致生成的视频缺乏所需的连续性。直接利用最新的文本到视频（T2V）预训练模型进行微调，特别是使用LoRA等常见技术，效果不佳。", "method": "本文提出了一种名为逐帧条件适应（FCA）的适应策略。FCA模块中设计了一个子模块，该子模块从输入文本中生成逐帧文本嵌入，作为额外的文本条件来辅助生成。FCA用于微调T2V模型，并将初始帧作为额外条件。研究还比较了将这些嵌入注入T2V模型的更有效策略，并进行了广泛的消融研究。", "result": "通过定量和定性性能分析，本文的方法在TVP任务上建立了新的最先进水平（SOTA）。", "conclusion": "逐帧条件适应（FCA）策略有效地解决了文本-视频预测中视频连续性不足的问题，并通过微调文本到视频模型达到了当前最佳性能。", "translation": "文本-视频预测（TVP）是一种下游视频生成任务，要求模型在给定一系列初始视频帧和描述所需运动的文本的情况下，生成后续视频帧。在实践中，TVP方法侧重于描绘人类或机器人手臂进行物体操作的特定类别视频。以前的方法适应了在文本到图像任务上预训练的模型，因此倾向于生成缺乏所需连续性的视频。一个自然的进展是利用更近期的预训练文本到视频（T2V）模型。然而，这种方法更具挑战性，因为最常见的微调技术——低秩适应（LoRA）——产生了不理想的结果。在这项工作中，我们提出了一种名为逐帧条件适应（FCA）的基于适应的策略。在该模块中，我们设计了一个子模块，从输入文本中生成逐帧文本嵌入，作为额外的文本条件来辅助生成。我们使用FCA微调T2V模型，该模型将初始帧作为额外条件。我们比较并讨论了将此类嵌入注入T2V模型的更有效策略。我们对我们的设计选择进行了广泛的消融研究，并进行了定量和定性性能分析。我们的方法在TVP任务上建立了新的最先进水平。项目页面位于https://github.com/Cuberick-Orion/FCA。", "summary": "本文提出了一种名为逐帧条件适应（FCA）的新策略，用于微调扩散模型以进行文本-视频预测（TVP）。针对现有方法在生成视频时缺乏连续性的问题，FCA通过一个子模块生成逐帧文本嵌入作为额外条件，并结合初始帧来微调预训练的文本到视频（T2V）模型。实验结果表明，该方法在TVP任务上达到了新的最先进水平。", "keywords": "文本-视频预测, 扩散模型, 微调, 逐帧条件适应, 视频生成", "comments": "这项工作提出了一种新颖的逐帧条件适应（FCA）策略，有效地解决了文本-视频预测中视频连续性不足的关键问题。其创新点在于引入了逐帧文本嵌入作为额外条件，并巧妙地将其与初始帧结合，用于微调现有的文本到视频模型。这为文本到视频生成领域提供了一个有前景的方向，特别是在需要高时间一致性的视频生成任务中。"}}
{"id": "2503.12955", "pdf": "https://arxiv.org/pdf/2503.12955", "abs": "https://arxiv.org/abs/2503.12955", "authors": ["Jiahe Zhao", "Ruibing Hou", "Zejie Tian", "Hong Chang", "Shiguang Shan"], "title": "HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new task to benchmark human-in-scene understanding for embodied\nagents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within\na 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,\nreason about its surrounding environment, and answer human-related questions\nwithin the scene. To support this new task, we present HIS-Bench, a multimodal\nbenchmark that systematically evaluates HIS understanding across a broad\nspectrum, from basic perception to commonsense reasoning and planning. Our\nevaluation of various vision-language models on HIS-Bench reveals significant\nlimitations in their ability to handle HIS-QA tasks. To this end, we propose\nHIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates\n3D scene context and human motion dynamics into large language models while\nincorporating specialized mechanisms to capture human-scene interactions.\nExtensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on\nHIS-QA tasks. We hope this work inspires future research on human behavior\nanalysis in 3D scenes, advancing embodied AI and world models.", "AI": {"title_translation": "HIS-GPT：迈向3D场景中人体多模态理解", "tldr": "本文提出了HIS-QA任务和HIS-Bench基准来评估3D场景中的人体理解，并引入了HIS-GPT模型，该模型在HIS-QA任务上取得了最先进的性能。", "motivation": "为了基准化具身智能体在场景中人体理解的能力，本文提出了一个新的任务：场景中人体问答（HIS-QA）。现有的视觉-语言模型在处理HIS-QA任务时表现出显著的局限性，因此需要一个新的基础模型来解决这些问题。", "method": "本文提出了一个新的任务——场景中人体问答（HIS-QA），并构建了多模态基准HIS-Bench来系统评估HIS理解。为了解决HIS-QA任务的挑战，本文提出了HIS-GPT，这是首个用于HIS理解的基础模型。HIS-GPT将3D场景上下文和人体运动动态整合到大型语言模型中，并结合了专门的机制来捕捉人-场景交互。", "result": "广泛的实验证明，HIS-GPT在HIS-QA任务上取得了新的最先进的性能。", "conclusion": "这项工作有望启发未来在3D场景中人体行为分析的研究，从而推动具身AI和世界模型的发展。", "translation": "我们提出了一个新任务，用于评估具身智能体在场景中人体理解的能力：场景中人体问答（HIS-QA）。给定3D场景中的人体运动，HIS-QA要求智能体理解人体状态和行为，推理其周围环境，并回答场景中与人体相关的问题。为了支持这项新任务，我们提出了HIS-Bench，一个多模态基准，系统地评估HIS理解，范围涵盖从基本感知到常识推理和规划。我们对HIS-Bench上各种视觉-语言模型的评估揭示了它们在处理HIS-QA任务方面的显著局限性。为此，我们提出了HIS-GPT，这是首个用于HIS理解的基础模型。HIS-GPT将3D场景上下文和人体运动动态整合到大型语言模型中，同时结合了专门的机制来捕捉人-场景交互。大量实验表明，HIS-GPT在HIS-QA任务上取得了新的最先进的性能。我们希望这项工作能启发未来在3D场景中人体行为分析方面的研究，推动具身AI和世界模型的发展。", "summary": "本文提出了一个新的任务——场景中人体问答（HIS-QA），旨在评估具身智能体对3D场景中人体的理解能力。为支持该任务，作者构建了多模态基准HIS-Bench，并发现现有视觉-语言模型在该任务上存在局限性。为此，本文提出了HIS-GPT，一个将3D场景和人体运动动态融入大型语言模型的基础模型，并加入了捕捉人-场景交互的机制。实验证明HIS-GPT在HIS-QA任务上达到了最先进的性能，有望推动3D场景中人体行为分析及具身AI的发展。", "keywords": "HIS-GPT, 3D人体理解, 场景中人体问答, 具身AI, 多模态", "comments": "本文的创新点在于提出了一个新颖的任务——HIS-QA，以及相应的多模态基准HIS-Bench，填补了3D场景中人体理解评估的空白。更重要的是，它引入了HIS-GPT，一个将3D上下文和人体动态与大型语言模型结合的基础模型，为解决具身AI中的复杂人-场景交互问题提供了新的SOTA解决方案。这项工作对于推动具身AI和世界模型的发展具有重要意义。"}}
{"id": "2503.12963", "pdf": "https://arxiv.org/pdf/2503.12963", "abs": "https://arxiv.org/abs/2503.12963", "authors": ["Chaolong Yang", "Kai Yao", "Yuyao Yan", "Chenru Jiang", "Weiguang Zhao", "Jie Sun", "Guangliang Cheng", "Yifei Zhang", "Bin Dong", "Kaizhu Huang"], "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.", "AI": {"title_translation": "解锁姿态多样性：一种精确高效的隐式关键点时空扩散模型用于音频驱动的说话肖像生成", "tldr": "KDTalker是一种新的框架，结合隐式3D关键点和时空扩散模型，用于音频驱动的说话肖像生成，实现了姿态多样性、细节捕捉、唇同步准确性和高效率。", "motivation": "现有的音频驱动说话肖像生成方法存在局限：基于关键点的方法难以捕捉精细面部细节且姿态多样性低；基于图像的方法生成质量高但导致身份失真且计算成本高昂。", "method": "本文提出KDTalker，首次将无监督隐式3D关键点与时空扩散模型相结合。KDTalker利用隐式3D关键点适应面部信息密度，使扩散过程能够灵活建模多样头部姿态并捕捉精细面部细节。同时，定制设计的时空注意力机制确保了精确的唇同步，生成时间一致、高质量的动画，并提高计算效率。", "result": "实验结果表明，KDTalker在唇同步准确性、头部姿态多样性和执行效率方面均达到了最先进的性能。", "conclusion": "KDTalker成功克服了现有音频驱动说话肖像生成方法的局限性，提供了一个精确、高效且能生成多样姿态和精细细节的解决方案。", "translation": "音频驱动的单图像说话肖像生成在虚拟现实、数字人创建和电影制作中发挥着关键作用。现有方法通常分为基于关键点和基于图像的方法。基于关键点的方法能有效保留角色身份，但由于3D可变形模型的固定点限制，难以捕捉精细面部细节。此外，传统生成网络在有限数据集上难以建立音频和关键点之间的因果关系，导致姿态多样性低。相比之下，基于图像的方法使用扩散网络生成高质量且细节多样的肖像，但会产生身份失真和高昂的计算成本。在这项工作中，我们提出了KDTalker，这是第一个结合无监督隐式3D关键点和时空扩散模型的框架。KDTalker利用无监督隐式3D关键点适应面部信息密度，使扩散过程能够灵活地建模多样化的头部姿态并捕捉精细的面部细节。定制设计的时空注意力机制确保了精确的唇同步，生成了时间一致、高质量的动画，同时提高了计算效率。实验结果表明，KDTalker在唇同步准确性、头部姿态多样性和执行效率方面均达到了最先进的性能。我们的代码可在https://github.com/chaolongy/KDTalker获取。", "summary": "本文提出KDTalker，一个结合无监督隐式3D关键点与时空扩散模型的新框架，旨在解决现有音频驱动说话肖像生成方法中姿态多样性不足、细节捕捉受限以及计算成本高昂的问题。KDTalker通过适应面部信息密度实现多样头部姿态和精细面部细节的灵活建模，并利用定制的时空注意力机制确保唇同步准确性和动画时间一致性，同时提升计算效率。实验证明KDTalker在多项指标上达到最先进水平。", "keywords": "音频驱动肖像, 隐式关键点, 时空扩散, 姿态多样性, 唇同步", "comments": "KDTalker的创新之处在于首次将无监督隐式3D关键点与时空扩散模型相结合，有效解决了传统关键点方法在细节和姿态多样性上的局限，并克服了图像方法的高计算成本和身份失真问题。其重要性在于为音频驱动的说话肖像生成提供了一个更精确、高效且能生成高质量、多样化动画的解决方案，在虚拟现实、数字人等领域具有广阔应用前景。"}}
{"id": "2503.12964", "pdf": "https://arxiv.org/pdf/2503.12964", "abs": "https://arxiv.org/abs/2503.12964", "authors": ["Zeeshan Patel", "Ethan He", "Parth Mannan", "Xiaowei Ren", "Ryan Wolf", "Niket Agarwal", "Jacob Huffman", "Zhuoyao Wang", "Carl Wang", "Jack Chang", "Yan Bai", "Tommy Huang", "Linnan Wang", "Sahil Jain", "Shanmugam Ramasamy", "Joseph Jennings", "Ekaterina Sirazitdinova", "Oleg Sudakov", "Mingyuan Ma", "Bobby Chen", "Forrest Lin", "Hao Wang", "Vasanth Rao Naik Sabavat", "Sriharsha Niverty", "Rong Ou", "Pallab Bhattacharya", "David Page", "Nima Tajbakhsh", "Ashwath Aithal"], "title": "Training Video Foundation Models with NVIDIA NeMo", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.", "AI": {"title_translation": "使用 NVIDIA NeMo 训练视频基础模型", "tldr": "本文介绍了使用 NVIDIA NeMo 训练可扩展、开源的视频基础模型 (VFM) 管道，以解决高质量 VFM 训练的挑战，并提供了性能分析和最佳实践。", "motivation": "视频基础模型（VFMs）在模拟现实世界和开发创意视觉体验方面有应用，但在训练能够生成高质量视频的大规模 VFM 时存在显著挑战。", "method": "本文提出了一个可扩展的开源 VFM 训练管道，该管道基于 NVIDIA NeMo，提供了加速视频数据集整理、多模态数据加载以及并行化的视频扩散模型训练和推理。", "result": "本文提供了一个可扩展的开源 VFM 训练管道，并提供了全面的性能分析，强调了高效 VFM 训练和推理的最佳实践。", "conclusion": "本文通过提供一个可扩展、开源的 NVIDIA NeMo 训练管道以及性能分析和最佳实践，解决了训练大规模高质量视频基础模型的挑战。", "translation": "视频基础模型（VFMs）最近被用于模拟真实世界，以训练物理AI系统和开发创意视觉体验。然而，训练能够生成高质量视频的大规模、高质量VFM存在显著挑战。我们提出了一个基于NVIDIA NeMo的可扩展、开源VFM训练管道，提供了加速视频数据集整理、多模态数据加载以及并行化的视频扩散模型训练和推理。我们还提供了全面的性能分析，强调了高效VFM训练和推理的最佳实践。", "summary": "本文针对训练大规模高质量视频基础模型（VFMs）所面临的挑战，提出了一个基于 NVIDIA NeMo 的可扩展、开源训练管道。该管道集成了加速视频数据集整理、多模态数据加载以及并行化的视频扩散模型训练和推理功能。此外，论文还提供了详细的性能分析，并分享了高效 VFM 训练和推理的最佳实践。", "keywords": "视频基础模型, NVIDIA NeMo, 视频生成, 训练管道, 扩散模型", "comments": "本文通过提供一个基于 NVIDIA NeMo 的开源、可扩展的管道，解决了训练大规模高质量视频基础模型这一关键挑战，对于推动视频生成和物理 AI 系统的发展具有重要意义。其提供的性能分析和最佳实践也为研究人员和开发者提供了宝贵的指导。"}}
{"id": "2503.12968", "pdf": "https://arxiv.org/pdf/2503.12968", "abs": "https://arxiv.org/abs/2503.12968", "authors": ["Guanhua Ding", "Yuxuan Xia", "Runwei Guan", "Qinchen Wu", "Tao Huang", "Weiping Ding", "Jinping Sun", "Guoqiang Mao"], "title": "OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as\nit enables robust perception, navigation, and planning in complex environments.\nWhile deep learning-based solutions have demonstrated impressive 3D MOT\nperformance, model-based approaches remain appealing for their simplicity,\ninterpretability, and data efficiency. Conventional model-based trackers\ntypically rely on random vector-based Bayesian filters within the\ntracking-by-detection (TBD) framework but face limitations due to heuristic\ndata association and track management schemes. In contrast, random finite set\n(RFS)-based Bayesian filtering handles object birth, survival, and death in a\ntheoretically sound manner, facilitating interpretability and parameter tuning.\nIn this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs\nan optimized Poisson multi-Bernoulli (PMB) filter while incorporating several\nkey innovative designs within the TBD framework. Specifically, we propose a\nmeasurement-driven hybrid adaptive birth model for improved track\ninitialization, employ adaptive detection probability parameters to effectively\nmaintain tracks for occluded objects, and optimize density pruning and track\nextraction modules to further enhance overall tracking performance. Extensive\nevaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior\ntracking accuracy compared with state-of-the-art methods, thereby establishing\na new benchmark for model-based 3D MOT and offering valuable insights for\nfuture research on RFS-based trackers in autonomous driving.", "AI": {"title_translation": "OptiPMB：通过优化泊松多伯努利滤波增强3D多目标跟踪", "tldr": "本文提出了OptiPMB，一种基于RFS的3D多目标跟踪方法，它使用优化的泊松多伯努利滤波器，并包含新的设计，如测量驱动的自适应生成模型、自适应检测概率和优化的密度剪枝。该方法在nuScenes和KITTI数据集上实现了卓越的精度。", "motivation": "准确的3D多目标跟踪（MOT）对自动驾驶至关重要。尽管深度学习解决方案表现出色，但基于模型的方法因其简单性、可解释性和数据效率而仍具吸引力。传统的基于模型的跟踪器受限于启发式数据关联和轨迹管理方案，而基于随机有限集（RFS）的贝叶斯滤波能以理论上严谨的方式处理目标生成、存活和消亡。", "method": "本文提出了OptiPMB，一种新颖的基于RFS的3D多目标跟踪（MOT）方法，它在检测跟踪（TBD）框架内采用了优化的泊松多伯努利（PMB）滤波器。其关键创新设计包括：1) 提出了一种测量驱动的混合自适应生成模型以改进轨迹初始化；2) 采用自适应检测概率参数以有效维护被遮挡对象的轨迹；3) 优化了密度剪枝和轨迹提取模块以进一步增强整体跟踪性能。", "result": "在nuScenes和KITTI数据集上的广泛评估表明，OptiPMB与现有最先进方法相比，实现了卓越的跟踪精度。", "conclusion": "OptiPMB为基于模型的3D多目标跟踪建立了新的基准，并为自动驾驶中基于随机有限集（RFS）的跟踪器的未来研究提供了宝贵的见解。", "translation": "准确的3D多目标跟踪（MOT）对于自动驾驶至关重要，因为它能够在复杂环境中实现鲁棒的感知、导航和规划。虽然基于深度学习的解决方案已展示出令人印象深刻的3D MOT性能，但基于模型的方法因其简单性、可解释性和数据效率仍具吸引力。传统的基于模型的跟踪器通常依赖于检测跟踪（TBD）框架内的基于随机向量的贝叶斯滤波器，但由于启发式数据关联和轨迹管理方案而面临局限性。相比之下，基于随机有限集（RFS）的贝叶斯滤波以理论上严谨的方式处理对象的生成、存活和消亡，从而促进了可解释性和参数调整。在本文中，我们提出了OptiPMB，一种新颖的基于RFS的3D MOT方法，它采用优化的泊松多伯努利（PMB）滤波器，同时在TBD框架内融入了几项关键的创新设计。具体来说，我们提出了一种测量驱动的混合自适应生成模型，用于改进轨迹初始化；采用自适应检测概率参数，以有效维护被遮挡对象的轨迹；并优化了密度剪枝和轨迹提取模块，以进一步增强整体跟踪性能。在nuScenes和KITTI数据集上的广泛评估表明，OptiPMB与现有最先进方法相比，实现了卓越的跟踪精度，从而为基于模型的3D MOT建立了新的基准，并为自动驾驶中基于RFS的跟踪器的未来研究提供了宝贵的见解。", "summary": "本文介绍了OptiPMB，一种新颖的基于模型的3D多目标跟踪方法。它利用优化的泊松多伯努利滤波器，在检测跟踪框架内融入了多项创新，例如测量驱动的自适应生成模型、针对被遮挡对象的自适应检测概率以及优化的密度剪枝。OptiPMB在nuScenes和KITTI数据集上进行了评估，其性能优于现有最先进方法，为基于模型的3D多目标跟踪树立了新基准，并为自动驾驶中基于RFS的跟踪器提供了重要见解。", "keywords": "3D多目标跟踪, 泊松多伯努利滤波, 随机有限集, 自动驾驶, 基于模型的跟踪", "comments": "该论文通过使用基于随机有限集（RFS）的方法推动了基于模型的3D多目标跟踪的发展，RFS方法在处理目标动态方面比传统的启发式方法更具理论严谨性，从而做出了重要贡献。在生成模型、遮挡处理和滤波器组件优化方面的具体创新展示了对实际多目标跟踪系统挑战的深刻理解。与现有最先进方法相比，建立新基准突出了其实用相关性以及对自动驾驶系统的潜在影响。"}}
{"id": "2503.12969", "pdf": "https://arxiv.org/pdf/2503.12969", "abs": "https://arxiv.org/abs/2503.12969", "authors": ["Kazuki Omi", "Jion Oshima", "Toru Tamaki"], "title": "Action tube generation by person query matching for spatio-temporal action detection", "categories": ["cs.CV"], "comment": "extended version of VISAPP2025", "summary": "This paper proposes a method for spatio-temporal action detection (STAD) that\ndirectly generates action tubes from the original video without relying on\npost-processing steps such as IoU-based linking and clip splitting. Our\napproach applies query-based detection (DETR) to each frame and matches DETR\nqueries to link the same person across frames. We introduce the Query Matching\nModule (QMM), which uses metric learning to bring queries for the same person\ncloser together across frames compared to queries for different people. Action\nclasses are predicted using the sequence of queries obtained from QMM matching,\nallowing for variable-length inputs from videos longer than a single clip.\nExperimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our\nmethod performs well for large position changes of people while offering\nsuperior computational efficiency and lower resource requirements.", "AI": {"title_translation": "基于人物查询匹配的动作管生成，用于时空动作检测", "tldr": "本文提出了一种直接从原始视频生成动作管的时空动作检测方法，避免了后处理，并通过查询匹配模块实现跨帧人物关联。", "motivation": "现有的时空动作检测方法依赖于IoU-based链接和片段分割等后处理步骤，这可能导致效率低下和资源消耗大。", "method": "该方法将基于查询的检测 (DETR) 应用于每一帧，并引入查询匹配模块 (QMM)，该模块利用度量学习来关联跨帧的同一个人查询。动作类别通过QMM匹配获得的查询序列进行预测，支持可变长度视频输入。", "result": "在JHMDB、UCF101-24和AVA数据集上的实验表明，该方法在处理人物位置大范围变化时表现良好，并具有卓越的计算效率和更低的资源要求。", "conclusion": "该方法通过直接生成动作管和引入查询匹配模块，有效解决了传统时空动作检测中后处理的弊端，提高了效率和性能。", "translation": "本文提出了一种用于时空动作检测（STAD）的方法，该方法直接从原始视频生成动作管，而无需依赖诸如基于IoU的链接和片段分割等后处理步骤。我们的方法将基于查询的检测（DETR）应用于每一帧，并匹配DETR查询以在帧间链接同一个人。我们引入了查询匹配模块（QMM），它使用度量学习来使同一人物的查询在跨帧时比不同人物的查询更接近。动作类别是使用从QMM匹配获得的查询序列进行预测的，这允许来自长于单个片段的视频的可变长度输入。在JHMDB、UCF101-24和AVA数据集上的实验结果表明，我们的方法在人物位置发生大范围变化时表现良好，同时提供了卓越的计算效率和更低的资源要求。", "summary": "本文提出了一种新颖的时空动作检测（STAD）方法，通过直接从原始视频生成动作管，避免了传统方法中繁琐的后处理步骤。该方法结合DETR检测和创新的查询匹配模块（QMM），利用度量学习实现跨帧的人物关联，并能处理可变长度的视频输入。实验证明，该方法在处理大范围人物位移时性能优异，并显著提高了计算效率和降低了资源消耗。", "keywords": "时空动作检测, 动作管, 查询匹配, DETR, 度量学习", "comments": "这项工作通过直接生成动作管并引入查询匹配模块，为时空动作检测提供了一种新颖且高效的解决方案，有效解决了传统方法中后处理的痛点。其创新点在于将DETR与跨帧查询匹配相结合，尤其是在处理人物大幅度位移和变长视频输入方面的表现，具有重要的实际应用价值。"}}
{"id": "2503.12972", "pdf": "https://arxiv.org/pdf/2503.12972", "abs": "https://arxiv.org/abs/2503.12972", "authors": ["Junming Liu", "Siyuan Meng", "Yanting Gao", "Song Mao", "Pinlong Cai", "Guohang Yan", "Yirong Chen", "Zilin Bian", "Botian Shi", "Ding Wang"], "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 7 figures, 6 tables", "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.", "AI": {"title_translation": "视觉与语言对齐：无文本多模态知识图谱构建以增强大型语言模型推理", "tldr": "本文提出VaLiK，一种无需手动文本标注即可构建多模态知识图谱的方法，通过视觉-语言模型和跨模态相似性验证，有效增强大型语言模型的推理能力并提高存储效率。", "motivation": "大型语言模型（LLMs）在多模态推理中面临知识不完整和幻觉问题，而现有文本知识图谱因模态隔离无法完全解决。多模态知识图谱（MMKGs）的构建又受限于手动文本标注的语义狭窄性及视觉-语义实体链接中的固有噪声。", "method": "本文提出VaLiK（Vision-align-to-Language integrated Knowledge Graph），通过级联预训练的视觉-语言模型（VLMs）将图像特征与文本对齐，转化为封装图像信息的描述。此外，开发了跨模态相似性验证机制来量化语义一致性并过滤噪声。即使没有手动标注的图像字幕，仅凭精炼的描述即可构建MMKG。", "result": "VaLiK构建的MMKG在多模态推理任务上，使LLMs的性能超越了以往的SOTA模型。与传统MMKG构建范式相比，该方法实现了显著的存储效率提升，同时保持了直接的实体-图像链接能力。", "conclusion": "VaLiK通过创新的无文本多模态知识图谱构建方法，有效补充了跨模态信息，解决了LLMs多模态推理中的知识不完整和幻觉问题，并显著提升了其推理性能和存储效率。", "translation": "大型语言模型（LLMs）中的多模态推理面临知识不完整和幻觉伪影的挑战，而文本知识图谱（KGs）由于其模态隔离只能部分缓解这些问题。尽管多模态知识图谱（MMKGs）有望增强跨模态理解，但其实际构建受到手动文本标注的语义狭窄性和视觉-语义实体链接中固有噪声的阻碍。在本文中，我们提出了视觉-语言对齐集成知识图谱（VaLiK），这是一种构建MMKGs的新颖方法，通过跨模态信息补充来增强LLMs的推理能力。具体而言，我们级联预训练的视觉-语言模型（VLMs）以将图像特征与文本对齐，将其转换为封装图像特定信息的描述。此外，我们开发了一种跨模态相似性验证机制来量化语义一致性，有效过滤掉特征对齐过程中引入的噪声。即使没有手动标注的图像字幕，仅凭精炼的描述就足以构建MMKG。与传统MMKGs构建范式相比，我们的方法在保持直接实体-图像链接能力的同时，实现了显著的存储效率提升。在多模态推理任务上的实验结果表明，用VaLiK增强的LLMs优于以往的最新模型。我们的代码已发布在https://github.com/Wings-Of-Disaster/VaLiK。", "summary": "本文提出VaLiK，一种新颖的多模态知识图谱（MMKG）构建方法，旨在解决大型语言模型（LLMs）在多模态推理中面临的知识不完整和幻觉问题。VaLiK通过级联预训练的视觉-语言模型将图像特征转化为文本描述，并利用跨模态相似性验证机制过滤噪声，实现无文本标注的MMKG构建。该方法不仅提高了存储效率并保持实体-图像链接，还在多模态推理任务中使LLMs超越了现有最佳模型。", "keywords": "多模态知识图谱, 大型语言模型, 视觉-语言模型, 无文本, 推理", "comments": "本文提出了一种创新的“无文本”多模态知识图谱构建范式，有效解决了传统MMKG构建中对大量手动标注的依赖和噪声问题。通过利用预训练VLM和跨模态相似性验证，VaLiK为LLMs的多模态推理提供了更丰富、更准确的知识，具有重要的实践意义和研究价值。"}}
{"id": "2503.12973", "pdf": "https://arxiv.org/pdf/2503.12973", "abs": "https://arxiv.org/abs/2503.12973", "authors": ["Colin Prieur", "Nassim Ait Ali Braham", "Paul Tresson", "Grégoire Vincent", "Jocelyn Chanussot"], "title": "Prospects for Mitigating Spectral Variability in Tropical Species Classification Using Self-Supervised Learning", "categories": ["cs.CV"], "comment": "5 pages, 3 figures, published as proceeding of the \"2024 14th\n  Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote\n  Sensing (WHISPERS)\"", "summary": "Airborne hyperspectral imaging is a promising method for identifying tropical\nspecies, but spectral variability between acquisitions hinders consistent\nresults. This paper proposes using Self-Supervised Learning (SSL) to encode\nspectral features that are robust to abiotic variability and relevant for\nspecies identification. By employing the state-of-the-art Barlow-Twins approach\non repeated spectral acquisitions, we demonstrate the ability to develop stable\nfeatures. For the classification of 40 tropical species, experiments show that\nthese features can outperform typical reflectance products in terms of\nrobustness to spectral variability by 10 points of accuracy across dates.", "AI": {"title_translation": "利用自监督学习缓解热带物种分类中光谱变异性的前景", "tldr": "该研究利用自监督学习（Barlow-Twins）来创建对光谱变异性更鲁棒的特征，从而提高热带物种分类的准确性。", "motivation": "机载高光谱成像在识别热带物种方面很有前景，但不同采集批次间的光谱变异性阻碍了结果的一致性。", "method": "本文提出使用自监督学习（SSL），特别是Barlow-Twins方法，对重复光谱采集数据进行编码，以生成对非生物变异性鲁棒且与物种识别相关的光谱特征。", "result": "实验表明，在40种热带物种的分类中，这些自监督学习生成的特征在对抗光谱变异性方面，比典型的反射率产品提高了10个百分点的准确性。", "conclusion": "自监督学习能够有效生成对光谱变异性鲁棒的特征，从而显著提高热带物种分类的准确性和一致性。", "translation": "机载高光谱成像是一种识别热带物种的有前景的方法，但不同采集批次间的光谱变异性阻碍了结果的一致性。本文提出使用自监督学习（SSL）来编码对非生物变异性鲁棒且与物种识别相关的光谱特征。通过对重复光谱采集数据采用最先进的Barlow-Twins方法，我们展示了开发稳定特征的能力。对于40种热带物种的分类，实验表明这些特征在对抗光谱变异性方面，比典型的反射率产品在跨日期准确性上提高了10个百分点。", "summary": "本文旨在解决机载高光谱成像在热带物种识别中面临的光谱变异性问题。研究提出利用自监督学习（SSL）中的Barlow-Twins方法，从重复光谱采集中学习生成对环境变化鲁棒的特征。实验结果表明，这些SSL生成的特征在40种热带物种分类任务中，比传统反射率产品在对抗光谱变异性方面提高了10个百分点的分类准确性，证明了其在提高分类一致性方面的潜力。", "keywords": "自监督学习, 光谱变异性, 热带物种分类, 高光谱成像, Barlow-Twins", "comments": "该论文创新性地将自监督学习应用于缓解高光谱图像在热带物种分类中的光谱变异性问题，解决了实际应用中的一大挑战。Barlow-Twins方法的引入显示了其在提取稳定特征方面的潜力，为提高遥感分类的鲁棒性提供了一条新途径。"}}
{"id": "2503.12974", "pdf": "https://arxiv.org/pdf/2503.12974", "abs": "https://arxiv.org/abs/2503.12974", "authors": ["Xueying Jiang", "Wenhao Li", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released.", "AI": {"title_translation": "探索3D活动推理与规划：从隐含人类意图到路径感知规划", "tldr": "本文提出了一个新颖的3D任务，即3D活动推理与规划，旨在从隐含指令中推断意图活动，并将其分解为包含步间路径的步骤，同时构建了ReasonPlan3D基准和设计了一个新框架来解决此任务。", "motivation": "现有的3D活动推理和规划工作主要存在两个限制：1) 过度依赖明确指令，对隐含用户意图的推理不足；2) 忽视机器人移动的步间路径规划。为了弥补这些空白，本文提出了新的方法。", "method": "本文从两个方面解决新提出的3D任务。首先，构建了ReasonPlan3D，一个大规模基准，涵盖多样的3D场景、丰富的隐含指令以及多步任务规划、步间路径规划和细粒度分割的详细标注。其次，设计了一个新颖的框架，引入了渐进式规划生成，在多步之间保持上下文一致性，并动态更新场景图以捕捉关键对象及其空间关系。", "result": "大量的实验证明了所提出的基准和框架在以下方面的有效性：从隐含人类指令中推理活动，生成准确的逐步任务计划，以及无缝集成多步移动的路径规划。", "conclusion": "本文提出的3D活动推理与规划任务、ReasonPlan3D基准和新颖框架有效地解决了从隐含人类意图进行活动推理和包含路径规划的多步任务规划的挑战，为人类-机器人交互和具身AI领域带来了显著进步。", "translation": "3D活动推理与规划在人类-机器人交互和具身AI领域受到了越来越多的关注，这得益于多模态学习的最新进展。然而，大多数现有工作存在两个限制：1) 严重依赖明确指令，对隐含用户意图的推理很少；2) 忽视机器人移动的步间路径规划。为了弥合这些差距，我们提出了3D活动推理与规划，这是一个新颖的3D任务，它从隐含指令中推理预期活动，并在细粒度3D对象形状和场景分割位置的指导下，将它们分解为包含步间路径和规划的步骤。我们从两个角度解决这个新的3D任务。首先，我们构建了ReasonPlan3D，一个大规模基准，涵盖了多样的3D场景，具有丰富的隐含指令以及多步任务规划、步间路径规划和细粒度分割的详细标注。其次，我们设计了一个新颖的框架，引入了多步之间具有上下文一致性的渐进式规划生成，以及一个动态更新的场景图，用于捕捉关键对象及其空间关系。大量的实验证明了我们的基准和框架在从隐含人类指令中推理活动、生成准确的逐步任务计划以及无缝集成多步移动的路径规划方面的有效性。数据集和代码将会发布。", "summary": "本文针对3D活动推理与规划领域中现有方法对隐含用户意图推理不足和忽视步间路径规划的问题，提出了一种新颖的3D任务。该任务旨在从隐含指令中推断意图活动，并将其分解为包含步间路径的步骤，同时考虑细粒度3D对象信息。为解决此任务，作者构建了ReasonPlan3D大规模基准数据集，并设计了一个创新的框架，该框架采用渐进式规划生成和动态场景图。实验结果验证了所提基准和框架在理解隐含指令、生成准确任务计划以及集成路径规划方面的有效性。", "keywords": "3D活动推理, 路径感知规划, 隐含意图, ReasonPlan3D, 人类-机器人交互", "comments": "本文的创新点在于首次明确提出了从隐含人类意图进行3D活动推理和规划的任务，并解决了现有方法中对隐含意图推理不足和缺乏步间路径规划的问题。其构建的大规模ReasonPlan3D基准数据集对于推动该领域的研究具有重要意义。所提出的渐进式规划生成框架和动态场景图方法也为解决复杂的多步任务规划提供了新的思路。该研究对于提升人类-机器人交互和具身AI的智能化水平具有重要价值。"}}
{"id": "2503.12981", "pdf": "https://arxiv.org/pdf/2503.12981", "abs": "https://arxiv.org/abs/2503.12981", "authors": ["Thu Tran", "Kenny Tsu Wei Choo", "Shaohui Foong", "Hitesh Bhardwaj", "Shane Kyi Hla Win", "Wei Jun Ang", "Kenneth Goh", "Rajesh Krishna Balan"], "title": "Analyzing Swimming Performance Using Drone Captured Aerial Videos", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, published to ACM Dronet'24", "summary": "Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m/s for stroke duration and velocity, respectively.", "AI": {"title_translation": "使用无人机航拍视频分析游泳表现", "tldr": "本文提出一种使用无人机航拍视频和计算机视觉算法来准确跟踪游泳运动员表现的新方法，克服了传统多相机和水花遮挡的局限性。", "motivation": "监测游泳运动员表现对提高训练和提升运动技术至关重要。传统跟踪方法（如水上和水下相机）存在需要多台相机和水花遮挡的局限性。", "method": "本文提出一种使用移动无人机跟踪游泳运动员的新方法。该系统利用配备高分辨率摄像头的无人机捕捉航拍画面，然后使用计算机视觉算法处理画面，以提取游泳运动员的位置和动作。该方法具有单相机使用和全面覆盖的优点。系统在训练和比赛视频中进行了准确性评估。", "result": "该系统能够准确跟踪游泳运动员的动作、肢体角度、划水持续时间和速度。划水持续时间和速度的最大误差分别为0.3秒和0.35米/秒。", "conclusion": "Not mentioned in abstract", "translation": "监测游泳运动员表现对于提高训练和增强运动技术至关重要。传统的游泳运动员追踪方法，如水上和水下摄像机，由于需要多台摄像机和水花飞溅的阻碍而面临局限性。本文提出了一种使用移动无人机追踪游泳运动员的新颖方法。所提出的系统采用配备高分辨率摄像头的无人机捕捉游泳运动员的航拍画面。然后使用计算机视觉算法处理这些画面，以提取游泳运动员的位置和动作。这种方法具有多项优势，包括单相机使用和全面覆盖。该系统的准确性通过训练和比赛视频进行了评估。结果表明，该系统能够准确追踪游泳运动员的动作、肢体角度、划水持续时间和速度，其中划水持续时间和速度的最大误差分别为0.3秒和0.35米/秒。", "summary": "本文提出了一种利用无人机航拍视频和计算机视觉算法来分析游泳运动员表现的新颖系统。该系统通过无人机搭载高分辨率相机捕获空中影像，并利用计算机视觉技术处理数据，以准确追踪游泳运动员的位置、动作、肢体角度、划水持续时间和速度。相较于传统多相机和易受水花干扰的方法，该方法具有单相机操作和全面覆盖的优势，并在评估中展现出高精度。", "keywords": "游泳表现分析, 无人机, 计算机视觉, 运动员追踪, 运动技术", "comments": "这项研究通过引入无人机和计算机视觉技术，为游泳表现分析提供了一种创新且高效的解决方案，有效克服了传统方法的局限性。其单相机使用和全面覆盖的特点具有显著的实用价值，有助于教练员和运动员更精确地监测和改进技术。"}}
{"id": "2503.12982", "pdf": "https://arxiv.org/pdf/2503.12982", "abs": "https://arxiv.org/abs/2503.12982", "authors": ["Yunshuang Yuan", "Yan Xia", "Daniel Cremers", "Monika Sester"], "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception can increase the view field and decrease the occlusion\nof an ego vehicle, hence improving the perception performance and safety of\nautonomous driving. Despite the success of previous works on cooperative object\ndetection, they mostly operate on dense Bird's Eye View (BEV) feature maps,\nwhich are computationally demanding and can hardly be extended to long-range\ndetection problems. More efficient fully sparse frameworks are rarely explored.\nIn this work, we design a fully sparse framework, SparseAlign, with three key\nfeatures: an enhanced sparse 3D backbone, a query-based temporal context\nlearning module, and a robust detection head specially tailored for sparse\nfeatures. Extensive experimental results on both OPV2V and DairV2X datasets\nshow that our framework, despite its sparsity, outperforms the state of the art\nwith less communication bandwidth requirements. In addition, experiments on the\nOPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also\nshow a significant performance gain compared to the baseline works.", "AI": {"title_translation": "SparseAlign：一个用于协同目标检测的完全稀疏框架", "tldr": "SparseAlign是一个用于协同目标检测的完全稀疏框架，在计算效率和性能上均优于现有方法。", "motivation": "现有的协同目标检测方法大多在密集的鸟瞰图（BEV）特征图上操作，计算量大且难以扩展到远距离检测问题。目前很少有研究探索更高效的完全稀疏框架。", "method": "本文设计了一个名为SparseAlign的完全稀疏框架，其包含三个关键特性：一个增强的稀疏3D骨干网络、一个基于查询的时间上下文学习模块，以及一个专门为稀疏特征量身定制的鲁棒检测头。", "result": "在OPV2V和DairV2X数据集上的实验结果表明，SparseAlign框架尽管是稀疏的，但其性能优于现有最先进技术，并且所需的通信带宽更少。此外，在OPV2Vt和DairV2Xt数据集上进行的针对时间对齐协同目标检测的实验也显示出与基线工作相比显著的性能提升。", "conclusion": "SparseAlign框架通过采用完全稀疏的方法，在协同目标检测任务中实现了先进的性能，同时显著提高了计算效率并减少了通信带宽，证明了稀疏方法在自动驾驶感知中的有效性。", "translation": "协同感知可以增加自车视野，减少遮挡，从而提高自动驾驶的感知性能和安全性。尽管之前关于协同目标检测的工作取得了成功，但它们大多在密集的鸟瞰图（BEV）特征图上操作，这计算量大，难以扩展到远距离检测问题。更高效的完全稀疏框架鲜有探索。在这项工作中，我们设计了一个完全稀疏的框架SparseAlign，它具有三个关键特性：一个增强的稀疏3D骨干网络、一个基于查询的时间上下文学习模块，以及一个专门为稀疏特征量身定制的鲁棒检测头。在OPV2V和DairV2X数据集上进行的大量实验结果表明，我们的框架尽管是稀疏的，但其性能优于现有技术，并且所需的通信带宽更少。此外，在OPV2Vt和DairV2Xt数据集上进行的针对时间对齐协同目标检测的实验也显示出与基线工作相比显著的性能提升。", "summary": "本文提出了一种名为SparseAlign的完全稀疏框架，用于解决协同目标检测中密集鸟瞰图特征图计算成本高、难以扩展的问题。SparseAlign包含增强的稀疏3D骨干网络、基于查询的时间上下文学习模块和为稀疏特征定制的检测头。实验结果表明，该框架在保持稀疏性的同时，在OPV2V和DairV2X数据集上超越了现有最佳方法，并显著减少了通信带宽，在时间对齐任务上也表现出色。", "keywords": "协同目标检测, 稀疏框架, 自动驾驶, BEV, SparseAlign", "comments": "本文的创新之处在于提出了一个用于协同感知的“完全稀疏框架”，解决了密集BEV方法在计算和可扩展性方面的问题。这对于将协同感知扩展到远距离和实时自动驾驶应用至关重要。其在带宽效率方面的优势也是一个显著的优点。"}}
{"id": "2503.12999", "pdf": "https://arxiv.org/pdf/2503.12999", "abs": "https://arxiv.org/abs/2503.12999", "authors": ["Ruichuan An", "Kai Zeng", "Ming Lu", "Sihan Yang", "Renrui Zhang", "Huitong Ji", "Qizhe Zhang", "Yulin Luo", "Hao Liang", "Wentao Zhang"], "title": "Concept-as-Tree: Synthetic Data is All You Need for VLM Personalization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated exceptional performance in\nvarious multi-modal tasks. Recently, there has been an increasing interest in\nimproving the personalization capabilities of VLMs. To better integrate\nuser-provided concepts into VLMs, many methods use positive and negative\nsamples to fine-tune these models. However, the scarcity of user-provided\npositive samples and the low quality of retrieved negative samples pose\nchallenges for fine-tuning. To reveal the relationship between sample and model\nperformance, we systematically investigate the impact of positive and negative\nsamples (easy and hard) and their diversity on VLM personalization tasks. Based\non the detailed analysis, we introduce Concept-as-Tree (CaT), which represents\na concept as a tree structure, thereby enabling the data generation of positive\nand negative samples with varying difficulty and diversity for VLM\npersonalization. With a well-designed data filtering strategy, our CaT\nframework can ensure the quality of generated data, constituting a powerful\npipeline. We perform thorough experiments with various VLM personalization\nbaselines to assess the effectiveness of the pipeline, alleviating the lack of\npositive samples and the low quality of negative samples. Our results\ndemonstrate that CaT equipped with the proposed data filter significantly\nenhances the personalization capabilities of VLMs across the MyVLM, Yo'LLaVA,\nand MC-LLaVA datasets. To our knowledge, this work is the first controllable\nsynthetic data pipeline for VLM personalization. The code is released at\n\\href{https://github.com/zengkaiya/CaT}{https://github.com/zengkaiya/CaT}.", "AI": {"title_translation": "概念即树：合成数据是VLM个性化所需的一切", "tldr": "本文提出了“概念即树”（CaT）框架，通过生成高质量的合成数据来解决视觉-语言模型（VLM）个性化中用户正样本稀缺和负样本质量低的问题，显著提升了VLM的个性化能力。", "motivation": "视觉-语言模型（VLM）的个性化能力提升面临挑战，主要原因在于用户提供的正样本稀缺以及检索到的负样本质量低下，这给模型的微调带来了困难。", "method": "本文首先系统地研究了正负样本（包括简单和困难样本）及其多样性对VLM个性化任务性能的影响。在此分析基础上，提出了“概念即树”（Concept-as-Tree, CaT）框架，该框架将一个概念表示为树状结构，从而能够生成具有不同难度和多样性的正负样本数据。此外，CaT框架还包含一个精心设计的数据过滤策略，以确保生成数据的质量。", "result": "实验结果表明，配备所提出数据过滤器的CaT框架显著增强了VLM在MyVLM、Yo'LLaVA和MC-LLaVA等数据集上的个性化能力，有效缓解了正样本缺乏和负样本质量低的问题。据作者所知，这是第一个用于VLM个性化的可控合成数据管道。", "conclusion": "“概念即树”（CaT）框架通过其独特的概念树结构和数据过滤策略，成功解决了VLM个性化中面临的数据稀缺和质量问题，并显著提升了模型的个性化性能，为VLM个性化提供了一个强大的可控合成数据生成管道。", "translation": "视觉-语言模型（VLM）在各种多模态任务中表现出色。最近，人们对提高VLM的个性化能力越来越感兴趣。为了更好地将用户提供的概念整合到VLM中，许多方法使用正负样本来微调这些模型。然而，用户提供的正样本稀缺以及检索到的负样本质量低下给微调带来了挑战。为了揭示样本与模型性能之间的关系，我们系统地研究了正负样本（简单和困难）及其多样性对VLM个性化任务的影响。基于详细分析，我们引入了“概念即树”（Concept-as-Tree, CaT）框架，它将概念表示为树结构，从而能够为VLM个性化生成具有不同难度和多样性的正负样本数据。通过精心设计的数据过滤策略，我们的CaT框架可以确保生成数据的质量，构成一个强大的管道。我们对各种VLM个性化基线进行了彻底的实验，以评估该管道的有效性，从而缓解了正样本缺乏和负样本质量低的问题。我们的结果表明，配备所提出数据过滤器的CaT显著增强了VLM在MyVLM、Yo'LLaVA和MC-LLaVA数据集上的个性化能力。据我们所知，这项工作是第一个用于VLM个性化的可控合成数据管道。代码已在https://github.com/zengkaiya/CaT发布。", "summary": "本文针对视觉-语言模型（VLM）个性化中用户正样本稀缺和负样本质量低的问题，提出了一种名为“概念即树”（Concept-as-Tree, CaT）的新型框架。CaT通过将概念表示为树结构，能够生成具有不同难度和多样性的高质量合成正负样本。结合有效的数据过滤策略，CaT框架显著提升了VLM在多个数据集上的个性化能力。这项工作是首个用于VLM个性化的可控合成数据管道，为解决数据挑战提供了创新方案。", "keywords": "VLM个性化, 合成数据, 概念即树, 数据生成, 微调", "comments": "本文的创新之处在于提出了“概念即树”（CaT）这一独特框架，通过将概念结构化为树来生成可控且多样化的合成数据，有效解决了VLM个性化中长期存在的样本稀缺和质量低下问题。其重要性体现在它是该领域首个可控的合成数据管道，为VLM的个性化微调提供了新的范式和强大的工具，对未来的研究具有指导意义。"}}
{"id": "2503.13004", "pdf": "https://arxiv.org/pdf/2503.13004", "abs": "https://arxiv.org/abs/2503.13004", "authors": ["Jiaxu Liu", "Li Li", "Hubert P. H. Shum", "Toby P. Breckon"], "title": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance.", "AI": {"title_translation": "TFDM: 基于时变频率的Mamba点云扩散模型", "tldr": "本文提出了TFDM，一个结合Mamba和时变频率编码器的新型点云扩散框架，用于3D点云生成，实现了SOTA性能并显著降低了计算成本。", "motivation": "尽管Mamba模型在处理长距离依赖和序列数据方面表现出色，但其与3D点云生成相结合的研究仍然有限。", "method": "本文提出了一种名为TFDM的新型扩散框架，包含双潜在Mamba块（DM-Block）和时变频率编码器（TF-Encoder）。DM-Block应用空间填充曲线将点重排序为适用于Mamba状态空间建模的序列，并在潜在空间中操作以减轻计算开销。TF-Encoder利用扩散模型在后期恢复阶段细化细节的能力，通过U-Net架构中优先处理关键点，以增强生成最后阶段的细节质量。", "result": "在ShapeNet-v2数据集上，该方法在特定类别的某些指标上实现了最先进的性能（1-NNA-Abs50 EMD为0.14%，COV EMD为57.90%），同时将计算参数和推理时间分别减少了高达10倍和9倍。", "conclusion": "TFDM成功地将Mamba模型的强大能力应用于3D点云生成，不仅提升了生成质量，还在计算效率上取得了显著改进。", "translation": "扩散模型目前在各种生成任务中表现出令人印象深刻的性能。最近关于图像扩散的工作强调了Mamba（状态空间模型）的强大能力，因为它能有效处理长程依赖和序列数据建模。不幸的是，将状态空间模型与3D点云生成结合的考虑仍然有限。为了利用Mamba模型在3D点云生成方面的强大能力，我们提出了一种新颖的扩散框架，其中包含双潜在Mamba块（DM-Block）和时变频率编码器（TF-Encoder）。DM-Block应用空间填充曲线将点重新排序为适用于Mamba状态空间建模的序列，同时在潜在空间中操作以减轻直接处理3D数据产生的计算开销。同时，TF-Encoder利用扩散模型在后期恢复阶段细化细节的能力，通过U-Net架构中优先处理关键点。这种基于频率的机制确保了生成最后阶段的细节质量得到增强。ShapeNet-v2数据集上的实验结果表明，我们的方法在特定类别的某些指标上实现了最先进的性能（ShapeNet-v2：1-NNA-Abs50 EMD为0.14%，COV EMD为57.90%），同时计算参数和推理时间分别减少了高达10倍和9倍。源代码可在补充材料中获取，并将在接受后发布。", "summary": "本文提出了TFDM，一个新颖的3D点云扩散框架，旨在将Mamba模型的高效长程依赖处理能力引入点云生成。TFDM包含双潜在Mamba块（DM-Block）和时变频率编码器（TF-Encoder），DM-Block将点云转换为序列并在潜在空间中处理，TF-Encoder则通过频率机制在U-Net中增强细节。实验证明，TFDM在ShapeNet-v2数据集上达到了SOTA性能，并显著降低了计算参数和推理时间。", "keywords": "点云生成, 扩散模型, Mamba, 状态空间模型, 时变频率, 3D生成", "comments": "这篇论文通过将Mamba模型引入3D点云扩散生成，填补了该领域的一个空白，展示了将序列模型应用于非序列数据的新颖方法。其创新点在于DM-Block将点云转换为序列以适应Mamba，以及TF-Encoder通过频率机制优化细节。显著的计算效率提升（参数和推理时间大幅减少）是其重要优势，使其在实际应用中更具潜力。"}}
{"id": "2503.13012", "pdf": "https://arxiv.org/pdf/2503.13012", "abs": "https://arxiv.org/abs/2503.13012", "authors": ["Xingguo Lv", "Xingbo Dong", "Liwen Wang", "Jiewen Yang", "Lei Zhao", "Bin Pu", "Zhe Jin", "Xuejun Li"], "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.", "AI": {"title_translation": "测试时间域泛化通过宇宙学习：一种用于医学图像分割的多图匹配方法", "tldr": "该研究提出了一种基于多图匹配的框架，通过引入可学习的宇宙嵌入来整合形态学先验知识，以解决医学图像分割中测试时间域泛化的问题，并在实验中表现优于现有SOTA方法。", "motivation": "尽管域泛化（DG）已显著解决了预训练模型因域偏移导致的性能下降问题，但在实际部署中仍显不足。现有的测试时间适应（TTA）方法在医学图像分割中表现不佳，主要是因为它们忽略了医学图像固有的关键先验知识。", "method": "该方法结合了形态学信息，并提出了一个基于多图匹配的框架。具体来说，它引入了可学习的宇宙嵌入，在多源训练期间整合形态学先验，并提出了新颖的无监督测试时间范式进行域适应。这种方法保证了多匹配中的循环一致性，并使模型能更有效地捕获未见数据的不变先验。", "result": "在两个医学图像分割基准上，该方法在多源和单源域泛化任务中均优于其他最先进的方法。", "conclusion": "通过整合形态学先验和采用多图匹配方法（宇宙学习），本研究提出的框架有效缓解了域偏移的影响，并在医学图像分割的测试时间域泛化任务中取得了卓越的性能。", "translation": "尽管域泛化（DG）已显著解决了预训练模型因域偏移导致的性能下降问题，但在实际部署中仍显不足。测试时间适应（TTA）通过使用未标记的测试数据调整学习模型，提供了一个有前景的解决方案。然而，大多数现有的TTA方法在医学图像分割中难以提供强大的性能，主要是因为它们忽略了医学图像固有的关键先验知识。为了解决这一挑战，我们结合了形态学信息，并提出了一个基于多图匹配的框架。具体来说，我们引入了可学习的宇宙嵌入，在多源训练期间整合形态学先验，并提出了新颖的无监督测试时间范式进行域适应。这种方法保证了多匹配中的循环一致性，同时使模型能更有效地捕获未见数据的不变先验，显著缓解了域偏移的影响。广泛的实验表明，我们的方法在两个医学图像分割基准上，对于多源和单源域泛化任务，均优于其他最先进的方法。源代码可在 https://github.com/Yore0/TTDG-MGM 获取。", "summary": "该论文提出了一种名为“宇宙学习”的多图匹配框架，用于解决医学图像分割中测试时间适应（TTA）的局限性。针对现有TTA方法忽视医学图像形态学先验知识的问题，该框架通过引入可学习的宇宙嵌入，在多源训练中整合形态学先验，并设计了新颖的无监督测试时间范式。此方法确保了多匹配的循环一致性，并能有效捕获未见数据的不变先验，从而显著缓解域偏移。实验结果表明，该方法在两个医学图像分割基准上，无论是在多源还是单源域泛化任务中，均优于现有最先进的方法。", "keywords": "域泛化, 测试时间适应, 医学图像分割, 多图匹配, 宇宙学习", "comments": "该论文的创新点在于通过引入可学习的宇宙嵌入和多图匹配方法，将形态学先验知识有效地融入到医学图像分割的测试时间域泛化中，这解决了现有方法普遍忽视的关键问题。该方法对于提升模型在真实世界部署中的鲁棒性具有重要意义。"}}
{"id": "2503.13016", "pdf": "https://arxiv.org/pdf/2503.13016", "abs": "https://arxiv.org/abs/2503.13016", "authors": ["Zijia Zhao", "Yuqi Huo", "Tongtian Yue", "Longteng Guo", "Haoyu Lu", "Bingning Wang", "Weipeng Chen", "Jing Liu"], "title": "Efficient Motion-Aware Video MLLM", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks.", "AI": {"title_translation": "高效运动感知视频多模态大语言模型", "tldr": "本文提出EMA，一个高效运动感知的视频多模态大语言模型，利用压缩视频结构作为输入，并通过运动感知GOP编码器融合空间和运动信息，实现了最先进的性能并降低了推理成本。", "motivation": "大多数当前视频多模态大语言模型（MLLM）依赖于统一帧采样和图像级编码器，导致数据处理效率低下和运动感知能力有限。", "method": "本文引入EMA，一个高效运动感知视频多模态大语言模型，利用压缩视频结构作为输入。提出了一种运动感知GOP（图像组）编码器，该编码器在GOP单元内融合空间和运动信息，生成紧凑的视觉标记。该方法将更少但更密集的RGB帧与更多但更稀疏的运动向量整合到原生的慢速-快速输入架构中。此外，还引入了MotionBench，一个用于评估四种运动类型（线性、曲线、旋转、接触）运动理解的基准。", "result": "EMA在MotionBench和流行的视频问答基准上均取得了最先进的性能，同时降低了推理成本。EMA还在长视频理解基准上展示了强大的可扩展性。", "conclusion": "EMA通过利用压缩视频结构和创新的运动感知GOP编码器，有效解决了现有视频多模态大语言模型在效率和运动感知方面的局限性，实现了卓越的性能、成本效益和可扩展性。", "translation": "大多数当前的视频多模态大语言模型（MLLM）依赖于统一帧采样和图像级编码器，导致数据处理效率低下和运动感知能力有限。为了解决这些挑战，我们引入了EMA，一种高效运动感知视频多模态大语言模型，它利用压缩视频结构作为输入。我们提出了一种运动感知GOP（图像组）编码器，该编码器在压缩视频流中融合GOP单元内的空间和运动信息，生成紧凑、信息丰富的视觉标记。通过将更少但更密集的RGB帧与更多但更稀疏的运动向量整合到这种原生的慢速-快速输入架构中，我们的方法减少了冗余并增强了运动表示。此外，我们引入了MotionBench，一个用于评估线性、曲线、旋转和接触四种运动类型理解的基准。实验结果表明，EMA在MotionBench和流行的视频问答基准上都取得了最先进的性能，同时降低了推理成本。此外，EMA展示了强大的可扩展性，其在长视频理解基准上的竞争性表现证明了这一点。", "summary": "EMA是一种高效运动感知的视频多模态大语言模型，通过利用压缩视频结构和运动感知GOP编码器，解决了现有视频MLLM数据处理效率低和运动感知能力弱的问题。它融合了空间和运动信息，减少了冗余，并增强了运动表示。EMA在新的MotionBench和现有视频问答基准上均达到了最先进的性能，同时降低了推理成本并展现出强大的可扩展性。", "keywords": "视频多模态大语言模型, 运动感知, 压缩视频, GOP编码器, MotionBench", "comments": "本文通过直接利用压缩视频结构和运动向量，为视频多模态大语言模型提供了一种创新方法，显著提升了效率和运动感知能力。引入MotionBench作为运动理解评估基准，也为该领域的研究提供了有价值的工具。"}}
{"id": "2503.13023", "pdf": "https://arxiv.org/pdf/2503.13023", "abs": "https://arxiv.org/abs/2503.13023", "authors": ["Michal Danilowicz", "Tomasz Kryjak"], "title": "Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA", "categories": ["cs.CV"], "comment": "Accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025", "summary": "Multi-object tracking (MOT) is one of the most important problems in computer\nvision and a key component of any vision-based perception system used in\nadvanced autonomous mobile robotics. Therefore, its implementation on low-power\nand real-time embedded platforms is highly desirable. Modern MOT algorithms\nshould be able to track objects of a given class (e.g. people or vehicles). In\naddition, the number of objects to be tracked is not known in advance, and they\nmay appear and disappear at any time, as well as be obscured. For these\nreasons, the most popular and successful approaches have recently been based on\nthe tracking paradigm. Therefore, the presence of a high quality object\ndetector is essential, which in practice accounts for the vast majority of the\ncomputational and memory complexity of the whole MOT system. In this paper, we\npropose an FPGA (Field-Programmable Gate Array) implementation of an embedded\nMOT system based on a quantized YOLOv8 detector and the SORT (Simple Online\nRealtime Tracker) tracker. We use a modified version of the FINN framework to\nutilize external memory for model parameters and to support operations\nnecessary required by YOLOv8. We discuss the evaluation of detection and\ntracking performance using the COCO and MOT15 datasets, where we achieve 0.21\nmAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC\nsystem (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed\nin reprogrammable logic and the tracking algorithm is implemented in the\nprocessor system.", "AI": {"title_translation": "基于SoC FPGA的YOLOv8和SORT实时多目标跟踪", "tldr": "本文提出了一种在SoC FPGA上实现的多目标跟踪系统，结合量化YOLOv8检测器和SORT跟踪器，旨在实现低功耗和实时性能。", "motivation": "多目标跟踪（MOT）是计算机视觉中的一个重要问题，也是先进自主移动机器人视觉感知系统的关键组成部分。在低功耗和实时嵌入式平台上实现MOT是高度期望的，因为现代MOT算法需要处理未知数量、动态出现消失且可能被遮挡的目标。", "method": "本文提出了一种基于量化YOLOv8检测器和SORT（Simple Online Realtime Tracker）跟踪器的嵌入式MOT系统的FPGA实现。研究人员使用了FINN框架的修改版本来利用外部存储器存储模型参数并支持YOLOv8所需的操作。该系统部署在AMD/Xilinx的Zynq UltraScale+ MPSoC系统上，其中检测器部署在可重编程逻辑中，跟踪算法在处理器系统中实现。", "result": "在COCO和MOT15数据集上，该系统实现了0.21的mAP和38.9的MOTA。计算平台是AMD/Xilinx的Zynq UltraScale+ MPSoC系统。", "conclusion": "本文成功地在SoC FPGA上实现了基于量化YOLOv8和SORT的实时多目标跟踪系统，验证了其在嵌入式平台上的可行性和性能，尤其适用于对功耗和实时性有严格要求的机器人应用。", "translation": "多目标跟踪（MOT）是计算机视觉中最重要的一个问题，也是先进自主移动机器人中任何基于视觉的感知系统的关键组成部分。因此，在低功耗和实时嵌入式平台上实现它具有高度的吸引力。现代MOT算法应该能够跟踪给定类别的对象（例如人或车辆）。此外，要跟踪的对象数量事先未知，它们可能随时出现和消失，也可能被遮挡。由于这些原因，最近最流行和成功的方法都基于跟踪范式。因此，高质量目标检测器的存在至关重要，这在实践中占据了整个MOT系统绝大部分的计算和内存复杂性。在本文中，我们提出了一种基于量化YOLOv8检测器和SORT（Simple Online Realtime Tracker）跟踪器的嵌入式MOT系统的FPGA（Field-Programmable Gate Array）实现。我们使用FINN框架的修改版本来利用外部存储器存储模型参数并支持YOLOv8所需的操作。我们讨论了使用COCO和MOT15数据集对检测和跟踪性能的评估，我们分别达到了0.21 mAP和38.9 MOTA。作为计算平台，我们使用一个MPSoC系统（来自AMD/Xilinx的Zynq UltraScale+设备），其中检测器部署在可重编程逻辑中，跟踪算法在处理器系统中实现。", "summary": "本文提出了一种在片上系统（SoC）FPGA上实现实时多目标跟踪（MOT）的方法，该方法结合了量化YOLOv8检测器和SORT跟踪器。鉴于MOT在先进自主移动机器人中的关键作用以及对低功耗、实时嵌入式平台的需求，研究人员通过修改FINN框架，在AMD/Xilinx Zynq UltraScale+ MPSoC设备上部署了该系统，其中YOLOv8部署在可重编程逻辑中，SORT在处理器系统中。实验结果显示，在COCO和MOT15数据集上，系统分别达到了0.21 mAP和38.9 MOTA的性能。", "keywords": "多目标跟踪, YOLOv8, SORT, FPGA, 嵌入式系统", "comments": "该论文的创新点在于将先进的YOLOv8检测器（经过量化）与SORT跟踪器结合，并成功地在SoC FPGA上实现了这一复杂的实时多目标跟踪系统。这种FPGA实现对于需要低功耗和高实时性的嵌入式应用（如自主机器人）具有重要意义，因为它能够有效处理计算和内存密集型任务。然而，0.21 mAP和38.9 MOTA的性能指标可能表明在检测和跟踪精度方面仍有提升空间，尤其是在与更强大的GPU或ASIC解决方案相比时。但对于资源受限的嵌入式平台而言，这是一个有价值的探索。"}}
{"id": "2503.13025", "pdf": "https://arxiv.org/pdf/2503.13025", "abs": "https://arxiv.org/abs/2503.13025", "authors": ["ChangHee Yang", "Hyeonseop Song", "Seokhun Choi", "Seungwoo Lee", "Jaechul Kim", "Hoseok Do"], "title": "PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data", "categories": ["cs.CV", "cs.AI"], "comment": "The first three authors contributed equally to this work", "summary": "Despite considerable efforts to enhance the generalization of 3D pose\nestimators without costly 3D annotations, existing data augmentation methods\nstruggle in real world scenarios with diverse human appearances and complex\nposes. We propose PoseSyn, a novel data synthesis framework that transforms\nabundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn\ncomprises two key components: Error Extraction Module (EEM), which identifies\nchallenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),\nwhich synthesizes motion sequences around the challenging poses. Then, by\ngenerating realistic 3D training data via a human animation model aligned with\nchallenging poses and appearances PoseSyn boosts the accuracy of various 3D\npose estimators by up to 14% across real world benchmarks including various\nbackgrounds and occlusions, challenging poses, and multi view scenarios.\nExtensive experiments further confirm that PoseSyn is a scalable and effective\napproach for improving generalization without relying on expensive 3D\nannotations, regardless of the pose estimator's model size or design.", "AI": {"title_translation": "PoseSyn：从野外2D数据合成多样化3D姿态数据", "tldr": "PoseSyn是一个新颖的数据合成框架，它将大量的野外2D姿态数据集转换为多样化的3D姿态图像对，显著提高了3D姿态估计器的准确性，无需昂贵的3D标注。", "motivation": "尽管在没有昂贵3D标注的情况下，研究人员付出了大量努力来增强3D姿态估计器的泛化能力，但现有数据增强方法在面对多样化人体外观和复杂姿态的真实世界场景时表现不佳。", "method": "本文提出了PoseSyn，一个新颖的数据合成框架，它将大量的野外2D姿态数据集转换为多样化3D姿态图像对。PoseSyn包含两个关键组件：错误提取模块（EEM），用于从2D姿态数据集中识别出具有挑战性的姿态；以及运动合成模块（MSM），用于围绕这些挑战性姿态合成运动序列。然后，通过与挑战性姿态和外观对齐的人体动画模型生成逼真的3D训练数据。", "result": "PoseSyn在真实世界基准测试中，将各种3D姿态估计器的准确性提高了高达14%，这些基准测试包括各种背景和遮挡、挑战性姿态以及多视角场景。广泛的实验进一步证实，PoseSyn是一种可扩展且有效的方法，可以改善泛化能力，而无需依赖昂贵的3D标注，无论姿态估计器的模型大小或设计如何。", "conclusion": "PoseSyn通过合成多样化的3D姿态数据，有效解决了3D姿态估计器在真实世界场景中泛化能力不足的问题，且无需昂贵的3D标注，证明了其在提高准确性和可扩展性方面的有效性。", "translation": "尽管在没有昂贵3D标注的情况下，研究人员付出了大量努力来增强3D姿态估计器的泛化能力，但现有数据增强方法在面对多样化人体外观和复杂姿态的真实世界场景时表现不佳。我们提出了PoseSyn，一个新颖的数据合成框架，它将大量的野外2D姿态数据集转换为多样化3D姿态图像对。PoseSyn包含两个关键组件：错误提取模块（EEM），用于从2D姿态数据集中识别出具有挑战性的姿态；以及运动合成模块（MSM），用于围绕这些挑战性姿态合成运动序列。然后，通过与挑战性姿态和外观对齐的人体动画模型生成逼真的3D训练数据，PoseSyn在真实世界基准测试中，将各种3D姿态估计器的准确性提高了高达14%，这些基准测试包括各种背景和遮挡、挑战性姿态以及多视角场景。广泛的实验进一步证实，PoseSyn是一种可扩展且有效的方法，可以改善泛化能力，而无需依赖昂贵的3D标注，无论姿态估计器的模型大小或设计如何。", "summary": "PoseSyn是一个创新的数据合成框架，旨在解决3D姿态估计器在复杂真实世界场景中泛化能力不足的问题，同时避免昂贵的3D标注。它通过错误提取模块识别2D数据中的挑战性姿态，并利用运动合成模块生成围绕这些姿态的运动序列，从而将野外2D数据转化为多样化的3D姿态图像对。实验证明，PoseSyn显著提升了多种3D姿态估计器在真实世界基准上的准确性，高达14%，并且具有良好的可扩展性和通用性。", "keywords": "3D姿态估计, 数据合成, 2D到3D, 数据增强, 泛化能力", "comments": "PoseSyn的创新点在于其利用易于获取的2D数据来合成多样化3D姿态数据的能力，这显著降低了对昂贵3D标注的依赖。其EEM和MSM组件的结合，使得合成的数据能够有效模拟真实世界中的复杂性和挑战性，从而提高了3D姿态估计器的泛化能力。这是一个非常实用的进步，对于减少数据标注成本和提升模型在实际应用中的鲁棒性具有重要意义。"}}
{"id": "2503.13026", "pdf": "https://arxiv.org/pdf/2503.13026", "abs": "https://arxiv.org/abs/2503.13026", "authors": ["Tao Wang", "Changxu Cheng", "Lingfeng Wang", "Senda Chen", "Wuyue Zhao"], "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model", "categories": ["cs.CV"], "comment": "technical report", "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.", "AI": {"title_translation": "HiMTok：学习分层掩码令牌用于大型多模态模型的图像分割", "tldr": "HiMTok引入分层掩码令牌，无需原始图像即可实现紧凑且由粗到精的图像分割，并在各项任务中达到SOTA。", "motivation": "当前大型多模态模型（LMM）驱动的图像分割方法存在掩码表示不足和架构复杂的问题，限制了LMM的潜力。", "method": "本文提出了分层掩码分词器（HiMTok），它使用最多32个令牌表示分割掩码，并在掩码去令牌化过程中无需原始图像输入。HiMTok支持紧凑且由粗到精的掩码表示，并与大型语言模型的下一令牌预测范式对齐。开发了一个三阶段训练方案，采用分层掩码损失实现有效的由粗到精学习，并支持边界框和掩码令牌之间的双向信息流以充分利用多任务训练潜力。", "result": "HiMTok在各种分割任务中实现了最先进的性能，同时增强了视觉基础并保持了整体视觉理解。", "conclusion": "HiMTok通过提供优越的掩码表示和架构，有效解决了现有LMM驱动分割方法的局限性，从而在图像分割任务中取得了最先进的性能并提升了视觉理解能力。", "translation": "大型多模态模型（LMM）的卓越性能引起了图像分割领域的广泛关注。为了与下一令牌预测范式对齐，当前的LMM驱动分割方法要么使用目标边界点来表示掩码，要么引入特殊的分割令牌，其隐藏状态由需要原始图像作为输入的分割模型解码。然而，这些方法通常存在掩码表示不足和架构复杂的问题，限制了LMM的潜力。在这项工作中，我们提出了分层掩码分词器（HiMTok），它使用最多32个令牌表示分割掩码，并在掩码去令牌化过程中无需原始图像。HiMTok允许紧凑且由粗到精的掩码表示，与LLM的下一令牌预测范式良好对齐，并有助于直接获取分割能力。我们开发了一个三阶段训练方案，用于逐步学习分割和视觉能力，其特点是分层掩码损失，用于有效的由粗到精学习。此外，我们启用了双向信息流，允许边界框和掩码令牌之间的转换，以充分利用多任务训练潜力。广泛的实验表明，我们的方法在各种分割任务中实现了最先进的性能，同时增强了视觉基础并保持了整体视觉理解。", "summary": "本文提出了HiMTok，一种新颖的分层掩码分词器，用于利用大型多模态模型进行图像分割。它通过使用分层令牌表示分割掩码，解决了现有方法的局限性，并在去令牌化过程中无需原始图像输入。HiMTok支持紧凑且由粗到精的掩码表示，并采用三阶段训练方案，结合分层掩码损失。它还支持边界框和掩码令牌之间的双向转换，以实现多任务训练。实验表明，HiMTok在各种分割任务中实现了最先进的性能，同时增强了视觉基础和整体视觉理解。", "keywords": "分层掩码令牌, 图像分割, 大型多模态模型, HiMTok, 由粗到精", "comments": "本文提出了一种创新方法，通过将掩码表示深度集成到LMM的下一令牌预测范式中，实现了图像分割，这是迈向更统一和高效的多模态模型的重要一步。在去令牌化过程中无需原始图像输入，简化了架构并提高了效率。分层掩码令牌和三阶段训练是关键创新点。"}}
{"id": "2503.13028", "pdf": "https://arxiv.org/pdf/2503.13028", "abs": "https://arxiv.org/abs/2503.13028", "authors": ["Tony Danjun Wang", "Lennart Bastian", "Tobias Czempiel", "Christian Heiliger", "Nassir Navab"], "title": "Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room", "categories": ["cs.CV", "J.3"], "comment": "26 pages, 14 figures, Submitted to Medical Image Analysis", "summary": "Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination.", "AI": {"title_translation": "超越基于角色的手术领域建模：手术室中的可泛化再识别", "tldr": "本文提出了一种新颖的以员工为中心的手术人员再识别框架，通过分析其独特的运动模式和物理特征，实现了在手术室中对个体人员的长期跟踪和分析，并在跨环境再识别方面表现出色。", "motivation": "现有的手术领域模型主要基于角色预测来优化工作流程，但有证据表明团队熟悉度和个体差异会影响手术结果。因此，需要一种能够识别和跟踪个体医护人员而非仅其角色的方法。", "method": "本文提出了一种新颖的以员工为中心的建模方法，通过分析个体团队成员独特的运动模式和物理特征来识别他们。为解决诊所间变异性，开发了一个可泛化的再识别框架，该框架编码3D点云序列以捕捉个体独特的形状和关节运动模式。", "result": "该方法在真实临床数据上实现了86.19%的准确率，在不同环境间转移时仍能保持75.27%的准确率，比现有方法提高了12%。在增强无标记人员跟踪时，准确率提高了50%以上。", "conclusion": "通过在三个数据集上的广泛验证和引入一种新颖的工作流可视化技术，该框架能够揭示手术团队动态和空间利用模式的新见解，从而推动手术工作流和团队协作分析方法的发展。", "translation": "手术领域模型通过自动化预测每位工作人员的手术角色来优化工作流程。然而，越来越多的证据表明，团队熟悉度和个体差异会影响手术结果。我们提出了一种新颖的以员工为中心的建模方法，通过其独特的运动模式和物理特征来表征个体团队成员，从而实现跨多个手术程序的长期跟踪和分析手术人员。为解决诊所间变异性的挑战，我们开发了一个可泛化的再识别框架，该框架编码3D点云序列以捕捉每个个体独特的形状和关节运动模式。我们的方法在真实临床数据上达到了86.19%的准确率，同时在不同环境间转移时保持了75.27%的准确率——比现有方法提高了12%。当用于增强无标记人员跟踪时，我们的方法将准确率提高了50%以上。通过对三个数据集的广泛验证和引入一种新颖的工作流可视化技术，我们展示了我们的框架如何揭示手术团队动态和空间利用模式的新见解，从而推动手术工作流和团队协作分析方法的发展。", "summary": "本文提出了一种超越传统基于角色的手术领域模型的新方法，旨在通过对个体医护人员的独特运动模式和物理特征进行建模，实现手术室中人员的长期跟踪和再识别。该方法利用3D点云序列构建了一个可泛化的再识别框架，有效应对了跨诊所变异性。实验结果表明，该方法在临床数据上具有高准确性，并且在跨环境迁移和增强无标记跟踪方面均显著优于现有技术，为分析手术工作流和团队协作提供了新的视角和工具。", "keywords": "手术领域建模, 再识别, 3D点云, 手术室, 团队动态", "comments": "这项研究的创新之处在于其从传统的“基于角色”模型转向“以员工为中心”的个体识别方法，并通过捕捉细微的运动和物理特征来实现高精度再识别。该框架的可泛化性解决了跨诊所数据变异性的挑战，使其在真实临床环境中具有很高的实用价值。通过提高人员跟踪准确性，并揭示团队动态和空间利用模式，该工作为手术室效率和安全性提供了新的分析维度，具有重要的临床应用潜力。"}}
{"id": "2503.13045", "pdf": "https://arxiv.org/pdf/2503.13045", "abs": "https://arxiv.org/abs/2503.13045", "authors": ["Gabriele Berton", "Kevin Musgrave", "Carlo Masone"], "title": "All You Need to Know About Training Image Retrieval Models", "categories": ["cs.CV"], "comment": null, "summary": "Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https://github.com/gmberton/image-retrieval", "AI": {"title_translation": "训练图像检索模型所需了解的一切", "tldr": "本文通过大量实验，系统研究了图像检索模型训练中各种因素对检索准确性的影响，并发现了跨数据集通用的最佳实践。", "motivation": "图像检索模型的性能受多种训练时因素影响，但这些因素对检索准确性的具体影响尚不明确，因此需要深入理解这些因素。", "method": "通过进行数万次训练运行，系统地分析了嵌入模型架构、损失函数、数据采样器、挖掘函数、学习率和批量大小等训练因素对检索准确性的影响。", "result": "发现了不同训练因素对检索准确性的具体影响，并总结出了在多个数据集上通用的最佳实践。", "conclusion": "通过大规模实证研究，本文揭示了影响图像检索模型性能的关键训练因素，并提供了可泛化的最佳实践，有助于优化图像检索模型的训练。", "translation": "图像检索的任务是在数据库中找到与给定查询图像最相似的图像。图像检索管道的性能取决于许多训练时因素，包括嵌入模型架构、损失函数、数据采样器、挖掘函数、学习率和批量大小。在这项工作中，我们进行了数万次训练运行，以了解这些因素中的每一个对检索准确性的影响。我们还发现了跨多个数据集都适用的最佳实践。代码可在 https://github.com/gmberton/image-retrieval 获取。", "summary": "本文深入探究了影响图像检索模型性能的关键训练因素。研究人员通过进行数万次训练实验，系统分析了嵌入模型架构、损失函数、数据采样器、挖掘函数、学习率和批量大小等因素对检索准确性的影响，并提炼出适用于多种数据集的最佳实践。", "keywords": "图像检索, 模型训练, 最佳实践, 深度学习, 实验研究", "comments": "这篇论文通过大规模的实证研究，系统地揭示了图像检索模型训练中的关键因素及其对性能的影响，为实践者提供了宝贵的指导和最佳实践，具有很强的实用价值。其创新点在于通过大量的实验数据来验证和发现规律，而不是仅仅依赖理论分析，填补了该领域实践指导的空白。"}}
{"id": "2503.13047", "pdf": "https://arxiv.org/pdf/2503.13047", "abs": "https://arxiv.org/abs/2503.13047", "authors": ["Ruiqi Song", "Xianda Guo", "Hangbin Wu", "Qinggong Wei", "Long Chen"], "title": "InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Directly generating planning results from raw sensors has become increasingly\nprevalent due to its adaptability and robustness in complex scenarios. Scene\nrepresentation, as a key module in the pipeline, has traditionally relied on\nconventional perception, which focus on the global scene. However, in driving\nscenarios, human drivers typically focus only on regions that directly impact\ndriving, which often coincide with those required for end-to-end autonomous\ndriving. In this paper, a novel end-to-end autonomous driving method called\nInsightDrive is proposed, which organizes perception by language-guided scene\nrepresentation. We introduce an instance-centric scene tokenizer that\ntransforms the surrounding environment into map- and object-aware instance\ntokens. Scene attention language descriptions, which highlight key regions and\nobstacles affecting the ego vehicle's movement, are generated by a\nvision-language model that leverages the cognitive reasoning capabilities of\nfoundation models. We then align scene descriptions with visual features using\nthe vision-language model, guiding visual attention through these descriptions\nto give effectively scene representation. Furthermore, we employ self-attention\nand cross-attention mechanisms to model the ego-agents and ego-map\nrelationships to comprehensively build the topological relationships of the\nscene. Finally, based on scene understanding, we jointly perform motion\nprediction and planning. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate that the proposed InsightDrive achieves state-of-the-art\nperformance in end-to-end autonomous driving. The code is available at\nhttps://github.com/songruiqi/InsightDrive", "AI": {"title_translation": "InsightDrive：面向端到端自动驾驶的洞察场景表示", "tldr": "本文提出了一种名为InsightDrive的端到端自动驾驶新方法，通过语言引导的场景表示来组织感知，利用视觉语言模型生成场景注意力描述，并在nuScenes基准测试中达到了最先进的性能。", "motivation": "传统的场景表示方法侧重于全局场景感知，而人类驾驶员通常只关注直接影响驾驶的关键区域。端到端自动驾驶也需要这种聚焦的感知，因此需要一种新的场景表示方法来提高在复杂场景中的适应性和鲁棒性。", "method": "本文提出InsightDrive，一种通过语言引导的场景表示来组织感知的端到端自动驾驶方法。它引入了一个以实例为中心的场景分词器，将环境转换为地图感知和物体感知的实例令牌。通过视觉语言模型（VLM）利用基础模型的认知推理能力，生成突出关键区域和障碍的场景注意力语言描述。然后，该模型将场景描述与视觉特征对齐，以指导有效的场景表示。此外，它采用自注意力机制和交叉注意力机制来建模自我代理和自我地图关系，以全面构建场景的拓扑关系。最后，基于场景理解，联合执行运动预测和规划。", "result": "在广泛使用的nuScenes基准测试上进行的广泛实验表明，所提出的InsightDrive在端到端自动驾驶方面取得了最先进的性能。", "conclusion": "InsightDrive通过其新颖的语言引导场景表示方法，有效地实现了端到端自动驾驶中的先进性能。", "translation": "直接从原始传感器生成规划结果因其在复杂场景中的适应性和鲁棒性而变得越来越普遍。作为流程中关键模块的场景表示，传统上依赖于关注全局场景的传统感知。然而，在驾驶场景中，人类驾驶员通常只关注直接影响驾驶的区域，这些区域也常常与端到端自动驾驶所需的区域重合。本文提出了一种名为InsightDrive的新型端到端自动驾驶方法，该方法通过语言引导的场景表示来组织感知。我们引入了一个以实例为中心的场景分词器，将周围环境转换为地图感知和物体感知的实例令牌。场景注意力语言描述，突出影响自我车辆移动的关键区域和障碍物，由一个视觉语言模型生成，该模型利用了基础模型的认知推理能力。然后，我们使用视觉语言模型将场景描述与视觉特征对齐，通过这些描述引导视觉注意力，从而有效地表示场景。此外，我们采用自注意力机制和交叉注意力机制来建模自我代理和自我地图关系，以全面构建场景的拓扑关系。最后，基于场景理解，我们联合执行运动预测和规划。在广泛使用的nuScenes基准测试上进行的广泛实验表明，所提出的InsightDrive在端到端自动驾驶方面取得了最先进的性能。代码可在https://github.com/songruiqi/InsightDrive获取。", "summary": "InsightDrive是一种新型的端到端自动驾驶方法，它通过语言引导的场景表示来优化感知。该方法引入了一个以实例为中心的场景分词器，将环境转化为地图和物体感知的令牌。利用视觉语言模型，它生成场景注意力语言描述来指导视觉注意力，并结合自注意力和交叉注意力机制构建场景拓扑关系。最终，模型在此基础上进行运动预测和规划，并在nuScenes基准测试中展现出最先进的性能。", "keywords": "端到端自动驾驶, 场景表示, 视觉语言模型, 语言引导, InsightDrive", "comments": "InsightDrive的创新之处在于其将语言引导的场景表示引入端到端自动驾驶，模仿人类驾驶员的聚焦感知。通过结合视觉语言模型和注意力机制，它有效地从全局场景中提取出与驾驶相关的关键信息，这对于提升自动驾驶系统在复杂环境下的鲁棒性和适应性至关重要。该方法在概念上新颖，并在SOTA性能上得到了验证，为未来的自动驾驶研究提供了新的视角。"}}
{"id": "2503.13053", "pdf": "https://arxiv.org/pdf/2503.13053", "abs": "https://arxiv.org/abs/2503.13053", "authors": ["Nassim Ali Ousalah", "Anis Kacem", "Enjie Ghorbel", "Emmanuel Koumandakis", "Djamila Aouada"], "title": "Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.", "AI": {"title_translation": "用于紧凑高效6自由度姿态估计的不确定性感知知识蒸馏", "tldr": "该论文提出了一种新颖的不确定性感知知识蒸馏框架，通过利用教师模型关键点预测的不确定性，实现了紧凑高效且准确的6自由度姿态估计。", "motivation": "在机器人、增强现实和空间自主导航系统等应用中，紧凑高效的6自由度物体姿态估计至关重要，因为轻量级模型对于实时准确性能至关重要。", "method": "本文提出了一种新颖的端到端不确定性感知知识蒸馏（KD）框架，专注于基于关键点的6自由度姿态估计。该方法利用大型教师模型预测的关键点所表现出的不同不确定性水平，在蒸馏过程中调整知识转移，以增强学生模型的准确性并确保其紧凑性。具体而言，通过根据每个教师关键点预测相关的不确定性来调整知识转移，从而对齐学生和教师的预测。此外，所提出的KD利用这种不确定性感知的关键点对齐来在其各自特征图的关键位置传输知识。", "result": "在广泛使用的LINEMOD基准测试上进行的实验表明，与现有最先进的方法相比，我们的方法使用轻量级模型实现了卓越的6自由度物体姿态估计。在SPEED+数据集上对航天器姿态估计的进一步验证突出了我们方法在不同6自由度姿态估计场景下的鲁棒性。", "conclusion": "本文提出的不确定性感知知识蒸馏框架能够有效提升6自由度姿态估计的准确性，同时保持模型的紧凑性和效率，并在不同场景下表现出良好的鲁棒性。", "translation": "紧凑高效的6自由度物体姿态估计在机器人、增强现实和空间自主导航系统等应用中至关重要，在这些应用中，轻量级模型对于实时准确性能至关重要。本文介绍了一种新颖的端到端不确定性感知知识蒸馏（KD）框架，专注于基于关键点的6自由度姿态估计。大型教师模型预测的关键点表现出不同程度的不确定性，这些不确定性可以在蒸馏过程中加以利用，以提高学生模型的准确性，同时确保其紧凑性。为此，我们提出了一种蒸馏策略，通过根据每个教师关键点预测相关的不确定性来调整知识转移，从而对齐学生和教师的预测。此外，所提出的KD利用这种不确定性感知的关键点对齐来在其各自特征图的关键位置传输知识。在广泛使用的LINEMOD基准测试上进行的实验证明了我们方法的有效性，与现有最先进的方法相比，使用轻量级模型实现了卓越的6自由度物体姿态估计。在SPEED+数据集上对航天器姿态估计的进一步验证突出了我们方法在不同6自由度姿态估计场景下的鲁棒性。", "summary": "本文提出了一种新颖的不确定性感知知识蒸馏（KD）框架，用于紧凑高效的基于关键点的6自由度姿态估计。该方法利用教师模型关键点预测的不确定性来指导知识转移，从而在提高学生模型准确性的同时保持其轻量级。实验结果表明，该方法在LINEMOD和SPEED+数据集上均优于现有先进方法，证明了其在不同场景下实现高性能和鲁棒性的能力。", "keywords": "不确定性感知, 知识蒸馏, 6自由度姿态估计, 紧凑模型, 关键点", "comments": "该论文的创新点在于将不确定性感知引入知识蒸馏过程，特别是在关键点级别的姿态估计中。通过利用教师模型预测的不确定性来指导知识转移，该方法能够更有效地将知识从大型教师模型传递给紧凑的学生模型，从而在保证模型轻量化的同时提升了姿态估计的精度和鲁棒性。这对于资源受限的实时应用具有重要意义。"}}
{"id": "2503.13058", "pdf": "https://arxiv.org/pdf/2503.13058", "abs": "https://arxiv.org/abs/2503.13058", "authors": ["Zeyi Huang", "Utkarsh Ojha", "Yuyang Ji", "Donghyun Lee", "Yong Jae Lee"], "title": "Do Vision Models Develop Human-Like Progressive Difficulty Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "When a human undertakes a test, their responses likely follow a pattern: if\nthey answered an easy question $(2 \\times 3)$ incorrectly, they would likely\nanswer a more difficult one $(2 \\times 3 \\times 4)$ incorrectly; and if they\nanswered a difficult question correctly, they would likely answer the easy one\ncorrectly. Anything else hints at memorization. Do current visual recognition\nmodels exhibit a similarly structured learning capacity? In this work, we\nconsider the task of image classification and study if those models' responses\nfollow that pattern. Since real images aren't labeled with difficulty, we first\ncreate a dataset of 100 categories, 10 attributes, and 3 difficulty levels\nusing recent generative models: for each category (e.g., dog) and attribute\n(e.g., occlusion), we generate images of increasing difficulty (e.g., a dog\nwithout occlusion, a dog only partly visible). We find that most of the models\ndo in fact behave similarly to the aforementioned pattern around 80-90% of the\ntime. Using this property, we then explore a new way to evaluate those models.\nInstead of testing the model on every possible test image, we create an\nadaptive test akin to GRE, in which the model's performance on the current\nround of images determines the test images in the next round. This allows the\nmodel to skip over questions too easy/hard for itself, and helps us get its\noverall performance in fewer steps.", "AI": {"title_translation": "视觉模型是否会发展出类人般的渐进式难度理解能力？", "tldr": "本文研究视觉模型是否像人类一样，在理解渐进式难度方面表现出结构化学习能力，并发现它们在80-90%的情况下表现相似，进而提出了一种新的自适应评估方法。", "motivation": "人类在考试中对问题的回答通常遵循一个模式：如果一个简单的问题回答错误，那么一个更难的问题也可能回答错误；如果一个难题回答正确，那么一个简单的问题也可能回答正确。这表明了结构化的学习能力而非死记硬背。本文旨在探究当前视觉识别模型是否也表现出类似的结构化学习能力。", "method": "研究人员首先创建了一个包含100个类别、10个属性和3个难度级别的数据集，利用生成模型为每个类别和属性生成不同难度的图像（例如，无遮挡的狗与部分可见的狗）。然后，他们观察模型对这些图像的反应是否遵循人类的渐进式难度理解模式。在此基础上，他们探索了一种新的模型评估方法，即一种类似于GRE的自适应测试，根据模型在当前轮次图像上的表现来决定下一轮的测试图像，从而减少评估步骤。", "result": "研究发现，大多数视觉模型在80-90%的时间里确实表现出与人类相似的渐进式难度理解模式。利用这一特性，研究人员提出了一种新的自适应测试方法，该方法允许模型跳过过简单或过难的问题，并能在更少的步骤中获取模型的整体性能。", "conclusion": "视觉模型在理解渐进式难度方面表现出与人类相似的模式，这为模型评估提供了一种新的、更高效的自适应测试方法。", "translation": "当人类进行测试时，他们的反应很可能遵循一种模式：如果他们错误地回答了一个简单的问题（2 × 3），他们也很可能会错误地回答一个更难的问题（2 × 3 × 4）；而如果他们正确地回答了一个难题，他们也很可能会正确地回答简单的问题。任何其他情况都暗示着死记硬背。当前的视觉识别模型是否表现出类似的结构化学习能力？在这项工作中，我们考虑图像分类任务，并研究这些模型的反应是否遵循这种模式。由于真实图像没有难度标签，我们首先使用最近的生成模型创建了一个包含100个类别、10个属性和3个难度级别的数据集：对于每个类别（例如，狗）和属性（例如，遮挡），我们生成了难度递增的图像（例如，没有遮挡的狗，只有部分可见的狗）。我们发现大多数模型在80-90%的时间里确实表现出与上述模式相似的行为。利用这一特性，我们接着探索了一种评估这些模型的新方法。我们不再对模型进行所有可能的测试图像的测试，而是创建了一种类似于GRE的自适应测试，其中模型在当前轮次图像上的表现决定了下一轮的测试图像。这使得模型可以跳过对其来说过简单/过难的问题，并帮助我们在更少的步骤中获得其整体性能。", "summary": "本文探讨了视觉模型是否像人类一样具备渐进式难度理解能力。通过创建一个具有明确难度层级的新数据集，研究发现多数模型在80-90%的情况下展现出与人类相似的模式。基于此发现，研究提出了一种创新的自适应测试方法，类似于GRE考试，能根据模型的表现动态调整测试难度，从而更高效地评估模型的整体性能。", "keywords": "视觉模型, 难度理解, 自适应测试, 图像分类, 生成数据", "comments": "这篇论文的创新点在于构建了一个带有难度标签的合成数据集，并基于此提出了一个新颖的自适应评估方法。这种方法有望提高视觉模型评估的效率和准确性，尤其是在资源有限或需要快速迭代的场景下。它也为理解模型学习机制提供了一个新的视角。"}}
{"id": "2503.13060", "pdf": "https://arxiv.org/pdf/2503.13060", "abs": "https://arxiv.org/abs/2503.13060", "authors": ["Harshal Kausadikar", "Tanvi Kale", "Onkar Susladkar", "Sparsh Mittal"], "title": "Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework for Transliteration of Modi Script to Devanagari", "categories": ["cs.CV"], "comment": "Under submission at a conference", "summary": "In medieval India, the Marathi language was written using the Modi script.\nThe texts written in Modi script include extensive knowledge about medieval\nsciences, medicines, land records and authentic evidence about Indian history.\nAround 40 million documents are in poor condition and have not yet been\ntransliterated. Furthermore, only a few experts in this domain can\ntransliterate this script into English or Devanagari. Most of the past research\npredominantly focuses on individual character recognition. A system that can\ntransliterate Modi script documents to Devanagari script is needed. We propose\nthe MoDeTrans dataset, comprising 2,043 images of Modi script documents\naccompanied by their corresponding textual transliterations in Devanagari. We\nfurther introduce MoScNet (\\textbf{Mo}di \\textbf{Sc}ript \\textbf{Net}work), a\nnovel Vision-Language Model (VLM) framework for transliterating Modi script\nimages into Devanagari text. MoScNet leverages Knowledge Distillation, where a\nstudent model learns from a teacher model to enhance transliteration\nperformance. The final student model of MoScNet has better performance than the\nteacher model while having 163$\\times$ lower parameters. Our work is the first\nto perform direct transliteration from the handwritten Modi script to the\nDevanagari script. MoScNet also shows competitive results on the optical\ncharacter recognition (OCR) task.", "AI": {"title_translation": "历史手稿到现代视觉：一种用于莫迪文字到天城文音译的新型数据集和视觉语言模型框架", "tldr": "本文提出了MoDeTrans数据集和MoScNet视觉语言模型框架，首次实现了手写莫迪文字到天城文的直接音译，并展示了卓越的性能和效率。", "motivation": "大量莫迪文字文献包含重要历史知识，但保存状况差且未被音译，专家稀缺，现有研究多集中于字符识别，缺乏将莫迪文字文档直接音译为天城文的系统。", "method": "本文提出了MoDeTrans数据集，包含2043张莫迪文字文档图片及其对应的天城文音译文本。在此基础上，引入了MoScNet（莫迪文字网络），一个新颖的视觉语言模型（VLM）框架，用于将莫迪文字图片直接音译为天城文文本。MoScNet利用知识蒸馏技术，使学生模型从教师模型学习以增强音译性能。", "result": "MoScNet的最终学生模型性能优于教师模型，同时参数量减少了163倍。这是首次实现从手写莫迪文字到天城文的直接音译。MoScNet在光学字符识别（OCR）任务上也表现出有竞争力的结果。", "conclusion": "该研究通过创建MoDeTrans数据集和开发MoScNet VLM框架，成功实现了手写莫迪文字到天城文的直接音译，解决了现有技术难题，并展示了高效和高性能。", "translation": "在印度中世纪，马拉地语使用莫迪文字书写。莫迪文字文本包含大量关于中世纪科学、医学、土地记录以及印度历史的真实证据。大约有4000万份文件状况不佳，尚未被音译。此外，该领域只有少数专家能够将这种文字音译成英语或天城文。过去的大部分研究主要集中于单个字符识别。因此，需要一个能够将莫迪文字文档音译成天城文的系统。我们提出了MoDeTrans数据集，该数据集包含2043张莫迪文字文档图片及其对应的天城文文本音译。我们进一步引入了MoScNet（莫迪文字网络），一个新颖的视觉语言模型（VLM）框架，用于将莫迪文字图片音译成天城文文本。MoScNet利用知识蒸馏技术，其中学生模型从教师模型学习以增强音译性能。MoScNet的最终学生模型性能优于教师模型，同时参数量减少了163倍。我们的工作是首次实现从手写莫迪文字到天城文的直接音译。MoScNet在光学字符识别（OCR）任务上也显示出有竞争力的结果。", "summary": "本文针对大量未音译且状况不佳的中世纪印度莫迪文字文献，提出了一个创新解决方案。研究构建了MoDeTrans数据集，包含2043张莫迪文字图片及其天城文音译。在此基础上，引入了MoScNet视觉语言模型框架，利用知识蒸馏技术直接将手写莫迪文字音译为天城文。MoScNet的学生模型不仅性能优于教师模型，而且参数量大幅减少，同时在OCR任务上表现出色，是首次实现手写莫迪文字到天城文的直接音译。", "keywords": "莫迪文字, 天城文, 音译, 视觉语言模型, 知识蒸馏", "comments": "该论文的创新点在于构建了首个大规模莫迪文字音译数据集MoDeTrans，并提出了一个高效的VLM框架MoScNet，首次实现了手写莫迪文字到天城文的直接端到端音译。通过知识蒸馏技术，MoScNet在保持甚至超越性能的同时大幅减少了模型参数，这对于实际部署具有重要意义。该工作对于保护和利用历史文献具有重要价值。"}}
{"id": "2503.13063", "pdf": "https://arxiv.org/pdf/2503.13063", "abs": "https://arxiv.org/abs/2503.13063", "authors": ["Zheng Wang", "Zihui Wang", "Zheng Wang", "Xiaoliang Fan", "Cheng Wang"], "title": "Federated Learning with Domain Shift Eraser", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Federated learning (FL) is emerging as a promising technique for\ncollaborative learning without local data leaving their devices. However,\nclients' data originating from diverse domains may degrade model performance\ndue to domain shifts, preventing the model from learning consistent\nrepresentation space. In this paper, we propose a novel FL framework, Federated\nDomain Shift Eraser (FDSE), to improve model performance by differently erasing\neach client's domain skew and enhancing their consensus. First, we formulate\nthe model forward passing as an iterative deskewing process that extracts and\nthen deskews features alternatively. This is efficiently achieved by\ndecomposing each original layer in the neural network into a Domain-agnostic\nFeature Extractor (DFE) and a Domain-specific Skew Eraser (DSE). Then, a\nregularization term is applied to promise the effectiveness of feature\ndeskewing by pulling local statistics of DSE's outputs close to the globally\nconsistent ones. Finally, DFE modules are fairly aggregated and broadcast to\nall the clients to maximize their consensus, and DSE modules are personalized\nfor each client via similarity-aware aggregation to erase their domain skew\ndifferently. Comprehensive experiments were conducted on three datasets to\nconfirm the advantages of our method in terms of accuracy, efficiency, and\ngeneralizability.", "AI": {"title_translation": "联邦学习与域偏移消除器", "tldr": "提出了一种名为联邦域偏移消除器（FDSE）的新型联邦学习框架，通过分解网络层为域无关特征提取器和域特定偏移消除器，并结合正则化和个性化聚合，有效解决了联邦学习中的域偏移问题，提高了模型性能。", "motivation": "联邦学习（FL）在不移动本地数据的情况下进行协作学习，但客户端数据来自不同域可能导致域偏移，从而降低模型性能并阻碍学习一致的表示空间。", "method": "提出了一种名为联邦域偏移消除器（FDSE）的FL框架。该方法将模型前向传播公式化为迭代去偏移过程，交替提取并去偏移特征。具体通过将神经网络的每一层分解为域无关特征提取器（DFE）和域特定偏移消除器（DSE）来实现。通过应用正则化项，使DSE输出的局部统计数据接近全局一致数据，以确保特征去偏移的有效性。DFE模块进行公平聚合并广播给所有客户端以最大化共识，而DSE模块则通过相似性感知聚合为每个客户端个性化，以不同方式消除其域偏移。", "result": "在三个数据集上进行了综合实验，证实了该方法在准确性、效率和泛化性方面的优势。", "conclusion": "该研究提出的联邦域偏移消除器（FDSE）框架，通过有效处理联邦学习中的域偏移问题，显著提升了模型的性能、效率和泛化能力。", "translation": "联邦学习（FL）正在成为一种有前景的协作学习技术，无需将本地数据移出其设备。然而，来自不同域的客户端数据可能会因域偏移而降低模型性能，阻碍模型学习一致的表示空间。在本文中，我们提出了一种新颖的FL框架，联邦域偏移消除器（FDSE），通过不同地消除每个客户端的域倾斜并增强它们的共识来提高模型性能。首先，我们将模型前向传播公式化为一个迭代去偏移过程，该过程交替提取然后去偏移特征。这通过将神经网络中的每个原始层分解为域无关特征提取器（DFE）和域特定偏移消除器（DSE）来有效地实现。然后，应用一个正则化项，通过将DSE输出的局部统计数据拉近到全局一致数据来确保特征去偏移的有效性。最后，DFE模块被公平聚合并广播给所有客户端以最大化它们的共识，而DSE模块则通过相似性感知聚合为每个客户端个性化，以不同地消除它们的域偏移。在三个数据集上进行了综合实验，以确认我们方法在准确性、效率和泛化性方面的优势。", "summary": "该论文介绍了一种名为联邦域偏移消除器（FDSE）的新型联邦学习框架，旨在解决由于客户端数据域偏移导致的模型性能下降问题。FDSE通过将神经网络层分解为域无关特征提取器（DFE）和域特定偏移消除器（DSE），并结合正则化和个性化聚合策略，有效地去除了各客户端的域偏移，同时增强了模型共识。实验结果表明，该方法在准确性、效率和泛化能力方面均优于现有方法。", "keywords": "联邦学习, 域偏移, 特征去偏移, 联邦域偏移消除器, 个性化聚合", "comments": "该论文提出了一种创新的联邦学习框架，通过将模型分解为域无关和域特定部分，并结合个性化处理和全局聚合，有效解决了联邦学习中常见的域偏移问题。其分解网络层为DFE和DSE的思路非常新颖，并辅以正则化和相似性感知聚合，体现了对联邦学习中异质性问题的深刻理解和有效应对。该方法在提高模型性能、效率和泛化性方面具有重要意义。"}}
{"id": "2503.13068", "pdf": "https://arxiv.org/pdf/2503.13068", "abs": "https://arxiv.org/abs/2503.13068", "authors": ["Henghui Du", "Guangyao Li", "Chang Zhou", "Chunjie Zhang", "Alan Zhao", "Di Hu"], "title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, numerous tasks have been proposed to encourage model to\ndevelop specified capability in understanding audio-visual scene, primarily\ncategorized into temporal localization, spatial localization, spatio-temporal\nreasoning, and pixel-level understanding. Instead, human possesses a unified\nunderstanding ability for diversified tasks. Therefore, designing an\naudio-visual model with general capability to unify these tasks is of great\nvalue. However, simply joint training for all tasks can lead to interference\ndue to the heterogeneity of audiovisual data and complex relationship among\ntasks. We argue that this problem can be solved through explicit cooperation\namong tasks. To achieve this goal, we propose a unified learning method which\nachieves explicit inter-task cooperation from both the perspectives of data and\nmodel thoroughly. Specifically, considering the labels of existing datasets are\nsimple words, we carefully refine these datasets and construct an Audio-Visual\nUnified Instruction-tuning dataset with Explicit reasoning process (AV-UIE),\nwhich clarifies the cooperative relationship among tasks. Subsequently, to\nfacilitate concrete cooperation in learning stage, an interaction-aware LoRA\nstructure with multiple LoRA heads is designed to learn different aspects of\naudiovisual data interaction. By unifying the explicit cooperation across the\ndata and model aspect, our method not only surpasses existing unified\naudio-visual model on multiple tasks, but also outperforms most specialized\nmodels for certain tasks. Furthermore, we also visualize the process of\nexplicit cooperation and surprisingly find that each LoRA head has certain\naudio-visual understanding ability. Code and dataset:\nhttps://github.com/GeWu-Lab/Crab", "AI": {"title_translation": "Crab：一个具有显式协作的统一视听场景理解模型", "tldr": "Crab是一个统一的视听模型，通过显式任务协作和新的指令微调数据集（AV-UIE）以及交互感知LoRA结构，在多种视听理解任务上表现优异，甚至超越了大多数专用模型。", "motivation": "近年来，许多视听场景理解任务被提出，但人类具有统一的理解能力。现有模型多为任务特定，简单联合训练会导致数据异质性和任务复杂关系带来的干扰。因此，动机是设计一个具有通用能力的视听模型来统一这些任务，并通过显式协作解决干扰问题。", "method": "提出了一种统一学习方法，从数据和模型两方面实现任务间显式协作。具体来说，构建了带有显式推理过程的视听统一指令微调数据集（AV-UIE），以阐明任务间的协作关系。随后，设计了一种具有多个LoRA头的交互感知LoRA结构，以促进学习阶段的具体协作，并学习视听数据交互的不同方面。", "result": "该方法不仅在多个任务上超越了现有统一视听模型，而且在某些任务上优于大多数专用模型。此外，可视化结果表明每个LoRA头都具有一定的视听理解能力。", "conclusion": "通过统一数据和模型方面的显式协作，所提出的Crab模型在统一视听场景理解方面取得了卓越性能，证明了显式任务协作的有效性。", "translation": "近年来，人们提出了许多任务来鼓励模型发展视听场景理解的特定能力，主要分为时间定位、空间定位、时空推理和像素级理解。然而，人类对多样化任务具有统一的理解能力。因此，设计一个具有通用能力的视听模型来统一这些任务具有重要价值。然而，由于视听数据的异质性以及任务之间复杂的关联，简单地对所有任务进行联合训练会导致干扰。我们认为这个问题可以通过任务之间的显式协作来解决。为了实现这一目标，我们提出了一种统一的学习方法，该方法从数据和模型两个角度彻底实现了任务间的显式协作。具体而言，考虑到现有数据集的标签是简单的词语，我们仔细地改进了这些数据集，并构建了一个带有显式推理过程的视听统一指令微调数据集（AV-UIE），该数据集阐明了任务之间的协作关系。随后，为了促进学习阶段的具体协作，设计了一种具有多个LoRA头的交互感知LoRA结构，以学习视听数据交互的不同方面。通过统一数据和模型方面的显式协作，我们的方法不仅在多个任务上超越了现有的统一视听模型，而且在某些任务上优于大多数专用模型。此外，我们还可视化了显式协作的过程，并惊奇地发现每个LoRA头都具有一定的视听理解能力。代码和数据集：https://github.com/GeWu-Lab/Crab", "summary": "本文提出了Crab，一个统一的视听场景理解模型，旨在通过显式协作解决现有模型在多任务联合训练中的干扰问题。该模型从数据和模型两方面实现协作：构建了新的AV-UIE指令微调数据集以明确任务间的合作关系，并设计了带有多个LoRA头的交互感知LoRA结构来促进学习阶段的协作。实验结果表明，Crab在多个视听理解任务上超越了现有统一模型，并在某些任务上甚至优于专用模型，证明了显式协作的有效性。", "keywords": "统一视听理解, 显式协作, 指令微调, LoRA, 多模态学习", "comments": "本文通过显式处理任务间的协作，为统一的视听场景理解引入了一种创新方法。将数据集提炼为指令微调格式（AV-UIE）以阐明协作关系的想法新颖且有价值。带有多个头的交互感知LoRA结构也是捕捉不同交互的巧妙方式。其在超越统一模型和专用模型方面的能力突出了其重要性。这项工作可能为多模态理解领域中更通用的AI模型铺平道路。"}}
{"id": "2503.13070", "pdf": "https://arxiv.org/pdf/2503.13070", "abs": "https://arxiv.org/abs/2503.13070", "authors": ["Yihong Luo", "Tianyang Hu", "Weijian Luo", "Kenji Kawaguchi", "Jing Tang"], "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.", "AI": {"title_translation": "奖励足以实现快速逼真的文本到图像生成", "tldr": "本文提出R0，一种通过正则化奖励最大化实现条件生成的新方法，证明在复杂条件下，奖励在文本到图像生成中起主导作用，而扩散损失则是一种昂贵的正则化形式。", "motivation": "在AIGC中，将生成图像与复杂文本提示和人类偏好对齐是一个核心挑战。现有的奖励增强型扩散蒸馏方法虽然能提升可控性和逼真度，但本文发现当条件更具体、奖励信号更强时，奖励本身成为生成中的主导力量，而扩散损失则成为一种过于昂贵的正则化形式，这挑战了传统的扩散后训练和条件生成范式。", "method": "本文提出了R0，一种新颖的通过正则化奖励最大化实现的条件生成方法。R0将图像生成视为数据空间中的优化问题，旨在寻找具有高组合奖励的有效图像，而不是依赖复杂的扩散蒸馏损失。通过创新性的生成器参数化设计和适当的正则化技术，R0能够训练出最先进的少步文本到图像生成模型。", "result": "R0成功训练出了最先进的少步文本到图像生成模型。实验结果表明，在复杂条件下，奖励在文本到图像生成中起主导作用，挑战了扩散后训练和条件生成的传统观念。", "conclusion": "本文的发现表明，在复杂条件下，奖励在文本到图像生成中起主导作用，而非扩散损失。这为AIGC领域中以人为中心和以奖励为中心的生成范式提供了新的研究方向。", "translation": "将生成的图像与复杂的文本提示和人类偏好对齐是人工智能生成内容（AIGC）中的一个核心挑战。随着奖励增强型扩散蒸馏作为一种有前景的方法出现，它提升了文本到图像模型的可控性和逼真度，我们发现了一个根本性的范式转变：当条件变得更具体且奖励信号更强时，奖励本身成为生成中的主导力量。相比之下，扩散损失则成为一种过于昂贵的正则化形式。为了彻底验证我们的假设，我们引入了R0，一种通过正则化奖励最大化实现的新颖条件生成方法。R0不依赖于棘手的扩散蒸馏损失，而是提出了一种新视角，将图像生成视为数据空间中的优化问题，旨在寻找具有高组合奖励的有效图像。通过生成器参数化和适当正则化技术的创新设计，我们用R0大规模训练出了最先进的少步文本到图像模型。我们的结果挑战了扩散后训练和条件生成的传统观念，证明了奖励在复杂条件场景中起主导作用。我们希望我们的发现能促进AIGC更广泛领域中以人为中心和以奖励为中心的生成范式的进一步研究。代码可在 https://github.com/Luo-Yihong/R0 获取。", "summary": "本文提出了一种名为R0的新型条件生成方法，通过正则化奖励最大化实现快速逼真的文本到图像生成。R0将图像生成视为数据空间中的优化问题，旨在直接最大化组合奖励，而非依赖复杂的扩散损失。研究发现，在复杂条件下，奖励信号而非扩散损失，是图像生成的主导力量。R0通过创新的生成器设计和正则化技术，成功训练出高性能的少步文本到图像模型，挑战了传统扩散模型的范式，并为未来的奖励驱动生成范式提供了新方向。", "keywords": "文本到图像生成, 奖励最大化, 扩散模型, 条件生成, AIGC", "comments": "本文提出R0，通过将图像生成重新定义为奖励最大化问题，而非依赖扩散损失，为文本到图像生成领域带来了范式转变。其创新点在于强调了在复杂条件下奖励信号的主导作用，并提供了一种更高效的训练方法。这对于提升AIGC的可控性和效率具有重要意义，可能启发未来更多以奖励为中心的生成研究。"}}
{"id": "2503.13073", "pdf": "https://arxiv.org/pdf/2503.13073", "abs": "https://arxiv.org/abs/2503.13073", "authors": ["Zhicheng Zhao", "Jinquan Yan", "Chenglong Li", "Xiao Wang", "Jin Tang"], "title": "DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Optical remote sensing image dehazing presents significant challenges due to\nits extensive spatial scale and highly non-uniform haze distribution, which\ntraditional single-image dehazing methods struggle to address effectively.\nWhile Synthetic Aperture Radar (SAR) imagery offers inherently haze-free\nreference information for large-scale scenes, existing SAR-guided dehazing\napproaches face two critical limitations: the integration of SAR information\noften diminishes the quality of haze-free regions, and the instability of\nfeature quality further exacerbates cross-modal domain shift. To overcome these\nchallenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built\non a progressive haze decoupling fusion strategy. Our approach incorporates two\nkey innovations: a Haze Perception and Decoupling Module (HPDM) that\ndynamically identifies haze-affected regions through optical-SAR difference\nanalysis, and a Progressive Fusion Module (PFM) that mitigates domain shift\nthrough a two-stage fusion process based on feature quality assessment. To\nfacilitate research in this domain, we present MRSHaze, a large-scale benchmark\ndataset comprising 8,000 pairs of temporally synchronized, precisely\ngeo-registered SAR-optical images with high resolution and diverse haze\nconditions. Extensive experiments demonstrate that DehazeMamba significantly\noutperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR\nand substantial enhancements in downstream tasks such as semantic segmentation.\nThe dataset is available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code.", "AI": {"title_translation": "DehazeMamba：SAR引导的自适应状态空间模型光学遥感图像去 Haze", "tldr": "DehazeMamba是一种新的SAR引导去Haze网络，通过渐进式Haze解耦融合策略克服了现有方法的局限性，并在MRSHaze数据集上表现优异，提升了PSNR和下游任务性能。", "motivation": "传统单幅图像去Haze方法难以有效处理光学遥感图像的大空间尺度和非均匀Haze分布。现有SAR引导去Haze方法存在SAR信息融合降低无Haze区域质量以及特征质量不稳定导致跨模态域偏移的问题。", "method": "本文提出了DehazeMamba，一个基于渐进式Haze解耦融合策略的SAR引导去Haze网络。该方法包含两个关键创新：一个Haze感知与解耦模块（HPDM），通过光学-SAR差异分析动态识别Haze影响区域；一个渐进式融合模块（PFM），通过基于特征质量评估的两阶段融合过程减轻域偏移。为促进研究，还提出了MRSHaze，一个包含8000对时序同步、精确地理配准的高分辨率和多样Haze条件SAR-光学图像的大规模基准数据集。", "result": "DehazeMamba显著优于现有最先进方法，PSNR提高了0.73 dB，并大幅提升了语义分割等下游任务的性能。", "conclusion": "DehazeMamba通过创新的HPDM和PFM模块，有效解决了光学遥感图像去Haze中的Haze分布不均和跨模态域偏移问题，并在大规模数据集上取得了显著的性能提升。", "translation": "光学遥感图像去Haze由于其广泛的空间尺度和高度非均匀的Haze分布，给传统单幅图像去Haze方法带来了巨大挑战。虽然合成孔径雷达（SAR）图像为大尺度场景提供了固有的无Haze参考信息，但现有的SAR引导去Haze方法面临两个关键限制：SAR信息的整合通常会降低无Haze区域的质量，以及特征质量的不稳定性进一步加剧了跨模态域偏移。为了克服这些挑战，我们引入了DehazeMamba，一个基于渐进式Haze解耦融合策略的新型SAR引导去Haze网络。我们的方法包含两个关键创新：一个Haze感知与解耦模块（HPDM），通过光学-SAR差异分析动态识别Haze影响区域；一个渐进式融合模块（PFM），通过基于特征质量评估的两阶段融合过程减轻域偏移。为了促进该领域的研究，我们提出了MRSHaze，一个大规模基准数据集，包含8000对时间同步、精确地理配准的高分辨率和多样Haze条件的SAR-光学图像。大量实验表明，DehazeMamba显著优于现有最先进方法，PSNR提高了0.73 dB，并大幅提升了语义分割等下游任务（如语义分割）的性能。数据集可在https://github.com/mmic-lcl/Datasets-and-benchmark-code 获取。", "summary": "DehazeMamba是一个新型的SAR引导去Haze网络，旨在解决光学遥感图像去Haze中Haze分布不均和跨模态域偏移的挑战。它引入了Haze感知与解耦模块（HPDM）和渐进式融合模块（PFM），分别用于动态识别Haze区域和减轻域偏移。该研究还构建了大规模的MRSHaze数据集。实验证明，DehazeMamba在去Haze性能上显著超越现有方法，并在下游任务中表现出色。", "keywords": "SAR引导去Haze, 遥感图像去Haze, DehazeMamba, 跨模态融合, MRSHaze数据集", "comments": "该论文的创新点在于提出了DehazeMamba网络，通过HPDM和PFM模块有效利用SAR信息进行光学遥感图像去Haze，解决了现有方法中SAR信息集成和跨模态域偏移的痛点。同时，构建并公开了大规模的MRSHaze数据集，这对于推动该领域的研究具有重要意义。性能提升和在下游任务中的表现也验证了其方法的有效性。"}}
{"id": "2503.13074", "pdf": "https://arxiv.org/pdf/2503.13074", "abs": "https://arxiv.org/abs/2503.13074", "authors": ["Shaolin Su", "Josep M. Rocafort", "Danna Xue", "David Serrano-Lozano", "Lei Sun", "Javier Vazquez-Corral"], "title": "Rethinking Image Evaluation in Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100\\%\ntrustworthy for model evaluations? How does GT quality affect this evaluation?\nAnd how to make fair evaluations if there exist imperfect GTs? To answer these\nquestions, this paper presents two main contributions. First, by systematically\nanalyzing seven state-of-the-art SR models across three real-world SR datasets,\nwe show that SR performances can be consistently affected across models by\nlow-quality GTs, and models can perform quite differently when GT quality is\ncontrolled. Second, we propose a novel perceptual quality metric, Relative\nQuality Index (RQI), that measures the relative quality discrepancy of image\npairs, thus issuing the biased evaluations caused by unreliable GTs. Our\nproposed model achieves significantly better consistency with human opinions.\nWe expect our work to provide insights for the SR community on how future\ndatasets, models, and metrics should be developed.", "AI": {"title_translation": "重新思考超分辨率中的图像评估", "tldr": "本文指出现有超分辨率（SR）数据集中用于评估的地面真实（GT）图像质量可能不佳，导致评估偏差。作者通过分析展示了低质量GT对模型性能的影响，并提出了一个新的感知质量度量RQI，以解决不可靠GT导致的评估偏差，该度量与人类意见具有更好的一致性。", "motivation": "当前的图像超分辨率（SR）技术在感知质量上不断进步，但在定量评估中却常常失败，这种不一致性导致人们对现有图像评估指标的信任度降低。研究人员通常忽略了地面真实（GT）图像的作用，认为它们是“完美”的参考。然而，由于数据收集于早期以及对其他类型失真控制的忽视，现有SR数据集中的GT图像可能质量相对较差，从而导致有偏见的评估。本文旨在探讨现有SR数据集中GT图像的可靠性、GT质量如何影响评估以及在存在不完美GT的情况下如何进行公平评估。", "method": "本文主要通过两种方法回答上述问题：\n1. 系统分析：对七个最先进的SR模型在三个真实世界SR数据集上进行了系统分析，以考察低质量GT对SR性能的影响。\n2. 提出新度量：提出了一种新颖的感知质量度量——相对质量指数（Relative Quality Index, RQI），该度量用于衡量图像对之间的相对质量差异，从而解决由不可靠GT引起的有偏评估。", "result": "1. 低质量GT对SR模型性能的影响：研究表明，低质量的地面真实（GT）图像会持续影响不同SR模型的性能，并且在控制GT质量后，模型的表现会显著不同。\n2. RQI的有效性：所提出的相对质量指数（RQI）度量与人类意见表现出显著更好的一致性，能够有效解决由不可靠GT导致的评估偏差。", "conclusion": "本文通过系统分析揭示了现有超分辨率数据集中地面真实（GT）图像质量可能不佳，并会严重影响模型评估的准确性。为此，本文提出了一种新的感知质量度量RQI，能够有效处理不可靠GT带来的评估偏差，并显示出与人类意见更高的一致性。本研究有望为SR社区在未来数据集、模型和度量开发方面提供新的见解。", "translation": "虽然最近的图像超分辨率（SR）技术不断提高其输出的感知质量，但它们通常在定量评估中失败。这种不一致性导致人们对现有SR评估图像指标的信任度日益降低。尽管图像评估取决于度量和参考地面真实（GT），但研究人员通常不检查GT的作用，因为它们通常被认为是“完美”的参考。然而，由于数据收集于早期以及对其他类型失真控制的忽视，我们指出现有SR数据集中的GT可能表现出相对较差的质量，从而导致有偏见的评估。根据这一观察，本文关注以下问题：现有SR数据集中的GT图像对于模型评估来说是否100%值得信赖？GT质量如何影响这种评估？以及如果存在不完美的GT，如何进行公平评估？为了回答这些问题，本文提出了两个主要贡献。首先，通过系统分析七个最先进的SR模型在三个真实世界SR数据集上的表现，我们表明SR性能可以持续受到低质量GT的影响，并且当GT质量受到控制时，模型的表现可能大不相同。其次，我们提出了一种新颖的感知质量度量——相对质量指数（RQI），它衡量图像对之间的相对质量差异，从而解决了由不可靠GT引起的有偏评估。我们提出的模型与人类意见实现了显著更好的一致性。我们期望我们的工作能为SR社区提供关于未来数据集、模型和度量如何开发的见解。", "summary": "本研究重新审视了超分辨率（SR）中的图像评估问题，指出现有SR数据集中地面真实（GT）图像的质量可能不佳，导致模型评估产生偏差。论文通过对七个SOTA SR模型在三个数据集上的系统分析，证明了低质量GT对模型性能的持续影响，并发现控制GT质量后模型表现差异显著。为解决此问题，本文提出了一种新型感知质量度量——相对质量指数（RQI），该度量通过测量图像对的相对质量差异来纠正由不可靠GT引起的评估偏差，并显示出与人类意见更好的一致性。本工作旨在为SR社区未来数据集、模型和度量的开发提供指导。", "keywords": "超分辨率, 图像评估, 地面真实, 感知质量, 相对质量指数", "comments": "这篇论文的创新点在于它挑战了超分辨率领域中长期以来被普遍接受的“地面真实（GT）图像是完美参考”的假设。它首次系统地指出并验证了现有SR数据集中GT图像质量可能存在问题，并深入探讨了这种问题如何导致评估偏差。提出的相对质量指数（RQI）是一种新颖且实用的方法，它不再依赖于绝对的完美参考，而是通过测量相对质量差异来提供更可靠的评估，这对于解决现实世界中GT质量不佳的情况具有重要意义。这项工作对于推动SR社区在数据集构建、模型训练和评估标准方面的未来发展具有重要的指导价值。"}}
{"id": "2503.13086", "pdf": "https://arxiv.org/pdf/2503.13086", "abs": "https://arxiv.org/abs/2503.13086", "authors": ["Yiwei Xu", "Yifei Yu", "Wentian Gan", "Tengfei Wang", "Zongqian Zhan", "Hao Cheng", "Xin Wang"], "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction.", "AI": {"title_translation": "高斯即时飞溅：一种鲁棒的近实时3DGS优化渐进框架", "tldr": "该论文提出了一种名为“即时飞溅高斯”（On-the-Fly GS）的渐进框架，可在图像捕获期间实现近实时3DGS优化，显著减少训练时间并保持渲染质量。", "motivation": "现有3D高斯飞溅（3DGS）方法依赖于完整的运动结构（SfM）处理后的离线训练，无法在图像捕获期间进行实时优化。", "method": "本工作引入了“即时飞溅GS”框架，在图像捕获期间进行近实时3DGS优化。每当新图像到达时，通过即时SfM更新其姿态和稀疏点，并立即将新优化的Gaussians集成到3DGS场中。它采用渐进式局部优化策略，根据重叠关系优先处理新图像及其邻居，使新图像及其重叠图像获得更多训练。为稳定新旧图像的训练，采用自适应学习率调度平衡迭代次数和学习率。此外，采用高效的全局优化方案以保持3DGS场的整体质量，防止对新添加图像的过拟合。", "result": "实验表明，On-the-Fly GS显著减少了训练时间，在数秒内优化每个新图像，且渲染损失极小。", "conclusion": "On-the-Fly GS是迈向快速、渐进式3DGS重建的第一个实用步骤。", "translation": "3D高斯飞溅（3DGS）实现了高保真渲染和快速实时性能，但现有方法依赖于完整的运动结构（SfM）处理后的离线训练。相比之下，这项工作引入了即时飞溅GS，这是一个渐进框架，可在图像捕获期间实现近实时3DGS优化。随着每个图像的到来，其姿态和稀疏点通过即时SfM进行更新，并且新优化的高斯立即集成到3DGS场中。我们提出了一种渐进式局部优化策略，根据其相应的重叠关系优先处理新图像及其邻居，允许新图像及其重叠图像获得更多的训练。为了进一步稳定新旧图像之间的训练，自适应学习率调度平衡了迭代次数和学习率。此外，为了保持3DGS场的整体质量，高效的全局优化方案防止了对新添加图像的过拟合。在多个基准数据集上的实验表明，我们的即时飞溅GS显著减少了训练时间，在数秒内优化每个新图像，且渲染损失极小，为快速、渐进式3DGS重建迈出了第一个实用步骤。", "summary": "本论文提出了一种名为“即时飞溅高斯”（On-the-Fly GS）的渐进式框架，旨在解决现有3DGS方法依赖离线训练的问题。该框架通过在图像捕获期间即时更新姿态和稀疏点，并集成新优化的Gaussians，实现了近实时的3DGS优化。为了提高训练效率和稳定性，引入了渐进式局部优化策略、自适应学习率调度和高效的全局优化方案。实验证明，On-the-Fly GS显著缩短了训练时间，并在保持渲染质量的同时实现了每秒优化一张新图像，为快速、渐进式3DGS重建提供了可行方案。", "keywords": "3DGS, 实时优化, 渐进式框架, SfM, 高斯飞溅", "comments": "这项工作在3DGS领域具有重要创新性，首次实现了在图像捕获期间进行近实时优化，极大地提升了3DGS的实用性和效率。其提出的渐进式局部优化、自适应学习率和全局优化方案是解决在线训练稳定性和质量问题的关键。这为3DGS技术在更广泛的实时应用场景中铺平了道路。"}}
{"id": "2503.13107", "pdf": "https://arxiv.org/pdf/2503.13107", "abs": "https://arxiv.org/abs/2503.13107", "authors": ["Hao Yin", "Guangzong Si", "Zilei Wang"], "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.", "AI": {"title_translation": "ClearSight：多模态大语言模型中视觉信号增强以缓解物体幻觉", "tldr": "ClearSight提出视觉放大融合（VAF），一种即插即用技术，通过增强模型中间层对视觉信号的关注，有效缓解多模态大语言模型中的物体幻觉，同时不影响推理速度和生成内容的连贯性及准确性。", "motivation": "现有的对比解码策略在缓解多模态大语言模型（MLLMs）中的物体幻觉时，存在两个主要限制：1）过度抑制语言先验会损害生成内容的连贯性和准确性；2）处理对比输入会增加计算负担，显著降低推理速度。", "method": "本文提出了视觉放大融合（Visual Amplification Fusion, VAF）技术，这是一种即插即用的技术。它通过在模型中间层（模态融合主要发生的地方）增强对视觉信号的注意力，从而更有效地捕获视觉特征，减少模型对语言模态的偏向。", "result": "实验结果表明，VAF在各种多模态大语言模型（MLLMs）中显著减少了幻觉，同时不影响推理速度，并保持了生成输出的连贯性和准确性。", "conclusion": "ClearSight提出的VAF技术有效解决了现有对比解码策略在缓解多模态大语言模型物体幻觉方面的局限性，实现了在不牺牲性能和效率的前提下，提升视觉信号利用率，从而减少幻觉。", "translation": "对比解码策略被广泛用于缓解多模态大语言模型（MLLMs）中的物体幻觉。通过减少对语言先验的过度依赖，这些策略确保生成内容与视觉输入紧密结合，产生上下文准确的输出。由于对比解码不需要额外的训练或外部工具，它提供了计算效率和多功能性，使其极具吸引力。然而，这些方法存在两个主要限制：（1）简单地抑制语言先验可能会损害生成内容的连贯性和准确性；（2）处理对比输入会增加计算负载，显著减慢推理速度。为了应对这些挑战，我们提出了视觉放大融合（VAF），这是一种即插即用技术，可以在模型中间层（模态融合主要发生的地方）增强对视觉信号的注意力。这种方法能够更有效地捕获视觉特征，减少模型对语言模态的偏向。实验结果表明，VAF在各种多模态大语言模型中显著减少了幻觉，同时不影响推理速度，并保持了生成输出的连贯性和准确性。", "summary": "本文提出ClearSight框架下的视觉放大融合（VAF）技术，旨在解决多模态大语言模型（MLLMs）中物体幻觉的缓解问题。针对现有对比解码策略在抑制语言先验时可能损害内容连贯性、准确性以及增加推理速度的局限性，VAF作为一种即插即用方案，通过在模型中间层增强对视觉信号的关注，有效捕获视觉特征，减少模型对语言模态的偏向。实验证明，VAF在不影响推理速度的前提下，显著减少了各种MLLMs的幻觉现象，并保持了生成内容的连贯性和准确性。", "keywords": "多模态大语言模型, 物体幻觉, 视觉信号增强, 对比解码, 即插即用", "comments": "本文提出的VAF技术具有创新性，它从中间层模态融合的角度解决多模态大语言模型的幻觉问题，避免了传统对比解码策略的弊端。其即插即用的特性也增加了实用性。通过在不牺牲推理速度和生成质量的前提下有效缓解幻觉，该方法对提升MLLMs的可靠性具有重要意义。"}}
{"id": "2503.13108", "pdf": "https://arxiv.org/pdf/2503.13108", "abs": "https://arxiv.org/abs/2503.13108", "authors": ["Hao Yin", "Guangzong Si", "Zilei Wang"], "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.", "AI": {"title_translation": "揭示多模态大型语言模型中视觉信息流的奥秘：解锁更快的推理路径", "tldr": "本文揭示了多模态大型语言模型（MLLM）中视觉信息流的层次性：浅层融合视觉信息到文本，深层优化视觉信息。基于此，提出了分层模态感知剪枝（HiMAP）方法，可在不牺牲性能的情况下，将MLLM推理的计算成本降低约65%。", "motivation": "多模态大型语言模型（MLLM）在视觉-语言任务中表现出色，但其处理和利用视觉信息的方式仍不明确。本研究旨在揭示这一机制，并利用其理解来加速推理过程。", "method": "作者研究了MLLM中视觉信息流的模式，发现浅层中图像tokens与指令tokens强交互，将视觉信息注入指令tokens形成跨模态语义表示；深层中图像tokens主要相互作用，聚合剩余视觉信息以优化视觉模态内的语义表示。基于这些发现，提出了分层模态感知剪枝（HiMAP），这是一种即插即用的推理加速方法，通过在特定层动态剪枝图像tokens来降低计算成本。", "result": "所提出的HiMAP方法能够在不牺牲性能的情况下，将计算成本降低约65%。", "conclusion": "本研究的发现为MLLM中的视觉信息处理提供了新的理解，并为高效推理提供了一种最先进的解决方案。", "translation": "多模态大型语言模型（MLLM）通过将预训练视觉编码器中的视觉特征集成到大型语言模型（LLM）中，从而提高视觉-语言任务的性能。然而，MLLM如何处理和利用视觉信息仍不清楚。本文揭示了视觉信息流的主导性转变：(1) 在浅层，观察到图像tokens和指令tokens之间存在强烈的交互，其中大部分视觉信息被注入到指令tokens中以形成跨模态语义表示；(2) 在深层，图像tokens主要相互作用，聚合剩余的视觉信息以优化视觉模态内的语义表示。基于这些见解，我们提出了分层模态感知剪枝（HiMAP），这是一种即插即用的推理加速方法，它可以在特定层动态剪枝图像tokens，从而在不牺牲性能的情况下将计算成本降低约65%。我们的发现为MLLM中的视觉信息处理提供了新的理解，并为高效推理提供了一种最先进的解决方案。", "summary": "本文探讨了多模态大型语言模型（MLLM）中视觉信息的处理方式，揭示了其分层流动的特性：浅层侧重于将视觉特征融入文本指令以形成跨模态表示，而深层则专注于优化视觉模态内部的语义信息。基于这些发现，论文提出了一种名为分层模态感知剪枝（HiMAP）的即插即用推理加速方法，该方法通过动态剪枝特定层的图像tokens，在不影响性能的前提下，将计算成本降低了约65%。", "keywords": "多模态大型语言模型, 视觉信息流, 推理加速, 剪枝, HiMAP", "comments": "该论文通过揭示MLLM内部视觉信息处理的机制，做出了重要贡献。所提出的HiMAP方法具有创新性，因为它利用了对信息流的理解来实现显著的推理加速，使MLLM更加实用和高效。其“即插即用”的特性表明其具有广泛的适用性。"}}
{"id": "2503.13110", "pdf": "https://arxiv.org/pdf/2503.13110", "abs": "https://arxiv.org/abs/2503.13110", "authors": ["Jing Li", "Yihang Fu", "Falai Chen"], "title": "DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry", "categories": ["cs.CV"], "comment": null, "summary": "Boundary representation (B-rep) of geometric models is a fundamental format\nin Computer-Aided Design (CAD). However, automatically generating valid and\nhigh-quality B-rep models remains challenging due to the complex\ninterdependence between the topology and geometry of the models. Existing\nmethods tend to prioritize geometric representation while giving insufficient\nattention to topological constraints, making it difficult to maintain\nstructural validity and geometric accuracy. In this paper, we propose\nDTGBrepGen, a novel topology-geometry decoupled framework for B-rep generation\nthat explicitly addresses both aspects. Our approach first generates valid\ntopological structures through a two-stage process that independently models\nedge-face and edge-vertex adjacency relationships. Subsequently, we employ\nTransformer-based diffusion models for sequential geometry generation,\nprogressively generating vertex coordinates, followed by edge geometries and\nface geometries which are represented as B-splines. Extensive experiments on\ndiverse CAD datasets show that DTGBrepGen significantly outperforms existing\nmethods in both topological validity and geometric accuracy, achieving higher\nvalidity rates and producing more diverse and realistic B-reps. Our code is\npublicly available at https://github.com/jinli99/DTGBrepGen.", "AI": {"title_translation": "DTGBrepGen：一种通过解耦拓扑和几何的新型B-rep生成模型", "tldr": "DTGBrepGen提出了一种新颖的B-rep生成方法，通过解耦拓扑和几何，显著提高了CAD模型生成的有效性和准确性。", "motivation": "计算机辅助设计(CAD)中，B-rep几何模型的自动生成面临挑战，因为拓扑和几何之间存在复杂的相互依赖性。现有方法侧重几何表示，忽视拓扑约束，难以维持结构有效性和几何精度。", "method": "提出DTGBrepGen框架，解耦拓扑和几何。首先，通过两阶段过程独立建模边-面和边-顶点邻接关系，生成有效的拓扑结构。随后，使用基于Transformer的扩散模型进行序列几何生成，逐步生成顶点坐标、边几何和面几何（表示为B样条）。", "result": "在多样化的CAD数据集上，DTGBrepGen在拓扑有效性和几何精度方面显著优于现有方法，实现了更高的有效率，并生成了更多样化和逼真的B-rep模型。", "conclusion": "DTGBrepGen通过解耦拓扑和几何，有效解决了B-rep模型生成中的挑战，在拓扑有效性和几何精度上均取得了显著提升。", "translation": "边界表示（B-rep）几何模型是计算机辅助设计（CAD）中的一种基本格式。然而，由于模型拓扑和几何之间复杂的相互依赖性，自动生成有效且高质量的B-rep模型仍然具有挑战性。现有方法倾向于优先考虑几何表示，而对拓扑约束的关注不足，这使得保持结构有效性和几何精度变得困难。在本文中，我们提出了DTGBrepGen，一种用于B-rep生成的新型拓扑-几何解耦框架，它明确地解决了这两个方面。我们的方法首先通过两阶段过程生成有效的拓扑结构，该过程独立地建模边-面和边-顶点邻接关系。随后，我们采用基于Transformer的扩散模型进行序列几何生成，逐步生成顶点坐标，然后是边几何和面几何（表示为B样条）。在多样化的CAD数据集上进行的广泛实验表明，DTGBrepGen在拓扑有效性和几何精度方面显著优于现有方法，实现了更高的有效率，并生成了更多样化和逼真的B-rep。我们的代码已在https://github.com/jinli99/DTGBrepGen 公开。", "summary": "本文提出了DTGBrepGen，一个用于B-rep模型生成的新型框架，旨在解决拓扑和几何之间复杂相互依赖性导致的生成难题。该方法通过解耦拓扑和几何，首先独立生成有效的拓扑结构，然后利用Transformer-based扩散模型顺序生成几何信息（顶点、边、面）。实验结果表明，DTGBrepGen在拓扑有效性和几何精度上均显著优于现有方法，能生成更有效、多样且逼真的B-rep模型。", "keywords": "B-rep生成, 拓扑-几何解耦, 扩散模型, CAD, 几何建模", "comments": "该论文的创新点在于提出了拓扑和几何解耦的策略来生成B-rep模型，这解决了传统方法中难以同时保证结构有效性和几何精度的问题。通过两阶段的生成过程，特别是利用Transformer-based扩散模型处理几何生成，该方法在CAD模型生成领域取得了显著的性能提升，具有重要的应用潜力。"}}
{"id": "2503.13111", "pdf": "https://arxiv.org/pdf/2503.13111", "abs": "https://arxiv.org/abs/2503.13111", "authors": ["Erik Daxberger", "Nina Wenzel", "David Griffiths", "Haiming Gang", "Justin Lazarow", "Gefen Kohavi", "Kai Kang", "Marcin Eichner", "Yinfei Yang", "Afshin Dehghan", "Peter Grasch"], "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.", "AI": {"title_translation": "MM-Spatial：探索多模态LLM中的三维空间理解", "tldr": "MM-Spatial通过引入新的数据集和基准，显著提升了多模态LLM在三维空间理解方面的能力。", "motivation": "多模态大型语言模型（MLLM）在二维视觉理解方面表现出色，但在推理三维空间的能力上仍然有限。", "method": "本文利用大规模高质量的三维场景数据和开放集标注，引入了一个新的监督微调数据集（CA-VQA）和一个新的评估基准，专注于室内场景。CA-VQA数据涵盖了空间关系预测、度量尺寸和距离估计以及三维定位等任务。在此基础上，训练了MM-Spatial模型，并通过结合度量深度和多视角输入来提升三维理解能力。", "result": "CA-VQA数据集使得MM-Spatial成为一个强大的通用多模态LLM，并在包括其自身在内的三维空间理解基准上取得了最先进的性能。研究还表明，结合度量深度和多视角输入可以进一步提高三维理解能力，并且仅凭数据就能使模型达到与专用单目深度估计模型相当的深度感知能力。", "conclusion": "MM-Spatial通过构建新颖的数据集和方法，显著提升了多模态LLM在三维空间理解方面的能力，并展现了数据驱动的深度感知潜力。", "translation": "多模态大型语言模型（MLLM）在二维视觉理解方面表现出色，但在推理三维空间的能力上仍然有限。在这项工作中，我们利用大规模高质量的三维场景数据和开放集标注，引入了1）一个新的监督微调数据集和2）一个新的评估基准，专注于室内场景。我们的Cubify Anything VQA（CA-VQA）数据涵盖了多样化的空间任务，包括空间关系预测、度量尺寸和距离估计以及三维定位。我们展示了CA-VQA使我们能够训练MM-Spatial，一个强大的通用MLLM，它还在包括我们自己的三维空间理解基准上取得了最先进的性能。我们展示了如何结合度量深度和多视角输入（CA-VQA中提供）可以进一步提高三维理解能力，并证明仅凭数据就能使我们的模型达到与专用单目深度估计模型相当的深度感知能力。我们将发布我们的SFT数据集和基准。", "summary": "本文针对多模态大型语言模型（MLLMs）在三维空间理解方面的局限性，引入了一个名为MM-Spatial的新模型。通过构建大规模高质量的室内三维场景数据集CA-VQA，并将其用于监督微调和评估，MM-Spatial在三维空间理解任务上取得了最先进的性能，包括空间关系、尺寸距离估计和三维定位。研究还表明，结合度量深度和多视角输入能进一步提升3D理解能力，且仅凭数据就能实现与专用模型媲美的深度感知。", "keywords": "多模态LLM, 三维空间理解, 数据集, 深度感知, MM-Spatial", "comments": "本文通过构建专门的三维场景数据集和评估基准，有效弥补了当前多模态LLM在三维空间理解方面的不足。其创新之处在于利用高质量三维数据进行监督微调，并证明了数据本身在提升模型深度感知能力方面的巨大潜力。这对于未来构建更全面、更具空间推理能力的多模态AI具有重要意义。"}}
{"id": "2503.13120", "pdf": "https://arxiv.org/pdf/2503.13120", "abs": "https://arxiv.org/abs/2503.13120", "authors": ["Siyuan Fan", "Wenke Huang", "Xiantao Cai", "Bo Du"], "title": "3D Human Interaction Generation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "3D human interaction generation has emerged as a key research area, focusing\non producing dynamic and contextually relevant interactions between humans and\nvarious interactive entities. Recent rapid advancements in 3D model\nrepresentation methods, motion capture technologies, and generative models have\nlaid a solid foundation for the growing interest in this domain. Existing\nresearch in this field can be broadly categorized into three areas: human-scene\ninteraction, human-object interaction, and human-human interaction. Despite the\nrapid advancements in this area, challenges remain due to the need for\nnaturalness in human motion generation and the accurate interaction between\nhumans and interactive entities. In this survey, we present a comprehensive\nliterature review of human interaction generation, which, to the best of our\nknowledge, is the first of its kind. We begin by introducing the foundational\ntechnologies, including model representations, motion capture methods, and\ngenerative models. Subsequently, we introduce the approaches proposed for the\nthree sub-tasks, along with their corresponding datasets and evaluation\nmetrics. Finally, we discuss potential future research directions in this area\nand conclude the survey. Through this survey, we aim to offer a comprehensive\noverview of the current advancements in the field, highlight key challenges,\nand inspire future research works.", "AI": {"title_translation": "3D人体交互生成：一项综述", "tldr": "一篇关于3D人体交互生成的综述，涵盖现有研究分类和未来方向。", "motivation": "3D人体交互生成是一个关键研究领域，旨在产生动态和上下文相关的交互。尽管有快速进展，但在生成自然人体动作和准确交互方面仍存在挑战，促使对现有研究进行系统回顾。", "method": "本文通过全面的文献综述，系统地回顾了3D人体交互生成领域。综述首先介绍基础技术，然后详细阐述了针对人-场景、人-物体和人-人三种子任务的方法、数据集和评估指标，最后探讨了未来的研究方向。", "result": "综述提供了该领域当前进展的全面概述，突出了关键挑战，并展望了未来的研究方向。", "conclusion": "本综述全面概述了3D人体交互生成领域的最新进展，明确了现有挑战，并旨在启发未来的研究工作。", "translation": "3D人体交互生成已成为一个关键研究领域，专注于在人类与各种交互实体之间产生动态且与上下文相关的交互。近年来，3D模型表示方法、运动捕捉技术和生成模型的快速发展为该领域日益增长的兴趣奠定了坚实基础。该领域的现有研究可大致分为三个方面：人-场景交互、人-物体交互和人-人交互。尽管该领域取得了快速进展，但由于需要生成自然的人体动作以及人类与交互实体之间准确的交互，挑战依然存在。在本综述中，我们对人体交互生成进行了全面的文献回顾，据我们所知，这是首次此类回顾。我们首先介绍基础技术，包括模型表示、运动捕捉方法和生成模型。随后，我们介绍了针对三个子任务提出的方法，以及它们相应的数据集和评估指标。最后，我们讨论了该领域潜在的未来研究方向并结束本综述。通过本综述，我们旨在全面概述该领域的当前进展，突出关键挑战，并启发未来的研究工作。", "summary": "本综述对3D人体交互生成这一关键研究领域进行了首次全面的文献回顾。该领域旨在生成人与交互实体之间动态且上下文相关的交互。文章首先介绍了模型表示、运动捕捉和生成模型等基础技术，随后详细阐述了人-场景、人-物体和人-人三种交互子任务的方法、数据集和评估指标。最后，讨论了该领域未来的研究方向。本综述旨在全面展示当前进展，突出主要挑战，并激发未来的研究。", "keywords": "3D人体交互生成, 综述, 人体运动生成, 人机交互, 生成模型", "comments": "这篇综述是该领域首次全面的文献回顾，对于新进入该领域的研究者具有重要价值，有助于系统梳理现有知识，并为未来研究指明方向。"}}
{"id": "2503.13125", "pdf": "https://arxiv.org/pdf/2503.13125", "abs": "https://arxiv.org/abs/2503.13125", "authors": ["Pan Liu"], "title": "Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser Chips Based On Consistent Texture Entropy Recursive Optimization Semi-Supervised Network", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Laser chips, the core components of semiconductor lasers, are extensively\nutilized in various industries, showing great potential for future application.\nSmoothness emitting surfaces are crucial in chip production, as even\nimperceptible scratches can significantly degrade performance and lifespan,\nthus impeding production efficiency and yield. Therefore, non-destructively\ndetecting these imperceptible scratches on the emitting surfaces is essential\nfor enhancing yield and reducing costs. These sub-micron level scratches,\nbarely visible against the background, are extremely difficult to detect with\nconventional methods, compounded by a lack of labeled datasets. To address this\nchallenge, this paper introduces TexRecNet, a consistent texture entropy\nrecursive optimization semi-supervised network. The network, based on a\nrecursive optimization architecture, iteratively improves the detection\naccuracy of imperceptible scratch edges, using outputs from previous cycles to\ninform subsequent inputs and guide the network's positional encoding. It also\nintroduces image texture entropy, utilizing a substantial amount of unlabeled\ndata to expand the training set while maintaining training signal reliability.\nUltimately, by analyzing the inconsistency of the network output sequences\nobtained during the recursive process, a semi-supervised training strategy with\nrecursive consistency constraints is proposed, using outputs from the recursive\nprocess for non-destructive signal augmentation and consistently optimizes the\nloss function for efficient end-to-end training. Experimental results show that\nthis method, utilizing a substantial amount of unsupervised data, achieves\n75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5%\nand 33.6% improvement over conventional Unet, enhancing quality control in\nlaser chips.", "AI": {"title_translation": "基于一致纹理熵递归优化半监督网络的激光芯片亚微米级不可察觉划痕无损检测", "tldr": "本文提出了一种名为TexRecNet的半监督网络，用于无损检测激光芯片上难以察觉的亚微米级划痕，显著提高了检测精度，尤其是在缺乏标记数据的情况下。", "motivation": "激光芯片表面的平滑度对性能和寿命至关重要，即使是亚微米级的不可察觉划痕也会严重降低性能和寿命，影响生产效率和良率。传统方法难以检测这些划痕，且缺乏标记数据集，因此需要一种新的无损检测方法来提高良率和降低成本。", "method": "本文提出了TexRecNet，一个基于一致纹理熵递归优化的半监督网络。该网络采用递归优化架构，通过前一循环的输出指导后续输入和网络的位置编码，迭代提高不可察觉划痕边缘的检测精度。它还引入了图像纹理熵，利用大量未标记数据扩展训练集并保持训练信号可靠性。通过分析递归过程中网络输出序列的不一致性，提出了一种带有递归一致性约束的半监督训练策略，利用递归过程的输出进行无损信号增强，并持续优化损失函数以实现高效的端到端训练。", "result": "该方法利用大量无监督数据，在不可察觉划痕检测中实现了75.6%的准确率和74.8%的召回率，比传统的Unet分别提高了8.5%和33.6%。", "conclusion": "TexRecNet能够有效无损检测激光芯片上的亚微米级不可察觉划痕，显著提高检测精度，对于提高激光芯片的质量控制具有重要意义。", "translation": "激光芯片作为半导体激光器的核心部件，在各行业中得到广泛应用，展现出巨大的未来应用潜力。发射表面的光滑度在芯片生产中至关重要，因为即使是不可察觉的划痕也会显著降低性能和寿命，从而阻碍生产效率和良率。因此，无损检测发射表面上这些不可察觉的划痕对于提高良率和降低成本至关重要。这些亚微米级的划痕，在背景下几乎不可见，传统方法极难检测，且缺乏标记数据集。为解决这一挑战，本文引入了TexRecNet，一个一致纹理熵递归优化半监督网络。该网络基于递归优化架构，迭代提高不可察觉划痕边缘的检测精度，使用前一循环的输出指导后续输入并引导网络的位置编码。它还引入了图像纹理熵，利用大量未标记数据扩展训练集，同时保持训练信号的可靠性。最终，通过分析递归过程中获得的网络输出序列的不一致性，提出了一种带有递归一致性约束的半监督训练策略，利用递归过程的输出进行无损信号增强，并持续优化损失函数以实现高效的端到端训练。实验结果表明，该方法利用大量无监督数据，在检测不可察觉划痕方面达到了75.6%的准确率和74.8%的召回率，比传统Unet分别提高了8.5%和33.6%，从而增强了激光芯片的质量控制。", "summary": "本文提出了一种名为TexRecNet的半监督网络，旨在解决激光芯片上亚微米级不可察觉划痕的无损检测难题。针对传统方法检测困难和缺乏标记数据的挑战，TexRecNet结合了递归优化架构和图像纹理熵，通过迭代优化和利用大量未标记数据进行训练集扩展，并提出了递归一致性约束的半监督训练策略。实验证明，该方法在准确率和召回率上均显著优于传统Unet，有效提升了激光芯片的质量控制。", "keywords": "激光芯片, 划痕检测, 半监督学习, 纹理熵, 递归优化", "comments": "本文的创新点在于提出了TexRecNet，它巧妙地结合了递归优化和纹理熵的概念来解决亚微米级划痕检测中数据标注稀缺的问题。递归优化架构通过迭代细化检测结果，而纹理熵的引入则有效地利用了大量的无标签数据，这对于实际工业应用中数据获取的挑战性是一个重要的突破。其半监督学习策略在提高检测精度的同时，也降低了对昂贵人工标注数据的依赖，具有重要的实用价值和工程意义。"}}
{"id": "2503.13130", "pdf": "https://arxiv.org/pdf/2503.13130", "abs": "https://arxiv.org/abs/2503.13130", "authors": ["Ling-An Zeng", "Guohong Huang", "Yi-Lin Wei", "Shengbo Gu", "Yu-Ming Tang", "Jingke Meng", "Wei-Shi Zheng"], "title": "ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We propose ChainHOI, a novel approach for text-driven human-object\ninteraction (HOI) generation that explicitly models interactions at both the\njoint and kinetic chain levels. Unlike existing methods that implicitly model\ninteractions using full-body poses as tokens, we argue that explicitly modeling\njoint-level interactions is more natural and effective for generating realistic\nHOIs, as it directly captures the geometric and semantic relationships between\njoints, rather than modeling interactions in the latent pose space. To this\nend, ChainHOI introduces a novel joint graph to capture potential interactions\nwith objects, and a Generative Spatiotemporal Graph Convolution Network to\nexplicitly model interactions at the joint level. Furthermore, we propose a\nKinematics-based Interaction Module that explicitly models interactions at the\nkinetic chain level, ensuring more realistic and biomechanically coherent\nmotions. Evaluations on two public datasets demonstrate that ChainHOI\nsignificantly outperforms previous methods, generating more realistic, and\nsemantically consistent HOIs. Code is available\n\\href{https://github.com/qinghuannn/ChainHOI}{here}.", "AI": {"title_translation": "ChainHOI：基于关节点的运动链建模用于人-物交互生成", "tldr": "ChainHOI是一种新颖的文本驱动人-物交互（HOI）生成方法，通过显式建模关节点和运动链层面的交互，生成更真实、生物力学更连贯的HOI，显著优于现有方法。", "motivation": "现有方法隐式地使用全身姿态作为标记来建模人-物交互，作者认为显式建模关节点层面的交互更为自然和有效，因为它直接捕获关节点间的几何和语义关系，而不是在潜在姿态空间中建模交互。", "method": "ChainHOI引入了一个新颖的关节点图来捕获与物体潜在的交互，并使用生成式时空图卷积网络在关节点层面显式建模交互。此外，它提出了一个基于运动学的交互模块，在运动链层面显式建模交互，以确保更真实和生物力学更连贯的运动。", "result": "在两个公共数据集上的评估表明，ChainHOI显著优于现有方法，生成了更真实、语义上更一致的人-物交互。", "conclusion": "通过显式建模关节点和运动链层面的交互，ChainHOI能够生成更真实、生物力学上更连贯的人-物交互，并显著超越现有方法。", "translation": "我们提出了ChainHOI，一种新颖的文本驱动人-物交互（HOI）生成方法，该方法在关节点和运动链层面明确建模交互。与现有使用全身姿态作为标记隐式建模交互的方法不同，我们认为明确建模关节点层面的交互对于生成真实的人-物交互更为自然和有效，因为它直接捕获了关节点之间的几何和语义关系，而不是在潜在姿态空间中建模交互。为此，ChainHOI引入了一个新颖的关节点图来捕获与物体的潜在交互，并使用生成式时空图卷积网络来明确建模关节点层面的交互。此外，我们提出了一个基于运动学的交互模块，该模块明确建模运动链层面的交互，确保更真实和生物力学上更连贯的运动。在两个公共数据集上的评估表明，ChainHOI显著优于现有方法，生成了更真实、语义上更一致的人-物交互。代码已在此处提供。", "summary": "ChainHOI是一种用于文本驱动人-物交互（HOI）生成的新方法，它通过显式建模关节点和运动链层面的交互来改进现有方法。该方法引入了一个新颖的关节点图和生成式时空图卷积网络来处理关节点层面的交互，并使用基于运动学的交互模块来确保运动链层面的生物力学一致性。实验结果表明，ChainHOI在生成真实且语义一致的HOI方面显著优于现有方法。", "keywords": "人-物交互, 运动链, 关节点建模, 文本驱动生成, 图卷积网络", "comments": "该论文的创新点在于其显式建模人-物交互的方式，突破了传统隐式建模的局限性。通过引入关节点图和运动链层面的建模，ChainHOI能够生成更符合生物力学原理且更真实的交互，这对于虚拟现实、机器人等领域具有重要意义。其贡献在于提供了一种更有效、更自然的HOI生成范式。"}}
{"id": "2503.13131", "pdf": "https://arxiv.org/pdf/2503.13131", "abs": "https://arxiv.org/abs/2503.13131", "authors": ["Yaxi Chen", "Simin Ni", "Aleksandra Ivanova", "Shaheer U. Saeed", "Rikin Hargunani", "Jie Huang", "Chaozong Liu", "Yipeng Hu"], "title": "Patient-specific radiomic feature selection with reconstructed healthy persona of knee MR images", "categories": ["cs.CV"], "comment": null, "summary": "Classical radiomic features have been designed to describe image appearance\nand intensity patterns. These features are directly interpretable and readily\nunderstood by radiologists. Compared with end-to-end deep learning (DL) models,\nlower dimensional parametric models that use such radiomic features offer\nenhanced interpretability but lower comparative performance in clinical tasks.\nIn this study, we propose an approach where a standard logistic regression\nmodel performance is substantially improved by learning to select radiomic\nfeatures for individual patients, from a pool of candidate features. This\napproach has potentials to maintain the interpretability of such approaches\nwhile offering comparable performance to DL. We also propose to expand the\nfeature pool by generating a patient-specific healthy persona via\nmask-inpainting using a denoising diffusion model trained on healthy subjects.\nSuch a pathology-free baseline feature set allows further opportunity in novel\nfeature discovery and improved condition classification. We demonstrate our\nmethod on multiple clinical tasks of classifying general abnormalities,\nanterior cruciate ligament tears, and meniscus tears. Experimental results\ndemonstrate that our approach achieved comparable or even superior performance\nthan state-of-the-art DL approaches while offering added interpretability by\nusing radiomic features extracted from images and supplemented by generating\nhealthy personas. Example clinical cases are discussed in-depth to demonstrate\nthe intepretability-enabled utilities such as human-explainable feature\ndiscovery and patient-specific location/view selection. These findings\nhighlight the potentials of the combination of subject-specific feature\nselection with generative models in augmenting radiomic analysis for more\ninterpretable decision-making. The codes are available at:\nhttps://github.com/YaxiiC/RadiomicsPersona.git", "AI": {"title_translation": "膝关节MR图像重建健康特征的患者特异性影像组学特征选择", "tldr": "本文提出了一种结合患者特异性特征选择和生成式模型（健康画像）的方法，显著提高了影像组学模型在膝关节MR图像诊断任务中的性能和可解释性，达到或超越了深度学习模型。", "motivation": "经典的影像组学特征虽然可解释性强，但在临床任务中的性能通常低于端到端深度学习模型。本研究的动机是提高影像组学模型的性能，同时保持其固有的可解释性。", "method": "本研究提出了一种方法，通过为个体患者从候选特征池中学习选择影像组学特征，显著提高了标准逻辑回归模型的性能。此外，通过使用在健康受试者上训练的去噪扩散模型进行掩膜修复，生成患者特异性健康画像来扩展特征池，从而利用这种无病理基线特征集进行新特征发现和改进疾病分类。该方法在分类一般异常、前交叉韧带撕裂和半月板撕裂等多个临床任务上进行了验证。", "result": "实验结果表明，本研究提出的方法在多个临床任务上实现了与最先进的深度学习方法相当甚至更优的性能。该方法通过使用从图像中提取并由生成健康画像补充的影像组学特征，提供了额外的可解释性。通过深入讨论临床案例，展示了可解释性带来的实用性，如人类可解释的特征发现和患者特异性位置/视图选择。", "conclusion": "研究结果强调了主题特异性特征选择与生成模型相结合在增强影像组学分析以实现更可解释决策方面的潜力。", "translation": "经典的影像组学特征旨在描述图像外观和强度模式。这些特征可直接解释并易于放射科医生理解。与端到端深度学习（DL）模型相比，使用此类影像组学特征的低维参数模型提供了更强的可解释性，但在临床任务中的比较性能较低。在本研究中，我们提出了一种方法，通过学习为个体患者从候选特征池中选择影像组学特征，从而显著提高了标准逻辑回归模型的性能。这种方法有可能保持此类方法的可解释性，同时提供与DL相当的性能。我们还提出通过使用在健康受试者上训练的去噪扩散模型进行掩膜修复，生成患者特异性健康画像来扩展特征池。这种无病理基线特征集为新特征发现和改进疾病分类提供了进一步的机会。我们在分类一般异常、前交叉韧带撕裂和半月板撕裂等多个临床任务上展示了我们的方法。实验结果表明，我们的方法实现了与最先进的DL方法相当甚至更优的性能，同时通过使用从图像中提取并由生成健康画像补充的影像组学特征提供了额外的可解释性。深入讨论了临床案例，以展示可解释性带来的实用性，例如人类可解释的特征发现和患者特异性位置/视图选择。这些发现强调了主题特异性特征选择与生成模型相结合在增强影像组学分析以实现更可解释决策的潜力。代码可在：https://github.com/YaxiiC/RadiomicsPersona.git 获取。", "summary": "本文提出了一种创新的影像组学分析方法，旨在克服传统影像组学模型性能不足和深度学习模型可解释性差的局限。该方法通过为每个患者动态选择影像组学特征，并结合通过去噪扩散模型生成的患者特异性“健康画像”来扩展特征集，显著提升了标准逻辑回归模型在膝关节MR图像诊断任务中的性能。实验结果表明，该方法在多个临床分类任务上达到了与最先进深度学习模型相当甚至更优的性能，同时保持了影像组学固有的可解释性，为更透明的临床决策提供了可能。", "keywords": "影像组学, 特征选择, 健康画像, 去噪扩散模型, 可解释性, 膝关节MR图像", "comments": "这篇论文的创新点在于将患者特异性特征选择与生成式模型（用于创建“健康画像”）相结合，有效地解决了医学图像分析中可解释性和性能之间的权衡问题。通过生成健康画像作为额外的特征维度，不仅扩展了特征池，还为发现与疾病相关的、人类可解释的特征提供了新的视角。这种方法对于需要高度可信度和解释性的临床决策场景尤其重要，因为它在保持或超越DL模型性能的同时，提供了更清晰的决策依据。"}}
{"id": "2503.13134", "pdf": "https://arxiv.org/pdf/2503.13134", "abs": "https://arxiv.org/abs/2503.13134", "authors": ["Prakhar Bhardwaj", "Sheethal Bhat", "Andreas Maier"], "title": "Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis", "categories": ["cs.CV"], "comment": null, "summary": "Due to the large volume of medical imaging data, advanced AI methodologies\nare needed to assist radiologists in diagnosing thoracic diseases from chest\nX-rays (CXRs). Existing deep learning models often require large, labeled\ndatasets, which are scarce in medical imaging due to the time-consuming and\nexpert-driven annotation process. In this paper, we extend the existing\napproach to enhance zero-shot learning in medical imaging by integrating\nContrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),\nresulting in our proposed model, MoCoCLIP. Our method addresses challenges\nposed by class-imbalanced and unlabeled datasets, enabling improved detection\nof pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset\ndemonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,\nachieving relative improvement of approximately 6.5%. Furthermore, on the\nCheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,\nachieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,\nhighlighting its enhanced generalization capabilities on unseen data.", "AI": {"title_translation": "增强医学影像中的零样本学习：整合CLIP与先进技术以改进胸部X光分析", "tldr": "MoCoCLIP模型通过结合CLIP和MoCo，在胸部X光分析中实现了零样本学习的显著改进，超越了现有最先进模型。", "motivation": "由于医学影像数据量大，且标注数据集稀缺、耗时，现有深度学习模型难以有效诊断胸腔疾病。因此需要先进的AI方法来辅助放射科医生，尤其是在面对类别不平衡和未标注数据集时。", "method": "本研究通过将对比语言-图像预训练（CLIP）与动量对比（MoCo）相结合，提出了MoCoCLIP模型，以增强医学影像中的零样本学习能力。该方法旨在解决类别不平衡和未标注数据集带来的挑战。", "result": "MoCoCLIP在NIH ChestXray14数据集上的表现优于最先进的CheXZero模型，相对提升约6.5%。在CheXpert数据集上，MoCoCLIP的零样本性能也更优，平均AUC为0.750，而CheXZero为0.746。", "conclusion": "MoCoCLIP模型通过整合CLIP和MoCo，在医学影像的零样本学习中展现出卓越的性能，特别是在胸部X光分析中，其泛化能力在未见数据上得到了显著增强。", "translation": "由于医学影像数据量庞大，需要先进的人工智能方法来协助放射科医生从胸部X光片（CXR）中诊断胸腔疾病。现有的深度学习模型通常需要大量标注数据集，而这在医学影像领域是稀缺的，因为标注过程耗时且需要专家参与。在本文中，我们扩展了现有方法，通过将对比语言-图像预训练（CLIP）与动量对比（MoCo）相结合，增强了医学影像中的零样本学习，从而提出了我们模型MoCoCLIP。我们的方法解决了类别不平衡和未标注数据集带来的挑战，从而改进了肺部病理的检测。在NIH ChestXray14数据集上的实验结果表明，MoCoCLIP优于最先进的CheXZero模型，实现了约6.5%的相对改进。此外，在CheXpert数据集上，MoCoCLIP展现出卓越的零样本性能，平均AUC为0.750，而CheXZero为0.746，这突显了其在未见数据上的泛化能力得到增强。", "summary": "本文提出了一种名为MoCoCLIP的新模型，旨在通过结合对比语言-图像预训练（CLIP）和动量对比（MoCo）来增强医学影像中的零样本学习，特别是在胸部X光分析方面。该模型解决了医学影像中缺乏大量标注数据的挑战，并在NIH ChestXray14和CheXpert数据集上表现出优于现有最先进模型CheXZero的性能，证明了其在检测肺部病理和处理未见数据方面的优越泛化能力。", "keywords": "零样本学习, 医学影像, 胸部X光, CLIP, MoCoCLIP", "comments": "该论文通过将CLIP与MoCo结合，为医学影像领域的零样本学习提供了一个创新且有效的解决方案，尤其在数据标注成本高昂的医疗领域具有重要意义。MoCoCLIP在多个数据集上超越了现有SOTA模型，展示了其强大的泛化能力和实用潜力。"}}
{"id": "2503.13139", "pdf": "https://arxiv.org/pdf/2503.13139", "abs": "https://arxiv.org/abs/2503.13139", "authors": ["Weiyu Guo", "Ziyang Chen", "Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Jinhui Ye", "Ying Sun", "Hui Xiong"], "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": "18 pages, under review", "summary": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.", "AI": {"title_translation": "Logic-in-Frames：通过视觉语义逻辑验证进行长视频理解的动态关键帧搜索", "tldr": "Logic-in-Frames：一个通过视觉语义逻辑依赖在长视频中动态搜索关键帧的语义驱动框架，在关键帧选择上达到SOTA，并提升下游视频问答任务表现。", "motivation": "现有长视频理解方法常忽略文本查询与视觉元素间的逻辑关系，且计算限制导致粗粒度帧采样，难以有效定位关键信息。", "method": "提出“视觉语义逻辑搜索”框架，将关键帧选择重构为语义驱动问题。系统定义了四种基本逻辑依赖：空间共现、时间邻近、属性依赖和因果顺序。这些关系通过迭代细化过程动态更新帧采样分布，以实现上下文感知地识别语义关键帧。", "result": "在关键帧选择指标的手动标注基准上取得了新的SOTA性能。在下游视频问答任务中，于LongVideoBench和Video-MME上表现出优于现有方法的最佳性能提升。", "conclusion": "所提出的方法有效弥合了文本查询与视觉-时间推理之间的逻辑鸿沟，验证了其在长视频理解中的有效性。", "translation": "理解长视频内容是一项复杂的任务，通常依赖于密集采样的帧字幕或端到端特征选择器，然而这些技术常常忽略文本查询与视觉元素之间的逻辑关系。在实践中，计算限制要求进行粗略的帧子采样，这类似于“大海捞针”的挑战。为了解决这个问题，我们引入了一个语义驱动的搜索框架，该框架在视觉语义逻辑搜索的范式下重新定义了关键帧选择。具体来说，我们系统地定义了四种基本逻辑依赖：1) 空间共现，2) 时间邻近，3) 属性依赖，和 4) 因果顺序。这些关系通过迭代细化过程动态更新帧采样分布，从而实现根据特定查询要求进行上下文感知地识别语义关键帧。我们的方法在关键帧选择指标的手动标注基准上取得了新的SOTA性能。此外，当应用于下游视频问答任务时，所提出的方法在LongVideoBench和Video-MME上表现出优于现有方法的最佳性能提升，验证了其在弥合文本查询与视觉-时间推理之间逻辑鸿沟方面的有效性。代码将公开可用。", "summary": "本文提出Logic-in-Frames，一个新颖的语义驱动框架，用于长视频中的动态关键帧搜索。它通过视觉语义逻辑搜索范式重新定义关键帧选择，解决了长视频理解的挑战。该框架定义并利用了四种基本逻辑依赖（空间共现、时间邻近、属性依赖、因果顺序），这些依赖通过迭代细化过程动态更新帧采样分布，以识别语义关键帧。该方法在关键帧选择方面取得了最先进的性能，并显著提升了下游视频问答任务的表现，验证了其弥合文本查询与视觉内容之间逻辑鸿沟的能力。", "keywords": "关键帧搜索, 长视频理解, 视觉语义逻辑验证, 动态采样, 视频问答", "comments": "本文的创新之处在于明确定义和利用语义逻辑依赖关系进行动态关键帧采样，超越了传统的密集采样或端到端特征选择方法。这解决了长视频理解中计算限制的实际挑战。其在下游任务中的应用进一步突出了其实用性。"}}
{"id": "2503.13147", "pdf": "https://arxiv.org/pdf/2503.13147", "abs": "https://arxiv.org/abs/2503.13147", "authors": ["Jiayi Fu", "Siyu Liu", "Zikun Liu", "Chun-Le Guo", "Hyunhee Park", "Ruiqi Wu", "Guoqing Wang", "Chongyi Li"], "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Acceptted by CVPR 2025", "summary": "We propose a novel Iterative Predictor-Critic Code Decoding framework for\nreal-world image dehazing, abbreviated as IPC-Dehaze, which leverages the\nhigh-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from\nprevious codebook-based methods that rely on one-shot decoding, our method\nutilizes high-quality codes obtained in the previous iteration to guide the\nprediction of the Code-Predictor in the subsequent iteration, improving code\nprediction accuracy and ensuring stable dehazing performance. Our idea stems\nfrom the observations that 1) the degradation of hazy images varies with haze\ndensity and scene depth, and 2) clear regions play crucial cues in restoring\ndense haze regions. However, it is non-trivial to progressively refine the\nobtained codes in subsequent iterations, owing to the difficulty in determining\nwhich codes should be retained or replaced at each iteration. Another key\ninsight of our study is to propose Code-Critic to capture interrelations among\ncodes. The Code-Critic is used to evaluate code correlations and then resample\na set of codes with the highest mask scores, i.e., a higher score indicates\nthat the code is more likely to be rejected, which helps retain more accurate\ncodes and predict difficult ones. Extensive experiments demonstrate the\nsuperiority of our method over state-of-the-art methods in real-world dehazing.", "AI": {"title_translation": "真实世界图像去雾的迭代预测器-评论器代码解码", "tldr": "提出了一种名为IPC-Dehaze的迭代预测器-评论器代码解码框架，通过迭代细化和Code-Critic机制，利用VQGAN代码本先验来提高真实世界图像去雾的性能。", "motivation": "现有代码本去雾方法依赖一次性解码，但雾霾图像的降解随密度和深度变化，且难以在迭代中有效精炼代码，因为难以确定保留或替换哪些代码。本研究旨在解决这些挑战，并利用清晰区域对恢复浓雾区域的关键作用。", "method": "提出IPC-Dehaze框架，利用预训练VQGAN的高质量代码本先验。与一次性解码不同，IPC-Dehaze通过Code-Predictor利用前一次迭代的高质量代码指导后续迭代的预测。引入Code-Critic来捕捉代码间的相互关系，评估代码相关性，并根据掩码分数重新采样代码，以保留更准确的代码并预测困难的代码。", "result": "大量实验证明，该方法在真实世界去雾方面优于现有最先进的方法。", "conclusion": "IPC-Dehaze框架通过迭代预测器和Code-Critic机制，有效解决了真实世界图像去雾中代码精炼的难题，显著提升了去雾性能。", "translation": "我们提出了一种新颖的用于真实世界图像去雾的迭代预测器-评论器代码解码框架，简称IPC-Dehaze，该框架利用了预训练VQGAN中封装的高质量代码本先验。与以往依赖一次性解码的代码本方法不同，我们的方法利用在前一次迭代中获得的高质量代码来指导Code-Predictor在后续迭代中的预测，从而提高代码预测精度并确保稳定的去雾性能。我们的想法源于以下观察：1）雾霾图像的降解随雾霾密度和场景深度而变化；2）清晰区域在恢复浓雾区域中起到关键作用。然而，在后续迭代中逐步细化所获得的代码并非易事，因为难以确定在每次迭代中哪些代码应该保留或替换。我们研究的另一个关键见解是提出了Code-Critic来捕捉代码之间的相互关系。Code-Critic用于评估代码相关性，然后重新采样一组具有最高掩码分数的代码，即分数越高表示该代码越有可能被拒绝，这有助于保留更准确的代码并预测困难的代码。大量的实验证明了我们的方法在真实世界去雾方面优于现有最先进的方法。", "summary": "本文提出了一种名为IPC-Dehaze的迭代预测器-评论器代码解码框架，用于真实世界图像去雾。该框架利用预训练VQGAN的代码本先验，并通过迭代方式，使用前一迭代的高质量代码指导Code-Predictor进行后续预测，以提高精度。为解决代码精炼难题，引入Code-Critic评估代码相关性并筛选保留准确代码。实验结果表明，IPC-Dehaze在真实世界去雾任务上表现优异。", "keywords": "图像去雾, 迭代预测, 代码解码, VQGAN, Code-Critic", "comments": "该论文的创新点在于提出了迭代预测器-评论器代码解码框架（IPC-Dehaze），特别是引入了Code-Critic机制来动态地评估和精炼代码，解决了以往代码本方法中一次性解码或迭代精炼困难的问题。这种迭代和自适应的代码优化方法对于提高图像去雾的稳定性和准确性具有重要意义，尤其是在处理真实世界中复杂多变的雾霾条件时。"}}
{"id": "2503.13156", "pdf": "https://arxiv.org/pdf/2503.13156", "abs": "https://arxiv.org/abs/2503.13156", "authors": ["Zakariae Zrimek", "Youssef Mourchid", "Mohammed El Hassouni"], "title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph Knowledge Distillation for Gait Disorders Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.", "AI": {"title_translation": "DynSTG-Mamba：结合跨图知识蒸馏的动态时空图Mamba用于步态障碍识别", "tldr": "DynSTG-Mamba是一个新的轻量级且高精度的框架，结合了动态时空图神经网络和Mamba架构，并通过跨图知识蒸馏来识别步态障碍，解决了现有方法内存需求高和捕获复杂时空依赖的挑战。", "motivation": "现有的时空图卷积网络（ST-GCNs）在步态障碍识别中面临内存需求高和难以捕获复杂时空依赖的问题，限制了其在临床应用中的效率。", "method": "我们提出了DynSTG-Mamba框架，它结合了DF-STGNN和STG-Mamba。DF-STGNN采用动态时空滤波器，自适应调整骨骼关节的空间连接和不同运动阶段的时间交互。STG-Mamba是Mamba的扩展，用于捕获长期依赖并降低计算复杂性。为减少模型参数和计算成本，同时保持一致性，我们提出了跨图关系知识蒸馏，通过共享内存对齐教师模型和学生模型之间的关系信息。", "result": "DynSTG-Mamba在KOA-NM、PD-WALK和ATAXIA数据集上，在准确率、F1分数和召回率方面均优于现有最先进的方法。", "conclusion": "我们的方法高效且鲁棒，为自动化步态分析和运动障碍评估提供了一个轻量级但高精度的解决方案。", "translation": "步态障碍识别在运动障碍的早期诊断和监测中起着至关重要的作用。包括时空图卷积网络（ST-GCNs）在内的现有方法通常面临高内存需求，并且难以捕获复杂的时空依赖性，这限制了它们在临床应用中的效率。为了应对这些挑战，我们引入了DynSTG-Mamba（动态时空图Mamba），这是一个结合了DF-STGNN和STG-Mamba的新型框架，旨在增强运动序列建模。DF-STGNN包含一个动态时空滤波器，该滤波器自适应地调整骨骼关节之间的空间连接以及跨不同运动阶段的时间交互。这种方法通过考虑骨骼步态数据的层次结构和动态性，确保了通过动态图结构更好地传播特征。同时，STG-Mamba是Mamba的扩展，适用于骨骼运动数据，确保状态的连续传播，有助于捕获长期依赖性，同时降低计算复杂性。为了减少模型参数和计算成本，同时保持一致性，我们提出了一种新颖的知识转移机制——跨图关系知识蒸馏，该机制在共享内存的同时对齐教师（大型架构）和学生模型（小型架构）之间的关系信息。这确保了在运动序列中准确保留关节的交互和运动模式。我们在KOA-NM、PD-WALK和ATAXIA数据集上验证了我们的DynSTG-Mamba，在准确率、F1分数和召回率方面均优于现有最先进的方法。我们的结果突出了我们方法的效率和鲁棒性，为自动化步态分析和运动障碍评估提供了一个轻量级但高精度的解决方案。", "summary": "本研究提出了一种名为DynSTG-Mamba的新型框架，用于步态障碍识别。该框架结合了动态时空图神经网络（DF-STGNN）和Mamba架构（STG-Mamba），以解决现有方法在处理复杂时空依赖和高内存需求方面的局限性。DynSTG-Mamba通过动态滤波器自适应调整空间和时间连接，并利用Mamba捕获长期依赖。为降低模型复杂性，还引入了跨图关系知识蒸馏机制。实验结果表明，DynSTG-Mamba在多个步态数据集上表现优异，提供了高效、轻量且高精度的解决方案。", "keywords": "步态障碍识别, 时空图, Mamba, 知识蒸馏, 运动序列建模", "comments": "该论文的创新点在于将动态时空图结构与Mamba架构相结合，有效解决了步态分析中长期依赖捕获和计算效率的问题。引入跨图知识蒸馏进一步优化了模型大小和性能，使其更适合实际临床应用，具有重要的实用价值。"}}
{"id": "2503.13160", "pdf": "https://arxiv.org/pdf/2503.13160", "abs": "https://arxiv.org/abs/2503.13160", "authors": ["Zihao Liu", "Xiaoyu Wu", "Jianqin Wu", "Xuxu Wang", "Linlin Yang"], "title": "Language-guided Open-world Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released.", "AI": {"title_translation": "语言引导的开放世界视频异常检测", "tldr": "提出LaGoVAD模型，通过自然语言指导在开放世界中检测视频异常，并构建了PreVAD数据集以支持训练。", "motivation": "现有视频异常检测方法假设异常定义不变，不适用于开放世界中异常定义会随需求变化的情况。", "method": "提出新的开放世界VAD范式，允许通过用户提供的自然语言指导检测。具体模型是LaGoVAD，通过动态视频合成多样化异常相对持续时间，并通过负挖掘对比学习增强特征鲁棒性。为训练模型，构建了迄今最大、最多样化的视频异常数据集PreVAD，包含35,279个带多级类别标签和明确异常定义的视频。", "result": "在七个数据集上的零样本实验中表现出最先进的性能。", "conclusion": "该研究成功提出了一个适应开放世界视频异常检测的语言引导范式和相应的模型与数据集，有效解决了现有方法的局限性。", "translation": "视频异常检测模型旨在检测偏离预期的异常。在开放世界场景中，预期事件可能随着需求变化而改变。例如，在流感爆发期间不戴口罩被认为是异常的，但在其他情况下则是正常的。然而，现有方法假设异常的定义是不变的，因此不适用于开放世界。为了解决这个问题，我们提出了一种新颖的开放世界视频异常检测（VAD）范式，该范式具有可变定义，允许在推理时通过用户提供的自然语言进行引导检测。这种范式需要建立视频和文本定义到异常分数的鲁棒映射。因此，我们提出了LaGoVAD（语言引导的开放世界VAD），一个通过两种正则化策略动态调整异常定义的模型：通过动态视频合成多样化异常的相对持续时间，以及通过负挖掘对比学习增强特征鲁棒性。训练这种适应性强的模型需要多样化的异常定义，但现有数据集通常提供给定标签而没有语义描述。为了弥补这一差距，我们收集了PreVAD（预训练视频异常数据集），这是迄今为止最大、最多样化的视频异常数据集，包含35,279个带有明确定义异常的多级类别标签和描述的视频。在七个数据集上的零样本实验证明了最先进的性能。数据和代码将发布。", "summary": "本文提出了一种新颖的语言引导开放世界视频异常检测（VAD）范式，旨在解决现有VAD方法无法适应异常定义随需求变化的问题。为此，作者开发了LaGoVAD模型，通过动态视频合成和对比学习增强其对可变异常定义的适应性。为支持模型训练，还构建了迄今为止规模最大、多样性最强的PreVAD数据集，该数据集包含大量带有明确异常描述的视频。实验结果表明，LaGoVAD在多个数据集上实现了零样本设置下的最先进性能。", "keywords": "视频异常检测, 开放世界, 语言引导, 对比学习, PreVAD数据集", "comments": "这篇论文的创新点在于提出了一个语言引导的开放世界视频异常检测范式，允许异常定义动态变化，这显著提升了VAD模型在现实世界复杂场景中的实用性。其重要性在于解决了传统VAD方法对固定异常定义的依赖，并通过构建大规模、高质量的PreVAD数据集为未来的研究提供了宝贵资源。"}}
{"id": "2503.13163", "pdf": "https://arxiv.org/pdf/2503.13163", "abs": "https://arxiv.org/abs/2503.13163", "authors": ["Shani Gamrian", "Hila Barel", "Feiran Li", "Masakazu Yoshimura", "Daisuke Iso"], "title": "Beyond RGB: Adaptive Parallel Processing for RAW Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Object detection models are typically applied to standard RGB images\nprocessed through Image Signal Processing (ISP) pipelines, which are designed\nto enhance sensor-captured RAW images for human vision. However, these ISP\nfunctions can lead to a loss of critical information that may be essential in\noptimizing for computer vision tasks, such as object detection. In this work,\nwe introduce Raw Adaptation Module (RAM), a module designed to replace the\ntraditional ISP, with parameters optimized specifically for RAW object\ndetection. Inspired by the parallel processing mechanisms of the human visual\nsystem, RAM departs from existing learned ISP methods by applying multiple ISP\nfunctions in parallel rather than sequentially, allowing for a more\ncomprehensive capture of image features. These processed representations are\nthen fused in a specialized module, which dynamically integrates and optimizes\nthe information for the target task. This novel approach not only leverages the\nfull potential of RAW sensor data but also enables task-specific\npre-processing, resulting in superior object detection performance. Our\napproach outperforms RGB-based methods and achieves state-of-the-art results\nacross diverse RAW image datasets under varying lighting conditions and dynamic\nranges.", "AI": {"title_translation": "超越RGB：用于RAW目标检测的自适应并行处理", "tldr": "提出Raw Adaptation Module (RAM)替代传统ISP，通过并行处理RAW图像提升目标检测性能，优于RGB方法。", "motivation": "传统ISP处理RGB图像会丢失对计算机视觉任务（如目标检测）至关重要的信息，影响模型性能。", "method": "引入Raw Adaptation Module (RAM)，旨在替代传统ISP，其参数专为RAW目标检测优化。RAM受人类视觉系统启发，并行应用多个ISP功能而非顺序处理，以更全面捕捉图像特征。这些处理后的表示随后在一个专门模块中融合，动态集成并优化信息以适应目标任务。", "result": "该方法利用了RAW传感器数据的全部潜力，实现了任务特定的预处理，从而获得卓越的目标检测性能。在不同光照条件和动态范围下的多样RAW图像数据集上，该方法优于基于RGB的方法并取得了最先进的结果。", "conclusion": "Not mentioned in abstract", "translation": "目标检测模型通常应用于通过图像信号处理（ISP）管道处理的标准RGB图像，这些管道旨在增强传感器捕获的RAW图像以适应人类视觉。然而，这些ISP功能可能导致关键信息的丢失，而这些信息对于优化计算机视觉任务（如目标检测）可能至关重要。在这项工作中，我们引入了原始适应模块（RAM），一个旨在替代传统ISP的模块，其参数专门针对RAW目标检测进行了优化。受人类视觉系统并行处理机制的启发，RAM通过并行而非顺序应用多个ISP功能，从而不同于现有学习型ISP方法，允许更全面地捕获图像特征。这些处理后的表示随后在一个专门模块中融合，该模块动态集成并优化目标任务的信息。这种新颖的方法不仅利用了RAW传感器数据的全部潜力，还实现了任务特定的预处理，从而获得了卓越的目标检测性能。我们的方法优于基于RGB的方法，并在不同光照条件和动态范围的各种RAW图像数据集上取得了最先进的结果。", "summary": "本文提出Raw Adaptation Module (RAM)，旨在替代传统ISP，直接处理RAW图像进行目标检测。RAM受人类视觉启发，采用并行ISP功能处理RAW数据，并动态融合多路特征，以保留更多原始信息。实验证明，该方法在RAW图像数据集上显著优于传统RGB方法，并达到了最先进的性能。", "keywords": "RAW图像, 目标检测, 并行处理, 图像信号处理 (ISP), Raw Adaptation Module (RAM)", "comments": "该论文的创新点在于引入了Raw Adaptation Module (RAM)，它打破了传统ISP的顺序处理模式，采用并行处理RAW图像的方式，并结合了任务特定的信息融合，有效解决了传统ISP在RGB图像处理中造成的信息丢失问题。这对于提升计算机视觉任务（特别是目标检测）在原始传感器数据上的性能具有重要意义，为未来的图像预处理方法提供了新思路。"}}
{"id": "2503.13165", "pdf": "https://arxiv.org/pdf/2503.13165", "abs": "https://arxiv.org/abs/2503.13165", "authors": ["Chen Zhao", "Zhizhou Chen", "Yunzhe Xu", "Enxuan Gu", "Jian Li", "Zili Yi", "Qian Wang", "Jian Yang", "Ying Tai"], "title": "From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Ultra-high-definition (UHD) image restoration faces significant challenges\ndue to its high resolution, complex content, and intricate details. To cope\nwith these challenges, we analyze the restoration process in depth through a\nprogressive spectral perspective, and deconstruct the complex UHD restoration\nproblem into three progressive stages: zero-frequency enhancement,\nlow-frequency restoration, and high-frequency refinement. Building on this\ninsight, we propose a novel framework, ERR, which comprises three collaborative\nsub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer\n(LFR), and the high-frequency refiner (HFR). Specifically, the ZFE integrates\nglobal priors to learn global mapping, while the LFR restores low-frequency\ninformation, emphasizing reconstruction of coarse-grained content. Finally, the\nHFR employs our designed frequency-windowed kolmogorov-arnold networks (FW-KAN)\nto refine textures and details, producing high-quality image restoration. Our\napproach significantly outperforms previous UHD methods across various tasks,\nwith extensive ablation studies validating the effectiveness of each component.\nThe code is available at \\href{https://github.com/NJU-PCALab/ERR}{here}.", "AI": {"title_translation": "从零到细节：从渐进光谱视角解构超高清图像恢复", "tldr": "本文提出了一种名为ERR的新框架，通过将超高清图像恢复分解为零频增强、低频恢复和高频细化三个渐进阶段，显著提升了图像恢复质量。", "motivation": "超高清（UHD）图像恢复由于其高分辨率、复杂内容和复杂细节而面临重大挑战。为了应对这些挑战，本文深入分析了恢复过程。", "method": "本文通过渐进光谱视角将复杂的UHD恢复问题解构为三个渐进阶段：零频增强、低频恢复和高频细化。在此基础上，提出了一种名为ERR的新框架，该框架包含三个协作子网络：零频增强器（ZFE）、低频恢复器（LFR）和高频细化器（HFR）。具体而言，ZFE整合全局先验以学习全局映射，LFR恢复低频信息并强调粗粒度内容的重建，HFR则采用设计的频率窗口Kolmogorov-Arnold网络（FW-KAN）来细化纹理和细节。", "result": "本文方法在各种任务中显著优于以前的UHD方法，并且通过广泛的消融研究验证了每个组件的有效性。", "conclusion": "通过将超高清图像恢复分解为渐进光谱阶段并引入ERR框架，本文成功克服了UHD图像恢复的挑战，并实现了高质量的图像恢复。", "translation": "超高清（UHD）图像恢复由于其高分辨率、复杂内容和复杂细节而面临重大挑战。为了应对这些挑战，我们通过渐进光谱视角深入分析了恢复过程，并将复杂的UHD恢复问题解构为三个渐进阶段：零频增强、低频恢复和高频细化。基于这一见解，我们提出了一种新颖的框架ERR，该框架包含三个协作子网络：零频增强器（ZFE）、低频恢复器（LFR）和高频细化器（HFR）。具体而言，ZFE整合全局先验以学习全局映射，而LFR恢复低频信息，强调粗粒度内容的重建。最后，HFR采用我们设计的频率窗口Kolmogorov-Arnold网络（FW-KAN）来细化纹理和细节，从而产生高质量的图像恢复。我们的方法在各种任务中显著优于以前的UHD方法，并通过广泛的消融研究验证了每个组件的有效性。代码可在https://github.com/NJU-PCALab/ERR获取。", "summary": "本文针对超高清（UHD）图像恢复的挑战，提出了一种从渐进光谱视角解构复杂恢复过程的方法。研究将UHD恢复分解为零频增强、低频恢复和高频细化三个阶段，并在此基础上设计了新颖的ERR框架。ERR包含零频增强器（ZFE）、低频恢复器（LFR）和高频细化器（HFR）三个协同子网络，其中HFR引入了频率窗口Kolmogorov-Arnold网络（FW-KAN）以精细化纹理和细节。实验结果表明，该方法在多项任务上显著超越了现有UHD恢复技术。", "keywords": "超高清图像恢复, 渐进光谱, 零频增强, 频率窗口Kolmogorov-Arnold网络, ERR框架", "comments": "本文的创新点在于将复杂的超高清图像恢复问题从渐进光谱视角进行了精细解构，并提出了一个模块化且高效的ERR框架。通过针对不同频率范围设计专门的子网络（ZFE, LFR, HFR），特别是引入频率窗口Kolmogorov-Arnold网络（FW-KAN）来处理高频细节，该方法有效地解决了UHD图像恢复中的挑战，具有重要的研究价值和实际应用潜力。"}}
{"id": "2503.13176", "pdf": "https://arxiv.org/pdf/2503.13176", "abs": "https://arxiv.org/abs/2503.13176", "authors": ["Rui Wang", "Quentin Lohmeyer", "Mirko Meboldt", "Siyu Tang"], "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing clean, distractor-free 3D scenes from real-world captures\nremains a significant challenge, particularly in highly dynamic and cluttered\nsettings such as egocentric videos. To tackle this problem, we introduce\nDeGauss, a simple and robust self-supervised framework for dynamic scene\nreconstruction based on a decoupled dynamic-static Gaussian Splatting design.\nDeGauss models dynamic elements with foreground Gaussians and static content\nwith background Gaussians, using a probabilistic mask to coordinate their\ncomposition and enable independent yet complementary optimization. DeGauss\ngeneralizes robustly across a wide range of real-world scenarios, from casual\nimage collections to long, dynamic egocentric videos, without relying on\ncomplex heuristics or extensive supervision. Experiments on benchmarks\nincluding NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that\nDeGauss consistently outperforms existing methods, establishing a strong\nbaseline for generalizable, distractor-free 3D reconstructionin highly dynamic,\ninteraction-rich environments.", "AI": {"title_translation": "DeGauss：结合高斯泼溅的动静态分解用于无干扰三维重建", "tldr": "DeGauss 是一种自监督框架，通过解耦的动静态高斯泼溅设计，实现动态场景中无干扰的 3D 重建，并在多个基准测试中表现优异。", "motivation": "从真实世界捕获中重建干净、无干扰的三维场景是一个重大挑战，尤其是在高度动态和杂乱的环境（如以自我为中心的视频）中。", "method": "DeGauss 引入了一个简单而鲁棒的自监督框架，基于解耦的动静态高斯泼溅设计。它使用前景高斯模型动态元素，背景高斯模型静态内容，并通过概率掩码协调它们的组合，实现独立而互补的优化。", "result": "DeGauss 在各种真实世界场景（从普通图像集合到长时间、动态的以自我为中心的视频）中表现出强大的泛化能力，无需复杂的启发式方法或大量监督。在 NeRF-on-the-go、ADT、AEA、Hot3D 和 EPIC-Fields 等基准测试中，DeGauss 始终优于现有方法。", "conclusion": "DeGauss 为在高度动态、交互丰富的环境中实现可泛化的无干扰三维重建建立了强大的基线。", "translation": "从真实世界捕获中重建干净、无干扰的三维场景仍然是一个重大挑战，尤其是在高度动态和杂乱的环境中，例如以自我为中心的视频。为了解决这个问题，我们引入了 DeGauss，一个简单而鲁棒的自监督框架，用于基于解耦的动静态高斯泼溅设计进行动态场景重建。DeGauss 使用前景高斯模型动态元素，背景高斯模型静态内容，并使用概率掩码协调它们的组合，实现独立而互补的优化。DeGauss 在各种真实世界场景中表现出强大的泛化能力，从普通图像集合到长时间、动态的以自我为中心的视频，无需依赖复杂的启发式方法或大量的监督。在包括 NeRF-on-the-go、ADT、AEA、Hot3D 和 EPIC-Fields 在内的基准测试中进行的实验表明，DeGauss 始终优于现有方法，为在高度动态、交互丰富的环境中实现可泛化的无干扰三维重建建立了强大的基线。", "summary": "DeGauss 是一个针对动态场景 3D 重建的自监督框架，它通过将动态元素和静态内容分别用前景和背景高斯模型表示，并利用概率掩码进行协调优化，实现了无干扰的 3D 重建。该方法在多种真实世界场景中表现出强大的泛化能力，并在多个基准测试中超越了现有方法，为动态环境下的无干扰 3D 重建提供了新基线。", "keywords": "动态三维重建, 高斯泼溅, 无干扰, 自监督, 动静态分解", "comments": "本文的创新之处在于其解耦的动静态高斯泼溅设计和自监督特性，使得在无需大量监督的情况下也能实现鲁棒的无干扰 3D 重建。其在各种动态场景中的出色表现突显了其潜力。"}}
{"id": "2503.13179", "pdf": "https://arxiv.org/pdf/2503.13179", "abs": "https://arxiv.org/abs/2503.13179", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin"], "title": "A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network", "categories": ["cs.CV"], "comment": null, "summary": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models.", "AI": {"title_translation": "一种基于扩展特征调制网络的轻量级建筑图像超分辨率重建方法", "tldr": "本研究提出了一种名为DCFMN的轻量级建筑图像超分辨率方法，通过整合扩张可分离调制单元和局部特征增强模块，有效解决了现有轻量级网络在建模长距离依赖方面的局限性，显著提高了重建质量和效率。", "motivation": "为了解决建筑图像中规则纹理和长距离依赖等挑战，并有效解决现有轻量级超分辨率网络在建模长距离依赖方面的局限性。", "method": "本研究提出了一种名为扩展上下文特征调制网络（DCFMN）的轻量级建筑图像超分辨率方法。该方法包括获取高分辨率图像、下采样为低分辨率、增强低分辨率图像、构建和训练轻量级网络模型以及生成超分辨率输出。DCFMN集成了扩展可分离调制单元和局部特征增强模块。前者利用多重扩张卷积高效聚合多尺度特征并结合简单注意力机制实现自适应性；后者编码局部特征，混合通道信息，并通过重参数化确保推理时无额外计算负担。", "result": "该方法在不增加计算成本的情况下，实现了准确高效的全局特征建模，并显著提高了建筑图像超分辨率模型的重建质量和轻量化效率。", "conclusion": "本研究提出的DCFMN方法有效解决了现有轻量级超分辨率网络在建模长距离依赖方面的局限性，在不增加计算成本的前提下，显著提高了建筑图像超分辨率模型的重建质量和轻量化效率。", "translation": "本研究提出了一种基于扩张上下文特征调制网络（DCFMN）的轻量级建筑图像超分辨率方法。该过程包括获取高分辨率图像、将其下采样为低分辨率、增强低分辨率图像、构建和训练轻量级网络模型，以及生成超分辨率输出。为了解决建筑图像中规则纹理和长距离依赖等挑战，DCFMN集成了扩张可分离调制单元和局部特征增强模块。前者采用多个等效于大核的扩张卷积，以高效聚合多尺度特征，同时利用简单的注意力机制实现自适应性。后者编码局部特征，混合通道信息，并通过重参数化确保在推理过程中不增加额外的计算负担。这种方法有效解决了现有轻量级超分辨率网络在建模长距离依赖方面的局限性，在不增加计算成本的情况下，实现了准确高效的全局特征建模，并显著提高了建筑图像超分辨率模型的重建质量和轻量化效率。", "summary": "本研究提出了一种名为扩张上下文特征调制网络（DCFMN）的轻量级建筑图像超分辨率重建方法。该方法通过引入扩张可分离调制单元和局部特征增强模块，有效解决了建筑图像中长距离依赖和规则纹理的建模难题。DCFMN能够高效聚合多尺度特征并增强局部特征，同时通过重参数化确保推理时无额外计算负担，从而在不增加计算成本的前提下，显著提升了建筑图像超分辨率的重建质量和模型效率。", "keywords": "超分辨率, 建筑图像, 轻量级网络, 特征调制, 长距离依赖", "comments": "该论文的创新之处在于提出了一种轻量级网络DCFMN，通过巧妙地结合扩张卷积和注意力机制来处理多尺度特征，并利用局部特征增强模块解决长距离依赖问题。其亮点在于在不增加计算成本的前提下，显著提升了模型性能和效率，这对于实际应用中的资源受限场景具有重要意义。"}}
{"id": "2503.13184", "pdf": "https://arxiv.org/pdf/2503.13184", "abs": "https://arxiv.org/abs/2503.13184", "authors": ["Yuanze Li", "Shihao Yuan", "Haolin Wang", "Qizhang Li", "Ming Liu", "Chen Xu", "Guangming Shi", "Wangmeng Zuo"], "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process", "categories": ["cs.CV"], "comment": null, "summary": "Although recent methods have tried to introduce large multimodal models\n(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD\nfield is far inferior to that for general purposes. We summarize the main\nreasons for this gap into two aspects. On one hand, general-purpose LMMs lack\ncognition of defects in the visual modality, thereby failing to sufficiently\nfocus on defect areas. Therefore, we propose to modify the AnyRes structure of\nthe LLaVA model, providing the potential anomalous areas identified by existing\nIAD models to the LMMs. On the other hand, existing methods mainly focus on\nidentifying defects by learning defect patterns or comparing with normal\nsamples, yet they fall short of understanding the causes of these defects.\nConsidering that the generation of defects is closely related to the\nmanufacturing process, we propose a manufacturing-driven IAD paradigm. An\ninstruction-tuning dataset for IAD (InstructIAD) and a data organization\napproach for Chain-of-Thought with manufacturing (CoT-M) are designed to\nleverage the manufacturing process for IAD. Based on the above two\nmodifications, we present Triad, a novel LMM-based method incorporating an\nexpert-guided region-of-interest tokenizer and manufacturing process for\nindustrial anomaly detection. Extensive experiments show that our Triad not\nonly demonstrates competitive performance against current LMMs but also\nachieves further improved accuracy when equipped with manufacturing processes.\nSource code, training data, and pre-trained models will be publicly available\nat https://github.com/tzjtatata/Triad.", "AI": {"title_translation": "Triad：通过视觉专家引导的视觉分词器和制造流程赋能基于LMM的异常检测", "tldr": "Triad是一个新的基于LMM的工业异常检测方法，通过引入专家引导的视觉分词器和制造过程信息，解决了通用LMM在工业异常检测中泛化能力差、缺乏缺陷认知和原因理解的问题，实验证明其性能优于现有LMM并能进一步提升准确性。", "motivation": "尽管大型多模态模型（LMMs）已被引入工业异常检测（IAD），但它们在该领域的泛化能力远逊于通用目的。主要原因在于通用LMM缺乏对缺陷的视觉认知，未能充分关注缺陷区域；同时，现有方法侧重识别缺陷模式或与正常样本比较，却未能理解缺陷成因。", "method": "本研究提出Triad方法。首先，修改LLaVA模型的AnyRes结构，将现有IAD模型识别的潜在异常区域提供给LMM，以增强LMM对缺陷区域的关注。其次，提出一种制造驱动的IAD范式，设计了用于IAD的指令微调数据集（InstructIAD）和结合制造过程的思维链数据组织方法（CoT-M），以利用制造过程信息理解缺陷成因。", "result": "Triad不仅展示了与当前LMMs相比具有竞争力的性能，而且在结合制造过程后，其准确性得到了进一步提升。", "conclusion": "Triad通过引入专家引导的视觉分词器和制造过程信息，显著提升了大型多模态模型在工业异常检测领域的泛化能力和缺陷理解能力。", "translation": "尽管最近的方法尝试将大型多模态模型（LMMs）引入工业异常检测（IAD），但它们在IAD领域的泛化能力远不如通用目的。我们将这种差距的主要原因总结为两个方面。一方面，通用LMM在视觉模态中缺乏对缺陷的认知，从而未能充分关注缺陷区域。因此，我们建议修改LLaVA模型的AnyRes结构，将现有IAD模型识别的潜在异常区域提供给LMM。另一方面，现有方法主要通过学习缺陷模式或与正常样本比较来识别缺陷，但它们未能理解这些缺陷的成因。考虑到缺陷的产生与制造过程密切相关，我们提出了一种制造驱动的IAD范式。设计了用于IAD的指令微调数据集（InstructIAD）和结合制造过程的思维链数据组织方法（CoT-M），以利用制造过程进行IAD。基于上述两项修改，我们提出了Triad，这是一种新颖的基于LMM的方法，结合了专家引导的感兴趣区域分词器和制造过程，用于工业异常检测。大量实验表明，我们的Triad不仅表现出与当前LMMs竞争的性能，而且在配备制造过程后，准确性得到了进一步提高。源代码、训练数据和预训练模型将公开在https://github.com/tzjtatata/Triad。", "summary": "本论文提出了Triad，一种基于大型多模态模型（LMM）的工业异常检测（IAD）新方法。针对通用LMM在IAD中泛化能力差和缺乏缺陷认知的问题，Triad通过修改LLaVA模型的AnyRes结构，引入专家引导的视觉分词器来聚焦潜在异常区域。此外，为解决现有方法未能理解缺陷成因的问题，Triad提出了一种制造驱动的IAD范式，并设计了InstructIAD数据集和CoT-M数据组织方法，以利用制造过程信息。实验证明，Triad不仅性能优异，结合制造过程后准确性进一步提升。", "keywords": "工业异常检测, 大型多模态模型, 视觉分词器, 制造过程, Triad", "comments": "Triad的创新点在于结合了视觉专家引导的分词器和制造过程知识，解决了LMM在工业异常检测中特异性不足和缺乏深层原因理解的痛点。这种结合多模态信息（视觉和过程知识）的方法，有望为工业领域的高精度异常检测提供新的思路和范式，具有重要的应用价值。其公开代码和数据也利于后续研究。"}}
{"id": "2503.13185", "pdf": "https://arxiv.org/pdf/2503.13185", "abs": "https://arxiv.org/abs/2503.13185", "authors": ["Dingning Liu", "Cheng Wang", "Peng Gao", "Renrui Zhang", "Xinzhu Ma", "Yuan Meng", "Zhihui Wang"], "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt\nengineering techniques.", "AI": {"title_translation": "3DAxisPrompt：促进GPT-4o中的3D定位与推理", "tldr": "引入3DAxisPrompt，一种新的视觉提示方法，通过3D坐标轴和SAM掩码提升多模态大语言模型（MLLMs）在真实世界场景中的3D理解和定位能力。", "motivation": "现有研究主要关注多模态大语言模型（MLLMs）的逻辑推理和视觉理解，但其在3D视觉方面的能力仍是持续探索的领域。", "method": "本文提出了一种名为3DAxisPrompt的新型视觉提示方法，旨在激发MLLMs在真实世界场景中的3D理解能力。该方法利用3D坐标轴和从Segment Anything Model (SAM) 生成的掩码，为MLLMs提供明确的几何先验知识，从而将其2D定位和推理能力扩展到真实世界的3D场景。此外，研究还对潜在的视觉提示格式进行了深入调查，并构建了包含ScanRefer、ScanNet、FMB和nuScene四个数据集的评估环境。", "result": "实验证明所提出的3DAxisPrompt方法是有效的，能够帮助多模态大语言模型有效感知真实世界场景中物体的3D位置。然而，研究也发现单一的提示工程方法并非对所有3D任务都能持续达到最佳效果。", "conclusion": "本研究揭示了利用提示工程技术，多模态大语言模型在3D视觉定位和推理方面的可行性，并证实了3DAxisPrompt能有效提升其3D感知能力。", "translation": "多模态大语言模型（MLLMs）在各种任务中展现出令人印象深刻的能力，尤其是在配备精心设计的视觉提示时。然而，现有研究主要集中在逻辑推理和视觉理解上，而MLLMs在3D视觉中有效运作的能力仍是一个持续探索的领域。在本文中，我们引入了一种新颖的视觉提示方法，称为3DAxisPrompt，以激发MLLMs在真实世界场景中的3D理解能力。更具体地说，我们的方法利用3D坐标轴和从Segment Anything Model (SAM) 生成的掩码，为MLLMs提供明确的几何先验知识，从而将其令人印象深刻的2D定位和推理能力扩展到真实世界的3D场景。此外，我们首次对潜在的视觉提示格式进行了彻底的调查，并总结了我们的发现，以揭示GPT-4o（作为MLLMs的代表）3D理解能力的潜力和局限。最后，我们构建了包含ScanRefer、ScanNet、FMB和nuScene数据集的评估环境，涵盖了各种3D任务。在此基础上，我们进行了广泛的定量和定性实验，证明了所提出方法的有效性。总的来说，我们的研究表明，在3DAxisPrompt的帮助下，MLLMs可以有效地感知真实世界场景中物体的3D位置。然而，单一的提示工程方法并不能始终在所有3D任务中达到最佳结果。这项研究强调了利用提示工程技术实现MLLMs进行3D视觉定位/推理的可行性。", "summary": "本文提出了一种名为3DAxisPrompt的新型视觉提示方法，旨在提升多模态大语言模型（MLLMs）在真实世界场景中的3D理解和定位能力。该方法通过结合3D坐标轴和SAM生成的掩码，为MLLMs提供几何先验知识，从而将其2D能力扩展到3D领域。研究通过对GPT-4o的广泛实验，并在ScanRefer等四个数据集上进行评估，证明了3DAxisPrompt的有效性，表明MLLMs能够感知物体的3D位置，同时也指出了单一提示工程方法的局限性，强调了利用MLLMs进行3D视觉定位和推理的可行性。", "keywords": "3D视觉, 多模态大语言模型, 视觉提示, GPT-4o, 3D定位", "comments": "这项研究通过引入3DAxisPrompt，创新性地解决了多模态大语言模型在3D视觉理解方面的不足，特别是在真实世界场景中的3D定位和推理。其结合3D坐标轴和SAM掩码的方法为MLLMs提供了关键的几何先验，有效弥补了现有MLLMs在3D空间感知上的空白。研究不仅展示了GPT-4o在3D理解方面的潜力，也坦诚地指出了单一提示工程方法的局限性，为未来的研究方向提供了宝贵的见解。"}}
{"id": "2503.13188", "pdf": "https://arxiv.org/pdf/2503.13188", "abs": "https://arxiv.org/abs/2503.13188", "authors": ["Matteo Sodano", "Federico Magistri", "Elias Marks", "Fares Hosn", "Aibek Zurbayev", "Rodrigo Marcuzzi", "Meher V. R. Malladi", "Jens Behley", "Cyrill Stachniss"], "title": "3D Hierarchical Panoptic Segmentation in Real Orchard Environments Across Different Sensors", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to IROS", "summary": "Crop yield estimation is a relevant problem in agriculture, because an\naccurate crop yield estimate can support farmers' decisions on harvesting or\nprecision intervention. Robots can help to automate this process. To do so,\nthey need to be able to perceive the surrounding environment to identify target\nobjects. In this paper, we introduce a novel approach to address the problem of\nhierarchical panoptic segmentation of apple orchards on 3D data from different\nsensors. Our approach is able to simultaneously provide semantic segmentation,\ninstance segmentation of trunks and fruits, and instance segmentation of plants\n(a single trunk with its fruits). This allows us to identify relevant\ninformation such as individual plants, fruits, and trunks, and capture the\nrelationship among them, such as precisely estimate the number of fruits\nassociated to each tree in an orchard. Additionally, to efficiently evaluate\nour approach for hierarchical panoptic segmentation, we provide a dataset\ndesigned specifically for this task. Our dataset is recorded in Bonn in a real\napple orchard with a variety of sensors, spanning from a terrestrial laser\nscanner to a RGB-D camera mounted on different robotic platforms. The\nexperiments show that our approach surpasses state-of-the-art approaches in 3D\npanoptic segmentation in the agricultural domain, while also providing full\nhierarchical panoptic segmentation. Our dataset has been made publicly\navailable at https://www.ipb.uni-bonn.de/data/hops/. We will provide the\nopen-source implementation of our approach and public competiton for\nhierarchical panoptic segmentation on the hidden test sets upon paper\nacceptance.", "AI": {"title_translation": "在真实果园环境中跨不同传感器的3D分层全景分割", "tldr": "本文提出一种在真实果园环境中，针对不同传感器3D数据进行分层全景分割的新方法，可同时提供语义分割、树干和果实实例分割以及植物实例分割，并超越现有技术。", "motivation": "提高农作物产量估计的准确性，支持农民决策；使机器人能够感知周围环境，识别目标对象以自动化农业过程。", "method": "本文提出一种新的方法，用于处理来自不同传感器的3D数据，对苹果园进行分层全景分割。该方法能够同时提供语义分割、树干和果实的实例分割以及植物（单个树干及其果实）的实例分割。此外，还提供了一个专门为此任务设计的数据集，该数据集在真实苹果园中用多种传感器（如地面激光扫描仪和RGB-D相机）记录。", "result": "实验表明，该方法在农业领域的3D全景分割方面超越了最先进的方法，同时提供了完整的分层全景分割。", "conclusion": "该方法成功实现了在真实果园环境中跨不同传感器的3D分层全景分割，并能准确识别单个植物、果实和树干，捕捉它们之间的关系，例如精确估算每棵树的果实数量，为农业机器人感知提供了有效方案。", "translation": "农作物产量估算是农业中的一个相关问题，因为准确的农作物产量估算可以支持农民的收割决策或精准干预。机器人可以帮助自动化这一过程。为此，它们需要能够感知周围环境以识别目标对象。在本文中，我们介绍了一种新方法，用于解决来自不同传感器的3D数据在苹果园中进行分层全景分割的问题。我们的方法能够同时提供语义分割、树干和果实的实例分割，以及植物（单个树干及其果实）的实例分割。这使我们能够识别相关信息，例如单个植物、果实和树干，并捕获它们之间的关系，例如精确估算果园中每棵树的果实数量。此外，为了有效地评估我们的分层全景分割方法，我们提供了一个专门为此任务设计的数据集。我们的数据集是在波恩的一个真实苹果园中，使用各种传感器记录的，包括从地面激光扫描仪到安装在不同机器人平台上的RGB-D相机。实验表明，我们的方法在农业领域的3D全景分割方面超越了最先进的方法，同时还提供了完整的分层全景分割。我们的数据集已在 https://www.ipb.uni-bonn.de/data/hops/ 公开。我们将在论文被接受后提供我们方法的开源实现，并对隐藏测试集上的分层全景分割进行公开竞赛。", "summary": "本文提出一种在真实果园环境中，针对来自不同传感器的3D数据进行分层全景分割的新方法。该方法能够同时进行语义分割、树干和果实的实例分割以及植物实例分割，从而精确估计每棵树的果实数量。为评估此方法，作者还构建了一个包含多种传感器数据的真实果园数据集。实验结果表明，该方法在农业3D全景分割领域优于现有技术，并提供了完整的分层全景信息。", "keywords": "3D分层全景分割, 果园环境, 农作物产量估计, 机器人感知, 数据集", "comments": "该论文提出了一种创新的3D分层全景分割方法，解决了农业机器人精确感知果园环境的关键问题。其亮点在于能够同时进行语义和多层实例分割，并捕捉植物内部的层次关系。此外，公开高质量的真实果园3D数据集，极大地促进了该领域的研究和发展。"}}
{"id": "2503.13203", "pdf": "https://arxiv.org/pdf/2503.13203", "abs": "https://arxiv.org/abs/2503.13203", "authors": ["Corentin Sautier", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Vincent Lepetit"], "title": "Clustering is back: Reaching state-of-the-art LiDAR instance segmentation without training", "categories": ["cs.CV"], "comment": null, "summary": "Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\nachieves performance comparable to current state-of-the-art supervised methods\non standard benchmarks including SemanticKITTI and nuScenes, and outperforms\nevery publicly available method on SemanticKITTI as a drop-in instance head\nreplacement, while running in real-time on a single-threaded CPU and requiring\nno instance labels. Our method is fully explainable, and requires no learning\nor parameter tuning. Code is available at https://github.com/valeoai/Alpine/", "AI": {"title_translation": "聚类回归：无需训练即可实现最先进的激光雷达实例分割", "tldr": "本文提出了一种无需训练和实例标注的激光雷达全景分割方法，性能媲美现有SOTA监督方法。", "motivation": "激光雷达点云的全景分割对于室外场景理解至关重要，但现有方法依赖于昂贵的深度学习架构和大量手动实例标注，这是该领域的主要瓶颈。", "method": "作者证明了仅使用语义标签即可实现具有竞争力的全景分割，实例预测无需任何训练或标注。该方法无需学习或参数调整，且完全可解释。", "result": "该方法在SemanticKITTI和nuScenes等标准基准测试上取得了与当前最先进的监督方法相当的性能，并且在SemanticKITTI上作为实例头替代方案时，优于所有公开可用方法，同时在单线程CPU上实时运行，且无需实例标签。", "conclusion": "通过无需训练和实例标注的聚类方法，可以实现与SOTA监督方法相当的激光雷达实例分割性能，显著降低了数据标注成本和时间投入。", "translation": "激光雷达点云的全景分割对于室外场景理解至关重要，自动驾驶是其主要应用。虽然最先进的方法通常依赖于端到端深度学习架构和大量的实例手动标注，但标记大规模点云数据集所需的巨大成本和时间投入仍然是该领域的主要瓶颈。在这项工作中，我们证明了仅使用语义标签即可实现具有竞争力的全景分割，实例预测无需任何训练或标注。我们的方法在SemanticKITTI和nuScenes等标准基准测试上取得了与当前最先进的监督方法相当的性能，并且在SemanticKITTI上作为即插即用实例头替代方案时，优于所有公开可用方法，同时在单线程CPU上实时运行，且无需实例标签。我们的方法完全可解释，无需学习或参数调整。代码可在 https://github.com/valeoai/Alpine/ 获取。", "summary": "这篇论文提出了一种无需训练和实例标注的激光雷达点云全景分割方法。该方法仅依赖于语义标签，便能在SemanticKITTI和nuScenes等基准测试上达到与当前最先进的监督方法相当的性能，甚至在某些情况下表现更优。其核心优势在于无需昂贵的数据标注、实时运行且完全可解释，解决了深度学习方法在点云分割中对大量标注的依赖问题。", "keywords": "激光雷达，实例分割，全景分割，无训练，聚类", "comments": "本文的创新点在于证明了传统聚类方法在激光雷达实例分割中仍具有强大潜力，尤其是在无需训练和实例标注的情况下达到SOTA性能，这极大地降低了部署成本和数据依赖。其可解释性和实时性也使其在实际应用中更具吸引力。这项工作为解决大规模点云标注瓶颈提供了一个有前景的方向。"}}
{"id": "2503.13211", "pdf": "https://arxiv.org/pdf/2503.13211", "abs": "https://arxiv.org/abs/2503.13211", "authors": ["Marvin Seyfarth", "Salman Ul Hassan Dar", "Isabelle Ayx", "Matthias Alexander Fink", "Stefan O. Schoenberg", "Hans-Ulrich Kauczor", "Sandy Engelhardt"], "title": "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.", "AI": {"title_translation": "MedLoRD：一种用于高分辨率三维CT图像合成的医学低资源扩散模型", "tldr": "MedLoRD是一种为计算资源受限环境设计的生成扩散模型，能够使用标准桌面工作站中常见的24GB VRAM GPU生成高分辨率三维医学图像，并优于现有最先进的模型。", "motivation": "AI在医学成像中的应用受限于数据可用性不足和患者隐私问题导致的数据共享意愿低。生成模型虽有潜力，但现有最先进的方法对计算资源要求高，不适用于资源受限的医疗环境，且依赖数据子采样。此外，仅依赖定量指标评估图像质量和临床意义具有误导性。", "method": "我们引入了MedLoRD，一种生成扩散模型，专为计算资源受限的环境设计。MedLoRD能够生成分辨率高达512x512x256的高维医学体，仅使用24GB VRAM的GPU即可运行。模型在冠状动脉CT血管造影和肺部CT数据集等多种模态上进行了评估。", "result": "通过放射学评估、相对区域体积分析、条件掩模依从性以及下游任务的广泛评估表明，MedLoRD生成的图像具有高保真度，并严格遵循分割掩模条件，在计算资源受限的环境下，其性能超越了当前最先进的医学图像合成生成模型。", "conclusion": "MedLoRD成功解决了在计算资源受限环境下进行高分辨率三维医学图像合成的挑战，并展现出优于现有最先进模型的性能，为生成高质量合成医疗数据提供了可行方案。", "translation": "人工智能在医学成像领域的进步具有巨大潜力。然而，其应用受到数据可用性有限以及医疗中心因患者隐私问题不愿共享数据的限制。生成模型通过创建合成数据作为真实患者数据的替代品，提供了一个有前景的解决方案。然而，医学图像通常是高维的，并且当前的最新方法对于计算资源受限的医疗环境来说往往不切实际。这些模型依赖于数据子采样，这对其可行性和实际应用性提出了质疑。此外，许多此类模型仅通过定量指标进行评估，这在评估生成图像的图像质量和临床意义方面可能具有误导性。为了解决这个问题，我们引入了MedLoRD，一个为计算资源受限环境设计的生成扩散模型。MedLoRD能够生成分辨率高达512×512×256的高维医学体积，仅需使用24GB VRAM的GPU，这种GPU常见于标准桌面工作站。MedLoRD在多种模态上进行了评估，包括冠状动脉计算机断层扫描血管造影和肺部计算机断层扫描数据集。通过放射学评估、相对区域体积分析、条件掩模依从性以及下游任务的广泛评估表明，MedLoRD生成的图像具有高保真度，并严格遵循分割掩模条件，在计算资源受限的环境下，其性能超越了当前最先进的医学图像合成生成模型。", "summary": "本研究提出了MedLoRD，一个专为计算资源受限的医疗环境设计的生成扩散模型。针对医学图像数据稀缺、隐私问题以及现有高维图像生成模型计算成本高的问题，MedLoRD能在仅有24GB VRAM的GPU上生成高达512x512x256分辨率的高维医学体。通过在多种CT数据集上的广泛评估，包括放射学评估和下游任务，结果显示MedLoRD生成了高保真度且严格遵循分割掩模条件的图像，性能优于当前最先进的模型，证明了其在低资源环境下合成高质量医学图像的有效性。", "keywords": "医学成像, 扩散模型, 低资源, 三维CT合成, 生成模型", "comments": "MedLoRD的创新之处在于它成功地将高分辨率三维医学图像合成的能力带入了计算资源受限的环境，这对于普及AI在医疗影像领域的应用具有重要意义。通过优化模型使其能在标准桌面工作站上运行，该研究降低了技术门槛，使得更多医疗机构和研究者能够利用生成模型解决数据稀缺和隐私保护问题。其在多模态数据上的广泛评估和对临床相关指标的关注也增加了其在实际应用中的潜力。"}}
{"id": "2503.13214", "pdf": "https://arxiv.org/pdf/2503.13214", "abs": "https://arxiv.org/abs/2503.13214", "authors": ["Jie Huang", "Haorui Chen", "Jiaxuan Ren", "Siran Peng", "Liangjian Deng"], "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": "This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025", "summary": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.", "AI": {"title_translation": "遥感全色锐化的通用自适应双层加权机制", "tldr": "提出一种通用自适应双层加权机制(ADWM)，通过处理特征异质性和冗余性，提升现有深度学习遥感全色锐化方法的效果。", "motivation": "现有的深度学习遥感全色锐化方法未能充分利用特征异质性和冗余性，限制了其有效性。", "method": "提出基于协方差矩阵的关联感知协方差加权(CACW)来调整特征异质性和冗余性。在此基础上，引入通用自适应双层加权机制(ADWM)，包含内部特征加权(IFW)以减少冗余和增强信息，以及跨特征加权(CFW)以调整层间贡献。", "result": "广泛的实验表明，ADWM的性能优于最新的SOTA方法，并通过通用性实验、冗余可视化、比较实验、关键变量和复杂度分析以及消融研究验证了其有效性。", "conclusion": "ADWM通过有效的双层加权机制，成功解决了现有遥感全色锐化方法中特征异质性和冗余性利用不足的问题，显著提升了性能。", "translation": "目前，基于深度学习的遥感全色锐化方法发展迅速。然而，许多现有方法难以充分利用特征异质性和冗余性，从而限制了它们的有效性。我们使用协方差矩阵来建模特征异质性和冗余性，并提出关联感知协方差加权（CACW）来调整它们。CACW通过协方差矩阵捕获这些相关性，然后通过非线性函数处理以生成用于调整的权重。在CACW的基础上，我们引入了一种通用自适应双层加权机制（ADWM），从两个关键角度解决这些挑战，从而增强了广泛的现有深度学习方法。首先，内部特征加权（IFW）评估每个特征内部通道之间的相关性，以减少冗余并增强独特信息。其次，跨特征加权（CFW）根据层间相关性调整跨层的贡献，从而优化最终输出。广泛的实验表明，与最新的最先进（SOTA）方法相比，ADWM表现出卓越的性能。此外，我们通过通用性实验、冗余可视化、比较实验、关键变量和复杂性分析以及消融研究验证了我们方法的有效性。我们的代码可在https://github.com/Jie-1203/ADWM获取。", "summary": "本文针对现有深度学习遥感全色锐化方法在利用特征异质性和冗余性方面的不足，提出了一种通用自适应双层加权机制（ADWM）。该机制首先通过关联感知协方差加权（CACW）基于协方差矩阵调整特征相关性，随后通过内部特征加权（IFW）和跨特征加权（CFW）在特征内部和层间两个层面进行自适应加权。实验证明ADWM显著提升了全色锐化性能，并具有良好的通用性。", "keywords": "遥感全色锐化, 深度学习, 自适应加权, 特征异质性, 冗余性", "comments": "该论文提出了一种新颖的自适应双层加权机制ADWM，通过引入协方差矩阵来精确建模和调整特征的异质性与冗余性，这在深度学习全色锐化领域具有创新性。其双层加权（IFW和CFW）设计思路清晰且有效，能够提升现有多种深度学习方法的性能，显示出良好的通用性和实用价值。"}}
{"id": "2503.13229", "pdf": "https://arxiv.org/pdf/2503.13229", "abs": "https://arxiv.org/abs/2503.13229", "authors": ["Yongkang Cheng", "Shaoli Huang"], "title": "HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures", "categories": ["cs.CV"], "comment": "Accepted by 3DV 2025", "summary": "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/.", "AI": {"title_translation": "HoloGest：用于生成整体表达性协同语音手势的解耦扩散和运动先验", "tldr": "HoloGest通过解耦扩散和运动先验，生成高质量、富有表现力的协同语音手势，克服了传统方法手势不自然的问题。", "motivation": "现有系统主要关注音频和手势之间的弱相关性，导致生成的手势物理上不自然，从而降低了用户体验。", "method": "引入HoloGest，一个基于解耦扩散和运动先验的新型神经网络框架。该系统利用大规模人体运动数据集学习具有低音频依赖和高运动依赖的鲁棒先验，以实现稳定的全局运动和详细的手指运动。为提高扩散模型的生成效率，集成了隐式关节约束与显式几何和条件约束。此外，设计了一个用于手势-转录文本对齐的共享嵌入空间，以生成语义正确的手势动作。", "result": "广泛的实验和用户反馈表明，该模型有效且具有潜在应用，其方法达到了接近真实数据的真实感水平，提供了沉浸式用户体验。", "conclusion": "HoloGest通过结合解耦扩散和运动先验，成功解决了协同语音手势生成中自然度不足的问题，能够生成高质量、语义正确且逼真的手势，显著提升了用户体验。", "translation": "动画化具有整体协同语音手势的虚拟角色是一项具有挑战性但至关重要的任务。以前的系统主要关注音频和手势之间的弱相关性，导致物理上不自然的结果，从而降低了用户体验。为了解决这个问题，我们引入了HoloGest，一个基于解耦扩散和运动先验的新型神经网络框架，用于自动生成高质量、富有表现力的协同语音手势。我们的系统利用大规模人体运动数据集学习一个具有低音频依赖和高运动依赖的鲁棒先验，从而实现稳定的全局运动和详细的手指运动。为了提高基于扩散模型的生成效率，我们将隐式关节约束与显式几何和条件约束相结合，捕获大步幅之间复杂的运动分布。这种集成显著提高了生成速度，同时保持了高质量的运动。此外，我们设计了一个用于手势-转录文本对齐的共享嵌入空间，从而能够生成语义正确的手势动作。广泛的实验和用户反馈证明了我们模型的有效性和潜在应用，我们的方法达到了接近真实数据的真实感水平，提供了沉浸式用户体验。我们的代码、模型和演示可在https://cyk990422.github.io/HoloGest.github.io/获取。", "summary": "本文介绍了HoloGest，一个用于生成高质量、富有表现力的协同语音手势的新型神经网络框架。该框架通过解耦扩散和运动先验，克服了传统方法生成手势不自然的问题。HoloGest利用大规模人体运动数据集学习鲁棒的运动先验，并结合隐式和显式约束来提高生成效率和运动质量。此外，它还设计了一个共享嵌入空间以实现语义正确的手势生成。实验和用户反馈证明了HoloGest能够生成接近真实水平的逼真手势。", "keywords": "协同语音手势, 扩散模型, 运动先验, 虚拟角色动画, 手势生成", "comments": "HoloGest的创新点在于其解耦扩散和运动先验的方法，这有效解决了传统模型中音频与手势弱相关导致手势不自然的问题。通过强调运动本身的鲁棒性，并结合多种约束和语义对齐，显著提升了生成手势的自然度和表达力，为虚拟角色动画和人机交互领域带来了重要进展。"}}
{"id": "2503.13241", "pdf": "https://arxiv.org/pdf/2503.13241", "abs": "https://arxiv.org/abs/2503.13241", "authors": ["Zhifu Tian", "Tao Hu", "Chaoyang Niu", "Di Wu", "Shu Wang"], "title": "Sampling Innovation-Based Adaptive Compressive Sensing", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR2025 accepted", "summary": "Scene-aware Adaptive Compressive Sensing (ACS) has attracted significant\ninterest due to its promising capability for efficient and high-fidelity\nacquisition of scene images. ACS typically prescribes adaptive sampling\nallocation (ASA) based on previous samples in the absence of ground truth.\nHowever, when confronting unknown scenes, existing ACS methods often lack\naccurate judgment and robust feedback mechanisms for ASA, thus limiting the\nhigh-fidelity sensing of the scene. In this paper, we introduce a Sampling\nInnovation-Based ACS (SIB-ACS) method that can effectively identify and\nallocate sampling to challenging image reconstruction areas, culminating in\nhigh-fidelity image reconstruction. An innovation criterion is proposed to\njudge ASA by predicting the decrease in image reconstruction error attributable\nto sampling increments, thereby directing more samples towards regions where\nthe reconstruction error diminishes significantly. A sampling innovation-guided\nmulti-stage adaptive sampling (AS) framework is proposed, which iteratively\nrefines the ASA through a multi-stage feedback process. For image\nreconstruction, we propose a Principal Component Compressed Domain Network\n(PCCD-Net), which efficiently and faithfully reconstructs images under AS\nscenarios. Extensive experiments demonstrate that the proposed SIB-ACS method\nsignificantly outperforms the state-of-the-art methods in terms of image\nreconstruction fidelity and visual effects. Codes are available at\nhttps://github.com/giant-pandada/SIB-ACS_CVPR2025.", "AI": {"title_translation": "基于采样创新的自适应压缩感知", "tldr": "提出了一种基于采样创新的自适应压缩感知（SIB-ACS）方法，通过预测重建误差的减少来智能分配采样，并结合新的重建网络，显著提高了图像重建质量。", "motivation": "现有的场景感知自适应压缩感知（ACS）方法在面对未知场景时，缺乏对自适应采样分配（ASA）的准确判断和鲁棒反馈机制，从而限制了场景的高保真感知。", "method": "提出了一种基于采样创新的自适应压缩感知（SIB-ACS）方法。该方法引入了创新准则，通过预测采样增量引起的图像重建误差降低来判断自适应采样分配（ASA），从而将更多样本导向重建误差显著减小的区域。同时，提出采样创新引导的多阶段自适应采样（AS）框架，通过多阶段反馈过程迭代细化ASA。此外，还提出主成分压缩域网络（PCCD-Net）用于图像重建，该网络在AS场景下能高效且忠实地重建图像。", "result": "提出的SIB-ACS方法在图像重建保真度和视觉效果方面显著优于现有最先进的方法。", "conclusion": "SIB-ACS方法通过引入采样创新准则和多阶段自适应采样框架，并结合新的PCCD-Net重建网络，有效解决了现有ACS方法在未知场景下采样分配的不足，实现了高保真图像重建。", "translation": "场景感知自适应压缩感知（ACS）因其高效、高保真地获取场景图像的潜力而引起了广泛关注。ACS通常在缺乏真实值的情况下，根据先前的样本来规定自适应采样分配（ASA）。然而，在面对未知场景时，现有ACS方法往往缺乏对ASA的准确判断和鲁棒反馈机制，从而限制了场景的高保真感知。在本文中，我们引入了一种基于采样创新的自适应压缩感知（SIB-ACS）方法，该方法可以有效地识别并为具有挑战性的图像重建区域分配采样，最终实现高保真图像重建。我们提出了一种创新准则，通过预测采样增量引起的图像重建误差的减少来判断ASA，从而将更多样本导向重建误差显著减小的区域。我们提出了一种采样创新引导的多阶段自适应采样（AS）框架，该框架通过多阶段反馈过程迭代细化ASA。对于图像重建，我们提出了一种主成分压缩域网络（PCCD-Net），该网络在AS场景下能高效且忠实地重建图像。大量的实验表明，所提出的SIB-ACS方法在图像重建保真度和视觉效果方面显著优于现有最先进的方法。代码可在https://github.com/giant-pandada/SIB-ACS_CVPR2025获取。", "summary": "本文提出了一种名为基于采样创新的自适应压缩感知（SIB-ACS）的新方法，旨在解决现有ACS方法在未知场景下自适应采样分配（ASA）判断和反馈不足的问题。SIB-ACS引入了一种创新准则，根据采样对重建误差的减少程度来智能分配采样，并通过一个多阶段自适应采样框架迭代优化采样分配。此外，还提出了一个主成分压缩域网络（PCCD-Net）以高效地重建图像。实验证明，SIB-ACS在图像重建保真度和视觉效果上显著超越了现有技术。", "keywords": "自适应压缩感知, 采样创新, 图像重建, 自适应采样分配, PCCD-Net", "comments": "这篇论文通过引入“采样创新”这一核心概念，为自适应压缩感知提供了一个新的视角，解决了传统方法在未知场景下采样分配不准确的问题。其创新点在于通过预测重建误差的减少来指导采样，并结合多阶段反馈机制，增强了系统的鲁棒性。同时，提出的PCCD-Net也为高效重建提供了支持。这项工作对于提高压缩感知在实际应用中的性能具有重要意义。"}}
{"id": "2503.13260", "pdf": "https://arxiv.org/pdf/2503.13260", "abs": "https://arxiv.org/abs/2503.13260", "authors": ["Amit Zalcher", "Navve Wasserman", "Roman Beliy", "Oliver Heinimann", "Michal Irani"], "title": "Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual perceptual tasks aim to predict human judgment of images (e.g.,\nemotions invoked by images, image quality assessment). Unlike objective tasks\nsuch as object/scene recognition, perceptual tasks rely on subjective human\nassessments, making its data-labeling difficult. The scarcity of such\nhuman-annotated data results in small datasets leading to poor generalization.\nTypically, specialized models were designed for each perceptual task, tailored\nto its unique characteristics and its own training dataset. We propose a\nunified architectural framework for solving multiple different perceptual tasks\nleveraging CLIP as a prior. Our approach is based on recent cognitive findings\nwhich indicate that CLIP correlates well with human judgment. While CLIP was\nexplicitly trained to align images and text, it implicitly also learned human\ninclinations. We attribute this to the inclusion of human-written image\ncaptions in CLIP's training data, which contain not only factual image\ndescriptions, but inevitably also human sentiments and emotions. This makes\nCLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest\nthat minimal adaptation of CLIP suffices for solving a variety of perceptual\ntasks. Our simple unified framework employs a lightweight adaptation to\nfine-tune CLIP to each task, without requiring any task-specific architectural\nchanges. We evaluate our approach on three tasks: (i) Image Memorability\nPrediction, (ii) No-reference Image Quality Assessment, and (iii) Visual\nEmotion Analysis. Our model achieves state-of-the-art results on all three\ntasks, while demonstrating improved generalization across different datasets.", "AI": {"title_translation": "先别急着评判，先用CLIP：一种统一的感知任务方法", "tldr": "本文提出了一种统一的框架，利用CLIP作为先验知识，通过轻量级微调来解决多种视觉感知任务，并在图像记忆性预测、无参考图像质量评估和视觉情感分析等任务上取得了最先进的结果，同时改善了泛化能力。", "motivation": "视觉感知任务（如图像情感、图像质量评估）依赖于主观的人类判断，导致数据标注困难、数据集稀缺、泛化能力差，且通常需要为每个任务设计专门的模型。本文旨在解决这些问题，提出一个统一的方法。", "method": "本文提出了一种统一的架构框架，利用CLIP作为先验知识来解决多个不同的感知任务。该方法基于认知发现，即CLIP与人类判断高度相关。通过对CLIP进行轻量级适配和微调，无需任何任务特定的架构更改，即可应用于不同的感知任务。", "result": "该模型在图像记忆性预测、无参考图像质量评估和视觉情感分析这三个任务上均取得了最先进的结果，并展示了跨不同数据集的改进泛化能力。", "conclusion": "通过对CLIP进行最小化适配，可以有效解决多种视觉感知任务，证明了CLIP作为感知任务强大先验知识的潜力，并提供了一个统一且高效的解决方案。", "translation": "视觉感知任务旨在预测人类对图像的判断（例如，图像唤起的情绪、图像质量评估）。与目标/场景识别等客观任务不同，感知任务依赖于主观的人类评估，这使得数据标注变得困难。此类人工标注数据的稀缺性导致数据集较小，从而泛化能力差。通常，会为每个感知任务设计专门的模型，以适应其独特的特性和自身的训练数据集。\n我们提出了一种统一的架构框架，利用CLIP作为先验知识来解决多个不同的感知任务。我们的方法基于最近的认知发现，这些发现表明CLIP与人类判断高度相关。虽然CLIP明确地训练用于对齐图像和文本，但它也隐含地学习了人类的倾向。我们将这归因于CLIP训练数据中包含了人类编写的图像标题，这些标题不仅包含事实性的图像描述，还不可避免地包含人类的情绪和情感。这使得CLIP成为感知任务特别强大的先验知识。因此，我们认为对CLIP进行最小化适配就足以解决各种感知任务。我们简单的统一框架采用轻量级适配来对CLIP进行每个任务的微调，而无需任何任务特定的架构更改。我们在三个任务上评估了我们的方法：(i) 图像记忆性预测，(ii) 无参考图像质量评估，和 (iii) 视觉情感分析。我们的模型在这三个任务上均取得了最先进的结果，同时展示了跨不同数据集的改进泛化能力。", "summary": "本文针对视觉感知任务中数据标注困难、数据集稀缺以及模型泛化能力差的问题，提出了一种统一的框架。该框架利用CLIP作为强大的先验知识，通过对其进行轻量级微调，无需任务特定架构更改，即可有效解决图像记忆性预测、无参考图像质量评估和视觉情感分析等多项感知任务。实验结果表明，该方法在所有评估任务上均取得了最先进的性能，并显著提升了模型的泛化能力。", "keywords": "CLIP, 感知任务, 统一方法, 图像质量评估, 情感分析", "comments": "本文提出了一种创新性的方法，通过利用CLIP模型的内在能力来解决视觉感知任务中的核心挑战。其创新点在于认识到CLIP训练数据中包含的人类情感和倾向，使其成为感知任务的强大先验。这种统一的、轻量级适配的方法极大地简化了模型设计，并提高了泛化能力，对于未来感知任务的研究具有重要意义。"}}
{"id": "2503.13265", "pdf": "https://arxiv.org/pdf/2503.13265", "abs": "https://arxiv.org/abs/2503.13265", "authors": ["Luxi Chen", "Zihan Zhou", "Min Zhao", "Yikai Wang", "Ge Zhang", "Wenhao Huang", "Hao Sun", "Ji-Rong Wen", "Chongxuan Li"], "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.", "AI": {"title_translation": "FlexWorld：逐步扩展3D场景以实现灵活视角合成", "tldr": "FlexWorld是一个新颖的框架，通过视频到视频扩散模型和渐进式扩展过程，从单张图像生成高质量的灵活视角3D场景，包括360度旋转和缩放。", "motivation": "从单张图像生成包括360度旋转和缩放的灵活视角3D场景具有挑战性，因为缺乏3D数据。", "method": "FlexWorld包含两个关键组件：(1) 一个强大的视频到视频(V2V)扩散模型，用于从粗糙场景渲染的不完整输入中生成高质量的新颖视图图像；(2) 一个渐进式扩展过程，通过几何感知的场景融合将新的3D内容整合到全局场景中。该V2V模型利用先进的预训练视频模型和精确的深度估计训练对，能够在大相机姿态变化下生成新颖视图。", "result": "实验证明FlexWorld能有效生成高质量的新颖视图视频和灵活视角3D场景。与现有最先进方法相比，它在多个流行指标和数据集上实现了卓越的视觉质量。FlexWorld能够生成具有360度旋转和缩放等灵活视角的高保真场景。", "conclusion": "FlexWorld成功解决了从单张图像生成灵活视角3D场景的挑战，通过其创新的V2V扩散模型和渐进式扩展过程，生成了高质量、高保真的3D场景。", "translation": "从单张图像生成灵活视角3D场景，包括360度旋转和缩放，由于缺乏3D数据而具有挑战性。为此，我们引入了FlexWorld，一个由两个关键组件组成的新颖框架：(1) 一个强大的视频到视频（V2V）扩散模型，用于从粗糙场景渲染的不完整输入中生成高质量的新颖视图图像；(2) 一个渐进式扩展过程，用于构建完整的3D场景。特别是，利用先进的预训练视频模型和精确的深度估计训练对，我们的V2V模型可以在大相机姿态变化下生成新颖视图。在此基础上，FlexWorld逐步生成新的3D内容，并通过几何感知的场景融合将其整合到全局场景中。大量的实验证明了FlexWorld从单张图像生成高质量新颖视图视频和灵活视角3D场景的有效性，与现有最先进方法相比，在多个流行指标和数据集上实现了卓越的视觉质量。定性地，我们强调FlexWorld可以生成具有360度旋转和缩放等灵活视角的高保真场景。项目页面：https://ml-gsai.github.io/FlexWorld。", "summary": "FlexWorld是一个旨在解决从单张图像生成灵活视角3D场景（包括360度旋转和缩放）挑战的框架。它结合了一个强大的视频到视频扩散模型，能够在大相机姿态变化下生成高质量的新颖视图，以及一个渐进式扩展过程，通过几何感知融合逐步构建完整的3D场景。实验结果表明，FlexWorld在生成高质量的新颖视图视频和高保真3D场景方面优于现有最先进的方法。", "keywords": "3D场景合成, 灵活视角, 扩散模型, 单图像, 渐进式扩展", "comments": "FlexWorld的创新之处在于其结合了强大的V2V扩散模型和渐进式场景扩展策略，有效解决了从单张图像生成灵活视角3D场景的难题。其能够处理大相机姿态变化并生成高保真场景的能力，对于虚拟现实、内容创作等领域具有重要意义。"}}
{"id": "2503.13272", "pdf": "https://arxiv.org/pdf/2503.13272", "abs": "https://arxiv.org/abs/2503.13272", "authors": ["Katja Schwarz", "Norman Mueller", "Peter Kontschieder"], "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing consistent and photorealistic 3D scenes is an open problem in\ncomputer vision. Video diffusion models generate impressive videos but cannot\ndirectly synthesize 3D representations, i.e., lack 3D consistency in the\ngenerated sequences. In addition, directly training generative 3D models is\nchallenging due to a lack of 3D training data at scale. In this work, we\npresent Generative Gaussian Splatting (GGS) -- a novel approach that integrates\na 3D representation with a pre-trained latent video diffusion model.\nSpecifically, our model synthesizes a feature field parameterized via 3D\nGaussian primitives. The feature field is then either rendered to feature maps\nand decoded into multi-view images, or directly upsampled into a 3D radiance\nfield. We evaluate our approach on two common benchmark datasets for scene\nsynthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model\nsignificantly improves both the 3D consistency of the generated multi-view\nimages, and the quality of the generated 3D scenes over all relevant baselines.\nCompared to a similar model without 3D representation, GGS improves FID on the\ngenerated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:\nhttps://katjaschwarz.github.io/ggs/", "AI": {"title_translation": "生成式高斯泼溅：利用视频扩散先验生成三维场景", "tldr": "本文提出生成式高斯泼溅（GGS），一种将3D表示与预训练视频扩散模型相结合的新方法，显著提升了生成3D场景的3D一致性和质量。", "motivation": "合成一致且逼真的三维场景是计算机视觉领域的开放问题。现有视频扩散模型无法直接合成具有3D一致性的三维表示，且大规模3D训练数据稀缺导致直接训练生成式3D模型面临挑战。", "method": "本文提出了生成式高斯泼溅（Generative Gaussian Splatting, GGS），该方法将3D表示与预训练的潜在视频扩散模型集成。具体而言，模型通过3D高斯基元参数化一个特征场，该特征场随后可被渲染为特征图并解码成多视图图像，或直接上采样为3D辐射场。", "result": "在RealEstate10K和ScanNet+两个常用场景合成基准数据集上，GGS显著提高了生成的多视图图像的3D一致性以及生成3D场景的质量，优于所有相关基线。与没有3D表示的类似模型相比，GGS在RealEstate10K和ScanNet+上生成的3D场景的FID（Fréchet Inception Distance）指标提高了约20%。", "conclusion": "生成式高斯泼溅（GGS）通过有效整合3D表示和预训练视频扩散模型，成功解决了3D场景合成中的一致性和质量问题，并取得了显著优于现有方法的性能。", "translation": "合成一致且逼真的三维场景是计算机视觉领域的一个开放问题。视频扩散模型能够生成令人印象深刻的视频，但无法直接合成三维表示，即在生成的序列中缺乏三维一致性。此外，由于缺乏大规模三维训练数据，直接训练生成式三维模型具有挑战性。在这项工作中，我们提出了生成式高斯泼溅 (Generative Gaussian Splatting, GGS)——一种将三维表示与预训练的潜在视频扩散模型集成的创新方法。具体而言，我们的模型通过三维高斯基元参数化特征场。然后，该特征场要么被渲染成特征图并解码为多视图图像，要么直接上采样为三维辐射场。我们在两个常用的场景合成基准数据集RealEstate10K和ScanNet+上评估了我们的方法，发现我们提出的GGS模型显著提高了生成的多视图图像的三维一致性以及生成三维场景的质量，优于所有相关基线。与没有三维表示的类似模型相比，GGS在RealEstate10K和ScanNet+上生成的3D场景的FID提高了约20%。项目页面：https://katjaschwarz.github.io/ggs/", "summary": "本文提出了生成式高斯泼溅（GGS），一种结合3D表示和预训练视频扩散模型的新方法，旨在解决3D场景生成中缺乏一致性和数据稀缺的问题。GGS通过3D高斯基元构建特征场，可渲染为多视图图像或上采样为3D辐射场。实验证明，GGS在3D一致性和场景质量上均显著优于现有方法，特别是在FID指标上实现了显著提升。", "keywords": "生成式高斯泼溅, 3D场景生成, 视频扩散模型, 3D一致性, 计算机视觉", "comments": "这项工作通过巧妙地将3D高斯泼溅与预训练的视频扩散模型结合，有效解决了3D场景生成中长期存在的3D一致性问题和数据稀缺挑战。其创新性在于利用视频扩散模型的强大生成能力，并通过3D表示强制实现空间一致性，为高质量、多视图一致的3D场景合成提供了新的范式，具有重要的实际应用潜力。"}}
{"id": "2503.13300", "pdf": "https://arxiv.org/pdf/2503.13300", "abs": "https://arxiv.org/abs/2503.13300", "authors": ["Ling-An Zeng", "Gaojie Wu", "Ancong Wu", "Jian-Fang Hu", "Wei-Shi Zheng"], "title": "Progressive Human Motion Generation Based on Text and Few Motion Frames", "categories": ["cs.CV"], "comment": null, "summary": "Although existing text-to-motion (T2M) methods can produce realistic human\nmotion from text description, it is still difficult to align the generated\nmotion with the desired postures since using text alone is insufficient for\nprecisely describing diverse postures. To achieve more controllable generation,\nan intuitive way is to allow the user to input a few motion frames describing\nprecise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M)\ngeneration task that aims to generate motions from text and very few given\nframes. Intuitively, the closer a frame is to a given frame, the lower the\nuncertainty of this frame is when conditioned on this given frame. Hence, we\npropose a novel Progressive Motion Generation (PMG) method to progressively\ngenerate a motion from the frames with low uncertainty to those with high\nuncertainty in multiple stages. During each stage, new frames are generated by\na Text-Frame Guided Generator conditioned on frame-aware semantics of the text,\ngiven frames, and frames generated in previous stages. Additionally, to\nalleviate the train-test gap caused by multi-stage accumulation of incorrectly\ngenerated frames during testing, we propose a Pseudo-frame Replacement Strategy\nfor training. Experimental results show that our PMG outperforms existing T2M\ngeneration methods by a large margin with even one given frame, validating the\neffectiveness of our PMG. Code will be released.", "AI": {"title_translation": "基于文本和少量运动帧的渐进式人体运动生成", "tldr": "提出了一种新的文本-帧到运动（TF2M）生成任务和渐进式运动生成（PMG）方法，通过结合文本和少量关键帧来更精确地控制人体运动生成，并显著优于现有方法。", "motivation": "现有的文本到运动（T2M）方法难以将生成的运动与期望的姿态精确对齐，因为仅凭文本不足以精确描述多样的姿态。为了实现更可控的生成，需要允许用户输入少量描述精确期望姿态的运动帧。", "method": "提出了一种新的文本-帧到运动（TF2M）生成任务，旨在从文本和少量给定帧生成运动。提出了一种新颖的渐进式运动生成（PMG）方法，该方法在多个阶段中从不确定性低的帧逐步生成到不确定性高的帧。在每个阶段，新的帧由一个文本-帧引导生成器生成，该生成器以文本的帧感知语义、给定帧和前一阶段生成的帧为条件。此外，为了缓解测试期间错误生成帧的多阶段累积引起的训练-测试差距，提出了一种伪帧替换策略进行训练。", "result": "实验结果表明，即使只给定一帧，我们的PMG也比现有的T2M生成方法有显著的优势，验证了我们PMG的有效性。", "conclusion": "本文提出的渐进式运动生成（PMG）方法通过结合文本和少量运动帧，显著提高了人体运动生成的控制性和精度，并优于现有的文本到运动生成方法。", "translation": "尽管现有的文本到运动（T2M）方法可以从文本描述中生成逼真的人体运动，但由于仅凭文本不足以精确描述多样的姿态，因此仍然难以将生成的运动与期望的姿态对齐。为了实现更可控的生成，一个直观的方法是允许用户输入少量描述精确期望姿态的运动帧。因此，我们探索了一种新的文本-帧到运动（TF2M）生成任务，旨在从文本和极少数给定帧生成运动。直观地，帧越接近给定帧，以该给定帧为条件时该帧的不确定性越低。因此，我们提出了一种新颖的渐进式运动生成（PMG）方法，以多阶段的方式从不确定性低的帧逐步生成到不确定性高的帧。在每个阶段，新的帧由一个文本-帧引导生成器生成，该生成器以文本的帧感知语义、给定帧以及前一阶段生成的帧为条件。此外，为了缓解测试期间错误生成帧的多阶段累积引起的训练-测试差距，我们提出了一种伪帧替换策略进行训练。实验结果表明，即使只给定一帧，我们的PMG也比现有的T2M生成方法有显著的优势，验证了我们PMG的有效性。代码将发布。", "summary": "本文针对现有文本到运动（T2M）方法难以精确控制姿态的问题，提出了一种新的文本-帧到运动（TF2M）生成任务。该任务旨在结合文本描述和少量关键运动帧来生成更可控的运动。为此，研究者提出了一种渐进式运动生成（PMG）方法，通过多阶段从低不确定性帧到高不确定性帧逐步生成运动，并引入了伪帧替换策略以缓解训练-测试差距。实验证明，PMG即使仅使用一帧也能显著优于现有T2M方法。", "keywords": "人体运动生成, 文本到运动, 渐进式生成, 姿态控制, 深度学习", "comments": "该论文的创新点在于提出了结合文本和少量关键帧的TF2M任务，并设计了渐进式生成策略（PMG）来逐步细化运动。这种方法通过引入额外的稀疏姿态控制，有效解决了纯文本生成中姿态控制不足的问题。伪帧替换策略也有效地缓解了多阶段生成中误差累积的问题。其重要性在于提升了运动生成的可控性和精度，对虚拟现实、动画制作等领域具有潜在应用价值。"}}
{"id": "2503.13303", "pdf": "https://arxiv.org/pdf/2503.13303", "abs": "https://arxiv.org/abs/2503.13303", "authors": ["Yinqiao Wang", "Hao Xu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures, 7 tables", "summary": "Estimating the 3D pose of hand and potential hand-held object from monocular\nimages is a longstanding challenge. Yet, existing methods are specialized,\nfocusing on either bare-hand or hand interacting with object. No method can\nflexibly handle both scenarios and their performance degrades when applied to\nthe other scenario. In this paper, we propose UniHOPE, a unified approach for\ngeneral 3D hand-object pose estimation, flexibly adapting both scenarios.\nTechnically, we design a grasp-aware feature fusion module to integrate\nhand-object features with an object switcher to dynamically control the\nhand-object pose estimation according to grasping status. Further, to uplift\nthe robustness of hand pose estimation regardless of object presence, we\ngenerate realistic de-occluded image pairs to train the model to learn\nobject-induced hand occlusions, and formulate multi-level feature enhancement\ntechniques for learning occlusion-invariant features. Extensive experiments on\nthree commonly-used benchmarks demonstrate UniHOPE's SOTA performance in\naddressing hand-only and hand-object scenarios. Code will be released on\nhttps://github.com/JoyboyWang/UniHOPE_Pytorch.", "AI": {"title_translation": "UniHOPE：一种统一的手部和手持物体姿态估计方法", "tldr": "UniHOPE是一种统一的3D手部和手持物体姿态估计方法，能灵活处理有无物体两种场景，并通过特征融合、对象切换和遮挡不变性学习实现最先进的性能。", "motivation": "现有方法在手部姿态估计方面是专门化的，要么只关注裸手，要么只关注手与物体的交互，无法灵活处理这两种场景，且在应用于其他场景时性能会下降。", "method": "提出UniHOPE，包含一个抓取感知特征融合模块用于整合手物特征，一个对象切换器根据抓取状态动态控制估计，以及生成去遮挡图像对和多级特征增强技术来学习遮挡不变性特征，以提高手部姿态估计的鲁棒性。", "result": "在三个常用基准测试中，UniHOPE在处理仅手部和手物场景时表现出SOTA（State-of-the-Art）性能。", "conclusion": "UniHOPE成功提供了一种统一且鲁棒的3D手部和手物姿态估计方法，能够灵活适应不同场景并取得领先性能。", "translation": "从单目图像估计手部和潜在手持物体的3D姿态是一个长期存在的挑战。然而，现有方法都是专门化的，要么专注于裸手，要么专注于手与物体的交互。没有一种方法能够灵活处理这两种场景，并且当应用于另一种场景时，它们的性能会下降。在本文中，我们提出了UniHOPE，一种用于通用3D手物姿态估计的统一方法，能够灵活适应这两种场景。技术上，我们设计了一个抓取感知特征融合模块来整合手物特征，并结合一个对象切换器根据抓取状态动态控制手物姿态估计。此外，为了提高手部姿态估计的鲁棒性，无论物体是否存在，我们生成逼真的去遮挡图像对来训练模型学习物体引起的手部遮挡，并制定多级特征增强技术来学习遮挡不变特征。在三个常用基准测试上的大量实验表明，UniHOPE在处理仅手部和手物场景方面表现出最先进的性能。代码将在https://github.com/JoyboyWang/UniHOPE_Pytorch 上发布。", "summary": "UniHOPE提出了一种统一的3D手部和手持物体姿态估计方法，旨在解决现有方法无法同时有效处理裸手和手物交互场景的问题。通过设计抓取感知特征融合模块和对象切换器，以及利用去遮挡图像和多级特征增强来提高对物体遮挡的鲁棒性，UniHOPE在多个基准测试中达到了最先进的性能，证明了其在两种场景下的灵活性和有效性。", "keywords": "手部姿态估计, 手物交互, 统一方法, 3D姿态估计, 遮挡不变性", "comments": "这篇论文的创新点在于提出了一个统一的框架UniHOPE，解决了以往手部姿态估计方法在处理手部有无物体场景时的局限性。其引入的抓取感知特征融合和对象切换机制，以及对物体遮挡鲁棒性的提升，是该方法的亮点。这对于推动通用手部姿态估计领域的发展具有重要意义。"}}
{"id": "2503.13319", "pdf": "https://arxiv.org/pdf/2503.13319", "abs": "https://arxiv.org/abs/2503.13319", "authors": ["Shitong Shao", "Hongwei Yi", "Hanzhong Guo", "Tian Ye", "Daquan Zhou", "Michael Lingelbach", "Zhiqiang Xu", "Zeke Xie"], "title": "MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Fine-tuning open-source large-scale VDMs for the portrait video synthesis\ntask can result in significant improvements across multiple dimensions, such as\nvisual quality and natural facial motion dynamics. Despite their advancements,\nhow to achieve step distillation and reduce the substantial computational\noverhead of large-scale VDMs remains unexplored. To fill this gap, this paper\nproposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue\nof insufficient training memory and the problem of training collapse observed\nin vanilla DMD during the training process. Specifically, we first leverage\nLoRA to fine-tune the fake diffusion transformer (DiT) to address the\nout-of-memory issue. Then, we employ the W2S distribution matching to adjust\nthe real DiT's parameter, subtly shifting it toward the fake DiT's parameter.\nThis adjustment is achieved by utilizing the weak weight of the low-rank\nbranch, effectively alleviate the conundrum where the video synthesized by the\nfew-step generator deviates from the real data distribution, leading to\ninaccuracies in the KL divergence approximation. Additionally, we minimize the\ndistance between the fake data distribution and the ground truth distribution\nto further enhance the visual quality of the synthesized videos. As\nexperimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard\nEuler, LCM, DMD and even the 28-step standard sampling in FID/FVD and VBench in\n1/4-step video synthesis. The project page is in\nhttps://w2svd.github.io/W2SVD/.", "AI": {"title_translation": "魔法蒸馏：用于大规模肖像少步合成的弱到强视频蒸馏", "tldr": "该论文提出了“弱到强视频蒸馏”（W2SVD）方法，通过解决大规模视频扩散模型（VDM）在肖像视频合成中的训练内存不足和训练崩溃问题，实现了少步高效的视频合成，并在多项指标上超越了现有方法。", "motivation": "尽管大规模视频扩散模型（VDM）在肖像视频合成任务上取得了显著进步，但在步长蒸馏和降低其巨大计算开销方面仍未被探索。此外，在训练过程中还存在内存不足和传统DMD训练崩溃的问题。", "method": "本文提出了弱到强视频蒸馏（W2SVD）。具体而言，首先利用LoRA对“伪扩散Transformer（DiT）”进行微调以解决内存不足问题。然后，采用W2S分布匹配来调整“真实DiT”的参数，使其微妙地向伪DiT的参数靠拢，通过利用低秩分支的弱权重，有效缓解了少步生成器合成视频偏离真实数据分布的问题。此外，通过最小化伪数据分布与真实数据分布之间的距离来进一步提升合成视频的视觉质量。", "result": "在HunyuanVideo上的实验证明，W2SVD在1/4步视频合成中，其FID/FVD和VBench表现优于标准的Euler、LCM、DMD，甚至超越了28步标准采样。", "conclusion": "W2SVD有效缓解了大规模肖像视频合成中VDM的训练内存问题和训练崩溃问题，实现了高效且高质量的少步视频合成，性能优于现有方法。", "translation": "微调开源大规模视频扩散模型（VDM）用于肖像视频合成任务可以在视觉质量和自然面部运动动力学等多个维度上实现显著改进。尽管取得了这些进步，但如何实现步长蒸馏并减少大规模VDM的巨大计算开销仍未被探索。为了填补这一空白，本文提出了弱到强视频蒸馏（W2SVD）来缓解训练内存不足和传统DMD在训练过程中观察到的训练崩溃问题。具体而言，我们首先利用LoRA微调伪扩散Transformer（DiT）以解决内存不足问题。然后，我们采用W2S分布匹配来调整真实DiT的参数，使其微妙地向伪DiT的参数靠拢。这种调整通过利用低秩分支的弱权重来实现，有效缓解了少步生成器合成的视频偏离真实数据分布，导致KL散度近似不准确的难题。此外，我们最小化伪数据分布与真实数据分布之间的距离，以进一步增强合成视频的视觉质量。正如在HunyuanVideo上通过实验证明的，W2SVD在1/4步视频合成中，其FID/FVD和VBench表现优于标准的Euler、LCM、DMD，甚至超越了28步标准采样。项目页面位于https://w2svd.github.io/W2SVD/。", "summary": "本文提出了“弱到强视频蒸馏”（W2SVD）方法，旨在解决大规模肖像视频合成中视频扩散模型（VDM）的计算开销和训练挑战。W2SVD通过结合LoRA来解决内存不足问题，并引入创新的W2S分布匹配策略来调整模型参数，以确保合成视频的分布与真实数据对齐，从而提高视觉质量。在HunyuanVideo上的实验结果表明，W2SVD在少步视频合成方面表现出色，超越了多种现有基线方法。", "keywords": "视频蒸馏, 肖像合成, 少步, 大规模VDM, 弱到强", "comments": "该论文在解决大规模视频扩散模型在肖像视频合成中的实际挑战方面具有重要创新。它不仅通过LoRA解决了内存限制，还引入了“弱到强”的分布匹配策略，有效缓解了训练崩溃和数据分布偏差问题。这项工作对于推动高效、高质量的视频生成，尤其是在计算资源受限的环境下，具有显著的实用价值和重要性。"}}
{"id": "2503.13327", "pdf": "https://arxiv.org/pdf/2503.13327", "abs": "https://arxiv.org/abs/2503.13327", "authors": ["Lan Chen", "Qi Mao", "Yuchao Gu", "Mike Zheng Shou"], "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.", "AI": {"title_translation": "编辑迁移：通过视觉上下文关系学习图像编辑", "tldr": "本文提出了一种名为“编辑迁移”的新方法，它通过从一个源-目标示例中学习变换，并将其应用于新的查询图像，解决了现有文本和参考图像编辑方法在处理精确几何和非刚性变换方面的局限性。", "motivation": "现有的文本生成图像编辑方法在语义操作上表现出色，但在精确的几何细节（如姿态和视点变化）上表现不佳。而基于参考的编辑通常侧重于风格或外观，无法处理非刚性变换。本文旨在通过显式学习源-目标对之间的编辑变换来弥补这些方法的不足。", "method": "本文引入了“编辑迁移”设置，并提出了一种视觉关系上下文学习范式。该方法借鉴了大型语言模型中的上下文学习，构建在一个基于DiT的文本到图像模型之上。它将编辑示例和查询图像排列成一个统一的四面板合成图，然后应用轻量级的LoRA微调来从极少的示例中捕获复杂的空间变换。", "result": "尽管仅使用了42个训练样本，“编辑迁移”在各种非刚性场景中显著优于最先进的TIE和RIE方法。", "conclusion": "“编辑迁移”通过少数样本视觉关系学习，有效解决了图像编辑中精确几何和非刚性变换的挑战，并展现出卓越的性能。", "translation": "我们引入了一种新的设置，即编辑迁移，模型通过一个单一的源-目标示例学习一种变换，并将其应用于新的查询图像。虽然基于文本的方法在通过文本提示进行语义操作方面表现出色，但它们通常在精确的几何细节（例如，姿态和视点变化）方面遇到困难。另一方面，基于参考的编辑通常侧重于风格或外观，并且在非刚性变换方面失败。通过从源-目标对中明确学习编辑变换，编辑迁移减轻了纯文本和以外观为中心的参考方法的局限性。借鉴大型语言模型中的上下文学习，我们提出了一种视觉关系上下文学习范式，建立在基于DiT的文本到图像模型之上。我们将编辑示例和查询图像排列成一个统一的四面板复合图，然后应用轻量级的LoRA微调，以从极少的示例中捕获复杂的空间变换。尽管只使用了42个训练样本，编辑迁移在各种非刚性场景中显著优于最先进的TIE和RIE方法，证明了少样本视觉关系学习的有效性。", "summary": "本文提出了一种名为“编辑迁移”的新型图像编辑方法，旨在通过从单个源-目标示例中学习变换来解决现有文本和参考图像编辑技术在处理精确几何和非刚性变换方面的不足。该方法借鉴了大型语言模型的上下文学习理念，构建于一个DiT模型之上，通过将示例和查询图像合成为四面板，并利用轻量级LoRA微调来捕捉复杂的空间变换。实验结果表明，即使仅用42个训练样本，该方法在非刚性场景下的表现也显著优于现有最先进的技术，证明了其在少样本视觉关系学习方面的有效性。", "keywords": "图像编辑, 视觉上下文学习, 非刚性变换, 少样本学习, LoRA", "comments": "这项工作引入了一种新颖的图像编辑范式，其创新点在于将大型语言模型中的“上下文学习”理念引入到视觉领域，并通过“视觉关系上下文学习”实现了从极少样本中学习复杂非刚性变换的能力。其优势在于克服了传统文本或参考驱动方法在处理精确几何和非刚性变形时的局限性。在仅使用42个训练样本的情况下便能超越现有SOTA方法，这充分展示了其高效性和潜力，对少样本学习和图像生成领域具有重要意义。"}}
{"id": "2503.13344", "pdf": "https://arxiv.org/pdf/2503.13344", "abs": "https://arxiv.org/abs/2503.13344", "authors": ["Shashikant Verma", "Harish Katti", "Soumyaratna Debnath", "Yamuna Swamy", "Shanmuganathan Raman"], "title": "STEP: Simultaneous Tracking and Estimation of Pose for Animals and Humans", "categories": ["cs.CV"], "comment": null, "summary": "We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state\ninitialized through a pre-trained detector or manual initialization in the\ninitial frame of a given video sequence. It then seamlessly tracks the target\nand estimates keypoints of anatomical importance as output for subsequent\nframes. Unlike prevalent top-down pose estimation methods, our approach doesn't\nrely on per-frame target detections due to its tracking capability. This\nfacilitates a significant advancement in inference efficiency and potential\napplications. We train and validate our approach on datasets encompassing\ndiverse species. Our experiments demonstrate superior results compared to\nexisting methods, opening doors to various applications, including but not\nlimited to action recognition and behavioral analysis.", "AI": {"title_translation": "STEP：动物和人类姿态的同步跟踪与估计", "tldr": "STEP是一个新颖的基于Transformer的框架，用于同时跟踪和估计动物和人类的姿态，通过引入GMSP和OMRA模块，避免了对预定义关键点目标状态的依赖，显著提高了推理效率并取得了优异结果。", "motivation": "该研究的灵感来源于人脑利用时空连续性进行并发定位和姿态估计的能力。传统的判别模型通常需要预定义目标状态来确定模型权重，这是一个挑战，本文旨在解决此问题，以实现跨物种和人类的姿态跟踪与估计。", "method": "本文提出了STEP，一个基于Transformer的判别模型框架，用于同时跟踪和估计动物和人类的姿态。该方法引入了高斯图软预测（GMSP）和偏移图回归适配器（OMRA）模块，以消除对关键点目标状态作为输入的需要。它从视频序列的第一帧中已知的目标状态（通过预训练检测器或手动初始化）开始，然后无缝跟踪目标并估计后续帧中的解剖学关键点。与主流的自上而下姿态估计方法不同，STEP不依赖于每帧的目标检测，因为它具有跟踪能力。", "result": "我们的实验表明，与现有方法相比，该方法在涵盖不同物种的数据集上取得了卓越的结果。", "conclusion": "该方法显著提升了推理效率和潜在应用范围，为包括但不限于动作识别和行为分析在内的各种应用打开了大门。", "translation": "我们引入了STEP，一个利用基于Transformer的判别模型预测的新颖框架，用于跨不同动物物种和人类的同步跟踪和姿态估计。我们受到以下事实的启发：人脑利用时空连续性，并在大脑区域专门处理形状和运动的情况下，执行并发的定位和姿态估计。传统的判别模型通常需要预定义的目标状态来确定模型权重，这是一个我们通过高斯图软预测（GMSP）和偏移图回归适配器（OMRA）模块解决的挑战。这些模块消除了关键点目标状态作为输入的必要性，从而简化了过程。我们的方法从给定视频序列初始帧中通过预训练检测器或手动初始化已知的目标状态开始。然后，它无缝跟踪目标并估计后续帧中具有解剖学重要性的关键点作为输出。与流行的自上而下姿态估计方法不同，我们的方法由于其跟踪能力而不依赖于每帧目标检测。这促进了推理效率和潜在应用的显著进步。我们在涵盖不同物种的数据集上训练和验证了我们的方法。我们的实验表明，与现有方法相比，取得了卓越的结果，为各种应用打开了大门，包括但不限于动作识别和行为分析。", "summary": "STEP是一个新颖的基于Transformer的框架，用于对动物和人类进行同步姿态跟踪和估计。它受人脑时空连续性处理的启发，通过引入高斯图软预测（GMSP）和偏移图回归适配器（OMRA）模块，解决了传统判别模型对预定义关键点状态的依赖问题。该方法在视频首帧初始化后，能够无缝跟踪目标并估计后续帧的关键点，无需每帧检测，从而显著提升了推理效率。在多样化物种数据集上的实验证明其性能优于现有方法，并为动作识别和行为分析等应用提供了广阔前景。", "keywords": "姿态估计, 动物跟踪, Transformer, 时空连续性, 判别模型", "comments": "该论文的创新之处在于其提出的STEP框架，它首次实现了动物和人类姿态的同步跟踪与估计，而非仅仅是单帧估计。通过引入GMSP和OMRA模块，它成功克服了传统判别模型对预定义关键点目标状态的依赖，这是一个重要的技术突破。此外，其不受每帧目标检测限制的跟踪能力，显著提升了推理效率，使其在实际应用中更具优势。该研究从人脑处理机制中汲取灵感，为多物种姿态估计领域带来了新的视角和解决方案，具有重要的理论和应用价值。"}}
{"id": "2503.13347", "pdf": "https://arxiv.org/pdf/2503.13347", "abs": "https://arxiv.org/abs/2503.13347", "authors": ["Jiaming Kang", "Keyan Chen", "Zhengxia Zou", "Zhenwei Shi"], "title": "TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing novel view synthesis (NVS) offers significant potential for 3D\ninterpretation of remote sensing scenes, with important applications in urban\nplanning and environmental monitoring. However, remote sensing scenes\nfrequently lack sufficient multi-view images due to acquisition constraints.\nWhile existing NVS methods tend to overfit when processing limited input views,\nadvanced few-shot NVS methods are computationally intensive and perform\nsub-optimally in remote sensing scenes. This paper presents TriDF, an efficient\nhybrid 3D representation for fast remote sensing NVS from as few as 3 input\nviews. Our approach decouples color and volume density information, modeling\nthem independently to reduce the computational burden on implicit radiance\nfields and accelerate reconstruction. We explore the potential of the triplane\nrepresentation in few-shot NVS tasks by mapping high-frequency color\ninformation onto this compact structure, and the direct optimization of feature\nplanes significantly speeds up convergence. Volume density is modeled as\ncontinuous density fields, incorporating reference features from neighboring\nviews through image-based rendering to compensate for limited input data.\nAdditionally, we introduce depth-guided optimization based on point clouds,\nwhich effectively mitigates the overfitting problem in few-shot NVS.\nComprehensive experiments across multiple remote sensing scenes demonstrate\nthat our hybrid representation achieves a 30x speed increase compared to\nNeRF-based methods, while simultaneously improving rendering quality metrics\nover advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%\nin LPIPS). The code is publicly available at https://github.com/kanehub/TriDF", "AI": {"title_translation": "TriDF：三平面加速密度场用于少样本遥感新视角合成", "tldr": "TriDF是一种高效的混合3D表示方法，能够从少量遥感图像中快速合成新视角，显著提高速度和渲染质量，并缓解过拟合问题。", "motivation": "遥感场景由于采集限制，通常缺乏足够的多视角图像。现有NVS方法在处理有限输入视图时容易过拟合，而先进的少样本NVS方法计算成本高昂且在遥感场景中表现不佳。", "method": "本文提出了TriDF，一种高效的混合3D表示方法。它将颜色信息和体密度信息解耦并独立建模，以减轻隐式辐射场的计算负担。通过将高频颜色信息映射到紧凑的三平面表示上并直接优化特征平面来加速收敛。体密度被建模为连续密度场，通过基于图像的渲染整合来自相邻视图的参考特征以弥补有限的输入数据。此外，引入了基于点云的深度引导优化，有效缓解了少样本NVS中的过拟合问题。", "result": "与基于NeRF的方法相比，速度提高了30倍。与先进的少样本方法相比，渲染质量指标有所提高（PSNR提高7.4%，SSIM提高12.2%，LPIPS提高18.7%）。", "conclusion": "TriDF这种混合表示方法，通过显著提高速度、改善渲染质量并缓解过拟合问题，有效地解决了少样本遥感新视角合成的挑战。", "translation": "遥感新视角合成（NVS）为遥感场景的3D解释提供了巨大的潜力，在城市规划和环境监测中具有重要的应用。然而，由于采集限制，遥感场景通常缺乏足够的多视角图像。虽然现有的NVS方法在处理有限输入视图时容易过拟合，但先进的少样本NVS方法计算成本高昂且在遥感场景中表现不佳。本文提出了TriDF，一种高效的混合3D表示方法，能够从最少3个输入视图中快速进行遥感NVS。我们的方法将颜色和体密度信息解耦，并独立建模，以减轻隐式辐射场的计算负担并加速重建。我们通过将高频颜色信息映射到这种紧凑的结构上，探索了三平面表示在少样本NVS任务中的潜力，并且特征平面的直接优化显著加快了收敛速度。体密度被建模为连续密度场，通过基于图像的渲染整合来自相邻视图的参考特征以弥补有限的输入数据。此外，我们引入了基于点云的深度引导优化，有效缓解了少样本NVS中的过拟合问题。在多个遥感场景中的综合实验表明，与基于NeRF的方法相比，我们的混合表示方法实现了30倍的速度提升，同时比先进的少样本方法提高了渲染质量指标（PSNR提高7.4%，SSIM提高12.2%，LPIPS提高18.7%）。代码已在https://github.com/kanehub/TriDF公开。", "summary": "针对遥感场景中多视角图像不足、现有NVS方法易过拟合及少样本方法计算效率低的问题，本文提出TriDF，一种高效的混合3D表示方法。TriDF通过解耦颜色与体密度信息、利用三平面表示加速颜色重建、以及引入深度引导优化来缓解过拟合，实现了从少量输入视图快速合成高质量遥感新视角。实验证明，TriDF在速度上比NeRF基线方法快30倍，并在渲染质量上超越了先进的少样本方法。", "keywords": "TriDF, 新视角合成, 遥感, 少样本, 三平面", "comments": "TriDF的创新点在于其混合3D表示和信息解耦策略，特别是将高频颜色信息映射到三平面表示，并结合深度引导优化来应对少样本挑战。这种方法在保证渲染质量的同时，大幅提升了计算效率，对于遥感这种数据获取受限的领域具有重要意义。其速度提升和质量改善的量化结果令人印象深刻。"}}
{"id": "2503.13354", "pdf": "https://arxiv.org/pdf/2503.13354", "abs": "https://arxiv.org/abs/2503.13354", "authors": ["Laura Girometti", "Jean-François Aujol", "Antoine Guennec", "Yann Traonmilin"], "title": "Parameter-free structure-texture image decomposition by unrolling", "categories": ["cs.CV", "cs.NA", "eess.IV", "math.NA", "68U10, 90C26"], "comment": "To be published in Conference Proceedings: Scale Space and\n  Variational Method in Computer Vision, 2025", "summary": "In this work, we propose a parameter-free and efficient method to tackle the\nstructure-texture image decomposition problem. In particular, we present a\nneural network LPR-NET based on the unrolling of the Low Patch Rank model. On\nthe one hand, this allows us to automatically learn parameters from data, and\non the other hand to be computationally faster while obtaining qualitatively\nsimilar results compared to traditional iterative model-based methods.\nMoreover, despite being trained on synthetic images, numerical experiments show\nthe ability of our network to generalize well when applied to natural images.", "AI": {"title_translation": "无参数结构-纹理图像分解通过展开", "tldr": "本文提出了一种基于低补丁秩模型展开的无参数神经网络LPR-NET，用于结构-纹理图像分解。该网络能够自动学习参数，计算速度快，且在自然图像上表现出良好的泛化能力。", "motivation": "解决结构-纹理图像分解问题，并提供一种无参数且高效的方法。", "method": "提出了一个名为LPR-NET的神经网络，它基于低补丁秩模型的展开（unrolling）。", "result": "该方法能够自动从数据中学习参数，计算速度比传统迭代模型方法更快，同时获得相似的定性结果。尽管在合成图像上训练，但在自然图像上表现出良好的泛化能力。", "conclusion": "LPR-NET提供了一种高效且泛化能力强的无参数结构-纹理图像分解方法。", "translation": "在这项工作中，我们提出了一种无参数且高效的方法来解决结构-纹理图像分解问题。特别是，我们提出了一个基于低补丁秩模型展开的神经网络LPR-NET。一方面，这使得我们能够自动从数据中学习参数，另一方面，与传统的基于模型的迭代方法相比，它在计算上更快，同时获得了定性相似的结果。此外，尽管在合成图像上进行了训练，数值实验表明我们的网络在应用于自然图像时具有良好的泛化能力。", "summary": "本文提出了一种名为LPR-NET的无参数且高效的神经网络，用于结构-纹理图像分解。LPR-NET通过展开低补丁秩模型构建，能够自动从数据中学习参数，并实现比传统迭代方法更快的计算速度和相似的分解质量。实验证明，该网络即使在合成图像上训练，也能很好地泛化到自然图像。", "keywords": "结构-纹理分解, 无参数, 神经网络, 模型展开, LPR-NET", "comments": "该论文的创新点在于将传统模型（低补丁秩模型）通过展开技术转化为神经网络，从而实现了参数的自动学习和计算效率的提升。其无参数的特性简化了使用，而良好的泛化能力则增强了其实用性。"}}
{"id": "2503.13358", "pdf": "https://arxiv.org/pdf/2503.13358", "abs": "https://arxiv.org/abs/2503.13358", "authors": ["Daniil Selikhanovych", "David Li", "Aleksei Leonov", "Nikita Gushchin", "Sergei Kushneriuk", "Alexander Filippov", "Evgeny Burnaev", "Iaroslav Koshelev", "Alexander Korotin"], "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.", "AI": {"title_translation": "通过蒸馏实现图像超分辨率的一步残差偏移扩散", "tldr": "本文提出了一种名为RSD的新蒸馏方法，用于加速扩散模型ResShift进行图像超分辨率，实现了单步恢复并超越了教师模型，同时解决了现有方法中感知细节不真实或结构幻觉的问题。", "motivation": "现有的扩散模型在图像超分辨率任务中虽然能产生高质量视觉结果，但计算成本高昂。尽管有加速方法，但一些（如SinSR）无法产生真实的感知细节，而另一些（如OSEDiff）可能产生不存在的结构幻ual。为了解决这些问题，本文提出了RSD。", "method": "本文提出了一种新的蒸馏方法RSD，用于顶级的基于扩散的超分辨率模型ResShift。该方法通过训练学生网络生成图像，使得基于这些图像训练的新伪ResShift模型与教师模型一致，从而实现单步恢复。", "result": "RSD实现了单步恢复并大幅超越了教师模型。它在性能上优于ResShift的其他基于蒸馏的方法SinSR，使其与最先进的基于扩散的超分辨率蒸馏方法相媲美。与基于预训练文本到图像模型的超分辨率方法相比，RSD产生了有竞争力的感知质量，提供了与降级输入图像更好对齐的图像，并且需要更少的参数和GPU内存。实验结果在RealSR、RealSet65、DRealSR、ImageNet和DIV2K等多种真实世界和合成数据集上得到验证。", "conclusion": "RSD是一种有效的蒸馏方法，能够显著加速基于扩散的图像超分辨率模型ResShift，同时保持或提高图像质量，并解决了现有加速方法中的感知细节和结构幻觉问题，使其在效率和性能上达到领先水平。", "translation": "扩散模型在超分辨率（SR）方面能产生高质量的视觉结果，但计算成本高昂。尽管开发了几种方法来加速基于扩散的SR模型，但有些（例如SinSR）未能产生真实的感知细节，而另一些（例如OSEDiff）可能会幻觉出不存在的结构。为了克服这些问题，我们提出了RSD，一种针对顶级基于扩散的SR模型ResShift的新蒸馏方法。我们的方法基于训练学生网络生成图像，使得在此基础上训练的新伪ResShift模型将与教师模型一致。RSD实现了单步恢复并大幅超越了教师模型。我们表明，我们的蒸馏方法可以超越ResShift的另一种基于蒸馏的方法——SinSR——使其与最先进的基于扩散的SR蒸馏方法相媲美。与基于预训练文本到图像模型的SR方法相比，RSD产生了有竞争力的感知质量，提供了与降级输入图像更好对齐的图像，并且需要更少的参数和GPU内存。我们提供了在各种真实世界和合成数据集上的实验结果，包括RealSR、RealSet65、DRealSR、ImageNet和DIV2K。", "summary": "本文提出了一种名为RSD的新型蒸馏方法，旨在解决基于扩散的图像超分辨率模型（如ResShift）计算成本高昂以及现有加速方法（如SinSR和OSEDiff）存在的感知细节不真实或结构幻觉问题。RSD通过训练学生网络生成与教师模型一致的图像，实现了单步恢复，并显著超越了教师模型。实验结果表明，RSD在性能上优于其他蒸馏方法，且在感知质量、图像对齐、参数和GPU内存方面优于基于预训练文本到图像模型的SR方法，达到了最先进水平。", "keywords": "图像超分辨率, 扩散模型, 蒸馏, ResShift, 单步恢复", "comments": "本文提出了一种创新的蒸馏策略，即训练学生网络使其生成的图像能够让一个新的伪教师模型与原教师模型保持一致，这是一种新颖的知识传递方式。该方法不仅显著加速了扩散模型，还解决了现有加速方案中常见的质量问题，同时在效率和效果上都达到了领先水平，对于实际应用具有重要价值。"}}
{"id": "2503.13360", "pdf": "https://arxiv.org/pdf/2503.13360", "abs": "https://arxiv.org/abs/2503.13360", "authors": ["Hai-Long Sun", "Zhun Sun", "Houwen Peng", "Han-Jia Ye"], "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.", "AI": {"title_translation": "多模态长链思维推理中通过随行视觉条件作用缓解视觉遗忘", "tldr": "MLLMs在长链推理中存在“视觉遗忘”问题，导致过度依赖文本。本文提出TVC策略，通过在关键推理阶段引入图像并动态剪枝冗余视觉tokens，有效缓解此问题，并在数学推理基准测试中达到SOTA。", "motivation": "多模态大型语言模型（MLLMs）在需要视觉输入的多模态任务中进行长链推理时，难以保持对视觉信息的关注，表现为对视觉信息的注意力逐渐下降，导致输出过度依赖文本。通过实验发现，在推理过程中即使移除图像输入，准确率也仅下降约2%，表明模型的文本输出主导了后续推理。", "method": "提出“随行视觉条件作用”（Take-along Visual Conditioning, TVC）策略。该策略将图像输入转移到关键推理阶段，并通过动态剪枝压缩冗余的视觉tokens，以帮助模型在整个推理过程中保持对视觉组件的注意力。", "result": "在五个数学推理基准测试中，平均性能达到最先进水平（比之前的SOTA提高了3.4%）。", "conclusion": "TVC策略有效增强了多模态推理系统，成功缓解了多模态LLMs在长链推理中的视觉遗忘问题。", "translation": "近年来，大型语言模型（LLMs）的进步展示了增强的推理能力，从思维链（CoT）提示演变为先进的、面向产品的解决方案，如OpenAI o1。在我们重新实现这个模型的过程中，我们注意到在需要视觉输入的多模态任务（例如几何问题）中，多模态LLMs（MLLMs）难以保持对视觉信息的关注，换句话说，MLLMs随着推理的进行，对视觉信息的注意力会逐渐下降，导致输出过度依赖文本。为了调查这一点，我们在长链推理过程中消融图像输入。具体来说，我们截断了推理过程，然后在移除输入图像的情况下重新完成推理过程。我们观察到在MathVista的test-hard子集上准确率仅下降约2%，这表明模型的文本输出主导了后续的推理过程。受此启发，我们提出了随行视觉条件作用（Take-along Visual Conditioning, TVC），这是一种将图像输入转移到关键推理阶段并通过动态剪枝压缩冗余视觉tokens的策略。这种方法有助于模型在整个推理过程中保持对视觉组件的注意力。我们的方法在五个数学推理基准测试中平均取得了最先进的性能（比之前的SOTA提高了3.4%），证明了TVC在增强多模态推理系统方面的有效性。", "summary": "本文研究了多模态大型语言模型（MLLMs）在长链推理中存在的“视觉遗忘”问题，即模型在推理过程中对视觉信息的注意力逐渐下降，导致过度依赖文本。通过实验验证了这一问题。为解决此问题，作者提出了随行视觉条件作用（TVC）策略，该策略通过在关键推理阶段引入图像并动态剪枝冗余视觉tokens来帮助模型维持对视觉信息的关注。实验结果表明，TVC在多个数学推理基准测试中达到了SOTA性能，有效提升了多模态推理能力。", "keywords": "多模态推理, 视觉遗忘, 随行视觉条件作用, 长链思维, 大型语言模型", "comments": "这篇论文通过深入分析MLLMs在多模态长链推理中的“视觉遗忘”现象，并提出创新的TVC策略来解决。其创新点在于将视觉信息有选择性地引入关键推理阶段并进行动态剪枝，这在效率和效果上都取得了平衡。实验结果显著，显示了其在增强多模态推理系统方面的潜力。"}}
{"id": "2503.13377", "pdf": "https://arxiv.org/pdf/2503.13377", "abs": "https://arxiv.org/abs/2503.13377", "authors": ["Ye Wang", "Boshen Xu", "Zihao Yue", "Zihan Xiao", "Ziheng Wang", "Liang Zhang", "Dingyi Yang", "Wenxuan Wang", "Qin Jin"], "title": "TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code: https://github.com/www-Ye/TimeZero", "summary": "We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero.", "AI": {"title_translation": "TimeZero：基于推理引导LVLM的时间视频定位", "tldr": "TimeZero是一个使用强化学习进行推理引导的LVLM，用于时间视频定位任务，并在Charades-STA上达到了SOTA性能。", "motivation": "时间视频定位（TVG）任务需要根据语言查询在长视频中精确地定位相关视频片段，这是一项具有挑战性的任务。", "method": "本文介绍了TimeZero，一个推理引导的LVLM，它通过扩展推理过程，并完全通过强化学习实现对视频-语言关系的推理。", "result": "TimeZero在两个基准测试上进行了实验，并在Charades-STA上实现了最先进的性能。", "conclusion": "TimeZero通过其推理引导的LVLM和强化学习方法，在时间视频定位任务上表现出色，尤其在特定基准测试中达到了SOTA。", "translation": "我们引入了TimeZero，一个为时间视频定位（TVG）任务设计的推理引导LVLM。这项任务要求根据给定的语言查询在长视频中精确地定位相关视频片段。TimeZero通过扩展推理过程来应对这一挑战，使模型能够完全通过强化学习对视频-语言关系进行推理。为了评估TimeZero的有效性，我们在两个基准测试上进行了实验，其中TimeZero在Charades-STA上取得了最先进的性能。代码可在https://github.com/www-Ye/TimeZero获取。", "summary": "本文介绍了TimeZero，一个用于时间视频定位（TVG）任务的推理引导大型视频语言模型（LVLM）。TimeZero通过强化学习扩展其推理过程，以理解视频-语言关系，从而在长视频中精确地定位相关片段。实验表明，TimeZero在Charades-STA基准测试上取得了最先进的性能。", "keywords": "时间视频定位, LVLM, 强化学习, 视频-语言推理, Charades-STA", "comments": "TimeZero的创新点在于其将推理引导的LVLM与强化学习相结合，以解决时间视频定位任务中的复杂视频-语言关系推理问题。这种方法可能为未来的视频理解和定位任务提供新的思路，特别是在处理长视频和复杂查询方面。"}}
{"id": "2503.13383", "pdf": "https://arxiv.org/pdf/2503.13383", "abs": "https://arxiv.org/abs/2503.13383", "authors": ["Mengyao Lyu", "Yan Li", "Huasong Zhong", "Wenhao Yang", "Hui Chen", "Jungong Han", "Guiguang Ding", "Zhenheng Yang"], "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "update comparison with sota and analysis", "summary": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.", "AI": {"title_translation": "优中选优：为指令微调获取丰富、可扩展和可迁移的多模态数据", "tldr": "该研究提出了一种名为mmSSR的新方法，通过分解质量指标和引入交互风格作为多样性指标，高效地筛选出高质量、多样化的多模态指令数据，在多种实验设置下显著优于现有方法。", "motivation": "尽管现有数据筛选方法在数据质量上有所提升，但其稳定性和泛化能力受实验设置和验证协议影响，未能超越随机采样。对于多模态大语言模型（MLLMs），数据选择的复杂性和重要性更高，因此需要一种更鲁棒、高效的方法来获取多模态指令数据。", "method": "作者重新定义了质量指标的粒度，将其分解为14种视觉-语言相关能力，并引入多模态丰富评分器（multi-modal rich scorers）来评估每个数据候选的能力。为了促进多样性，他们将交互风格作为多样性指标，并使用多模态丰富风格器（multi-modal rich styler）来识别数据指令模式。这些评分器和风格器（mmSSR）共同确保高质量信息以多样化的形式传达给用户。该方法不依赖于基于嵌入的聚类或贪婪采样。", "result": "mmSSR能够高效地扩展到数百万数据，支持通用或特定能力获取的定制化，并促进在策展新领域的免训练泛化。在10多种实验设置下，并通过14个多模态基准验证，mmSSR在与随机采样、基线策略和最先进的选择方法比较时，表现出持续的改进，仅用2.6M数据中的30%就达到了99.1%的完整性能。", "conclusion": "本研究提出了一种名为mmSSR的新型数据筛选方法，通过细化质量评估和引入多样性指标，解决了多模态指令数据筛选的挑战，实现了高效、可扩展且性能优越的数据获取，显著提升了多模态大语言模型的微调效果。", "translation": "预训练大型语言模型（LLMs）在微调（SFT）阶段仅需最少监督的假设（Zhou 等，2024）已通过数据管理和选择研究的最新进展得到证实。然而，由于对实验设置和验证协议的脆弱性，它们的稳定性和泛化能力受到损害，未能超越随机采样（Diddee & Ippolito，2024；Xia 等，2024b）。在LLMs基础上构建的多模态LLMs（MLLMs），结合巨大的令牌量和更高的数据源异质性，放大了数据选择的意义和复杂性。\n为了以鲁棒和高效的方式获取多模态指令数据，我们通过将其分解为14种视觉-语言相关能力来重新定义质量指标的粒度，并引入多模态丰富评分器来评估每个数据候选的能力。为了促进多样性，鉴于对齐阶段的内在目标，我们以交互风格作为多样性指标，并使用多模态丰富风格器来识别数据指令模式。通过这样做，我们的多模态丰富评分器和风格器（mmSSR）保证高分信息以多样化的形式传达给用户。mmSSR无需基于嵌入的聚类或贪婪采样，高效地扩展到数百万数据，并具有不同的预算约束，支持通用或特定能力获取的定制，并促进在策展新领域的免训练泛化。在10多种实验设置下，并通过14个多模态基准验证，我们展示了相对于随机采样、基线策略和最先进选择方法的一致改进，仅用2.6M数据中的30%就达到了99.1%的完整性能。", "summary": "该论文提出了一种名为mmSSR（multi-modal rich scorers and styler）的新型方法，旨在高效、鲁棒地筛选多模态指令数据，以解决现有数据选择方法在稳定性、泛化性及应对多模态数据复杂性方面的不足。mmSSR通过将质量指标细化为14种视觉-语言能力，并引入多模态评分器进行评估；同时，以交互风格作为多样性指标，利用多模态风格器识别数据模式。该方法无需复杂的聚类或贪婪采样，可扩展至数百万数据，并能针对特定能力进行定制。实验结果表明，mmSSR在多种设置下均优于随机采样和现有先进方法，仅用30%的数据量就达到了99.1%的完整性能。", "keywords": "多模态数据, 指令微调, 数据选择, 数据策展, MLLMs", "comments": "这篇论文的创新点在于其提出了一种无需嵌入聚类或贪婪采样即可高效扩展的多模态数据筛选框架mmSSR。通过将质量指标细化并引入交互风格作为多样性指标，mmSSR有效解决了多模态数据筛选中质量与多样性兼顾的挑战。其在多种基准测试中表现出的显著性能提升，尤其是在大幅减少数据量的情况下仍能保持高水平性能，凸显了该方法在提升多模态大语言模型指令微调效率和效果方面的重要性。"}}
{"id": "2503.13385", "pdf": "https://arxiv.org/pdf/2503.13385", "abs": "https://arxiv.org/abs/2503.13385", "authors": ["Qing Zhou", "Junyu Gao", "Qi Wang"], "title": "Scale Efficient Training for Large Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.", "AI": {"title_translation": "大规模数据集的尺度高效训练", "tldr": "SeTa提出了一种动态样本剪枝方法，通过移除低价值样本，在大规模数据集上显著降低训练成本，同时保持或提高模型性能。", "motivation": "随着数据集规模的增长，深度学习训练过程因存在冗余、过度挑战和低效的简单样本等低价值样本而变得低效，本研究旨在解决这一挑战。", "method": "本研究提出了尺度高效训练（SeTa）方法。SeTa首先进行随机剪枝以消除冗余样本；然后，根据损失值衡量学习难度对剩余样本进行聚类；最后，采用滑动窗口策略，遵循从易到难的课程，逐步移除过度挑战和低效的简单聚类。", "result": "SeTa在包含超过300万样本的大规模合成数据集（ToCa、SS1M、ST+MJ）上，将训练成本降低了高达50%，同时保持或提高了性能，即使成本降低70%也只有最小的性能下降。此外，在各种规模的真实数据集上，跨不同骨干网络（CNN、Transformers、Mambas）和多样化任务（指令调优、多视图立体、地理定位、组合图像检索、指代图像分割）的实验均证明了该方法的强大有效性和普适性。", "conclusion": "SeTa是一种有效且通用的动态样本剪枝方法，能够在大规模数据集上显著提高训练效率，同时保持或提升模型性能。", "translation": "数据集规模的快速增长一直是推动深度学习研究进展的关键驱动力。然而，随着数据集规模的增加，由于存在低价值样本，包括过度冗余的样本、挑战性过高的样本以及对模型改进贡献甚微的低效简单样本，训练过程变得越来越低效。为了解决这一挑战，我们提出了一种针对大规模数据集的尺度高效训练（SeTa）方法，这是一种无损减少训练时间的动态样本剪枝方法。为了移除低价值样本，SeTa首先进行随机剪枝以消除冗余样本，然后根据通过损失衡量的学习难度对剩余样本进行聚类。在此聚类基础上，采用滑动窗口策略，遵循从易到难的课程，逐步移除挑战性过高的聚类和低效的简单聚类。我们对大规模合成数据集进行了广泛实验，包括ToCa、SS1M和ST+MJ，每个数据集都包含超过300万个样本。SeTa将训练成本降低了高达50%，同时保持或提高了性能，即使成本降低70%也只有最小的性能下降。此外，对各种规模的真实数据集在各种骨干网络（CNN、Transformers和Mambas）和多样化任务（指令调优、多视图立体、地理定位、组合图像检索、指代图像分割）上的实验都证明了我们方法的强大有效性和普适性。代码可在https://github.com/mrazhou/SeTa获取。", "summary": "本论文提出了一种名为尺度高效训练（SeTa）的动态样本剪枝方法，旨在解决大规模数据集训练效率低下的问题。SeTa通过识别并移除低价值样本，包括冗余样本、过度挑战样本和低效简单样本，来实现无损训练时间缩减。其方法包括随机剪枝、基于损失的样本难度聚类，以及遵循从易到难课程的滑动窗口策略来移除低价值聚类。实验证明，SeTa在合成和真实大规模数据集上，能将训练成本降低高达50%甚至更多，同时保持或提升模型性能，并展现出在多种骨干网络和任务上的普适性。", "keywords": "尺度高效训练, 样本剪枝, 大规模数据集, 深度学习, 训练效率", "comments": "SeTa的创新之处在于其结合了随机剪枝、基于损失的难度聚类和滑动窗口策略的动态样本剪枝方法，有效解决了大规模数据集中低价值样本导致的训练效率问题。该方法在不牺牲性能的前提下显著降低了计算成本，对于当前深度学习领域面临的大模型、大数据集训练挑战具有重要意义。其在多种骨干网络和任务上的普适性也表明了其广泛的应用潜力。"}}
{"id": "2503.13399", "pdf": "https://arxiv.org/pdf/2503.13399", "abs": "https://arxiv.org/abs/2503.13399", "authors": ["James Burgess", "Jeffrey J Nirschl", "Laura Bravo-Sánchez", "Alejandro Lozano", "Sanket Rajan Gupte", "Jesus G. Galaz-Montoya", "Yuhui Zhang", "Yuchang Su", "Disha Bhowmik", "Zachary Coman", "Sarina M. Hasan", "Alexandra Johannesson", "William D. Leineweber", "Malvika G Nair", "Ridhi Yarlagadda", "Connor Zuraski", "Wah Chiu", "Sarah Cohen", "Jan N. Hansen", "Manuel D Leonetti", "Chad Liu", "Emma Lundberg", "Serena Yeung-Levy"], "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.CB"], "comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa", "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.", "AI": {"title_translation": "MicroVQA：一个基于显微镜的科学研究多模态推理基准", "tldr": "MicroVQA是一个新的VQA基准，用于评估AI在显微镜科学研究中复杂多模态推理能力，并揭示了当前MLLM在感知和知识方面的挑战。", "motivation": "现有的多模态推理基准仅达到大学水平或侧重低级感知，无法满足科学发现所需的复杂多模态推理能力，尤其是在生物学研究中。", "method": "引入MicroVQA，一个包含1,042个由生物学专家策划的多选VQA问题，涵盖多种显微镜模态，旨在评估专家图像理解、假设生成和实验提出三项推理能力。为避免语言捷径，开发了一个新的两阶段管道：首先通过LLM提示构建问答对，然后使用基于代理的“RefineBot”更新和移除捷径。", "result": "对最先进的MLLM进行基准测试显示，最高性能为53%；小型LLM模型的表现略低于顶级模型，表明基于语言的推理挑战性低于多模态推理；用科学文章进行微调可提高性能。专家对思维链响应的分析显示，感知错误最常见，其次是知识错误和过度泛化错误。", "conclusion": "MicroVQA揭示了多模态科学推理中的挑战，是推动AI驱动生物医学研究的宝贵资源。", "translation": "科学研究要求对多模态数据进行复杂的推理，这在生物学领域尤为普遍。尽管多模态大型语言模型（MLLM）在AI辅助研究方面取得了最新进展，但现有的多模态推理基准仅针对大学水平的难度，而研究级别的基准则强调低级感知，未能满足科学发现所需的复杂多模态推理。为了弥补这一差距，我们引入了MicroVQA，一个视觉问答（VQA）基准，旨在评估研究工作流程中至关重要的三种推理能力：专家图像理解、假设生成和实验提出。MicroVQA包含1,042个由生物学专家在不同显微镜模态下精心策划的多项选择题（MCQ），确保VQA样本代表真实的科学实践。在构建基准时，我们发现标准MCQ生成方法会引入语言捷径，因此我们提出了一种新的两阶段管道：优化的LLM提示将问答对构建成MCQ；然后，一个基于代理的“RefineBot”更新它们以消除捷径。对最先进的MLLM进行基准测试显示，最高性能为53%；较小的LLM模型仅略低于顶级模型，这表明基于语言的推理比多模态推理的挑战性更小；并且用科学文章进行微调可以提高性能。对思维链响应的专家分析表明，感知错误是最常见的，其次是知识错误，然后是过度泛化错误。这些见解突出了多模态科学推理中的挑战，表明MicroVQA是推动AI驱动生物医学研究的宝贵资源。MicroVQA可在https://huggingface.co/datasets/jmhb/microvqa获取，项目页面为https://jmhb0.github.io/microvqa。", "summary": "该论文介绍了MicroVQA，一个用于评估AI在显微镜科学研究中复杂多模态推理能力的视觉问答基准。它旨在弥补现有基准在研究级别推理方面的不足，特别关注专家图像理解、假设生成和实验提出。MicroVQA包含由生物学专家策划的1,042个多选问题，并采用了一种新颖的两阶段管道来消除语言捷径。基准测试结果显示，当前最先进的多模态大型语言模型在该任务上表现有限（最高53%），且主要挑战在于多模态推理本身而非语言推理，感知错误是主要瓶颈。该基准被认为是推动AI驱动生物医学研究的重要资源。", "keywords": "多模态推理, 视觉问答, 显微镜, 科学研究, 基准", "comments": "MicroVQA通过引入针对研究级别、显微镜图像的多模态推理基准，填补了现有AI辅助科学研究领域的一个重要空白。其创新之处在于不仅关注高级推理能力（假设生成、实验提出），还通过“RefineBot”机制有效解决了VQA基准中常见的语言捷径问题。基准测试结果清晰地指出了当前MLLM在科学领域多模态推理中的主要挑战，特别是图像感知和领域知识方面的不足，为未来模型的发展指明了方向。"}}
{"id": "2503.13424", "pdf": "https://arxiv.org/pdf/2503.13424", "abs": "https://arxiv.org/abs/2503.13424", "authors": ["Xinyu Lian", "Zichao Yu", "Ruiming Liang", "Yitong Wang", "Li Ray Luo", "Kaixu Chen", "Yuanzhen Zhou", "Qihong Tang", "Xudong Xu", "Zhaoyang Lyu", "Bo Dai", "Jiangmiao Pang"], "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation", "categories": ["cs.CV"], "comment": "Project page: https://infinite-mobility.github.io 10 pages,12 figures", "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility", "AI": {"title_translation": "无限机动性：通过程序生成实现可伸缩的高保真铰接对象合成", "tldr": "提出Infinite Mobility，一种通过程序生成合成大规模高保真铰接对象的方法，优于现有方法，并可用于训练生成模型。", "motivation": "具身AI任务急需大规模高质量的铰接对象，而现有数据驱动或基于仿真的方法受限于数据规模、质量、保真度或繁重的工作量。", "method": "提出Infinite Mobility，一种通过程序生成来合成高保真铰接对象的新方法。", "result": "用户研究和定量评估表明，该方法在物理属性和网格质量方面均优于当前最先进的方法，并与人工标注数据集相当。此外，合成数据可作为生成模型的训练数据，实现下一步的扩展。", "conclusion": "Infinite Mobility通过程序生成成功解决了大规模高保真铰接对象合成的挑战，其生成的数据具有高质量并可用于进一步的AI模型训练。", "translation": "具身AI相关的多项任务迫切需要大规模高质量的铰接对象。大多数现有创建铰接对象的方法要么是数据驱动的，要么是基于仿真的，这受限于训练数据的规模和质量，或者仿真的保真度和繁重的工作量。在本文中，我们提出了Infinite Mobility，一种通过程序生成合成高保真铰接对象的新方法。用户研究和定量评估表明，我们的方法在物理属性和网格质量方面均能产生优于当前最先进方法的结果，并与人工标注数据集相当。此外，我们展示了我们的合成数据可以作为生成模型的训练数据，从而实现下一步的扩展。代码可在https://github.com/Intern-Nexus/Infinite-Mobility获取。", "summary": "本文提出了“Infinite Mobility”，一种利用程序生成技术合成大规模高保真铰接对象的新方法。该方法旨在克服现有数据驱动和仿真方法的局限性，解决了具身AI对高质量铰接对象的需求。实验证明，“Infinite Mobility”在物理属性和网格质量上均优于现有最先进技术，并可生成高质量数据用于训练生成模型，具有良好的可扩展性。", "keywords": "铰接对象, 程序生成, 高保真合成, 具身AI, 合成数据", "comments": "该论文的创新点在于采用程序生成而非传统的数据驱动或仿真方法来合成铰接对象，有效解决了大规模高保真数据获取的难题。其重要性体现在为具身AI等领域提供了高质量、可扩展的合成数据来源，有助于推动相关研究进展。"}}
{"id": "2503.13429", "pdf": "https://arxiv.org/pdf/2503.13429", "abs": "https://arxiv.org/abs/2503.13429", "authors": ["Nhi Pham", "Bernt Schiele", "Adam Kortylewski", "Jonas Fischer"], "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes", "categories": ["cs.CV"], "comment": null, "summary": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness.", "AI": {"title_translation": "逃离柏拉图洞穴：通过可解释的3D神经对象体积实现鲁棒的概念推理", "tldr": "引入CAVE，一种结合可解释性和鲁棒性的图像分类器，通过从3D体积表示中提取概念，实现了概念一致性和优越的鲁棒性。", "motivation": "神经网络在关键应用中需要鲁棒性和可解释性以确保安全。尽管基于3D体积表示的分类器在分布外数据上表现出增强的鲁棒性，但它们的可解释性尚未得到充分研究。", "method": "提出CAVE（概念感知体积解释），通过从其体积表示中提取概念来扩展现有的3D感知分类器，设计了一个固有可解释且鲁棒的分类器，以统一图像分类中的可解释性和鲁棒性。", "result": "CAVE在可解释性定量指标上，与现有可解释AI文献中的概念方法相比，能够发现良好基础且在图像间一致使用的概念，同时实现了卓越的鲁棒性。", "conclusion": "CAVE成功地将可解释性和鲁棒性结合起来，为图像分类提供了一种新的、更安全的神经网络方法。", "translation": "随着神经网络的兴起，特别是在高风险应用中，这些网络需要具备(i)鲁棒性和(ii)可解释性两种特性以确保其安全性。最近在具有3D体积对象表示的分类器方面的进展，已经证明了在分布外数据中鲁棒性大大增强。然而，这些3D感知分类器尚未从可解释性的角度进行研究。我们引入了CAVE——概念感知体积解释——这是一个在图像分类中统一可解释性和鲁棒性的新方向。我们通过从其体积表示中提取概念用于分类，从而扩展现有3D感知分类器，设计了一个固有可解释且鲁棒的分类器。在一系列可解释性定量指标中，我们与可解释AI文献中不同的基于概念的方法进行了比较，结果表明CAVE能够发现良好基础且在图像间一致使用的概念，同时实现了卓越的鲁棒性。", "summary": "本文提出了CAVE（概念感知体积解释），一种新型的图像分类器，旨在解决神经网络在关键应用中对鲁棒性和可解释性的需求。CAVE通过扩展现有的3D感知分类器，从其体积表示中提取概念进行分类，从而实现固有的可解释性和增强的鲁棒性。实验结果表明，CAVE在可解释性方面优于其他概念方法，能够发现一致且有意义的概念，同时保持了卓越的分布外数据鲁棒性。", "keywords": "神经网络, 鲁棒性, 可解释性, 3D体积表示, 概念学习", "comments": "这篇论文通过引入CAVE，创新性地将神经网络的鲁棒性和可解释性这两个关键属性统一起来，特别是在3D感知分类器领域。其方法通过从体积表示中提取概念，为理解模型决策提供了更深层次的洞察，这对于高风险应用至关重要。该研究的贡献在于提供了一种既安全又透明的AI系统设计思路，有望推动可信AI的发展。"}}
{"id": "2503.13430", "pdf": "https://arxiv.org/pdf/2503.13430", "abs": "https://arxiv.org/abs/2503.13430", "authors": ["Thomas Monninger", "Md Zafar Anwar", "Stanislaw Antol", "Steffen Staab", "Sihao Ding"], "title": "AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Autonomous driving requires an understanding of the infrastructure elements,\nsuch as lanes and crosswalks. To navigate safely, this understanding must be\nderived from sensor data in real-time and needs to be represented in vectorized\nform. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set\nof camera images from multiple views into one joint latent BEV grid.\nTraditionally, from this latent space, an intermediate raster map is predicted,\nproviding dense spatial supervision but requiring post-processing into the\ndesired vectorized form. More recent models directly derive infrastructure\nelements as polylines using vectorized map decoders, providing instance-level\ninformation. Our approach, Augmentation Map Network (AugMapNet), proposes\nlatent BEV grid augmentation, a novel technique that significantly enhances the\nlatent BEV representation. AugMapNet combines vector decoding and dense spatial\nsupervision more effectively than existing architectures while remaining as\nstraightforward to integrate and as generic as auxiliary supervision.\nExperiments on nuScenes and Argoverse2 datasets demonstrate significant\nimprovements in vectorized map prediction performance up to 13.3% over the\nStreamMapNet baseline on 60m range and greater improvements on larger ranges.\nWe confirm transferability by applying our method to another baseline and find\nsimilar improvements. A detailed analysis of the latent BEV grid confirms a\nmore structured latent space of AugMapNet and shows the value of our novel\nconcept beyond pure performance improvement. The code will be released soon.", "AI": {"title_translation": "AugMapNet：通过BEV网格增强改进空间潜在结构，以增强矢量化在线高清地图构建", "tldr": "AugMapNet通过引入潜在BEV网格增强，显著提升了矢量化在线高清地图的构建性能，并改善了潜在BEV空间的结构化。", "motivation": "自动驾驶需要实时、准确地从传感器数据中提取并以矢量化形式表示基础设施元素（如车道、人行横道），而现有BEV编码器在生成联合潜在BEV网格方面仍有改进空间，以更好地支持矢量化地图的直接生成。", "method": "本文提出了Augmentation Map Network (AugMapNet) 方法，引入了一种新颖的潜在BEV网格增强技术。该方法更有效地结合了矢量解码和密集空间监督，旨在显著增强潜在BEV表示。", "result": "在nuScenes和Argoverse2数据集上的实验表明，AugMapNet在矢量化地图预测性能上比StreamMapNet基线在60米范围内提高了13.3%，在更大范围内有更大的改进。研究还证实了该方法的可迁移性，并在应用于其他基线时也取得了类似改进。对潜在BEV网格的详细分析证实了AugMapNet具有更结构化的潜在空间。", "conclusion": "AugMapNet通过其新颖的潜在BEV网格增强概念，不仅显著提升了矢量化在线高清地图构建的性能，更重要的是，它改善了潜在BEV网格的结构化，证明了其超越纯粹性能提升的价值。", "translation": "自动驾驶需要理解基础设施元素，例如车道和人行横道。为了安全导航，这种理解必须实时从传感器数据中获取，并以矢量化形式表示。学习到的鸟瞰图（BEV）编码器通常用于将多视图的相机图像集组合成一个联合潜在BEV网格。传统上，从这个潜在空间预测中间栅格地图，提供密集的空间监督，但需要后处理才能转换为所需的矢量化形式。最近的模型使用矢量化地图解码器直接将基础设施元素提取为折线，提供实例级信息。我们的方法，增强地图网络（AugMapNet），提出了潜在BEV网格增强，这是一种显著增强潜在BEV表示的新颖技术。AugMapNet比现有架构更有效地结合了矢量解码和密集空间监督，同时保持了与辅助监督一样易于集成和通用。在nuScenes和Argoverse2数据集上的实验表明，在60米范围内，矢量化地图预测性能比StreamMapNet基线显著提高了13.3%，在更大范围内有更大的改进。我们通过将我们的方法应用于另一个基线并发现类似的改进来证实了可迁移性。对潜在BEV网格的详细分析证实了AugMapNet具有更结构化的潜在空间，并显示了我们新颖概念超越纯粹性能改进的价值。代码即将发布。", "summary": "AugMapNet是一种新颖的方法，通过引入潜在BEV网格增强来显著提升在线高清地图的矢量化构建。它有效地结合了矢量解码和密集空间监督，在nuScenes和Argoverse2数据集上，相比现有基线（如StreamMapNet），在矢量化地图预测性能上取得了高达13.3%的显著提升。此外，通过对潜在BEV网格的分析，证实了AugMapNet能够产生更结构化的潜在空间，展现了其通用性和超越纯粹性能的价值。", "keywords": "BEV网格增强, 矢量化地图构建, 自动驾驶, 潜在空间, HD地图", "comments": "本文提出了一种新颖的BEV网格增强技术，不仅在矢量化地图构建性能上取得了显著提升，更重要的是，它改善了潜在BEV空间的结构化。这种对潜在表示的优化对于自动驾驶中基础设施元素的准确和实时感知至关重要，显示了其在表示学习方面的创新性以及对下游任务的深远影响。"}}
{"id": "2503.13433", "pdf": "https://arxiv.org/pdf/2503.13433", "abs": "https://arxiv.org/abs/2503.13433", "authors": ["Johan Edstedt"], "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC", "categories": ["cs.CV"], "comment": null, "summary": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1.", "AI": {"title_translation": "用于阈值鲁棒RANSAC的偏差更小的噪声尺度估计", "tldr": "RANSAC的内点阈值难以手动设置。本文通过修正SIMFIT噪声尺度估计方法的偏差，并提出其多对扩展，实现了更鲁棒的噪声尺度估计，从而自动确定最优阈值。", "motivation": "RANSAC在图像匹配中估计相对姿态时，需要手动设置内点阈值，这很难调整且通常需要地面真实数据。因此，需要一种能够自动确定最优阈值的方法。", "method": "本文重新审视了内点噪声尺度估计，因为它与最优阈值呈线性关系。作者重新审视了SIMFIT噪声尺度估计方法，并修正了其估计中的偏差，特别解决了使用相同数据进行模型拟合和内点噪声估计以及未将阈值本身考虑在内导致的低估问题。其次，作者提出SIMFIT++的多对扩展，通过过滤估计来改进结果，因为场景内的最优阈值近似恒定。", "result": "我们的方法在不同阈值范围内表现出鲁棒的性能。", "conclusion": "本文通过修正SIMFIT噪声尺度估计方法的偏差并引入多对扩展，有效解决了RANSAC内点阈值手动设置的难题，实现了自动且鲁棒的阈值确定。", "translation": "通过图像匹配鲁棒地估计相对姿态的黄金标准是RANSAC。虽然RANSAC功能强大，但它需要设置内点阈值，该阈值决定了在估计模型下对应关系的误差是否足够小以包含在其共识集中。设置此阈值通常是手动完成的，并且在没有地面真实数据的情况下很难调整。因此，一种能够自动确定最佳阈值的方法是可取的。在本文中，我们重新审视了内点噪声尺度估计，这是一种有吸引力的方法，因为内点噪声尺度与最佳阈值呈线性关系。我们重新审视了噪声尺度估计方法SIMFIT，发现噪声尺度估计中存在偏差。特别是，我们修正了由于使用相同数据进行模型拟合和内点噪声估计以及未将阈值本身考虑在内而导致的低估问题。其次，由于场景内的最佳阈值近似恒定，我们提出了SIMFIT++的多对扩展，通过过滤估计来改进结果。我们的方法在不同阈值范围内表现出鲁棒的性能，如图1所示。", "summary": "本文关注RANSAC中手动设置内点阈值的挑战。作者重新审视了内点噪声尺度估计方法SIMFIT，并发现了其估计中存在的偏差。通过修正使用相同数据进行模型拟合和噪声估计的低估问题，并考虑阈值本身，他们改进了SIMFIT。此外，他们提出了SIMFIT++的多对扩展，利用场景内阈值近似恒定的特性，通过过滤估计进一步提升了性能。该方法在各种阈值下均表现出鲁棒的性能，实现了更自动、更准确的RANSAC阈值确定。", "keywords": "RANSAC, 噪声尺度估计, 阈值, SIMFIT, 鲁棒性", "comments": "这篇论文的创新点在于它解决了RANSAC中一个实际且普遍存在的问题——内点阈值的设置。通过深入分析并修正现有噪声尺度估计方法SIMFIT的偏差，并引入多对扩展，该研究提供了一种更自动化、更鲁棒的解决方案。其重要性在于能够减少对人工调参的依赖，提高RANSAC在实际应用中的易用性和性能。"}}
{"id": "2503.13434", "pdf": "https://arxiv.org/pdf/2503.13434", "abs": "https://arxiv.org/abs/2503.13434", "authors": ["Yaowei Li", "Lingen Li", "Zhaoyang Zhang", "Xiaoyu Li", "Guangzhi Wang", "Hongxiang Li", "Xiaodong Cun", "Ying Shan", "Yuexian Zou"], "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/", "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/", "AI": {"title_translation": "BlobCtrl: 统一灵活的元素级图像生成与编辑框架", "tldr": "BlobCtrl是一个统一的、基于概率blob的框架，用于精确灵活的元素级图像生成和编辑，解决了现有扩散模型缺乏精度和灵活性的问题。", "motivation": "当前的扩散模型在元素级视觉操作中缺乏传统工具的精度和灵活性。", "method": "引入BlobCtrl框架，使用概率blob表示作为视觉原语，解耦并表示空间位置、语义内容和身份信息。关键贡献包括：1) 带有分层特征融合的双分支扩散架构；2) 带有定制数据增强和分数函数的自监督训练范式；3) 可控的dropout策略平衡保真度和多样性。同时引入BlobData用于大规模训练和BlobBench用于系统评估。", "result": "实验表明BlobCtrl在各种元素级操作任务中表现出色，同时保持计算效率。", "conclusion": "BlobCtrl提供了一个实用解决方案，用于精确灵活的视觉内容创作。", "translation": "元素级视觉操作在数字内容创作中至关重要，但当前的基于扩散的方法缺乏传统工具的精度和灵活性。在这项工作中，我们引入了BlobCtrl，一个使用概率blob表示统一元素级生成和编辑的框架。通过将blob作为视觉原语，我们的方法有效地解耦并表示空间位置、语义内容和身份信息，从而实现精确的元素级操作。我们的主要贡献包括：1）一个带有分层特征融合的双分支扩散架构，用于无缝的前景-背景整合；2）一个带有定制数据增强和分数函数的自监督训练范式；以及3）可控的dropout策略，以平衡保真度和多样性。为了支持进一步的研究，我们引入了BlobData用于大规模训练和BlobBench用于系统评估。实验表明，BlobCtrl在各种元素级操作任务中表现出色，同时保持计算效率，为精确灵活的视觉内容创作提供了实用解决方案。项目页面：https://liyaowei-stu.github.io/project/BlobCtrl/", "summary": "BlobCtrl是一个创新的框架，旨在解决现有扩散模型在元素级图像生成和编辑中精度和灵活性不足的问题。它采用概率blob作为视觉原语，有效解耦并处理图像元素的空间、语义和身份信息。该框架包含双分支扩散架构、自监督训练范式和可控dropout策略。通过引入BlobData和BlobBench，BlobCtrl在多项任务中展现出卓越的性能和计算效率，为视觉内容创作提供了一种实用且精确的解决方案。", "keywords": "元素级图像生成, 图像编辑, 扩散模型, blob表示, 视觉操作", "comments": "BlobCtrl的创新点在于其统一的、基于概率blob的表示，有效地解耦了图像元素的各项属性，从而实现了比传统扩散模型更精细的控制。其双分支架构和自监督训练范式也增强了实用性。该框架通过提供BlobData和BlobBench，为未来的研究奠定了基础，具有重要的研究和应用价值。"}}
{"id": "2503.13435", "pdf": "https://arxiv.org/pdf/2503.13435", "abs": "https://arxiv.org/abs/2503.13435", "authors": ["Ling Yang", "Kaixin Zhu", "Juanxi Tian", "Bohan Zeng", "Mingbao Lin", "Hongjuan Pei", "Wentao Zhang", "Shuicheng Yan"], "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/WideRange4D", "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D", "AI": {"title_translation": "WideRange4D：实现大范围运动和场景的高质量4D重建", "tldr": "本文提出了一个用于大范围运动的4D重建基准WideRange4D和一种新的4D重建方法Progress4D，该方法在复杂场景中表现优异。", "motivation": "现有4D重建基准和方法在处理大范围空间运动场景时存在局限性，特别是在多视角视频数据获取和形变场对大范围运动的估计方面。", "method": "本文提出了一个新的4D重建基准WideRange4D，包含大量具有大空间变化的4D场景数据。同时，引入了一种新的4D重建方法Progress4D，旨在生成稳定且高质量的大范围复杂4D场景重建结果。", "result": "在WideRange4D基准上进行的定量和定性比较实验表明，Progress4D优于现有的最先进4D重建方法。", "conclusion": "通过引入WideRange4D基准和Progress4D方法，本文成功解决了现有4D重建技术在处理大范围空间运动场景时的局限性，并实现了更高质量的4D重建。", "translation": "随着3D重建技术的快速发展，4D重建研究也在不断进步，现有的4D重建方法能够生成高质量的4D场景。然而，由于获取多视角视频数据的挑战，当前的4D重建基准主要展示在有限场景内进行的原地动作，例如舞蹈。在实际场景中，许多场景涉及大范围的空间运动，这突显了现有4D重建数据集的局限性。此外，现有4D重建方法依赖于形变场来估计3D物体的动态，但形变场难以处理大范围空间运动，这限制了在大范围空间运动下实现高质量4D场景重建的能力。在本文中，我们专注于具有显著物体空间运动的4D场景重建，并提出了一个新的4D重建基准WideRange4D。该基准包含丰富的大空间变化的4D场景数据，可以更全面地评估4D生成方法的生成能力。此外，我们引入了一种新的4D重建方法Progress4D，它能在各种复杂的4D场景重建任务中生成稳定且高质量的4D结果。我们在WideRange4D上进行了定量和定性比较实验，结果表明我们的Progress4D优于现有的最先进4D重建方法。项目地址：https://github.com/Gen-Verse/WideRange4D", "summary": "本文针对现有4D重建方法在处理大范围空间运动场景时的局限性，提出了一个新的4D重建基准WideRange4D，该基准包含了丰富的大范围运动数据。同时，本文还提出了一种名为Progress4D的新型4D重建方法，该方法在WideRange4D基准上进行了验证，并被证明在处理复杂4D场景重建任务时，能生成比现有先进方法更稳定和高质量的4D结果。", "keywords": "4D重建, 大范围运动, 数据集, 基准, Progress4D", "comments": "本文的创新点在于提出了一个专门针对大范围运动的4D重建基准WideRange4D，填补了现有数据集的空白。同时，所提出的Progress4D方法有效地解决了形变场在大范围运动中的局限性，提升了4D重建的质量和稳定性。这项工作对于推动4D重建技术在更广阔实际应用场景中的发展具有重要意义。"}}
{"id": "2503.13436", "pdf": "https://arxiv.org/pdf/2503.13436", "abs": "https://arxiv.org/abs/2503.13436", "authors": ["Lijie Fan", "Luming Tang", "Siyang Qin", "Tianhong Li", "Xuan Yang", "Siyuan Qiao", "Andreas Steiner", "Chen Sun", "Yuanzhen Li", "Tao Zhu", "Michael Rubinstein", "Michalis Raptis", "Deqing Sun", "Radu Soricut"], "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Tech report", "summary": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.", "AI": {"title_translation": "基于连续标记的统一自回归视觉生成与理解", "tldr": "UniFluid是一个统一的自回归框架，利用连续视觉标记实现联合视觉生成和理解。它在图像生成和理解任务上表现出色，甚至超越了单一任务基线，并展示了强大的迁移能力。", "motivation": "尽管图像生成和理解任务之间存在固有的权衡，但通过精心调整的训练方法可以使它们相互促进，从而实现一个统一的模型来解决这两个任务。", "method": "本文提出了UniFluid，一个统一的自回归框架，用于联合视觉生成和理解。该框架利用连续视觉标记，并采用自回归架构处理多模态图像和文本输入，为文本生成离散标记，为图像生成连续标记。通过选择合适的损失平衡权重，并采用更强的预训练LLMs和随机顺序生成进行训练，以实现高保真图像生成。该模型基于Gemma系列模型构建。", "result": "UniFluid在图像生成和理解任务上取得了与单一任务基线相当或超越的结果。它实现了高保真图像生成，并在各种下游任务（包括图像编辑、视觉字幕和问答）上展示了强大的可迁移性。", "conclusion": "UniFluid作为基于连续标记的统一自回归框架，在视觉生成和理解方面表现出竞争力，证明了通过精心设计的训练可以有效平衡并提升多任务性能，并具有良好的泛化能力。", "translation": "我们提出了UniFluid，一个利用连续视觉标记进行联合视觉生成和理解的统一自回归框架。我们统一的自回归架构处理多模态图像和文本输入，为文本生成离散标记，为图像生成连续标记。我们发现，尽管图像生成和理解任务之间存在固有的权衡，但精心调整的训练方法使它们能够相互促进。通过选择适当的损失平衡权重，统一模型在这两项任务上取得了与单一任务基线相当或超越的结果。此外，我们证明在训练过程中采用更强的预训练LLM和随机顺序生成对于在该统一框架内实现高保真图像生成至关重要。UniFluid基于Gemma模型系列构建，在图像生成和理解方面均表现出竞争力，并展示了对各种下游任务的强大可迁移性，包括用于生成的图像编辑，以及用于理解的视觉字幕和问答。", "summary": "UniFluid是一个统一的自回归框架，旨在联合进行视觉生成和理解，它通过利用连续的视觉标记实现。该框架采用多模态输入（图像和文本），并分别为其生成连续和离散标记。研究发现，通过精心调优的训练策略和平衡损失权重，UniFluid能够克服生成与理解之间的固有权衡，并在两项任务上均达到或超越单一任务基线的性能。此外，结合强大的预训练大型语言模型（LLMs）和随机顺序生成对于实现高保真图像生成至关重要。基于Gemma系列模型，UniFluid展现出在图像生成和理解方面的竞争性表现，并对图像编辑、视觉字幕和问答等下游任务具有强大的迁移能力。", "keywords": "统一模型, 自回归, 视觉生成, 视觉理解, 连续标记", "comments": "这篇论文的创新点在于提出了一个统一的自回归框架UniFluid，它巧妙地结合了视觉生成和理解任务，并引入了连续视觉标记。通过精心设计的训练策略和对损失平衡的优化，该模型能够有效处理多任务间的权衡，并取得了超越单一任务基线的表现。其基于Gemma模型系列并利用强大LLMs的特点，使其在性能和泛化能力上具有重要意义。该框架为多模态统一模型的发展提供了新的思路和实践。"}}
{"id": "2503.13439", "pdf": "https://arxiv.org/pdf/2503.13439", "abs": "https://arxiv.org/abs/2503.13439", "authors": ["Tianhao Wu", "Chuanxia Zheng", "Frank Guan", "Andrea Vedaldi", "Tat-Jen Cham"], "title": "Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images", "categories": ["cs.CV"], "comment": "Project Page: https://sm0kywu.github.io/Amodal3R/", "summary": "Most image-based 3D object reconstructors assume that objects are fully\nvisible, ignoring occlusions that commonly occur in real-world scenarios. In\nthis paper, we introduce Amodal3R, a conditional 3D generative model designed\nto reconstruct 3D objects from partial observations. We start from a\n\"foundation\" 3D generative model and extend it to recover plausible 3D geometry\nand appearance from occluded objects. We introduce a mask-weighted multi-head\ncross-attention mechanism followed by an occlusion-aware attention layer that\nexplicitly leverages occlusion priors to guide the reconstruction process. We\ndemonstrate that, by training solely on synthetic data, Amodal3R learns to\nrecover full 3D objects even in the presence of occlusions in real scenes. It\nsubstantially outperforms existing methods that independently perform 2D amodal\ncompletion followed by 3D reconstruction, thereby establishing a new benchmark\nfor occlusion-aware 3D reconstruction.", "AI": {"title_translation": "Amodal3R：从遮挡的2D图像进行非模态3D重建", "tldr": "Amodal3R是一个新的条件3D生成模型，能够从部分观察（遮挡）中重建完整的3D物体，显著优于现有方法，并为遮挡感知3D重建设定了新基准。", "motivation": "大多数基于图像的3D物体重建器都假设物体是完全可见的，忽略了现实世界中常见的遮挡情况，导致它们无法处理部分观察到的物体。", "method": "本文引入了Amodal3R，一个条件3D生成模型，旨在从部分观察中重建3D物体。它从一个“基础”3D生成模型开始，并对其进行扩展，以从被遮挡的物体中恢复合理的3D几何形状和外观。该方法引入了一种掩码加权的Muti-head交叉注意力机制，后跟一个遮挡感知注意力层，明确利用遮挡先验来指导重建过程。", "result": "Amodal3R仅通过合成数据训练，就能从真实场景中的遮挡情况下恢复完整的3D物体。它显著优于现有独立执行2D非模态补全再进行3D重建的方法，从而为遮挡感知3D重建建立了新的基准。", "conclusion": "Amodal3R成功地解决了从遮挡的2D图像进行3D重建的挑战，通过直接生成完整物体而非两阶段方法，实现了卓越的性能，并为该领域设立了新的标准。", "translation": "大多数基于图像的3D物体重建器都假设物体是完全可见的，忽略了现实世界中常见的遮挡情况。在本文中，我们引入了Amodal3R，一个条件3D生成模型，旨在从部分观察中重建3D物体。我们从一个“基础”3D生成模型开始，并对其进行扩展，以从被遮挡的物体中恢复合理的3D几何形状和外观。我们引入了一种掩码加权的Muti-head交叉注意力机制，后跟一个遮挡感知注意力层，明确利用遮挡先验来指导重建过程。我们证明，仅通过合成数据训练，Amodal3R就能从真实场景中的遮挡情况下恢复完整的3D物体。它显著优于现有独立执行2D非模态补全再进行3D重建的方法，从而为遮挡感知3D重建建立了新的基准。", "summary": "Amodal3R是一个针对从遮挡2D图像进行非模态3D重建的条件3D生成模型。它通过扩展基础生成模型并引入掩码加权的Muti-head交叉注意力机制和遮挡感知注意力层，有效利用遮挡先验来恢复完整的3D物体。该模型仅通过合成数据训练，便能成功处理真实场景中的遮挡，并且在性能上显著超越了传统的两阶段方法，为遮挡感知3D重建领域树立了新的里程碑。", "keywords": "非模态3D重建, 遮挡, 生成模型, 注意力机制, 部分观察", "comments": "本文的创新之处在于直接构建了一个能够处理遮挡的非模态3D重建模型，而非依赖于先进行2D补全再进行3D重建的两阶段方法。其利用专门的注意力机制来融入遮挡先验信息，以及仅通过合成数据训练就能泛化到真实场景的能力，是其重要的贡献和亮点。"}}
{"id": "2503.13440", "pdf": "https://arxiv.org/pdf/2503.13440", "abs": "https://arxiv.org/abs/2503.13440", "authors": ["Yingyue Li", "Bencheng Liao", "Wenyu Liu", "Xinggang Wang"], "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling", "categories": ["cs.CV"], "comment": "Code and model are available at http://github.com/hustvl/MaTVLM", "summary": "With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http://github.com/hustvl/MaTVLM.", "AI": {"title_translation": "MaTVLM：用于高效视觉-语言建模的混合Mamba-Transformer模型", "tldr": "MaTVLM是一个混合Mamba-Transformer模型，通过用Mamba-2层替换部分Transformer解码器层，并结合注意力权重初始化和知识蒸馏，显著提高了视觉-语言模型的推理速度和内存效率，同时保持了性能。", "motivation": "Transformer模型存在二次复杂度问题，而RNN模型（如Mamba-2）虽然具有线性复杂度，但难以捕获长距离依赖、收敛缓慢、资源需求高，并在下游理解和复杂推理任务上表现不佳。", "method": "本研究提出了混合模型MaTVLM，通过在预训练的VLM中用Mamba-2层替换部分Transformer解码器层。为了加速收敛，利用注意力与Mamba-2的内在关系，用相应的注意力权重初始化Mamba-2。随后，采用单阶段蒸馏过程，使用预训练的VLM作为教师模型将知识转移到MaTVLM，进一步提高收敛速度和性能。此外，还研究了训练框架中差分蒸馏损失的影响。", "result": "MaTVLM在多个基准测试中表现出与教师模型和现有VLM相当的竞争力，同时超越了基于Mamba的VLM和参数规模相当的模型。MaTVLM的推理速度比教师模型快3.6倍，GPU内存消耗减少27.5%，且性能没有下降。", "conclusion": "MaTVLM是一个高效且高性能的混合视觉-语言模型，它成功地结合了Mamba-2的效率和Transformer的强大能力，解决了现有模型在复杂度和资源方面的挑战，同时在性能上取得了显著提升。", "translation": "随着具有线性复杂度的RNN模型的进步，Transformer的二次复杂度挑战有望被克服。值得注意的是，新兴的Mamba-2展示了竞争性能，弥合了RNN模型和Transformer之间的差距。然而，由于顺序处理和梯度消失问题，RNN模型难以捕获长距离依赖，限制了上下文理解。这导致收敛缓慢、资源需求高，以及在下游理解和复杂推理任务上表现不佳。在这项工作中，我们提出了一个混合模型MaTVLM，通过在预训练的VLM中用Mamba-2层替换部分Transformer解码器层。利用注意力与Mamba-2的内在关系，我们用相应的注意力权重初始化Mamba-2以加速收敛。随后，我们采用单阶段蒸馏过程，使用预训练的VLM作为教师模型将知识转移到MaTVLM，进一步提高收敛速度和性能。此外，我们研究了训练框架中差分蒸馏损失的影响。我们在多个基准测试中评估了MaTVLM，证明了其与教师模型和现有VLM相比具有竞争性能，同时超越了基于Mamba的VLM和参数规模相当的模型。值得注意的是，MaTVLM的推理速度比教师模型快3.6倍，同时将GPU内存消耗降低了27.5%，所有这些都没有牺牲性能。代码和模型已在http://github.com/hustvl/MaTVLM发布。", "summary": "MaTVLM是一个创新的混合视觉-语言模型，旨在解决Transformer的二次复杂度和RNN在长距离依赖上的不足。它通过将Mamba-2层嵌入到预训练VLM的Transformer解码器中，并结合注意力权重初始化和知识蒸馏策略，显著提升了模型效率。实验结果表明，MaTVLM在保持甚至超越现有模型性能的同时，实现了高达3.6倍的推理速度提升和27.5%的GPU内存节省。", "keywords": "视觉-语言建模, Mamba-Transformer, 混合模型, 知识蒸馏, 高效推理", "comments": "MaTVLM的创新之处在于其混合架构，巧妙地结合了Mamba-2的线性复杂度和Transformer的强大建模能力。通过引入注意力权重初始化和知识蒸馏，该模型不仅克服了Mamba-2在长距离依赖上的局限，还加速了收敛并提升了性能。其在推理速度和内存效率上的显著提升，对于实际部署和资源受限环境下的视觉-语言任务具有重要意义。"}}
{"id": "2503.13443", "pdf": "https://arxiv.org/pdf/2503.13443", "abs": "https://arxiv.org/abs/2503.13443", "authors": ["Haoyang Li", "Liang Wang", "Chao Wang", "Jing Jiang", "Yan Peng", "Guodong Long"], "title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 (CVPR 2025)", "summary": "The Base-New Trade-off (BNT) problem universally exists during the\noptimization of CLIP-based prompt tuning, where continuous fine-tuning on base\n(target) classes leads to a simultaneous decrease of generalization ability on\nnew (unseen) classes. Existing approaches attempt to regulate the prompt tuning\nprocess to balance BNT by appending constraints. However, imposed on the same\ntarget prompt, these constraints fail to fully avert the mutual exclusivity\nbetween the optimization directions for base and new. As a novel solution to\nthis challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)\nframework, the first that decoupling the optimization processes of base and new\ntasks at the prompt level. Specifically, we clone a learnable parallel prompt\nbased on the backbone prompt, and introduce a variable Weighting-Decoupling\nframework to independently control the optimization directions of dual prompts\nspecific to base or new tasks, thus avoiding the conflict in generalization.\nMeanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts\nto construct a more challenging optimization task on base classes for\nenhancement. For interpretability, we prove the feature channel invariance of\nthe prompt vector during the optimization process, providing theoretical\nsupport for the Weighting-Decoupling of DPC. Extensive experiments on multiple\nbackbones demonstrate that DPC can significantly improve base performance\nwithout introducing any external knowledge beyond the base classes, while\nmaintaining generalization to new classes. Code is available at:\nhttps://github.com/JREion/DPC.", "AI": {"title_translation": "DPC：用于调整视觉-语言模型的双提示协作", "tldr": "提出DPC框架，通过双提示解耦基类和新类任务的优化，解决CLIP提示微调中的基类-新类权衡问题，显著提升基类性能并保持新类泛化能力。", "motivation": "CLIP提示微调中存在基类-新类权衡（BNT）问题，即对基类进行连续微调会导致对新类泛化能力下降。现有方法通过约束来平衡，但在同一目标提示上无法完全避免基类和新类优化方向的互斥性。", "method": "提出DPC（Dual-Prompt Collaboration）框架，首次在提示级别解耦基类和新类任务的优化过程。具体包括：1) 克隆一个可学习的并行提示；2) 引入可变加权解耦框架独立控制双提示针对基类或新类任务的优化方向；3) 提出动态难负样本优化器，利用双提示构建更具挑战性的基类优化任务；4) 证明提示向量的特征通道不变性，为加权解耦提供理论支持。", "result": "在多个骨干网络上进行的大量实验表明，DPC在不引入基类之外的任何外部知识的情况下，可以显著提高基类性能，同时保持对新类的泛化能力。", "conclusion": "DPC框架通过解耦优化过程有效解决了CLIP提示微调中的基类-新类权衡问题，实现了基类性能的显著提升和新类泛化能力的保持。", "translation": "CLIP提示微调优化中普遍存在基新权衡（BNT）问题，即对基（目标）类进行连续微调会导致对新（未见）类的泛化能力同时下降。现有方法试图通过附加约束来规范提示微调过程以平衡BNT。然而，施加在同一目标提示上，这些约束未能完全避免基类和新类优化方向之间的互斥性。作为应对这一挑战的新颖解决方案，我们提出了即插即用的双提示协作（DPC）框架，这是第一个在提示级别解耦基类和新类任务优化过程的框架。具体来说，我们基于骨干提示克隆了一个可学习的并行提示，并引入了一个可变加权解耦框架，以独立控制特定于基类或新类任务的双提示的优化方向，从而避免了泛化冲突。同时，我们提出了一个动态难负样本优化器，利用双提示在基类上构建更具挑战性的优化任务以进行增强。为了可解释性，我们证明了提示向量在优化过程中的特征通道不变性，为DPC的加权解耦提供了理论支持。在多个骨干网络上进行的大量实验表明，DPC在不引入基类之外的任何外部知识的情况下，可以显著提高基类性能，同时保持对新类的泛化能力。代码可在https://github.com/JREion/DPC 获取。", "summary": "本文提出了DPC（Dual-Prompt Collaboration）框架，旨在解决CLIP提示微调中基类-新类权衡问题。DPC通过引入双提示和可变加权解耦框架，首次在提示级别解耦基类和新类任务的优化，从而避免了优化方向的冲突。此外，DPC还设计了动态难负样本优化器以增强基类任务。理论上，论文证明了提示向量的特征通道不变性。实验结果表明，DPC在不引入额外知识的情况下显著提升了基类性能，并保持了对新类的泛化能力。", "keywords": "视觉-语言模型, 提示学习, 基新权衡, 双提示协作, 泛化", "comments": "DPC框架的创新之处在于其首次在提示级别解耦了基类和新类任务的优化，通过双提示协作有效解决了CLIP微调中的基类-新类权衡问题。这种即插即用的设计使其具有较高的实用性。理论证明也增强了其可解释性。"}}
{"id": "2503.13444", "pdf": "https://arxiv.org/pdf/2503.13444", "abs": "https://arxiv.org/abs/2503.13444", "authors": ["Ye Liu", "Kevin Qinghong Lin", "Chang Wen Chen", "Mike Zheng Shou"], "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://videomind.github.io/", "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.", "AI": {"title_translation": "VideoMind：一种用于长视频推理的Chain-of-LoRA智能体", "tldr": "VideoMind是一个用于长视频理解的视频语言智能体，它通过角色代理工作流和Chain-of-LoRA策略，在多项视频理解任务上实现了最先进的性能。", "motivation": "尽管大型语言模型在推理能力上取得了突破，但多模态推理（尤其是视频）仍未被充分探索，视频需要精确的、与视觉证据直接关联的接地理解。", "method": "本文提出VideoMind，一个用于时间接地视频理解的新型视频-语言智能体。它包含两个关键创新：(i) 识别视频时间推理的基本能力，并开发基于角色的代理工作流，包括规划器、接地器、验证器和回答器。(ii) 提出Chain-of-LoRA策略，通过轻量级LoRA适配器实现无缝角色切换，避免多模型开销，平衡效率和灵活性。", "result": "在14个公共基准测试中，VideoMind在各种视频理解任务（包括3个接地视频问答、6个视频时间接地和5个通用视频问答）上实现了最先进的性能。", "conclusion": "VideoMind有效推动了视频智能体和长视频时间推理的进步。", "translation": "视频以其独特的时间维度，要求精确的接地理解，即答案直接与视觉的、可解释的证据相关联。尽管大型语言模型在推理能力方面取得了重大突破，但多模态推理——尤其是对于视频——仍未被充分探索。在这项工作中，我们介绍了VideoMind，这是一种专为时间接地视频理解而设计的新型视频-语言智能体。VideoMind包含两个关键创新：(i) 我们确定了视频时间推理的基本能力，并开发了一种基于角色的代理工作流，包括用于协调不同角色的规划器、用于时间定位的接地器、用于评估时间间隔准确性的验证器以及用于问答的回答器。(ii) 为了有效地整合这些不同的角色，我们提出了一种新颖的Chain-of-LoRA策略，通过轻量级LoRA适配器实现无缝的角色切换，同时避免了多模型的开销，从而平衡了效率和灵活性。在14个公共基准测试中进行的广泛实验表明，我们的智能体在各种视频理解任务上实现了最先进的性能，其中包括3个接地视频问答、6个视频时间接地和5个通用视频问答，这突显了它在推进视频智能体和长视频时间推理方面的有效性。", "summary": "本文提出VideoMind，一个用于时间接地长视频理解的视频-语言智能体。它通过创新的基于角色的代理工作流（包括规划器、接地器、验证器和回答器）和新颖的Chain-of-LoRA策略（通过轻量级LoRA适配器实现高效角色切换），解决了视频多模态推理的挑战。在14个公共基准测试上的广泛实验证明，VideoMind在多种视频理解任务上达到了最先进的性能，有效提升了视频智能体和长视频时间推理能力。", "keywords": "视频理解, 视频推理, Chain-of-LoRA, 多模态智能体, 长视频", "comments": "VideoMind的创新点在于其将复杂视频理解任务分解为多个角色代理，并通过Chain-of-LoRA策略高效地管理这些角色，避免了多模型部署的复杂性和开销。这种方法在平衡效率和灵活性方面具有重要意义，为长视频推理提供了一个通用且高性能的框架。"}}
{"id": "2503.11677", "pdf": "https://arxiv.org/pdf/2503.11677", "abs": "https://arxiv.org/abs/2503.11677", "authors": ["Jungyeon Park", "Anna Kochnev Goldstein", "Yueming Zhou", "Nathan Jensen", "Daniel Palanker"], "title": "Simulation of prosthetic vision with PRIMA system and enhancement of face representation", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Objective. Patients implanted with the PRIMA photovoltaic subretinal\nprosthesis in geographic atrophy report form vision with the average acuity\nmatching the 100um pixel size. Although this remarkable outcome enables them to\nread and write, they report difficulty with perceiving faces. This paper\nprovides a novel, non-pixelated algorithm for simulating prosthetic vision the\nway it is experienced by PRIMA patients, compares the algorithm's predictions\nto clinical perceptual outcomes, and offers computer vision and machine\nlearning (ML) methods to improve face representation. Approach. Our simulation\nalgorithm integrates a grayscale filter, spatial resolution filter, and\ncontrast filter. This accounts for the limited sampling density of the retinal\nimplant, as well as the reduced contrast sensitivity of prosthetic vision.\nPatterns of Landolt C and faces created using this simulation algorithm are\ncompared to reports from actual PRIMA users. To recover the facial features\nlost in prosthetic vision, we apply an ML facial landmarking model as well as\ncontrast adjusting tone curves to the face image prior to its projection onto\nthe implant. Main results. Simulated prosthetic vision matches the maximum\nletter acuity observed in clinical studies as well as patients' subjective\ndescriptions. Application of the inversed contrast filter helps preserve the\ncontrast in prosthetic vision. Identification of the facial features using an\nML facial landmarking model and accentuating them further improve face\nrepresentation. Significance. Spatial and contrast constraints of prosthetic\nvision limit resolvable features and degrade natural images. ML based methods\nand contrast adjustments mitigate some limitations and improve face\nrepresentation. Even though higher spatial resolution can be expected with\nimplants having smaller pixels, contrast enhancement still remains essential\nfor face recognition.", "AI": {"title_translation": "PRIMA系统假体视觉模拟与面部表征增强", "tldr": "本文提出了一种模拟PRIMA假体视觉的新算法，并利用机器学习方法改善了假体视觉中面部识别的质量。", "motivation": "尽管PRIMA光伏视网膜下假体植入患者能获得形状视觉，但他们报告在感知面部方面存在困难。本研究旨在解决这一问题，通过模拟假体视觉并开发方法来改善面部表征。", "method": "研究开发了一种新型非像素化算法来模拟PRIMA患者的假体视觉，该算法整合了灰度、空间分辨率和对比度滤波器。通过将模拟的Landolt C和面部图像与真实PRIMA用户的报告进行比较来验证算法。为改善面部识别，在图像投射到植入物之前，应用了机器学习面部地标模型和对比度调整色调曲线。", "result": "模拟的假体视觉与临床研究中观察到的最大字母视力以及患者的主观描述相符。应用反向对比度滤波器有助于保持假体视觉中的对比度。使用机器学习面部地标模型识别面部特征并进一步突出它们改善了面部表征。", "conclusion": "假体视觉的空间和对比度限制会限制可分辨特征并降低自然图像质量。基于机器学习的方法和对比度调整可以缓解这些局限性并改善面部表征。即使未来的植入物具有更高的空间分辨率，对比度增强对于面部识别仍然至关重要。", "translation": "目标。植入PRIMA光伏视网膜下假体治疗地理性萎缩的患者报告称，其形状视觉平均视力与100微米像素尺寸相匹配。尽管这一显著成果使他们能够阅读和书写，但他们报告在感知面部方面存在困难。本文提供了一种新颖的、非像素化的算法，用于模拟PRIMA患者体验到的假体视觉，将该算法的预测与临床感知结果进行比较，并提供了计算机视觉和机器学习（ML）方法来改善面部表征。方法。我们的模拟算法集成了灰度滤波器、空间分辨率滤波器和对比度滤波器。这考虑了视网膜植入物有限的采样密度以及假体视觉对比敏感度降低的情况。使用此模拟算法创建的Landolt C和面部图案与实际PRIMA用户的报告进行比较。为了恢复假体视觉中丢失的面部特征，我们在图像投射到植入物之前，对面部图像应用了ML面部地标模型以及对比度调整色调曲线。主要结果。模拟的假体视觉与临床研究中观察到的最大字母视力以及患者的主观描述相符。应用反向对比度滤波器有助于保持假体视觉中的对比度。使用ML面部地标模型识别面部特征并进一步突出它们改善了面部表征。意义。假体视觉的空间和对比度限制会限制可分辨特征并降低自然图像质量。基于ML的方法和对比度调整缓解了一些局限性并改善了面部表征。尽管可以预期具有更小像素的植入物会带来更高的空间分辨率，但对比度增强对于面部识别仍然至关重要。", "summary": "本文提出了一种新颖的非像素化算法，用于模拟PRIMA系统下的假体视觉，该算法通过整合灰度、空间分辨率和对比度滤波器，准确再现了患者的视觉体验。研究发现，尽管PRIMA患者能形成形状视觉，但在面部识别上存在困难。为解决此问题，研究利用机器学习面部地标模型和对比度调整技术，显著改善了假体视觉中面部特征的表示。结果表明，模拟视觉与临床观察一致，且所提增强方法有效提升了面部识别能力，强调了对比度增强对于假体视觉的重要性。", "keywords": "假体视觉, PRIMA系统, 面部识别, 机器学习, 对比度增强", "comments": "这项研究通过提出一种新的假体视觉模拟算法，并结合机器学习方法来解决PRIMA植入患者在面部识别上的实际困难，具有重要的临床应用潜力。其创新点在于不仅准确模拟了患者的视觉体验，还针对性地提出了改善方案，尤其强调了对比度增强在低分辨率视觉中的关键作用，这对于未来假体视觉设备的设计和优化具有指导意义。"}}
{"id": "2503.11685", "pdf": "https://arxiv.org/pdf/2503.11685", "abs": "https://arxiv.org/abs/2503.11685", "authors": ["Omkar Kokane", "Adam Teman", "Anushka Jha", "Guru Prasath SL", "Gopal Raut", "Mukul Lokhande", "S. V. Jaya Chand", "Tanushree Dewangan", "Santosh Kumar Vishvakarma"], "title": "CORDIC Is All You Need", "categories": ["cs.AR", "cs.CV", "eess.IV"], "comment": null, "summary": "Artificial intelligence necessitates adaptable hardware accelerators for\nefficient high-throughput million operations. We present pipelined architecture\nwith CORDIC block for linear MAC computations and nonlinear iterative\nActivation Functions (AF) such as $tanh$, $sigmoid$, and $softmax$. This\napproach focuses on a Reconfigurable Processing Engine (RPE) based systolic\narray, with 40\\% pruning rate, enhanced throughput up to 4.64$\\times$, and\nreduction in power and area by 5.02 $\\times$ and 4.06 $\\times$ at CMOS 28 nm,\nwith minor accuracy loss. FPGA implementation achieves a reduction of up to 2.5\n$\\times$ resource savings and 3 $\\times$ power compared to prior works. The\nSystolic CORDIC engine for Reconfigurability and Enhanced throughput (SYCore)\ndeploys an output stationary dataflow with the CAESAR control engine for\ndiverse AI workloads such as Transformers, RNNs/LSTMs, and DNNs for\napplications like image detection, LLMs, and speech recognition. The\nenergy-efficient and flexible approach extends the enhanced approach for edge\nAI accelerators supporting emerging workloads.", "AI": {"title_translation": "CORDIC 足矣", "tldr": "本文提出了一种基于CORDIC的流水线架构SYCore，用于AI硬件加速，显著提升吞吐量并降低功耗和面积，支持多种AI模型和边缘AI应用。", "motivation": "人工智能需要高效、高吞吐量的可适应性硬件加速器。", "method": "本文提出了一种基于CORDIC模块的流水线架构，用于线性MAC计算和非线性迭代激活函数（如tanh, sigmoid, softmax）。该方法聚焦于基于可重构处理引擎（RPE）的脉动阵列，并部署了名为SYCore的脉动CORDIC引擎，采用输出静止数据流和CAESAR控制引擎。", "result": "实现了40%的剪枝率，吞吐量提高高达4.64倍，在CMOS 28 nm工艺下功耗和面积分别降低5.02倍和4.06倍，且精度损失微小。FPGA实现相比现有工作，资源节省高达2.5倍，功耗降低3倍。该引擎支持Transformer、RNN/LSTM、DNN等多种AI工作负载，应用于图像检测、LLM和语音识别等。", "conclusion": "所提出的节能且灵活的方法扩展了增强型边缘AI加速器，支持新兴工作负载。", "translation": "人工智能需要高效、高吞吐量的百万级操作的可适应性硬件加速器。我们提出了一种带有CORDIC模块的流水线架构，用于线性MAC计算和非线性迭代激活函数（AF），如tanh、sigmoid和softmax。该方法专注于基于可重构处理引擎（RPE）的脉动阵列，具有40%的剪枝率，吞吐量提高高达4.64倍，在CMOS 28 nm工艺下功耗和面积分别降低5.02倍和4.06倍，且精度损失微小。与现有工作相比，FPGA实现实现了高达2.5倍的资源节省和3倍的功耗降低。用于可重构性和增强吞吐量的脉动CORDIC引擎（SYCore）部署了输出静止数据流和CAESAR控制引擎，以支持Transformer、RNNs/LSTMs和DNNs等多种AI工作负载，应用于图像检测、LLMs和语音识别等。这种节能且灵活的方法扩展了增强型边缘AI加速器，支持新兴工作负载。", "summary": "本文提出了一种名为SYCore的基于CORDIC的流水线架构，用于AI硬件加速器。该架构利用可重构处理引擎（RPE）的脉动阵列，有效处理线性MAC计算和非线性激活函数。实验结果表明，该设计在CMOS 28 nm工艺下显著提升了吞吐量（高达4.64倍），并大幅降低了功耗和面积，同时在FPGA上实现了显著的资源和功耗节省。SYCore支持多种AI模型（如Transformer、RNN、DNN）及其应用，为边缘AI加速器提供了一种高效、灵活且节能的解决方案。", "keywords": "CORDIC, 硬件加速器, 脉动阵列, AI, 边缘计算", "comments": "该论文提出了一种创新的基于CORDIC的硬件加速器SYCore，通过结合流水线架构和脉动阵列，显著提升了AI计算的效率。其在功耗、面积和吞吐量方面的显著改进，以及对多种AI模型和边缘AI工作负载的广泛支持，展现了其在下一代AI硬件设计中的重要潜力。特别是其对非线性激活函数的CORDIC实现，是其创新点之一。"}}
{"id": "2503.11692", "pdf": "https://arxiv.org/pdf/2503.11692", "abs": "https://arxiv.org/abs/2503.11692", "authors": ["Rashik Shrestha", "Madhav Rijal", "Trevor Smith", "Yu Gu"], "title": "FloPE: Flower Pose Estimation for Precision Pollination", "categories": ["cs.RO", "cs.CV"], "comment": "IROS2025 under review", "summary": "This study presents Flower Pose Estimation (FloPE), a real-time flower pose\nestimation framework for computationally constrained robotic pollination\nsystems. Robotic pollination has been proposed to supplement natural\npollination to ensure global food security due to the decreased population of\nnatural pollinators. However, flower pose estimation for pollination is\nchallenging due to natural variability, flower clusters, and high accuracy\ndemands due to the flowers' fragility when pollinating. This method leverages\n3D Gaussian Splatting to generate photorealistic synthetic datasets with\nprecise pose annotations, enabling effective knowledge distillation from a\nhigh-capacity teacher model to a lightweight student model for efficient\ninference. The approach was evaluated on both single and multi-arm robotic\nplatforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees\nwithin a low computational cost. Our experiments validate the effectiveness of\nFloPE, achieving up to 78.75% pollination success rate and outperforming prior\nrobotic pollination techniques.", "AI": {"title_translation": "FloPE：用于精准授粉的花朵姿态估计", "tldr": "FloPE是一种实时花朵姿态估计算法，利用3D高斯泼溅生成数据并进行知识蒸馏，实现了低计算成本下的高精度和高授粉成功率，优于现有技术。", "motivation": "由于自然授粉者数量减少，机器人授粉被提出以补充自然授粉，确保全球粮食安全。然而，花朵姿态估计面临自然变异性、花簇以及对精度要求高（因花朵脆弱）的挑战。", "method": "该方法提出了FloPE框架，利用3D高斯泼溅技术生成带有精确姿态注释的逼真合成数据集，从而实现从高容量教师模型到轻量级学生模型的有效知识蒸馏，以实现高效推理。", "result": "该方法在单臂和多臂机器人平台上进行了评估，实现了平均姿态估计误差0.6厘米和19.14度，计算成本低。实验验证了FloPE的有效性，授粉成功率高达78.75%，并优于现有的机器人授粉技术。", "conclusion": "FloPE是一种有效的实时花朵姿态估计算法，能够以低计算成本实现高精度，并显著提高机器人授粉的成功率，超越了现有技术。", "translation": "本研究提出了花朵姿态估计（FloPE），一个用于计算受限的机器人授粉系统的实时花朵姿态估计框架。由于自然授粉者数量减少，机器人授粉被提议作为自然授粉的补充，以确保全球粮食安全。然而，由于自然变异性、花簇以及授粉时花朵脆弱导致的高精度要求，用于授粉的花朵姿态估计具有挑战性。该方法利用3D高斯泼溅生成具有精确姿态注释的逼真合成数据集，从而实现从高容量教师模型到轻量级学生模型的有效知识蒸馏，以实现高效推理。该方法在单臂和多臂机器人平台上进行了评估，实现了平均姿态估计误差0.6厘米和19.14度，计算成本低。我们的实验验证了FloPE的有效性，实现了高达78.75%的授粉成功率，并优于现有的机器人授粉技术。", "summary": "本研究引入了FloPE，一个针对计算受限机器人授粉系统的实时花朵姿态估计框架。鉴于自然授粉者减少对全球粮食安全的威胁，机器人授粉成为关键补充。然而，花朵姿态估计面临自然变异、花簇和高精度要求的挑战。FloPE通过利用3D高斯泼溅生成带有精确姿态注释的合成数据集，并采用知识蒸馏技术将高容量教师模型的知识转移到轻量级学生模型，实现了高效且高精度的推理。实验结果表明，FloPE在低计算成本下实现了0.6厘米和19.14度的平均姿态估计误差，授粉成功率高达78.75%，显著优于现有机器人授粉技术。", "keywords": "花朵姿态估计, 机器人授粉, 3D高斯泼溅, 知识蒸馏, 精准授粉", "comments": "该论文的创新点在于提出了FloPE框架，特别是在数据生成方面利用了3D高斯泼溅技术来创建逼真的合成数据集，有效解决了真实世界数据获取困难和标注成本高的问题。此外，通过知识蒸馏将复杂模型压缩为轻量级模型，使其适用于计算资源受限的机器人系统，具有重要的实际应用价值。其在授粉成功率上的显著提升表明了该方法的有效性和优越性。"}}
{"id": "2503.11846", "pdf": "https://arxiv.org/pdf/2503.11846", "abs": "https://arxiv.org/abs/2503.11846", "authors": ["Alexander Weers", "Alexander H. Berger", "Laurin Lux", "Peter Schüffler", "Daniel Rueckert", "Johannes C. Paetzold"], "title": "From Pixels to Histopathology: A Graph-Based Framework for Interpretable Whole Slide Image Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "comment": "11 pages, 2 figures", "summary": "The histopathological classification of whole-slide images (WSIs) is a\nfundamental task in digital pathology; yet it requires extensive time and\nexpertise from specialists. While deep learning methods show promising results,\nthey typically process WSIs by dividing them into artificial patches, which\ninherently prevents a network from learning from the entire image context,\ndisregards natural tissue structures and compromises interpretability. Our\nmethod overcomes this limitation through a novel graph-based framework that\nconstructs WSI graph representations. The WSI-graph efficiently captures\nessential histopathological information in a compact form. We build tissue\nrepresentations (nodes) that follow biological boundaries rather than arbitrary\npatches all while providing interpretable features for explainability. Through\nadaptive graph coarsening guided by learned embeddings, we progressively merge\nregions while maintaining discriminative local features and enabling efficient\nglobal information exchange. In our method's final step, we solve the\ndiagnostic task through a graph attention network. We empirically demonstrate\nstrong performance on multiple challenging tasks such as cancer stage\nclassification and survival prediction, while also identifying predictive\nfactors using Integrated Gradients. Our implementation is publicly available at\nhttps://github.com/HistoGraph31/pix2pathology", "AI": {"title_translation": "从像素到组织病理学：一种用于可解释全玻片图像分析的图基框架", "tldr": "提出一种新的图基框架，通过构建遵循生物边界的WSI图表示来克服深度学习在全玻片图像分析中缺乏上下文和可解释性的问题，并在癌症分类和生存预测等任务上表现出色。", "motivation": "组织病理学全玻片图像（WSI）分类是数字病理学中的一项基本任务，但需要专家大量时间和专业知识。现有的深度学习方法通常将WSI分割成人工补丁，这阻碍了网络学习整个图像上下文，忽略了自然组织结构，并损害了可解释性。", "method": "提出了一种新颖的图基框架，用于构建WSI图表示。该方法构建遵循生物边界而非任意补丁的组织表示（节点），并提供可解释的特征。通过学习到的嵌入指导自适应图粗化，逐步合并区域，同时保持判别性局部特征并实现高效的全局信息交换。最终通过图注意力网络解决诊断任务。", "result": "在癌症分期分类和生存预测等多个具有挑战性的任务上表现出强大的性能，并使用集成梯度识别了预测因子。", "conclusion": "该图基框架有效克服了传统深度学习方法在WSI分析中的局限性，提供了可解释的特征，并在多项任务中取得了优异的性能。", "translation": "组织病理学全玻片图像（WSI）分类是数字病理学中的一项基本任务；然而，它需要专家付出大量时间和专业知识。虽然深度学习方法显示出有希望的结果，但它们通常通过将WSI分割成人工补丁来处理，这固有地阻止了网络从整个图像上下文中学习，忽略了自然组织结构并损害了可解释性。我们的方法通过一种新颖的图基框架克服了这一限制，该框架构建了WSI图表示。WSI图以紧凑的形式高效捕获了重要的组织病理学信息。我们构建了遵循生物边界而非任意补丁的组织表示（节点），同时为可解释性提供了可解释的特征。通过学习到的嵌入指导的自适应图粗化，我们逐步合并区域，同时保持判别性局部特征并实现高效的全局信息交换。在我们方法的最后一步，我们通过图注意力网络解决了诊断任务。我们凭经验证明了在癌症分期分类和生存预测等多个具有挑战性的任务上的强大性能，同时还使用集成梯度识别了预测因子。我们的实现已在https://github.com/HistoGraph31/pix2pathology 公开。", "summary": "该论文提出了一种新颖的图基框架，用于可解释的全玻片图像（WSI）分析，旨在克服传统深度学习方法在处理WSI时因分割成人工补丁而导致的上下文丢失和可解释性差的问题。该框架通过构建遵循生物边界的WSI图表示来捕获组织病理学信息，并通过自适应图粗化和图注意力网络进行诊断。实验结果表明，该方法在癌症分期分类和生存预测等任务上表现出强大的性能，并能识别预测因子。", "keywords": "全玻片图像分析, 图神经网络, 组织病理学, 可解释性, 癌症分类", "comments": "这篇论文通过引入图基框架来处理全玻片图像，创新性地解决了传统深度学习方法在数字病理学中面临的上下文丢失和可解释性不足的问题。其构建遵循生物边界的节点表示，并通过图粗化和图注意力网络进行分析，显著提高了模型的透明度和诊断准确性，对临床应用具有重要意义。"}}
{"id": "2503.11851", "pdf": "https://arxiv.org/pdf/2503.11851", "abs": "https://arxiv.org/abs/2503.11851", "authors": ["Jutika Borah", "Hidam Kumarjit Singh"], "title": "DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "18 pages, 8 figures, 5 tables", "summary": "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.", "AI": {"title_translation": "DCAT：用于放射图像疾病分类的双交叉注意力融合与不确定性估计", "tldr": "本文提出了DCAT模型，利用双交叉注意力融合EfficientNetB4和ResNet34特征，并结合不确定性估计，显著提高了放射图像疾病分类的准确性和可靠性。", "motivation": "放射学中准确可靠的图像分类对患者预后至关重要，但传统深度学习模型常产生过度自信的预测，可能导致误诊。注意力机制和特征融合能有效解决不确定性挑战，交叉注意力在医学图像分析中捕获特征和模态间依赖关系日益重要。", "method": "本文提出了DCAT（Dual Cross-Attention Fusion）模型，引入双向交叉注意力机制，结合精炼的通道和空间注意力，动态融合EfficientNetB4和ResNet34的特征图，利用多网络上下文依赖性。通过通道和空间注意力精炼的特征突出判别模式。模型还结合了不确定性估计。", "result": "模型在Covid-19、肺结核、肺炎胸部X射线图像和视网膜OCT图像上分别取得了99.75%、100%、99.93%和98.69%的AUC，以及99.81%、100%、99.97%和96.36%的AUPR。熵值和高不确定性样本提供了可解释的可视化。", "conclusion": "通过结合多尺度特征提取、双向注意力机制和不确定性估计，所提出的DCAT模型对医学图像分析产生了重要影响，提高了分类的准确性和可靠性。", "translation": "在放射学中，准确可靠的图像分类至关重要，因为诊断决策会显著影响患者的治疗结果。传统的深度学习模型往往会产生过度自信的预测，尽管存在潜在的不确定性，这可能导致误诊。注意力机制已成为深度学习中强大的工具，使模型能够专注于输入数据的相关部分。结合特征融合，它们可以有效地解决不确定性挑战。交叉注意力在医学图像分析中对于捕获特征和模态之间的依赖关系变得越来越重要。本文提出了一种新颖的双交叉注意力融合模型，用于医学图像分析，旨在解决特征整合和可解释性方面的关键挑战。我们的方法引入了一种双向交叉注意力机制，结合了精炼的通道和空间注意力，动态融合来自EfficientNetB4和ResNet34的特征图，利用多网络上下文依赖性。通过通道和空间注意力精炼的特征突出了对准确分类至关重要的判别模式。所提出的模型在Covid-19、肺结核、肺炎胸部X射线图像和视网膜OCT图像上分别取得了99.75%、100%、99.93%和98.69%的AUC，以及99.81%、100%、99.97%和96.36%的AUPR。熵值和一些高不确定性样本提供了模型可解释的可视化，增强了透明度。通过结合多尺度特征提取、双向注意力机制和不确定性估计，我们提出的模型对医学图像分析产生了强烈影响。", "summary": "本文提出了DCAT（Dual Cross-Attention Fusion）模型，旨在解决放射图像疾病分类中传统深度学习模型过度自信预测的问题。DCAT采用新颖的双向交叉注意力机制，并结合精炼的通道和空间注意力，动态融合EfficientNetB4和ResNet34的多尺度特征。该模型通过不确定性估计提高了诊断的可靠性和可解释性，并在多种医学图像数据集上取得了高精度分类性能。", "keywords": "医学图像分类, 双交叉注意力, 不确定性估计, 特征融合, 深度学习", "comments": "该论文的创新点在于提出了双交叉注意力融合机制，结合了多网络特征和不确定性估计，这对于提高医学图像诊断的准确性和可靠性具有重要意义。特别是在处理传统模型过度自信问题上，不确定性估计的引入增强了模型的透明度和临床可用性。"}}
{"id": "2503.11954", "pdf": "https://arxiv.org/pdf/2503.11954", "abs": "https://arxiv.org/abs/2503.11954", "authors": ["Ahcen Aliouat", "Elsa Dupraz"], "title": "Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.IT", "cs.LG", "math.IT", "94A29, 94A08, 94B05, 68T01, 68P30", "I.4.2; E.4; I.2.10; I.5.4; I.5.1; I.4.1"], "comment": "11 pages, 13 figures, Submitted to IEEE Transactions on\n  Communications (Under Review)", "summary": "In the emerging field of goal-oriented communications, the focus has shifted\nfrom reconstructing data to directly performing specific learning tasks, such\nas classification, segmentation, or pattern recognition, on the received coded\ndata. In the commonly studied scenario of classification from compressed\nimages, a key objective is to enable learning directly on entropy-coded data,\nthereby bypassing the computationally intensive step of data reconstruction.\nConventional entropy-coding methods, such as Huffman and Arithmetic coding, are\neffective for compression but disrupt the data structure, making them less\nsuitable for direct learning without decoding. This paper investigates the use\nof low-density parity-check (LDPC) codes -- originally designed for channel\ncoding -- as an alternative entropy-coding approach. It is hypothesized that\nthe structured nature of LDPC codes can be leveraged more effectively by deep\nlearning models for tasks like classification. At the receiver side, gated\nrecurrent unit (GRU) models are trained to perform image classification\ndirectly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST,\nand CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in\nclassification tasks, while requiring significantly smaller learning models.\nFurthermore, the paper analyzes why LDPC codes preserve data structure more\neffectively than traditional entropy-coding techniques and explores the impact\nof key code parameters on classification performance. These results suggest\nthat LDPC-based entropy coding offers an optimal balance between learning\nefficiency and model complexity, eliminating the need for prior decoding.", "AI": {"title_translation": "面向目标的信源编码：使用LDPC码进行压缩域图像分类", "tldr": "本研究提出使用LDPC码作为一种新的熵编码方法，以实现在压缩域上直接进行图像分类，避免数据重建，并发现其性能优于传统方法。", "motivation": "在面向目标的通信中，直接对接收到的编码数据执行学习任务（如分类）是一个关键目标。传统的熵编码方法（如霍夫曼编码和算术编码）虽然有效压缩数据，但会破坏数据结构，使得在不解码的情况下直接学习变得困难。", "method": "本文研究了使用低密度奇偶校验（LDPC）码作为替代熵编码方法，LDPC码最初设计用于信道编码。假设LDPC码的结构化特性可以被深度学习模型更有效地利用。在接收端，训练门控循环单元（GRU）模型直接在LDPC编码数据上执行图像分类。", "result": "在MNIST、Fashion-MNIST和CIFAR数据集上的实验表明，LDPC码在分类任务中优于霍夫曼编码和算术编码，并且需要显著更小的学习模型。此外，研究分析了LDPC码比传统熵编码技术更有效地保留数据结构的原因，并探讨了关键码参数对分类性能的影响。", "conclusion": "这些结果表明，基于LDPC的熵编码在学习效率和模型复杂度之间提供了最佳平衡，消除了预先解码的需要。", "translation": "在新兴的面向目标通信领域，重点已从数据重建转向直接在接收到的编码数据上执行特定的学习任务，例如分类、分割或模式识别。在通常研究的压缩图像分类场景中，一个关键目标是能够直接在熵编码数据上进行学习，从而绕过计算密集型的数据重建步骤。传统的熵编码方法，如霍夫曼编码和算术编码，对于压缩是有效的，但它们会破坏数据结构，使其在不解码的情况下不适合直接学习。本文研究了使用低密度奇偶校验（LDPC）码——最初设计用于信道编码——作为一种替代的熵编码方法。假设LDPC码的结构化特性可以被深度学习模型更有效地利用，用于分类等任务。在接收端，训练门控循环单元（GRU）模型直接在LDPC编码数据上执行图像分类。在MNIST、Fashion-MNIST和CIFAR等数据集上的实验表明，LDPC码在分类任务中优于霍夫曼编码和算术编码，同时需要显著更小的学习模型。此外，本文分析了LDPC码为何比传统熵编码技术更有效地保留数据结构，并探讨了关键码参数对分类性能的影响。这些结果表明，基于LDPC的熵编码在学习效率和模型复杂度之间提供了最佳平衡，消除了预先解码的需要。", "summary": "本文提出了一种面向目标的信源编码方法，利用LDPC码在压缩域直接进行图像分类，旨在避免传统熵编码（如霍夫曼和算术编码）破坏数据结构导致无法直接学习的问题。通过训练GRU模型在LDPC编码数据上进行分类，实验证明LDPC码在分类性能上优于传统方法，且所需的学习模型更小。研究还分析了LDPC码保持数据结构的机制及其参数对分类表现的影响，表明LDPC编码在学习效率和模型复杂度之间实现了优化平衡，无需预先解码。", "keywords": "LDPC码, 目标导向通信, 图像分类, 熵编码, 压缩域学习", "comments": "该论文的创新点在于将通常用于信道编码的LDPC码应用于信源编码，并结合深度学习模型实现压缩域的直接分类，有效地解决了传统熵编码破坏数据结构、阻碍直接学习的问题。这对于资源受限的设备和实时应用具有重要意义，能够显著降低计算开销。"}}
{"id": "2503.11978", "pdf": "https://arxiv.org/pdf/2503.11978", "abs": "https://arxiv.org/abs/2503.11978", "authors": ["Eric M. Chen", "Di Liu", "Sizhuo Ma", "Michael Vasilkovsky", "Bing Zhou", "Qiang Gao", "Wenzhou Wang", "Jiahao Luo", "Dimitris N. Metaxas", "Vincent Sitzmann", "Jian Wang"], "title": "Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars", "categories": ["cs.GR", "cs.CV"], "comment": "N/A", "summary": "The increasing popularity of personalized avatar systems, such as Snapchat\nBitmojis and Apple Memojis, highlights the growing demand for digital\nself-representation. Despite their widespread use, existing avatar platforms\nface significant limitations, including restricted expressivity due to\npredefined assets, tedious customization processes, or inefficient rendering\nrequirements. Addressing these shortcomings, we introduce Snapmoji, an avatar\ngeneration system that instantly creates animatable, dual-stylized avatars from\na selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on\nlarge-scale Gaussian models using 3D data from sources such as Objaverse and\nfine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This\nenables Snapmoji to transform a selfie into a primary stylized avatar, like the\nBitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all\nwhile preserving the user's identity and the primary style's integrity. Our\nsystem is capable of producing 3D Gaussian avatars that support dynamic\nanimation, including accurate facial expression transfer. Designed for\nefficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds\nand supports real-time interactions on mobile devices at 30 to 40 frames per\nsecond. Extensive testing confirms that Snapmoji outperforms existing methods\nin versatility and speed, making it a convenient tool for automatic avatar\ncreation in various styles.", "AI": {"title_translation": "Snapmoji：可动画双风格化头像的即时生成", "tldr": "Snapmoji是一个创新的头像生成系统，能够从单张自拍即时创建可动画、双风格化的3D头像，解决了现有平台的局限性，并实现了快速高效的转换和实时交互。", "motivation": "现有头像平台存在表达受限、定制过程繁琐或渲染效率低下等局限性，无法满足用户对个性化数字自我表达日益增长的需求。", "method": "Snapmoji系统通过引入高斯域适应（GDA）模型实现。GDA在大型高斯模型上预训练（使用Objaverse等3D数据），并用2D风格迁移任务进行微调，从而获得丰富的3D先验。该方法能够将自拍转换为主要风格化头像（如Bitmoji），并应用次要风格（如Plastic Toy或Alien），同时保留用户身份和主要风格的完整性。", "result": "Snapmoji系统能够在0.9秒内完成自拍到头像的转换，并在移动设备上支持每秒30到40帧的实时交互。它能生成支持动态动画（包括精确面部表情迁移）的3D高斯头像。广泛测试证实Snapmoji在多功能性和速度方面优于现有方法。", "conclusion": "Snapmoji是一个便捷的自动多风格头像创建工具，在多功能性和速度上表现优异。", "translation": "标题：Snapmoji：可动画双风格化头像的即时生成\n摘要：个性化头像系统（如Snapchat Bitmoji和Apple Memoji）日益普及，凸显了对数字自我表达日益增长的需求。尽管它们被广泛使用，但现有头像平台面临显著局限性，包括由于预定义资产导致的表达受限、繁琐的定制过程或低效的渲染要求。为了解决这些缺点，我们引入了Snapmoji，一个头像生成系统，可以从自拍即时创建可动画、双风格化头像。我们提出了高斯域适应（GDA），它在大规模高斯模型上进行预训练，使用来自Objaverse等来源的3D数据，并通过2D风格迁移任务进行微调，赋予其丰富的3D先验。这使得Snapmoji能够将自拍转换为主要风格化头像（如Bitmoji风格），并应用次要风格（如塑料玩具或外星人），同时保留用户身份和主要风格的完整性。我们的系统能够生成支持动态动画（包括精确面部表情迁移）的3D高斯头像。Snapmoji设计高效，可在0.9秒内完成自拍到头像的转换，并在移动设备上支持每秒30到40帧的实时交互。广泛测试证实Snapmoji在多功能性和速度方面优于现有方法，使其成为一种便捷的自动多风格头像创建工具。", "summary": "Snapmoji是一种新型的头像生成系统，旨在解决当前个性化头像平台在表达、定制和渲染效率上的不足。该系统通过其核心技术——高斯域适应（GDA）模型，能够从单张自拍即时生成可动画、双风格化的3D头像。GDA通过结合3D数据预训练和2D风格迁移微调，确保了用户身份和主要风格的保留，并支持次要风格的灵活应用。Snapmoji实现了0.9秒的快速转换速度，并能在移动设备上进行实时（30-40 fps）交互，在多功能性和性能上均超越了现有方法，提供了一个便捷高效的自动头像创建解决方案。", "keywords": "头像生成, 双风格化, 高斯域适应, 实时动画, 自拍转换", "comments": "Snapmoji的创新点在于其提出的高斯域适应（GDA）方法，它巧妙地结合了3D先验知识和2D风格迁移技术，实现了从单张自拍即时生成高质量、可动画且支持双风格的3D头像。其在处理速度（0.9秒完成转换）和移动设备实时交互能力（30-40 fps）上的显著提升，是该研究的重要贡献。这使得数字自我表达的门槛大大降低，用户体验得到显著提升，预示着未来头像生成技术的发展方向。"}}
{"id": "2503.11999", "pdf": "https://arxiv.org/pdf/2503.11999", "abs": "https://arxiv.org/abs/2503.11999", "authors": ["Tongxuan Tian", "Haoyang Li", "Bo Ai", "Xiaodi Yuan", "Zhiao Huang", "Hao Su"], "title": "Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Manipulating deformable objects like cloth is challenging due to their\ncomplex dynamics, near-infinite degrees of freedom, and frequent\nself-occlusions, which complicate state estimation and dynamics modeling. Prior\nwork has struggled with robust cloth state estimation, while dynamics models,\nprimarily based on Graph Neural Networks (GNNs), are limited by their locality.\nInspired by recent advances in generative models, we hypothesize that these\nexpressive models can effectively capture intricate cloth configurations and\ndeformation patterns from data. Building on this insight, we propose a\ndiffusion-based generative approach for both perception and dynamics modeling.\nSpecifically, we formulate state estimation as reconstructing the full cloth\nstate from sparse RGB-D observations conditioned on a canonical cloth mesh and\ndynamics modeling as predicting future states given the current state and robot\nactions. Leveraging a transformer-based diffusion model, our method achieves\nhigh-fidelity state reconstruction while reducing long-horizon dynamics\nprediction errors by an order of magnitude compared to GNN-based approaches.\nIntegrated with model-predictive control (MPC), our framework successfully\nexecutes cloth folding on a real robotic system, demonstrating the potential of\ngenerative models for manipulation tasks with partial observability and complex\ndynamics.", "AI": {"title_translation": "用于布料操作的扩散动力学模型与生成式状态估计", "tldr": "该论文提出了一种基于扩散的生成式方法，用于布料操作中的状态估计和动力学建模。与现有方法相比，它在状态重建和长期动力学预测方面表现出色，并在真实机器人系统上成功实现了布料折叠。", "motivation": "操纵布料等可变形物体具有挑战性，因为它们动力学复杂、自由度近乎无限且频繁自遮挡，这使得状态估计和动力学建模变得复杂。现有工作在鲁棒的布料状态估计方面遇到困难，而主要基于图神经网络（GNN）的动力学模型则受限于其局部性。", "method": "该论文提出了一种基于扩散的生成式方法，用于感知和动力学建模。具体来说，将状态估计公式化为从稀疏RGB-D观测中重建完整布料状态（以规范布料网格为条件），将动力学建模公式化为给定当前状态和机器人动作预测未来状态。该方法利用基于Transformer的扩散模型。", "result": "该方法实现了高保真度状态重建，并将长期动力学预测误差比基于GNN的方法降低了一个数量级。与模型预测控制（MPC）集成后，该框架在真实机器人系统上成功执行了布料折叠。", "conclusion": "生成模型在处理部分可观测性和复杂动力学的操作任务中具有巨大潜力。", "translation": "由于布料等可变形物体动力学复杂、自由度近乎无限且频繁自遮挡，使得其状态估计和动力学建模变得复杂，因此对它们进行操作具有挑战性。以往的工作在鲁棒的布料状态估计方面举步维艰，而主要基于图神经网络（GNN）的动力学模型则受限于其局部性。受生成模型最新进展的启发，我们假设这些表达能力强的模型可以有效地从数据中捕获复杂的布料配置和变形模式。基于这一洞察，我们提出了一种基于扩散的生成式方法，用于感知和动力学建模。具体来说，我们将状态估计公式化为从稀疏RGB-D观测中重建完整的布料状态，并以规范的布料网格为条件；将动力学建模公式化为给定当前状态和机器人动作预测未来状态。利用基于Transformer的扩散模型，我们的方法实现了高保真度状态重建，同时与基于GNN的方法相比，将长期动力学预测误差降低了一个数量级。结合模型预测控制（MPC），我们的框架在真实机器人系统上成功执行了布料折叠，展示了生成模型在处理部分可观测性和复杂动力学的操作任务中的潜力。", "summary": "该论文提出了一种创新的基于扩散的生成式方法，用于解决可变形物体（特别是布料）操作中的状态估计和动力学建模难题。针对现有方法在鲁棒状态估计和局部性限制上的不足，作者利用Transformer-based扩散模型，实现了从稀疏RGB-D数据中高保真度重建布料状态，并显著降低了长期动力学预测误差。结合模型预测控制，该框架在真实机器人上成功演示了布料折叠任务，突出了生成模型在复杂、部分可观测操作场景下的巨大潜力。", "keywords": "布料操作, 扩散模型, 状态估计, 动力学建模, 生成模型", "comments": "该论文的创新点在于将扩散模型应用于布料操作中的状态估计和动力学建模，解决了传统方法在处理复杂可变形物体时的局限性。其利用生成模型的高表达能力来捕捉复杂的布料配置和变形模式，并通过Transformer-based扩散模型实现了显著的性能提升，尤其是在长期预测方面。在真实机器人系统上的成功部署证明了该方法的实用性和潜力，为未来可变形物体操作研究开辟了新方向。"}}
{"id": "2503.12030", "pdf": "https://arxiv.org/pdf/2503.12030", "abs": "https://arxiv.org/abs/2503.12030", "authors": ["Zhenxin Li", "Shihao Wang", "Shiyi Lan", "Zhiding Yu", "Zuxuan Wu", "Jose M. Alvarez"], "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt.", "AI": {"title_translation": "Hydra-NeXt: 基于开环训练的鲁棒闭环驾驶", "tldr": "Hydra-NeXt提出一个多分支框架，通过统一轨迹预测、控制预测和轨迹优化，弥合了自动驾驶中开环训练与闭环部署的差距，实现了更鲁棒、更快的闭环驾驶，显著提升了性能。", "motivation": "当前端到端自动驾驶研究面临开环训练与闭环部署之间的关键挑战，导致现有方法在闭环环境中对动态情况反应迟缓，并可能生成运动学上不可行的规划。", "method": "本文引入了Hydra-NeXt，一个新颖的多分支规划框架，它在一个模型中统一了轨迹预测、控制预测和轨迹优化网络。Hydra-NeXt利用控制解码器专注于短期动作以实现快速响应，并提出了轨迹优化模块来增强和细化规划决策，确保遵守闭环环境中的运动学约束。", "result": "在Bench2Drive数据集上，Hydra-NeXt在不依赖外部专家数据收集的情况下，实现了65.89的驾驶分数（DS）和48.20%的成功率（SR）。它超越了之前最先进的水平，DS提高了22.98，SR提高了17.49。", "conclusion": "Hydra-NeXt通过其统一的框架成功弥合了开环训练与闭环驾驶之间的差距，并在自动驾驶领域取得了显著的性能提升，证明了其在实现鲁棒闭环驾驶方面的有效性。", "translation": "端到端自动驾驶研究目前面临着弥合开环训练与闭环部署之间差距的关键挑战。当前的方法被训练用于在开环环境中预测轨迹，这在闭环环境中难以对其他智能体做出快速反应，并可能由于开环训练与闭环驾驶之间的差距而产生运动学上不可行的规划。在本文中，我们介绍了Hydra-NeXt，一个新颖的多分支规划框架，它在一个模型中统一了轨迹预测、控制预测和轨迹优化网络。与当前仅处理一般情况规划的开环轨迹预测模型不同，Hydra-NeXt进一步利用控制解码器专注于短期动作，这使得对动态情况和反应性智能体能够更快地响应。此外，我们提出了轨迹优化模块，通过有效遵守闭环环境中的运动学约束来增强和细化规划决策。这种统一的方法弥合了开环训练与闭环驾驶之间的差距，在Bench2Drive数据集上展示了卓越的性能，驾驶分数（DS）达到65.89，成功率（SR）达到48.20%，且不依赖外部专家进行数据收集。Hydra-NeXt超越了之前最先进的水平，DS提高了22.98，SR提高了17.49，标志着自动驾驶领域的一个重大进步。代码将在https://github.com/woxihuanjiangguo/Hydra-NeXt提供。", "summary": "Hydra-NeXt是一个创新的多分支规划框架，旨在解决自动驾驶中开环训练与闭环部署的差距。它通过整合轨迹预测、控制预测和轨迹优化网络，实现了对动态环境的快速响应和运动学约束的有效遵守。该模型在Bench2Drive数据集上表现出色，显著超越了现有技术水平，证明了其在鲁棒闭环驾驶方面的潜力。", "keywords": "自动驾驶, 开环训练, 闭环驾驶, 轨迹预测, 轨迹优化", "comments": "本文提出了一种创新的统一框架Hydra-NeXt，通过结合短期控制预测和轨迹优化，有效解决了自动驾驶领域开环训练与闭环部署之间的关键难题。其在性能上的显著提升表明了该方法在提高自动驾驶系统鲁棒性和反应性方面的巨大潜力。"}}
{"id": "2503.12042", "pdf": "https://arxiv.org/pdf/2503.12042", "abs": "https://arxiv.org/abs/2503.12042", "authors": ["Zhedong Zhang", "Liang Li", "Chenggang Yan", "Chunshan Liu", "Anton van den Hengel", "Yuankai Qi"], "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Movie dubbing describes the process of transforming a script into speech that\naligns temporally and emotionally with a given movie clip while exemplifying\nthe speaker's voice demonstrated in a short reference audio clip. This task\ndemands the model bridge character performances and complicated prosody\nstructures to build a high-quality video-synchronized dubbing track. The\nlimited scale of movie dubbing datasets, along with the background noise\ninherent in audio data, hinder the acoustic modeling performance of trained\nmodels. To address these issues, we propose an acoustic-prosody disentangled\ntwo-stage method to achieve high-quality dubbing generation with precise\nprosody alignment. First, we propose a prosody-enhanced acoustic pre-training\nto develop robust acoustic modeling capabilities. Then, we freeze the\npre-trained acoustic system and design a disentangled framework to model\nprosodic text features and dubbing style while maintaining acoustic quality.\nAdditionally, we incorporate an in-domain emotion analysis module to reduce the\nimpact of visual domain shifts across different movies, thereby enhancing\nemotion-prosody alignment. Extensive experiments show that our method performs\nfavorably against the state-of-the-art models on two primary benchmarks. The\ndemos are available at https://zzdoog.github.io/ProDubber/.", "AI": {"title_translation": "电影配音中的韵律增强声学预训练和声学解耦韵律自适应", "tldr": "本文提出了一种韵律增强的声学预训练和声学解耦的韵律自适应的两阶段方法，用于高质量电影配音。", "motivation": "电影配音任务要求模型连接角色表演和复杂的韵律结构以构建高质量的视频同步配音轨道。然而，电影配音数据集规模有限以及音频数据中固有的背景噪声阻碍了训练模型的声学建模性能。", "method": "本文提出了一种声学-韵律解耦的两阶段方法。首先，提出韵律增强的声学预训练以开发鲁棒的声学建模能力。然后，冻结预训练的声学系统，并设计一个解耦框架来建模韵律文本特征和配音风格，同时保持声学质量。此外，还结合了域内情感分析模块以减少不同电影之间视觉域偏移的影响，从而增强情感-韵律对齐。", "result": "广泛的实验表明，该方法在两个主要基准上优于最先进的模型。", "conclusion": "本文提出的韵律增强的声学预训练和声学解耦的韵律自适应方法有效地解决了电影配音中的挑战，实现了高质量的配音生成和精确的韵律对齐，并优于现有最先进的模型。", "translation": "电影配音描述的是将脚本转换为与给定电影片段在时间上和情感上对齐的语音，同时体现短参考音频片段中所示扬声器声音的过程。这项任务要求模型连接角色表演和复杂的韵律结构，以构建高质量的视频同步配音轨道。电影配音数据集的规模有限，加上音频数据中固有的背景噪声，阻碍了训练模型的声学建模性能。为了解决这些问题，我们提出了一种声学-韵律解耦的两阶段方法，以实现高质量的配音生成和精确的韵律对齐。首先，我们提出了一种韵律增强的声学预训练，以开发鲁棒的声学建模能力。然后，我们冻结预训练的声学系统，并设计一个解耦框架来建模韵律文本特征和配音风格，同时保持声学质量。此外，我们还结合了一个域内情感分析模块，以减少不同电影之间视觉域偏移的影响，从而增强情感-韵律对齐。广泛的实验表明，我们的方法在两个主要基准上优于最先进的模型。演示可在 https://zzdoog.github.io/ProDubber/ 获取。", "summary": "本文通过提出一种两阶段的声学-韵律解耦方法，解决了电影配音中数据集有限和背景噪声等挑战。该方法包括用于鲁棒声学建模的韵律增强声学预训练，以及用于建模韵律文本特征和配音风格的声学解耦韵律自适应框架。此外，还引入了域内情感分析模块以增强情感-韵律对齐。实验证明，该方法在两个基准上表现优于现有最先进的模型，从而实现高质量、精确对齐的配音。", "keywords": "电影配音, 韵律, 声学预训练, 解耦学习, 情感分析", "comments": "本文的创新之处在于其两阶段的声学-韵律解耦方法，特别是韵律增强的预训练和对韵律特征及配音风格的独立建模。引入域内情感分析模块以处理视觉域偏移，从而提高配音中的情感对齐，也是一个显著的贡献。"}}
{"id": "2503.12141", "pdf": "https://arxiv.org/pdf/2503.12141", "abs": "https://arxiv.org/abs/2503.12141", "authors": ["Shayan Rokhva", "Babak Teimourpour", "Romina Babaei"], "title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries.", "AI": {"title_translation": "结合情感强度分析器与模糊逻辑增强伊朗餐厅评论情感分析", "tldr": "该研究结合模糊逻辑和VADER改进了伊朗餐厅评论的情感分析，通过根变换减少了中立性偏差并提高了情感强度捕获能力。", "motivation": "传统情感强度分析器（VADER）在处理伊朗餐厅评论时存在中立性偏见，导致情感强度表示不准确，因此需要引入新的方法来缓解这一问题。", "method": "收集并预处理包含1266条评论及其星级的数据集。使用情感强度分析器（VADER）进行初步情感分析。引入两种基于模糊视角的细化技术：平方根和四次方根变换，以放大积极和消极情感分数。提出三种方法：方法1（原始VADER分数）、方法2（平方根修改）、方法3（四次方根修改）。开发一个包含综合模糊规则的模糊推理系统，处理细化后的分数并为每条评论生成连续的情感值。通过人工监督和与客户星级对齐进行比较分析。", "result": "细化后的方法显著改善了情感分析，减少了中立性偏差，并更好地捕获了情感强度。识别出轻微的过度放大和在特定领域案例中持续存在的中立性问题。", "conclusion": "该研究的方法和结果为寻求更精确理解消费者情绪的企业提供了宝贵见解。建议未来的研究解决偶尔出现的障碍（过度放大和持续中立性）。", "translation": "本研究提出了一种针对伊朗餐厅评论的先进情感分析框架，该框架结合模糊逻辑与传统情感分析技术，以评估情感极性和强度。研究编译并预处理了一个包含1266条评论及相应星级的数据集用于分析。初步情感分析使用情感强度分析器（VADER）进行，这是一种基于规则的工具，可为积极、消极和中立类别分配情感分数。然而，明显的中立性偏见常常导致情感强度表示不准确。为缓解此问题，基于模糊视角引入了两种细化技术，应用平方根和四次方根变换来放大积极和消极情感分数，同时保持中立性。这导致了三种不同的方法：方法1，使用未修改的VADER分数；方法2，使用平方根修改情感值；以及方法3，应用四次方根进行进一步细化。随后，开发了一个包含综合模糊规则的模糊推理系统，以处理这些细化后的分数，并根据每种方法为每条评论生成一个单一的、连续的情感值。包括人工监督和与客户星级对齐在内的比较分析表明，细化后的方法通过减少中立性偏差和更好地捕获情感强度，显著改善了情感分析。尽管取得了这些进展，但仍发现轻微的过度放大和在特定领域案例中持续存在的中立性，这促使我们提出了几项未来的研究来解决这些偶尔出现的障碍。本研究的方法和成果为寻求更精确理解消费者情绪的企业提供了宝贵见解，从而增强了各行业的情感分析。", "summary": "本研究提出了一个结合模糊逻辑和传统情感分析（VADER）的增强型框架，用于分析伊朗餐厅评论。针对VADER的中立性偏差问题，引入了平方根和四次方根变换来细化情感分数。通过比较原始VADER和两种细化方法，并结合模糊推理系统生成连续情感值，结果显示细化后的方法显著提高了情感分析的准确性，减少了中立性偏差并更好地捕捉了情感强度，为企业提供了更精确的消费者情绪洞察。", "keywords": "情感分析, 模糊逻辑, VADER, 餐厅评论, 情感强度", "comments": "该论文的创新点在于将模糊逻辑与传统的情感分析工具VADER相结合，并通过数学变换（平方根和四次方根）来解决VADER在特定领域（伊朗餐厅评论）中立性偏见的问题。这种混合方法有效地提高了情感强度的捕捉能力和分析的精确性。其重要性在于为企业提供了更准确的消费者情绪理解，具有实际应用价值。局限性在于仍存在轻微的过度放大和特定领域中立性问题，需要进一步研究。"}}
{"id": "2503.12170", "pdf": "https://arxiv.org/pdf/2503.12170", "abs": "https://arxiv.org/abs/2503.12170", "authors": ["Tao Wang", "Cong Zhang", "Xingguang Qu", "Kun Li", "Weiwei Liu", "Chang Huang"], "title": "DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 6 figures", "summary": "End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising\napproach toward achieving full autonomy. However, existing E2E-AD systems\ntypically adopt a traditional multi-task framework, addressing perception,\nprediction, and planning tasks through separate task-specific heads. Despite\nbeing trained in a fully differentiable manner, they still encounter issues\nwith task coordination, and the system complexity remains high. In this work,\nwe introduce DiffAD, a novel diffusion probabilistic model that redefines\nautonomous driving as a conditional image generation task. By rasterizing\nheterogeneous targets onto a unified bird's-eye view (BEV) and modeling their\nlatent distribution, DiffAD unifies various driving objectives and jointly\noptimizes all driving tasks in a single framework, significantly reducing\nsystem complexity and harmonizing task coordination. The reverse process\niteratively refines the generated BEV image, resulting in more robust and\nrealistic driving behaviors. Closed-loop evaluations in Carla demonstrate the\nsuperiority of the proposed method, achieving a new state-of-the-art Success\nRate and Driving Score. The code will be made publicly available.", "AI": {"title_translation": "DiffAD：一种统一的扩散建模自动驾驶方法", "tldr": "DiffAD提出了一种新的扩散概率模型，将自动驾驶重新定义为条件图像生成任务，统一了感知、预测和规划，显著降低了系统复杂性并提高了性能。", "motivation": "现有的端到端自动驾驶（E2E-AD）系统采用传统的多任务框架，通过独立的任务特定头部处理感知、预测和规划任务，导致任务协调问题和系统复杂性高。", "method": "DiffAD是一种新颖的扩散概率模型，将自动驾驶重新定义为条件图像生成任务。它通过将异构目标光栅化到统一的鸟瞰图（BEV）上并建模其潜在分布，统一了各种驾驶目标，并在一个框架中联合优化所有驾驶任务。逆向过程迭代细化生成的BEV图像。", "result": "在Carla中的闭环评估表明，该方法优于现有技术，实现了新的最先进的成功率和驾驶分数。", "conclusion": "DiffAD通过统一扩散建模方法解决了端到端自动驾驶中的任务协调和系统复杂性问题，显著提升了自动驾驶的性能和鲁棒性。", "translation": "端到端自动驾驶（E2E-AD）已迅速成为实现完全自动驾驶的一种有前景的方法。然而，现有的E2E-AD系统通常采用传统的多任务框架，通过独立的任务特定头部处理感知、预测和规划任务。尽管它们以完全可微分的方式进行训练，但仍面临任务协调问题，且系统复杂性较高。在这项工作中，我们引入了DiffAD，这是一种新颖的扩散概率模型，它将自动驾驶重新定义为条件图像生成任务。通过将异构目标光栅化到统一的鸟瞰图（BEV）上并建模其潜在分布，DiffAD统一了各种驾驶目标，并在一个框架中联合优化所有驾驶任务，显著降低了系统复杂性并协调了任务。逆向过程迭代细化生成的BEV图像，从而产生更鲁棒和真实的驾驶行为。在Carla中的闭环评估证明了所提出方法的优越性，实现了新的最先进的成功率和驾驶分数。代码将公开发布。", "summary": "DiffAD提出了一种新颖的扩散概率模型，旨在解决现有端到端自动驾驶（E2E-AD）系统中任务协调和高复杂性的问题。该方法将自动驾驶重新定义为条件图像生成任务，通过将感知、预测和规划等异构目标统一到鸟瞰图（BEV）上进行建模和联合优化。DiffAD显著降低了系统复杂性，改善了任务协调，并通过迭代细化生成更鲁棒和真实的驾驶行为。在Carla的闭环评估中，DiffAD展示了优越的性能，达到了新的最先进的成功率和驾驶分数。", "keywords": "自动驾驶, 扩散模型, 端到端, 鸟瞰图, 任务统一", "comments": "DiffAD的创新点在于将自动驾驶任务统一为条件图像生成问题，利用扩散模型强大的生成能力来解决多任务协调和系统复杂性问题。这种统一的BEV表示和联合优化方法有望提高端到端自动驾驶系统的效率和性能。在Carla中的SOTA结果证明了其有效性。"}}
{"id": "2503.12172", "pdf": "https://arxiv.org/pdf/2503.12172", "abs": "https://arxiv.org/abs/2503.12172", "authors": ["Kasra Arabi", "R. Teal Witter", "Chinmay Hegde", "Niv Cohen"], "title": "SEAL: Semantic Aware Image Watermarking", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.", "AI": {"title_translation": "SEAL: 语义感知图像水印", "tldr": "提出了一种语义感知图像水印方法SEAL，无需数据库即可检测，且能抵御特定攻击，解决AI生成内容的水印问题。", "motivation": "生成模型产生的逼真输出使得区分自然内容和AI生成内容变得困难，因此需要鲁棒的水印技术。现有方法要么扭曲生成图像的分布，要么依赖于搜索长密钥字典进行检测。", "method": "本文提出了一种名为SEAL的新型水印方法，该方法将生成图像的语义信息直接嵌入到水印中，从而实现无失真水印。通过局部敏感哈希，可以从图像的语义嵌入中推断出密钥模式，无需密钥数据库即可验证。此外，将水印检测与原始图像内容相结合，提高了对伪造攻击的鲁棒性。", "result": "实现了无失真水印，且无需密钥模式数据库即可验证。对两种此前被忽视的攻击策略（提取初始噪声生成新图像；在水印图像中插入无关对象）表现出更高的鲁棒性。", "conclusion": "内容感知水印可以减轻图像生成模型带来的风险。", "translation": "生成模型已经迅速发展，能够生成逼真的输出。然而，它们的合成输出日益挑战了自然内容和AI生成内容之间的明确区分，因此需要鲁棒的水印技术。水印通常被期望保持目标图像的完整性，抵御移除尝试，并防止未经授权地复制到不相关的图像上。为了满足这一需求，最近的方法使用初始噪声将持久水印嵌入到扩散模型生成的图像中。然而，这样做要么扭曲了生成图像的分布，要么依赖于搜索长字典中的使用过的密钥进行检测。\n在本文中，我们提出了一种新颖的水印方法，将生成图像的语义信息直接嵌入到水印中，从而实现无失真水印，并且无需密钥模式数据库即可进行验证。相反，密钥模式可以通过使用局部敏感哈希从图像的语义嵌入中推断出来。此外，以原始图像内容为条件的水印检测提高了对伪造攻击的鲁棒性。为了证明这一点，我们考虑了两种在很大程度上被忽视的攻击策略：(i) 攻击者提取初始噪声并生成具有相同模式的新图像；(ii) 攻击者在水印图像中插入一个不相关的（可能是有害的）对象，同时可能保留水印。我们通过实验验证了我们方法对这些攻击的鲁棒性有所提高。总而言之，我们的结果表明，内容感知水印可以减轻图像生成模型带来的风险。", "summary": "本文提出SEAL（语义感知图像水印），一种新颖的水印方法，旨在解决AI生成内容的水印挑战。SEAL通过将图像的语义信息直接嵌入水印，实现了无失真且无需密钥数据库的检测。它利用局部敏感哈希从语义嵌入中推断密钥模式，并通过以原始图像内容为条件来增强对伪造攻击的鲁棒性。实验证明，SEAL对提取初始噪声和插入无关对象的攻击具有更强的抵抗力，表明内容感知水印能有效降低图像生成模型带来的风险。", "keywords": "图像水印, 语义感知, 生成模型, 鲁棒性, 局部敏感哈希", "comments": "SEAL的创新之处在于将语义信息直接融入水印，摆脱了传统水印对外部密钥数据库的依赖，并通过局部敏感哈希实现了高效、无失真检测。其对特定伪造攻击的鲁棒性提升，尤其是在考虑了以往被忽视的攻击策略方面，具有重要意义，有助于增强AI生成内容的溯源和真实性保障。"}}
{"id": "2503.12174", "pdf": "https://arxiv.org/pdf/2503.12174", "abs": "https://arxiv.org/abs/2503.12174", "authors": ["Jijiang Li", "Qingyue Deng", "Haibin Ling", "Bingyao Huang"], "title": "DPCS: Path Tracing-Based Differentiable Projector-Camera Systems", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages,16 figures", "summary": "Projector-camera systems (ProCams) simulation aims to model the physical\nproject-and-capture process and associated scene parameters of a ProCams, and\nis crucial for spatial augmented reality (SAR) applications such as ProCams\nrelighting and projector compensation. Recent advances use an end-to-end neural\nnetwork to learn the project-and-capture process. However, these neural\nnetwork-based methods often implicitly encapsulate scene parameters, such as\nsurface material, gamma, and white balance in the network parameters, and are\nless interpretable and hard for novel scene simulation. Moreover, neural\nnetworks usually learn the indirect illumination implicitly in an\nimage-to-image translation way which leads to poor performance in simulating\ncomplex projection effects such as soft-shadow and interreflection. In this\npaper, we introduce a novel path tracing-based differentiable projector-camera\nsystems (DPCS), offering a differentiable ProCams simulation method that\nexplicitly integrates multi-bounce path tracing. Our DPCS models the physical\nproject-and-capture process using differentiable physically-based rendering\n(PBR), enabling the scene parameters to be explicitly decoupled and learned\nusing much fewer samples. Moreover, our physically-based method not only\nenables high-quality downstream ProCams tasks, such as ProCams relighting and\nprojector compensation, but also allows novel scene simulation using the\nlearned scene parameters. In experiments, DPCS demonstrates clear advantages\nover previous approaches in ProCams simulation, offering better\ninterpretability, more efficient handling of complex interreflection and\nshadow, and requiring fewer training samples.", "AI": {"title_translation": "DPCS：基于路径追踪的可微分投影仪-相机系统", "tldr": "本文提出了DPCS，一种基于路径追踪的可微分投影仪-相机系统模拟方法，解决了现有神经网络方法在场景参数可解释性和复杂投影效果模拟方面的不足，提供更高效、可解释的ProCams模拟。", "motivation": "现有的投影仪-相机系统（ProCams）模拟方法多采用端到端神经网络，但这些方法通常隐式封装场景参数（如表面材质、伽马、白平衡），导致可解释性差，难以用于新场景模拟。此外，神经网络隐式学习间接照明，在模拟复杂投影效果（如软阴影和相互反射）时性能不佳。", "method": "本文引入了一种新颖的基于路径追踪的可微分投影仪-相机系统（DPCS），该方法通过可微分物理渲染（PBR）明确集成多重弹射路径追踪，模拟物理投影和捕获过程。这使得场景参数能够被显式解耦并用更少的样本学习。", "result": "实验表明，DPCS在ProCams模拟方面比现有方法具有明显优势，提供了更好的可解释性，更有效地处理复杂的相互反射和阴影，并且需要更少的训练样本。", "conclusion": "DPCS作为一种基于物理的可微分ProCams模拟方法，不仅能够实现高质量的下游ProCams任务（如ProCams重照明和投影仪补偿），还能利用学习到的场景参数进行新场景模拟。", "translation": "投影仪-相机系统（ProCams）模拟旨在建模ProCams的物理投影和捕获过程以及相关的场景参数，这对于空间增强现实（SAR）应用（如ProCams重照明和投影仪补偿）至关重要。最近的进展使用端到端神经网络来学习投影和捕获过程。然而，这些基于神经网络的方法通常将场景参数（如表面材质、伽马和白平衡）隐式封装在网络参数中，导致可解释性较差，难以用于新场景模拟。此外，神经网络通常以图像到图像转换的方式隐式学习间接照明，这导致在模拟复杂投影效果（如软阴影和相互反射）时性能不佳。在本文中，我们介绍了一种新颖的基于路径追踪的可微分投影仪-相机系统（DPCS），提供了一种可微分的ProCams模拟方法，该方法明确集成了多重弹射路径追踪。我们的DPCS使用可微分物理渲染（PBR）对物理投影和捕获过程进行建模，使得场景参数能够被显式解耦并用更少的样本学习。此外，我们基于物理的方法不仅能够实现高质量的下游ProCams任务（如ProCams重照明和投影仪补偿），还能利用学习到的场景参数进行新场景模拟。在实验中，DPCS在ProCams模拟方面比以前的方法表现出明显的优势，提供了更好的可解释性，更有效地处理复杂的相互反射和阴影，并且需要更少的训练样本。", "summary": "本文提出了一种名为DPCS（Path Tracing-Based Differentiable Projector-Camera Systems）的新型可微分投影仪-相机系统模拟方法。针对现有神经网络方法在ProCams模拟中存在的场景参数隐式封装、可解释性差以及难以处理复杂间接照明和阴影的问题，DPCS通过显式集成多重弹射路径追踪和可微分物理渲染（PBR）来建模物理投影与捕获过程。该方法能够显式解耦并学习场景参数，所需样本更少，并能有效模拟复杂投影效果，从而提升ProCams模拟的可解释性、效率和在新场景下的适用性。", "keywords": "投影仪-相机系统, 路径追踪, 可微分渲染, 物理渲染, ProCams模拟", "comments": "DPCS通过引入基于物理的路径追踪和可微分渲染，显著提升了ProCams模拟的可解释性和对复杂光照效果的处理能力，解决了传统神经网络方法在这些方面的固有局限性。其创新点在于将物理模拟与可微分框架相结合，使得场景参数能够被显式学习和控制，这对于实际SAR应用具有重要意义。"}}
{"id": "2503.12180", "pdf": "https://arxiv.org/pdf/2503.12180", "abs": "https://arxiv.org/abs/2503.12180", "authors": ["Yuhang Peng", "Sidong Wang", "Jihaoyu Yang", "Shilong Li", "Han Wang", "Jiangtao Gong"], "title": "Bench2FreeAD: A Benchmark for Vision-based End-to-end Navigation in Unstructured Robotic Environments", "categories": ["cs.RO", "cs.CV", "68T45"], "comment": "7 pages, 9 figures", "summary": "Most current end-to-end (E2E) autonomous driving algorithms are built on\nstandard vehicles in structured transportation scenarios, lacking exploration\nof robot navigation for unstructured scenarios such as auxiliary roads, campus\nroads, and indoor settings. This paper investigates E2E robot navigation in\nunstructured road environments. First, we introduce two data collection\npipelines - one for real-world robot data and another for synthetic data\ngenerated using the Isaac Sim simulator, which together produce an unstructured\nrobotics navigation dataset -- FreeWorld Dataset. Second, we fine-tuned an\nefficient E2E autonomous driving model -- VAD -- using our datasets to validate\nthe performance and adaptability of E2E autonomous driving models in these\nenvironments. Results demonstrate that fine-tuning through our datasets\nsignificantly enhances the navigation potential of E2E autonomous driving\nmodels in unstructured robotic environments. Thus, this paper presents the\nfirst dataset targeting E2E robot navigation tasks in unstructured scenarios,\nand provides a benchmark based on vision-based E2E autonomous driving\nalgorithms to facilitate the development of E2E navigation technology for\nlogistics and service robots. The project is available on Github.", "AI": {"title_translation": "Bench2FreeAD：非结构化机器人环境中基于视觉的端到端导航基准", "tldr": "本文提出了一个针对非结构化机器人环境的视觉端到端导航数据集和基准，并通过微调现有模型验证了其有效性。", "motivation": "大多数当前的端到端（E2E）自动驾驶算法主要针对结构化交通场景中的标准车辆，而缺乏对辅助道路、校园道路和室内环境等非结构化场景中机器人导航的探索。本文旨在解决这一不足，研究非结构化道路环境中的E2E机器人导航。", "method": "研究团队首先引入了两个数据收集管道：一个用于真实世界机器人数据，另一个用于使用Isaac Sim模拟器生成的合成数据，共同构建了一个非结构化机器人导航数据集——FreeWorld数据集。随后，他们使用该数据集对一个高效的E2E自动驾驶模型——VAD进行了微调，以验证其在非结构化环境中的性能和适应性。", "result": "结果表明，通过使用FreeWorld数据集进行微调，E2E自动驾驶模型在非结构化机器人环境中的导航潜力得到了显著增强。", "conclusion": "本文首次提出了一个针对非结构化场景中E2E机器人导航任务的数据集，并提供了一个基于视觉的E2E自动驾驶算法基准，旨在促进物流和服务机器人的E2E导航技术发展。", "translation": "大多数当前的端到端（E2E）自动驾驶算法都是基于标准车辆在结构化交通场景中构建的，缺乏对辅助道路、校园道路和室内环境等非结构化场景中机器人导航的探索。本文研究了非结构化道路环境中的E2E机器人导航。首先，我们引入了两个数据收集管道——一个用于真实世界机器人数据，另一个用于使用Isaac Sim模拟器生成的合成数据，它们共同生成了一个非结构化机器人导航数据集——FreeWorld数据集。其次，我们使用我们的数据集对一个高效的E2E自动驾驶模型——VAD进行了微调，以验证E2E自动驾驶模型在这些环境中的性能和适应性。结果表明，通过我们的数据集进行微调显著增强了E2E自动驾驶模型在非结构化机器人环境中的导航潜力。因此，本文提出了第一个针对非结构化场景中E2E机器人导航任务的数据集，并提供了一个基于视觉的E2E自动驾驶算法的基准，以促进物流和服务机器人的E2E导航技术的发展。该项目已在Github上开源。", "summary": "本文针对当前端到端自动驾驶算法在非结构化机器人导航方面的不足，提出了一个名为FreeWorld的数据集，该数据集通过真实世界和模拟数据收集而成。研究人员利用此数据集微调了VAD模型，并验证了其在非结构化环境中的导航性能显著提升。该工作首次提供了针对非结构化场景中E2E机器人导航任务的数据集及基于视觉的E2E自动驾驶算法基准，旨在推动物流和服务机器人的E2E导航技术发展。", "keywords": "端到端导航, 非结构化环境, 机器人导航, FreeWorld数据集, 视觉自动驾驶", "comments": "本文的创新之处在于填补了端到端自动驾驶在非结构化机器人环境中的研究空白，为物流和服务机器人提供了关键的导航技术支持。FreeWorld数据集的创建和相关基准的建立是重要的贡献，为该领域未来的研究和开发提供了宝贵的资源。"}}
{"id": "2503.12229", "pdf": "https://arxiv.org/pdf/2503.12229", "abs": "https://arxiv.org/abs/2503.12229", "authors": ["William Louis Rothman", "Yasuyuki Matsushita"], "title": "Shadow Art Kanji: Inverse Rendering Application", "categories": ["cs.GR", "cs.CV"], "comment": "7 pages, 10 figures, 8 references", "summary": "Finding a balance between artistic beauty and machine-generated imagery is\nalways a difficult task. This project seeks to create 3D models that, when\nilluminated, cast shadows resembling Kanji characters. It aims to combine\nartistic expression with computational techniques, providing an accurate and\nefficient approach to visualizing these Japanese characters through shadows.", "AI": {"title_translation": "影绘汉字：逆渲染应用", "tldr": "该项目旨在创建能投射出汉字阴影的3D模型，结合艺术与计算技术。", "motivation": "在艺术美感和机器生成图像之间找到平衡始终是一项艰巨的任务。", "method": "本项目通过创建3D模型，使其在光照下能够投射出类似于汉字的阴影，旨在将艺术表达与计算技术相结合。", "result": "提供了一种准确高效的通过阴影可视化日文汉字的方法。", "conclusion": "该项目提供了一种准确高效的通过阴影可视化日文汉字的方法，结合了艺术表达与计算技术。", "translation": "在艺术美感和机器生成图像之间找到平衡始终是一项艰巨的任务。本项目旨在创建三维模型，使其在光照下能够投射出类似于汉字的阴影。它旨在将艺术表达与计算技术相结合，提供一种准确高效的方法，通过阴影来可视化这些日文汉字。", "summary": "该项目旨在解决艺术美感与机器生成图像之间的平衡难题，通过逆渲染应用创建特殊的3D模型，使其在光照下能投射出汉字阴影，从而提供一种准确高效的日文汉字阴影可视化方法，结合了艺术表达与计算技术。", "keywords": "影绘艺术, 汉字, 逆渲染, 3D模型, 计算艺术", "comments": "该项目将传统的影绘艺术与现代逆渲染技术相结合，为汉字的可视化提供了新颖的计算艺术应用。其创新点在于通过计算方法实现了艺术与机器生成图像的平衡，具有独特的视觉和技术结合价值。"}}
{"id": "2503.12365", "pdf": "https://arxiv.org/pdf/2503.12365", "abs": "https://arxiv.org/abs/2503.12365", "authors": ["Xiangfei Fang", "Boying Wang", "Chengying Huan", "Shaonan Ma", "Heng Zhang", "Chen Zhao"], "title": "HyperKAN: Hypergraph Representation Learning with Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.CV", "cs.SI"], "comment": "Accepted by ICASSP2025", "summary": "Hypergraph representation learning has garnered increasing attention across\nvarious domains due to its capability to model high-order relationships.\nTraditional methods often rely on hypergraph neural networks (HNNs) employing\nmessage passing mechanisms to aggregate vertex and hyperedge features. However,\nthese methods are constrained by their dependence on hypergraph topology,\nleading to the challenge of imbalanced information aggregation, where\nhigh-degree vertices tend to aggregate redundant features, while low-degree\nvertices often struggle to capture sufficient structural features. To overcome\nthe above challenges, we introduce HyperKAN, a novel framework for hypergraph\nrepresentation learning that transcends the limitations of message-passing\ntechniques. HyperKAN begins by encoding features for each vertex and then\nleverages Kolmogorov-Arnold Networks (KANs) to capture complex nonlinear\nrelationships. By adjusting structural features based on similarity, our\napproach generates refined vertex representations that effectively addresses\nthe challenge of imbalanced information aggregation. Experiments conducted on\nthe real-world datasets demonstrate that HyperKAN significantly outperforms\nstate of-the-art HNN methods, achieving nearly a 9% performance improvement on\nthe Senate dataset.", "AI": {"title_translation": "HyperKAN：基于Kolmogorov-Arnold网络的超图表示学习", "tldr": "HyperKAN是一个新的超图表示学习框架，它利用Kolmogorov-Arnold网络来解决传统超图神经网络中信息聚合不平衡的问题，并在真实数据集上取得了显著的性能提升。", "motivation": "传统的超图神经网络（HNNs）依赖于消息传递机制，但受限于超图拓扑结构，导致信息聚合不平衡：高阶顶点聚合冗余特征，低阶顶点难以捕获足够的结构特征。本研究旨在克服这些挑战。", "method": "HyperKAN框架首先编码每个顶点的特征，然后利用Kolmogorov-Arnold网络（KANs）捕获复杂的非线性关系。通过基于相似性调整结构特征，该方法生成精细的顶点表示，有效解决了信息聚合不平衡的问题。", "result": "在真实世界数据集上的实验表明，HyperKAN显著优于最先进的HNN方法，在Senate数据集上实现了近9%的性能提升。", "conclusion": "HyperKAN通过引入Kolmogorov-Arnold网络和相似性调整机制，成功解决了传统超图表示学习中信息聚合不平衡的挑战，并取得了优于现有方法的性能。", "translation": "超图表示学习因其建模高阶关系的能力而在各个领域受到越来越多的关注。传统方法通常依赖于采用消息传递机制聚合顶点和超边特征的超图神经网络（HNNs）。然而，这些方法受限于它们对超图拓扑的依赖，导致信息聚合不平衡的挑战，即高阶顶点倾向于聚合冗余特征，而低阶顶点往往难以捕获足够的结构特征。为了克服上述挑战，我们引入了HyperKAN，一个用于超图表示学习的新颖框架，它超越了消息传递技术的局限性。HyperKAN首先编码每个顶点的特征，然后利用Kolmogorov-Arnold网络（KANs）捕获复杂的非线性关系。通过基于相似性调整结构特征，我们的方法生成了精细的顶点表示，有效解决了信息聚合不平衡的挑战。在真实世界数据集上进行的实验表明，HyperKAN显著优于最先进的HNN方法，在Senate数据集上实现了近9%的性能提升。", "summary": "本论文介绍了HyperKAN，一个新颖的超图表示学习框架，旨在解决传统超图神经网络中因消息传递机制导致的信息聚合不平衡问题。HyperKAN通过编码顶点特征并利用Kolmogorov-Arnold网络捕获非线性关系，同时基于相似性调整结构特征，生成了更精细的顶点表示。实验结果表明，HyperKAN在真实数据集上表现优异，显著超越了现有最先进的超图神经网络方法。", "keywords": "超图表示学习, Kolmogorov-Arnold网络, 信息聚合, HyperKAN, 非线性关系", "comments": "本论文的创新点在于将Kolmogorov-Arnold网络（KANs）引入超图表示学习领域，并提出了通过相似性调整结构特征来解决传统消息传递机制中信息聚合不平衡的挑战。这种方法提供了一种新的视角来处理超图数据，尤其是在处理高阶和低阶顶点特征聚合时。其在真实世界数据集上的显著性能提升证明了该方法的有效性和潜力。"}}
{"id": "2503.12466", "pdf": "https://arxiv.org/pdf/2503.12466", "abs": "https://arxiv.org/abs/2503.12466", "authors": ["Jiahang Cao", "Qiang Zhang", "Hanzhong Guo", "Jiaxu Wang", "Hao Cheng", "Renjing Xu"], "title": "Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ICLR 2025 Generative Models for Robot Learning Workshop", "summary": "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps://github.com/AndyCao1125/MCDP.", "AI": {"title_translation": "模态可组合扩散策略通过推理时分布级组合", "tldr": "提出MCDP，通过组合预训练的单模态扩散策略来提高多模态适应性和性能，无需额外训练。", "motivation": "当前扩散策略（DP）通常基于单一视觉模态，限制了其准确性和泛化潜力；训练处理异构多模态数据的通用DP成本高昂。", "method": "提出模态可组合扩散策略（MCDP），通过结合多个基于独立视觉模态的预训练DP的分布分数来形成更具表达力的策略，无需额外训练。", "result": "在RoboTwin数据集上的广泛实验表明，MCDP能够提高适应性和性能。", "conclusion": "该研究为现有DP的灵活组合提供了有价值的见解，促进了可泛化的跨模态、跨领域甚至跨具身策略的开发。", "translation": "扩散策略（DP）作为一种有效的策略表示方法，因其建模多分布动态的能力而受到广泛关注。然而，当前的DP通常基于单一视觉模态（例如RGB或点云），这限制了它们的准确性和泛化潜力。尽管训练一个能够处理异构多模态数据的通用DP会提高性能，但这会带来巨大的计算和数据相关成本。为了解决这些挑战，我们提出了一种新颖的策略组合方法：通过利用多个基于独立视觉模态的预训练DP，我们可以结合它们的分布分数，形成一个更具表达力的模态可组合扩散策略（MCDP），而无需额外的训练。通过在RoboTwin数据集上进行广泛的实证实验，我们证明了MCDP在提高适应性和性能方面的潜力。这项探索旨在为现有DP的灵活组合提供有价值的见解，促进可泛化的跨模态、跨领域甚至跨具身策略的开发。我们的代码已在https://github.com/AndyCao1125/MCDP开源。", "summary": "本文提出一种新颖的模态可组合扩散策略（MCDP），旨在解决当前扩散策略在单一视觉模态下的泛化限制及训练多模态DP的高成本问题。MCDP通过在推理时组合多个预训练的单模态扩散策略的分布分数，无需额外训练即可形成更具表达力的策略。在RoboTwin数据集上的实验证明，MCDP能有效提升策略的适应性和性能，为开发通用的跨模态、跨领域和跨具身策略提供了新思路。", "keywords": "扩散策略, 多模态学习, 策略组合, 机器人学习, 泛化", "comments": "该论文的创新点在于提出了一种无需额外训练即可实现多模态策略组合的方法，即在推理时结合不同预训练单模态扩散策略的分布分数。这显著降低了训练多模态通用策略的计算和数据成本，为机器人学习和策略泛化提供了一条高效的途径。"}}
{"id": "2503.12505", "pdf": "https://arxiv.org/pdf/2503.12505", "abs": "https://arxiv.org/abs/2503.12505", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Jiaxin Ai", "Wangbo Zhao", "Kai Wang", "Xiaojiang Peng", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.", "AI": {"title_translation": "MPBench：一个用于过程错误识别的综合多模态推理基准", "tldr": "引入MPBench，一个全面的多模态基准，用于评估过程级奖励模型在识别推理过程错误方面的有效性。", "motivation": "现有过程级奖励模型（PRMs）的基准是基于文本且侧重于错误检测，忽略了推理搜索等其他场景，无法全面评估PRMs在多模态和多任务场景下的有效性。", "method": "本文引入了MPBench，一个综合的、多任务、多模态基准，通过三种评估范式系统地评估PRMs：(1) 步骤正确性，评估每个中间推理步骤的正确性；(2) 答案聚合，聚合多个解决方案并选择最佳方案；(3) 推理过程搜索，指导推理过程中最优推理步骤的搜索。", "result": "MPBench实现了对多模态PRMs的全面评估，并为多模态PRMs的开发提供了见解。", "conclusion": "MPBench作为第一个综合的多模态推理基准，填补了现有PRM评估的空白，为未来多模态PRM的发展提供了重要的评估工具和方向。", "translation": "推理是大型语言模型（LLMs）解决复杂任务的基本能力，其中过程错误的识别对于提高这种能力至关重要。最近，过程级奖励模型（PRMs）被提出，用于提供分步奖励，以促进训练期间的强化学习和数据生产，并在推理期间引导LLMs走向正确的步骤，从而提高推理准确性。然而，现有的PRMs基准是基于文本的，并且侧重于错误检测，忽略了推理搜索等其他场景。为了解决这一空白，我们引入了MPBench，一个综合的、多任务、多模态基准，旨在系统地评估PRMs在不同场景下的有效性。MPBench采用了三种评估范式，每种范式都针对PRMs在推理过程中的特定作用：(1) 步骤正确性，评估每个中间推理步骤的正确性；(2) 答案聚合，聚合多个解决方案并选择最佳方案；(3) 推理过程搜索，指导推理过程中最优推理步骤的搜索。通过这些范式，MPBench进行了全面评估，并为多模态PRMs的开发提供了见解。", "summary": "本文介绍了MPBench，一个综合的多模态推理基准，旨在弥补现有过程级奖励模型（PRMs）评估中缺乏多模态和推理搜索场景的不足。MPBench通过“步骤正确性”、“答案聚合”和“推理过程搜索”三种评估范式，系统地评估PRMs在识别和纠正推理过程错误方面的能力，从而为多模态PRMs的发展提供全面评估和指导。", "keywords": "多模态推理, 过程级奖励模型, 基准, 错误识别, 大型语言模型", "comments": "MPBench的创新之处在于它是第一个将PRM评估扩展到多模态和推理搜索场景的基准，填补了现有文本基准的空白。其提出的三种评估范式全面覆盖了PRM在推理过程中的不同作用，对于促进多模态PRM的开发和提高LLMs的推理能力具有重要意义。"}}
{"id": "2503.12536", "pdf": "https://arxiv.org/pdf/2503.12536", "abs": "https://arxiv.org/abs/2503.12536", "authors": ["Lin-Chun Huang", "Ching Chieh Tsao", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model", "categories": ["cs.LG", "cs.CV", "cs.CY"], "comment": null, "summary": "Image generative models, particularly diffusion-based models, have surged in\npopularity due to their remarkable ability to synthesize highly realistic\nimages. However, since these models are data-driven, they inherit biases from\nthe training datasets, frequently leading to disproportionate group\nrepresentations that exacerbate societal inequities. Traditionally, efforts to\ndebiase these models have relied on predefined sensitive attributes,\nclassifiers trained on such attributes, or large language models to steer\noutputs toward fairness. However, these approaches face notable drawbacks:\npredefined attributes do not adequately capture complex and continuous\nvariations among groups. To address these issues, we introduce the Debiasing\nDiffusion Model (DDM), which leverages an indicator to learn latent\nrepresentations during training, promoting fairness through balanced\nrepresentations without requiring predefined sensitive attributes. This\napproach not only demonstrates its effectiveness in scenarios previously\naddressed by conventional techniques but also enhances fairness without relying\non predefined sensitive attributes as conditions. In this paper, we discuss the\nlimitations of prior bias mitigation techniques in diffusion-based models,\nelaborate on the architecture of the DDM, and validate the effectiveness of our\napproach through experiments.", "AI": {"title_translation": "去偏扩散模型：通过稳定扩散模型中的潜在表示学习增强公平性", "tldr": "本文提出一种去偏扩散模型（DDM），通过学习潜在表示来增强扩散模型的公平性，无需预定义敏感属性，解决了传统方法在处理复杂群体差异时的局限性。", "motivation": "图像生成模型（特别是扩散模型）因其生成逼真图像的能力而广受欢迎，但它们从训练数据中继承偏见，导致群体表示不均衡，加剧社会不平等。传统去偏方法依赖预定义敏感属性，无法充分捕捉复杂和连续的群体差异。", "method": "本文引入了去偏扩散模型（DDM），该模型利用一个指示器在训练过程中学习潜在表示，通过平衡的表示来促进公平性，而无需预定义敏感属性。", "result": "DDM不仅在传统技术解决的场景中表现出有效性，而且在不依赖预定义敏感属性作为条件的情况下增强了公平性。", "conclusion": "DDM通过学习潜在表示提供了一种有效且无需预定义敏感属性的去偏方法，克服了传统技术的局限性，在扩散模型中实现了更公平的图像生成。", "translation": "图像生成模型，特别是基于扩散的模型，因其合成高度逼真图像的卓越能力而广受欢迎。然而，由于这些模型是数据驱动的，它们从训练数据集中继承了偏见，经常导致群体表示不均衡，从而加剧了社会不平等。传统上，去偏这些模型的努力依赖于预定义的敏感属性、基于此类属性训练的分类器或大型语言模型来引导输出趋向公平。然而，这些方法面临显著的缺点：预定义的属性不足以捕捉群体之间复杂和连续的变化。为了解决这些问题，我们引入了去偏扩散模型（DDM），它利用一个指示器在训练过程中学习潜在表示，通过平衡的表示来促进公平性，而无需预定义敏感属性。这种方法不仅在以前由传统技术解决的场景中展示了其有效性，而且在不依赖预定义敏感属性作为条件的情况下增强了公平性。在本文中，我们讨论了基于扩散模型中先前偏见缓解技术的局限性，详细阐述了DDM的架构，并通过实验验证了我们方法的有效性。", "summary": "本文提出了去偏扩散模型（DDM），旨在解决扩散模型中因训练数据偏见导致的群体表示不均衡问题。与传统依赖预定义敏感属性的方法不同，DDM通过在训练过程中学习潜在表示来促进公平性，无需预设敏感属性。实验证明，DDM在增强图像生成公平性方面表现出有效性，并克服了现有技术的局限性。", "keywords": "扩散模型, 偏见消除, 公平性, 潜在表示学习, 稳定扩散模型", "comments": "这项工作通过引入一种无需预定义敏感属性的去偏方法，为扩散模型的公平性研究带来了创新。它解决了传统方法在处理复杂和连续群体差异时的局限性，使得去偏过程更加灵活和通用。这项研究对于减少AI生成内容中的社会偏见具有重要意义。"}}
{"id": "2503.12549", "pdf": "https://arxiv.org/pdf/2503.12549", "abs": "https://arxiv.org/abs/2503.12549", "authors": ["Alexander Koebler", "Ralf Gross", "Florian Buettner", "Ingo Thon"], "title": "Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Published at ECML PKDD 2022", "summary": "Flexible industrial production systems will play a central role in the future\nof manufacturing due to higher product individualization and customization. A\nkey component in such systems is the robotic grasping of known or unknown\nobjects in random positions. Real-world applications often come with challenges\nthat might not be considered in grasping solutions tested in simulation or lab\nsettings. Partial occlusion of the target object is the most prominent.\nExamples of occlusion can be supporting structures in the camera's field of\nview, sensor imprecision, or parts occluding each other due to the production\nprocess. In all these cases, the resulting lack of information leads to\nshortcomings in calculating grasping points. In this paper, we present an\nalgorithm to reconstruct the missing information. Our inpainting solution\nfacilitates the real-world utilization of robust object matching approaches for\ngrasping point calculation. We demonstrate the benefit of our solution by\nenabling an existing grasping system embedded in a real-world industrial\napplication to handle occlusions in the input. With our solution, we\ndrastically decrease the number of objects discarded by the process.", "AI": {"title_translation": "使用基于自编码器点云修复的局部遮挡物体抓取", "tldr": "本文提出一种基于自编码器的点云修复算法，用于解决工业机器人抓取局部遮挡物体时信息缺失的问题，显著减少废弃物体数量。", "motivation": "未来的柔性工业生产系统对产品个性化和定制化要求更高，其中机器人抓取已知或未知物体是关键组成部分。实际应用中，目标物体的局部遮挡是主要挑战，导致抓取点计算信息不足。", "method": "本文提出一种算法来重建缺失的信息。该修复方案（基于自编码器点云修复）有助于实现鲁棒的物体匹配，从而进行抓取点计算。", "result": "该解决方案使现有抓取系统能够在真实的工业应用中处理输入中的遮挡，从而大幅减少了被流程废弃的物体数量。", "conclusion": "所提出的基于自编码器的点云修复算法能够有效重建因局部遮挡而缺失的信息，从而显著提升了机器人抓取在真实工业环境中的鲁棒性和效率。", "translation": "由于更高的产品个性化和定制化，柔性工业生产系统将在未来的制造业中发挥核心作用。此类系统中的一个关键组成部分是机器人对随机位置的已知或未知物体的抓取。实际应用中经常会遇到在仿真或实验室环境中测试的抓取解决方案可能未考虑到的挑战。目标物体的局部遮挡是最突出的问题。遮挡的例子可以是相机视野中的支撑结构、传感器精度不足，或由于生产过程而相互遮挡的部件。在所有这些情况下，由此导致的信息缺失会导致抓取点计算的不足。在本文中，我们提出了一种算法来重建缺失的信息。我们的修复解决方案有助于在抓取点计算中实际利用鲁棒的物体匹配方法。我们通过使嵌入在真实工业应用中的现有抓取系统能够处理输入中的遮挡来证明我们解决方案的优势。通过我们的解决方案，我们大幅减少了被流程废弃的物体数量。", "summary": "本文针对柔性工业生产中机器人抓取局部遮挡物体时信息缺失的挑战，提出了一种基于自编码器的点云修复算法。该算法旨在重建缺失的物体信息，从而提高抓取点计算的准确性。在实际工业应用中，该解决方案被证明能够使现有抓取系统有效处理输入中的遮挡，显著降低了废弃物体的数量。", "keywords": "机器人抓取, 点云修复, 自编码器, 局部遮挡, 工业应用", "comments": "本文的创新点在于将基于自编码器的点云修复技术应用于解决机器人抓取中的实际遮挡问题，这对于工业部署至关重要。其重要性体现在大幅减少了废弃物体数量，表明了该方案在提高生产效率和系统鲁棒性方面的显著优势。"}}
{"id": "2503.12553", "pdf": "https://arxiv.org/pdf/2503.12553", "abs": "https://arxiv.org/abs/2503.12553", "authors": ["Xianzu Wu", "Zhenxin Ai", "Harry Yang", "Ser-Nam Lim", "Jun Liu", "Huan Wang"], "title": "Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in single-view 3D scene reconstruction have highlighted the\nchallenges in capturing fine geometric details and ensuring structural\nconsistency, particularly in high-fidelity outdoor scene modeling. This paper\npresents Niagara, a new single-view 3D scene reconstruction framework that can\nfaithfully reconstruct challenging outdoor scenes from a single input image for\nthe first time.\n  Our approach integrates monocular depth and normal estimation as input, which\nsubstantially improves its ability to capture fine details, mitigating common\nissues like geometric detail loss and deformation.\n  Additionally, we introduce a geometric affine field (GAF) and 3D\nself-attention as geometry-constraint, which combines the structural properties\nof explicit geometry with the adaptability of implicit feature fields, striking\na balance between efficient rendering and high-fidelity reconstruction.\n  Our framework finally proposes a specialized encoder-decoder architecture,\nwhere a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian\nparameters, which can be used for novel view synthesis. Extensive results and\nanalyses suggest that our Niagara surpasses prior SoTA approaches such as\nFlash3D in both single-view and dual-view settings, significantly enhancing the\ngeometric accuracy and visual fidelity, especially in outdoor scenes.", "AI": {"title_translation": "Niagara：用于单视角场景重建的法线集成几何仿射场", "tldr": "Niagara是一个新的单视角3D场景重建框架，它通过集成法线信息、引入几何仿射场和3D自注意力机制，并采用专门的编码器-解码器架构，首次实现了高保真户外场景的重建，超越了现有最先进的方法。", "motivation": "单视角3D场景重建面临在捕捉精细几何细节和确保结构一致性方面的挑战，尤其是在高保真户外场景建模中。", "method": "Niagara框架通过以下方式进行重建：1. 集成单目深度和法线估计作为输入，以捕捉精细细节。2. 引入几何仿射场（GAF）和3D自注意力作为几何约束，结合显式几何的结构特性和隐式特征场的适应性。3. 提出一种专门的编码器-解码器架构，其中包含一个基于深度的3D高斯解码器，用于预测3D高斯参数，可用于新视角合成。", "result": "广泛的实验结果和分析表明，Niagara在单视角和双视角设置下均超越了先前的最先进方法（如Flash3D），显著提升了几何精度和视觉保真度，尤其是在户外场景中表现突出。", "conclusion": "Niagara框架首次成功地从单个输入图像高保真地重建了具有挑战性的户外场景，解决了单视角3D场景重建中捕捉精细几何细节和确保结构一致性的关键挑战，并超越了现有技术水平。", "translation": "近期在单视角3D场景重建方面的进展突显了在捕捉精细几何细节和确保结构一致性方面的挑战，尤其是在高保真户外场景建模中。本文提出了Niagara，一个全新的单视角3D场景重建框架，首次能够从单个输入图像忠实地重建具有挑战性的户外场景。\n我们的方法整合了单目深度和法线估计作为输入，这大大提高了其捕捉精细细节的能力，缓解了几何细节丢失和变形等常见问题。\n此外，我们引入了几何仿射场（GAF）和3D自注意力作为几何约束，它结合了显式几何的结构特性和隐式特征场的适应性，在高效渲染和高保真重建之间取得了平衡。\n我们的框架最终提出了一个专门的编码器-解码器架构，其中提出了一个基于深度的3D高斯解码器来预测3D高斯参数，可用于新视角合成。广泛的结果和分析表明，我们的Niagara在单视角和双视角设置下都超越了先前的最先进方法，如Flash3D，显著提升了几何精度和视觉保真度，尤其是在户外场景中。", "summary": "Niagara是一个新颖的单视角3D场景重建框架，旨在解决户外场景中精细几何细节捕捉和结构一致性的挑战。该方法通过集成单目深度和法线估计作为输入，并引入几何仿射场（GAF）和3D自注意力作为几何约束，从而平衡了高效渲染和高保真重建。此外，它还提出了一种专门的编码器-解码器架构，其中包含一个基于深度的3D高斯解码器。实验结果表明，Niagara在几何精度和视觉保真度方面均超越了现有最先进的方法，尤其是在户外场景重建中表现出色。", "keywords": "单视角3D重建, 几何仿射场, 户外场景, 法线集成, 3D高斯", "comments": "Niagara通过将法线集成到输入中、引入几何仿射场（GAF）和3D自注意力作为几何约束，以及采用创新的基于深度的3D高斯解码器，在单视角3D场景重建领域取得了显著突破。它首次实现了高保真户外场景的重建，克服了传统方法在细节捕捉和结构一致性上的局限性，对该领域具有重要意义。"}}
{"id": "2503.12609", "pdf": "https://arxiv.org/pdf/2503.12609", "abs": "https://arxiv.org/abs/2503.12609", "authors": ["Yitian Shi", "Di Wen", "Guanqi Chen", "Edgar Welte", "Sheng Liu", "Kunyu Peng", "Rainer Stiefelhagen", "Rania Rayyes"], "title": "VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active View Planning and Grasping in Clutter and Invisibility", "categories": ["cs.RO", "cs.CV"], "comment": "Under review", "summary": "We propose VISO-Grasp, a novel vision-language-informed system designed to\nsystematically address visibility constraints for grasping in severely occluded\nenvironments. By leveraging Foundation Models (FMs) for spatial reasoning and\nactive view planning, our framework constructs and updates an instance-centric\nrepresentation of spatial relationships, enhancing grasp success under\nchallenging occlusions. Furthermore, this representation facilitates active\nNext-Best-View (NBV) planning and optimizes sequential grasping strategies when\ndirect grasping is infeasible. Additionally, we introduce a multi-view\nuncertainty-driven grasp fusion mechanism that refines grasp confidence and\ndirectional uncertainty in real-time, ensuring robust and stable grasp\nexecution. Extensive real-world experiments demonstrate that VISO-Grasp\nachieves a success rate of $87.5\\%$ in target-oriented grasping with the fewest\ngrasp attempts outperforming baselines. To the best of our knowledge,\nVISO-Grasp is the first unified framework integrating FMs into target-aware\nactive view planning and 6-DoF grasping in environments with severe occlusions\nand entire invisibility constraints.", "AI": {"title_translation": "VISO-Grasp：在杂乱和不可见环境中，基于视觉-语言的空间物体中心六自由度主动视角规划与抓取", "tldr": "VISO-Grasp是一个利用基础模型进行空间推理和主动视角规划的系统，旨在解决严重遮挡环境下的抓取可见性问题，并实现了高成功率。", "motivation": "该研究旨在解决在严重遮挡环境下进行抓取时遇到的可见性限制问题。", "method": "VISO-Grasp系统利用基础模型（FMs）进行空间推理和主动视角规划，构建并更新以实例为中心的空间关系表示。这种表示有助于主动规划“下一最佳视角”（NBV）并优化顺序抓取策略。此外，系统引入了一种多视角不确定性驱动的抓取融合机制，实时细化抓取置信度和方向不确定性。", "result": "VISO-Grasp在目标导向抓取中实现了87.5%的成功率，且抓取尝试次数最少，优于基线方法。", "conclusion": "VISO-Grasp是首个将基础模型整合到目标感知主动视角规划和六自由度抓取中的统一框架，适用于存在严重遮挡和完全不可见限制的环境。", "translation": "我们提出了VISO-Grasp，一个新颖的视觉-语言信息系统，旨在系统地解决在严重遮挡环境下抓取时的可见性限制。通过利用基础模型（FMs）进行空间推理和主动视角规划，我们的框架构建并更新了以实例为中心的空间关系表示，从而在具有挑战性的遮挡下提高了抓取成功率。此外，这种表示促进了主动的“下一最佳视角”（NBV）规划，并在直接抓取不可行时优化了顺序抓取策略。此外，我们引入了一种多视角不确定性驱动的抓取融合机制，该机制实时细化抓取置信度和方向不确定性，确保了稳健和稳定的抓取执行。广泛的真实世界实验表明，VISO-Grasp在目标导向抓取中实现了87.5%的成功率，且抓取尝试次数最少，优于基线方法。据我们所知，VISO-Grasp是第一个将基础模型整合到目标感知主动视角规划和六自由度抓取中的统一框架，适用于存在严重遮挡和完全不可见限制的环境。", "summary": "VISO-Grasp是一个创新的视觉-语言系统，专门用于解决严重遮挡环境中机器人抓取的可见性问题。它利用基础模型进行空间推理和主动视角规划，构建实例级空间关系表示，从而在复杂遮挡下提高抓取成功率。系统还包括下一最佳视角规划、优化顺序抓取策略以及一个多视角不确定性驱动的抓取融合机制，以确保实时抓取的鲁棒性。实验证明，VISO-Grasp在目标导向抓取中表现出色，成功率高达87.5%，且抓取尝试次数更少，是首个将基础模型应用于严重遮挡和不可见环境中的目标感知主动视角规划和六自由度抓取的统一框架。", "keywords": "视觉-语言, 抓取, 遮挡, 主动视角规划, 基础模型", "comments": "该论文的创新点在于首次将基础模型整合到目标感知的主动视角规划和六自由度抓取中，有效解决了严重遮挡和完全不可见环境下的抓取挑战。其提出的实例中心空间表示和多视角不确定性驱动的抓取融合机制，显著提升了在复杂环境中的抓取成功率和鲁棒性，对机器人操作领域具有重要意义。"}}
{"id": "2503.12623", "pdf": "https://arxiv.org/pdf/2503.12623", "abs": "https://arxiv.org/abs/2503.12623", "authors": ["Vrushank Ahire", "Kunal Shah", "Mudasir Nazir Khan", "Nikhil Pakhale", "Lownish Rai Sookha", "M. A. Ganaie", "Abhinav Dhall"], "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal\nEmotion Network), a novel architecture for dynamic emotion recognition through\ndimensional modeling of affect. The model uniquely integrates visual, audio,\nand textual modalities via a bi-directional cross-modal attention mechanism\nwith six distinct attention pathways, enabling comprehensive interactions\nbetween all modality pairs. Our proposed approach employs modality-specific\nencoders to extract rich feature representations from synchronized video\nframes, audio segments, and transcripts. The architecture's novelty lies in its\ncross-modal enhancement strategy, where each modality representation is refined\nthrough weighted attention from other modalities, followed by self-attention\nrefinement through modality-specific encoders. Rather than directly predicting\nvalence-arousal values, MAVEN predicts emotions in a polar coordinate form,\naligning with psychological models of the emotion circumplex. Experimental\nevaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our\napproach, with performance measured using Concordance Correlation Coefficient\n(CCC). The multi-stage architecture demonstrates superior ability to capture\nthe complex, nuanced nature of emotional expressions in conversational videos,\nadvancing the state-of-the-art (SOTA) in continuous emotion recognition\nin-the-wild. Code can be found at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW.", "AI": {"title_translation": "MAVEN：用于效价-唤醒情绪的多模态注意力网络", "tldr": "MAVEN是一种新颖的多模态注意力网络，通过结合视觉、音频和文本模态的交叉注意力机制，以极坐标形式识别动态情绪，并在野外连续情绪识别方面达到了SOTA性能。", "motivation": "该论文旨在通过情感的维度建模来解决动态情绪识别问题，并提出一种能够有效整合多模态信息的新型架构。", "method": "MAVEN模型通过双向跨模态注意力机制（包含六个独立的注意力路径）独特地整合了视觉、音频和文本模态。它使用模态特定的编码器从同步的视频帧、音频片段和文本中提取特征。其新颖之处在于跨模态增强策略，即通过来自其他模态的加权注意力来细化每种模态的表示，随后通过模态特定编码器进行自注意力细化。模型以极坐标形式而非直接的效价-唤醒值来预测情绪。", "result": "MAVEN在Aff-Wild2数据集上进行了实验评估，并使用一致性相关系数（CCC）衡量性能，结果表明其方法有效。该多阶段架构在捕捉对话视频中复杂、细微的情绪表达方面表现出卓越的能力，并提升了野外连续情绪识别的最新技术水平（SOTA）。", "conclusion": "MAVEN通过其创新的多模态注意力网络和极坐标情绪预测方法，有效捕捉了复杂的情绪表达，显著推进了野外连续情绪识别的最新技术水平。", "translation": "本文介绍了MAVEN（Multi-modal Attention for Valence-Arousal Emotion Network），一种通过情感维度建模进行动态情绪识别的新颖架构。该模型通过一个具有六个不同注意力路径的双向跨模态注意力机制，独特地整合了视觉、音频和文本模态，从而实现了所有模态对之间的全面交互。我们提出的方法采用模态特定的编码器从同步的视频帧、音频片段和转录文本中提取丰富的特征表示。该架构的新颖之处在于其跨模态增强策略，其中每种模态表示通过来自其他模态的加权注意力进行细化，然后通过模态特定编码器进行自注意力细化。MAVEN不直接预测效价-唤醒值，而是以极坐标形式预测情绪，这与情绪圆周心理模型相符。在Aff-Wild2数据集上的实验评估证明了我们方法的有效性，性能使用一致性相关系数（CCC）进行衡量。该多阶段架构展示了捕捉对话视频中复杂、细微情绪表达的卓越能力，推动了野外连续情绪识别的最新技术（SOTA）。代码可在以下链接找到：https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW。", "summary": "MAVEN是一种用于动态情绪识别的新型多模态注意力网络。它独特地整合了视觉、音频和文本模态，通过双向跨模态注意力机制实现模态间的全面交互和增强。模型利用模态特定编码器提取特征，并以极坐标形式预测情绪，与心理学模型对齐。在Aff-Wild2数据集上的实验证明，MAVEN在捕捉复杂情绪方面表现出色，并显著提升了野外连续情绪识别的SOTA水平。", "keywords": "多模态注意力, 情绪识别, 效价-唤醒, 跨模态, 深度学习", "comments": "MAVEN的创新之处在于其独特的双向跨模态注意力机制和跨模态增强策略，能够全面整合并细化多模态信息。此外，以极坐标形式预测情绪，而非直接预测效价-唤醒值，使其更符合心理学情绪模型。该模型在野外连续情绪识别领域达到了SOTA性能，对该领域的研究具有重要意义。"}}
{"id": "2503.12642", "pdf": "https://arxiv.org/pdf/2503.12642", "abs": "https://arxiv.org/abs/2503.12642", "authors": ["Anjali Dharmik"], "title": "COVID 19 Diagnosis Analysis using Transfer Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Coronaviruses transmit COVID-19, a rapidly spreading disease. A Coronavirus\ninfection (COVID-19) was first discovered in December 2019 in Wuhan, China, and\nspread rapidly throughout the planet in exactly some months. because of this,\nthe virus can cause severe symptoms and even death, especially within the\nelderly and in people with medical conditions. The virus causes acute\nrespiratory infections in humans. the primary case was diagnosed in China in\n2019 and the pandemic started in 2020. Since the quantity of cases of COVID-19\nis increasing daily, there are only a limited number of test kits available in\nhospitals. So, to stop COVID-19 from spreading among people, an automatic\ndiagnosis system must be implemented. during this study, three pre-trained\nneural networks supported convolutional neural networks (VGG16, VGG19,\nResNet50) are proposed for detecting Coronavirus pneumonia infected patients\nthrough X-rays and computerized tomography (CT). By using cross-validation,\nwe've got implemented binary classifications with two classes (COVID-19, Normal\n(healthy)). Taking into consideration the results obtained, the pre-trained\nResNet50 model provides the simplest classification performance (97.77%\naccuracy, 100% sensitivity, 93.33% specificity, 98.00% F1-score) among the\nopposite three used models over 6259 images.", "AI": {"title_translation": "使用迁移学习的COVID-19诊断分析", "tldr": "鉴于COVID-19的快速传播和检测套件的有限性，本研究提出了一种基于迁移学习的自动化诊断系统。研究比较了VGG16、VGG19和ResNet50三种预训练卷积神经网络在X射线和CT图像上对COVID-19的二分类诊断性能，其中ResNet50模型表现最佳，准确率达97.77%。", "motivation": "由于COVID-19的病例数量日益增加且医院检测套件有限，为了阻止病毒传播，需要实现一个自动诊断系统。", "method": "本研究提出了三种基于卷积神经网络的预训练模型（VGG16、VGG19、ResNet50）来检测通过X射线和计算机断层扫描（CT）图像的冠状病毒肺炎感染患者。通过交叉验证，实现了二分类（COVID-19，正常（健康））。", "result": "在6259张图像上，预训练的ResNet50模型在所有使用的模型中提供了最佳的分类性能，达到了97.77%的准确率、100%的敏感性、93.33%的特异性和98.00%的F1分数。", "conclusion": "ResNet50模型在基于X射线和CT图像的COVID-19自动诊断方面表现出卓越的性能，为快速检测提供了有效的解决方案。", "translation": "冠状病毒传播COVID-19，这是一种迅速蔓延的疾病。冠状病毒感染（COVID-19）于2019年12月在中国武汉首次发现，并在短短几个月内迅速蔓延到全球。因此，这种病毒可能导致严重的症状甚至死亡，尤其是在老年人和有基础疾病的人群中。该病毒导致人类急性呼吸道感染。首例病例于2019年在中国诊断，大流行始于2020年。由于COVID-19病例数量每日增加，医院可用的检测试剂盒数量有限。因此，为了阻止COVID-19在人群中传播，必须实施一个自动诊断系统。在本研究中，提出了三种基于卷积神经网络的预训练神经网络（VGG16、VGG19、ResNet50），用于通过X射线和计算机断层扫描（CT）检测冠状病毒肺炎感染患者。通过使用交叉验证，我们实现了两类（COVID-19，正常（健康））的二分类。考虑到所获得的结果，在6259张图像中，预训练的ResNet50模型在其他三个使用的模型中提供了最佳的分类性能（97.77%的准确率、100%的敏感性、93.33%的特异性、98.00%的F1分数）。", "summary": "本研究旨在开发一种自动化的COVID-19诊断系统，以应对有限的检测资源和快速传播的疫情。研究利用迁移学习，比较了VGG16、VGG19和ResNet50三种预训练卷积神经网络模型在X射线和CT图像上对COVID-19感染的检测能力。实验结果表明，ResNet50模型表现最为出色，在二分类任务中达到了97.77%的准确率、100%的敏感性、93.33%的特异性和98.00%的F1分数，证实了其在快速准确诊断COVID-19方面的潜力。", "keywords": "COVID-19, 迁移学习, 卷积神经网络, 诊断, 医疗图像", "comments": "该论文解决了大流行期间对快速、自动化诊断的迫切需求。采用迁移学习结合预训练的CNN模型是医学图像分析领域的有效策略，尤其是在数据量可能有限的情况下。ResNet50模型的高敏感性（100%）对于疾病诊断至关重要，因为它能有效减少漏诊。"}}
{"id": "2503.12698", "pdf": "https://arxiv.org/pdf/2503.12698", "abs": "https://arxiv.org/abs/2503.12698", "authors": ["Dazhou Guo", "Zhanghexuan Ji", "Yanzhou Su", "Dandan Zheng", "Heng Guo", "Puyang Wang", "Ke Yan", "Yirui Wang", "Qinji Yu", "Zi Li", "Minfeng Xu", "Jianfeng Zhang", "Haoshen Li", "Jia Ge", "Tsung-Ying Ho", "Bing-Shen Huang", "Tashan Ai", "Kuaile Zhao", "Na Shen", "Qifeng Wang", "Yun Bian", "Tingyu Wu", "Peng Du", "Hua Zhang", "Feng-Ming Kong", "Alan L. Yuille", "Cher Heng Tan", "Chunyan Miao", "Perry J. Pickhardt", "Senxiang Yan", "Ronald M. Summers", "Le Lu", "Dakai Jin", "Xianghua Ye"], "title": "A Continual Learning-driven Model for Accurate and Generalizable Segmentation of Clinically Comprehensive and Fine-grained Whole-body Anatomies in CT", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Precision medicine in the quantitative management of chronic diseases and\noncology would be greatly improved if the Computed Tomography (CT) scan of any\npatient could be segmented, parsed and analyzed in a precise and detailed way.\nHowever, there is no such fully annotated CT dataset with all anatomies\ndelineated for training because of the exceptionally high manual cost, the need\nfor specialized clinical expertise, and the time required to finish the task.\nTo this end, we proposed a novel continual learning-driven CT model that can\nsegment complete anatomies presented using dozens of previously partially\nlabeled datasets, dynamically expanding its capacity to segment new ones\nwithout compromising previously learned organ knowledge. Existing multi-dataset\napproaches are not able to dynamically segment new anatomies without\ncatastrophic forgetting and would encounter optimization difficulty or\ninfeasibility when segmenting hundreds of anatomies across the whole range of\nbody regions. Our single unified CT segmentation model, CL-Net, can highly\naccurately segment a clinically comprehensive set of 235 fine-grained\nwhole-body anatomies. Composed of a universal encoder, multiple optimized and\npruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and\n16 private high-quality partially labeled CT datasets of various vendors,\ndifferent contrast phases, and pathologies. Extensive evaluation demonstrates\nthat CL-Net consistently outperforms the upper limit of an ensemble of 36\nspecialist nnUNets trained per dataset with the complexity of 5% model size and\nsignificantly surpasses the segmentation accuracy of recent leading Segment\nAnything-style medical image foundation models by large margins. Our continual\nlearning-driven CL-Net model would lay a solid foundation to facilitate many\ndownstream tasks of oncology and chronic diseases using the most widely adopted\nCT imaging.", "AI": {"title_translation": "一种基于持续学习的CT影像全身解剖结构精确且泛化分割模型，用于临床综合和精细化分析", "tldr": "CL-Net是一个基于持续学习的CT分割模型，能够利用多个部分标注数据集，动态扩展分割能力，实现对235种全身解剖结构的高精度分割，并显著优于现有方法。", "motivation": "现有CT数据集缺乏对所有解剖结构的完整标注，主要由于手动标注成本高、需要专业临床知识且耗时。这限制了精确细致地分割、解析和分析CT扫描，从而阻碍了慢性病和肿瘤学领域精准医学的发展。", "method": "本文提出了一种名为CL-Net的持续学习驱动的CT分割模型。该模型由一个通用编码器和多个优化剪枝的解码器组成。它利用来自20个公共和16个私人高质量部分标注CT数据集（包含13,952次CT扫描）进行开发，能够动态扩展其分割新解剖结构的能力，同时不忘却先前学习到的器官知识，解决了现有方法在动态分割新解剖结构时存在的灾难性遗忘和优化困难问题。", "result": "CL-Net模型能够高精度地分割235种临床综合的精细化全身解剖结构。在广泛评估中，CL-Net始终优于36个nnUNet专家模型集合的性能上限（仅用其5%的模型大小），并显著超越了近期领先的Segment Anything风格医学图像基础模型的分割精度。", "conclusion": "所提出的持续学习驱动的CL-Net模型为利用最广泛采用的CT成像技术，促进肿瘤学和慢性病的许多下游任务奠定了坚实的基础。", "translation": "在慢性病和肿瘤学的定量管理中，如果能以精确和详细的方式分割、解析和分析任何患者的计算机断层扫描（CT），精准医学将得到极大改善。然而，由于极高的手动成本、对专业临床知识的需求以及完成任务所需的时间，目前没有一个完全标注了所有解剖结构的CT数据集可用于训练。为此，我们提出了一种新颖的持续学习驱动的CT模型，该模型能够使用数十个先前部分标注的数据集来分割呈现的完整解剖结构，动态扩展其分割新解剖结构的能力，同时不损害先前学习到的器官知识。现有的多数据集方法无法在没有灾难性遗忘的情况下动态分割新的解剖结构，并且在分割全身数百个解剖结构时会遇到优化困难或不可行的问题。我们单一的统一CT分割模型CL-Net可以高精度地分割临床上全面的235个精细全身解剖结构。CL-Net由一个通用编码器和多个优化剪枝的解码器组成，使用来自20个公共和16个私人高质量部分标注CT数据集的13,952次CT扫描进行开发，这些数据集来自不同的供应商、不同的对比剂期和病理。广泛的评估表明，CL-Net始终优于36个nnUNet专家模型集合的性能上限，其模型复杂度仅为后者的5%，并且显著超越了近期领先的Segment Anything风格医学图像基础模型的分割精度。我们持续学习驱动的CL-Net模型将为利用最广泛采用的CT成像技术，促进肿瘤学和慢性病的许多下游任务奠定坚实的基础。", "summary": "本研究提出了一种名为CL-Net的持续学习驱动CT分割模型，旨在解决现有CT数据集标注不完整以及多数据集方法存在的灾难性遗忘问题。CL-Net能够利用大量部分标注的CT数据集，动态学习并高精度分割235种全身解剖结构，同时保持对已学习知识的记忆。实验结果表明，CL-Net在模型大小仅为5%的情况下，性能优于多个专业模型的集成，并显著超越了现有医学图像基础模型，为基于CT的肿瘤和慢性病管理提供了坚实基础。", "keywords": "持续学习, CT分割, 全身解剖, 灾难性遗忘, CL-Net", "comments": "该论文提出了一种创新的持续学习方法（CL-Net），有效解决了医学图像分割领域长期存在的标注成本高昂和“灾难性遗忘”问题。通过利用大量部分标注数据集，模型能够动态扩展其识别新解剖结构的能力，同时保持对已学知识的记忆。其在分割精度和模型效率上的显著提升，尤其是在与现有SOTA模型和集成模型的对比中，展现了其巨大的应用潜力，有望推动CT影像在精准医疗中的普及和应用。"}}
{"id": "2503.12768", "pdf": "https://arxiv.org/pdf/2503.12768", "abs": "https://arxiv.org/abs/2503.12768", "authors": ["Tatsuro Sakai", "Kanji Tanaka", "Jonathan Tay Yu Liang", "Muhammad Adil Luqman", "Daiki Iwata"], "title": "Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 4 figures, technical report", "summary": "In robot vision, thermal cameras have significant potential for recognizing\nhumans even in complete darkness. However, their application to multi-person\ntracking (MPT) has lagged due to data scarcity and difficulties in individual\nidentification. In this study, we propose a cooperative MPT system that\nutilizes co-located RGB and thermal cameras, using pseudo-annotations (bounding\nboxes + person IDs) to train RGB and T trackers. Evaluation experiments\ndemonstrate that the T tracker achieves remarkable performance in both bright\nand dark scenes. Furthermore, results suggest that a tracker-switching approach\nusing a binary brightness classifier is more suitable than a tracker-fusion\napproach for information integration. This study marks a crucial first step\ntoward ``Dynamic-Dark SLAM,\" enabling effective recognition, understanding, and\nreconstruction of individuals, occluding objects, and traversable areas in\ndynamic environments, both bright and dark.", "AI": {"title_translation": "动态-黑暗SLAM：RGB-热合作机器人视觉策略，用于明亮和低光场景下的多人跟踪", "tldr": "该研究提出了一种RGB-热合作多人跟踪系统，通过伪标注训练追踪器，并发现追踪器切换方法优于融合方法，为“动态-黑暗SLAM”迈出了关键一步。", "motivation": "在机器人视觉中，尽管热像仪在完全黑暗中识别人类方面潜力巨大，但由于数据稀缺和个体识别困难，其在多人跟踪（MPT）中的应用滞后。", "method": "本研究提出了一种利用共定位RGB和热像仪的合作MPT系统，并使用伪标注（边界框+人员ID）来训练RGB和T追踪器。研究还发现，使用二元亮度分类器的追踪器切换方法比追踪器融合方法更适合信息集成。", "result": "评估实验表明，T追踪器在明亮和黑暗场景中均表现出色。此外，结果表明，使用二元亮度分类器的追踪器切换方法比追踪器融合方法更适合信息集成。", "conclusion": "这项研究是迈向“动态-黑暗SLAM”的关键第一步，能够有效识别、理解和重建动态环境中（无论是明亮还是黑暗）的个体、遮挡物体和可通行区域。", "translation": "在机器人视觉中，热像仪在完全黑暗中识别人类方面具有巨大潜力。然而，由于数据稀缺和个体识别困难，其在多人跟踪（MPT）中的应用一直滞后。在本研究中，我们提出了一种利用共定位RGB和热像仪的合作MPT系统，使用伪标注（边界框+人员ID）来训练RGB和T追踪器。评估实验表明，T追踪器在明亮和黑暗场景中均表现出色。此外，结果表明，使用二元亮度分类器的追踪器切换方法比追踪器融合方法更适合信息集成。这项研究标志着迈向“动态-黑暗SLAM”的关键第一步，能够有效识别、理解和重建动态环境中（无论是明亮还是黑暗）的个体、遮挡物体和可通行区域。", "summary": "本文提出了一种结合RGB和热像仪的合作多人跟踪（MPT）系统，旨在解决热像仪在MPT中因数据稀缺和个体识别困难而应用受限的问题。通过伪标注训练RGB和热追踪器，实验证明热追踪器在不同光照条件下均表现出色。研究还发现，基于亮度分类器的追踪器切换策略优于融合策略。这项工作为未来在明亮和黑暗动态环境中实现“动态-黑暗SLAM”（即有效识别、理解和重建个体及环境）奠定了基础。", "keywords": "多人跟踪, RGB-热像仪, 机器人视觉, SLAM, 动态环境", "comments": "该论文的创新点在于提出了RGB-热合作视觉策略用于多人跟踪，并在光照条件变化的环境中验证了其有效性。特别是，它通过伪标注解决了热像仪数据稀缺的问题，并明确指出追踪器切换方法优于融合方法，为多模态传感器融合提供了实用的指导。这项研究为全天候、全光照条件下的机器人视觉和SLAM系统发展迈出了重要一步。"}}
{"id": "2503.12793", "pdf": "https://arxiv.org/pdf/2503.12793", "abs": "https://arxiv.org/abs/2503.12793", "authors": ["Yechao Zhang", "Yingzhe Xu", "Junyu Shi", "Leo Yu Zhang", "Shengshan Hu", "Minghui Li", "Yanjun Zhang"], "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted in AAAI 2025", "summary": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.", "AI": {"title_translation": "动态最大最小优化提升通用对抗扰动的泛化能力", "tldr": "现有UAP生成方法效率不足，本文提出DM-UAP，通过动态最大最小优化和课程学习，显著提升UAP的跨样本和跨模型泛化能力，在ImageNet上表现优于SOTA。", "motivation": "现有通用对抗扰动（UAP）生成方法在优化时使用静态模型参数快照，未能充分利用深度神经网络的潜力来生成更有效的UAP，导致其泛化能力不足。", "method": "提出了一种名为DM-UAP的动态最大最小优化策略，旨在通过动态模型-数据对来优化UAP。该方法采用迭代的max-min-min优化框架，结合课程UAP学习算法，以彻底探索模型参数和数据的组合空间。", "result": "DM-UAP显著增强了UAP的跨样本通用性和跨模型可迁移性。在仅使用500个样本生成UAP的情况下，DM-UAP的愚弄率平均提高了12.108%，优于现有最先进的方法。", "conclusion": "通过引入动态最大最小优化策略（DM-UAP），可以有效提升通用对抗扰动（UAPs）的泛化能力和可迁移性，使其在实际应用中更具威胁性。", "translation": "深度神经网络（DNNs）容易受到通用对抗扰动（UAPs）的影响。这些扰动经过精心设计，旨在在所有样本类别上普遍欺骗目标模型。与特定实例的对抗样本（AEs）不同，生成UAPs更为复杂，因为它们必须在广泛的数据样本和模型中实现泛化。我们的研究表明，现有使用静态模型参数快照优化UAPs的通用攻击方法，并未充分利用DNN生成更有效UAPs的潜力。我们建议不针对固定训练集上的静态DNN模型优化UAPs，而是使用动态模型-数据对来生成UAPs。特别是，我们引入了一种动态最大最小优化策略，旨在在各种最优模型-数据对中优化UAP。我们将这种方法称为DM-UAP。DM-UAP利用迭代的max-min-min优化框架，该框架细化模型-数据对，并结合课程UAP学习算法，以彻底检查模型参数和数据的组合空间。在ImageNet数据集上的综合实验表明，所提出的DM-UAP显著增强了UAPs的跨样本通用性和跨模型可迁移性。在仅使用500个样本生成UAP的情况下，DM-UAP的愚弄率平均提高了12.108%，优于现有最先进的方法。", "summary": "本文针对深度神经网络中通用对抗扰动（UAPs）泛化能力不足的问题，提出了一种名为DM-UAP的动态最大最小优化策略。该方法通过使用动态模型-数据对和迭代的max-min-min优化框架，结合课程UAP学习，有效提升了UAPs的跨样本通用性和跨模型可迁移性。实验证明，DM-UAP在ImageNet数据集上以更少的样本显著优于现有SOTA方法。", "keywords": "通用对抗扰动, 动态最大最小优化, 深度神经网络, 泛化能力, 对抗性攻击", "comments": "这项工作通过引入动态优化策略，解决了通用对抗扰动在泛化能力上的限制，突破了传统静态优化的瓶颈。其创新点在于将模型参数和数据优化纳入UAP生成过程，显著提升了攻击效果，对于理解和防御对抗性攻击具有重要意义。"}}
{"id": "2503.12806", "pdf": "https://arxiv.org/pdf/2503.12806", "abs": "https://arxiv.org/abs/2503.12806", "authors": ["Hadam Baek", "Hannie Shin", "Jiyoung Seo", "Chanwoo Kim", "Saerom Kim", "Hyeongbok Kim", "Sangpil Kim"], "title": "AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Accurately modeling sound propagation with complex real-world environments is\nessential for Novel View Acoustic Synthesis (NVAS). While previous studies have\nleveraged visual perception to estimate spatial acoustics, the combined use of\nsurface normal and structural details from 3D representations in acoustic\nmodeling has been underexplored. Given their direct impact on sound wave\nreflections and propagation, surface normals should be jointly modeled with\nstructural details to achieve accurate spatial acoustics. In this paper, we\npropose a surface-enhanced geometry-aware approach for NVAS to improve spatial\nacoustic modeling. To achieve this, we exploit geometric priors, such as image,\ndepth map, surface normals, and point clouds obtained using a 3D Gaussian\nSplatting (3DGS) based framework. We introduce a dual cross-attention-based\ntransformer integrating geometrical constraints into frequency query to\nunderstand the surroundings of the emitter. Additionally, we design a\nConvNeXt-based spectral features processing network called Spectral Refinement\nNetwork (SRN) to synthesize realistic binaural audio. Experimental results on\nthe RWAVS and SoundSpace datasets highlight the necessity of our approach, as\nit surpasses existing methods in novel view acoustic synthesis.", "AI": {"title_translation": "AV-Surf：表面增强几何感知的新视角声学合成", "tldr": "AV-Surf提出了一种表面增强的几何感知方法，利用3DGS获取的几何先验和新的网络结构，显著提高了新视角声学合成的准确性。", "motivation": "现有研究在新视角声学合成（NVAS）中未能充分利用3D表示中的表面法线和结构细节来建模声学传播，而这些细节对声波反射和传播有直接影响，导致空间声学建模不够准确。", "method": "本文提出了一种表面增强的几何感知新视角声学合成（NVAS）方法。该方法利用3D高斯溅射（3DGS）框架获取的几何先验，如图像、深度图、表面法线和点云。它引入了一个基于双交叉注意力的Transformer，将几何约束集成到频率查询中，以理解声源周围环境。此外，还设计了一个基于ConvNeXt的频谱特征处理网络，名为频谱细化网络（SRN），用于合成逼真的双耳音频。", "result": "在RWAVS和SoundSpace数据集上的实验结果表明，该方法在新视角声学合成方面超越了现有方法。", "conclusion": "本文提出的表面增强几何感知方法对于准确建模复杂真实环境中的声传播是必要的，并显著提升了新视角声学合成的性能。", "translation": "准确建模复杂真实环境中的声音传播对于新视角声学合成（NVAS）至关重要。虽然之前的研究已经利用视觉感知来估计空间声学，但声学建模中结合使用来自3D表示的表面法线和结构细节尚未得到充分探索。鉴于它们对声波反射和传播的直接影响，表面法线应与结构细节联合建模以实现准确的空间声学。在本文中，我们提出了一种用于NVAS的表面增强几何感知方法，以改善空间声学建模。为此，我们利用了通过基于3D高斯溅射（3DGS）的框架获得的几何先验，例如图像、深度图、表面法线和点云。我们引入了一个基于双交叉注意力的Transformer，将几何约束集成到频率查询中，以理解声源周围环境。此外，我们设计了一个基于ConvNeXt的频谱特征处理网络，称为频谱细化网络（SRN），用于合成逼真的双耳音频。在RWAVS和SoundSpace数据集上的实验结果突出了我们方法的必要性，因为它在新视角声学合成方面超越了现有方法。", "summary": "AV-Surf提出了一种创新的表面增强几何感知方法，用于新视角声学合成（NVAS），旨在解决现有方法未能充分利用3D表示中表面法线和结构细节的问题。该方法利用3D高斯溅射（3DGS）获得的几何先验（图像、深度、法线、点云），并引入了双交叉注意力Transformer来整合几何约束，同时设计了基于ConvNeXt的频谱细化网络（SRN）以合成逼真的双耳音频。实验证明，AV-Surf在NVAS任务上显著优于现有方法。", "keywords": "新视角声学合成, 几何感知, 表面法线, 3D高斯溅射, 空间声学", "comments": "本文的创新之处在于首次将表面法线和结构细节等精细几何信息引入到新视角声学合成中，并通过结合3DGS、双交叉注意力Transformer和ConvNeXt-based SRN等先进技术，有效提升了空间声学建模的准确性和双耳音频的真实感。该方法填补了现有研究的空白，为未来声学模拟和虚拟现实领域提供了新的思路和技术支持。"}}
{"id": "2503.12840", "pdf": "https://arxiv.org/pdf/2503.12840", "abs": "https://arxiv.org/abs/2503.12840", "authors": ["Chen Liu", "Liying Yang", "Peike Li", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Sound-guided object segmentation has drawn considerable attention for its\npotential to enhance multimodal perception. Previous methods primarily focus on\ndeveloping advanced architectures to facilitate effective audio-visual\ninteractions, without fully addressing the inherent challenges posed by audio\nnatures, \\emph{\\ie}, (1) feature confusion due to the overlapping nature of\naudio signals, and (2) audio-visual matching difficulty from the varied sounds\nproduced by the same object. To address these challenges, we propose Dynamic\nDerivation and Elimination (DDESeg): a novel audio-visual segmentation\nframework. Specifically, to mitigate feature confusion, DDESeg reconstructs the\nsemantic content of the mixed audio signal by enriching the distinct semantic\ninformation of each individual source, deriving representations that preserve\nthe unique characteristics of each sound. To reduce the matching difficulty, we\nintroduce a discriminative feature learning module, which enhances the semantic\ndistinctiveness of generated audio representations. Considering that not all\nderived audio representations directly correspond to visual features (e.g.,\noff-screen sounds), we propose a dynamic elimination module to filter out\nnon-matching elements. This module facilitates targeted interaction between\nsounding regions and relevant audio semantics. By scoring the interacted\nfeatures, we identify and filter out irrelevant audio information, ensuring\naccurate audio-visual alignment. Comprehensive experiments demonstrate that our\nframework achieves superior performance in AVS datasets.", "AI": {"title_translation": "动态推导与消除：增强音频语义的音视频分割", "tldr": "本文提出DDESeg框架，通过动态推导和消除来解决音频特征混淆和音视频匹配困难，实现更好的音视频分割。", "motivation": "现有声音引导对象分割方法主要关注架构，但未充分解决音频固有的挑战：1) 音频信号重叠导致的特征混淆；2) 同一物体产生不同声音导致的音视频匹配困难。", "method": "本文提出DDESeg框架。为解决特征混淆，DDESeg通过丰富每个独立声源的语义信息来重建混合音频信号的语义内容，推导出保留声音独特特征的表示。为减少匹配困难，引入判别性特征学习模块，增强生成音频表示的语义区分度。针对非匹配元素（如屏幕外声音），提出动态消除模块，过滤掉不匹配的元素，确保准确的音视频对齐。", "result": "综合实验表明，该框架在AVS数据集上取得了卓越的性能。", "conclusion": "DDESeg通过解决音频特征混淆和音视频匹配困难，显著提升了音视频分割的性能。", "translation": "声音引导的对象分割因其增强多模态感知的潜力而受到广泛关注。以往的方法主要侧重于开发先进的架构以促进有效的音视频交互，而未能充分解决音频固有性质带来的挑战，即：（1）由于音频信号的重叠性质导致的特征混淆，以及（2）同一物体产生不同声音导致的音视频匹配困难。为了应对这些挑战，我们提出了动态推导与消除（DDESeg）：一种新颖的音视频分割框架。具体来说，为了缓解特征混淆，DDESeg通过丰富每个独立声源的独特语义信息来重建混合音频信号的语义内容，推导出保留每种声音独特特征的表示。为了减少匹配困难，我们引入了一个判别性特征学习模块，该模块增强了生成音频表示的语义区分度。考虑到并非所有推导出的音频表示都直接对应视觉特征（例如，屏幕外声音），我们提出了一个动态消除模块来过滤掉不匹配的元素。该模块促进了发声区域和相关音频语义之间的有针对性交互。通过对交互特征进行评分，我们识别并过滤掉不相关的音频信息，确保准确的音视频对齐。综合实验表明，我们的框架在AVS数据集上取得了卓越的性能。", "summary": "本文提出一种名为DDESeg的新型音视频分割框架，旨在解决现有方法在处理音频固有挑战方面的不足，包括特征混淆和音视频匹配困难。DDESeg通过动态推导模块重建混合音频信号的语义内容并增强独立声源的区分度，同时引入动态消除模块过滤非匹配音频信息，以实现更准确的音视频对齐。实验证明，DDESeg在AVS数据集上表现优越。", "keywords": "音视频分割, 音频语义, 特征混淆, 动态推导, 动态消除", "comments": "这篇论文通过提出“动态推导与消除”的创新机制，直接解决了声音引导对象分割中长期存在的音频特征混淆和音视频匹配困难两大核心问题。其亮点在于不仅关注了如何从混淆音频中提取清晰语义，还考虑了如何有效过滤不相关的音频信息，从而显著提升了多模态感知的准确性和鲁棒性。这种从音频特性本身出发进行优化的思路，对于未来音视频理解领域的研究具有重要借鉴意义。"}}
{"id": "2503.12847", "pdf": "https://arxiv.org/pdf/2503.12847", "abs": "https://arxiv.org/abs/2503.12847", "authors": ["Chen Liu", "Peike Li", "Liying Yang", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation.", "AI": {"title_translation": "通过音频引导的视觉收敛对齐实现鲁棒的视听分割", "tldr": "提出一种新的视听分割框架，通过音频引导对齐和不确定性估计，解决了模糊对应和声音状态频繁变化导致的过分割或欠分割问题，提高了分割精度。", "motivation": "现有视听分割方法侧重空间或时间多模态建模，但忽视了来自模糊视听对应（例如，附近视觉相似但声音不同的物体）和物体发声状态频繁变化的挑战，导致音视频线索关联不可靠，出现过分割或欠分割。", "method": "提出一个新框架，包含两个主要组件：音频引导模态对齐（AMA）模块和不确定性估计（UE）模块。AMA通过多组内的视听交互和基于对音频线索响应性的紧凑表示来引导模型关注音频相关区域，并利用对比学习区分发声和无声区域。UE模块整合时空信息，识别由声音状态频繁变化引起的高不确定性区域，通过降低这些区域的置信度来减少预测误差。", "result": "实验结果表明，该方法比现有最先进方法取得了更高的精度，特别是在传统方法难以保持可靠分割的挑战性场景中表现更优。", "conclusion": "该提出的框架通过有效解决模糊的视听对应和频繁的声音状态变化问题，显著提升了视听分割的鲁棒性和准确性。", "translation": "**标题：** 通过音频引导的视觉收敛对齐实现鲁棒的视听分割\n\n**摘要：** 依据视听线索准确地定位可听物体是视听分割的核心目标。大多数现有方法强调空间或时间多模态建模，但忽略了来自模糊视听对应（例如，附近视觉相似但声音不同的物体）和物体发声状态频繁变化的挑战。因此，它们可能难以可靠地关联音频和视觉线索，导致过分割或欠分割。为了解决这些局限性，我们提出了一种具有两个主要组件的新颖框架：音频引导模态对齐（AMA）模块和不确定性估计（UE）模块。AMA没有通过全局注意力机制不加区分地关联视听线索，而是在多个组内执行视听交互，并根据它们对音频线索的响应性将组特征整合为紧凑表示，有效地将模型的注意力引导至与音频相关的区域。AMA利用对比学习，通过将具有强音频响应的特征视为正样本，将较弱响应的特征视为负样本，进一步区分发声区域和无声区域。此外，UE整合了空间和时间信息，以识别由声音状态频繁变化引起的高不确定性区域，通过降低这些区域的置信度来减少预测误差。实验结果表明，与现有最先进方法相比，我们的方法取得了更高的精度，特别是在传统方法难以保持可靠分割的挑战性场景中表现出色。", "summary": "本文针对视听分割中模糊对应和声音状态频繁变化导致的挑战，提出了一种名为“音频引导的视觉收敛对齐”的鲁棒框架。该框架包含音频引导模态对齐（AMA）模块和不确定性估计（UE）模块。AMA通过分组交互和对比学习，有效引导模型关注音频相关区域并区分发声与无声区域；UE则通过识别并降低高不确定性区域的置信度来减少错误。实验证明，该方法在复杂场景下显著提升了视听分割的准确性。", "keywords": "视听分割, 音频引导, 模态对齐, 不确定性估计, 对比学习", "comments": "该论文创新性地解决了视听分割中“视觉相似但声音不同”和“声音状态频繁变化”这两个核心挑战。通过引入音频引导模态对齐（AMA）和不确定性估计（UE）模块，它不仅提高了模型对音频相关区域的注意力，还通过显式处理不确定性来增强鲁棒性，是视听分割领域的重要进展。"}}
{"id": "2503.12926", "pdf": "https://arxiv.org/pdf/2503.12926", "abs": "https://arxiv.org/abs/2503.12926", "authors": ["Cheng Yuan", "Zhening Liu", "Jiashu Lv", "Jiawei Shao", "Yufei Jiang", "Jun Zhang", "Xuelong Li"], "title": "Task-Oriented Feature Compression for Multimodal Understanding via Device-Edge Co-Inference", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "With the rapid development of large multimodal models (LMMs), multimodal\nunderstanding applications are emerging. As most LMM inference requests\noriginate from edge devices with limited computational capabilities, the\npredominant inference pipeline involves directly forwarding the input data to\nan edge server which handles all computations. However, this approach\nintroduces high transmission latency due to limited uplink bandwidth of edge\ndevices and significant computation latency caused by the prohibitive number of\nvisual tokens, thus hindering delay-sensitive tasks and degrading user\nexperience. To address this challenge, we propose a task-oriented feature\ncompression (TOFC) method for multimodal understanding in a device-edge\nco-inference framework, where visual features are merged by clustering and\nencoded by a learnable and selective entropy model before feature projection.\nSpecifically, we employ density peaks clustering based on K nearest neighbors\nto reduce the number of visual features, thereby minimizing both data\ntransmission and computational complexity. Subsequently, a learnable entropy\nmodel with hyperprior is utilized to encode and decode merged features, further\nreducing transmission overhead. To enhance compression efficiency, multiple\nentropy models are adaptively selected based on the characteristics of the\nvisual features, enabling a more accurate estimation of the probability\ndistribution. Comprehensive experiments on seven visual question answering\nbenchmarks validate the effectiveness of the proposed TOFC method. Results show\nthat TOFC achieves up to 60% reduction in data transmission overhead and 50%\nreduction in system latency while maintaining identical task performance,\ncompared with traditional image compression methods.", "AI": {"title_translation": "面向任务的多模态理解特征压缩：基于设备-边缘协同推理", "tldr": "该论文提出了一种面向任务的特征压缩（TOFC）方法，用于设备-边缘协同推理中的多模态理解，显著降低了数据传输开销和系统延迟，同时保持了任务性能。", "motivation": "随着大型多模态模型（LMMs）的快速发展，多模态理解应用不断涌现。然而，由于边缘设备计算能力有限以及上行带宽受限，将输入数据直接转发到边缘服务器进行计算会导致高传输延迟和计算延迟，从而阻碍了对延迟敏感的任务并降低了用户体验。", "method": "本文提出了一种面向任务的特征压缩（TOFC）方法，用于设备-边缘协同推理框架中的多模态理解。该方法在特征投影之前，通过聚类合并视觉特征，并利用可学习和选择性的熵模型进行编码。具体而言，采用基于K近邻的密度峰值聚类来减少视觉特征的数量，从而最小化数据传输和计算复杂性。随后，使用带有超先验的可学习熵模型对合并后的特征进行编码和解码，进一步减少传输开销。为了提高压缩效率，根据视觉特征的特性自适应地选择多个熵模型，以实现更准确的概率分布估计。", "result": "在七个视觉问答基准上的综合实验验证了所提出的TOFC方法的有效性。结果表明，与传统图像压缩方法相比，TOFC在保持相同任务性能的同时，数据传输开销减少了高达60%，系统延迟减少了50%。", "conclusion": "所提出的面向任务的特征压缩（TOFC）方法通过设备-边缘协同推理，有效解决了大型多模态模型在边缘设备上部署时面临的传输和计算延迟问题，显著提高了效率而没有牺牲性能。", "translation": "随着大型多模态模型（LMMs）的快速发展，多模态理解应用不断涌现。由于大多数LMM推理请求源自计算能力有限的边缘设备，主流的推理流程涉及将输入数据直接转发到处理所有计算的边缘服务器。然而，这种方法由于边缘设备有限的上行带宽导致高传输延迟，以及视觉token数量庞大造成的显著计算延迟，从而阻碍了对延迟敏感的任务并降低了用户体验。为了应对这一挑战，我们提出了一种面向任务的特征压缩（TOFC）方法，用于设备-边缘协同推理框架中的多模态理解，其中视觉特征在特征投影之前通过聚类合并并由可学习和选择性的熵模型进行编码。具体而言，我们采用基于K近邻的密度峰值聚类来减少视觉特征的数量，从而最小化数据传输和计算复杂性。随后，使用带有超先验的可学习熵模型对合并后的特征进行编码和解码，进一步减少传输开销。为了提高压缩效率，根据视觉特征的特性自适应地选择多个熵模型，以实现更准确的概率分布估计。在七个视觉问答基准上的综合实验验证了所提出的TOFC方法的有效性。结果表明，与传统图像压缩方法相比，TOFC在保持相同任务性能的同时，数据传输开销减少了高达60%，系统延迟减少了50%。", "summary": "本文提出了一种名为面向任务的特征压缩（TOFC）的新方法，旨在解决大型多模态模型在边缘设备上进行多模态理解时，由于传输带宽和计算资源限制导致的高延迟问题。TOFC通过在设备-边缘协同推理框架中，对视觉特征进行密度峰值聚类以减少数量，并利用可学习的自适应熵模型进行编码，显著降低了数据传输开销和系统延迟。实验证明，该方法在保持相同任务性能的前提下，可将数据传输开销减少高达60%，系统延迟减少50%。", "keywords": "特征压缩, 多模态理解, 设备-边缘协同推理, 密度峰值聚类, 熵模型", "comments": "该论文的创新点在于提出了面向任务的特征压缩（TOFC）方法，结合了密度峰值聚类和可学习的自适应熵模型，以优化设备-边缘协同推理中的多模态理解。其重要性在于有效解决了边缘设备在处理大型多模态模型时面临的传输和计算瓶颈，对于推动延迟敏感型多模态应用的普及具有重要意义。"}}
{"id": "2503.12937", "pdf": "https://arxiv.org/pdf/2503.12937", "abs": "https://arxiv.org/abs/2503.12937", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Huanjin Yao", "Shunyu Liu", "Xikun Zhang", "Shijian Lu", "Dacheng Tao"], "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.", "AI": {"title_translation": "R1-VL：通过分步组相对策略优化学习多模态大型语言模型的推理能力", "tldr": "提出一种新的强化学习框架StepGRPO，通过分步奖励提升多模态大模型的推理能力，优于现有方法。", "motivation": "现有方法通过有监督微调模仿推理路径，但缺乏对错误路径的理解。本文旨在提升多模态大模型的推理能力，使其超越被动模仿。", "method": "提出Step-wise Group Relative Policy Optimization (StepGRPO)，一个在线强化学习框架，通过简单、有效且密集的步进式奖励使模型自我提升。引入Step-wise Reasoning Accuracy Reward (StepRAR)和Step-wise Reasoning Validity Reward (StepRVR)两种基于规则的推理奖励，分别奖励包含必要中间步骤和遵循逻辑一致过程的推理路径。基于StepGRPO提出R1-VL系列模型。", "result": "在8个基准测试上的大量实验表明该方法的优越性。", "conclusion": "通过提出的StepGRPO框架和R1-VL模型，可以有效提升多模态大模型的步进式推理能力，并优于现有方法。", "translation": "最近的研究通常通过在高质量思维链推理数据上进行有监督微调来增强多模态大型语言模型（MLLMs）的推理能力，但这往往导致模型仅仅模仿成功的推理路径，而未能理解错误的推理路径。在这项工作中，我们旨在增强MLLMs的推理能力，使其超越被动模仿正向推理路径。为此，我们设计了Step-wise Group Relative Policy Optimization (StepGRPO)，一个新的在线强化学习框架，它通过简单、有效且密集的步进式奖励使MLLMs能够自我提升推理能力。具体来说，StepGRPO引入了两种新颖的基于规则的推理奖励：Step-wise Reasoning Accuracy Reward (StepRAR)和Step-wise Reasoning Validity Reward (StepRVR)。StepRAR通过软关键步骤匹配技术奖励包含必要中间推理步骤的推理路径，而StepRVR通过推理完整性和逻辑评估策略奖励遵循结构良好且逻辑一致的推理过程的推理路径。凭借提出的StepGRPO，我们引入了R1-VL，一系列在步进式推理方面具有出色能力的MLLMs。在8个基准测试上的大量实验证明了我们方法的优越性。", "summary": "本文提出一种新的在线强化学习框架StepGRPO，用于增强多模态大型语言模型的推理能力。与依赖有监督微调模仿成功路径的方法不同，StepGRPO通过引入StepRAR和StepRVR两种基于规则的步进式奖励，使模型能够自我提升推理能力，理解并避免错误的推理过程。基于此框架提出了R1-VL模型系列，并在多个基准测试中展示了其优越的步进式推理性能。", "keywords": "多模态大型语言模型, 推理, 强化学习, 分步奖励, 策略优化", "comments": "该工作创新性地将强化学习应用于多模态大模型的推理能力提升，特别是通过设计细粒度的步进式奖励，克服了传统有监督方法只模仿成功路径的局限性，为提高模型的鲁棒推理能力提供了新的思路。"}}
{"id": "2503.12990", "pdf": "https://arxiv.org/pdf/2503.12990", "abs": "https://arxiv.org/abs/2503.12990", "authors": ["Roba Al Majzoub", "Hashmat Malik", "Muzammal Naseer", "Zaigham Zaheer", "Tariq Mahmood", "Salman Khan", "Fahad Khan"], "title": "How Good is my Histopathology Vision-Language Foundation Model? A Holistic Benchmark", "categories": ["eess.IV", "cs.CV", "I.4.0; J.3"], "comment": null, "summary": "Recently, histopathology vision-language foundation models (VLMs) have gained\npopularity due to their enhanced performance and generalizability across\ndifferent downstream tasks. However, most existing histopathology benchmarks\nare either unimodal or limited in terms of diversity of clinical tasks, organs,\nand acquisition instruments, as well as their partial availability to the\npublic due to patient data privacy. As a consequence, there is a lack of\ncomprehensive evaluation of existing histopathology VLMs on a unified benchmark\nsetting that better reflects a wide range of clinical scenarios. To address\nthis gap, we introduce HistoVL, a fully open-source comprehensive benchmark\ncomprising images acquired using up to 11 various acquisition tools that are\npaired with specifically crafted captions by incorporating class names and\ndiverse pathology descriptions. Our Histo-VL includes 26 organs, 31 cancer\ntypes, and a wide variety of tissue obtained from 14 heterogeneous patient\ncohorts, totaling more than 5 million patches obtained from over 41K WSIs\nviewed under various magnification levels. We systematically evaluate existing\nhistopathology VLMs on Histo-VL to simulate diverse tasks performed by experts\nin real-world clinical scenarios. Our analysis reveals interesting findings,\nincluding large sensitivity of most existing histopathology VLMs to textual\nchanges with a drop in balanced accuracy of up to 25% in tasks such as\nMetastasis detection, low robustness to adversarial attacks, as well as\nimproper calibration of models evident through high ECE values and low model\nprediction confidence, all of which can affect their clinical implementation.", "AI": {"title_translation": "我的组织病理学视觉-语言基础模型表现如何？一个整体基准测试", "tldr": "引入HistoVL，一个全面、开源的组织病理学视觉-语言模型基准测试，包含多样化的数据和任务，并揭示现有模型在鲁棒性、校准和对文本变化的敏感性方面的不足。", "motivation": "现有组织病理学基准测试在多样性、公开性方面存在局限性，缺乏在统一设置下全面评估现有组织病理学视觉-语言基础模型的基准，无法充分反映广泛的临床场景。", "method": "引入了一个名为HistoVL的全面、完全开源的基准测试。HistoVL包含使用多达11种不同采集工具获取的图像，这些图像与精心制作的包含类别名称和多样化病理描述的标题配对。HistoVL包括26个器官、31种癌症类型和来自14个异质患者队列的各种组织，总计超过500万个图像块，这些图像块来自41K多张WSI，在不同放大级别下查看。系统地评估了现有组织病理学VLMs在HistoVL上的表现，以模拟现实世界临床场景中专家执行的各种任务。", "result": "现有组织病理学VLMs对文本变化高度敏感，在转移检测等任务中平衡准确率下降高达25%；对对抗性攻击的鲁棒性较低；模型校准不当，表现为高ECE值和低模型预测置信度。", "conclusion": "现有组织病理学视觉-语言基础模型在鲁棒性、对文本变化的敏感性和校准方面存在显著局限性，这些问题会影响其临床应用。需要进一步研究来解决这些问题，以提高模型在现实世界临床场景中的可靠性。", "translation": "最近，组织病理学视觉-语言基础模型（VLMs）因其在不同下游任务中增强的性能和泛化能力而受到欢迎。然而，大多数现有的组织病理学基准测试要么是单模态的，要么在临床任务、器官、采集仪器多样性方面存在局限性，并且由于患者数据隐私，其部分可用性有限。因此，缺乏在统一基准设置下对现有组织病理学VLMs进行全面评估的基准，该设置能更好地反映广泛的临床场景。为了弥补这一差距，我们引入了HistoVL，一个完全开源的综合基准测试，包含使用多达11种不同采集工具获取的图像，这些图像与通过结合类别名称和多样化病理描述而精心制作的标题配对。我们的Histo-VL包括26个器官、31种癌症类型以及来自14个异质患者队列的各种组织，总计超过500万个图像块，这些图像块来自41K多张WSI，在各种放大级别下查看。我们系统地评估了现有组织病理学VLMs在Histo-VL上的表现，以模拟现实世界临床场景中专家执行的各种任务。我们的分析揭示了有趣的发现，包括大多数现有组织病理学VLMs对文本变化的高度敏感性，在转移检测等任务中平衡准确率下降高达25%，对对抗性攻击的低鲁棒性，以及通过高ECE值和低模型预测置信度体现的模型校准不当，所有这些都会影响其临床实施。", "summary": "本文提出了HistoVL，一个全面、开源的组织病理学视觉-语言模型基准测试，旨在解决现有基准在数据多样性和任务覆盖上的不足。HistoVL包含来自多种采集工具、器官、癌症类型和患者队列的大量图像和配对标题。通过在HistoVL上评估现有VLMs，研究发现这些模型对文本变化敏感、鲁棒性差且校准不当，揭示了它们在实际临床应用中存在的挑战。", "keywords": "组织病理学, 视觉-语言模型, 基准测试, HistoVL, 模型评估", "comments": "HistoVL的提出为组织病理学视觉-语言模型提供了一个急需的、多样化且开源的评估平台。其全面的数据覆盖范围（包括多种器官、癌症类型、采集工具和患者队列）是其主要创新点。对现有模型进行的系统评估揭示了当前VLMs在临床应用中面临的关键挑战，特别是鲁棒性、校准和对文本变化的敏感性问题，为未来的模型开发指明了方向。"}}
{"id": "2503.13008", "pdf": "https://arxiv.org/pdf/2503.13008", "abs": "https://arxiv.org/abs/2503.13008", "authors": ["David E. Hernandez", "Jose Ramon Chang", "Torbjörn E. M. Nordling"], "title": "Knowledge Distillation: Enhancing Neural Network Compression with Integrated Gradients", "categories": ["cs.LG", "cs.CV", "68T05, 68T07", "I.2.6; I.4.2; I.4.9"], "comment": "15 pages, 3 figures, conference", "summary": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.", "AI": {"title_translation": "知识蒸馏：利用集成梯度增强神经网络压缩", "tldr": "通过利用集成梯度指导学生模型，增强了知识蒸馏在神经网络压缩中的应用，在资源受限设备上实现了更高的精度和更快的速度。", "motivation": "在资源受限设备上高效部署深度神经网络需要先进的压缩技术，这些技术必须在保持精度和互操作性的同时进行压缩。传统的知识蒸馏方法存在改进空间。", "method": "提出了一种结合知识蒸馏（KD）和归因方法集成梯度（IG）的机器学习框架。引入了一种新颖的数据增强策略，将从教师模型预计算的IG映射叠加到训练图像上，以指导紧凑的学生模型学习关键特征表示。", "result": "在CIFAR-10上的实验表明该方法有效：从MobileNet-V2教师模型压缩4.1倍的学生模型达到了92.5%的分类精度，超过了基线学生模型的91.4%和传统KD方法；推理延迟从140毫秒减少到13毫秒，速度提高了十倍。进行了超参数优化和全面的消融研究，揭示了KD和IG的协同效应。", "conclusion": "该方法通过IG进行特征级指导，提供了一种可扩展、可解释的压缩技术，使其区别于传统KD，为神经网络架构中可迁移知识的挖掘提供了一种数据驱动的解决方案。它非常适合对效率和透明度要求极高的边缘计算应用。", "translation": "在资源受限设备上高效部署深度神经网络需要先进的压缩技术，这些技术既要保持精度又要保持互操作性。本文提出了一种机器学习框架，通过结合归因方法集成梯度（IG）来增强知识蒸馏（KD），从而优化卷积神经网络的压缩。我们引入了一种新颖的数据增强策略，将从教师模型预计算的IG映射叠加到训练图像上，以指导紧凑的学生模型学习关键特征表示。这种方法利用了教师模型的决策洞察力，增强了学生模型以更少的参数复制复杂模式的能力。在CIFAR-10上的实验证明了我们方法的有效性：从MobileNet-V2教师模型压缩4.1倍的学生模型达到了92.5%的分类精度，超过了基线学生模型的91.4%和传统KD方法，同时将推理延迟从140毫秒减少到13毫秒——速度提高了十倍。我们对超参数进行了优化以实现高效学习。全面的消融研究剖析了KD和IG的贡献，揭示了它们在提升性能和模型可解释性方面的协同效应。我们的方法通过IG强调特征级指导，使其区别于传统KD，为神经网络架构中可迁移知识的挖掘提供了一种数据驱动的解决方案。这项工作通过提供一种可扩展、可解释的压缩技术，为机器学习做出了贡献，非常适合效率和透明度至关重要的边缘计算应用。", "summary": "本文提出了一种结合知识蒸馏（KD）和集成梯度（IG）的神经网络压缩增强方法。通过将从教师模型获得的IG映射作为数据增强叠加到训练图像上，该方法指导学生模型学习关键特征。在CIFAR-10上的实验表明，与基线和传统方法相比，该方法提高了精度并显著降低了推理时间，非常适用于资源受限设备。", "keywords": "知识蒸馏, 集成梯度, 神经网络压缩, 模型压缩, 边缘计算", "comments": "本文的创新之处在于将集成梯度（IG）整合到知识蒸馏（KD）中，以实现特征级指导，从而在模型性能和可解释性方面优于传统KD。这种方法对于在资源受限设备上部署模型尤其重要。"}}
{"id": "2503.13021", "pdf": "https://arxiv.org/pdf/2503.13021", "abs": "https://arxiv.org/abs/2503.13021", "authors": ["Omri Suissa", "Muhiim Ali", "Ariana Azarbal", "Hui Shen", "Shekhar Pradhan"], "title": "Dynamic Relation Inference via Verb Embeddings", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.", "AI": {"title_translation": "通过动词嵌入进行动态关系推理", "tldr": "CLIP在对象关系推理方面表现不佳。本文提出了DRIVE方法，利用动词嵌入、数据增强、硬负样本微调和新的损失函数来提高零样本关系推理准确性，性能优于CLIP和现有SOTA模型。", "motivation": "CLIP擅长图像-文本匹配，但在推断图像中对象之间的关系方面存在不足。以往仅使用语言监督在关系检测数据集上训练CLIP的方法效果有限。本文的动机是推进图像关系推理领域，并创建一个能够通过语言监督有效检测图像中对象之间关系的模型。", "method": "本文提出了通过动词嵌入进行动态关系推理（DRIVE）的方法。该方法包括增强COCO数据集，使用硬负样本（主语-关系-宾语三元组和对应图像）对CLIP进行微调，并引入一个新的损失函数来改进关系检测。", "result": "在多个基于CLIP的模型上进行评估，本文方法在冻结和微调设置下都显著提高了零样本关系推理的准确性。该方法显著优于CLIP和现有最先进的模型，并且对未见数据具有良好的泛化能力。", "conclusion": "本文的结论是，所提出的DRIVE方法有效解决了CLIP在关系推理方面的局限性，与现有方法相比，在零样本准确性和泛化能力方面取得了显著改进。", "translation": "CLIP由于在对比学习任务上进行训练，展示了卓越的图像-文本匹配能力。过去的研究表明，虽然CLIP在仅通过匹配文本与图像中的对象即可实现匹配时表现出色，但在匹配依赖于表示图像中对象之间的关系（即推断关系）时，CLIP会遇到困难。先前通过仅使用语言监督在关系检测数据集上训练CLIP来解决这一限制的尝试收效甚微。在本文中，我们提供了见解和实用方法，以推进图像关系推理领域。本文通过生成能够通过语言监督捕获关系的文本和图像嵌入，来解决创建有效检测图像中对象之间关系的模型的任务。为此，我们提出了通过动词嵌入进行动态关系推理（DRIVE），该方法增强了COCO数据集，使用硬负样本主语-关系-宾语三元组和对应图像对CLIP进行微调，并引入了一个新的损失函数来改进关系检测。在多个基于CLIP的模型上进行评估，我们的方法在冻结和微调设置下都显著提高了零样本关系推理的准确性，显著优于CLIP和最先进的模型，同时对未见数据具有良好的泛化能力。", "summary": "本文旨在解决CLIP在推断对象之间关系方面的不足。提出了通过动词嵌入进行动态关系推理（DRIVE）的方法，通过增强COCO数据集、使用硬负样本微调CLIP以及引入新的损失函数来实现。实验表明，DRIVE显著提高了零样本关系推理的准确性，优于CLIP和现有SOTA模型，并具有良好的泛化能力。", "keywords": "关系推理, CLIP, 动词嵌入, 语言监督, 硬负样本", "comments": "该论文提出了一种实用的方法（DRIVE），通过语言监督和特定的训练技术（硬负样本、新的损失函数）来改进关系推理，解决了CLIP的一个已知局限性。结果表明性能显著提升且泛化能力良好。"}}
{"id": "2503.13051", "pdf": "https://arxiv.org/pdf/2503.13051", "abs": "https://arxiv.org/abs/2503.13051", "authors": ["Kai Uwe Barthel", "Florian Barthel", "Peter Eisert"], "title": "Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical.", "AI": {"title_translation": "仅用 N 个参数学习置换：从 SoftSort 到自组织高斯", "tldr": "提出了一种仅使用 N 个参数学习置换的新方法，相比现有方法（如 Gumbel-Sinkhorn 和 SoftSort），显著提高了内存效率和可扩展性，特别适用于大规模问题和多维数据。", "motivation": "现有的排序和置换学习方法（如 Gumbel-Sinkhorn、低秩近似和 SoftSort）在处理大规模数据集和高维数据时存在参数过多、计算成本高或难以处理多维数据和复杂置换的问题。需要一种能够大幅降低存储成本的新方法。", "method": "提出了一种仅使用 N 个参数学习置换的新方法。该方法基于 SoftSort，但通过迭代地、可分离地混洗 N 个元素的索引来对其进行扩展。", "result": "该方法显著提高了排序质量，尤其对于多维数据和复杂优化标准。它优于纯 SoftSort，并提供了比现有方法更好的内存效率和可扩展性，同时保持高质量的置换学习。", "conclusion": "该方法显著降低的内存需求使其特别适用于大规模优化任务，例如“自组织高斯”，其中高效且可扩展的置换学习至关重要。", "translation": "排序和置换学习是优化和机器学习中的关键概念，尤其是在将高维数据组织成有意义的空间布局时。Gumbel-Sinkhorn 方法虽然有效，但需要 N*N 个参数来确定完整的置换矩阵，这对于大型数据集来说计算成本高昂。低秩矩阵分解近似将内存需求降低到 2MN (其中 M << N)，但它们仍然难以处理非常大的问题。SoftSort 通过提供 argsort 算子的连续松弛，允许可微分的一维排序，但它在多维数据和复杂置换方面面临挑战。在本文中，我们提出了一种仅使用 N 个参数学习置换的新方法，这极大地降低了存储成本。我们的方法建立在 SoftSort 的基础上，但通过迭代地、可分离地混洗待排序元素的 N 个索引来对其进行扩展。这种修改显著提高了排序质量，尤其对于多维数据和复杂优化标准，并且优于纯 SoftSort。与现有方法相比，我们的方法提供了改进的内存效率和可扩展性，同时保持高质量的置换学习。其显著降低的内存需求使其特别适用于大规模优化任务，例如“自组织高斯”，其中高效且可扩展的置换学习至关重要。", "summary": "本文提出了一种仅使用 N 个参数学习置换的新方法，从而显著降低了存储成本。该方法基于 SoftSort，通过迭代地、可分离地混洗 N 个元素的索引来扩展 SoftSort，从而显著提高了排序质量，尤其对于多维数据和复杂优化标准，并优于纯 SoftSort。该方法相比现有方法具有更高的内存效率和可扩展性，使其特别适用于大规模优化任务。", "keywords": "置换学习, SoftSort, N 参数, 内存效率, 可扩展性", "comments": "该论文的主要创新在于将置换学习所需的参数数量从 N*N 或 2MN 大幅减少到仅 N 个。这显著提高了内存效率和可扩展性，解决了现有方法在大规模数据集上的主要限制。通过迭代的、可分离的索引混洗对 SoftSort 进行扩展是其具体的技朧贡献。"}}
{"id": "2503.13057", "pdf": "https://arxiv.org/pdf/2503.13057", "abs": "https://arxiv.org/abs/2503.13057", "authors": ["Robin Zbinden", "Nina van Tiel", "Gencer Sumbul", "Chiara Vanalli", "Benjamin Kellenberger", "Devis Tuia"], "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.", "AI": {"title_translation": "MaskSDM与Shapley值：提升物种分布模型在灵活性、鲁棒性和可解释性方面的表现", "tldr": "MaskSDM是一种新的深度学习物种分布模型，通过掩码训练和Shapley值提高了变量选择的灵活性、对缺失数据的鲁棒性以及对变量贡献的可解释性，表现优于现有方法。", "motivation": "现有物种分布模型（SDMs）在变量选择方面缺乏灵活性、对缺失数据的鲁棒性以及对变量贡献的可解释性。", "method": "提出了MaskSDM，一种基于深度学习的SDM，采用掩码训练策略实现灵活的预测因子选择，并利用Shapley值进行精确的预测因子贡献评估。", "result": "在全局sPlotOpen数据集上对12,738种植物进行建模，结果表明MaskSDM优于基于插补的方法，并且接近在特定变量子集上训练的模型。", "conclusion": "MaskSDM提高了SDMs的适用性和普及性，为开发可应用于多种生态学应用的基础模型奠定了基础。", "translation": "物种分布模型（SDMs）通过根据环境条件预测物种分布，在生物多样性研究、保护规划和生态位建模中发挥着至关重要的作用。预测因子的选择至关重要，它强烈影响模型的准确性以及预测反映生态模式的程度。为了确保有意义的见解，必须仔细选择输入变量，使其与研究目标和目标物种的生态需求相匹配。然而，现有的SDMs，包括传统方法和基于深度学习的方法，通常缺乏变量选择的关键能力：（i）在推理时选择相关预测因子而无需重新训练的灵活性；（ii）在不影响准确性的情况下处理缺失预测因子值的鲁棒性；以及（iii）解释和准确量化每个预测因子贡献的可解释性。为了克服这些限制，我们引入了MaskSDM，一种新颖的基于深度学习的SDM，它通过采用掩码训练策略实现了灵活的预测因子选择。这种方法允许模型使用任意输入变量子集进行预测，同时对缺失数据保持鲁棒性。它还更清晰地理解了添加或删除给定预测因子如何影响模型性能和预测。此外，MaskSDM利用Shapley值进行精确的预测因子贡献评估，改进了传统的近似方法。我们在全局sPlotOpen数据集上评估了MaskSDM，对12,738种植物的分布进行了建模。我们的结果表明，MaskSDM优于基于插补的方法，并且接近在特定变量子集上训练的模型。这些发现强调了MaskSDM提高SDMs适用性和普及性的潜力，为开发可轻松应用于多种生态学应用的基础模型奠定了基础。", "summary": "本文提出了一种新的深度学习物种分布模型（SDM）MaskSDM，旨在解决现有SDMs在变量选择灵活性、对缺失数据的鲁棒性和可解释性方面的不足。MaskSDM采用掩码训练策略，允许模型使用任意变量子集进行预测，并利用Shapley值精确评估变量贡献。在对12,738种植物进行建模的实验中，MaskSDM表现优于插补方法，并接近在特定变量子集上训练的模型。研究结果表明，MaskSDM有望增强SDMs的实用性，并为构建生态学领域的基础模型铺平道路。", "keywords": "物种分布模型, MaskSDM, Shapley值, 变量选择, 鲁棒性, 可解释性", "comments": "MaskSDM的创新之处在于结合了掩码训练和Shapley值，显著提升了物种分布模型在实际应用中的灵活性、鲁鲁棒性和可解释性。这对于处理不完整或多样的环境数据至关重要，并为SDMs领域的基础模型研究提供了新的方向。"}}
{"id": "2503.13080", "pdf": "https://arxiv.org/pdf/2503.13080", "abs": "https://arxiv.org/abs/2503.13080", "authors": ["Hubert Szolc", "Mateusz Wasala", "Remigiusz Mietla", "Kacper Iwicki", "Tomasz Kryjak"], "title": "Vision-based automatic fruit counting with UAV", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Accepted for the 29th Conference on Automation - Innovations and\n  Future Perspectives Automation 2025, May 7 - 9, 2025, Warsaw, Poland", "summary": "The use of unmanned aerial vehicles (UAVs) for smart agriculture is becoming\nincreasingly popular. This is evidenced by recent scientific works, as well as\nthe various competitions organised on this topic. Therefore, in this work we\npresent a system for automatic fruit counting using UAVs. To detect them, our\nsolution uses a vision algorithm that processes streams from an RGB camera and\na depth sensor using classical image operations. Our system also allows the\nplanning and execution of flight trajectories, taking into account the\nminimisation of flight time and distance covered. We tested the proposed\nsolution in simulation and obtained an average score of 87.27/100 points from a\ntotal of 500 missions. We also submitted it to the UAV Competition organised as\npart of the ICUAS 2024 conference, where we achieved an average score of\n84.83/100 points, placing 6th in a field of 23 teams and advancing to the\nfinals.", "AI": {"title_translation": "基于视觉的无人机自动水果计数", "tldr": "本文提出了一种基于视觉算法和无人机（UAV）的自动水果计数系统，并在模拟和竞赛中取得了良好结果。", "motivation": "智能农业中无人机的使用日益普及，自动水果计数是一个重要的应用。本文旨在开发一个基于无人机的自动水果计数系统。", "method": "系统使用视觉算法处理来自RGB相机和深度传感器的图像流，并采用经典的图像操作。此外，系统还规划飞行轨迹以最小化飞行时间和距离。", "result": "在模拟中，平均得分87.27/100（共500次任务）。在ICUAS 2024无人机竞赛中，平均得分84.83/100，在23支队伍中排名第6，进入决赛。", "conclusion": "该系统在模拟和竞赛中表现良好，证明了其在基于无人机的自动水果计数方面的有效性。", "translation": "无人机（UAV）在智能农业中的使用正变得越来越流行。最近的科学研究以及围绕这一主题组织的各种竞赛都证明了这一点。因此，在这项工作中，我们提出了一种使用无人机的自动水果计数系统。为了检测水果，我们的解决方案使用一种视觉算法，该算法通过经典的图像操作处理来自RGB相机和深度传感器的图像流。我们的系统还允许规划和执行飞行轨迹，同时考虑到飞行时间和覆盖距离的最小化。我们在模拟中测试了所提出的解决方案，在总共500次任务中获得了87.27/100的平均得分。我们还将其提交给ICUAS 2024会议期间组织的无人机竞赛，在那里我们获得了84.83/100的平均得分，在23支队伍中排名第6，并进入了决赛。", "summary": "本文介绍了一种基于无人机的自动水果计数系统。该系统利用视觉算法结合RGB相机和深度传感器数据进行水果检测，并优化飞行路径。系统在模拟和ICUAS 2024竞赛中进行了测试，取得了令人满意的结果，并在竞赛中进入决赛。", "keywords": "无人机, 水果计数, 视觉处理, 智能农业, 图像操作", "comments": "该论文提出了一种实用的基于无人机的智能农业应用，结合了视觉处理和路径规划。在实际竞赛中取得的成绩证明了系统的有效性和潜力。"}}
{"id": "2503.13082", "pdf": "https://arxiv.org/pdf/2503.13082", "abs": "https://arxiv.org/abs/2503.13082", "authors": ["Runyu Jiao", "Alice Fasoli", "Francesco Giuliari", "Matteo Bortolon", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Free-form language-based robotic reasoning and grasping", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://tev-fbk.github.io/FreeGrasp/", "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.", "AI": {"title_translation": "自由形式语言的机器人推理与抓取", "tldr": "研究了基于自由形式语言的机器人抓取任务，提出了FreeGrasp方法，利用VLM（如GPT-4o）进行零样本推理和空间理解，通过关键点标注辅助VLM，并创建了新数据集，实现了最先进的性能。", "motivation": "基于人类自由形式语言指令从杂乱环境中抓取物体是机器人面临的挑战，需要理解语言和空间关系。虽然VLM具有推理能力，但在零样本设置下用于此任务的有效性和局限性尚不清楚。", "method": "提出FreeGrasp方法，利用预训练的VLM（如GPT-4o）的世界知识进行指令和空间推理。该方法检测物体关键点并在图像上标注，以促进GPT-4o的零样本空间推理，从而判断目标物体是否可直接抓取或需要先移除其他物体。引入合成数据集FreeGraspData进行训练和评估。", "result": "在FreeGraspData数据集和真实世界验证中，该方法在抓取推理和执行方面表现出最先进的性能。", "conclusion": "FreeGrasp方法成功地利用VLM实现了基于自由形式语言的机器人抓取任务，并在抓取推理和执行上达到了最先进水平。", "translation": "基于人类指令从杂乱箱中执行机器人抓取是一项具有挑战性的任务，因为它需要理解自由形式语言的细微之处以及物体之间的空间关系。在网络规模数据上训练的视觉语言模型（VLM），例如 GPT-4o，在文本和图像方面展示了卓越的推理能力。但是，它们真的可以在零样本设置下用于此任务吗？它们的局限性是什么？在本文中，我们通过基于自由形式语言的机器人抓取任务探索这些研究问题，并提出一种新方法 FreeGrasp，利用预训练 VLM 的世界知识来推理人类指令和物体空间排列。我们的方法将所有物体检测为关键点，并使用这些关键点在图像上标注标记，旨在促进 GPT-4o 的零样本空间推理。这使得我们的方法能够确定请求的物体是可直接抓取还是必须先抓取并移除其他物体。由于没有专门为此任务设计的数据集，我们通过扩展 MetaGraspNetV2 数据集并添加人工标注的指令和抓取序列地面真实数据，引入了一个合成数据集 FreeGraspData。我们在 FreeGraspData 和配备抓爪的机械臂的真实世界验证中进行了广泛分析，证明了在抓取推理和执行方面的最先进性能。项目网站：https://tev-fbk.github.io/FreeGrasp/。", "summary": "本文研究了基于人类自由形式语言指令的机器人从杂乱环境中抓取物体的挑战。提出了FreeGrasp方法，利用预训练的视觉语言模型（VLM）如GPT-4o进行零样本推理和空间理解。该方法通过检测物体关键点并标注图像来辅助VLM的空间推理，从而判断抓取顺序。为解决数据集缺乏问题，构建了合成数据集FreeGraspData。实验结果表明，FreeGrasp在抓取推理和执行上达到了最先进水平。", "keywords": "机器人抓取, 自由形式语言, 视觉语言模型, 零样本学习, 空间推理", "comments": "这篇论文探讨了利用强大的预训练VLM（如GPT-4o）解决复杂的自由形式语言机器人抓取任务的可行性和局限性。通过将物体检测转化为关键点标注以辅助VLM进行零样本空间推理是一种新颖的方法。构建专门的数据集也对该领域的研究有贡献。这项工作展示了VLM在具身智能任务中的潜力。"}}
{"id": "2503.13090", "pdf": "https://arxiv.org/pdf/2503.13090", "abs": "https://arxiv.org/abs/2503.13090", "authors": ["Václav Truhlařík", "Tomáš Pivoňka", "Michal Kasarda", "Libor Přeučil"], "title": "Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 5 figures", "summary": "Uniform and variable environments still remain a challenge for stable visual\nlocalization and mapping in mobile robot navigation. One of the possible\napproaches suitable for such environments is appearance-based teach-and-repeat\nnavigation, relying on simplified localization and reactive robot motion\ncontrol - all without a need for standard mapping. This work brings an\ninnovative solution to such a system based on visual place recognition\ntechniques. Here, the major contributions stand in the employment of a new\nvisual place recognition technique, a novel horizontal shift computation\napproach, and a multi-platform system design for applications across various\ntypes of mobile robots. Secondly, a new public dataset for experimental testing\nof appearance-based navigation methods is introduced. Moreover, the work also\nprovides real-world experimental testing and performance comparison of the\nintroduced navigation system against other state-of-the-art methods. The\nresults confirm that the new system outperforms existing methods in several\ntesting scenarios, is capable of operation indoors and outdoors, and exhibits\nrobustness to day and night scene variations.", "AI": {"title_translation": "基于深度学习局部特征的视觉地点识别实现多平台示教复现导航", "tldr": "提出一种基于深度学习局部特征视觉地点识别的多平台示教复现导航系统，在多种环境下表现优于现有方法且鲁棒。", "motivation": "在统一和变化的环境中，稳定的视觉定位和建图对于移动机器人导航仍然是挑战。本文提出一种基于视觉地点识别的示教复现导航方法来解决这一问题。", "method": "提出一种基于深度学习局部特征的视觉地点识别技术，包含新的视觉地点识别技术、新颖的水平偏移计算方法和多平台系统设计。同时引入一个新的公共数据集用于实验测试。", "result": "新系统在多个测试场景中优于现有方法，能够在室内外运行，并且对昼夜场景变化具有鲁棒性。", "conclusion": "本文提出的基于深度学习局部特征的视觉地点识别多平台示教复现导航系统在各种环境下表现出卓越的性能和鲁棒性。", "translation": "在统一和变化的环境中，稳定的视觉定位和建图对于移动机器人导航仍然是挑战。适用于此类环境的一种可能方法是基于外观的示教复现导航，它依赖于简化的定位和反应式机器人运动控制，而无需标准建图。这项工作为这种系统带来了一种基于视觉地点识别技术的创新解决方案。其中，主要贡献在于采用了新的视觉地点识别技术、新颖的水平偏移计算方法以及适用于各种类型移动机器人的多平台系统设计。其次，引入了一个新的用于外观导航方法实验测试的公共数据集。此外，这项工作还提供了实际场景的实验测试，并将引入的导航系统与其他最先进的方法进行了性能比较。结果证实，新系统在多个测试场景中优于现有方法，能够在室内外运行，并且对昼夜场景变化具有鲁棒性。", "summary": "本文针对移动机器人在复杂环境下的稳定导航问题，提出了一种基于深度学习局部特征和视觉地点识别的多平台示教复现导航系统。该系统包含创新的视觉地点识别技术和水平偏移计算方法，并设计为多平台通用。研究引入了一个新的公共数据集，并通过实验证明该系统在多种室内外环境和昼夜变化下，性能优于现有方法且具有鲁棒性。", "keywords": "示教复现导航, 视觉地点识别, 深度学习, 移动机器人, 定位", "comments": "这项工作的创新点在于结合深度学习局部特征进行视觉地点识别，并提出了新的水平偏移计算方法和多平台架构，增强了示教复现导航在复杂环境下的适应性和鲁棒性。引入新的公共数据集对该领域的研究具有重要意义。"}}
{"id": "2503.13205", "pdf": "https://arxiv.org/pdf/2503.13205", "abs": "https://arxiv.org/abs/2503.13205", "authors": ["Zhen Chen", "Zhihao Peng", "Xusheng Liang", "Cheng Wang", "Peigan Liang", "Linsheng Zeng", "Minjie Ju", "Yixuan Yuan"], "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MA"], "comment": null, "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.", "AI": {"title_translation": "MAP：评估与多智能体增强大型语言模型在住院路径中的应用", "tldr": "针对LLMs在住院路径中面临的数据和基准挑战，本研究构建了IPDS基准并提出了MAP多智能体框架，显著提高了诊断准确性和临床依从性，优于现有SOTA模型和人类医生。", "motivation": "住院路径中的复杂临床决策对医生构成挑战。尽管LLMs在医疗中有进展，但由于缺乏大规模住院数据集和现有基准未能涵盖住院环境的多方面临床决策，针对AI住院路径系统的研究有限。", "method": "1. 基于MIMIC-IV数据库构建了住院路径决策支持（IPDS）基准，包含51,274个病例。 2. 提出了多智能体住院路径（MAP）框架，包含分诊、诊断、治疗三个临床智能体和一个总智能体。 3. 进行了广泛实验，将MAP与现有SOTA LLM（HuatuoGPT2-13B）和三位认证医生进行比较。", "result": "1. 与HuatuoGPT2-13B相比，MAP的诊断准确率提高了25.10%。 2. MAP在临床依从性方面比三位认证医生高出10%-12%。", "conclusion": "MAP框架和IPDS基准为住院路径系统奠定了基础，在准确性和临床依从性方面表现出色。", "translation": "住院路径需要基于全面的患者信息做出复杂的临床决策，这对临床医生提出了严峻挑战。尽管大型语言模型（LLMs）在医疗应用中取得了进展，但由于缺乏大规模住院数据集，针对人工智能（AI）住院路径系统的研究有限。此外，现有医疗基准通常侧重于医疗问答和考试，忽略了住院环境中临床决策的多方面性。为了解决这些差距，我们首先从MIMIC-IV数据库开发了住院路径决策支持（IPDS）基准，涵盖了九个分诊部门和17个主要疾病类别的51,274个病例以及16个标准化治疗方案。然后，我们提出了多智能体住院路径（MAP）框架，通过三个临床智能体来完成住院路径：包括管理患者入院的分诊智能体、作为科室主要决策者的诊断智能体以及提供治疗方案的治疗智能体。此外，我们的MAP框架还包括一个监督住院路径的总智能体，用于指导和促进这三个临床智能体。广泛的实验表明，与现有最先进的LLM HuatuoGPT2-13B相比，我们的MAP将诊断准确率提高了25.10%。值得注意的是，我们的MAP表现出显著的临床依从性，比三位获得董事会认证的临床医生高出10%-12%，为住院路径系统奠定了基础。", "summary": "本研究旨在解决LLMs在住院路径应用中面临的数据和基准挑战。为此，研究构建了首个住院路径决策支持（IPDS）基准，并提出了多智能体住院路径（MAP）框架。MAP框架包含多个协同工作的临床智能体，旨在模拟和优化住院流程中的决策。实验结果表明，MAP框架在诊断准确性和临床依从性方面显著优于现有的SOTA LLM和人类临床医生，为AI在住院路径中的应用奠定了坚实基础。", "keywords": "住院路径, 大型语言模型, 多智能体系统, 临床决策支持, 医疗AI", "comments": "该研究解决了将LLMs应用于复杂临床流程（如住院路径）的关键空白。创建专门的基准（IPDS）和多智能体框架（MAP）是创新方法。与SOTA LLM和人类临床医生进行的比较提供了强有力的验证。对临床依从性的关注对于医疗AI尤为重要。"}}
{"id": "2503.13217", "pdf": "https://arxiv.org/pdf/2503.13217", "abs": "https://arxiv.org/abs/2503.13217", "authors": ["Yue Su", "Xinyu Zhan", "Hongjie Fang", "Han Xue", "Hao-Shu Fang", "Yong-Lu Li", "Cewu Lu", "Lixin Yang"], "title": "Dense Policy: Bidirectional Autoregressive Learning of Actions", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/.", "AI": {"title_translation": "密集策略：动作的双向自回归学习", "tldr": "引入了一种名为密集策略的双向扩展学习方法，用于动作预测中的自回归策略，通过轻量级编码器架构实现对动作序列的粗到细展开，实验表明其优于现有整体生成策略。", "motivation": "主流视觉运动策略主要依赖生成模型进行整体动作预测，而当前的自回归策略表现不佳，这促使研究人员寻找更有效的学习方法来释放自回归策略在机器人操作中的潜力。", "method": "本文提出一种名为密集策略的双向扩展学习方法，采用轻量级编码器架构，以对数时间推理，将动作序列从初始单帧迭代地粗到细展开到目标序列。", "result": "广泛的实验验证了密集策略具有优越的自回归学习能力，并且可以超越现有的整体生成策略。", "conclusion": "密集策略通过双向扩展学习和轻量级编码器架构，在动作预测中展现出优于现有方法的自回归学习能力。", "translation": "主流视觉运动策略主要依赖生成模型进行整体动作预测，而当前的自回归策略（预测下一个标记或块）表现出次优结果。这促使人们寻求更有效的学习方法，以释放自回归策略在机器人操作中的潜力。本文引入了一种称为密集策略的双向扩展学习方法，为动作预测中的自回归策略建立了一个新范式。它采用轻量级编码器架构，以对数时间推理，通过从初始单帧粗到细地迭代展开动作序列到目标序列。广泛的实验验证了我们的密集策略具有优越的自回归学习能力，并且可以超越现有的整体生成策略。我们的策略、示例数据和训练代码将在发布时公开可用。项目页面：https://selen-suyue.github.io/DspNet/。", "summary": "本文提出一种名为密集策略的双向自回归学习方法，旨在改进机器人操作中的动作预测。该方法采用轻量级编码器架构，通过对数时间推理实现动作序列的粗到细展开。实验结果表明，密集策略在自回归学习能力上优于现有的整体生成策略。", "keywords": "自回归策略, 动作预测, 机器人操作, 双向学习, 密集策略", "comments": "这篇论文提出了一种新的自回归策略范式，通过双向扩展学习和轻量级编码器架构解决了当前自回归策略在动作预测中表现不佳的问题，其对数时间推理的效率也是一个亮点。"}}
{"id": "2503.13227", "pdf": "https://arxiv.org/pdf/2503.13227", "abs": "https://arxiv.org/abs/2503.13227", "authors": ["Yijie Liu", "Xinyi Shang", "Yiqun Zhang", "Yang Lu", "Chen Gong", "Jing-Hao Xue", "Hanzi Wang"], "title": "Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data\nacross clients with limited labeled data to train a global model with strong\ngeneralization ability. Most FSSL methods rely on consistency regularization\nwith pseudo-labels, converting predictions from local or global models into\nhard pseudo-labels as supervisory signals. However, we discover that the\nquality of pseudo-label is largely deteriorated by data heterogeneity, an\nintrinsic facet of federated learning. In this paper, we study the problem of\nFSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label\nmismatches, further degrading model performance and convergence, and (2) local\nand global models' predictive tendencies diverge as heterogeneity increases.\nMotivated by these findings, we propose a simple and effective method called\nSemi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can\nflexibly correct pseudo-labels based on confidence discrepancies. This strategy\neffectively mitigates performance degradation caused by incorrect pseudo-labels\nand enhances consensus between local and global models. Experimental results\ndemonstrate that SAGE outperforms existing FSSL methods in both performance and\nconvergence. Our code is available at https://github.com/Jay-Codeman/SAGE", "AI": {"title_translation": "关注差距：置信度差异可指导跨伪匹配的联邦半监督学习", "tldr": "该研究提出了一种名为SAGE的新方法，通过利用置信度差异来纠正联邦半监督学习中的伪标签，以应对数据异质性问题，实验证明其效果优于现有方法。", "motivation": "联邦半监督学习（FSSL）旨在利用客户端的未标记数据来训练一个具有强大泛化能力的全局模型。然而，数据异质性（联邦学习的固有特性）会严重影响伪标签的质量，导致模型性能和收敛性下降。", "method": "提出了一种名为SAGE（Semi-supervised Aggregation for Globally-Enhanced Ensemble）的简单有效的方法，该方法通过置信度差异灵活地纠正伪标签，以减轻错误伪标签造成的性能下降，并增强局部和全局模型之间的一致性。", "result": "实验结果表明，SAGE在性能和收敛性方面均优于现有的FSSL方法。", "conclusion": "研究发现，数据异质性加剧了伪标签的不匹配，导致模型性能和收敛性下降，并且随着异质性的增加，局部和全局模型的预测趋势会发生分歧。提出的SAGE方法通过置信度差异纠正伪标签，有效缓解了这些问题。", "translation": "联邦半监督学习（FSSL）旨在利用客户端的未标记数据来训练一个具有强大泛化能力的全局模型。大多数FSSL方法依赖于一致性正则化和伪标签，将本地或全局模型的预测转换为硬伪标签作为监督信号。然而，我们发现伪标签的质量因数据异质性（联邦学习的内在特性）而大大降低。在本文中，我们深入研究了FSSL问题，并表明（1）异质性加剧了伪标签的不匹配，进一步降低了模型性能和收敛性，以及（2）随着异质性的增加，局部和全局模型的预测趋势会发生分歧。受这些发现的启发，我们提出了一种简单有效的方法，称为SAGE（Semi-supervised Aggregation for Globally-Enhanced Ensemble），该方法可以根据置信度差异灵活地纠正伪标签。这种策略有效地缓解了由错误伪标签引起的性能下降，并增强了局部和全局模型之间的一致性。实验结果表明，SAGE在性能和收敛性方面均优于现有的FSSL方法。我们的代码可在https://github.com/Jay-Codeman/SAGE上找到。", "summary": "该研究提出了一种名为SAGE的新方法，用于解决联邦半监督学习中的数据异质性问题。SAGE通过分析模型预测的置信度差异来纠正伪标签，从而提高模型性能和收敛速度。实验结果证明了SAGE的有效性。", "keywords": "联邦半监督学习, 数据异质性, 伪标签, 置信度差异, SAGE", "comments": "这项研究解决了联邦半监督学习中的一个关键挑战——数据异质性对伪标签质量的影响。提出的SAGE方法通过利用置信度差异来纠正伪标签，是一种创新且实用的解决方案。该方法不仅提高了模型性能，还加速了收敛过程，这在实际应用中具有重要意义。然而，对于SAGE在不同类型的数据异质性场景下的鲁棒性以及其计算复杂度仍需进一步的评估。"}}
{"id": "2503.13236", "pdf": "https://arxiv.org/pdf/2503.13236", "abs": "https://arxiv.org/abs/2503.13236", "authors": ["Ihab Asaad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Gradient Extrapolation for Debiased Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine learning classification models trained with empirical risk\nminimization (ERM) often inadvertently rely on spurious correlations. When\nabsent in the test data, these unintended associations between non-target\nattributes and target labels lead to poor generalization. This paper addresses\nthis problem from a model optimization perspective and proposes a novel method,\nGradient Extrapolation for Debiased Representation Learning (GERNE), designed\nto learn debiased representations in both known and unknown attribute training\ncases. GERNE uses two distinct batches with different amounts of spurious\ncorrelations to define the target gradient as the linear extrapolation of two\ngradients computed from each batch's loss. It is demonstrated that the\nextrapolated gradient, if directed toward the gradient of the batch with fewer\namount of spurious correlation, can guide the training process toward learning\na debiased model. GERNE can serve as a general framework for debiasing with\nmethods, such as ERM, reweighting, and resampling, being shown as special\ncases. The theoretical upper and lower bounds of the extrapolation factor are\nderived to ensure convergence. By adjusting this factor, GERNE can be adapted\nto maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The\nproposed approach is validated on five vision and one NLP benchmarks,\ndemonstrating competitive and often superior performance compared to\nstate-of-the-art baseline methods.", "AI": {"title_translation": "梯度外插用于去偏表示学习", "tldr": "该研究提出了一种名为GERNE的新方法，通过梯度外插来学习去偏表示，以解决机器学习模型中因虚假相关性导致的泛化能力差的问题。GERNE通过结合不同虚假相关性批次计算出的梯度来定义目标梯度，并已在多个基准测试中证明其有效性。", "motivation": "机器学习分类模型在经验风险最小化（ERM）训练中，常常会无意中依赖于虚假相关性。当测试数据中这些非目标属性与目标标签之间的非预期关联缺失时，会导致泛化能力下降。", "method": "提出了一种名为GERNE的新方法，该方法使用两个具有不同数量虚假相关性的批次来定义目标梯度。目标梯度是通过对从每个批次损失计算出的两个梯度进行线性外插来定义的。如果外插梯度指向虚假相关性较少批次的梯度方向，则可以指导训练过程学习一个去偏模型。", "result": "GERNE已被证明是一种通用的去偏框架，ERM、重加权和重采样等方法可视为其特例。理论上推导了外插因子的上下界以确保收敛。通过调整该因子，GERNE可以适应最大化分组平衡准确率（GBA）或最差分组准确率。该方法在五个视觉和一个自然语言处理基准测试中得到了验证，与现有最先进的方法相比，表现具有竞争力且通常更优。", "conclusion": "GERNE通过梯度外插为学习去偏表示提供了一个有效的框架，能够解决因虚假相关性导致的泛化能力问题，并在多个基准测试中取得了优于现有方法的性能。", "translation": "机器学习分类模型在经验风险最小化（ERM）训练中，常常会无意中依赖于虚假相关性。当测试数据中这些非目标属性与目标标签之间的非预期关联缺失时，会导致泛化能力下降。本研究从模型优化角度解决了这个问题，并提出了一种新颖的方法，梯度外插用于去偏表示学习（GERNE），旨在学习已知和未知属性训练情况下的去偏表示。GERNE使用两个具有不同数量虚假相关性的批次来定义目标梯度，该目标梯度是通过对从每个批次损失计算出的两个梯度进行线性外插来定义的。实验证明，如果外插梯度指向虚假相关性较少批次的梯度方向，则可以指导训练过程学习一个去偏模型。GERNE可以作为一种通用的去偏框架，其中ERM、重加权和重采样等方法可视为其特例。理论上推导了外插因子的上下界以确保收敛。通过调整该因子，GERNE可以适应最大化分组平衡准确率（GBA）或最差分组准确率。该方法在五个视觉和一个自然语言处理基准测试中得到了验证，与现有最先进的方法相比，表现具有竞争力且通常更优。", "summary": "本研究提出了一种名为GERNE的新方法，通过梯度外插技术来学习去偏表示，以解决机器学习模型中因虚假相关性导致的泛化能力差的问题。GERNE通过结合不同虚假相关性批次计算出的梯度来定义目标梯度，从而引导模型学习更鲁棒的表示。该方法具有通用性，并已在多个视觉和NLP基准测试中验证了其有效性和优越性。", "keywords": "虚假相关性,去偏表示学习,梯度外插,经验风险最小化,分组平衡准确率", "comments": "该研究提出了一种名为GERNE的新颖方法，通过梯度外插来解决机器学习中的虚假相关性问题，这在理论和实践上都具有重要意义。方法的核心在于利用不同虚假相关性批次的梯度信息来指导模型学习去偏表示，并提供了理论保证。其通用性和在多个基准测试上的优异表现也证明了该方法的潜力。然而，该方法在实际应用中的计算成本和对超参数（如外插因子）的敏感性可能需要进一步研究。"}}
{"id": "2503.13277", "pdf": "https://arxiv.org/pdf/2503.13277", "abs": "https://arxiv.org/abs/2503.13277", "authors": ["Alfred Simbun", "Suresh Kumar"], "title": "Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "27 pages, 6 figures, 10 tables", "summary": "Background: The COVID-19 pandemic has overwhelmed healthcare systems,\nemphasizing the need for AI-driven tools to assist in rapid and accurate\npatient prognosis. Chest X-ray imaging is a widely available diagnostic tool,\nbut existing methods for prognosis classification lack scalability and\nefficiency. Objective: This study presents a high-accuracy deep learning model\nfor classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest\nX-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a\ndataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained\nand validated a deep learning model leveraging Convolutional Neural Networks\n(CNNs). The model was evaluated on an unseen dataset to measure accuracy,\nprecision, and recall. Results: Our model achieved an average accuracy of 97%,\nwith specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When\nclassifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),\n95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's\npotential for real-world clinical applications, aiding in faster\ndecision-making and improved resource allocation. Conclusion: AI-driven\nprognosis classification using deep learning can significantly enhance COVID-19\npatient management, enabling early intervention and efficient triaging. Our\nstudy provides a scalable, high-accuracy AI framework for integrating deep\nlearning into routine clinical workflows. Future work should focus on expanding\ndatasets, external validation, and regulatory compliance to facilitate clinical\nadoption.", "AI": {"title_translation": "人工智能驱动的基于胸部X光片的COVID-19预后分类：一种深度学习方法", "tldr": "该研究提出了一种高精度的深度学习模型，使用胸部X光片对COVID-19的严重程度进行分类，准确率达97%，可用于临床决策和资源分配。", "motivation": "COVID-19大流行给医疗系统带来了巨大压力，需要人工智能驱动的工具来辅助快速准确的患者预后评估。胸部X光片是一种易于获得的诊断工具，但现有的预后分类方法缺乏可扩展性和效率。", "method": "使用包含1,103张确诊COVID-19的X光片的公开数据集，利用卷积神经网络（CNN）训练和验证了一个深度学习模型。模型在未见过的数据集上进行了评估，以衡量准确率、精确率和召回率。", "result": "该模型在COVID-19预后分类任务中取得了平均97%的准确率，特异性为99%，敏感性为87%，F1分数为93.11%。在区分不同严重程度（轻度、中度、重度）时，模型准确率分别为89.03%、95.77%和81.16%。", "conclusion": "人工智能驱动的深度学习预后分类能够显著改善COVID-19患者的管理，实现早期干预和高效分诊。该研究提供了一个可扩展、高精度的AI框架，可将深度学习整合到常规临床工作流程中。", "translation": "背景：COVID-19大流行给医疗保健系统带来了巨大压力，强调了需要人工智能驱动的工具来协助快速准确的患者预后评估。胸部X光成像是一种广泛可用的诊断工具，但现有的预后分类方法缺乏可扩展性和效率。目标：本研究提出了一种使用胸部X光图像对COVID-19严重程度（轻度、中度、重度）进行分类的高精度深度学习模型，该模型是在Microsoft Azure Custom Vision上开发的。方法：我们使用来自AIforCOVID的数据集中1,103张已确认的COVID-19 X光片，利用卷积神经网络（CNN）训练和验证了一个深度学习模型。在未见过的数据集上对模型进行了评估，以衡量准确率、精确率和召回率。结果：我们的模型取得了97%的平均准确率，特异性为99%，敏感性为87%，F1分数为93.11%。在对COVID-19严重程度进行分类时，模型分别取得了89.03%（轻度）、95.77%（中度）和81.16%（重度）的准确率。这些结果证明了该模型在现实临床应用中的潜力，有助于加快决策制定和改善资源分配。结论：人工智能驱动的深度学习预后分类可以显著改善COVID-19患者的管理，实现早期干预和高效分诊。我们的研究提供了一个可扩展、高精度的AI框架，用于将深度学习整合到常规临床工作流程中。未来的工作应侧重于扩展数据集、外部验证和监管合规性，以促进临床应用。", "summary": "这项研究开发了一种基于深度学习的先进模型，利用胸部X光片对COVID-19患者的病情严重程度进行分类。该模型在公开数据集上进行了训练和验证，并在未见过的数据集上取得了高准确率（97%），显示出其在临床实践中辅助诊断和资源分配的巨大潜力。", "keywords": "人工智能, COVID-19, 胸部X光片, 深度学习, 预后分类", "comments": "该研究展示了深度学习在COVID-19预后分类方面的强大能力，并取得了令人印象深刻的准确率。然而，未来工作需要关注数据集的扩展、外部验证以及确保符合监管要求，以促进其在临床实践中的广泛应用。"}}
{"id": "2503.13309", "pdf": "https://arxiv.org/pdf/2503.13309", "abs": "https://arxiv.org/abs/2503.13309", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Milad Amir Toutounchian", "Arash Mohammadi"], "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available.", "AI": {"title_translation": "将人工智能应用于以人为本的乳腺癌诊断：一种多尺度、多视图Swin Transformer框架", "tldr": "该研究提出了一种名为MSMV-Swin的混合、多尺度、多视图Swin Transformer框架，用于改进乳腺癌的AI辅助诊断。该框架利用SAM模型进行乳腺叶分割，提取多尺度特征，并采用混合融合结构来应对临床实践中常见的遗漏视图问题，旨在提高诊断的准确性和鲁棒性，并促进放射科医生与AI的协同工作。", "motivation": "尽管在计算机辅助诊断（CAD）系统方面取得了进展，但乳腺癌仍然是女性癌症相关死亡的主要原因之一。现有的AI深度学习（DL）诊断方法在依赖详细的肿瘤注释和应对视图缺失方面存在挑战。因此，有必要将AI整合到以人为中心的工作流程中，以提高乳腺癌诊断的鲁棒性和准确性。", "method": "提出了一种混合、多尺度、多视图Swin Transformer（MSMV-Swin）框架。该框架利用Segment Anything Model（SAM）来分割乳腺叶，以减少背景噪声并提取特征。其多尺度设计能够捕捉肿瘤区域和周围组织的上下文信息，以模拟放射科医生的诊断方式。此外，该框架采用混合融合结构来应对视图缺失的问题。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "尽管在计算机辅助诊断（CAD）系统方面取得了进展，但乳腺癌仍然是女性癌症相关死亡的主要原因之一。近期人工智能（AI）的突破在通过乳腺X线摄影术诊断乳腺癌的先进深度学习（DL）架构开发方面显示出巨大潜力。在此背景下，本文着重于将AI整合到以人为中心的工作流程中，以增强乳腺癌的诊断能力。然而，诸如依赖详细的肿瘤注释和易受视图缺失（尤其是在测试时）影响等关键挑战在很大程度上被忽视了。为了解决这些问题，我们提出了一种混合、多尺度、多视图的Swin Transformer（MSMV-Swin）框架，以提高诊断的鲁棒性和准确性。提出的MSMV-Swin框架旨在作为一种决策支持工具，帮助放射科医生更有效地分析多视图乳腺X线照片。更具体地说，MSMV-Swin框架利用Segment Anything Model（SAM）来分离乳腺叶，减少背景噪声，并实现全面的特征提取。提出的MSMV-Swin框架的多尺度性质考虑了肿瘤特异性区域以及肿瘤周围组织的空间特征，捕获了局部和上下文信息。上下文和局部数据的整合确保MSMV-Swin的输出与放射科医生解释乳腺X线照片的方式保持一致，从而促进了更好的人机交互和信任。然后设计了一个混合融合结构，以确保在临床实践中常见的仅有一个乳腺X线照片视图可用时，能够抵抗视图缺失。", "summary": "本研究提出了一种名为MSMV-Swin的混合、多尺度、多视图Swin Transformer框架，旨在通过整合AI到以人为中心的工作流程中来改进乳腺癌的诊断。该框架利用SAM模型进行乳腺叶分割，并结合多尺度特征提取和混合融合结构，以解决现有方法在依赖详细注释和处理视图缺失方面的挑战，从而提高诊断的准确性和鲁棒性，并促进放射科医生与AI的协同工作。", "keywords": "乳腺癌诊断, AI, Swin Transformer, 多尺度, 多视图", "comments": "该研究提出了一种创新的多尺度、多视图Swin Transformer框架，用于改进乳腺癌的AI辅助诊断。通过整合SAM模型和混合融合结构，该框架有望解决现有方法中的关键挑战，并提高诊断的准确性和鲁棒性。然而，其在实际临床应用中的效果和与放射科医生的协同作用仍需进一步验证。"}}
{"id": "2503.13330", "pdf": "https://arxiv.org/pdf/2503.13330", "abs": "https://arxiv.org/abs/2503.13330", "authors": ["Ricardo Bigolin Lanfredi", "Yan Zhuang", "Mark Finkelstein", "Praveen Thoppey Srinivasan Balamuralikrishna", "Luke Krembs", "Brandon Khoury", "Arthi Reddy", "Pritam Mukherjee", "Neil M. Rofsky", "Ronald M. Summers"], "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.", "AI": {"title_translation": "基于大语言模型的腹部CT监督标签器：LEAVS", "tldr": "提出了一种名为LEAVS的基于大语言模型的腹部CT影像监督标签器，用于从放射学报告中提取异常的确定性和紧急性信息，并在腹部器官异常检测任务中表现优于现有方法和人工标注。", "motivation": "现有工作主要集中在胸部区域的影像模型构建，而腹部影像学报告的研究较少，因为腹部解剖结构更复杂、病变范围更广。需要一种方法来从腹部CT放射学报告中提取结构化标签，以构建腹部影像模型。", "method": "利用专门的链式思考提示策略，结合句子提取和多项选择题，在一个基于树的决策系统中运行本地化的大语言模型（LLM），用于标注七种腹部异常的确定性和紧急性。", "result": "LEAVS在提取多种腹部器官的异常类型方面，平均F1得分为0.89，显著优于竞争性标签器和人工标注。在紧急性标签提取方面，性能与人工标注相当。此外，所提取的异常标签可用于训练一个单一的影像模型，对多个器官进行正常或异常分类。", "conclusion": "LEAVS是一种有效的大语言模型驱动的标签器，能够从腹部CT放射学报告中准确提取异常和紧急性信息，为腹部影像模型的研究和开发提供了有价值的资源，并展示了其在训练下游视觉模型方面的潜力。", "translation": "从放射学报告中提取结构化标签已被用于创建视觉模型，以同时检测多种异常类型。然而，现有工作主要集中在胸部区域。由于腹部解剖结构更复杂且病变范围更广，对腹部放射学报告的研究很少。我们提出了LEAVS（腹部影像监督的大语言模型提取器）。该标签器可以标注CT放射学报告中七种异常的确定性和紧急性。为了确保广泛的覆盖范围，我们选择了涵盖CT报告中大多数发现类型的异常。我们的方法采用了一种专门的链式思考提示策略，用于使用句子提取和多项选择题在一个基于树的决策系统中运行本地化的大语言模型。我们证明了该大语言模型可以提取腹部器官的多种异常类型，平均F1得分为0.89，显著优于竞争性标签器和人工。此外，我们表明紧急性标签的提取性能与人工标注相当。最后，我们证明了异常标签包含有价值的信息，可用于训练一个将多个器官分类为正常或异常的单一视觉模型。我们发布了我们的代码和结构化注释，用于包含超过1000个CT容积的公共CT数据集。", "summary": "本研究提出了LEAVS，一个基于大语言模型的腹部CT影像监督标签器。该方法利用链式思考提示策略从腹部CT放射学报告中提取七种异常的确定性和紧急性信息，旨在解决腹部影像研究中存在的挑战。实验结果表明，LEAVS在异常类型提取方面取得了0.89的平均F1分数，优于现有方法和人工标注，并且在紧急性标签提取方面表现与人工相当。此外，LEAVS生成的标签可用于训练有效的腹部器官异常检测视觉模型。研究团队公开了代码和标注数据，以促进该领域的进一步研究。", "keywords": "腹部CT, 大语言模型, 影像监督, 放射学报告, 异常检测", "comments": "该研究提出了一种创新的方法，利用大语言模型（LLM）处理腹部CT放射学报告，以生成用于视觉模型训练的结构化标签。其优势在于解决了腹部影像学研究的复杂性，并实现了优于现有方法和人工标注的性能。链式思考提示策略和树状决策系统是其关键技术。然而，对于LLM在处理医学文本时的潜在偏见或局限性，以及模型在不同CT扫描设备和报告风格下的泛化能力，可能需要进一步的探讨。总的来说，这项工作为利用LLM推动医学影像分析提供了有前景的途径。"}}
{"id": "2503.13369", "pdf": "https://arxiv.org/pdf/2503.13369", "abs": "https://arxiv.org/abs/2503.13369", "authors": ["Wan Ju Kang", "Eunki Kim", "Na Min An", "Sangryul Kim", "Haemin Choi", "Ki Hoon Kwak", "James Thorne"], "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "37 pages, 10 figures, 21 tables", "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.", "AI": {"title_translation": "视觉化计数：利用有视力用户的反馈构建与BLV对齐的图示描述数据集", "tldr": "该研究提出了一种名为Sightation的新方法，利用有视力用户对视觉语言模型（VLM）生成的图示描述进行评估，而不是直接生成描述，以解决为视障和低视力（BLV）用户创建图示描述的挑战。研究结果表明，这种方法是有效的，并且可以用于为BLV用户创建有用的数据集，该数据集已发布并可用于各种下游任务。", "motivation": "为视障和低视力（BLV）用户生成图示描述是一个具有挑战性的领域，因为有视力注释者的直接描述可能成本高昂、存在偏见且不符合BLV用户的标准。", "method": "利用有视力用户评估由视觉语言模型（VLM）生成的图示描述，这些模型通过多轮推理并利用潜在监督进行指导。", "result": "有视力用户的评估被证明是有效的，并且对教授视障学习者的BLV专业教育者有用。研究发布了一个名为Sightation的数据集，包含5000个图示和137000个样本，用于各种训练任务，并展示了其在下游任务中的微调潜力。", "conclusion": "Sightation方法通过利用有视力用户的反馈来评估VLM生成的图示描述，为BLV用户创建高质量数据集提供了一种有效且有用的途径，并且该数据集在下游任务中具有良好的微调潜力。", "translation": "通常，注释者群体和最终用户群体之间的需求和视觉能力存在差异。为视障和低视力（BLV）用户生成详细的图示描述是其中一个具有挑战性的领域。有视力注释者可以轻松地描述视觉内容，但现有研究表明，由他们直接生成描述成本高昂、存在偏见，并且在一定程度上不符合BLV用户的标准。在本研究中，我们要求有视力者评估而非生成由视觉语言模型（VLM）生成的图示描述，这些模型通过多轮推理并利用潜在监督进行指导。有视力者的评估被证明是有效的，并且对身为BLV用户且教授视障学习者的专业教育者有用。我们发布了Sightation，这是一个包含5000个图示和137000个样本的图示描述数据集集合，用于完成、偏好、检索、问答和推理训练目的，并展示了它们在各种下游任务中的微调潜力。", "summary": "该研究提出了一种名为Sightation的新方法，旨在解决为视障和低视力（BLV）用户创建图示描述的挑战。与传统的由有视力者直接生成描述不同，该方法利用有视力用户评估由视觉语言模型（VLM）生成的描述。研究结果表明，这种评估方法是有效的，并且对BLV专业教育者有用。研究团队发布了一个名为Sightation的大型数据集，包含5000个图示和137000个样本，可用于多种训练任务，并证明了其在下游应用中的微调潜力。", "keywords": "图示描述, 视障用户, 视觉语言模型, 用户反馈, 数据集", "comments": "这项研究的创新之处在于将有视力用户的角色从“描述生成者”转变为“评估者”，从而克服了传统方法中存在的成本高、偏见和不符合BLV用户标准的问题。利用有视力用户的反馈来指导和改进VLM生成的描述，是一种巧妙且有效的方法。Sightation数据集的发布为BLV用户相关的视觉辅助技术研究提供了宝贵的资源。然而，未来可以进一步探索如何更精细化地利用用户的反馈，以及如何评估不同类型图示的描述效果。"}}
{"id": "2503.13400", "pdf": "https://arxiv.org/pdf/2503.13400", "abs": "https://arxiv.org/abs/2503.13400", "authors": ["Qi Zhang", "Xiuyuan Chen", "Ziyi He", "Kun Wang", "Lianming Wu", "Hongxing Shen", "Jianqi Sun"], "title": "U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "T2 hyperintensities in spinal cord MR images are crucial biomarkers for\nconditions such as degenerative cervical myelopathy. However, current clinical\ndiagnoses primarily rely on manual evaluation. Deep learning methods have shown\npromise in lesion detection, but most supervised approaches are heavily\ndependent on large, annotated datasets. Unsupervised anomaly detection (UAD)\noffers a compelling alternative by eliminating the need for abnormal data\nannotations. However, existing UAD methods rely on curated normal datasets and\ntheir performance frequently deteriorates when applied to clinical datasets due\nto domain shifts. We propose an Uncertainty-based Unsupervised Anomaly\nDetection framework, termed U2AD, to address these limitations. Unlike\ntraditional methods, U2AD is designed to be trained and tested within the same\nclinical dataset, following a \"mask-and-reconstruction\" paradigm built on a\nVision Transformer-based architecture. We introduce an uncertainty-guided\nmasking strategy to resolve task conflicts between normal reconstruction and\nanomaly detection to achieve an optimal balance. Specifically, we employ a\nMonte-Carlo sampling technique to estimate reconstruction uncertainty mappings\nduring training. By iteratively optimizing reconstruction training under the\nguidance of both epistemic and aleatoric uncertainty, U2AD reduces overall\nreconstruction variance while emphasizing regions. Experimental results\ndemonstrate that U2AD outperforms existing supervised and unsupervised methods\nin patient-level identification and segment-level localization tasks. This\nframework establishes a new benchmark for incorporating uncertainty guidance\ninto UAD, highlighting its clinical utility in addressing domain shifts and\ntask conflicts in medical image anomaly detection. Our code is available:\nhttps://github.com/zhibaishouheilab/U2AD", "AI": {"title_translation": "基于不确定性的无监督异常检测框架用于检测MRI脊髓中的T2高信号", "tldr": "U2AD是一个基于不确定性的无监督异常检测框架，用于检测脊髓MRI中的T2高信号。它使用不确定性引导的掩码策略和蒙特卡洛采样来解决正常重构和异常检测之间的冲突，并在同一临床数据集上进行训练和测试，克服了领域转移问题，并在患者级识别和分割级定位任务上优于现有方法。", "motivation": "目前的临床诊断严重依赖手动评估，而深度学习方法虽然有潜力，但监督方法需要大量带标注的数据集。无监督异常检测（UAD）无需异常数据标注，但现有方法依赖于精心挑选的正常数据集，在临床数据集上由于领域转移性能会下降。", "method": "提出了一种名为U2AD的不确定性无监督异常检测框架。该框架采用基于视觉Transformer的掩码和重构范式，并引入了不确定性引导的掩码策略。通过蒙特卡洛采样估计重构不确定性映射，并结合认知不确定性和偶然不确定性来优化重构训练，以减少整体重构方差并突出显示感兴趣区域。", "result": "实验结果表明，U2AD在患者级识别和分割级定位任务上优于现有的监督和无监督方法。该框架在解决领域转移和任务冲突方面具有临床应用价值。", "conclusion": "U2AD框架通过引入不确定性引导，成功解决了无监督异常检测中的领域转移和任务冲突问题，在脊髓MRI T2高信号检测方面取得了优于现有方法的性能，并为医学图像异常检测领域的不确定性应用树立了新的标杆。", "translation": "脊髓MRI中的T2高信号是退行性颈髓病等疾病的关键生物标志物。然而，目前的临床诊断主要依赖手动评估。深度学习方法在病灶检测方面显示出潜力，但大多数监督方法严重依赖大型标注数据集。无监督异常检测（UAD）提供了一种有吸引力的替代方案，无需异常数据标注。然而，现有的UAD方法依赖于精心挑选的正常数据集，并且由于领域转移，在应用于临床数据集时其性能经常会下降。我们提出了一种名为U2AD的不确定性无监督异常检测框架来解决这些局限性。与传统方法不同，U2AD旨在同一临床数据集内进行训练和测试，遵循基于视觉Transformer架构的“掩码和重构”范式。我们引入了一种不确定性引导的掩码策略来解决正常重构和异常检测之间的任务冲突，以实现最佳平衡。具体来说，我们采用蒙特卡洛采样技术来估计训练期间的重构不确定性映射。通过在认知不确定性和偶然不确定性的指导下迭代优化重构训练，U2AD减少了整体重构方差，同时突出了感兴趣区域。实验结果表明，U2AD在患者级识别和分割级定位任务上优于现有的监督和无监督方法。该框架为将不确定性引导纳入UAD树立了新的标杆，突显了其在解决医学图像异常检测中的领域转移和任务冲突方面的临床应用价值。我们的代码可在：https://github.com/zhibaishouheilab/U2AD 获取。", "summary": "本研究提出了U2AD，一种用于脊髓MRI中T2高信号检测的不确定性无监督异常检测框架。该框架克服了传统无监督方法在临床数据集上因领域转移而导致的性能下降问题，并且无需手动标注异常数据。U2AD采用基于Transformer的掩码和重构方法，并通过不确定性引导的掩码策略来平衡重构和异常检测任务。实验证明，U2AD在患者级识别和分割级定位方面均优于现有方法。", "keywords": "无监督异常检测, 脊髓MRI, T2高信号, 不确定性引导, 视觉Transformer", "comments": "该研究提出的U2AD框架在解决医学图像异常检测中的领域转移和任务冲突方面具有重要意义。通过利用不确定性引导和蒙特卡洛采样，该方法在无需大量标注数据的情况下实现了高性能，这对于临床应用非常有价值。代码的公开也为该领域的研究提供了便利。"}}
{"id": "2503.13441", "pdf": "https://arxiv.org/pdf/2503.13441", "abs": "https://arxiv.org/abs/2503.13441", "authors": ["Ri-Zhao Qiu", "Shiqi Yang", "Xuxin Cheng", "Chaitanya Chawla", "Jialong Li", "Tairan He", "Ge Yan", "Lars Paulsen", "Ge Yang", "Sha Yi", "Guanya Shi", "Xiaolong Wang"], "title": "Humanoid Policy ~ Human Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Code and data: https://human-as-robot.github.io/", "summary": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/", "AI": {"title_translation": "人形机器人策略 ~ 人类策略", "tldr": "本研究提出了一种从人类演示中学习机器人操作策略的方法，以解决数据收集成本高昂的问题。通过收集以自我为中心的人类演示数据集（PH2D），并训练一个名为Human Action Transformer（HAT）的跨体策略模型，该模型能够统一人类和人形机器人的状态-动作空间，并可区分地重新定向到机器人动作。实验证明，人类数据能提高HAT的泛化性和鲁棒性，并显著提高数据收集效率。", "motivation": "传统的机器人操作策略训练依赖于机器人演示，但数据收集成本高昂且难以扩展。本研究旨在探索更具扩展性的人类演示数据源，以作为机器人学习的跨体训练数据。", "method": "本研究收集了一个以自我为中心、面向任务的数据集（PH2D），该数据集与人形机器人的操作演示直接对齐。然后，训练了一个名为Human Action Transformer（HAT）的人类-人形机器人行为策略模型。HAT的状态-动作空间对人类和人形机器人是统一的，并且可以进行可区分的重定向以适应机器人动作。通过与少量机器人数据进行联合训练，HAT能够直接对人形机器人和人类进行建模，将它们视为不同的体，而无需额外的监督。", "result": "与仅使用机器人数据训练的策略相比，HAT在泛化性和鲁棒性方面均有所提高，并且数据收集效率显著更高。", "conclusion": "使用人类演示数据作为跨体训练数据，可以有效提高人形机器人操作策略的泛化性和鲁棒性，同时降低数据收集成本。", "translation": "训练具有多样化数据的人形机器人操作策略可以增强其在不同任务和平台上的鲁棒性和泛化能力。然而，仅从机器人演示中学习是劳动密集型的，需要昂贵的人工操作数据收集，难以扩展。本研究研究了一个更具扩展性数据源，即以自我为中心的人类演示，作为机器人学习的跨体训练数据。我们从数据和建模两个角度缓解了人形机器人和人类之间的体差异。我们收集了一个以自我为中心、面向任务的数据集（PH2D），该数据集直接与人形机器人的操作演示对齐。然后，我们训练了一个人类-人形机器人行为策略，我们称之为Human Action Transformer（HAT）。HAT的状态-动作空间对人类和人形机器人是统一的，并且可以进行可区分的重定向以适应机器人动作。通过与少量机器人数据进行联合训练，HAT能够直接对人形机器人和人类进行建模，将它们视为不同的体，而无需额外的监督。我们表明，人类数据能够提高HAT的泛化性和鲁棒性，并具有显著更高的数据收集效率。代码和数据：https://human-as-robot.github.io/", "summary": "本研究提出了一种利用人类演示数据来训练人形机器人操作策略的新方法。通过收集以自我为中心的人类演示数据集（PH2D）并训练名为Human Action Transformer（HAT）的跨体策略模型，该模型能够统一人类和人形机器人的状态-动作空间，并实现可区分的重定向。实验结果表明，这种方法能够有效提高机器人策略的泛化性和鲁棒性，并显著提高数据收集效率。", "keywords": "人形机器人, 操作策略, 人类演示, 跨体学习, HAT模型", "comments": "该研究提出了一种创新的方法，利用人类演示数据来训练人形机器人，解决了传统方法数据收集成本高、难以扩展的问题。通过PH2D数据集和HAT模型，有效弥合了人类与机器人之间的体差异，并在泛化性、鲁棒性和数据效率方面取得了显著成果。未来可以进一步探索更多样化的人类演示数据和更复杂的任务场景。"}}
{"id": "2503.13446", "pdf": "https://arxiv.org/pdf/2503.13446", "abs": "https://arxiv.org/abs/2503.13446", "authors": ["Zhenyu Wu", "Yuheng Zhou", "Xiuwei Xu", "Ziwei Wang", "Haibin Yan"], "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://gary3410.github.io/momanipVLA/", "summary": "Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models.", "AI": {"title_translation": "面向通用移动操作的视觉-语言-动作模型迁移", "tldr": "提出MoManipVLA框架，将固定基座操作的视觉-语言-动作(VLA)模型迁移到移动操作任务，通过生成导航点和优化机器人基座与手臂的运动规划目标，实现了跨任务和跨环境的泛化能力。实验证明，该方法在OVMM和真实世界中比现有技术成功率高4.2%，且训练成本低。", "motivation": "现有移动操作方法因缺乏大规模训练而难以泛化到不同任务和环境；而现有的视觉-语言-动作(VLA)模型虽然泛化能力强，但仅限于固定基座操作。", "method": "提出MoManipVLA框架，利用预训练的VLA模型生成高泛化能力的末端执行器导航点，并设计了优化移动基座和机器人手臂运动规划的目标以最大化轨迹的物理可行性。采用双层目标优化框架，上层优化预测基座移动导航点以扩展机械臂策略空间，下层优化选择最优末端执行器轨迹以完成任务。", "result": "MoManipVLA在OVMM和真实世界实验中，成功率比现有技术高4.2%，且由于预训练VLA模型的强大泛化能力，真实世界部署的训练成本仅为50。", "conclusion": "MoManipVLA框架能够有效地将固定基座操作的VLA模型迁移到移动操作任务，通过优化导航点和运动规划，实现了在不同任务和环境下的高泛化能力，并降低了训练成本。", "translation": "移动操作是机器人学中在日常生活中协助人类完成各种任务和环境的基本挑战。然而，传统的移动操作方法由于缺乏大规模训练，在跨任务和环境的泛化方面常常遇到困难。相比之下，视觉-语言-动作（VLA）模型近期的进展展示了令人印象深刻的泛化能力，但这些基础模型是为固定基座操作任务开发的。因此，我们提出了一个名为MoManipVLA的高效策略适应框架，将预训练的固定基座操作VLA模型迁移到移动操作，以在移动操作策略中实现跨任务和环境的高泛化能力。具体来说，我们利用预训练的VLA模型生成具有高泛化能力的末端执行器导航点。我们为移动基座和机器人手臂设计了运动规划目标，旨在最大化轨迹的物理可行性。最后，我们提出了一个高效的双层目标优化框架用于轨迹生成，其中上层优化预测基座移动的导航点以增强机械臂策略空间，下层优化选择最优的末端执行器轨迹以完成操作任务。通过这种方式，MoManipVLA可以以零样本（zero-shot）的方式调整机器人基座的位置，从而使固定基座VLA模型预测的导航点变得可行。在OVMM和真实世界的广泛实验结果表明，MoManipVLA比最先进的移动操作方法成功率高4.2%，并且由于预训练VLA模型的强大泛化能力，在真实世界部署中仅需50的训练成本。", "summary": "该研究提出了MoManipVLA框架，旨在解决移动操作中的泛化性挑战。该框架通过迁移固定基座操作的视觉-语言-动作（VLA）模型到移动操作任务，利用VLA模型生成末端执行器导航点，并结合双层优化策略来规划移动基座和机器人手臂的运动，以提高轨迹的物理可行性。实验结果表明，MoManipVLA在提高成功率和降低训练成本方面优于现有技术。", "keywords": "移动操作,视觉-语言-动作模型,策略迁移,导航点生成,双层优化", "comments": "该研究有效地解决了移动操作中的泛化性问题，通过迁移预训练的VLA模型并结合创新的双层优化框架，实现了显著的性能提升。其零样本适应能力和低训练成本是该方法的突出优点。然而，未来可以进一步探索该方法在更复杂和动态环境中的鲁棒性。"}}
