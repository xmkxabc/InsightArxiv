{"id": "2503.20791", "pdf": "https://arxiv.org/pdf/2503.20791", "abs": "https://arxiv.org/abs/2503.20791", "authors": ["John Murzaku", "Zifan Liu", "Vaishnavi Muppala", "Md Mehrab Tanjim", "Xiang Chen", "Yunyao Li"], "title": "ECLAIR: Enhanced Clarification for Interactive Responses in an Enterprise AI Assistant", "categories": ["cs.CL", "68T50", "I.2.7; H.5.2"], "comment": "3 pages, 1 figure", "summary": "Large language models (LLMs) have shown remarkable progress in understanding\nand generating natural language across various applications. However, they\noften struggle with resolving ambiguities in real-world, enterprise-level\ninteractions, where context and domain-specific knowledge play a crucial role.\nIn this demonstration, we introduce ECLAIR (Enhanced CLArification for\nInteractive Responses), a multi-agent framework for interactive disambiguation.\nECLAIR enhances ambiguous user query clarification through an interactive\nprocess where custom agents are defined, ambiguity reasoning is conducted by\nthe agents, clarification questions are generated, and user feedback is\nleveraged to refine the final response. When tested on real-world customer\ndata, ECLAIR demonstrates significant improvements in clarification question\ngeneration compared to standard few-shot methods."}
{"id": "2503.20794", "pdf": "https://arxiv.org/pdf/2503.20794", "abs": "https://arxiv.org/abs/2503.20794", "authors": ["Veysel Kocaman", "Muhammed Santas", "Yigit Gul", "Mehmet Butgul", "David Talby"], "title": "Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text DeIdentification?", "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG", "H.3, F.2.2, I.2.7"], "comment": "14 pages, accepted at Text2Story Workshop at ECIR 2025", "summary": "We systematically assess the performance of three leading API-based\nde-identification systems - Azure Health Data Services, AWS Comprehend Medical,\nand OpenAI GPT-4o - against our de-identification systems on a ground truth\ndataset of 48 clinical documents annotated by medical experts. Our analysis,\nconducted at both entity-level and token-level, demonstrates that our solution,\nHealthcare NLP, achieves the highest accuracy, with a 96% F1-score in protected\nhealth information (PHI) detection, significantly outperforming Azure (91%),\nAWS (83%), and GPT-4o (79%). Beyond accuracy, Healthcare NLP is also the most\ncost-effective solution, reducing processing costs by over 80% compared to\nAzure and GPT-4o. Its fixed-cost local deployment model avoids the escalating\nper-request fees of cloud-based services, making it a scalable and economical\nchoice. Our results underscore a critical limitation: zero-shot commercial APIs\nfail to meet the accuracy, adaptability, and cost-efficiency required for\nregulatory-grade clinical de-identification. Healthcare NLP's superior\nperformance, customization capabilities, and economic advantages position it as\nthe more viable solution for healthcare organizations seeking compliance and\nscalability in clinical NLP workflows."}
{"id": "2503.20797", "pdf": "https://arxiv.org/pdf/2503.20797", "abs": "https://arxiv.org/abs/2503.20797", "authors": ["Muhammad Haroon", "Magdalena Wojcieszak", "Anshuman Chhabra"], "title": "\"Whose Side Are You On?\" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "The rapid growth of social media platforms has led to concerns about\nradicalization, filter bubbles, and content bias. Existing approaches to\nclassifying ideology are limited in that they require extensive human effort,\nthe labeling of large datasets, and are not able to adapt to evolving\nideological contexts. This paper explores the potential of Large Language\nModels (LLMs) for classifying the political ideology of online content in the\ncontext of the two-party US political spectrum through in-context learning\n(ICL). Our extensive experiments involving demonstration selection in\nlabel-balanced fashion, conducted on three datasets comprising news articles\nand YouTube videos, reveal that our approach significantly outperforms\nzero-shot and traditional supervised methods. Additionally, we evaluate the\ninfluence of metadata (e.g., content source and descriptions) on ideological\nclassification and discuss its implications. Finally, we show how providing the\nsource for political and non-political content influences the LLM's\nclassification."}
{"id": "2503.20801", "pdf": "https://arxiv.org/pdf/2503.20801", "abs": "https://arxiv.org/abs/2503.20801", "authors": ["Tao Meng", "Shuo Shan", "Hongen Shao", "Yuntao Shou", "Wei Ai", "Keqin Li"], "title": "SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative Optimization for Semi-supervised Entity Alignment", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Entity alignment aims to use pre-aligned seed pairs to find other equivalent\nentities from different knowledge graphs (KGs) and is widely used in graph\nfusion-related fields. However, as the scale of KGs increases, manually\nannotating pre-aligned seed pairs becomes difficult. Existing research utilizes\nentity embeddings obtained by aggregating single structural information to\nidentify potential seed pairs, thus reducing the reliance on pre-aligned seed\npairs. However, due to the structural heterogeneity of KGs, the quality of\npotential seed pairs obtained using only a single structural information is not\nideal. In addition, although existing research improves the quality of\npotential seed pairs through semi-supervised iteration, they underestimate the\nimpact of embedding distortion produced by noisy seed pairs on the alignment\neffect. In order to solve the above problems, we propose a seed expanded-aware\ngraph neural network with iterative optimization for semi-supervised entity\nalignment, named SE-GNN. First, we utilize the semantic attributes and\nstructural features of entities, combined with a conditional filtering\nmechanism, to obtain high-quality initial potential seed pairs. Next, we\ndesigned a local and global awareness mechanism. It introduces initial\npotential seed pairs and combines local and global information to obtain a more\ncomprehensive entity embedding representation, which alleviates the impact of\nKGs structural heterogeneity and lays the foundation for the optimization of\ninitial potential seed pairs. Then, we designed the threshold nearest neighbor\nembedding correction strategy. It combines the similarity threshold and the\nbidirectional nearest neighbor method as a filtering mechanism to select\niterative potential seed pairs and also uses an embedding correction strategy\nto eliminate the embedding distortion."}
{"id": "2503.20827", "pdf": "https://arxiv.org/pdf/2503.20827", "abs": "https://arxiv.org/abs/2503.20827", "authors": ["Meng Yang", "Jun Chen", "Wenping Gong", "Longsheng Wei", "Xin Tian"], "title": "Multimodal Image Matching based on Frequency-domain Information of Local Energy Response", "categories": ["cs.CV", "68U10 (Primary), 68T45, 68Wxx (Secondary)", "I.4.7; I.5.1"], "comment": "34 pages, 11 figures", "summary": "Complicated nonlinear intensity differences, nonlinear local geometric\ndistortions, noises and rotation transformation are main challenges in\nmultimodal image matching. In order to solve these problems, we propose a\nmethod based on Frequency-domain Information of Local Energy Response called\nFILER. The core of FILER is the local energy response model based on\nfrequency-domain information, which can overcome the effect of nonlinear\nintensity differences. To improve the robustness to local nonlinear geometric\ndistortions and noises, we design a new edge structure enhanced feature\ndetector and convolutional feature weighted descriptor, respectively. In\naddition, FILER overcomes the sensitivity of the frequency-domain information\nto the rotation angle and achieves rotation invariance. Extensive experiments\nmultimodal image pairs show that FILER outperforms other state-of-the-art\nalgorithms and has good robustness and universality."}
{"id": "2503.20835", "pdf": "https://arxiv.org/pdf/2503.20835", "abs": "https://arxiv.org/abs/2503.20835", "authors": ["Qichen Sun", "Yuxing Lu", "Kun Xia", "Li Chen", "He Sun", "Jinzhuo Wang"], "title": "Comprehensive Manuscript Assessment with Text Summarization Using 69707 articles", "categories": ["cs.CL"], "comment": null, "summary": "Rapid and efficient assessment of the future impact of research articles is a\nsignificant concern for both authors and reviewers. The most common standard\nfor measuring the impact of academic papers is the number of citations. In\nrecent years, numerous efforts have been undertaken to predict citation counts\nwithin various citation windows. However, most of these studies focus solely on\na specific academic field or require early citation counts for prediction,\nrendering them impractical for the early-stage evaluation of papers. In this\nwork, we harness Scopus to curate a significantly comprehensive and large-scale\ndataset of information from 69707 scientific articles sourced from 99 journals\nspanning multiple disciplines. We propose a deep learning methodology for the\nimpact-based classification tasks, which leverages semantic features extracted\nfrom the manuscripts and paper metadata. To summarize the semantic features,\nsuch as titles and abstracts, we employ a Transformer-based language model to\nencode semantic features and design a text fusion layer to capture shared\ninformation between titles and abstracts. We specifically focus on the\nfollowing impact-based prediction tasks using information of scientific\nmanuscripts in pre-publication stage: (1) The impact of journals in which the\nmanuscripts will be published. (2) The future impact of manuscripts themselves.\nExtensive experiments on our datasets demonstrate the superiority of our\nproposed model for impact-based prediction tasks. We also demonstrate\npotentials in generating manuscript's feedback and improvement suggestions."}
{"id": "2503.20830", "pdf": "https://arxiv.org/pdf/2503.20830", "abs": "https://arxiv.org/abs/2503.20830", "authors": ["Chamani Shiranthika", "Zahra Hafezi Kafshgari", "Hadi Hadizadeh", "Parvaneh Saeedi"], "title": "MedSegNet10: A Publicly Accessible Network Repository for Split Federated Medical Image Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "Machine Learning (ML) and Deep Learning (DL) have shown significant promise\nin healthcare, particularly in medical image segmentation, which is crucial for\naccurate disease diagnosis and treatment planning. Despite their potential,\nchallenges such as data privacy concerns, limited annotated data, and\ninadequate training data persist. Decentralized learning approaches such as\nfederated learning (FL), split learning (SL), and split federated learning\n(SplitFed/SFL) address these issues effectively. This paper introduces\n\"MedSegNet10,\" a publicly accessible repository designed for medical image\nsegmentation using split-federated learning. MedSegNet10 provides a collection\nof pre-trained neural network architectures optimized for various medical image\ntypes, including microscopic images of human blastocysts, dermatoscopic images\nof skin lesions, and endoscopic images of lesions, polyps, and ulcers, with\napplications extending beyond these examples. By leveraging SplitFed's\nbenefits, MedSegNet10 allows collaborative training on privately stored,\nhorizontally split data, ensuring privacy and integrity. This repository\nsupports researchers, practitioners, trainees, and data scientists, aiming to\nadvance medical image segmentation while maintaining patient data privacy. The\nrepository is available at: https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX\n(password upon request to the authors)."}
{"id": "2503.20836", "pdf": "https://arxiv.org/pdf/2503.20836", "abs": "https://arxiv.org/abs/2503.20836", "authors": ["Colin Brisson", "Ayoub Kahfy", "Marc Bui", "Frédéric Constant"], "title": "Named Entity Recognition in Context", "categories": ["cs.CL"], "comment": null, "summary": "We present the Named Entity Recognition system developed by the Edit Dunhuang\nteam for the EvaHan2025 competition. Our approach integrates three core\ncomponents: (1) Pindola, a modern transformer-based bidirectional encoder\npretrained on a large corpus of Classical Chinese texts; (2) a retrieval module\nthat fetches relevant external context for each target sequence; and (3) a\ngenerative reasoning step that summarizes retrieved context in Classical\nChinese for more robust entity disambiguation. Using this approach, we achieve\nan average F1 score of 85.58, improving upon the competition baseline by nearly\n5 points."}
{"id": "2503.20853", "pdf": "https://arxiv.org/pdf/2503.20853", "abs": "https://arxiv.org/abs/2503.20853", "authors": ["Alexander Swerdlow", "Mihir Prabhudesai", "Siddharth Gandhi", "Deepak Pathak", "Katerina Fragkiadaki"], "title": "Unified Multimodal Discrete Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Website: https://unidisc.github.io", "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io."}
{"id": "2503.20850", "pdf": "https://arxiv.org/pdf/2503.20850", "abs": "https://arxiv.org/abs/2503.20850", "authors": ["Qing Yao", "Kanishka Misra", "Leonie Weissweiler", "Kyle Mahowald"], "title": "Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) tend to show human-like preferences on a number of\nsyntactic phenomena, but the extent to which these are attributable to direct\nexposure to the phenomena or more general properties of language is unclear. We\nexplore this with the English dative alternation (DO: \"gave Y the X\" vs. PO:\n\"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively\ntrain small LMs on systematically manipulated input. We focus on properties\nthat affect the choice of alternant: length and animacy. Both properties are\ndirectly present in datives but also reflect more global tendencies for shorter\nelements to precede longer ones and animates to precede inanimates. First, by\nmanipulating and ablating datives for these biases in the input, we show that\ndirect evidence of length and animacy matters, but easy-first preferences\npersist even without such evidence. Then, using LMs trained on systematically\nperturbed datasets to manipulate global length effects (re-linearizing\nsentences globally while preserving dependency structure), we find that dative\npreferences can emerge from indirect evidence. We conclude that LMs' emergent\nsyntactic preferences come from a mix of direct and indirect sources."}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives."}
{"id": "2503.20919", "pdf": "https://arxiv.org/pdf/2503.20919", "abs": "https://arxiv.org/abs/2503.20919", "authors": ["Yupei Li", "Qiyang Sun", "Sunil Munthumoduku Krishna Murthy", "Emran Alturki", "Björn W. Schuller"], "title": "GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Affective Computing (AC) is essential for advancing Artificial General\nIntelligence (AGI), with emotion recognition serving as a key component.\nHowever, human emotions are inherently dynamic, influenced not only by an\nindividual's expressions but also by interactions with others, and\nsingle-modality approaches often fail to capture their full dynamics.\nMultimodal Emotion Recognition (MER) leverages multiple signals but\ntraditionally relies on utterance-level analysis, overlooking the dynamic\nnature of emotions in conversations. Emotion Recognition in Conversation (ERC)\naddresses this limitation, yet existing methods struggle to align multimodal\nfeatures and explain why emotions evolve within dialogues. To bridge this gap,\nwe propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly\nconsiders voice and transcripts of both the speaker and their conversational\npartner(s) to identify the most influential sentences driving emotional shifts.\nBy integrating Contrastive Language-Audio Pretraining (CLAP) for improved\ncross-modal alignment and employing a gating mechanism to emphasise emotionally\nimpactful utterances, GatedxLSTM enhances both interpretability and\nperformance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion\npredictions by modelling contextual dependencies. Experiments on the IEMOCAP\ndataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA)\nperformance among open-source methods in four-class emotion classification.\nThese results validate its effectiveness for ERC applications and provide an\ninterpretability analysis from a psychological perspective."}
{"id": "2503.20880", "pdf": "https://arxiv.org/pdf/2503.20880", "abs": "https://arxiv.org/abs/2503.20880", "authors": ["Amaya Gallagher-Syed", "Henry Senior", "Omnia Alwazzan", "Elena Pontarini", "Michele Bombardieri", "Costantino Pitzalis", "Myles J. Lewis", "Michael R. Barnes", "Luca Rossi", "Gregory Slabaugh"], "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "q-bio.TO"], "comment": "Accepted for publication at CVPR 2025", "summary": "The development of biologically interpretable and explainable models remains\na key challenge in computational pathology, particularly for multistain\nimmunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable\ngraph neural network architecture for whole slide image (WSI) classification\nthat leverages both spatial and semantic features across multiple stains. At\nits core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP)\nmodule that generates biologically meaningful, stain-aware patient embeddings.\nOur approach achieves state-of-the-art performance on both Rheumatoid Arthritis\nand Sjogren's Disease multistain datasets. Beyond performance metrics,\nBioX-CPath provides interpretable insights through stain attention scores,\nentropy measures, and stain interaction scores, that permit measuring model\nalignment with known pathological mechanisms. This biological grounding,\ncombined with strong classification performance, makes BioX-CPath particularly\nsuitable for clinical applications where interpretability is key. Source code\nand documentation can be found at: https://github.com/AmayaGS/BioX-CPath."}
{"id": "2503.20939", "pdf": "https://arxiv.org/pdf/2503.20939", "abs": "https://arxiv.org/abs/2503.20939", "authors": ["Horacio Thompson", "Maximiliano Sapino", "Edgardo Ferretti", "Marcelo Errecalde"], "title": "Hacia la interpretabilidad de la detección anticipada de riesgos de depresión utilizando grandes modelos de lenguaje", "categories": ["cs.CL"], "comment": "In Spanish language, In 30{\\deg} Congreso Argentino de Ciencias de la\n  Computaci\\'on (CACIC 2024), La Plata, Argentina", "summary": "Early Detection of Risks (EDR) on the Web involves identifying at-risk users\nas early as possible. Although Large Language Models (LLMs) have proven to\nsolve various linguistic tasks efficiently, assessing their reasoning ability\nin specific domains is crucial. In this work, we propose a method for solving\ndepression-related EDR using LLMs on Spanish texts, with responses that can be\ninterpreted by humans. We define a reasoning criterion to analyze users through\na specialist, apply in-context learning to the Gemini model, and evaluate its\nperformance both quantitatively and qualitatively. The results show that\naccurate predictions can be obtained, supported by explanatory reasoning,\nproviding a deeper understanding of the solution. Our approach offers new\nperspectives for addressing EDR problems by leveraging the power of LLMs."}
{"id": "2503.20897", "pdf": "https://arxiv.org/pdf/2503.20897", "abs": "https://arxiv.org/abs/2503.20897", "authors": ["Venuri Amarasinghe", "Asini Jayakody", "Isun Randila", "Kalinga Bandara", "Chamuditha Jayanga Galappaththige", "Ranga Rodrigo"], "title": "Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised domain generalization (SSDG) leverages a small fraction of\nlabeled data alongside unlabeled data to enhance model generalization. Most of\nthe existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data,\noften assuming access to domain labels-a privilege not always available.\nHowever, domain shifts introduce domain noise, leading to inconsistent PLs that\ndegrade model performance. Methods derived from FixMatch suffer particularly\nfrom lower PL accuracy, reducing the effectiveness of unlabeled data. To\naddress this, we tackle the more challenging domain-label agnostic SSDG, where\ndomain labels for unlabeled data are not available during training. First, we\npropose a feature modulation strategy that enhances class-discriminative\nfeatures while suppressing domain-specific information. This modulation shifts\nfeatures toward Similar Average Representations-a modified version of class\nprototypes-that are robust across domains, encouraging the classifier to\ndistinguish between closely related classes and feature extractor to form\ntightly clustered, domain-invariant representations. Second, to mitigate domain\nnoise and improve pseudo-label accuracy, we introduce a loss-scaling function\nthat dynamically lowers the fixed confidence threshold for pseudo-labels,\noptimizing the use of unlabeled data. With these key innovations, our approach\nachieves significant improvements on four major domain generalization\nbenchmarks-even without domain labels. We will make the code available."}
{"id": "2503.20953", "pdf": "https://arxiv.org/pdf/2503.20953", "abs": "https://arxiv.org/abs/2503.20953", "authors": ["Julia Ive", "Felix Jozsa", "Nick Jackson", "Paulina Bondaronek", "Ciaran Scott Hill", "Richard Dobson"], "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 0.98 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals."}
{"id": "2503.20925", "pdf": "https://arxiv.org/pdf/2503.20925", "abs": "https://arxiv.org/abs/2503.20925", "authors": ["Venkat Adithya Amula", "Sunayana Samavedam", "Saurabh Saini", "Avani Gupta", "Narayanan P J"], "title": "Prototype Guided Backdoor Defense", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}."}
{"id": "2503.20959", "pdf": "https://arxiv.org/pdf/2503.20959", "abs": "https://arxiv.org/abs/2503.20959", "authors": ["Joss Moorkens", "Andy Way", "Séamus Lankford"], "title": "Sociotechnical Effects of Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While the previous chapters have shown how machine translation (MT) can be\nuseful, in this chapter we discuss some of the side-effects and risks that are\nassociated, and how they might be mitigated. With the move to neural MT and\napproaches using Large Language Models (LLMs), there is an associated impact on\nclimate change, as the models built by multinational corporations are massive.\nThey are hugely expensive to train, consume large amounts of electricity, and\noutput huge volumes of kgCO2 to boot. However, smaller models which still\nperform to a high level of quality can be built with much lower carbon\nfootprints, and tuning pre-trained models saves on the requirement to train\nfrom scratch. We also discuss the possible detrimental effects of MT on\ntranslators and other users. The topics of copyright and ownership of data are\ndiscussed, as well as ethical considerations on data and MT use. Finally, we\nshow how if done properly, using MT in crisis scenarios can save lives, and we\nprovide a method of how this might be done."}
{"id": "2503.20936", "pdf": "https://arxiv.org/pdf/2503.20936", "abs": "https://arxiv.org/abs/2503.20936", "authors": ["Daniel Etaat", "Dvij Kalaria", "Nima Rahmanian", "Shankar Sastry"], "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy."}
{"id": "2503.20960", "pdf": "https://arxiv.org/pdf/2503.20960", "abs": "https://arxiv.org/abs/2503.20960", "authors": ["Arnav Arora", "Srishti Yadav", "Maria Antoniak", "Serge Belongie", "Isabelle Augenstein"], "title": "Multi-Modal Framing Analysis of News", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Automated frame analysis of political communication is a popular task in\ncomputational social science that is used to study how authors select aspects\nof a topic to frame its reception. So far, such studies have been narrow, in\nthat they use a fixed set of pre-defined frames and focus only on the text,\nignoring the visual contexts in which those texts appear. Especially for\nframing in the news, this leaves out valuable information about editorial\nchoices, which include not just the written article but also accompanying\nphotographs. To overcome such limitations, we present a method for conducting\nmulti-modal, multi-label framing analysis at scale using large\n(vision-)language models. Grounding our work in framing theory, we extract\nlatent meaning embedded in images used to convey a certain point and contrast\nthat to the text by comparing the respective frames used. We also identify\nhighly partisan framing of topics with issue-specific frame analysis found in\nprior qualitative work. We demonstrate a method for doing scalable integrative\nframing analysis of both text and image in news, providing a more complete\npicture for understanding media bias."}
{"id": "2503.20967", "pdf": "https://arxiv.org/pdf/2503.20967", "abs": "https://arxiv.org/abs/2503.20967", "authors": ["David Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Akshay Chaudhari", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Lopes Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H. Miller", "Amir Borhani", "Hatice Savas", "Eric Hart", "Drew Torigian", "Jayaram K. Udupa", "Elizabeth Krupinski", "Ulas Bagci"], "title": "Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "The demand for high-quality synthetic data for model training and\naugmentation has never been greater in medical imaging. However, current\nevaluations predominantly rely on computational metrics that fail to align with\nhuman expert recognition. This leads to synthetic images that may appear\nrealistic numerically but lack clinical authenticity, posing significant\nchallenges in ensuring the reliability and effectiveness of AI-driven medical\ntools. To address this gap, we introduce GazeVal, a practical framework that\nsynergizes expert eye-tracking data with direct radiological evaluations to\nassess the quality of synthetic medical images. GazeVal leverages gaze patterns\nof radiologists as they provide a deeper understanding of how experts perceive\nand interact with synthetic data in different tasks (i.e., diagnostic or Turing\ntests). Experiments with sixteen radiologists revealed that 96.6% of the\ngenerated images (by the most recent state-of-the-art AI algorithm) were\nidentified as fake, demonstrating the limitations of generative AI in producing\nclinically accurate images."}
{"id": "2503.20978", "pdf": "https://arxiv.org/pdf/2503.20978", "abs": "https://arxiv.org/abs/2503.20978", "authors": ["Yiqiao Jin", "Stefano Petrangeli", "Yu Shen", "Gang Wu"], "title": "ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction", "categories": ["cs.CL"], "comment": "Accepted to MM4SG Workshop at The Web Conference 2025", "summary": "Graphical User Interface (GUI) agents are autonomous systems that interpret\nand generate actions, enabling intelligent user assistance and automation.\nEffective training of these agent presents unique challenges, such as sparsity\nin supervision signals, scalability for large datasets, and the need for\nnuanced user understanding. We propose stateful screen schema, an efficient\nrepresentation of GUI interactions that captures key user actions and\nintentions over time. Building on this foundation, we introduce ScreenLLM, a\nset of multimodal large language models (MLLMs) tailored for advanced UI\nunderstanding and action prediction. Extensive experiments on both open-source\nand proprietary models show that ScreenLLM accurately models user behavior and\npredicts actions. Our work lays the foundation for scalable, robust, and\nintelligent GUI agents that enhance user interaction in diverse software\nenvironments."}
{"id": "2503.20991", "pdf": "https://arxiv.org/pdf/2503.20991", "abs": "https://arxiv.org/abs/2503.20991", "authors": ["Tai D. Nguyen", "Matthew C. Stamm"], "title": "MVFNet: Multipurpose Video Forensics Network using Multiple Forms of Forensic Evidence", "categories": ["cs.CV"], "comment": "Proceedings of the Winter Conference on Applications of Computer\n  Vision (WACV) 2025", "summary": "While videos can be falsified in many different ways, most existing forensic\nnetworks are specialized to detect only a single manipulation type (e.g.\ndeepfake, inpainting). This poses a significant issue as the manipulation used\nto falsify a video is not known a priori. To address this problem, we propose\nMVFNet - a multipurpose video forensics network capable of detecting multiple\ntypes of manipulations including inpainting, deepfakes, splicing, and editing.\nOur network does this by extracting and jointly analyzing a broad set of\nforensic feature modalities that capture both spatial and temporal anomalies in\nfalsified videos. To reliably detect and localize fake content of all shapes\nand sizes, our network employs a novel Multi-Scale Hierarchical Transformer\nmodule to identify forensic inconsistencies across multiple spatial scales.\nExperimental results show that our network obtains state-of-the-art performance\nin general scenarios where multiple different manipulations are possible, and\nrivals specialized detectors in targeted scenarios."}
{"id": "2503.20981", "pdf": "https://arxiv.org/pdf/2503.20981", "abs": "https://arxiv.org/abs/2503.20981", "authors": ["Xiaoran Xu", "Zhaoqian Xue", "Chi Zhang", "Jhonatan Medri", "Junjie Xiong", "Jiayan Zhou", "Jin Jin", "Yongfeng Zhang", "Siyuan Ma", "Lingyao Li"], "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare."}
{"id": "2503.21003", "pdf": "https://arxiv.org/pdf/2503.21003", "abs": "https://arxiv.org/abs/2503.21003", "authors": ["Tai D. Nguyen", "Aref Azizpour", "Matthew C. Stamm"], "title": "Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images", "categories": ["cs.CV"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "The emergence of advanced AI-based tools to generate realistic images poses\nsignificant challenges for forensic detection and source attribution,\nespecially as new generative techniques appear rapidly. Traditional methods\noften fail to generalize to unseen generators due to reliance on features\nspecific to known sources during training. To address this problem, we propose\na novel approach that explicitly models forensic microstructures - subtle,\npixel-level patterns unique to the image creation process. Using only real\nimages in a self-supervised manner, we learn a set of diverse predictive\nfilters to extract residuals that capture different aspects of these\nmicrostructures. By jointly modeling these residuals across multiple scales, we\nobtain a compact model whose parameters constitute a unique forensic\nself-description for each image. This self-description enables us to perform\nzero-shot detection of synthetic images, open-set source attribution of images,\nand clustering based on source without prior knowledge. Extensive experiments\ndemonstrate that our method achieves superior accuracy and adaptability\ncompared to competing techniques, advancing the state of the art in synthetic\nmedia forensics."}
{"id": "2503.20988", "pdf": "https://arxiv.org/pdf/2503.20988", "abs": "https://arxiv.org/abs/2503.20988", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "categories": ["cs.CL", "cs.GR"], "comment": null, "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component."}
{"id": "2503.21022", "pdf": "https://arxiv.org/pdf/2503.21022", "abs": "https://arxiv.org/abs/2503.21022", "authors": ["W. Riley Casper", "Bobby Orozco"], "title": "Reconstructing Gridded Data from Higher Autocorrelations", "categories": ["cs.CV", "math.GR", "physics.data-an", "20K01, 68T45, 68T10"], "comment": "13 pages, 1 figure", "summary": "The higher-order autocorrelations of integer-valued or rational-valued\ngridded data sets appear naturally in X-ray crystallography, and have\napplications in computer vision systems, correlation tomography, correlation\nspectroscopy, and pattern recognition. In this paper, we consider the problem\nof reconstructing a gridded data set from its higher-order autocorrelations. We\ndescribe an explicit reconstruction algorithm, and prove that the\nautocorrelations up to order 3r + 3 are always sufficient to determine the data\nup to translation, where r is the dimension of the grid. We also provide\nexamples of rational-valued gridded data sets which are not determined by their\nautocorrelations up to order 3r + 2."}
{"id": "2503.20995", "pdf": "https://arxiv.org/pdf/2503.20995", "abs": "https://arxiv.org/abs/2503.20995", "authors": ["Xiaomin Li", "Xupeng Chen", "Jingxuan Fan", "Eric Hanchen Jiang", "Mingye Gao"], "title": "Multi-head Reward Aggregation Guided by Entropy", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with safety guidelines typically\ninvolves reinforcement learning from human feedback (RLHF), relying on\nhuman-generated preference annotations. However, assigning consistent overall\nquality ratings is challenging, prompting recent research to shift towards\ndetailed evaluations based on multiple specific safety criteria. This paper\nuncovers a consistent observation: safety rules characterized by high rating\nentropy are generally less reliable in identifying responses preferred by\nhumans. Leveraging this finding, we introduce ENCORE, a straightforward\nentropy-guided approach that composes multi-head rewards by downweighting rules\nexhibiting high rating entropy. Theoretically, we demonstrate that rules with\nelevated entropy naturally receive minimal weighting in the Bradley-Terry\noptimization framework, justifying our entropy-based penalization. Through\nextensive experiments on RewardBench safety tasks, our method significantly\nsurpasses several competitive baselines, including random weighting, uniform\nweighting, single-head Bradley-Terry models, and LLM-based judging methods. Our\nproposed approach is training-free, broadly applicable to various datasets, and\nmaintains interpretability, offering a practical and effective solution for\nmulti-attribute reward modeling."}
{"id": "2503.21055", "pdf": "https://arxiv.org/pdf/2503.21055", "abs": "https://arxiv.org/abs/2503.21055", "authors": ["Chi-Hsi Kung", "Frangil Ramirez", "Juhyung Ha", "Yi-Ting Chen", "David Crandall", "Yi-Hsuan Tsai"], "title": "What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning", "categories": ["cs.CV"], "comment": "16 pages, 4 figures", "summary": "Understanding a procedural activity requires modeling both how action steps\ntransform the scene, and how evolving scene transformations can influence the\nsequence of action steps, even those that are accidental or erroneous. Existing\nwork has studied procedure-aware video representations by proposing novel\napproaches such as modeling the temporal order of actions and has not\nexplicitly learned the state changes (scene transformations). In this work, we\nstudy procedure-aware video representation learning by incorporating\nstate-change descriptions generated by Large Language Models (LLMs) as\nsupervision signals for video encoders. Moreover, we generate state-change\ncounterfactuals that simulate hypothesized failure outcomes, allowing models to\nlearn by imagining the unseen ``What if'' scenarios. This counterfactual\nreasoning facilitates the model's ability to understand the cause and effect of\neach step in an activity. To verify the procedure awareness of our model, we\nconduct extensive experiments on procedure-aware tasks, including temporal\naction segmentation and error detection. Our results demonstrate the\neffectiveness of the proposed state-change descriptions and their\ncounterfactuals and achieve significant improvements on multiple tasks. We will\nmake our source code and data publicly available soon."}
{"id": "2503.21004", "pdf": "https://arxiv.org/pdf/2503.21004", "abs": "https://arxiv.org/abs/2503.21004", "authors": ["Mahmoud Alwakeel", "Emory Buck", "Jonathan G. Martin", "Imran Aslam", "Sudarshan Rajagopal", "Jian Pei", "Mihai V. Podgoreanu", "Christopher J. Lindsell", "An-Kwok Ian Wong"], "title": "Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters", "categories": ["cs.CL"], "comment": null, "summary": "Pulmonary embolism (PE) is a leading cause of cardiovascular mortality, yet\nour understanding of optimal management remains limited due to heterogeneous\nand inaccessible radiology documentation. The PERT Consortium registry\nstandardizes PE management data but depends on resource-intensive manual\nabstraction. Large language models (LLMs) offer a scalable alternative for\nautomating concept extraction from computed tomography PE (CTPE) reports. This\nstudy evaluated the accuracy of LLMs in extracting PE-related concepts compared\nto a human-curated criterion standard. We retrospectively analyzed MIMIC-IV and\nDuke Health CTPE reports using multiple LLaMA models. Larger models (70B)\noutperformed smaller ones (8B), achieving kappa values of 0.98 (PE detection),\n0.65-0.75 (PE location), 0.48-0.51 (right heart strain), and 0.65-0.70 (image\nartifacts). Moderate temperature tuning (0.2-0.5) improved accuracy, while\nexcessive in-context examples reduced performance. A dual-model review\nframework achieved >80-90% precision. LLMs demonstrate strong potential for\nautomating PE registry abstraction, minimizing manual workload while preserving\naccuracy."}
{"id": "2503.21056", "pdf": "https://arxiv.org/pdf/2503.21056", "abs": "https://arxiv.org/abs/2503.21056", "authors": ["Yiqing Shen", "Bohan Liu", "Chenjia Li", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reasoning segmentation (RS) aims to identify and segment objects of interest\nbased on implicit text queries. As such, RS is a catalyst for embodied AI\nagents, enabling them to interpret high-level commands without requiring\nexplicit step-by-step guidance. However, current RS approaches rely heavily on\nthe visual perception capabilities of multimodal large language models (LLMs),\nleading to several major limitations. First, they struggle with queries that\nrequire multiple steps of reasoning or those that involve complex\nspatial/temporal relationships. Second, they necessitate LLM fine-tuning, which\nmay require frequent updates to maintain compatibility with contemporary LLMs\nand may increase risks of catastrophic forgetting during fine-tuning. Finally,\nbeing primarily designed for static images or offline video processing, they\nscale poorly to online video data. To address these limitations, we propose an\nagent framework that disentangles perception and reasoning for online video RS\nwithout LLM fine-tuning. Our innovation is the introduction of a just-in-time\ndigital twin concept, where -- given an implicit query -- a LLM plans the\nconstruction of a low-level scene representation from high-level video using\nspecialist vision models. We refer to this approach to creating a digital twin\nas \"just-in-time\" because the LLM planner will anticipate the need for specific\ninformation and only request this limited subset instead of always evaluating\nevery specialist model. The LLM then performs reasoning on this digital twin\nrepresentation to identify target objects. To evaluate our approach, we\nintroduce a new comprehensive video reasoning segmentation benchmark comprising\n200 videos with 895 implicit text queries. The benchmark spans three reasoning\ncategories (semantic, spatial, and temporal) with three different reasoning\nchain complexity."}
{"id": "2503.21011", "pdf": "https://arxiv.org/pdf/2503.21011", "abs": "https://arxiv.org/abs/2503.21011", "authors": ["Ana Ma", "Derek Powell"], "title": "Can Large Language Models Predict Associations Among Human Attitudes?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems."}
{"id": "2503.21061", "pdf": "https://arxiv.org/pdf/2503.21061", "abs": "https://arxiv.org/abs/2503.21061", "authors": ["Mehraveh Javan Roshtkhari", "Matthew Toews", "Marco Pedersoli"], "title": "Neural Architecture Search by Learning a Hierarchical Search Space", "categories": ["cs.CV"], "comment": null, "summary": "Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable\nsearch related problems such as adversarial games. However, the performance of\nsuch approach highly depends on the order of the nodes that are considered at\neach branching of the tree. If the first branches cannot distinguish between\npromising and deceiving configurations for the final task, the efficiency of\nthe search is exponentially reduced. In Neural Architecture Search (NAS), as\nonly the final architecture matters, the visiting order of the branching can be\noptimized to improve learning. In this paper, we study the application of MCTS\nto NAS for image classification. We analyze several sampling methods and\nbranching alternatives for MCTS and propose to learn the branching by\nhierarchical clustering of architectures based on their similarity. The\nsimilarity is measured by the pairwise distance of output vectors of\narchitectures. Extensive experiments on two challenging benchmarks on CIFAR10\nand ImageNet show that MCTS, if provided with a good branching hierarchy, can\nyield promising solutions more efficiently than other approaches for NAS\nproblems."}
{"id": "2503.21029", "pdf": "https://arxiv.org/pdf/2503.21029", "abs": "https://arxiv.org/abs/2503.21029", "authors": ["Jungyeul Park", "Yige Chen", "Kyuwon Kim", "KyungTae Lim", "Chulwoo Park"], "title": "Enhancing Korean Dependency Parsing with Morphosyntactic Features", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces UniDive for Korean, an integrated framework that\nbridges Universal Dependencies (UD) and Universal Morphology (UniMorph) to\nenhance the representation and processing of Korean {morphosyntax}. Korean's\nrich inflectional morphology and flexible word order pose challenges for\nexisting frameworks, which often treat morphology and syntax separately,\nleading to inconsistencies in linguistic analysis. UniDive unifies syntactic\nand morphological annotations by preserving syntactic dependencies while\nincorporating UniMorph-derived features, improving consistency in annotation.\nWe construct an integrated dataset and apply it to dependency parsing,\ndemonstrating that enriched morphosyntactic features enhance parsing accuracy,\nparticularly in distinguishing grammatical relations influenced by morphology.\nOur experiments, conducted with both encoder-only and decoder-only models,\nconfirm that explicit morphological information contributes to more accurate\nsyntactic analysis."}
{"id": "2503.21069", "pdf": "https://arxiv.org/pdf/2503.21069", "abs": "https://arxiv.org/abs/2503.21069", "authors": ["Fan Qi", "Yu Duan", "Changsheng Xu"], "title": "Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-guided diffusion models have revolutionized\nconditional image generation, yet they struggle to synthesize complex scenes\nwith multiple objects due to imprecise spatial grounding and limited\nscalability. We address these challenges through two key modules: 1)\nJanus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridges\ntext understanding and layout generation via a compact 1B-parameter\narchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integrating\nLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRA\nis capable of preserving the base model's parameters and ensuring plug-and-play\nadaptability, minimizing architectural intrusion while enabling efficient\nfine-tuning. To support a comprehensive evaluation, we create DescripBox and\nDescripBox-1024, benchmarks that span diverse scenes and resolutions. The\nproposed method achieves state-of-the-art performance on COCO and LVIS\nbenchmarks while maintaining parameter efficiency, demonstrating superior\nlayout fidelity and scalability for open-world synthesis."}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073", "abs": "https://arxiv.org/abs/2503.21073", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Viégas", "Martin Wattenberg"], "title": "Shared Global and Local Geometry of Language Model Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn this work, we find that the token embeddings of language models exhibit\ncommon geometric structure. First, we find ``global'' similarities: token\nembeddings often share similar relative orientations. Next, we characterize\nlocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by\ndefining a simple measure for the intrinsic dimension of each token embedding.\nOur intrinsic dimension measure demonstrates that token embeddings lie on a\nlower dimensional manifold. We qualitatively show that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Both characterizations allow us to\nfind similarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we empirically demonstrate that steering vectors from\none language model can be transferred to another, despite the two models having\ndifferent dimensions."}
{"id": "2503.21072", "pdf": "https://arxiv.org/pdf/2503.21072", "abs": "https://arxiv.org/abs/2503.21072", "authors": ["Judy X Yang", "Jing Wang", "Zhuanfeng", "Li", "Chenhong Sui Zekun Long", "Jun Zhou"], "title": "HSLiNets: Evaluating Band Ordering Strategies in Hyperspectral and LiDAR Fusion", "categories": ["cs.CV"], "comment": "2 figures, 5 pages", "summary": "The integration of hyperspectral imaging (HSI) and Light Detection and\nRanging (LiDAR) data provides complementary spectral and spatial information\nfor remote sensing applications. While previous studies have explored the role\nof band selection and grouping in HSI classification, little attention has been\ngiven to how the spectral sequence or band order affects classification\noutcomes when fused with LiDAR. In this work, we systematically investigate the\ninfluence of band order on HSI-LiDAR fusion performance. Through extensive\nexperiments, we demonstrate that band order significantly impacts\nclassification accuracy, revealing a previously overlooked factor in\nfusion-based models. Motivated by this observation, we propose a novel fusion\narchitecture that not only integrates HSI and LiDAR data but also learns from\nmultiple band order configurations. The proposed method enhances feature\nrepresentation by adaptively fusing different spectral sequences, leading to\nimproved classification accuracy. Experimental results on the Houston 2013 and\nTrento datasets show that our approach outperforms state-of-the-art fusion\nmodels. Data and code are available at https://github.com/Judyxyang/HSLiNets."}
{"id": "2503.21080", "pdf": "https://arxiv.org/pdf/2503.21080", "abs": "https://arxiv.org/abs/2503.21080", "authors": ["Yuhan Liu", "Yunbo Long"], "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."}
{"id": "2503.21074", "pdf": "https://arxiv.org/pdf/2503.21074", "abs": "https://arxiv.org/abs/2503.21074", "authors": ["Ooha Lakkadi Reddy"], "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21\n  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this\n  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at\n  Duke Kunshan University; accepted for presentation at the 2025 Computer\n  Applications and Quantitative Methods in Archaeology Conference, Athens", "summary": "This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized."}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25."}
{"id": "2503.21076", "pdf": "https://arxiv.org/pdf/2503.21076", "abs": "https://arxiv.org/abs/2503.21076", "authors": ["Yusong Hu", "Zichen Liang", "Fei Yang", "Qibin Hou", "Xialei Liu", "Ming-Ming Cheng"], "title": "KAC: Kolmogorov-Arnold Classifier for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025", "summary": "Continual learning requires models to train continuously across consecutive\ntasks without forgetting. Most existing methods utilize linear classifiers,\nwhich struggle to maintain a stable classification space while learning new\ntasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in\npreserving learning stability during simple continual regression tasks, we set\nout to explore their potential in more complex continual learning scenarios. In\nthis paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel\nclassifier developed for continual learning based on the KAN structure. We\ndelve into the impact of KAN's spline functions and introduce Radial Basis\nFunctions (RBF) for improved compatibility with continual learning. We replace\nlinear classifiers with KAC in several recent approaches and conduct\nexperiments across various continual learning benchmarks, all of which\ndemonstrate performance improvements, highlighting the effectiveness and\nrobustness of KAC in continual learning. The code is available at\nhttps://github.com/Ethanhuhuhu/KAC."}
{"id": "2503.21106", "pdf": "https://arxiv.org/pdf/2503.21106", "abs": "https://arxiv.org/abs/2503.21106", "authors": ["Gus G. Xia"], "title": "Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations", "categories": ["cs.CL", "68T27, 91E45", "I.2.0; I.2.4; F.4.1"], "comment": "12 pages, 2 figures. Part I of a multi-part position paper on a new\n  theory of mind", "summary": "This paper introduces function alignment, a novel theory of mind and\nintelligence that is both intuitively compelling and structurally grounded. It\nexplicitly models how meaning, interpretation, and analogy emerge from\ninteractions among layered representations, forming a coherent framework\ncapable not only of modeling minds but also of serving as a blueprint for\nbuilding them. One of the key theoretical insights derived from function\nalignment is bounded interpretability, which provides a unified explanation for\npreviously fragmented ideas in cognitive science, such as bounded rationality,\nsymbol grounding, and analogy-making. Beyond modeling, the function alignment\nframework bridges disciplines often kept apart, linking computational\narchitecture, psychological theory, and even contemplative traditions such as\nZen. Rather than building on any philosophical systems, it offers a structural\nfoundation upon which multiple ways of understanding the mind may be\nreconstructed."}
{"id": "2503.21082", "pdf": "https://arxiv.org/pdf/2503.21082", "abs": "https://arxiv.org/abs/2503.21082", "authors": ["Jinjie Mai", "Wenxuan Zhu", "Haozhe Liu", "Bing Li", "Cheng Zheng", "Jürgen Schmidhuber", "Bernard Ghanem"], "title": "Can Video Diffusion Model Reconstruct 4D Geometry?", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is\nan important yet challenging problem. Conventional multiview geometry-based\napproaches often struggle with dynamic motion, whereas recent learning-based\nmethods either require specialized 4D representation or sophisticated\noptimization. In this paper, we present Sora3R, a novel framework that taps\ninto the rich spatiotemporal priors of large-scale video diffusion models to\ndirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stage\npipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring\ncompatibility between the geometry and video latent spaces; (2) we finetune a\ndiffusion backbone in combined video and pointmap latent space to generate\ncoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward\nmanner, requiring no external modules (e.g., depth, optical flow, or\nsegmentation) or iterative global alignment. Extensive experiments demonstrate\nthat Sora3R reliably recovers both camera poses and detailed scene geometry,\nachieving performance on par with state-of-the-art methods for dynamic 4D\nreconstruction across diverse scenarios."}
{"id": "2503.21115", "pdf": "https://arxiv.org/pdf/2503.21115", "abs": "https://arxiv.org/abs/2503.21115", "authors": ["Yinzhu Quan", "Yujia Xu", "Guanlin Chen", "Frederick Benaben", "Benoit Montreuil"], "title": "Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment", "categories": ["cs.CL"], "comment": null, "summary": "The growing emphasis on energy efficiency and environmental sustainability in\nglobal supply chains introduces new challenges in the deployment of\nhyperconnected logistic hub networks. In current volatile, uncertain, complex,\nand ambiguous (VUCA) environments, dynamic risk assessment becomes essential to\nensure successful hub deployment. However, traditional methods often struggle\nto effectively capture and analyze unstructured information. In this paper, we\ndesign an Large Language Model (LLM)-driven risk assessment pipeline integrated\nwith multiple analytical tools to evaluate logistic hub deployment. This\nframework enables LLMs to systematically identify potential risks by analyzing\nunstructured data, such as geopolitical instability, financial trends,\nhistorical storm events, traffic conditions, and emerging risks from news\nsources. These data are processed through a suite of analytical tools, which\nare automatically called by LLMs to support a structured and data-driven\ndecision-making process for logistic hub selection. In addition, we design\nprompts that instruct LLMs to leverage these tools for assessing the\nfeasibility of hub selection by evaluating various risk types and levels.\nThrough risk-based similarity analysis, LLMs cluster logistic hubs with\ncomparable risk profiles, enabling a structured approach to risk assessment. In\nconclusion, the framework incorporates scalability with long-term memory and\nenhances decision-making through explanation and interpretation, enabling\ncomprehensive risk assessments for logistic hub deployment in hyperconnected\nsupply chain networks."}
{"id": "2503.21099", "pdf": "https://arxiv.org/pdf/2503.21099", "abs": "https://arxiv.org/abs/2503.21099", "authors": ["Yun Zhu", "Le Hui", "Hang Yang", "Jianjun Qian", "Jin Xie", "Jian Yang"], "title": "Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Both indoor and outdoor scene perceptions are essential for embodied\nintelligence. However, current sparse supervised 3D object detection methods\nfocus solely on outdoor scenes without considering indoor settings. To this\nend, we propose a unified sparse supervised 3D object detection method for both\nindoor and outdoor scenes through learning class prototypes to effectively\nutilize unlabeled objects. Specifically, we first propose a prototype-based\nobject mining module that converts the unlabeled object mining into a matching\nproblem between class prototypes and unlabeled features. By using optimal\ntransport matching results, we assign prototype labels to high-confidence\nfeatures, thereby achieving the mining of unlabeled objects. We then present a\nmulti-label cooperative refinement module to effectively recover missed\ndetections through pseudo label quality control and prototype label\ncooperation. Experiments show that our method achieves state-of-the-art\nperformance under the one object per scene sparse supervised setting across\nindoor and outdoor datasets. With only one labeled object per scene, our method\nachieves about 78%, 90%, and 96% performance compared to the fully supervised\ndetector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the\nscalability of our method. Code is available at\nhttps://github.com/zyrant/CPDet3D."}
{"id": "2503.21127", "pdf": "https://arxiv.org/pdf/2503.21127", "abs": "https://arxiv.org/abs/2503.21127", "authors": ["Ziyi Zhou", "Xiaoming Zhang", "Shenghan Tan", "Litian Zhang", "Chaozhuo Li"], "title": "Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The proliferation of fake news on social media platforms has exerted a\nsubstantial influence on society, leading to discernible impacts and\ndeleterious consequences. Conventional deep learning methodologies employing\nsmall language models (SLMs) suffer from the necessity for extensive supervised\ntraining and the challenge of adapting to rapidly evolving circumstances. Large\nlanguage models (LLMs), despite their robust zero-shot capabilities, have\nfallen short in effectively identifying fake news due to a lack of pertinent\ndemonstrations and the dynamic nature of knowledge. In this paper, a novel\nframework Multi-Round Collaboration Detection (MRCD) is proposed to address\nthese aforementioned limitations. The MRCD framework is capable of enjoying the\nmerits from both LLMs and SLMs by integrating their generalization abilities\nand specialized functionalities, respectively. Our approach features a\ntwo-stage retrieval module that selects relevant and up-to-date demonstrations\nand knowledge, enhancing in-context learning for better detection of emerging\nnews events. We further design a multi-round learning framework to ensure more\nreliable detection results. Our framework MRCD achieves SOTA results on two\nreal-world datasets Pheme and Twitter16, with accuracy improvements of 7.4\\%\nand 12.8\\% compared to using only SLMs, which effectively addresses the\nlimitations of current models and improves the detection of emergent fake news."}
{"id": "2503.21104", "pdf": "https://arxiv.org/pdf/2503.21104", "abs": "https://arxiv.org/abs/2503.21104", "authors": ["Yuyin Chen", "Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Yifei Zhan", "Xianpeng Lang"], "title": "StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Urban scene reconstruction requires modeling both static infrastructure and\ndynamic elements while supporting diverse environmental conditions. We present\n\\textbf{StyledStreets}, a multi-style street simulator that achieves\ninstruction-driven scene editing with guaranteed spatial and temporal\nconsistency. Building on a state-of-the-art Gaussian Splatting framework for\nstreet scenarios enhanced by our proposed pose optimization and multi-view\ntraining, our method enables photorealistic style transfers across seasons,\nweather conditions, and camera setups through three key innovations: First, a\nhybrid embedding scheme disentangles persistent scene geometry from transient\nstyle attributes, allowing realistic environmental edits while preserving\nstructural integrity. Second, uncertainty-aware rendering mitigates supervision\nnoise from diffusion priors, enabling robust training across extreme style\nvariations. Third, a unified parametric model prevents geometric drift through\nregularized updates, maintaining multi-view consistency across seven\nvehicle-mounted cameras.\n  Our framework preserves the original scene's motion patterns and geometric\nrelationships. Qualitative results demonstrate plausible transitions between\ndiverse conditions (snow, sandstorm, night), while quantitative evaluations\nshow state-of-the-art geometric accuracy under style transfers. The approach\nestablishes new capabilities for urban simulation, with applications in\nautonomous vehicle testing and augmented reality systems requiring reliable\nenvironmental consistency. Codes will be publicly available upon publication."}
{"id": "2503.21193", "pdf": "https://arxiv.org/pdf/2503.21193", "abs": "https://arxiv.org/abs/2503.21193", "authors": ["Hongxuan Tang", "Hao Liu", "Xinyan Xiao"], "title": "UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We introduce UGen, a unified autoregressive multimodal model that\ndemonstrates strong performance across text processing, image understanding,\nand image generation tasks simultaneously. UGen converts both texts and images\ninto discrete token sequences and utilizes a single transformer to generate\nthem uniformly in an autoregressive manner. To address the challenges\nassociated with unified multimodal learning, UGen is trained using a novel\nmechanism, namely progressive vocabulary learning. In this process, visual\ntoken IDs are incrementally activated and integrated into the training phase,\nultimately enhancing the effectiveness of unified multimodal learning.\nExperiments on comprehensive text and image tasks show that UGen achieves a\nsignificant overall performance improvement of 13.3% compared to the vanilla\nunified autoregressive method, and it also delivers competitive results across\nall tasks against several task-specific models."}
{"id": "2503.21122", "pdf": "https://arxiv.org/pdf/2503.21122", "abs": "https://arxiv.org/abs/2503.21122", "authors": ["Teng Huang", "Han Ding", "Wenxin Sun", "Cui Zhao", "Ge Wang", "Fei Wang", "Kun Zhao", "Zhi Wang", "Wei Xi"], "title": "One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation", "categories": ["cs.CV"], "comment": "IEEE INFOCOM 2025", "summary": "Wireless sensing systems, particularly those using mmWave technology, offer\ndistinct advantages over traditional vision-based approaches, such as enhanced\nprivacy and effectiveness in poor lighting conditions. These systems,\nleveraging FMCW signals, have shown success in human-centric applications like\nlocalization, gesture recognition, and so on. However, comprehensive mmWave\ndatasets for diverse applications are scarce, often constrained by\npre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistent\nannotation formats. To overcome these limitations, we propose mmGen, a novel\nand generalized framework tailored for full-scene mmWave signal generation. By\nconstructing physical signal transmission models, mmGen synthesizes\nhuman-reflected and environment-reflected mmWave signals from the constructed\n3D meshes. Additionally, we incorporate methods to account for material\nproperties, antenna gains, and multipath reflections, enhancing the realism of\nthe synthesized signals. We conduct extensive experiments using a prototype\nsystem with commercial mmWave devices and Kinect sensors. The results show that\nthe average similarity of Range-Angle and micro-Doppler signatures between the\nsynthesized and real-captured signals across three different environments\nexceeds 0.91 and 0.89, respectively, demonstrating the effectiveness and\npractical applicability of mmGen."}
{"id": "2503.21227", "pdf": "https://arxiv.org/pdf/2503.21227", "abs": "https://arxiv.org/abs/2503.21227", "authors": ["Hengyuan Zhao", "Ziqin Wang", "Qixin Sun", "Kaiyou Song", "Yilin Li", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Although applying Mixture of Experts to large language models for learning\nnew tasks is widely regarded as an effective strategy for continuous learning,\nthere still remain two major challenges: (1) As the number of tasks grows,\nsimple parameter expansion strategies can lead to excessively large models. (2)\nModifying the parameters of the existing router results in the erosion of\npreviously acquired knowledge. In this paper, we present an innovative\nframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)\narchitecture without any replay data. Specifically, we have developed a method\ncalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts to\nassess whether additional knowledge is required for a specific layer. This\napproach enables the model to adaptively expand its network parameters based on\ntask distribution, thereby significantly improving the efficiency of parameter\nexpansion. Additionally, we introduce a hierarchical routing algorithm called\nProbabilistic Task Locator (PTL), where high-level routing captures inter-task\ninformation and low-level routing focuses on intra-task details, ensuring that\nnew task experts do not interfere with existing ones. Our experiments shows\nthat our efficient architecture has substantially improved model performance on\nthe Coin benchmark while maintaining a reasonable parameter count."}
{"id": "2503.21124", "pdf": "https://arxiv.org/pdf/2503.21124", "abs": "https://arxiv.org/abs/2503.21124", "authors": ["Shuaiyu Zhang", "Xun Lin", "Rongxiang Zhang", "Yu Bai", "Yong Xu", "Tao Tan", "Xunbin Zheng", "Zitong Yu"], "title": "AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "The integration of pathologic images and genomic data for survival analysis\nhas gained increasing attention with advances in multimodal learning. However,\ncurrent methods often ignore biological characteristics, such as heterogeneity\nand sparsity, both within and across modalities, ultimately limiting their\nadaptability to clinical practice. To address these challenges, we propose\nAdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for\nefficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is\nspecifically adapted to the uniqueness of medical data, enabling accurate\npredictions with minimal resource consumption, even under challenging scenarios\nwith missing modalities. Initially, AdaMHF employs an experts expansion and\nresidual structure to activate specialized experts for extracting heterogeneous\nand sparse features. Extracted tokens undergo refinement via selection and\naggregation, reducing the weight of non-dominant features while preserving\ncomprehensive information. Subsequently, the encoded features are\nhierarchically fused, allowing multi-grained interactions across modalities to\nbe captured. Furthermore, we introduce a survival prediction benchmark designed\nto resolve scenarios with missing modalities, mirroring real-world clinical\nconditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF\nsurpasses current state-of-the-art (SOTA) methods, showcasing exceptional\nperformance in both complete and incomplete modality settings."}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248", "abs": "https://arxiv.org/abs/2503.21248", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention."}
{"id": "2503.21125", "pdf": "https://arxiv.org/pdf/2503.21125", "abs": "https://arxiv.org/abs/2503.21125", "authors": ["Jiajie Quan", "Ao Tong", "Yuxuan Cai", "Xinwei He", "Yulong Wang", "Yang Zhou"], "title": "Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "In multi-class unsupervised anomaly detection(MUAD), reconstruction-based\nmethods learn to map input images to normal patterns to identify anomalous\npixels. However, this strategy easily falls into the well-known \"learning\nshortcut\" issue when decoders fail to capture normal patterns and reconstruct\nboth normal and abnormal samples naively. To address that, we propose to learn\nthe input features in global and local manners, forcing the network to memorize\nthe normal patterns more comprehensively. Specifically, we design a two-branch\ndecoder block, named Omni-block. One branch corresponds to global feature\nlearning, where we serialize two self-attention blocks but replace the query\nand (key, value) with learnable tokens, respectively, thus capturing global\nfeatures of normal patterns concisely and thoroughly. The local branch\ncomprises depth-separable convolutions, whose locality enables effective and\nefficient learning of local features for normal patterns. By stacking\nOmni-blocks, we build a framework, Omni-AD, to learn normal patterns of\ndifferent granularity and reconstruct them progressively. Comprehensive\nexperiments on public anomaly detection benchmarks show that our method\noutperforms state-of-the-art approaches in MUAD. Code is available at\nhttps://github.com/easyoo/Omni-AD.git."}
{"id": "2503.21263", "pdf": "https://arxiv.org/pdf/2503.21263", "abs": "https://arxiv.org/abs/2503.21263", "authors": ["Wenxuan Lu", "Jiangyang He", "Zhanqiu Zhang", "Yiwen Guo", "Tianning Zang"], "title": "Cultivating Game Sense for Yourself: Making VLMs Gaming Experts", "categories": ["cs.CL"], "comment": null, "summary": "Developing agents capable of fluid gameplay in first/third-person games\nwithout API access remains a critical challenge in Artificial General\nIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as\ndirect controllers, frequently pausing the game to analyze screens and plan\naction through language reasoning. However, this inefficient paradigm\nfundamentally restricts agents to basic and non-fluent interactions: relying on\nisolated VLM reasoning for each action makes it impossible to handle tasks\nrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,\nACT combat). To handle this, we propose a paradigm shift in gameplay agent\ndesign: instead of directly controlling gameplay, VLM develops specialized\nexecution modules tailored for tasks like shooting and combat. These modules\nhandle real-time game interactions, elevating VLM to a high-level developer.\nBuilding upon this paradigm, we introduce GameSense, a gameplay agent framework\nwhere VLM develops task-specific game sense modules by observing task execution\nand leveraging vision tools and neural network training pipelines. These\nmodules encapsulate action-feedback logic, ranging from direct action rules to\nneural network-based decisions. Experiments demonstrate that our framework is\nthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, and\nFlappy Bird, setting a new benchmark for game-playing agents."}
{"id": "2503.21140", "pdf": "https://arxiv.org/pdf/2503.21140", "abs": "https://arxiv.org/abs/2503.21140", "authors": ["Junjie Chen", "Weilong Chen", "Yifan Zuo", "Yuming Fang"], "title": "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Category-agnostic pose estimation aims to locate keypoints on query images\naccording to a few annotated support images for arbitrary novel classes.\nExisting methods generally extract support features via heatmap pooling, and\nobtain interacted features from support and query via cross-attention. Hence,\nthese works neglect to mine fine-grained and structure-aware (FGSA) features\nfrom both support and query images, which are crucial for pixel-level keypoint\nlocalization. To this end, we propose a novel yet concise framework, which\nrecurrently mines FGSA features from both support and query images.\nSpecifically, we design a FGSA mining module based on deformable attention\nmechanism. On the one hand, we mine fine-grained features by applying\ndeformable attention head over multi-scale feature maps. On the other hand, we\nmine structure-aware features by offsetting the reference points of keypoints\nto their linked keypoints. By means of above module, we recurrently mine FGSA\nfeatures from support and query images, and thus obtain better support features\nand query estimations. In addition, we propose to use mixup keypoints to pad\nvarious classes to a unified keypoint number, which could provide richer\nsupervision than the zero padding used in existing works. We conduct extensive\nexperiments and in-depth studies on large-scale MP-100 dataset, and outperform\nSOTA method dramatically (+3.2\\%PCK@0.05). Code is avaiable at\nhttps://github.com/chenbys/FMMP."}
{"id": "2503.21295", "pdf": "https://arxiv.org/pdf/2503.21295", "abs": "https://arxiv.org/abs/2503.21295", "authors": ["Shuaijie She", "Junxiao Liu", "Yifeng Liu", "Jiajun Chen", "Xin Huang", "Shujian Huang"], "title": "R-PRM: Reasoning-Driven Process Reward Modeling", "categories": ["cs.CL"], "comment": "The project is available at https://github.com/NJUNLP/R-PRM", "summary": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential."}
{"id": "2503.21144", "pdf": "https://arxiv.org/pdf/2503.21144", "abs": "https://arxiv.org/abs/2503.21144", "authors": ["Jinwei Qi", "Chaonan Ji", "Sheng Xu", "Peng Zhang", "Bang Zhang", "Liefeng Bo"], "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model", "categories": ["cs.CV"], "comment": "Project Page: https://humanaigc.github.io/chat-anyone/", "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements."}
{"id": "2503.21332", "pdf": "https://arxiv.org/pdf/2503.21332", "abs": "https://arxiv.org/abs/2503.21332", "authors": ["Taewon Yun", "Jihwan Oh", "Hyangsuk Min", "Yuho Lee", "Jihwan Bang", "Jason Cai", "Hwanjun Song"], "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released."}
{"id": "2503.21150", "pdf": "https://arxiv.org/pdf/2503.21150", "abs": "https://arxiv.org/abs/2503.21150", "authors": ["Yuhan Liu", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the\npixel-level segmentation capabilities learned from large-scale source-domain\ndatasets to downstream target-domain datasets, with only a few annotated images\nper class. In this paper, we focus on a well-observed but unresolved phenomenon\nin CDFSS: for target domains, particularly those distant from the source\ndomain, segmentation performance peaks at the very early epochs, and declines\nsharply as the source-domain training proceeds. We delve into this phenomenon\nfor an interpretation: low-level features are vulnerable to domain shifts,\nleading to sharper loss landscapes during the source-domain training, which is\nthe devil of CDFSS. Based on this phenomenon and interpretation, we further\npropose a method that includes two plug-and-play modules: one to flatten the\nloss landscapes for low-level features during source-domain training as a novel\nsharpness-aware minimization method, and the other to directly supplement\ntarget-domain information to the model during target-domain testing by\nlow-level-based calibration. Extensive experiments on four target datasets\nvalidate our rationale and demonstrate that our method surpasses the\nstate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU\nin 1-shot and 5-shot scenarios, respectively."}
{"id": "2503.21349", "pdf": "https://arxiv.org/pdf/2503.21349", "abs": "https://arxiv.org/abs/2503.21349", "authors": ["Noah Losch", "Lucas Plagwitz", "Antonius Büscher", "Julian Varghese"], "title": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7; J.3"], "comment": "4 pages, 2 tables,", "summary": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text."}
{"id": "2503.21158", "pdf": "https://arxiv.org/pdf/2503.21158", "abs": "https://arxiv.org/abs/2503.21158", "authors": ["Eugene Denteh", "Andrews Danyo", "Joshua Kofi Asamoah", "Blessing Agyei Kyem", "Twitchell Addai", "Armstrong Aboah"], "title": "Integrating Travel Behavior Forecasting and Generative Modeling for Predicting Future Urban Mobility and Spatial Transformations", "categories": ["cs.CV"], "comment": null, "summary": "Transportation planning plays a critical role in shaping urban development,\neconomic mobility, and infrastructure sustainability. However, traditional\nplanning methods often struggle to accurately predict long-term urban growth\nand transportation demands. This may sometimes result in infrastructure\ndemolition to make room for current transportation planning demands. This study\nintegrates a Temporal Fusion Transformer to predict travel patterns from\ndemographic data with a Generative Adversarial Network to predict future urban\nsettings through satellite imagery. The framework achieved a 0.76 R-square\nscore in travel behavior prediction and generated high-fidelity satellite\nimages with a Structural Similarity Index of 0.81. The results demonstrate that\nintegrating predictive analytics and spatial visualization can significantly\nimprove the decision-making process, fostering more sustainable and efficient\nurban development. This research highlights the importance of data-driven\nmethodologies in modern transportation planning and presents a step toward\noptimizing infrastructure placement, capacity, and long-term viability."}
{"id": "2503.21360", "pdf": "https://arxiv.org/pdf/2503.21360", "abs": "https://arxiv.org/abs/2503.21360", "authors": ["Manuela Sanguinetti", "Alessandra Perniciano", "Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Maurizio Atzori"], "title": "From User Preferences to Optimization Constraints Using Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain"}
{"id": "2503.21164", "pdf": "https://arxiv.org/pdf/2503.21164", "abs": "https://arxiv.org/abs/2503.21164", "authors": ["Samra Irshad", "Seungkyu Lee", "Nassir Navab", "Hong Joo Lee", "Seong Tae Kim"], "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 9 figures", "summary": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."}
{"id": "2503.21378", "pdf": "https://arxiv.org/pdf/2503.21378", "abs": "https://arxiv.org/abs/2503.21378", "authors": ["Kota Dohi", "Tomoya Nishida", "Harsh Purohit", "Takashi Endo", "Yohei Kawaguchi"], "title": "Retrieving Time-Series Differences Using Natural Language Queries", "categories": ["cs.CL"], "comment": null, "summary": "Effectively searching time-series data is essential for system analysis;\nhowever, traditional methods often require domain expertise to define search\ncriteria. Recent advancements have enabled natural language-based search, but\nthese methods struggle to handle differences between time-series data. To\naddress this limitation, we propose a natural language query-based approach for\nretrieving pairs of time-series data based on differences specified in the\nquery. Specifically, we define six key characteristics of differences,\nconstruct a corresponding dataset, and develop a contrastive learning-based\nmodel to align differences between time-series data with query texts.\nExperimental results demonstrate that our model achieves an overall mAP score\nof 0.994 in retrieving time-series pairs."}
{"id": "2503.21169", "pdf": "https://arxiv.org/pdf/2503.21169", "abs": "https://arxiv.org/abs/2503.21169", "authors": ["Jiahao Lyu", "Minghua Zhao", "Jing Hu", "Xuewen Huang", "Yifei Chen", "Shuangli Du"], "title": "VADMamba: Exploring State Space Models for Fast Video Anomaly Detection", "categories": ["cs.CV"], "comment": "Accpeted by ICME 2025", "summary": "Video anomaly detection (VAD) methods are mostly CNN-based or\nTransformer-based, achieving impressive results, but the focus on detection\naccuracy often comes at the expense of inference speed. The emergence of state\nspace models in computer vision, exemplified by the Mamba model, demonstrates\nimproved computational efficiency through selective scans and showcases the\ngreat potential for long-range modeling. Our study pioneers the application of\nMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frame\nprediction and optical flow reconstruction. Specifically, we propose the\nVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)\nlayer and Mamba-based Non-negative Visual State Space (NVSS) block.\nFurthermore, two individual VQ-MaU networks separately predict frames and\nreconstruct corresponding optical flows, further boosting accuracy through a\nclip-level fusion evaluation strategy. Experimental results validate the\nefficacy of the proposed VADMamba across three benchmark datasets,\ndemonstrating superior performance in inference speed compared to previous\nwork. Code is available at https://github.com/jLooo/VADMamba."}
{"id": "2503.21380", "pdf": "https://arxiv.org/pdf/2503.21380", "abs": "https://arxiv.org/abs/2503.21380", "authors": ["Haoxiang Sun", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Zheng Liu", "Zhongyuan Wang", "Lei Fang", "Ji-Rong Wen"], "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark", "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."}
{"id": "2503.21172", "pdf": "https://arxiv.org/pdf/2503.21172", "abs": "https://arxiv.org/abs/2503.21172", "authors": ["Jingye Chen", "Yuzhong Zhao", "Yupan Huang", "Lei Cui", "Li Dong", "Tengchao Lv", "Qifeng Chen", "Furu Wei"], "title": "Model as a Game: On Numerical and Spatial Consistency for Generative Games", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Recent advances in generative models have significantly impacted game\ngeneration. However, despite producing high-quality graphics and adequately\nreceiving player input, existing models often fail to maintain fundamental game\nproperties such as numerical and spatial consistency. Numerical consistency\nensures gameplay mechanics correctly reflect score changes and other\nquantitative elements, while spatial consistency prevents jarring scene\ntransitions, providing seamless player experiences. In this paper, we revisit\nthe paradigm of generative games to explore what truly constitutes a Model as a\nGame (MaaG) with a well-developed mechanism. We begin with an empirical study\non ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet\nchallenging generative models in maintaining consistency. Based on the DiT\narchitecture, we design two specialized modules: (1) a numerical module that\nintegrates a LogicNet to determine event triggers, with calculations processed\nexternally as conditions for image generation; and (2) a spatial module that\nmaintains a map of explored areas, retrieving location-specific information\nduring generation and linking new observations to ensure continuity.\nExperiments across three games demonstrate that our integrated modules\nsignificantly enhance performance on consistency metrics compared to baselines,\nwhile incurring minimal time overhead during inference."}
{"id": "2503.21383", "pdf": "https://arxiv.org/pdf/2503.21383", "abs": "https://arxiv.org/abs/2503.21383", "authors": ["Chengxing Jia", "Ziniu Li", "Pengyuan Wang", "Yi-Chen Li", "Zhenyu Hou", "Yuxiao Dong", "Yang Yu"], "title": "Controlling Large Language Model with Latent Actions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications."}
{"id": "2503.21187", "pdf": "https://arxiv.org/pdf/2503.21187", "abs": "https://arxiv.org/abs/2503.21187", "authors": ["Yimin Xu"], "title": "DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg"}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393", "abs": "https://arxiv.org/abs/2503.21393", "authors": ["Rohitash Chandra", "Aryan Chaudhary", "Yeshwanth Rayavarapu"], "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate."}
{"id": "2503.21190", "pdf": "https://arxiv.org/pdf/2503.21190", "abs": "https://arxiv.org/abs/2503.21190", "authors": ["Erika Mori", "Yue Qiu", "Hirokatsu Kataoka", "Yoshimitsu Aoki"], "title": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence."}
{"id": "2503.21460", "pdf": "https://arxiv.org/pdf/2503.21460", "abs": "https://arxiv.org/abs/2503.21460", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "categories": ["cs.CL"], "comment": "329 papers surveyed, resources are at\n  https://github.com/luo-junyu/Awesome-Agent-Papers", "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers."}
{"id": "2503.21208", "pdf": "https://arxiv.org/pdf/2503.21208", "abs": "https://arxiv.org/abs/2503.21208", "authors": ["Wenxuan Qiu", "Chengxin Xie", "Jingui Huang"], "title": "An improved EfficientNetV2 for garbage classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an enhanced waste classification framework based on\nEfficientNetV2 to address challenges in data acquisition cost, generalization,\nand real-time performance. We propose a Channel-Efficient Attention\n(CE-Attention) module that mitigates feature loss during global pooling without\nintroducing dimensional scaling, effectively enhancing critical feature\nextraction. Additionally, a lightweight multi-scale spatial feature extraction\nmodule (SAFM) is developed by integrating depthwise separable convolutions,\nsignificantly reducing model complexity. Comprehensive data augmentation\nstrategies are further employed to improve generalization. Experiments on the\nHuawei Cloud waste classification dataset demonstrate that our method achieves\na classification accuracy of 95.4\\%, surpassing the baseline by 3.2\\% and\noutperforming mainstream models. The results validate the effectiveness of our\napproach in balancing accuracy and efficiency for practical waste\nclassification scenarios."}
{"id": "2503.21464", "pdf": "https://arxiv.org/pdf/2503.21464", "abs": "https://arxiv.org/abs/2503.21464", "authors": ["Ryan Marinelli", "Josef Pichlmeier", "Tamas Bisztray"], "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."}
{"id": "2503.21210", "pdf": "https://arxiv.org/pdf/2503.21210", "abs": "https://arxiv.org/abs/2503.21210", "authors": ["Yueying Gao", "Dongliang Chang", "Bingyao Yu", "Haotian Qin", "Lei Chen", "Kongming Liang", "Zhanyu Ma"], "title": "FakeReasoning: Towards Generalizable Forgery Detection and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and interpretable detection of AI-generated images is essential for\nmitigating risks associated with AI misuse. However, the substantial domain gap\namong generative models makes it challenging to develop a generalizable forgery\ndetection model. Moreover, since every pixel in an AI-generated image is\nsynthesized, traditional saliency-based forgery explanation methods are not\nwell suited for this task. To address these challenges, we propose modeling\nAI-generated image detection and explanation as a Forgery Detection and\nReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide\naccurate detection through structured and reliable reasoning over forgery\nattributes. To facilitate this task, we introduce the Multi-Modal Forgery\nReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images\nacross 10 generative models, with 10 types of forgery reasoning annotations,\nenabling comprehensive evaluation of FDR-Task. Additionally, we propose\nFakeReasoning, a forgery detection and reasoning framework with two key\ncomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'\nunderstanding of forgery-related semantics through both cross-modal and\nintra-modal contrastive learning between images and forgery attribute\nreasoning. Second, a Classification Probability Mapper bridges the optimization\ngap between forgery detection and language modeling by mapping the output\nlogits of VLMs to calibrated binary classification probabilities. Experiments\nacross multiple generative models demonstrate that FakeReasoning not only\nachieves robust generalization but also outperforms state-of-the-art methods on\nboth detection and reasoning tasks."}
{"id": "2503.21480", "pdf": "https://arxiv.org/pdf/2503.21480", "abs": "https://arxiv.org/abs/2503.21480", "authors": ["John Murzaku", "Owen Rambow"], "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs", "categories": ["cs.CL"], "comment": "Submitted to COLM 2025. Preprint", "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs."}
{"id": "2503.21214", "pdf": "https://arxiv.org/pdf/2503.21214", "abs": "https://arxiv.org/abs/2503.21214", "authors": ["Alan Dao", "Norapat Buppodom"], "title": "VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Comprehending 3D environments is vital for intelligent systems in domains\nlike robotics and autonomous navigation. Voxel grids offer a structured\nrepresentation of 3D space, but extracting high-level semantic meaning remains\nchallenging. This paper proposes a novel approach utilizing a Vision-Language\nModel (VLM) to extract \"voxel semantics\"-object identity, color, and\nlocation-from voxel data. Critically, instead of employing complex 3D networks,\nour method processes the voxel space by systematically slicing it along a\nprimary axis (e.g., the Z-axis, analogous to CT scan slices). These 2D slices\nare then formatted and sequentially fed into the image encoder of a standard\nVLM. The model learns to aggregate information across slices and correlate\nspatial patterns with semantic concepts provided by the language component.\nThis slice-based strategy aims to leverage the power of pre-trained 2D VLMs for\nefficient 3D semantic understanding directly from voxel representations."}
{"id": "2503.21500", "pdf": "https://arxiv.org/pdf/2503.21500", "abs": "https://arxiv.org/abs/2503.21500", "authors": ["Haote Yang", "Xingjian Wei", "Jiang Wu", "Noémi Ligeti-Nagy", "Jiaxing Sun", "Yinfan Wang", "Zijian Győző Yang", "Junyuan Gao", "Jingchao Wang", "Bowen Jiang", "Shasha Wang", "Nanjun Yu", "Zihao Zhang", "Shixin Hong", "Hongwei Liu", "Wei Li", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Gábor Prószéky", "Conghui He"], "title": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics", "categories": ["cs.CL"], "comment": null, "summary": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval ."}
{"id": "2503.21219", "pdf": "https://arxiv.org/pdf/2503.21219", "abs": "https://arxiv.org/abs/2503.21219", "authors": ["Sibo Wu", "Congrong Xu", "Binbin Huang", "Andreas Geiger", "Anpei Chen"], "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach."}
{"id": "2503.21504", "pdf": "https://arxiv.org/pdf/2503.21504", "abs": "https://arxiv.org/abs/2503.21504", "authors": ["Yuxue Hu", "Junsong Li", "Meixuan Chen", "Dongyu Su", "Tongguan Wang", "Ying Sha"], "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets."}
{"id": "2503.21226", "pdf": "https://arxiv.org/pdf/2503.21226", "abs": "https://arxiv.org/abs/2503.21226", "authors": ["Yishai Lavi", "Leo Segre", "Shai Avidan"], "title": "Frequency-Aware Gaussian Splatting Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis with\nits efficient, explicit representation. However, it lacks frequency\ninterpretability, making it difficult to separate low-frequency structures from\nfine details. We introduce a frequency-decomposed 3D-GS framework that groups\n3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the input\nimages. Our approach enforces coherence within each subband (i.e., group of 3D\nGaussians) through dedicated regularization, ensuring well-separated frequency\ncomponents. We extend color values to both positive and negative ranges,\nallowing higher-frequency layers to add or subtract residual details. To\nstabilize optimization, we employ a progressive training scheme that refines\ndetails in a coarse-to-fine manner. Beyond interpretability, this\nfrequency-aware design unlocks a range of practical benefits. Explicit\nfrequency separation enables advanced 3D editing and stylization, allowing\nprecise manipulation of specific frequency bands. It also supports dynamic\nlevel-of-detail control for progressive rendering, streaming, foveated\nrendering and fast geometry interaction. Through extensive experiments, we\ndemonstrate that our method provides improved control and flexibility for\nemerging applications in scene editing and interactive rendering. Our code will\nbe made publicly available."}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems."}
{"id": "2503.21236", "pdf": "https://arxiv.org/pdf/2503.21236", "abs": "https://arxiv.org/abs/2503.21236", "authors": ["Shuai Li", "Jie Zhang", "Yuang Qi", "Kejiang Chen", "Tianwei Zhang", "Weiming Zhang", "Nenghai Yu"], "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TMM", "summary": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method."}
{"id": "2503.21513", "pdf": "https://arxiv.org/pdf/2503.21513", "abs": "https://arxiv.org/abs/2503.21513", "authors": ["Ana-Maria Bucur", "Andreea-Codrina Moldovan", "Krutika Parvatikar", "Marcos Zampieri", "Ashiqur R. KhudaBukhsh", "Liviu P. Dinu"], "title": "Datasets for Depression Modeling in Social Media: An Overview", "categories": ["cs.CL"], "comment": "Accepted to CLPsych Workshop, NAACL 2025", "summary": "Depression is the most common mental health disorder, and its prevalence\nincreased during the COVID-19 pandemic. As one of the most extensively\nresearched psychological conditions, recent research has increasingly focused\non leveraging social media data to enhance traditional methods of depression\nscreening. This paper addresses the growing interest in interdisciplinary\nresearch on depression, and aims to support early-career researchers by\nproviding a comprehensive and up-to-date list of datasets for analyzing and\npredicting depression through social media data. We present an overview of\ndatasets published between 2019 and 2024. We also make the comprehensive list\nof datasets available online as a continuously updated resource, with the hope\nthat it will facilitate further interdisciplinary research into the linguistic\nexpressions of depression on social media."}
{"id": "2503.21246", "pdf": "https://arxiv.org/pdf/2503.21246", "abs": "https://arxiv.org/abs/2503.21246", "authors": ["Haoyu Zhao", "Zhongang Qi", "Cong Wang", "Qingping Zheng", "Guansong Lu", "Fei Chen", "Hang Xu", "Zuxuan Wu"], "title": "DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation", "categories": ["cs.CV"], "comment": "11 pages, 10 figures", "summary": "Human image animation has recently gained significant attention due to\nadvancements in generative models. However, existing methods still face two\nmajor challenges: (1) architectural limitations, most models rely on U-Net,\nwhich underperforms compared to the MM-DiT; and (2) the neglect of textual\ninformation, which can enhance controllability. In this work, we introduce\nDynamiCtrl, a novel framework that not only explores different pose-guided\ncontrol structures in MM-DiT, but also reemphasizes the crucial role of text in\nthis task. Specifically, we employ a Shared VAE encoder for both reference\nimages and driving pose videos, eliminating the need for an additional pose\nencoder and simplifying the overall framework. To incorporate pose features\ninto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),\nwhich utilizes adaptive layer normalization to encode sparse pose features. The\nencoded features are directly added to the visual input, preserving the\nspatiotemporal consistency of the backbone while effectively introducing pose\ncontrol into MM-DiT. Furthermore, within the full attention mechanism, we align\ntextual and visual features to enhance controllability. By leveraging text, we\nnot only enable fine-grained control over the generated content, but also, for\nthe first time, achieve simultaneous control over both background and motion.\nExperimental results verify the superiority of DynamiCtrl on benchmark\ndatasets, demonstrating its strong identity preservation, heterogeneous\ncharacter driving, background controllability, and high-quality synthesis. The\nproject page is available at https://gulucaptain.github.io/DynamiCtrl/."}
{"id": "2503.21530", "pdf": "https://arxiv.org/pdf/2503.21530", "abs": "https://arxiv.org/abs/2503.21530", "authors": ["Umer Butt", "Stalin Veranasi", "Günter Neumann"], "title": "Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks."}
{"id": "2503.21250", "pdf": "https://arxiv.org/pdf/2503.21250", "abs": "https://arxiv.org/abs/2503.21250", "authors": ["Mohamed Lamine Mekhalfi", "Paul Chippendale", "Francisco Fraile", "Marcos Rico"], "title": "Orange Quality Grading with Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Orange grading is a crucial step in the fruit industry, as it helps to sort\noranges according to different criteria such as size, quality, ripeness, and\nhealth condition, ensuring safety for human consumption and better price\nallocation and client satisfaction. Automated grading enables faster\nprocessing, precision, and reduced human labor. In this paper, we implement a\ndeep learning-based solution for orange grading via machine vision. Unlike\ntypical grading systems that analyze fruits from a single view, we capture\nmultiview images of each single orange in order to enable a richer\nrepresentation. Afterwards, we compose the acquired images into one collage.\nThis enables the analysis of the whole orange skin. We train a convolutional\nneural network (CNN) on the composed images to grade the oranges into three\nclasses, namely good, bad, and undefined. We also evaluate the performance with\ntwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally that\nmulti-view grading is superior to single view grading."}
{"id": "2503.21544", "pdf": "https://arxiv.org/pdf/2503.21544", "abs": "https://arxiv.org/abs/2503.21544", "authors": ["Yuwei Yin", "EunJeong Hwang", "Giuseppe Carenini"], "title": "SWI: Speaking with Intent in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "24 pages. Code: https://github.com/YuweiYin/SWI", "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions."}
{"id": "2503.21254", "pdf": "https://arxiv.org/pdf/2503.21254", "abs": "https://arxiv.org/abs/2503.21254", "authors": ["Zhaokai Wang", "Chenxi Bao", "Le Zhuo", "Jingrui Han", "Yang Yue", "Yihong Tang", "Victor Shea-Jay Huang", "Yue Liao"], "title": "Vision-to-Music Generation: A Survey", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation."}
{"id": "2503.21613", "pdf": "https://arxiv.org/pdf/2503.21613", "abs": "https://arxiv.org/abs/2503.21613", "authors": ["Javier Coronado-Blázquez"], "title": "Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach", "categories": ["cs.CL"], "comment": "22 pages, 6 figures", "summary": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems."}
{"id": "2503.21258", "pdf": "https://arxiv.org/pdf/2503.21258", "abs": "https://arxiv.org/abs/2503.21258", "authors": ["Jizhou Han", "Chenhao Ding", "Yuhang He", "Songlin Dong", "Qiang Wang", "Xinyuan Gao", "Yihong Gong"], "title": "Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods."}
{"id": "2503.21614", "pdf": "https://arxiv.org/pdf/2503.21614", "abs": "https://arxiv.org/abs/2503.21614", "authors": ["Xiaoye Qu", "Yafu Li", "Zhaochen Su", "Weigao Sun", "Jianhao Yan", "Dongrui Liu", "Ganqu Cui", "Daizong Liu", "Shuxian Liang", "Junxian He", "Peng Li", "Wei Wei", "Jing Shao", "Chaochao Lu", "Yue Zhang", "Xian-Sheng Hua", "Bowen Zhou", "Yu Cheng"], "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond", "categories": ["cs.CL"], "comment": "Survey, 32 pages, Large Reasoning Models, Efficient Reasoning for\n  Language, Multimodality, and Beyond", "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area."}
{"id": "2503.21259", "pdf": "https://arxiv.org/pdf/2503.21259", "abs": "https://arxiv.org/abs/2503.21259", "authors": ["Wencheng Han", "Dongqian Guo", "Xiao Chen", "Pang Lyu", "Yi Jin", "Jianbing Shen"], "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data", "categories": ["cs.CV"], "comment": null, "summary": "Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https://um-lab.github.io/GSI-MAR/"}
{"id": "2503.21670", "pdf": "https://arxiv.org/pdf/2503.21670", "abs": "https://arxiv.org/abs/2503.21670", "authors": ["Rajvee Sheth", "Himanshu Beniwal", "Mayank Singh"], "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."}
{"id": "2503.21262", "pdf": "https://arxiv.org/pdf/2503.21262", "abs": "https://arxiv.org/abs/2503.21262", "authors": ["Yunusa Haruna", "Adamu Lawan"], "title": "vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Capturing long-range dependencies efficiently is essential for visual\nrecognition tasks, yet existing methods face limitations. Convolutional neural\nnetworks (CNNs) struggle with restricted receptive fields, while Vision\nTransformers (ViTs) achieve global context and long-range modeling at a high\ncomputational cost. State-space models (SSMs) offer an alternative, but their\napplication in vision remains underexplored. This work introduces vGamba, a\nhybrid vision backbone that integrates SSMs with attention mechanisms to\nenhance efficiency and expressiveness. At its core, the Gamba bottleneck block\nthat includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,\nalongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion\nModule for effective feature representation. The interplay of these components\nensures that vGamba leverages the low computational demands of SSMs while\nmaintaining the accuracy of attention mechanisms for modeling long-range\ndependencies in vision tasks. Additionally, the Fusion module enables seamless\ninteraction between these components. Extensive experiments on classification,\ndetection, and segmentation tasks demonstrate that vGamba achieves a superior\ntrade-off between accuracy and computational efficiency, outperforming several\nexisting models."}
{"id": "2503.21676", "pdf": "https://arxiv.org/pdf/2503.21676", "abs": "https://arxiv.org/abs/2503.21676", "authors": ["Nicolas Zucchet", "Jörg Bornschein", "Stephanie Chan", "Andrew Lampinen", "Razvan Pascanu", "Soham De"], "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training."}
{"id": "2503.21268", "pdf": "https://arxiv.org/pdf/2503.21268", "abs": "https://arxiv.org/abs/2503.21268", "authors": ["Ming Yan", "Xincheng Lin", "Yuhua Luo", "Shuqi Fan", "Yudi Dai", "Qixin Zhong", "Lincai Zhong", "Yuexin Ma", "Lan Xu", "Chenglu Wen", "Siqi Shen", "Cheng Wang"], "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate", "categories": ["cs.CV"], "comment": "CVPR2025, project in \\href{this\n  link}{http://www.lidarhumanmotion.net/climbingcap/}", "summary": "Human Motion Recovery (HMR) research mainly focuses on ground-based motions\nsuch as running. The study on capturing climbing motion, an off-ground motion,\nis sparse. This is partly due to the limited availability of climbing motion\ndatasets, especially large-scale and challenging 3D labeled datasets. To\naddress the insufficiency of climbing motion datasets, we collect AscendMotion,\na large-scale well-annotated, and challenging climbing motion dataset. It\nconsists of 412k RGB, LiDAR frames, and IMU measurements, including the\nchallenging climbing motions of 22 skilled climbing coaches across 12 different\nrock walls. Capturing the climbing motions is challenging as it requires\nprecise recovery of not only the complex pose but also the global position of\nclimbers. Although multiple global HMR methods have been proposed, they cannot\nfaithfully capture climbing motions. To address the limitations of HMR methods\nfor climbing, we propose ClimbingCap, a motion recovery method that\nreconstructs continuous 3D human climbing motion in a global coordinate system.\nOne key insight is to use the RGB and LiDAR modalities to separately\nreconstruct motions in camera coordinates and global coordinates and to\noptimize them jointly. We demonstrate the quality of the AscendMotion dataset\nand present promising results from ClimbingCap. The AscendMotion dataset and\nsource code release publicly at \\href{this\nlink}{http://www.lidarhumanmotion.net/climbingcap/}"}
{"id": "2503.21679", "pdf": "https://arxiv.org/pdf/2503.21679", "abs": "https://arxiv.org/abs/2503.21679", "authors": ["Yunze Xiao", "Tingyu He", "Lionel Z. Wang", "Yiming Ma", "Xingyu Song", "Xiaohang Xu", "Irene Li", "Ka Chung Ng"], "title": "JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community", "categories": ["cs.CL", "cs.CY"], "comment": "20 pages, 1 figures", "summary": "This paper introduces JiraiBench, the first bilingual benchmark for\nevaluating large language models' effectiveness in detecting self-destructive\ncontent across Chinese and Japanese social media communities. Focusing on the\ntransnational \"Jirai\" (landmine) online subculture that encompasses multiple\nforms of self-destructive behaviors including drug overdose, eating disorders,\nand self-harm, we present a comprehensive evaluation framework incorporating\nboth linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese\nposts and 5,000 Japanese posts with multidimensional annotation along three\nbehavioral categories, achieving substantial inter-annotator agreement.\nExperimental evaluations across four state-of-the-art models reveal significant\nperformance variations based on instructional language, with Japanese prompts\nunexpectedly outperforming Chinese prompts when processing Chinese content.\nThis emergent cross-cultural transfer suggests that cultural proximity can\nsometimes outweigh linguistic similarity in detection tasks. Cross-lingual\ntransfer experiments with fine-tuned models further demonstrate the potential\nfor knowledge transfer between these language systems without explicit target\nlanguage training. These findings highlight the need for culturally-informed\napproaches to multilingual content moderation and provide empirical evidence\nfor the importance of cultural context in developing more effective detection\nsystems for vulnerable online communities."}
{"id": "2503.21269", "pdf": "https://arxiv.org/pdf/2503.21269", "abs": "https://arxiv.org/abs/2503.21269", "authors": ["Zhaoyi Yan", "Kangjun Liu", "Qixiang Ye"], "title": "Delving Deep into Semantic Relation Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge distillation has become a cornerstone technique in deep learning,\nfacilitating the transfer of knowledge from complex models to lightweight\ncounterparts. Traditional distillation approaches focus on transferring\nknowledge at the instance level, but fail to capture nuanced semantic\nrelationships within the data. In response, this paper introduces a novel\nmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), which\nreimagines knowledge distillation through a semantics-relation lens among each\nsample. By leveraging semantic components, \\ie, superpixels, SeRKD enables a\nmore comprehensive and context-aware transfer of knowledge, which skillfully\nintegrates superpixel-based semantic extraction with relation-based knowledge\ndistillation for a sophisticated model compression and distillation.\nParticularly, the proposed method is naturally relevant in the domain of Vision\nTransformers (ViTs), where visual tokens serve as fundamental units of\nrepresentation. Experimental evaluations on benchmark datasets demonstrate the\nsuperiority of SeRKD over existing methods, underscoring its efficacy in\nenhancing model performance and generalization capabilities."}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases."}
{"id": "2503.21277", "pdf": "https://arxiv.org/pdf/2503.21277", "abs": "https://arxiv.org/abs/2503.21277", "authors": ["Hiroya Makino", "Takahiro Yamaguchi", "Hiroyuki Sakai"], "title": "Zero-Shot Visual Concept Blending Without Text Guidance", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel, zero-shot image generation technique called \"Visual\nConcept Blending\" that provides fine-grained control over which features from\nmultiple reference images are transferred to a source image. If only a single\nreference image is available, it is difficult to isolate which specific\nelements should be transferred. However, using multiple reference images, the\nproposed approach distinguishes between common and unique features by\nselectively incorporating them into a generated output. By operating within a\npartially disentangled Contrastive Language-Image Pre-training (CLIP) embedding\nspace (from IP-Adapter), our method enables the flexible transfer of texture,\nshape, motion, style, and more abstract conceptual transformations without\nrequiring additional training or text prompts. We demonstrate its effectiveness\nacross a diverse range of tasks, including style transfer, form metamorphosis,\nand conceptual transformations, showing how subtle or abstract attributes\n(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly\ncombined into a new image. In a user study, participants accurately recognized\nwhich features were intended to be transferred. Its simplicity, flexibility,\nand high-level control make Visual Concept Blending valuable for creative\nfields such as art, design, and content creation, where combining specific\nvisual qualities from multiple inspirations is crucial."}
{"id": "2503.21714", "pdf": "https://arxiv.org/pdf/2503.21714", "abs": "https://arxiv.org/abs/2503.21714", "authors": ["Pietro Tropeano", "Maria Maistro", "Tuukka Ruotsalo", "Christina Lioma"], "title": "As easy as PIE: understanding when pruning causes language models to disagree", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Findings)", "summary": "Language Model (LM) pruning compresses the model by removing weights, nodes,\nor other parts of its architecture. Typically, pruning focuses on the resulting\nefficiency gains at the cost of effectiveness. However, when looking at how\nindividual data points are affected by pruning, it turns out that a particular\nsubset of data points always bears most of the brunt (in terms of reduced\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\naccuracy of all data points. These data points are called PIEs and have been\nstudied in image processing, but not in NLP. In a study of various NLP\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\ninference quality considerably, regardless of class frequency, and that BERT is\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\ndata points that have the largest influence on how well the model generalises\nto unseen data. This means that when pruning, with seemingly moderate loss to\naccuracy across all data points, we in fact hurt tremendously those data points\nthat matter the most. We trace what makes PIEs both hard and impactful to\ninference to their overall longer and more semantically complex text. These\nfindings are novel and contribute to understanding how LMs are affected by\npruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE"}
{"id": "2503.21284", "pdf": "https://arxiv.org/pdf/2503.21284", "abs": "https://arxiv.org/abs/2503.21284", "authors": ["Hanyue Tu", "Siqi Wu", "Li Li", "Wengang Zhou", "Houqiang Li"], "title": "Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE Transactions on Multimedia 2025", "summary": "Autoencoder-based structures have dominated recent learned image compression\nmethods. However, the inherent information loss associated with autoencoders\nlimits their rate-distortion performance at high bit rates and restricts their\nflexibility of rate adaptation. In this paper, we present a variable-rate image\ncompression model based on invertible transform to overcome these limitations.\nSpecifically, we design a lightweight multi-scale invertible neural network,\nwhich bijectively maps the input image into multi-scale latent representations.\nTo improve the compression efficiency, a multi-scale spatial-channel context\nmodel with extended gain units is devised to estimate the entropy of the latent\nrepresentation from high to low levels. Experimental results demonstrate that\nthe proposed method achieves state-of-the-art performance compared to existing\nvariable-rate methods, and remains competitive with recent multi-model\napproaches. Notably, our method is the first learned image compression solution\nthat outperforms VVC across a very wide range of bit rates using a single\nmodel, especially at high bit rates.The source code is available at\n\\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}."}
{"id": "2503.21717", "pdf": "https://arxiv.org/pdf/2503.21717", "abs": "https://arxiv.org/abs/2503.21717", "authors": ["Jiefu Ou", "William Gantt Walden", "Kate Sanders", "Zhengping Jiang", "Kaiser Sun", "Jeffrey Cheng", "William Jurayj", "Miriam Wanner", "Shaobo Liang", "Candice Morgan", "Seunghoon Han", "Weiqi Wang", "Chandler May", "Hannah Recknor", "Daniel Khashabi", "Benjamin Van Durme"], "title": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?", "categories": ["cs.CL"], "comment": null, "summary": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks."}
{"id": "2503.21307", "pdf": "https://arxiv.org/pdf/2503.21307", "abs": "https://arxiv.org/abs/2503.21307", "authors": ["Dongchen Lu", "Yuyao Sun", "Zilu Zhang", "Leping Huang", "Jianliang Zeng", "Mao Shu", "Huo Cao"], "title": "InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Most multimodal large language models (MLLMs) treat visual tokens as \"a\nsequence of text\", integrating them with text tokens into a large language\nmodel (LLM). However, a great quantity of visual tokens significantly increases\nthe demand for computational resources and time. In this paper, we propose\nInternVL-X, which outperforms the InternVL model in both performance and\nefficiency by incorporating three visual token compression methods. First, we\npropose a novel vision-language projector, PVTC. This component integrates\nadjacent visual embeddings to form a local query and utilizes the transformed\nCLS token as a global query, then performs point-to-region cross-attention\nthrough these local and global queries to more effectively convert visual\nfeatures. Second, we present a layer-wise visual token compression module,\nLVTC, which compresses tokens in the LLM shallow layers and then expands them\nthrough upsampling and residual connections in the deeper layers. This\nsignificantly enhances the model computational efficiency. Futhermore, we\npropose an efficient high resolution slicing method, RVTC, which dynamically\nadjusts the number of visual tokens based on image area or length filtering.\nRVTC greatly enhances training efficiency with only a slight reduction in\nperformance. By utilizing 20% or fewer visual tokens, InternVL-X achieves\nstate-of-the-art performance on 7 public MLLM benchmarks, and improves the\naverage metric by 2.34% across 12 tasks."}
{"id": "2503.21718", "pdf": "https://arxiv.org/pdf/2503.21718", "abs": "https://arxiv.org/abs/2503.21718", "authors": ["Iuri Macocco", "Nora Graichen", "Gemma Boleda", "Marco Baroni"], "title": "Outlier dimensions favor frequent tokens in language model", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "9 pages, 4 figures", "summary": "We study last-layer outlier dimensions, i.e.dimensions that display extreme\nactivations for the majority of inputs. We show that outlier dimensions arise\nin many different modern language models, and trace their function back to the\nheuristic of constantly predicting frequent words. We further show how a model\ncan block this heuristic when it is not contextually appropriate, by assigning\na counterbalancing weight mass to the remaining dimensions, and we investigate\nwhich model parameters boost outlier dimensions and when they arise during\ntraining. We conclude that outlier dimensions are a specialized mechanism\ndiscovered by many distinct models to implement a useful token prediction\nheuristic."}
{"id": "2503.21309", "pdf": "https://arxiv.org/pdf/2503.21309", "abs": "https://arxiv.org/abs/2503.21309", "authors": ["Zixu Li", "Zhiheng Fu", "Yupeng Hu", "Zhiwei Chen", "Haokun Wen", "Liqiang Nie"], "title": "FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git."}
{"id": "2503.21720", "pdf": "https://arxiv.org/pdf/2503.21720", "abs": "https://arxiv.org/abs/2503.21720", "authors": ["Souradip Chakraborty", "Sujay Bhatt", "Udari Madhushani Sehwag", "Soumya Suvra Ghosal", "Jiahao Qiu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Alec Koppel", "Sumitra Ganesh"], "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICLR 2025", "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."}
{"id": "2503.21313", "pdf": "https://arxiv.org/pdf/2503.21313", "abs": "https://arxiv.org/abs/2503.21313", "authors": ["Zerui Chen", "Rolandos Alexandros Potamias", "Shizhe Chen", "Cordelia Schmid"], "title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers", "categories": ["cs.CV"], "comment": "Project Page: https://zerchen.github.io/projects/hort.html", "summary": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images."}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729", "abs": "https://arxiv.org/abs/2503.21729", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG)."}
{"id": "2503.21323", "pdf": "https://arxiv.org/pdf/2503.21323", "abs": "https://arxiv.org/abs/2503.21323", "authors": ["Ling Feng", "Tianyu Xie", "Wei Ma", "Ruijie Fu", "Yingxiao Zhang", "Jun Li", "Bei Zhou"], "title": "DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming."}
{"id": "2503.21730", "pdf": "https://arxiv.org/pdf/2503.21730", "abs": "https://arxiv.org/abs/2503.21730", "authors": ["Yongce Li", "Chung-En Sun", "Tsui-Wei Weng"], "title": "Effective Skill Unlearning through Intervention and Abstention", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to NAACL 2025 main conference", "summary": "Large language Models (LLMs) have demonstrated remarkable skills across\nvarious domains. Understanding the mechanisms behind their abilities and\nimplementing controls over them is becoming increasingly important for\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\nspecifically unlearning a particular skill while retaining their overall\ncapabilities. We introduce two lightweight, training-free machine skill\nunlearning techniques for LLMs. First, we observe that the pre-activation\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\ndemonstrates different skills. Additionally, we find that queries triggering\nthe same skill cluster within the FFL key space and can be separated from other\nqueries using a hypercube. Based on these observations, we propose two\nlightweight, training-free skill unlearning methods via \\textit{intervention}\nand \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key\nSpace Detection}. We evaluate our methods on unlearning math-solving,\nPython-coding, and comprehension skills across seven different languages. The\nresults demonstrate their strong unlearning capabilities for the designated\nskills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative\nperformance drop on the forgetting skill and less than 10\\% relative\nperformance drop on other skills and the model's general knowledge (MMLU) for\nmost unlearning tasks. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning"}
{"id": "2503.21338", "pdf": "https://arxiv.org/pdf/2503.21338", "abs": "https://arxiv.org/abs/2503.21338", "authors": ["Yehui Shen", "Lei Zhang", "Qingqiu Li", "Xiongwei Zhao", "Yue Wang", "Huimin Lu", "Xieyuanli Chen"], "title": "UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based on Uncertainty-Guided NeRF Augmentation", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Visual place recognition (VPR) is crucial for robots to identify previously\nvisited locations, playing an important role in autonomous navigation in both\nindoor and outdoor environments. However, most existing VPR datasets are\nlimited to single-viewpoint scenarios, leading to reduced recognition accuracy,\nparticularly in multi-directional driving or feature-sparse scenes. Moreover,\nobtaining additional data to mitigate these limitations is often expensive.\nThis paper introduces a novel training paradigm to improve the performance of\nexisting VPR networks by enhancing multi-view diversity within current datasets\nthrough uncertainty estimation and NeRF-based data augmentation. Specifically,\nwe initially train NeRF using the existing VPR dataset. Then, our devised\nself-supervised uncertainty estimation network identifies places with high\nuncertainty. The poses of these uncertain places are input into NeRF to\ngenerate new synthetic observations for further training of VPR networks.\nAdditionally, we propose an improved storage method for efficient organization\nof augmented and original training data. We conducted extensive experiments on\nthree datasets and tested three different VPR backbone networks. The results\ndemonstrate that our proposed training paradigm significantly improves VPR\nperformance by fully utilizing existing data, outperforming other training\napproaches. We further validated the effectiveness of our approach on\nself-recorded indoor and outdoor datasets, consistently demonstrating superior\nresults. Our dataset and code have been released at\n\\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}."}
{"id": "2503.21760", "pdf": "https://arxiv.org/pdf/2503.21760", "abs": "https://arxiv.org/abs/2503.21760", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks."}
{"id": "2503.21364", "pdf": "https://arxiv.org/pdf/2503.21364", "abs": "https://arxiv.org/abs/2503.21364", "authors": ["Zhenxiang Ma", "Zhenyu Yang", "Miao Tao", "Yuanzhen Zhou", "Zeyu He", "Yuchang Zhang", "Rong Fu", "Hengjie Li"], "title": "LandMarkSystem Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction is vital for applications in autonomous driving, virtual\nreality, augmented reality, and the metaverse. Recent advancements such as\nNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed\nthe field, yet traditional deep learning frameworks struggle to meet the\nincreasing demands for scene quality and scale. This paper introduces\nLandMarkSystem, a novel computing framework designed to enhance multi-scale\nscene reconstruction and rendering. By leveraging a componentized model\nadaptation layer, LandMarkSystem supports various NeRF and 3DGS structures\nwhile optimizing computational efficiency through distributed parallel\ncomputing and model parameter offloading. Our system addresses the limitations\nof existing frameworks, providing dedicated operators for complex 3D sparse\ncomputations, thus facilitating efficient training and rapid inference over\nextensive scenes. Key contributions include a modular architecture, a dynamic\nloading strategy for limited resources, and proven capabilities across multiple\nrepresentative algorithms.This comprehensive solution aims to advance the\nefficiency and effectiveness of 3D reconstruction tasks.To facilitate further\nresearch and collaboration, the source code and documentation for the\nLandMarkSystem project are publicly available in an open-source repository,\naccessing the repository at: https://github.com/InternLandMark/LandMarkSystem."}
{"id": "2209.07775", "pdf": "https://arxiv.org/pdf/2209.07775", "abs": "https://arxiv.org/abs/2209.07775", "authors": ["Daniel Bermuth", "Alexander Poeppel", "Wolfgang Reif"], "title": "Jaco: An Offline Running Privacy-aware Voice Assistant", "categories": ["cs.CR", "cs.CL", "cs.HC"], "comment": null, "summary": "With the recent advance in speech technology, smart voice assistants have\nbeen improved and are now used by many people. But often these assistants are\nrunning online as a cloud service and are not always known for a good\nprotection of users' privacy. This paper presents the architecture of a novel\nvoice assistant, called Jaco, with the following features: (a) It can run\ncompletely offline, even on low resource devices like a RaspberryPi. (b)\nThrough a skill concept it can be easily extended. (c) The architectural focus\nis on protecting users' privacy, but without restricting capabilities for\ndevelopers. (d) It supports multiple languages. (e) It is competitive with\nother voice assistant solutions. In this respect the assistant combines and\nextends the advantages of other approaches."}
{"id": "2503.21367", "pdf": "https://arxiv.org/pdf/2503.21367", "abs": "https://arxiv.org/abs/2503.21367", "authors": ["Bořek Reich", "Matej Kunda", "Fedor Zolotarev", "Tuomas Eerola", "Pavel Zemčík", "Tomi Kauppi"], "title": "Multimodal surface defect detection from wooden logs for sawing optimization", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel, good-quality, and less demanding method for detecting\nknots on the surface of wooden logs using multimodal data fusion. Knots are a\nprimary factor affecting the quality of sawn timber, making their detection\nfundamental to any timber grading or cutting optimization system. While X-ray\ncomputed tomography provides accurate knot locations and internal structures,\nit is often too slow or expensive for practical use. An attractive alternative\nis to use fast and cost-effective log surface measurements, such as laser\nscanners or RGB cameras, to detect surface knots and estimate the internal\nstructure of wood. However, due to the small size of knots and noise caused by\nfactors, such as bark and other natural variations, detection accuracy often\nremains low when only one measurement modality is used. In this paper, we\ndemonstrate that by using a data fusion pipeline consisting of separate streams\nfor RGB and point cloud data, combined by a late fusion module, higher knot\ndetection accuracy can be achieved compared to using either modality alone. We\nfurther propose a simple yet efficient sawing angle optimization method that\nutilizes surface knot detections and cross-correlation to minimize the amount\nof unwanted arris knots, demonstrating its benefits over randomized sawing\nangles."}
{"id": "2503.20807", "pdf": "https://arxiv.org/pdf/2503.20807", "abs": "https://arxiv.org/abs/2503.20807", "authors": ["Pin-Yu Chen", "Han Shen", "Payel Das", "Tianyi Chen"], "title": "Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "The first two authors contribute equally to this work and are listed\n  in alphabetical order", "summary": "Fine-tuning Large Language Models (LLMs) on some task-specific datasets has\nbeen a primary use of LLMs. However, it has been empirically observed that this\napproach to enhancing capability inevitably compromises safety, a phenomenon\nalso known as the safety-capability trade-off in LLM fine-tuning. This paper\npresents a theoretical framework for understanding the interplay between safety\nand capability in two primary safety-aware LLM fine-tuning strategies,\nproviding new insights into the effects of data similarity, context overlap,\nand alignment loss landscape. Our theoretical results characterize the\nfundamental limits of the safety-capability trade-off in LLM fine-tuning, which\nare also validated by numerical experiments."}
{"id": "2503.21377", "pdf": "https://arxiv.org/pdf/2503.21377", "abs": "https://arxiv.org/abs/2503.21377", "authors": ["Hamadi Chihaoui", "Paolo Favaro"], "title": "Unsupervised Real-World Denoising: Sparsity is All You Need", "categories": ["cs.CV"], "comment": null, "summary": "Supervised training for real-world denoising presents challenges due to the\ndifficulty of collecting large datasets of paired noisy and clean images.\nRecent methods have attempted to address this by utilizing unpaired datasets of\nclean and noisy images. Some approaches leverage such unpaired data to train\ndenoisers in a supervised manner by generating synthetic clean-noisy pairs.\nHowever, these methods often fall short due to the distribution gap between\nsynthetic and real noisy images. To mitigate this issue, we propose a solution\nbased on input sparsification, specifically using random input masking. Our\nmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiser\nto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,\ninput sparsification reduces the gap between synthetic and real noisy images.\nOn the other hand, an inpainter trained in a supervised manner can still\naccurately reconstruct sparse inputs by predicting missing clean pixels using\nthe remaining unmasked pixels. Our approach begins with a synthetic Gaussian\nnoise sampler and iteratively refines it using a noise dataset derived from the\ndenoiser's predictions. The noise dataset is created by subtracting predicted\npseudo-clean images from real noisy images at each iteration. The core\nintuition is that improving the denoiser results in a more accurate noise\ndataset and, consequently, a better noise sampler. We validate our method\nthrough extensive experiments on real-world noisy image datasets, demonstrating\ncompetitive performance compared to existing unsupervised denoising methods."}
{"id": "2503.20846", "pdf": "https://arxiv.org/pdf/2503.20846", "abs": "https://arxiv.org/abs/2503.20846", "authors": ["Viktor Schlegel", "Anil A Bharath", "Zilong Zhao", "Kevin Yee"], "title": "Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead", "categories": ["cs.CR", "cs.CL", "cs.CV"], "comment": "23 pages + references + Appendix. Preprint", "summary": "Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential."}
{"id": "2503.21408", "pdf": "https://arxiv.org/pdf/2503.21408", "abs": "https://arxiv.org/abs/2503.21408", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "title": "VALLR: Visual ASR Language Model for Lip Reading", "categories": ["cs.CV"], "comment": null, "summary": "Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex\ntask requiring the interpretation of spoken language exclusively from visual\ncues, primarily lip movements and facial expressions. This task is especially\nchallenging due to the absence of auditory information and the inherent\nambiguity when visually distinguishing phonemes that have overlapping visemes\nwhere different phonemes appear identical on the lips. Current methods\ntypically attempt to predict words or characters directly from these visual\ncues, but this approach frequently encounters high error rates due to\ncoarticulation effects and viseme ambiguity. We propose a novel two-stage,\nphoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that\naddresses these longstanding challenges. First, our model predicts a compact\nsequence of phonemes from visual inputs using a Video Transformer with a CTC\nhead, thereby reducing the task complexity and achieving robust speaker\ninvariance. This phoneme output then serves as the input to a fine-tuned Large\nLanguage Model (LLM), which reconstructs coherent words and sentences by\nleveraging broader linguistic context. Unlike existing methods that either\npredict words directly-often faltering on visually similar phonemes-or rely on\nlarge-scale multimodal pre-training, our approach explicitly encodes\nintermediate linguistic structure while remaining highly data efficient. We\ndemonstrate state-of-the-art performance on two challenging datasets, LRS2 and\nLRS3, where our method achieves significant reductions in Word Error Rate (WER)\nachieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data\nthan the next best approach."}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives."}
{"id": "2503.21410", "pdf": "https://arxiv.org/pdf/2503.21410", "abs": "https://arxiv.org/abs/2503.21410", "authors": ["Hamadi Chihaoui", "Paolo Favaro"], "title": "Diffusion Image Prior", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot image restoration (IR) methods based on pretrained diffusion models\nhave recently achieved significant success. These methods typically require at\nleast a parametric form of the degradation model. However, in real-world\nscenarios, the degradation may be too complex to define explicitly. To handle\nthis general case, we introduce the Diffusion Image Prior (DIIP). We take\ninspiration from the Deep Image Prior (DIP)[16], since it can be used to remove\nartifacts without the need for an explicit degradation model. However, in\ncontrast to DIP, we find that pretrained diffusion models offer a much stronger\nprior, despite being trained without knowledge from corrupted data. We show\nthat, the optimization process in DIIP first reconstructs a clean version of\nthe image before eventually overfitting to the degraded input, but it does so\nfor a broader range of degradations than DIP. In light of this result, we\npropose a blind image restoration (IR) method based on early stopping, which\ndoes not require prior knowledge of the degradation model. We validate DIIP on\nvarious degradation-blind IR tasks, including JPEG artifact removal, waterdrop\nremoval, denoising and super-resolution with state-of-the-art results."}
{"id": "2503.20914", "pdf": "https://arxiv.org/pdf/2503.20914", "abs": "https://arxiv.org/abs/2503.20914", "authors": ["Michel Boeglin", "David Kahn", "Josiane Mothe", "Diego Ortiz", "David Panzoli"], "title": "D4R -- Exploring and Querying Relational Graphs Using Natural Language and Large Language Models -- the Case of Historical Documents", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3; H.3.3; I.2.7"], "comment": "8 pages, 7 figures", "summary": "D4R is a digital platform designed to assist non-technical users,\nparticularly historians, in exploring textual documents through advanced\ngraphical tools for text analysis and knowledge extraction. By leveraging a\nlarge language model, D4R translates natural language questions into Cypher\nqueries, enabling the retrieval of data from a Neo4J database. A user-friendly\ngraphical interface allows for intuitive interaction, enabling users to\nnavigate and analyse complex relational data extracted from unstructured\ntextual documents. Originally designed to bridge the gap between AI\ntechnologies and historical research, D4R's capabilities extend to various\nother domains. A demonstration video and a live software demo are available."}
{"id": "2503.21438", "pdf": "https://arxiv.org/pdf/2503.21438", "abs": "https://arxiv.org/abs/2503.21438", "authors": ["Anis Ur Rahman", "Einari Heinaro", "Mete Ahishali", "Samuli Junttila"], "title": "Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery", "categories": ["cs.CV"], "comment": "11 pages, 4 figures, 4 tables", "summary": "Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning."}
{"id": "2503.20992", "pdf": "https://arxiv.org/pdf/2503.20992", "abs": "https://arxiv.org/abs/2503.20992", "authors": ["Michael Brown", "Sofia Martinez", "Priya Singh"], "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer", "categories": ["cs.GR", "cs.CL"], "comment": null, "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer."}
{"id": "2503.21449", "pdf": "https://arxiv.org/pdf/2503.21449", "abs": "https://arxiv.org/abs/2503.21449", "authors": ["Lucas Nunes", "Rodrigo Marcuzzi", "Jens Behley", "Cyrill Stachniss"], "title": "Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Semantic scene understanding is crucial for robotics and computer vision\napplications. In autonomous driving, 3D semantic segmentation plays an\nimportant role for enabling safe navigation. Despite significant advances in\nthe field, the complexity of collecting and annotating 3D data is a bottleneck\nin this developments. To overcome that data annotation limitation, synthetic\nsimulated data has been used to generate annotated data on demand. There is\nstill however a domain gap between real and simulated data. More recently,\ndiffusion models have been in the spotlight, enabling close-to-real data\nsynthesis. Those generative models have been recently applied to the 3D data\ndomain for generating scene-scale data with semantic annotations. Still, those\nmethods either rely on image projection or decoupled models trained with\ndifferent resolutions in a coarse-to-fine manner. Such intermediary\nrepresentations impact the generated data quality due to errors added in those\ntransformations. In this work, we propose a novel approach able to generate 3D\nsemantic scene-scale data without relying on any projection or decoupled\ntrained multi-resolution models, achieving more realistic semantic scene data\ngeneration compared to previous state-of-the-art methods. Besides improving 3D\nsemantic scene-scale data synthesis, we thoroughly evaluate the use of the\nsynthetic scene samples as labeled data to train a semantic segmentation\nnetwork. In our experiments, we show that using the synthetic annotated data\ngenerated by our method as training data together with the real semantic\nsegmentation labels, leads to an improvement in the semantic segmentation model\nperformance. Our results show the potential of generated scene-scale point\nclouds to generate more training data to extend existing datasets, reducing the\ndata annotation effort. Our code is available at\nhttps://github.com/PRBonn/3DiSS."}
{"id": "2503.21067", "pdf": "https://arxiv.org/pdf/2503.21067", "abs": "https://arxiv.org/abs/2503.21067", "authors": ["Enzo B Onofre", "Leonardo M P Moraes", "Cristina D Aguiar"], "title": "AskSport: Web Application for Sports Question-Answering", "categories": ["cs.AI", "cs.CL", "I.2.1; I.2.7"], "comment": "for accessing the application, see\n  https://huggingface.co/spaces/leomaurodesenv/qasports-website", "summary": "This paper introduces AskSport, a question-answering web application about\nsports. It allows users to ask questions using natural language and retrieve\nthe three most relevant answers, including related information and documents.\nThe paper describes the characteristics and functionalities of the application,\nincluding use cases demonstrating its ability to return names and numerical\nvalues. AskSport and its implementation are available for public access on\nHuggingFace."}
{"id": "2503.21457", "pdf": "https://arxiv.org/pdf/2503.21457", "abs": "https://arxiv.org/abs/2503.21457", "authors": ["Xiaoqin Wang", "Xusen Ma", "Xianxu Hou", "Meidan Ding", "Yudong Li", "Junliang Chen", "Wenting Chen", "Xiaoyang Peng", "Linlin Shen"], "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in various tasks. However, effectively evaluating these MLLMs on\nface perception remains largely unexplored. To address this gap, we introduce\nFaceBench, a dataset featuring hierarchical multi-view and multi-level\nattributes specifically designed to assess the comprehensive face perception\nabilities of MLLMs. Initially, we construct a hierarchical facial attribute\nstructure, which encompasses five views with up to three levels of attributes,\ntotaling over 210 attributes and 700 attribute values. Based on the structure,\nthe proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs\nfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a\nrobust face perception MLLM baseline, Face-LLaVA, by training with our proposed\nface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA\nare conducted to test their face perception ability, with results also compared\nagainst human performance. The results reveal that, the existing MLLMs are far\nfrom satisfactory in understanding the fine-grained facial attributes, while\nour Face-LLaVA significantly outperforms existing open-source models with a\nsmall amount of training data and is comparable to commercial ones like GPT-4o\nand Gemini. The dataset will be released at\nhttps://github.com/CVI-SZU/FaceBench."}
{"id": "2503.21074", "pdf": "https://arxiv.org/pdf/2503.21074", "abs": "https://arxiv.org/abs/2503.21074", "authors": ["Ooha Lakkadi Reddy"], "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21\n  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this\n  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at\n  Duke Kunshan University; accepted for presentation at the 2025 Computer\n  Applications and Quantitative Methods in Archaeology Conference, Athens", "summary": "This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized."}
{"id": "2503.21459", "pdf": "https://arxiv.org/pdf/2503.21459", "abs": "https://arxiv.org/abs/2503.21459", "authors": ["Chirag Parikh", "Deepti Rawat", "Rakshitha R. T.", "Tathagata Ghosh", "Ravi Kiran Sarvadevabhatla"], "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/", "summary": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs."}
{"id": "2503.21114", "pdf": "https://arxiv.org/pdf/2503.21114", "abs": "https://arxiv.org/abs/2503.21114", "authors": ["Jamshid Sourati", "Grace Shao"], "title": "Measuring and Analyzing Subjective Uncertainty in Scientific Communications", "categories": ["cs.DL", "cs.CL"], "comment": "Coming with Appendix and supplementary material", "summary": "Uncertainty of scientific findings are typically reported through statistical\nmetrics such as $p$-values, confidence intervals, etc. The magnitude of this\nobjective uncertainty is reflected in the language used by the authors to\nreport their findings primarily through expressions carrying\nuncertainty-inducing terms or phrases. This language uncertainty is a\nsubjective concept and is highly dependent on the writing style of the authors.\nThere is evidence that such subjective uncertainty influences the impact of\nscience on public audience. In this work, we turned our focus to scientists\nthemselves, and measured/analyzed the subjective uncertainty and its impact\nwithin scientific communities across different disciplines. We showed that the\nlevel of this type of uncertainty varies significantly across different fields,\nyears of publication and geographical locations. We also studied the\ncorrelation between subjective uncertainty and several bibliographical metrics,\nsuch as number/gender of authors, centrality of the field's community, citation\ncount, etc. The underlying patterns identified in this work are useful in\nidentification and documentation of linguistic norms in scientific\ncommunication in different communities/societies."}
{"id": "2503.21465", "pdf": "https://arxiv.org/pdf/2503.21465", "abs": "https://arxiv.org/abs/2503.21465", "authors": ["Deependra Singh", "Saksham Agarwal", "Subhankar Mishra"], "title": "Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T10, 68T45, 92C55", "I.2.10; I.5.4; J.3"], "comment": "17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)", "summary": "Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction."}
{"id": "2503.21214", "pdf": "https://arxiv.org/pdf/2503.21214", "abs": "https://arxiv.org/abs/2503.21214", "authors": ["Alan Dao", "Norapat Buppodom"], "title": "VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Comprehending 3D environments is vital for intelligent systems in domains\nlike robotics and autonomous navigation. Voxel grids offer a structured\nrepresentation of 3D space, but extracting high-level semantic meaning remains\nchallenging. This paper proposes a novel approach utilizing a Vision-Language\nModel (VLM) to extract \"voxel semantics\"-object identity, color, and\nlocation-from voxel data. Critically, instead of employing complex 3D networks,\nour method processes the voxel space by systematically slicing it along a\nprimary axis (e.g., the Z-axis, analogous to CT scan slices). These 2D slices\nare then formatted and sequentially fed into the image encoder of a standard\nVLM. The model learns to aggregate information across slices and correlate\nspatial patterns with semantic concepts provided by the language component.\nThis slice-based strategy aims to leverage the power of pre-trained 2D VLMs for\nefficient 3D semantic understanding directly from voxel representations."}
{"id": "2503.21477", "pdf": "https://arxiv.org/pdf/2503.21477", "abs": "https://arxiv.org/abs/2503.21477", "authors": ["Wenyi Xiong", "Jian Chen", "Ziheng Qi"], "title": "Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction Method", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE TIM for possible publication", "summary": "Trajectory prediction, as a critical component of autonomous driving systems,\nhas attracted the attention of many researchers. Existing prediction algorithms\nfocus on extracting more detailed scene features or selecting more reasonable\ntrajectory destinations. However, in the face of dynamic and evolving future\nmovements of the target vehicle, these algorithms cannot provide a fine-grained\nand continuous description of future behaviors and lane constraints, which\ndegrades the prediction accuracy. To address this challenge, we present BLNet,\na novel dualstream architecture that synergistically integrates behavioral\nintention recognition and lane constraint modeling through parallel attention\nmechanisms. The framework generates fine-grained behavior state queries\n(capturing spatial-temporal movement patterns) and lane queries (encoding lane\ntopology constraints), supervised by two auxiliary losses, respectively.\nSubsequently, a two-stage decoder first produces trajectory proposals, then\nperforms point-level refinement by jointly incorporating both the continuity of\npassed lanes and future motion features. Extensive experiments on two large\ndatasets, nuScenes and Argoverse, show that our network exhibits significant\nperformance gains over existing direct regression and goal-based algorithms."}
{"id": "2503.21237", "pdf": "https://arxiv.org/pdf/2503.21237", "abs": "https://arxiv.org/abs/2503.21237", "authors": ["Karanbir Singh", "William Ngu"], "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI."}
{"id": "2503.21483", "pdf": "https://arxiv.org/pdf/2503.21483", "abs": "https://arxiv.org/abs/2503.21483", "authors": ["Shuming Liu", "Chen Zhao", "Tianqi Xu", "Bernard Ghanem"], "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT."}
{"id": "2503.21394", "pdf": "https://arxiv.org/pdf/2503.21394", "abs": "https://arxiv.org/abs/2503.21394", "authors": ["Rifat Mehreen Amin", "Oliver Hans Kühle", "Daniel Buschek", "Andreas Butz"], "title": "Composable Prompting Workspaces for Creative Writing: Exploration and Iteration Using Dynamic Widgets", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "11 pages, 9 figures, 2 tables, ACM CHI 2025 LBW", "summary": "Generative AI models offer many possibilities for text creation and\ntransformation. Current graphical user interfaces (GUIs) for prompting them\nlack support for iterative exploration, as they do not represent prompts as\nactionable interface objects. We propose the concept of a composable prompting\ncanvas for text exploration and iteration using dynamic widgets. Users generate\nwidgets through system suggestions, prompting, or manually to capture\ntask-relevant facets that affect the generated text. In a comparative study\nwith a baseline (conversational UI), 18 participants worked on two writing\ntasks, creating diverse prompting environments with custom widgets and spatial\nlayouts. They reported having more control over the generated text and\npreferred our system over the baseline. Our design significantly outperformed\nthe baseline on the Creativity Support Index, and participants felt the results\nwere worth the effort. This work highlights the need for GUIs that support\nuser-driven customization and (re-)structuring to increase both the flexibility\nand efficiency of prompting."}
{"id": "2503.21486", "pdf": "https://arxiv.org/pdf/2503.21486", "abs": "https://arxiv.org/abs/2503.21486", "authors": ["Hamadi Chihaoui", "Paolo Favaro"], "title": "Invert2Restore: Zero-Shot Degradation-Blind Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Two of the main challenges of image restoration in real-world scenarios are\nthe accurate characterization of an image prior and the precise modeling of the\nimage degradation operator. Pre-trained diffusion models have been very\nsuccessfully used as image priors in zero-shot image restoration methods.\nHowever, how to best handle the degradation operator is still an open problem.\nIn real-world data, methods that rely on specific parametric assumptions about\nthe degradation model often face limitations in their applicability. To address\nthis, we introduce Invert2Restore, a zero-shot, training-free method that\noperates in both fully blind and partially blind settings -- requiring no prior\nknowledge of the degradation model or only partial knowledge of its parametric\nform without known parameters. Despite this, Invert2Restore achieves\nhigh-fidelity results and generalizes well across various types of image\ndegradation. It leverages a pre-trained diffusion model as a deterministic\nmapping between normal samples and undistorted image samples. The key insight\nis that the input noise mapped by a diffusion model to a degraded image lies in\na low-probability density region of the standard normal distribution. Thus, we\ncan restore the degraded image by carefully guiding its input noise toward a\nhigher-density region. We experimentally validate Invert2Restore across several\nimage restoration tasks, demonstrating that it achieves state-of-the-art\nperformance in scenarios where the degradation operator is either unknown or\npartially known."}
{"id": "2503.21557", "pdf": "https://arxiv.org/pdf/2503.21557", "abs": "https://arxiv.org/abs/2503.21557", "authors": ["Xingdi Yuan", "Morgane M Moss", "Charbel El Feghali", "Chinmay Singh", "Darya Moldavskaya", "Drew MacPhee", "Lucas Caccia", "Matheus Pereira", "Minseon Kim", "Alessandro Sordoni", "Marc-Alexandre Côté"], "title": "debug-gym: A Text-Based Environment for Interactive Debugging", "categories": ["cs.AI", "cs.CL", "cs.PL", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly relied upon for coding tasks,\nyet in most scenarios it is assumed that all relevant information can be either\naccessed in context or matches their training data. We posit that LLMs can\nbenefit from the ability to interactively explore a codebase to gather the\ninformation relevant to their task. To achieve this, we present a textual\nenvironment, namely debug-gym, for developing LLM-based agents in an\ninteractive coding setting. Our environment is lightweight and provides a\npreset of useful tools, such as a Python debugger (pdb), designed to facilitate\nan LLM-based agent's interactive debugging. Beyond coding and debugging tasks,\nthis approach can be generalized to other tasks that would benefit from\ninformation-seeking behavior by an LLM agent."}
{"id": "2503.21489", "pdf": "https://arxiv.org/pdf/2503.21489", "abs": "https://arxiv.org/abs/2503.21489", "authors": ["Edwin Tay", "Nazli Tümer", "Amir A. Zadpoor"], "title": "Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric Mapping to Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Living biological tissue is a complex system, constantly growing and changing\nin response to external and internal stimuli. These processes lead to\nremarkable and intricate changes in shape. Modeling and understanding both\nnatural and pathological (or abnormal) changes in the shape of anatomical\nstructures is highly relevant, with applications in diagnostic, prognostic, and\ntherapeutic healthcare. Nevertheless, modeling the longitudinal shape change of\nbiological tissue is a non-trivial task due to its inherent nonlinear nature.\nIn this review, we highlight several existing methodologies and tools for\nmodeling longitudinal shape change (i.e., spatiotemporal shape modeling). These\nmethods range from diffeomorphic metric mapping to deep-learning based\napproaches (e.g., autoencoders, generative networks, recurrent neural networks,\netc.). We discuss the synergistic combinations of existing technologies and\npotential directions for future research, underscoring key deficiencies in the\ncurrent research landscape."}
{"id": "2503.21657", "pdf": "https://arxiv.org/pdf/2503.21657", "abs": "https://arxiv.org/abs/2503.21657", "authors": ["Yi-Kai Zhang", "Jin Wang", "Xu-Xiang Zhong", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Model Assembly Learning with Heterogeneous Layer Weight Merging", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICLR 2025 Workshop on Neural Network Weights as a New Data Modality", "summary": "Model merging acquires general capabilities without extra data or training by\ncombining multiple models' parameters. Previous approaches achieve linear mode\nconnectivity by aligning parameters into the same loss basin using permutation\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\nparadigm for model merging that iteratively integrates parameters from diverse\nmodels in an open-ended model zoo to enhance the base model's capabilities.\nUnlike previous works that require identical architectures, MAL allows the\nmerging of heterogeneous architectures and selective parameters across layers.\nSpecifically, the base model can incorporate parameters from different layers\nof multiple pre-trained models. We systematically investigate the conditions\nand fundamental settings of heterogeneous parameter merging, addressing all\npossible mismatches in layer widths between the base and target models.\nFurthermore, we establish key laws and provide practical guidelines for\neffectively implementing MAL."}
{"id": "2503.21525", "pdf": "https://arxiv.org/pdf/2503.21525", "abs": "https://arxiv.org/abs/2503.21525", "authors": ["Yuxi Hu", "Jun Zhang", "Zhe Zhang", "Rafael Weilharter", "Yuchen Rao", "Kuangyi Chen", "Runze Yuan", "Friedrich Fraundorfer"], "title": "ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point\nclouds from a series of overlapping images. Recent learning-based MVS\nframeworks overlook the geometric information embedded in features and\ncorrelations, leading to weak cost matching. In this paper, we propose\nICG-MVSNet, which explicitly integrates intra-view and cross-view relationships\nfor depth estimation. Specifically, we develop an intra-view feature fusion\nmodule that leverages the feature coordinate correlations within a single image\nto enhance robust cost matching. Additionally, we introduce a lightweight\ncross-view aggregation module that efficiently utilizes the contextual\ninformation from volume correlations to guide regularization. Our method is\nevaluated on the DTU dataset and Tanks and Temples benchmark, consistently\nachieving competitive performance against state-of-the-art works, while\nrequiring lower computational resources."}
{"id": "2503.21683", "pdf": "https://arxiv.org/pdf/2503.21683", "abs": "https://arxiv.org/abs/2503.21683", "authors": ["Hui Wang"], "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced."}
{"id": "2503.21541", "pdf": "https://arxiv.org/pdf/2503.21541", "abs": "https://arxiv.org/abs/2503.21541", "authors": ["Achint Soni", "Meet Soni", "Sirisha Rambhatla"], "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\n\\method consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/"}
{"id": "2503.21704", "pdf": "https://arxiv.org/pdf/2503.21704", "abs": "https://arxiv.org/abs/2503.21704", "authors": ["Yan-Ying Chen", "Yue Weng", "Alexandre Filipowicz", "Rumen Iliev", "Francine Chen", "Shabnam Hakimi", "Yanxia Zhang", "Matthew Lee", "Kent Lyons", "Charlene Wu"], "title": "Learning to Represent Individual Differences for Choice Decision Making", "categories": ["cs.LG", "cs.CL"], "comment": "Published in IJCAI MRC 2022", "summary": "Human decision making can be challenging to predict because decisions are\naffected by a number of complex factors. Adding to this complexity,\ndecision-making processes can differ considerably between individuals, and\nmethods aimed at predicting human decisions need to take individual differences\ninto account. Behavioral science offers methods by which to measure individual\ndifferences (e.g., questionnaires, behavioral models), but these are often\nnarrowed down to low dimensions and not tailored to specific prediction tasks.\nThis paper investigates the use of representation learning to measure\nindividual differences from behavioral experiment data. Representation learning\noffers a flexible approach to create individual embeddings from data that are\nboth structured (e.g., demographic information) and unstructured (e.g., free\ntext), where the flexibility provides more options for individual difference\nmeasures for personalization, e.g., free text responses may allow for\nopen-ended questions that are less privacy-sensitive. In the current paper we\nuse representation learning to characterize individual differences in human\nperformance on an economic decision-making task. We demonstrate that models\nusing representation learning to capture individual differences consistently\nimprove decision predictions over models without representation learning, and\neven outperform well-known theory-based behavioral models used in these\nenvironments. Our results propose that representation learning offers a useful\nand flexible tool to capture individual differences."}
{"id": "2503.21562", "pdf": "https://arxiv.org/pdf/2503.21562", "abs": "https://arxiv.org/abs/2503.21562", "authors": ["Jonathan Lee", "Bolivar Solarte", "Chin-Hsuan Wu", "Jin-Cheng Jhang", "Fu-En Wang", "Yi-Hsuan Tsai", "Min Sun"], "title": "uLayout: Unified Room Layout Estimation for Perspective and Panoramic Images", "categories": ["cs.CV"], "comment": "Accepted to WACV-2025", "summary": "We present uLayout, a unified model for estimating room layout geometries\nfrom both perspective and panoramic images, whereas traditional solutions\nrequire different model designs for each image type. The key idea of our\nsolution is to unify both domains into the equirectangular projection,\nparticularly, allocating perspective images into the most suitable latitude\ncoordinate to effectively exploit both domains seamlessly. To address the\nField-of-View (FoV) difference between the input domains, we design uLayout\nwith a shared feature extractor with an extra 1D-Convolution layer to condition\neach domain input differently. This conditioning allows us to efficiently\nformulate a column-wise feature regression problem regardless of the FoV input.\nThis simple yet effective approach achieves competitive performance with\ncurrent state-of-the-art solutions and shows for the first time a single\nend-to-end model for both domains. Extensive experiments in the real-world\ndatasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence the\ncontribution of our approach. Code is available at\nhttps://github.com/JonathanLee112/uLayout."}
{"id": "2503.21708", "pdf": "https://arxiv.org/pdf/2503.21708", "abs": "https://arxiv.org/abs/2503.21708", "authors": ["Felix Stollenwerk"], "title": "Elementwise Layer Normalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 3 figures", "summary": "A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer\nNormalization. Although the method is empirically well-motivated and appealing\nfrom a practical point of view, it lacks a theoretical foundation. In this\nwork, we derive DyT mathematically and show that a well-defined approximation\nis needed to do so. By dropping said approximation, an alternative element-wise\ntransformation is obtained, which we call Elementwise Layer Normalization\n(ELN). We demonstrate that ELN resembles Layer Normalization more accurately\nthan DyT does."}
{"id": "2503.21566", "pdf": "https://arxiv.org/pdf/2503.21566", "abs": "https://arxiv.org/abs/2503.21566", "authors": ["Tongchao Luo", "Mingquan Qiu", "Zhenyu Wu", "Zebo Zhao", "Dingyou Zhang"], "title": "Bearing fault diagnosis based on multi-scale spectral images and convolutional neural network", "categories": ["cs.CV"], "comment": "12pages, 10 figures and 8 tables", "summary": "To address the challenges of low diagnostic accuracy in traditional bearing\nfault diagnosis methods, this paper proposes a novel fault diagnosis approach\nbased on multi-scale spectrum feature images and deep learning. Firstly, the\nvibration signal are preprocessed through mean removal and then converted to\nmulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novel\nfeature called multi-scale spectral image (MSSI) is constructed by multi-length\nspectrum paving scheme. Finally, a deep learning framework, convolutional\nneural network (CNN), is formulated to diagnose the bearing faults. Two\nexperimental cases are utilized to verify the effectiveness of the proposed\nmethod. Experimental results demonstrate that the proposed method significantly\nimproves the accuracy of fault diagnosis."}
{"id": "2503.21735", "pdf": "https://arxiv.org/pdf/2503.21735", "abs": "https://arxiv.org/abs/2503.21735", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems."}
{"id": "2503.21581", "pdf": "https://arxiv.org/pdf/2503.21581", "abs": "https://arxiv.org/abs/2503.21581", "authors": ["Liuyue Xie", "Jiancong Guo", "Ozan Cakmakci", "Andre Araujo", "Laszlo A. Jeni", "Zhiheng Jia"], "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets."}
{"id": "2503.21775", "pdf": "https://arxiv.org/pdf/2503.21775", "abs": "https://arxiv.org/abs/2503.21775", "authors": ["Ziyu Guo", "Young Yoon Lee", "Joseph Liu", "Yizhak Ben-Shabat", "Victor Zordan", "Mubbasir Kapadia"], "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://stylemotif.github.io", "summary": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io"}
{"id": "2503.21595", "pdf": "https://arxiv.org/pdf/2503.21595", "abs": "https://arxiv.org/abs/2503.21595", "authors": ["Jincheng Yan", "Yun Wang", "Xiaoyan Luo", "Yu-Wing Tai"], "title": "FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) plays a critical role in applications like\nsecurity surveillance and criminal investigations by matching individuals\nacross large image galleries captured by non-overlapping cameras. Traditional\nReID methods rely on unimodal inputs, typically images, but face limitations\ndue to challenges like occlusions, lighting changes, and pose variations. While\nadvancements in image-based and text-based ReID systems have been made, the\nintegration of both modalities has remained under-explored. This paper presents\nFusionSegReID, a multimodal model that combines both image and text inputs for\nenhanced ReID performance. By leveraging the complementary strengths of these\nmodalities, our model improves matching accuracy and robustness, particularly\nin complex, real-world scenarios where one modality may struggle. Our\nexperiments show significant improvements in Top-1 accuracy and mean Average\nPrecision (mAP) for ReID, as well as better segmentation results in challenging\nscenarios like occlusion and low-quality images. Ablation studies further\nconfirm that multimodal fusion and segmentation modules contribute to enhanced\nre-identification and mask accuracy. The results show that FusionSegReID\noutperforms traditional unimodal models, offering a more robust and flexible\nsolution for real-world person ReID tasks."}
{"id": "2503.21616", "pdf": "https://arxiv.org/pdf/2503.21616", "abs": "https://arxiv.org/abs/2503.21616", "authors": ["Jiahui Chen", "Yang Huan", "Runhua Shi", "Chanfan Ding", "Xiaoqi Mo", "Siyu Xiong", "Yinong He"], "title": "Audio-driven Gesture Generation via Deviation Feature in the Latent Space", "categories": ["cs.CV"], "comment": "6 pages, 5 figures", "summary": "Gestures are essential for enhancing co-speech communication, offering visual\nemphasis and complementing verbal interactions. While prior work has\nconcentrated on point-level motion or fully supervised data-driven methods, we\nfocus on co-speech gestures, advocating for weakly supervised learning and\npixel-level motion deviations. We introduce a weakly supervised framework that\nlearns latent representation deviations, tailored for co-speech gesture video\ngeneration. Our approach employs a diffusion model to integrate latent motion\nfeatures, enabling more precise and nuanced gesture representation. By\nleveraging weakly supervised deviations in latent space, we effectively\ngenerate hand gestures and mouth movements, crucial for realistic video\nproduction. Experiments show our method significantly improves video quality,\nsurpassing current state-of-the-art techniques."}
{"id": "2503.21622", "pdf": "https://arxiv.org/pdf/2503.21622", "abs": "https://arxiv.org/abs/2503.21622", "authors": ["Lars Heckler-Kram", "Jan-Hendrik Neudeck", "Ulla Scheler", "Rebecca König", "Carsten Steger"], "title": "The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": "paper under review; dataset first released for the VAND3.0 challenge\n  @ CVPR 2025 https://sites.google.com/view/vand30cvpr2025/challenge", "summary": "In recent years, performance on existing anomaly detection benchmarks like\nMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with\nstate-of-the-art models often competing in the range of less than one\npercentage point. This lack of discriminatory power prevents a meaningful\ncomparison of models and thus hinders progress of the field, especially when\nconsidering the inherent stochastic nature of machine learning results. We\npresent MVTec AD 2, a collection of eight anomaly detection scenarios with more\nthan 8000 high-resolution images. It comprises challenging and highly relevant\nindustrial inspection use cases that have not been considered in previous\ndatasets, including transparent and overlapping objects, dark-field and back\nlight illumination, objects with high variance in the normal data, and\nextremely small defects. We provide comprehensive evaluations of\nstate-of-the-art methods and show that their performance remains below 60%\naverage AU-PRO. Additionally, our dataset provides test scenarios with lighting\ncondition changes to assess the robustness of methods under real-world\ndistribution shifts. We host a publicly accessible evaluation server that holds\nthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).\nAll image data is available at\nhttps://www.mvtec.com/company/research/datasets/mvtec-ad-2."}
{"id": "2503.21659", "pdf": "https://arxiv.org/pdf/2503.21659", "abs": "https://arxiv.org/abs/2503.21659", "authors": ["Kuang Wu", "Chuan Yang", "Zhanbin Li"], "title": "InteractionMap: Improving Online Vectorized HDMap Construction with Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Vectorized high-definition (HD) maps are essential for an autonomous driving\nsystem. Recently, state-of-the-art map vectorization methods are mainly based\non DETR-like framework to generate HD maps in an end-to-end manner. In this\npaper, we propose InteractionMap, which improves previous map vectorization\nmethods by fully leveraging local-to-global information interaction in both\ntime and space. Firstly, we explore enhancing DETR-like detectors by explicit\nposition relation prior from point-level to instance-level, since map elements\ncontain strong shape priors. Secondly, we propose a key-frame-based\nhierarchical temporal fusion module, which interacts temporal information from\nlocal to global. Lastly, the separate classification branch and regression\nbranch lead to the problem of misalignment in the output distribution. We\ninteract semantic information with geometric information by introducing a novel\ngeometric-aware classification loss in optimization and a geometric-aware\nmatching cost in label assignment. InteractionMap achieves state-of-the-art\nperformance on both nuScenes and Argoverse2 benchmarks."}
{"id": "2503.21690", "pdf": "https://arxiv.org/pdf/2503.21690", "abs": "https://arxiv.org/abs/2503.21690", "authors": ["Nikin~Matharaarachchi", "Muhammad~Fermi Pasha", "Sonya~Coleman", "Kah PengWong"], "title": "CMED: A Child Micro-Expression Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions are short bursts of emotion that are difficult to hide.\nTheir detection in children is an important cue to assist psychotherapists in\nconducting better therapy. However, existing research on the detection of\nmicro-expressions has focused on adults, whose expressions differ in their\ncharacteristics from those of children. The lack of research is a direct\nconsequence of the lack of a child-based micro-expressions dataset as it is\nmuch more challenging to capture children's facial expressions due to the lack\nof predictability and controllability. This study compiles a dataset of\nspontaneous child micro-expression videos, the first of its kind, to the best\nof the authors knowledge. The dataset is captured in the wild using video\nconferencing software. This dataset enables us to then explore key features and\ndifferences between adult and child micro-expressions. This study also\nestablishes a baseline for the automated spotting and recognition of\nmicro-expressions in children using three approaches comprising of hand-created\nand learning-based approaches."}
{"id": "2503.21692", "pdf": "https://arxiv.org/pdf/2503.21692", "abs": "https://arxiv.org/abs/2503.21692", "authors": ["Daniel Bermuth", "Alexander Poeppel", "Wolfgang Reif"], "title": "RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond", "categories": ["cs.CV"], "comment": null, "summary": "The integration of multi-view imaging and pose estimation represents a\nsignificant advance in computer vision applications, offering new possibilities\nfor understanding human movement and interactions. This work presents a new\nalgorithm that improves multi-view multi-person pose estimation, focusing on\nfast triangulation speeds and good generalization capabilities. The approach\nextends to whole-body pose estimation, capturing details from facial\nexpressions to finger movements across multiple individuals and viewpoints.\nAdaptability to different settings is demonstrated through strong performance\nacross unseen datasets and configurations. To support further progress in this\nfield, all of this work is publicly accessible."}
{"id": "2503.21695", "pdf": "https://arxiv.org/pdf/2503.21695", "abs": "https://arxiv.org/abs/2503.21695", "authors": ["Jiahe Qian", "Yaoyu Fang", "Jinkui Hao", "Bo Zhou"], "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 tables, 2 figures", "summary": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches."}
{"id": "2503.21721", "pdf": "https://arxiv.org/pdf/2503.21721", "abs": "https://arxiv.org/abs/2503.21721", "authors": ["Jaywon Koo", "Jefferson Hernandez", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance", "categories": ["cs.CV"], "comment": null, "summary": "Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix."}
{"id": "2503.21723", "pdf": "https://arxiv.org/pdf/2503.21723", "abs": "https://arxiv.org/abs/2503.21723", "authors": ["Mallika Garg", "Debashis Ghosh", "Pyari Mohan Pradhan"], "title": "OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted in NATIONAL CONFERENCE ON COMMUNICATIONS (NCC) 2025", "summary": "Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets."}
{"id": "2503.21732", "pdf": "https://arxiv.org/pdf/2503.21732", "abs": "https://arxiv.org/abs/2503.21732", "authors": ["Xianglong He", "Zi-Xin Zou", "Chia-Hao Chen", "Yuan-Chen Guo", "Ding Liang", "Chun Yuan", "Wanli Ouyang", "Yan-Pei Cao", "Yangguang Li"], "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling", "categories": ["cs.CV"], "comment": "Project page: https://xianglonghe.github.io/TripoSF", "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to $1024^3$\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling."}
{"id": "2503.21745", "pdf": "https://arxiv.org/pdf/2503.21745", "abs": "https://arxiv.org/abs/2503.21745", "authors": ["Yuhan Zhang", "Mengchen Zhang", "Tong Wu", "Tengfei Wang", "Gordon Wetzstein", "Dahua Lin", "Ziwei Liu"], "title": "3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications."}
{"id": "2503.21747", "pdf": "https://arxiv.org/pdf/2503.21747", "abs": "https://arxiv.org/abs/2503.21747", "authors": ["Aniket Didolkar", "Andrii Zadaianchuk", "Rabiul Awal", "Maximilian Seitzer", "Efstratios Gavves", "Aishwarya Agrawal"], "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at CVPR 2025", "summary": "Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering."}
{"id": "2503.21749", "pdf": "https://arxiv.org/pdf/2503.21749", "abs": "https://arxiv.org/abs/2503.21749", "authors": ["Shitian Zhao", "Qilong Wu", "Xinyue Li", "Bo Zhang", "Ming Li", "Qi Qin", "Dongyang Liu", "Kaipeng Zhang", "Hongsheng Li", "Yu Qiao", "Peng Gao", "Bin Fu", "Zhen Li"], "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis", "categories": ["cs.CV"], "comment": "Project page: https://zhaoshitian.github.io/lexart/", "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable."}
{"id": "2503.21751", "pdf": "https://arxiv.org/pdf/2503.21751", "abs": "https://arxiv.org/abs/2503.21751", "authors": ["Yan Xia", "Xiaowei Zhou", "Etienne Vouga", "Qixing Huang", "Georgios Pavlakos"], "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Webpage: https://isshikihugh.github.io/HSMR/", "summary": "In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https://isshikihugh.github.io/HSMR/"}
{"id": "2503.21755", "pdf": "https://arxiv.org/pdf/2503.21755", "abs": "https://arxiv.org/abs/2503.21755", "authors": ["Dian Zheng", "Ziqi Huang", "Hongbo Liu", "Kai Zou", "Yinan He", "Fan Zhang", "Yuanhan Zhang", "Jingwen He", "Wei-Shi Zheng", "Yu Qiao", "Ziwei Liu"], "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness", "categories": ["cs.CV"], "comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench", "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."}
{"id": "2503.21757", "pdf": "https://arxiv.org/pdf/2503.21757", "abs": "https://arxiv.org/abs/2503.21757", "authors": ["Adrian Bulat", "Yassine Ouali", "Georgios Tzimiropoulos"], "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality."}
{"id": "2503.21758", "pdf": "https://arxiv.org/pdf/2503.21758", "abs": "https://arxiv.org/abs/2503.21758", "authors": ["Qi Qin", "Le Zhuo", "Yi Xin", "Ruoyi Du", "Zhen Li", "Bin Fu", "Yiting Lu", "Jiakang Yuan", "Xinyue Li", "Dongyang Liu", "Xiangyang Zhu", "Manyuan Zhang", "Will Beddow", "Erwann Millon", "Victor Perez", "Wenhai Wang", "Conghui He", "Bo Zhang", "Xiaohong Liu", "Hongsheng Li", "Yu Qiao", "Chang Xu", "Peng Gao"], "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework", "categories": ["cs.CV"], "comment": "Tech Report, 21 pages, 12 figures", "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0."}
{"id": "2503.21761", "pdf": "https://arxiv.org/pdf/2503.21761", "abs": "https://arxiv.org/abs/2503.21761", "authors": ["David Yifan Yao", "Albert J. Zhai", "Shenlong Wang"], "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025. Project page (with code):\n  https://davidyao99.github.io/uni4d", "summary": "This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding."}
{"id": "2503.21765", "pdf": "https://arxiv.org/pdf/2503.21765", "abs": "https://arxiv.org/abs/2503.21765", "authors": ["Minghui Lin", "Xiang Wang", "Yishan Wang", "Shu Wang", "Fengqi Dai", "Pengxiang Ding", "Cunxiang Wang", "Zhengrong Zuo", "Nong Sang", "Siteng Huang", "Donglin Wang"], "title": "Exploring the Evolution of Physics Cognition in Video Generation: A Survey", "categories": ["cs.CV"], "comment": "A comprehensive list of papers studied in this survey is available at\n  https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation", "summary": "Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''."}
{"id": "2503.21766", "pdf": "https://arxiv.org/pdf/2503.21766", "abs": "https://arxiv.org/abs/2503.21766", "authors": ["Haolin Liu", "Xiaohang Zhan", "Zizheng Yan", "Zhongjin Luo", "Yuxin Wen", "Xiaoguang Han"], "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/", "summary": "Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results."}
{"id": "2503.21767", "pdf": "https://arxiv.org/pdf/2503.21767", "abs": "https://arxiv.org/abs/2503.21767", "authors": ["Hairong Yin", "Huangying Zhan", "Yi Xu", "Raymond A. Yeh"], "title": "Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary querying in 3D Gaussian Splatting aims to identify\nsemantically relevant regions within a 3D Gaussian representation based on a\ngiven text query. Prior work, such as LangSplat, addressed this task by\nretrieving these regions in the form of segmentation masks on 2D renderings.\nMore recently, OpenGaussian introduced point-level querying, which directly\nselects a subset of 3D Gaussians. In this work, we propose a point-level\nquerying method that builds upon LangSplat's framework. Our approach improves\nthe framework in two key ways: (a) we leverage masklets from the Segment\nAnything Model 2 (SAM2) to establish semantic consistent ground-truth for\ndistilling the language Gaussians; (b) we introduces a novel two-step querying\napproach that first retrieves the distilled ground-truth and subsequently uses\nthe ground-truth to query the individual Gaussians. Experimental evaluations on\nthree benchmark datasets demonstrate that the proposed method achieves better\nperformance compared to state-of-the-art approaches. For instance, our method\nachieves an mIoU improvement of +20.42 on the 3D-OVS dataset."}
{"id": "2503.21770", "pdf": "https://arxiv.org/pdf/2503.21770", "abs": "https://arxiv.org/abs/2503.21770", "authors": ["Anand Bhattad", "Konpat Preechakul", "Alexei A. Efros"], "title": "Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting", "categories": ["cs.CV"], "comment": "project page: https://visualjenga.github.io/", "summary": "This paper proposes a novel scene understanding task called Visual Jenga.\nDrawing inspiration from the game Jenga, the proposed task involves\nprogressively removing objects from a single image until only the background\nremains. Just as Jenga players must understand structural dependencies to\nmaintain tower stability, our task reveals the intrinsic relationships between\nscene elements by systematically exploring which objects can be removed while\npreserving scene coherence in both physical and geometric sense. As a starting\npoint for tackling the Visual Jenga task, we propose a simple, data-driven,\ntraining-free approach that is surprisingly effective on a range of real-world\nimages. The principle behind our approach is to utilize the asymmetry in the\npairwise relationships between objects within a scene and employ a large\ninpainting model to generate a set of counterfactuals to quantify the\nasymmetry."}
{"id": "2503.21771", "pdf": "https://arxiv.org/pdf/2503.21771", "abs": "https://arxiv.org/abs/2503.21771", "authors": ["Hongkai Lin", "Dingkang Liang", "Zhenghao Qi", "Xiang Bai"], "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. The code is available at https:\n  //github.com/HongkLin/TIDE", "summary": "Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: //github.com/HongkLin/TIDE."}
{"id": "2503.21772", "pdf": "https://arxiv.org/pdf/2503.21772", "abs": "https://arxiv.org/abs/2503.21772", "authors": ["Zilin Xiao", "Pavel Suma", "Ayush Sachdeva", "Hao-Jen Wang", "Giorgos Kordopatis-Zilos", "Giorgos Tolias", "Vicente Ordonez"], "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers."}
{"id": "2503.21774", "pdf": "https://arxiv.org/pdf/2503.21774", "abs": "https://arxiv.org/abs/2503.21774", "authors": ["Jianning Pei", "Han Hu", "Shuyang Gu"], "title": "Optimal Stepsize for Diffusion Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps."}
{"id": "2503.21775", "pdf": "https://arxiv.org/pdf/2503.21775", "abs": "https://arxiv.org/abs/2503.21775", "authors": ["Ziyu Guo", "Young Yoon Lee", "Joseph Liu", "Yizhak Ben-Shabat", "Victor Zordan", "Mubbasir Kapadia"], "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://stylemotif.github.io", "summary": "We present StyleMotif, a novel Stylized Motion Latent Diffusion model,\ngenerating motion conditioned on both content and style from multiple\nmodalities. Unlike existing approaches that either focus on generating diverse\nmotion content or transferring style from sequences, StyleMotif seamlessly\nsynthesizes motion across a wide range of content while incorporating stylistic\ncues from multi-modal inputs, including motion, text, image, video, and audio.\nTo achieve this, we introduce a style-content cross fusion mechanism and align\na style encoder with a pre-trained multi-modal model, ensuring that the\ngenerated motion accurately captures the reference style while preserving\nrealism. Extensive experiments demonstrate that our framework surpasses\nexisting methods in stylized motion generation and exhibits emergent\ncapabilities for multi-modal motion stylization, enabling more nuanced motion\nsynthesis. Source code and pre-trained models will be released upon acceptance.\nProject Page: https://stylemotif.github.io"}
{"id": "2503.21776", "pdf": "https://arxiv.org/pdf/2503.21776", "abs": "https://arxiv.org/abs/2503.21776", "authors": ["Kaituo Feng", "Kaixiong Gong", "Bohao Li", "Zonghao Guo", "Yibing Wang", "Tianshuo Peng", "Benyou Wang", "Xiangyu Yue"], "title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "categories": ["cs.CV"], "comment": "Project page: https://github.com/tulerfeng/Video-R1", "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released."}
{"id": "2503.21777", "pdf": "https://arxiv.org/pdf/2503.21777", "abs": "https://arxiv.org/abs/2503.21777", "authors": ["Jiahao Xie", "Alessio Tonioni", "Nathalie Rauschmayr", "Federico Tombari", "Bernt Schiele"], "title": "Test-Time Visual In-Context Tuning", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/Jiahao000/VICT", "summary": "Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT."}
{"id": "2503.21778", "pdf": "https://arxiv.org/pdf/2503.21778", "abs": "https://arxiv.org/abs/2503.21778", "authors": ["Ziren Gong", "Fabio Tosi", "Youmin Zhang", "Stefano Mattoccia", "Matteo Poggi"], "title": "HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM", "categories": ["cs.CV"], "comment": "ICRA 2025. Project Page: https://zorangong.github.io/HS-SLAM/", "summary": "NeRF-based SLAM has recently achieved promising results in tracking and\nreconstruction. However, existing methods face challenges in providing\nsufficient scene representation, capturing structural information, and\nmaintaining global consistency in scenes emerging significant movement or being\nforgotten. To this end, we present HS-SLAM to tackle these problems. To enhance\nscene representation capacity, we propose a hybrid encoding network that\ncombines the complementary strengths of hash-grid, tri-planes, and one-blob,\nimproving the completeness and smoothness of reconstruction. Additionally, we\nintroduce structural supervision by sampling patches of non-local pixels rather\nthan individual rays to better capture the scene structure. To ensure global\nconsistency, we implement an active global bundle adjustment (BA) to eliminate\ncamera drifts and mitigate accumulative errors. Experimental results\ndemonstrate that HS-SLAM outperforms the baselines in tracking and\nreconstruction accuracy while maintaining the efficiency required for robotics."}
{"id": "2503.21779", "pdf": "https://arxiv.org/pdf/2503.21779", "abs": "https://arxiv.org/abs/2503.21779", "authors": ["Weihao Yu", "Yuanhao Cai", "Ruyi Zha", "Zhiwen Fan", "Chenxin Li", "Yixuan Yuan"], "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction", "categories": ["cs.CV"], "comment": "Project Page: https://x2-gaussian.github.io/", "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/."}
{"id": "2503.21780", "pdf": "https://arxiv.org/pdf/2503.21780", "abs": "https://arxiv.org/abs/2503.21780", "authors": ["Reza Qorbani", "Gianluca Villani", "Theodoros Panagiotakopoulos", "Marc Botet Colomer", "Linus Härenstam-Nielsen", "Mattia Segu", "Pier Luigi Dovesi", "Jussi Karlgren", "Daniel Cremers", "Federico Tombari", "Matteo Poggi"], "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://thegoodailab.org/semla Code:\n  https://github.com/rezaqorbani/SemLA", "summary": "Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation."}
{"id": "2503.21781", "pdf": "https://arxiv.org/pdf/2503.21781", "abs": "https://arxiv.org/abs/2503.21781", "authors": ["Chi-Pin Huang", "Yen-Siang Wu", "Hung-Kai Chung", "Kai-Po Chang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page:\n  https://jasper0314-huang.github.io/videomage-customization", "summary": "Customized text-to-video generation aims to produce high-quality videos that\nincorporate user-specified subject identities or motion patterns. However,\nexisting methods mainly focus on personalizing a single concept, either subject\nidentity or motion pattern, limiting their effectiveness for multiple subjects\nwith the desired motion patterns. To tackle this challenge, we propose a\nunified framework VideoMage for video customization over both multiple subjects\nand their interactive motions. VideoMage employs subject and motion LoRAs to\ncapture personalized content from user-provided images and videos, along with\nan appearance-agnostic motion learning approach to disentangle motion patterns\nfrom visual appearance. Furthermore, we develop a spatial-temporal composition\nscheme to guide interactions among subjects within the desired motion patterns.\nExtensive experiments demonstrate that VideoMage outperforms existing methods,\ngenerating coherent, user-controlled videos with consistent subject identities\nand interactions."}
{"id": "2503.21782", "pdf": "https://arxiv.org/pdf/2503.21782", "abs": "https://arxiv.org/abs/2503.21782", "authors": ["Abdelrahman Shaker", "Muhammad Maaz", "Chenhui Gou", "Hamid Rezatofighi", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model", "categories": ["cs.CV"], "comment": "Technical Report. Project Page:\n  https://amshaker.github.io/Mobile-VideoGPT", "summary": "Video understanding models often struggle with high computational\nrequirements, extensive parameter counts, and slow inference speed, making them\ninefficient for practical use. To tackle these challenges, we propose\nMobile-VideoGPT, an efficient multimodal framework designed to operate with\nfewer than a billion parameters. Unlike traditional video large multimodal\nmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,\nefficient projectors, and a small language model (SLM), enabling real-time\nthroughput. To further improve efficiency, we present an Attention-Based Frame\nScoring mechanism to select the key-frames, along with an efficient token\nprojector that prunes redundant visual tokens and preserves essential\ncontextual cues. We evaluate our model across well-established six video\nunderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).\nOur results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per\nsecond while outperforming existing state-of-the-art 0.5B-parameter models by 6\npoints on average with 40% fewer parameters and more than 2x higher throughput.\nOur code and models are publicly available at:\nhttps://github.com/Amshaker/Mobile-VideoGPT."}
{"id": "2503.20808", "pdf": "https://arxiv.org/pdf/2503.20808", "abs": "https://arxiv.org/abs/2503.20808", "authors": ["Xiaoming Qi", "Jingyang Zhang", "Huazhu Fu", "Guanyu Yang", "Shuo Li", "Yueming Jin"], "title": "Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for Federated Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (FedDAH). It is to facilitate collaborative\nlearning under the distinct and dynamic task streams across clients. To\nalleviate the catastrophic forgetting, we propose a dynamic allocation\nhypernetwork (DAHyper) where a continually updated hypernetwork is designed to\nmanage the mapping between task identities and their associated model\nparameters, enabling the dynamic allocation of the model across clients. For\nthe biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https://github.com/jinlab-imvr/FedDAH."}
{"id": "2503.20846", "pdf": "https://arxiv.org/pdf/2503.20846", "abs": "https://arxiv.org/abs/2503.20846", "authors": ["Viktor Schlegel", "Anil A Bharath", "Zilong Zhao", "Kevin Yee"], "title": "Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead", "categories": ["cs.CR", "cs.CL", "cs.CV"], "comment": "23 pages + references + Appendix. Preprint", "summary": "Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential."}
{"id": "2503.20998", "pdf": "https://arxiv.org/pdf/2503.20998", "abs": "https://arxiv.org/abs/2503.20998", "authors": ["Youngkyoon Jang", "Eduardo Pérez-Pellitero"], "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to CVPR 2025, Mistakenly submitted as a replacement for\n  arXiv:2402.11057", "summary": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to\nrecover underrepresented sparse regions in sparse novel view synthesis. CoMapGS\naddresses both high- and low-uncertainty regions by constructing covisibility\nmaps, enhancing initial point clouds, and applying uncertainty-aware weighted\nsupervision using a proximity classifier. Our contributions are threefold: (1)\nCoMapGS reframes novel view synthesis by leveraging covisibility maps as a core\ncomponent to address region-specific uncertainty; (2) Enhanced initial point\nclouds for both low- and high-uncertainty regions compensate for sparse\nCOLMAP-derived point clouds, improving reconstruction quality and benefiting\nfew-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based\nweighting and proximity classification achieves consistent performance gains\nacross scenes with varying sparsity scores derived from covisibility maps.\nExperimental results demonstrate that CoMapGS outperforms state-of-the-art\nmethods on datasets including Mip-NeRF 360 and LLFF."}
{"id": "2503.21054", "pdf": "https://arxiv.org/pdf/2503.21054", "abs": "https://arxiv.org/abs/2503.21054", "authors": ["Yiqing Shen", "Chenjia Li", "Bohan Liu", "Cheng-Yi Li", "Tito Porras", "Mathias Unberath"], "title": "Operating Room Workflow Analysis via Reasoning Segmentation over Digital Twins", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Analyzing operating room (OR) workflows to derive quantitative insights into\nOR efficiency is important for hospitals to maximize patient care and financial\nsustainability. Prior work on OR-level workflow analysis has relied on\nend-to-end deep neural networks. While these approaches work well in\nconstrained settings, they are limited to the conditions specified at\ndevelopment time and do not offer the flexibility necessary to accommodate the\nOR workflow analysis needs of various OR scenarios (e.g., large academic center\nvs. rural provider) without data collection, annotation, and retraining.\nReasoning segmentation (RS) based on foundation models offers this flexibility\nby enabling automated analysis of OR workflows from OR video feeds given only\nan implicit text query related to the objects of interest. Due to the reliance\non large language model (LLM) fine-tuning, current RS approaches struggle with\nreasoning about semantic/spatial relationships and show limited generalization\nto OR video due to variations in visual characteristics and domain-specific\nterminology. To address these limitations, we first propose a novel digital\ntwin (DT) representation that preserves both semantic and spatial relationships\nbetween the various OR components. Then, building on this foundation, we\npropose ORDiRS (Operating Room Digital twin representation for Reasoning\nSegmentation), an LLM-tuning-free RS framework that reformulates RS into a\n\"reason-retrieval-synthesize\" paradigm. Finally, we present ORDiRS-Agent, an\nLLM-based agent that decomposes OR workflow analysis queries into manageable RS\nsub-queries and generates responses by combining detailed textual explanations\nwith supporting visual evidence from RS. Experimental results on both an\nin-house and a public OR dataset demonstrate that our ORDiRS achieves a cIoU\nimprovement of 6.12%-9.74% compared to the existing state-of-the-arts."}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25."}
{"id": "2503.21130", "pdf": "https://arxiv.org/pdf/2503.21130", "abs": "https://arxiv.org/abs/2503.21130", "authors": ["Saelyne Yang", "Anh Truong", "Juho Kim", "Dingzeyu Li"], "title": "VideoMix: Aggregating How-To Videos for Task-Oriented Learning", "categories": ["cs.HC", "cs.CV"], "comment": "In Proceedings of the 30th International Conference on Intelligent\n  User Interfaces (IUI '25) 2025", "summary": "Tutorial videos are a valuable resource for people looking to learn new\ntasks. People often learn these skills by viewing multiple tutorial videos to\nget an overall understanding of a task by looking at different approaches to\nachieve the task. However, navigating through multiple videos can be\ntime-consuming and mentally demanding as these videos are scattered and not\neasy to skim. We propose VideoMix, a system that helps users gain a holistic\nunderstanding of a how-to task by aggregating information from multiple videos\non the task. Insights from our formative study (N=12) reveal that learners\nvalue understanding potential outcomes, required materials, alternative\nmethods, and important details shared by different videos. Powered by a\nVision-Language Model pipeline, VideoMix extracts and organizes this\ninformation, presenting concise textual summaries alongside relevant video\nclips, enabling users to quickly digest and navigate the content. A comparative\nuser study (N=12) demonstrated that VideoMix enabled participants to gain a\nmore comprehensive understanding of tasks with greater efficiency than a\nbaseline video interface, where videos are viewed independently. Our findings\nhighlight the potential of a task-oriented, multi-video approach where videos\nare organized around a shared goal, offering an enhanced alternative to\nconventional video-based learning."}
{"id": "2503.21193", "pdf": "https://arxiv.org/pdf/2503.21193", "abs": "https://arxiv.org/abs/2503.21193", "authors": ["Hongxuan Tang", "Hao Liu", "Xinyan Xiao"], "title": "UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We introduce UGen, a unified autoregressive multimodal model that\ndemonstrates strong performance across text processing, image understanding,\nand image generation tasks simultaneously. UGen converts both texts and images\ninto discrete token sequences and utilizes a single transformer to generate\nthem uniformly in an autoregressive manner. To address the challenges\nassociated with unified multimodal learning, UGen is trained using a novel\nmechanism, namely progressive vocabulary learning. In this process, visual\ntoken IDs are incrementally activated and integrated into the training phase,\nultimately enhancing the effectiveness of unified multimodal learning.\nExperiments on comprehensive text and image tasks show that UGen achieves a\nsignificant overall performance improvement of 13.3% compared to the vanilla\nunified autoregressive method, and it also delivers competitive results across\nall tasks against several task-specific models."}
{"id": "2503.21197", "pdf": "https://arxiv.org/pdf/2503.21197", "abs": "https://arxiv.org/abs/2503.21197", "authors": ["Bingyan Xie", "Yongpeng Wu", "Yuxuan Shi", "Biqian Feng", "Wenjun Zhang", "Jihong Park", "Tony Q. S. Quek"], "title": "WVSC: Wireless Video Semantic Communication with Multi-frame Compensation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Existing wireless video transmission schemes directly conduct video coding in\npixel level, while neglecting the inner semantics contained in videos. In this\npaper, we propose a wireless video semantic communication framework,\nabbreviated as WVSC, which integrates the idea of semantic communication into\nwireless video transmission scenarios. WVSC first encodes original video frames\nas semantic frames and then conducts video coding based on such compact\nrepresentations, enabling the video coding in semantic level rather than pixel\nlevel. Moreover, to further reduce the communication overhead, a reference\nsemantic frame is introduced to substitute motion vectors of each frame in\ncommon video coding methods. At the receiver, multi-frame compensation (MFC) is\nproposed to produce compensated current semantic frame with a multi-frame\nfusion attention module. With both the reference frame transmission and MFC,\nthe bandwidth efficiency improves with satisfying video transmission\nperformance. Experimental results verify the performance gain of WVSC over\nother DL-based methods e.g. DVSC about 1 dB and traditional schemes about 2 dB\nin terms of PSNR."}
{"id": "2503.21242", "pdf": "https://arxiv.org/pdf/2503.21242", "abs": "https://arxiv.org/abs/2503.21242", "authors": ["Bashar Tahir", "Philipp Svoboda", "Markus Rupp"], "title": "PLAIN: Scalable Estimation Architecture for Integrated Sensing and Communication", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to the IEEE Transactions on Wireless Communications. Code\n  available at GitHub: https://github.com/bashar-tahir/plain", "summary": "Integrated sensing and communication (ISAC) is envisioned be to one of the\nparadigms upon which next-generation mobile networks will be built, extending\nlocalization and tracking capabilities, as well as giving birth to\nenvironment-aware wireless access. A key aspect of sensing integration is\nparameter estimation, which involves extracting information about the\nsurrounding environment, such as the direction, distance, and velocity of\nvarious objects within. This is typically of a high-dimensional nature, which\nleads to significant computational complexity, if performed jointly across\nmultiple sensing dimensions, such as space, frequency, and time. Additionally,\ndue to the incorporation of sensing on top of the data transmission, the time\nwindow available for sensing is likely to be short, resulting in an estimation\nproblem where only a single snapshot is accessible. In this work, we propose\nPLAIN, a tensor-based estimation architecture that flexibly scales with\nmultiple sensing dimensions and can handle high dimensionality, limited\nmeasurement time, and super-resolution requirements. It consists of three\nstages: a compression stage, where the high dimensional input is converted into\nlower dimensionality, without sacrificing resolution; a decoupled estimation\nstage, where the parameters across the different dimensions are estimated in\nparallel with low complexity; an input-based fusion stage, where the decoupled\nparameters are fused together to form a paired multidimensional estimate. We\ninvestigate the performance of the architecture for different configurations\nand compare it against practical sequential and joint estimation baselines, as\nwell as theoretical bounds. Our results show that PLAIN, using tools from\ntensor algebra, subspace-based processing, and compressed sensing, can scale\nflexibly with dimensionality, while operating with low complexity and\nmaintaining super-resolution."}
{"id": "2503.21397", "pdf": "https://arxiv.org/pdf/2503.21397", "abs": "https://arxiv.org/abs/2503.21397", "authors": ["Erik Wallin", "Fredrik Kahl", "Lars Hammarstrand"], "title": "ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification via Multi-Depth Networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "CVPR2025", "summary": "Out-of-distribution (OOD) detection in deep learning has traditionally been\nframed as a binary task, where samples are either classified as belonging to\nthe known classes or marked as OOD, with little attention given to the semantic\nrelationships between OOD samples and the in-distribution (ID) classes. We\npropose a framework for detecting and classifying OOD samples in a given class\nhierarchy. Specifically, we aim to predict OOD data to their correct internal\nnodes of the class hierarchy, whereas the known ID classes should be predicted\nas their corresponding leaf nodes. Our approach leverages the class hierarchy\nto create a probabilistic model and we implement this model by using networks\ntrained for ID classification at multiple hierarchy depths. We conduct\nexperiments on three datasets with predefined class hierarchies and show the\neffectiveness of our method. Our code is available at\nhttps://github.com/walline/prohoc."}
{"id": "2503.21425", "pdf": "https://arxiv.org/pdf/2503.21425", "abs": "https://arxiv.org/abs/2503.21425", "authors": ["Yongxu Wang", "Xu Cao", "Weiyun Yi", "Zhaoxin Fan"], "title": "STAMICS: Splat, Track And Map with Integrated Consistency and Semantics for Dense RGB-D SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,\nenabling systems to autonomously navigate and understand complex environments.\nCurrent SLAM approaches predominantly rely on geometric cues for mapping and\nlocalization, but they often fail to ensure semantic consistency, particularly\nin dynamic or densely populated scenes. To address this limitation, we\nintroduce STAMICS, a novel method that integrates semantic information with 3D\nGaussian representations to enhance both localization and mapping accuracy.\nSTAMICS consists of three key components: a 3D Gaussian-based scene\nrepresentation for high-fidelity reconstruction, a graph-based clustering\ntechnique that enforces temporal semantic consistency, and an open-vocabulary\nsystem that allows for the classification of unseen objects. Extensive\nexperiments show that STAMICS significantly improves camera pose estimation and\nmap quality, outperforming state-of-the-art methods while reducing\nreconstruction errors. Code will be public available."}
{"id": "2503.21442", "pdf": "https://arxiv.org/pdf/2503.21442", "abs": "https://arxiv.org/abs/2503.21442", "authors": ["Qiyu Dai", "Xingyu Ni", "Qianfan Shen", "Wenzheng Chen", "Baoquan Chen", "Mengyu Chu"], "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We consider the problem of adding dynamic rain effects to in-the-wild scenes\nin a physically-correct manner. Recent advances in scene modeling have made\nsignificant progress, with NeRF and 3DGS techniques emerging as powerful tools\nfor reconstructing complex scenes. However, while effective for novel view\nsynthesis, these methods typically struggle with challenging scene editing\ntasks, such as physics-based rain simulation. In contrast, traditional\nphysics-based simulations can generate realistic rain effects, such as\nraindrops and splashes, but they often rely on skilled artists to carefully set\nup high-fidelity scenes. This process lacks flexibility and scalability,\nlimiting its applicability to broader, open-world environments. In this work,\nwe introduce RainyGS, a novel approach that leverages the strengths of both\nphysics-based modeling and 3DGS to generate photorealistic, dynamic rain\neffects in open-world scenes with physical accuracy. At the core of our method\nis the integration of physically-based raindrop and shallow water simulation\ntechniques within the fast 3DGS rendering framework, enabling realistic and\nefficient simulations of raindrop behavior, splashes, and reflections. Our\nmethod supports synthesizing rain effects at over 30 fps, offering users\nflexible control over rain intensity -- from light drizzles to heavy downpours.\nWe demonstrate that RainyGS performs effectively for both real-world outdoor\nscenes and large-scale driving scenarios, delivering more photorealistic and\nphysically-accurate rain effects compared to state-of-the-art methods. Project\npage can be found at https://pku-vcl-geometry.github.io/RainyGS/"}
{"id": "2503.21443", "pdf": "https://arxiv.org/pdf/2503.21443", "abs": "https://arxiv.org/abs/2503.21443", "authors": ["Felix Terhag", "Philipp Knechtges", "Achim Basermann", "Anja Bach", "Darius Gerlach", "Jens Tank", "Raúl Tempone"], "title": "Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI", "categories": ["stat.ME", "cs.CV", "math.PR", "math.ST", "stat.AP", "stat.TH", "62F15, 92C55, 62P10, 94A12, 62F07, 62K99", "G.3; J.3"], "comment": null, "summary": "Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology\nthat images the heart at up to 50 frames per second, offering insight into the\nrespiratory effects on the heartbeat. However, this method significantly\nincreases the number of images that must be segmented to derive critical health\nindicators. Although neural networks perform well on inner slices, predictions\non outer slices are often unreliable.\n  This work proposes sparse Bayesian learning (SBL) to predict the ventricular\nvolume on outer slices with minimal manual labeling to address this challenge.\nThe ventricular volume over time is assumed to be dominated by sparse\nfrequencies corresponding to the heart and respiratory rates. Moreover, SBL\nidentifies these sparse frequencies on well-segmented inner slices by\noptimizing hyperparameters via type -II likelihood, automatically pruning\nirrelevant components. The identified sparse frequencies guide the selection of\nouter slice images for labeling, minimizing posterior variance.\n  This work provides performance guarantees for the greedy algorithm. Testing\non patient data demonstrates that only a few labeled images are necessary for\naccurate volume prediction. The labeling procedure effectively avoids selecting\ninefficient images. Furthermore, the Bayesian approach provides uncertainty\nestimates, highlighting unreliable predictions (e.g., when choosing suboptimal\nlabels)."}
{"id": "2503.21469", "pdf": "https://arxiv.org/pdf/2503.21469", "abs": "https://arxiv.org/abs/2503.21469", "authors": ["Yuxiao Sun", "Yao Zhao", "Meiqin Liu", "Chao Yao", "Weisi Lin"], "title": "Embedding Compression Distortion in Video Coding for Machines", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Currently, video transmission serves not only the Human Visual System (HVS)\nfor viewing but also machine perception for analysis. However, existing codecs\nare primarily optimized for pixel-domain and HVS-perception metrics rather than\nthe needs of machine vision tasks. To address this issue, we propose a\nCompression Distortion Representation Embedding (CDRE) framework, which\nextracts machine-perception-related distortion representation and embeds it\ninto downstream models, addressing the information lost during compression and\nimproving task performance. Specifically, to better analyze the\nmachine-perception-related distortion, we design a compression-sensitive\nextractor that identifies compression degradation in the feature domain. For\nefficient transmission, a lightweight distortion codec is introduced to\ncompress the distortion information into a compact representation.\nSubsequently, the representation is progressively embedded into the downstream\nmodel, enabling it to be better informed about compression degradation and\nenhancing performance. Experiments across various codecs and downstream tasks\ndemonstrate that our framework can effectively boost the rate-task performance\nof existing codecs with minimal overhead in terms of bitrate, execution time,\nand number of parameters. Our codes and supplementary materials are released in\nhttps://github.com/Ws-Syx/CDRE/."}
{"id": "2503.21501", "pdf": "https://arxiv.org/pdf/2503.21501", "abs": "https://arxiv.org/abs/2503.21501", "authors": ["Brett Levac", "Ajil Jalal", "Kannan Ramchandran", "Jonathan I. Tamir"], "title": "Double Blind Imaging with Generative Modeling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Blind inverse problems in imaging arise from uncertainties in the system used\nto collect (noisy) measurements of images. Recovering clean images from these\nmeasurements typically requires identifying the imaging system, either\nimplicitly or explicitly. A common solution leverages generative models as\npriors for both the images and the imaging system parameters (e.g., a class of\npoint spread functions). To learn these priors in a straightforward manner\nrequires access to a dataset of clean images as well as samples of the imaging\nsystem. We propose an AmbientGAN-based generative technique to identify the\ndistribution of parameters in unknown imaging systems, using only unpaired\nclean images and corrupted measurements. This learned distribution can then be\nused in model-based recovery algorithms to solve blind inverse problems such as\nblind deconvolution. We successfully demonstrate our technique for learning\nGaussian blur and motion blur priors from noisy measurements and show their\nutility in solving blind deconvolution with diffusion posterior sampling."}
{"id": "2503.21504", "pdf": "https://arxiv.org/pdf/2503.21504", "abs": "https://arxiv.org/abs/2503.21504", "authors": ["Yuxue Hu", "Junsong Li", "Meixuan Chen", "Dongyu Su", "Tongguan Wang", "Ying Sha"], "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets."}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems."}
{"id": "2503.21510", "pdf": "https://arxiv.org/pdf/2503.21510", "abs": "https://arxiv.org/abs/2503.21510", "authors": ["Samuel Bilson", "Anna Pustogvar"], "title": "Uncertainty-aware Bayesian machine learning modelling of land cover classification", "categories": ["cs.LG", "cs.CV"], "comment": "31 pages, 10 figures", "summary": "Land cover classification involves the production of land cover maps, which\ndetermine the type of land through remote sensing imagery. Over recent years,\nsuch classification is being performed by machine learning classification\nmodels, which can give highly accurate predictions on land cover per pixel\nusing large quantities of input training data. However, such models do not\ncurrently take account of input measurement uncertainty, which is vital for\ntraceability in metrology. In this work we propose a Bayesian classification\nframework using generative modelling to take account of input measurement\nuncertainty. We take the specific case of Bayesian quadratic discriminant\nanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in\n2020 and 2021. We benchmark the performance of the model against more popular\nclassification models used in land cover maps such as random forests and neural\nnetworks. We find that such Bayesian models are more trustworthy, in the sense\nthat they are more interpretable, explicitly model the input measurement\nuncertainty, and maintain predictive performance of class probability outputs\nacross datasets of different years and sizes, whilst also being computationally\nefficient."}
{"id": "2503.21555", "pdf": "https://arxiv.org/pdf/2503.21555", "abs": "https://arxiv.org/abs/2503.21555", "authors": ["Hyunjun Lee", "Hyunsoo Lee", "Sookwan Han"], "title": "SyncSDE: A Probabilistic Framework for Diffusion Synchronization", "categories": ["cs.LG", "cs.CV", "cs.GR"], "comment": "Accepted to CVPR2025", "summary": "There have been many attempts to leverage multiple diffusion models for\ncollaborative generation, extending beyond the original domain. A prominent\napproach involves synchronizing multiple diffusion trajectories by mixing the\nestimated scores to artificially correlate the generation processes. However,\nexisting methods rely on naive heuristics, such as averaging, without\nconsidering task specificity. These approaches do not clarify why such methods\nwork and often fail when a heuristic suitable for one task is blindly applied\nto others. In this paper, we present a probabilistic framework for analyzing\nwhy diffusion synchronization works and reveal where heuristics should be\nfocused - modeling correlations between multiple trajectories and adapting them\nto each specific task. We further identify optimal correlation models per task,\nachieving better results than previous approaches that apply a single heuristic\nacross all tasks without justification."}
{"id": "2503.21634", "pdf": "https://arxiv.org/pdf/2503.21634", "abs": "https://arxiv.org/abs/2503.21634", "authors": ["Yassir Lairgi"], "title": "When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility."}
{"id": "2503.21668", "pdf": "https://arxiv.org/pdf/2503.21668", "abs": "https://arxiv.org/abs/2503.21668", "authors": ["Danaja Rutar", "Alva Markelius", "Konstantinos Voudouris", "José Hernández-Orallo", "Lucy Cheke"], "title": "Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts."}
{"id": "2503.21694", "pdf": "https://arxiv.org/pdf/2503.21694", "abs": "https://arxiv.org/abs/2503.21694", "authors": ["Zhiyuan Ma", "Xinyue Liang", "Rongyuan Wu", "Xiangyu Zhu", "Zhen Lei", "Lei Zhang"], "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025.\n  Code:https://github.com/theEricMa/TriplaneTurbo.\n  Demo:https://huggingface.co/spaces/ZhiyuanthePony/TriplaneTurbo", "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo."}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases."}
{"id": "2503.21699", "pdf": "https://arxiv.org/pdf/2503.21699", "abs": "https://arxiv.org/abs/2503.21699", "authors": ["Liuyue Xie", "George Z. Wei", "Avik Kuthiala", "Ce Zheng", "Ananya Bal", "Mosam Dabhi", "Liting Wen", "Taru Rustagi", "Ethan Lai", "Sushil Khyalia", "Rohan Choudhury", "Morteza Ziyadi", "Xu Zhang", "Hao Yang", "László A. Jeni"], "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": null, "summary": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence."}
