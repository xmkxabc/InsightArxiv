{"id": "2504.02860", "pdf": "https://arxiv.org/pdf/2504.02860", "abs": "https://arxiv.org/abs/2504.02860", "authors": ["Karthik Shivashankar"], "title": "Computer Vision and Deep Learning for 4D Augmented Reality", "categories": ["cs.CV", "cs.AI"], "comment": "My Master Thesis , University of Surrey 2019", "summary": "The prospect of 4D video in Extended Reality (XR) platform is huge and\nexciting, it opens a whole new way of human computer interaction and the way we\nperceive the reality and consume multimedia. In this thesis, we have shown that\nfeasibility of rendering 4D video in Microsoft mixed reality platform. This\nenables us to port any 3D performance capture from CVSSP into XR product like\nthe HoloLens device with relative ease. However, if the 3D model is too complex\nand is made up of millions of vertices, the data bandwidth required to port the\nmodel is a severe limitation with the current hardware and communication\nsystem. Therefore, in this project we have also developed a compact\nrepresentation of both shape and appearance of the 4d video sequence using deep\nlearning models to effectively learn the compact representation of 4D video\nsequence and reconstruct it without affecting the shape and appearance of the\nvideo sequence."}
{"id": "2504.02862", "pdf": "https://arxiv.org/pdf/2504.02862", "abs": "https://arxiv.org/abs/2504.02862", "authors": ["Sudong Wang", "Yunjian Zhang", "Yao Zhu", "Jianing Li", "Zizhe Wang", "Yanwei Liu", "Xiangyang Ji"], "title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are gradually becoming the foundation\nfor many artificial intelligence applications. However, understanding their\ninternal working mechanisms has continued to puzzle researchers, which in turn\nlimits the further enhancement of their capabilities. In this paper, we seek to\ninvestigate how multimodal knowledge evolves and eventually induces natural\nlanguages in LVLMs. We design a series of novel strategies for analyzing\ninternal knowledge within LVLMs, and delve into the evolution of multimodal\nknowledge from three levels, including single token probabilities, token\nprobability distributions, and feature encodings. In this process, we identify\ntwo key nodes in knowledge evolution: the critical layers and the mutation\nlayers, dividing the evolution process into three stages: rapid evolution,\nstabilization, and mutation. Our research is the first to reveal the trajectory\nof knowledge evolution in LVLMs, providing a fresh perspective for\nunderstanding their underlying mechanisms. Our codes are available at\nhttps://github.com/XIAO4579/Vlm-interpretability."}
{"id": "2504.02866", "pdf": "https://arxiv.org/pdf/2504.02866", "abs": "https://arxiv.org/abs/2504.02866", "authors": ["Xiucheng Liang", "Jinheng Xie", "Tianhong Zhao", "Rudi Stouffs", "Filip Biljecki"], "title": "OpenFACADES: An Open Framework for Architectural Caption and Attribute Data Enrichment via Street View Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Building properties, such as height, usage, and material composition, play a\ncrucial role in spatial data infrastructures, supporting applications such as\nenergy simulation, risk assessment, and environmental modeling. Despite their\nimportance, comprehensive and high-quality building attribute data remain\nscarce in many urban areas. Recent advances have enabled the extraction and\ntagging of objective building attributes using remote sensing and street-level\nimagery. However, establishing a method and pipeline that integrates diverse\nopen datasets, acquires holistic building imagery at scale, and infers\ncomprehensive building attributes remains a significant challenge. Among the\nfirst, this study bridges the gaps by introducing OpenFACADES, an open\nframework that leverages multimodal crowdsourced data to enrich building\nprofiles with both objective attributes and semantic descriptors through\nmultimodal large language models. Our methodology proceeds in three major\nsteps. First, we integrate street-level image metadata from Mapillary with\nOpenStreetMap geometries via isovist analysis, effectively identifying images\nthat provide suitable vantage points for observing target buildings. Second, we\nautomate the detection of building facades in panoramic imagery and tailor a\nreprojection approach to convert objects into holistic perspective views that\napproximate real-world observation. Third, we introduce an innovative approach\nthat harnesses and systematically investigates the capabilities of open-source\nlarge vision-language models (VLMs) for multi-attribute prediction and\nopen-vocabulary captioning in building-level analytics, leveraging a globally\nsourced dataset of 30,180 labeled images from seven cities. Evaluation shows\nthat fine-tuned VLM excel in multi-attribute inference, outperforming\nsingle-attribute computer vision models and zero-shot ChatGPT-4o."}
{"id": "2504.02876", "pdf": "https://arxiv.org/pdf/2504.02876", "abs": "https://arxiv.org/abs/2504.02876", "authors": ["Yangxiao Lu", "Ruosen Li", "Liqiang Jing", "Jikai Wang", "Xinya Du", "Yunhui Guo", "Nicholas Ruozzi", "Yu Xiang"], "title": "Multimodal Reference Visual Grounding", "categories": ["cs.CV", "cs.LG"], "comment": "Project page with our code and dataset:\n  https://irvlutd.github.io/MultiGrounding", "summary": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-7B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding.\nProject page with our code and dataset:\nhttps://irvlutd.github.io/MultiGrounding"}
{"id": "2504.02858", "pdf": "https://arxiv.org/pdf/2504.02858", "abs": "https://arxiv.org/abs/2504.02858", "authors": ["Evgenii Evstafev"], "title": "Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 4 figures", "summary": "Large language models (LLMs) demonstrate increasing capabilities in creative\ntext generation, yet systematic evaluations of their humor production remain\nunderexplored. This study presents a comprehensive analysis of 13\nstate-of-the-art LLMs across five architectural families, evaluating their\nperformance in generating technically relevant humor for software developers.\nThrough a full factorial design testing 715 unique configurations of\ntemperature settings and prompt variations, we assess model outputs using five\nweighted criteria: humor quality, domain relevance, concept originality, tone\nprecision, and delivery efficiency. Our methodology employs rigorous\nstatistical analysis including ANOVA, correlation studies, and quadratic\nregression to identify optimal configurations and architectural influences.\nResults reveal significant performance variations across models, with certain\narchitectures achieving 21.8% superiority over baseline systems. Temperature\nsensitivity analysis demonstrates that 73% of models achieve peak performance\nat lower stochasticity settings (<= 0.5), though optimal ranges vary\nsubstantially by architecture. We identify distinct model clusters: compact\nhigh-performers maintaining efficiency-quality balance versus verbose\nspecialists requiring longer outputs for marginal gains. Statistical validation\nconfirms model architecture explains 38.7% of performance variance, with\nsignificant correlations between humor quality and concept originality. The\nstudy establishes practical guidelines for model selection and configuration,\ndemonstrating how temperature adjustments and architectural considerations\nimpact humor generation effectiveness. These findings advance understanding of\nLLM capabilities in creative technical writing and provide empirically\nvalidated configuration strategies for developers implementing humor-generation\nsystems."}
{"id": "2504.02878", "pdf": "https://arxiv.org/pdf/2504.02878", "abs": "https://arxiv.org/abs/2504.02878", "authors": ["Lilin Xu", "Kaiyuan Hou", "Xiaofan Jiang"], "title": "Exploring the Capabilities of LLMs for IMU-based Fine-grained Human Activity Understanding", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to The 2nd International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys 2025)", "summary": "Human activity recognition (HAR) using inertial measurement units (IMUs)\nincreasingly leverages large language models (LLMs), yet existing approaches\nfocus on coarse activities like walking or running. Our preliminary study\nindicates that pretrained LLMs fail catastrophically on fine-grained HAR tasks\nsuch as air-written letter recognition, achieving only near-random guessing\naccuracy. In this work, we first bridge this gap for flat-surface writing\nscenarios: by fine-tuning LLMs with a self-collected dataset and few-shot\nlearning, we achieved up to a 129x improvement on 2D data. To extend this to 3D\nscenarios, we designed an encoder-based pipeline that maps 3D data into 2D\nequivalents, preserving the spatiotemporal information for robust letter\nprediction. Our end-to-end pipeline achieves 78% accuracy on word recognition\nwith up to 5 letters in mid-air writing scenarios, establishing LLMs as viable\ntools for fine-grained HAR."}
{"id": "2504.02863", "pdf": "https://arxiv.org/pdf/2504.02863", "abs": "https://arxiv.org/abs/2504.02863", "authors": ["Girma Yohannis Bade", "Zahra Ahani", "Olga Kolesnikova", "José Luis Oropeza", "Grigori Sidorov"], "title": "GS_DravidianLangTech@2025: Women Targeted Abusive Texts Detection on Social Media", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The increasing misuse of social media has become a concern; however,\ntechnological solutions are being developed to moderate its content\neffectively. This paper focuses on detecting abusive texts targeting women on\nsocial media platforms. Abusive speech refers to communication intended to harm\nor incite hatred against vulnerable individuals or groups. Specifically, this\nstudy aims to identify abusive language directed toward women. To achieve this,\nwe utilized logistic regression and BERT as base models to train datasets\nsourced from DravidianLangTech@2025 for Tamil and Malayalam languages. The\nmodels were evaluated on test datasets, resulting in a 0.729 macro F1 score for\nBERT and 0.6279 for logistic regression in Tamil and Malayalam, respectively."}
{"id": "2504.02884", "pdf": "https://arxiv.org/pdf/2504.02884", "abs": "https://arxiv.org/abs/2504.02884", "authors": ["Baba Ibrahim", "Zhou Kui"], "title": "Enhancing Traffic Sign Recognition On The Performance Based On Yolov8", "categories": ["cs.CV", "cs.PF"], "comment": "27 Pages, 6 Figures, 10 Tables and 20 References", "summary": "This paper Traffic sign recognition plays a crucial role in the development\nof autonomous vehicles and advanced driver-assistance systems (ADAS). Despite\nsignificant advances in deep learning and object detection, accurately\ndetecting and classifying traffic signs remains challenging due to their small\nsizes, variable environmental conditions, occlusion, and class imbalance. This\nthesis presents an enhanced YOLOv8-based detection system that integrates\nadvanced data augmentation techniques, novel architectural enhancements\nincluding Coordinate Attention (CA), Bidirectional Feature Pyramid Network\n(BiFPN), and dynamic modules such as ODConv and LSKA, along with refined loss\nfunctions (EIoU and WIoU combined with Focal Loss). Extensive experiments\nconducted on datasets including GTSRB, TT100K, and GTSDB demonstrate marked\nimprovements in detection accuracy, robustness under adverse conditions, and\nreal-time inference on edge devices. The findings contribute actionable\ninsights for deploying reliable traffic sign recognition systems in real-world\nautonomous driving scenarios."}
{"id": "2504.02864", "pdf": "https://arxiv.org/pdf/2504.02864", "abs": "https://arxiv.org/abs/2504.02864", "authors": ["Peter Adelson", "Julian Nyarko"], "title": "The Material Contracts Corpus", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Material Contracts Corpus (MCC), a publicly\navailable dataset comprising over one million contracts filed by public\ncompanies with the U.S. Securities and Exchange Commission (SEC) between 2000\nand 2023. The MCC facilitates empirical research on contract design and legal\nlanguage, and supports the development of AI-based legal tools. Contracts in\nthe corpus are categorized by agreement type and linked to specific parties\nusing machine learning and natural language processing techniques, including a\nfine-tuned LLaMA-2 model for contract classification. The MCC further provides\nmetadata such as filing form, document format, and amendment status. We\ndocument trends in contractual language, length, and complexity over time, and\nhighlight the dominance of employment and security agreements in SEC filings.\nThis resource is available for bulk download and online access at\nhttps://mcc.law.stanford.edu."}
{"id": "2504.02895", "pdf": "https://arxiv.org/pdf/2504.02895", "abs": "https://arxiv.org/abs/2504.02895", "authors": ["Farida Al Haddad", "Yuxin Wang", "Malcolm Mielle"], "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "Artificial intelligence has the potential to impact safety and efficiency in\nsafety-critical domains such as construction, manufacturing, and healthcare.\nFor example, using sensor data from wearable devices, such as inertial\nmeasurement units (IMUs), human gestures can be detected while maintaining\nprivacy, thereby ensuring that safety protocols are followed. However, strict\nsafety requirements in these domains have limited the adoption of AI, since\naccurate calibration of predicted probabilities and robustness against\nout-of-distribution (OOD) data is necessary.\n  This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step\nmethod to address these challenges in IMU-based gesture recognition. First, we\npresent an uncertainty-aware gesture network architecture that predicts both\ngesture probabilities and their associated uncertainties from IMU data. This\nuncertainty is then used to calibrate the probabilities of each potential\ngesture. Second, an entropy-weighted expectation of predictions over multiple\nIMU data windows is used to improve accuracy while maintaining correct\ncalibration.\n  Our method is evaluated using three publicly available IMU datasets for\ngesture detection and is compared to three state-of-the-art calibration methods\nfor neural networks: temperature scaling, entropy maximization, and Laplace\napproximation. UAC outperforms existing methods, achieving improved accuracy\nand calibration in both OOD and in-distribution scenarios. Moreover, we find\nthat, unlike our method, none of the state-of-the-art methods significantly\nimprove the calibration of IMU-based gesture recognition models. In conclusion,\nour work highlights the advantages of uncertainty-aware calibration of neural\nnetworks, demonstrating improvements in both calibration and accuracy for\ngesture detection using IMU data."}
{"id": "2504.02865", "pdf": "https://arxiv.org/pdf/2504.02865", "abs": "https://arxiv.org/abs/2504.02865", "authors": ["Yining Wang", "Yuquan Wang", "Xi Li", "Mi Zhang", "Geng Hong", "Min Yang"], "title": "The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances", "categories": ["cs.CL", "cs.LG"], "comment": "work in progress", "summary": "As Large Language Models (LLMs) continue to advance, they are increasingly\nrelied upon as real-time sources of information by non-expert users. To ensure\nthe factuality of the information they provide, much research has focused on\nmitigating hallucinations in LLM responses, but only in the context of formal\nuser queries, rather than maliciously crafted ones. In this study, we introduce\nThe Illusionist's Prompt, a novel hallucination attack that incorporates\nlinguistic nuances into adversarial queries, challenging the factual accuracy\nof LLMs against five types of fact-enhancing strategies. Our attack\nautomatically generates highly transferrable illusory prompts to induce\ninternal factual errors, all while preserving user intent and semantics.\nExtensive experiments confirm the effectiveness of our attack in compromising\nblack-box LLMs, including commercial APIs like GPT-4o and Gemini-2.0, even with\nvarious defensive mechanisms."}
{"id": "2504.02900", "pdf": "https://arxiv.org/pdf/2504.02900", "abs": "https://arxiv.org/abs/2504.02900", "authors": ["Matheus Martins Batista"], "title": "Comparative Analysis of Deepfake Detection Models: New Approaches and Perspectives", "categories": ["cs.CV", "cs.LG", "stat.CO", "stat.ML"], "comment": "Bachelor's thesis", "summary": "The growing threat posed by deepfake videos, capable of manipulating\nrealities and disseminating misinformation, drives the urgent need for\neffective detection methods. This work investigates and compares different\napproaches for identifying deepfakes, focusing on the GenConViT model and its\nperformance relative to other architectures present in the DeepfakeBenchmark.\nTo contextualize the research, the social and legal impacts of deepfakes are\naddressed, as well as the technical fundamentals of their creation and\ndetection, including digital image processing, machine learning, and artificial\nneural networks, with emphasis on Convolutional Neural Networks (CNNs),\nGenerative Adversarial Networks (GANs), and Transformers. The performance\nevaluation of the models was conducted using relevant metrics and new datasets\nestablished in the literature, such as WildDeep-fake and DeepSpeak, aiming to\nidentify the most effective tools in the battle against misinformation and\nmedia manipulation. The obtained results indicated that GenConViT, after\nfine-tuning, exhibited superior performance in terms of accuracy (93.82%) and\ngeneralization capacity, surpassing other architectures in the\nDeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the\nadvancement of deepfake detection techniques, offering contributions to the\ndevelopment of more robust and effective solutions against the dissemination of\nfalse information."}
{"id": "2504.02867", "pdf": "https://arxiv.org/pdf/2504.02867", "abs": "https://arxiv.org/abs/2504.02867", "authors": ["Hongliu Cao", "Ilias Driouich", "Robin Singh", "Eoin Thomas"], "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at SophiaSummit2024", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\ndiverse domains, yet they still encounter challenges such as insufficient\ndomain-specific knowledge, biases, and hallucinations. This underscores the\nneed for robust evaluation methodologies to accurately assess LLM-based\napplications. Traditional evaluation methods, which rely on word overlap or\ntext embeddings, are inadequate for capturing the nuanced semantic information\nnecessary to evaluate dynamic, open-ended text generation. Recent research has\nexplored leveraging LLMs to mimic human reasoning and decision-making processes\nfor evaluation purposes known as LLM-as-a-judge framework. However, these\nexisting frameworks have two significant limitations. First, they lack the\nflexibility to adapt to different text styles, including various answer and\nground truth styles, thereby reducing their generalization performance. Second,\nthe evaluation scores produced by these frameworks are often skewed and hard to\ninterpret, showing a low correlation with human judgment. To address these\nchallenges, we propose a novel dynamic multi-agent system that automatically\ndesigns personalized LLM judges for various natural language generation\napplications. This system iteratively refines evaluation prompts and balances\nthe trade-off between the adaptive requirements of downstream tasks and the\nalignment with human perception. Our experimental results show that the\nproposed multi-agent LLM Judge framework not only enhances evaluation accuracy\ncompared to existing methods but also produces evaluation scores that better\nalign with human perception."}
{"id": "2504.02912", "pdf": "https://arxiv.org/pdf/2504.02912", "abs": "https://arxiv.org/abs/2504.02912", "authors": ["Rohit Agarwal", "Aryan Dessai", "Arif Ahmed Sekh", "Krishna Agarwal", "Alexander Horsch", "Dilip K. Prasad"], "title": "Haphazard Inputs as Images in Online Learning", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "The field of varying feature space in online learning settings, also known as\nhaphazard inputs, is very prominent nowadays due to its applicability in\nvarious fields. However, the current solutions to haphazard inputs are\nmodel-dependent and cannot benefit from the existing advanced deep-learning\nmethods, which necessitate inputs of fixed dimensions. Therefore, we propose to\ntransform the varying feature space in an online learning setting to a\nfixed-dimension image representation on the fly. This simple yet novel approach\nis model-agnostic, allowing any vision-based models to be applicable for\nhaphazard inputs, as demonstrated using ResNet and ViT. The image\nrepresentation handles the inconsistent input data seamlessly, making our\nproposed approach scalable and robust. We show the efficacy of our method on\nfour publicly available datasets. The code is available at\nhttps://github.com/Rohit102497/HaphazardInputsAsImages."}
{"id": "2504.02870", "pdf": "https://arxiv.org/pdf/2504.02870", "abs": "https://arxiv.org/abs/2504.02870", "authors": ["Frank P. -W. Lo", "Jianing Qiu", "Zeyu Wang", "Haibao Yu", "Yeming Chen", "Gao Zhang", "Benny Lo"], "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by CVPR 2025 Workshop", "summary": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows."}
{"id": "2504.02918", "pdf": "https://arxiv.org/pdf/2504.02918", "abs": "https://arxiv.org/abs/2504.02918", "authors": ["Chenyu Zhang", "Daniil Cherniavskii", "Andrii Zadaianchuk", "Antonios Tragoudaras", "Antonios Vozikis", "Thijmen Nijdam", "Derck W. E. Prinzhorn", "Mark Bodracska", "Nicu Sebe", "Efstratios Gavves"], "title": "Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image and video generation raise hopes that these models\npossess world modeling capabilities, the ability to generate realistic,\nphysically plausible videos. This could revolutionize applications in robotics,\nautonomous driving, and scientific simulation. However, before treating these\nmodels as world models, we must ask: Do they adhere to physical conservation\nlaws? To answer this, we introduce Morpheus, a benchmark for evaluating video\ngeneration models on physical reasoning. It features 80 real-world videos\ncapturing physical phenomena, guided by conservation laws. Since artificial\ngenerations lack ground truth, we assess physical plausibility using\nphysics-informed metrics evaluated with respect to infallible conservation laws\nknown per physical setting, leveraging advances in physics-informed neural\nnetworks and vision-language foundation models. Our findings reveal that even\nwith advanced prompting and video conditioning, current models struggle to\nencode physical principles despite generating aesthetically pleasing videos.\nAll data, leaderboard, and code are open-sourced at our project page."}
{"id": "2504.02871", "pdf": "https://arxiv.org/pdf/2504.02871", "abs": "https://arxiv.org/abs/2504.02871", "authors": ["Enshuo Hsu", "Martin Ugbala", "Krishna Kumar Kookal", "Zouaidi Kawtar", "Nicholas L. Rider", "Muhammad F. Walji", "Kirk Roberts"], "title": "Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Generative information extraction using large language models, particularly\nthrough few-shot learning, has become a popular method. Recent studies indicate\nthat providing a detailed, human-readable guideline-similar to the annotation\nguidelines traditionally used for training human annotators can significantly\nimprove performance. However, constructing these guidelines is both labor- and\nknowledge-intensive. Additionally, the definitions are often tailored to meet\nspecific needs, making them highly task-specific and often non-reusable.\nHandling these subtle differences requires considerable effort and attention to\ndetail. In this study, we propose a self-improving method that harvests the\nknowledge summarization and text generation capacity of LLMs to synthesize\nannotation guidelines while requiring virtually no human input. Our zero-shot\nexperiments on the clinical named entity recognition benchmarks, 2012 i2b2\nEVENT, 2012 i2b2 TIMEX, 2014 i2b2, and 2018 n2c2 showed 25.86%, 4.36%, 0.20%,\nand 7.75% improvements in strict F1 scores from the no-guideline baseline. The\nLLM-synthesized guidelines showed equivalent or better performance compared to\nhuman-written guidelines by 1.15% to 4.14% in most tasks. In conclusion, this\nstudy proposes a novel LLM self-improving method that requires minimal\nknowledge and human input and is applicable to multiple biomedical domains."}
{"id": "2504.02920", "pdf": "https://arxiv.org/pdf/2504.02920", "abs": "https://arxiv.org/abs/2504.02920", "authors": ["Anurag Kulkarni"], "title": "LiDAR-based Object Detection with Real-time Voice Specifications", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures, submitted as part of MSc research", "summary": "This paper presents a LiDAR-based object detection system with real-time\nvoice specifications, integrating KITTI's 3D point clouds and RGB images\nthrough a multi-modal PointNet framework. It achieves 87.0% validation accuracy\non a 3000-sample subset, surpassing a 200-sample baseline of 67.5% by combining\nspatial and visual data, addressing class imbalance with weighted loss, and\nrefining training via adaptive techniques. A Tkinter prototype provides natural\nIndian male voice output using Edge TTS (en-IN-PrabhatNeural), alongside 3D\nvisualizations and real-time feedback, enhancing accessibility and safety in\nautonomous navigation, assistive technology, and beyond. The study offers a\ndetailed methodology, comprehensive experimental analysis, and a broad review\nof applications and challenges, establishing this work as a scalable\nadvancement in human-computer interaction and environmental perception, aligned\nwith current research trends."}
{"id": "2504.02872", "pdf": "https://arxiv.org/pdf/2504.02872", "abs": "https://arxiv.org/abs/2504.02872", "authors": ["Ingmar Bakermans", "Daniel De Pascale", "Gonçalo Marcelino", "Giuseppe Cascavilla", "Zeno Geradts"], "title": "Scraping the Shadows: Deep Learning Breakthroughs in Dark Web Intelligence", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "comment": "17 pages, 17 images", "summary": "Darknet markets (DNMs) facilitate the trade of illegal goods on a global\nscale. Gathering data on DNMs is critical to ensuring law enforcement agencies\ncan effectively combat crime. Manually extracting data from DNMs is an\nerror-prone and time-consuming task. Aiming to automate this process we develop\na framework for extracting data from DNMs and evaluate the application of three\nstate-of-the-art Named Entity Recognition (NER) models, ELMo-BiLSTM\n\\citep{ShahEtAl2022}, UniversalNER \\citep{ZhouEtAl2024}, and GLiNER\n\\citep{ZaratianaEtAl2023}, at the task of extracting complex entities from DNM\nproduct listing pages. We propose a new annotated dataset, which we use to\ntrain, fine-tune, and evaluate the models. Our findings show that\nstate-of-the-art NER models perform well in information extraction from DNMs,\nachieving 91% Precision, 96% Recall, and an F1 score of 94%. In addition,\nfine-tuning enhances model performance, with UniversalNER achieving the best\nperformance."}
{"id": "2504.02949", "pdf": "https://arxiv.org/pdf/2504.02949", "abs": "https://arxiv.org/abs/2504.02949", "authors": ["Xianwei Zhuang", "Yuxin Xie", "Yufan Deng", "Dongchao Yang", "Liming Liang", "Jinghan Ru", "Yuguo Yin", "Yuexian Zou"], "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at: https://github.com/VARGPT-family/VARGPT-v1.1.\n  arXiv admin note: text overlap with arXiv:2501.12327", "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1."}
{"id": "2504.02873", "pdf": "https://arxiv.org/pdf/2504.02873", "abs": "https://arxiv.org/abs/2504.02873", "authors": ["Dongjun Wei", "Minjia Mao", "Xiao Fang", "Michael Chau"], "title": "Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion", "categories": ["cs.CL"], "comment": null, "summary": "The malicious usage of large language models (LLMs) has motivated the\ndetection of LLM-generated texts. Previous work in topological data analysis\nshows that the persistent homology dimension (PHD) of text embeddings can serve\nas a more robust and promising score than other zero-shot methods. However,\neffectively detecting short LLM-generated texts remains a challenge. This paper\npresents Short-PHD, a zero-shot LLM-generated text detection method tailored\nfor short texts. Short-PHD stabilizes the estimation of the previous PHD method\nfor short texts by inserting off-topic content before the given input text and\nidentifies LLM-generated text based on an established detection threshold.\nExperimental results on both public and generated datasets demonstrate that\nShort-PHD outperforms existing zero-shot methods in short LLM-generated text\ndetection. Implementation codes are available online."}
{"id": "2504.02971", "pdf": "https://arxiv.org/pdf/2504.02971", "abs": "https://arxiv.org/abs/2504.02971", "authors": ["Binh M. Le", "Shaoyuan Xu", "Jinmiao Fu", "Zhishen Huang", "Moyan Li", "Yanhui Guo", "Hongdong Li", "Sameera Ramasinghe", "Bryan Wang"], "title": "QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "8 pages, accepted by CVPR 2025 MULA", "summary": "In Visual Document Understanding (VDU) tasks, fine-tuning a pre-trained\nVision-Language Model (VLM) with new datasets often falls short in optimizing\nthe vision encoder to identify query-specific regions in text-rich document\nimages. Existing methods that directly inject queries into model layers by\nmodifying the network architecture often struggle to adapt to new datasets with\nlimited annotations. To address this, we introduce QID, a novel, streamlined,\narchitecture-preserving approach that integrates query embeddings into the\nvision encoder, leading to notable performance gains, particularly in\ndata-scarce fine-tuning scenarios. Specifically, our approach introduces a\ndual-module framework: a query-aware module that generates a unique query\nvector to precisely guide the model's focus, as well as a query-agnostic module\nthat captures the positional relationships among tokens, ensuring robust\nspatial understanding. Notably, both modules operate independently of the\nvision attention blocks, facilitating targeted learning of query embeddings and\nenhancing visual semantic identification. Experiments with OCR-free VLMs across\nmultiple datasets demonstrate significant performance improvements using our\nmethod, especially in handling text-rich documents in data-scarce environments."}
{"id": "2504.02874", "pdf": "https://arxiv.org/pdf/2504.02874", "abs": "https://arxiv.org/abs/2504.02874", "authors": ["Luis Felipe", "Carlos Garcia", "Issam El Naqa", "Monique Shotande", "Aakash Tripathi", "Vivek Rudrapatna", "Ghulam Rasool", "Danielle Bitterman", "Gilmer Valdes"], "title": "TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet", "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 10 tables", "summary": "The need for robust and diverse data sets to train clinical large language\nmodels (cLLMs) is critical given that currently available public repositories\noften prove too limited in size or scope for comprehensive medical use. While\nresources like PubMed provide foundational medical literature, they capture\nonly a narrow range of formal publications and omit the broader medical\ndiscourse on the internet. To address these deficits, we introduce\nTheBlueScrubs-v1, a curated dataset of over 25 billion medical tokens - nearly\nthree times larger than PubMed - drawn from a broad-scale internet corpus. Our\ntwo-stage filtering pipeline employs a Logistic Regression model for document\nscreening (achieving an AUC of approximately 0.95 on external validation),\nfollowed by verification via a 70B-parameter Llama 3.1 instruct model. Each\ntext is assigned three LLM-based quality scores encompassing medical relevance,\nprecision and factual detail, and safety and ethical standards. Clinician\nreviews confirm high concordance with these automated evaluations, and a\nspecialized cancer classifier further labels approximately 11 billion oncology\ntokens. Two demonstration tasks highlight the dataset's practical value: first,\nwe distill the safety evaluations to a smaller BERT-style model that reaches an\nAUC near 0.96 on unseen data; second, we fine-tune a compact LLM on a filtered\nsubset, showing measurable improvements over standard baselines in medical\nbenchmarks as well as private ones. This Data Descriptor details the dataset's\ncreation and validation, underscoring its potential utility for medical AI\nresearch."}
{"id": "2504.03006", "pdf": "https://arxiv.org/pdf/2504.03006", "abs": "https://arxiv.org/abs/2504.03006", "authors": ["Jing Gao", "Ce Zheng", "Laszlo A. Jeni", "Zackory Erickson"], "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery", "categories": ["cs.CV"], "comment": "16 pages, 19 figures. Accepted to CVPR 2025", "summary": "In-bed human mesh recovery can be crucial and enabling for several healthcare\napplications, including sleep pattern monitoring, rehabilitation support, and\npressure ulcer prevention. However, it is difficult to collect large real-world\nvisual datasets in this domain, in part due to privacy and expense constraints,\nwhich in turn presents significant challenges for training and deploying deep\nlearning models. Existing in-bed human mesh estimation methods often rely\nheavily on real-world data, limiting their ability to generalize across\ndifferent in-bed scenarios, such as varying coverings and environmental\nsettings. To address this, we propose a Sim-to-Real Transfer Framework for\nin-bed human mesh recovery from overhead depth images, which leverages\nlarge-scale synthetic data alongside limited or no real-world samples. We\nintroduce a diffusion model that bridges the gap between synthetic data and\nreal data to support generalization in real-world in-bed pose and body\ninference scenarios. Extensive experiments and ablation studies validate the\neffectiveness of our framework, demonstrating significant improvements in\nrobustness and adaptability across diverse healthcare scenarios."}
{"id": "2504.02877", "pdf": "https://arxiv.org/pdf/2504.02877", "abs": "https://arxiv.org/abs/2504.02877", "authors": ["DongHyun Choi", "Lucas Spangher", "Chris Hidey", "Peter Grabowski", "Ramy Eskander"], "title": "Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Large Language Models, which suffer from high computational\ncosts, advance so quickly that techniques proposed to streamline earlier\niterations are not guaranteed to benefit more modern models. Building upon the\nFunnel Transformer proposed by Dai and Le (2020), which progressively\ncompresses intermediate representations, we investigate the impact of funneling\nin contemporary Gemma2 Transformer architectures. We systematically evaluate\nvarious funnel configurations and recovery methods, comparing: (1) standard\npretraining to funnel-aware pretraining strategies, (2) the impact of\nfunnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our\nresults demonstrate that funneling creates information bottlenecks that\npropagate through deeper network layers, particularly in larger models (e.g.,\nGemma 7B), leading to at times unmanageable performance lost. However,\ncarefully selecting the funneling layer and employing effective recovery\nstrategies, can substantially mitigate performance losses, achieving up to a\n44\\% reduction in latency. Our findings highlight key trade-offs between\ncomputational efficiency and model accuracy, providing practical guidance for\ndeploying funnel-based approaches in large-scale natural language applications."}
{"id": "2504.03010", "pdf": "https://arxiv.org/pdf/2504.03010", "abs": "https://arxiv.org/abs/2504.03010", "authors": ["Shaoyuan Xu", "Yang Cheng", "Qian Lin", "Jan P. Allebach"], "title": "Emotion Recognition Using Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Emotion has an important role in daily life, as it helps people better\ncommunicate with and understand each other more efficiently. Facial expressions\ncan be classified into 7 categories: angry, disgust, fear, happy, neutral, sad\nand surprise. How to detect and recognize these seven emotions has become a\npopular topic in the past decade. In this paper, we develop an emotion\nrecognition system that can apply emotion recognition on both still images and\nreal-time videos by using deep learning.\n  We build our own emotion recognition classification and regression system\nfrom scratch, which includes dataset collection, data preprocessing , model\ntraining and testing. Given a certain image or a real-time video, our system is\nable to show the classification and regression results for all of the 7\nemotions. The proposed system is tested on 2 different datasets, and achieved\nan accuracy of over 80\\%. Moreover, the result obtained from real-time testing\nproves the feasibility of implementing convolutional neural networks in real\ntime to detect emotions accurately and efficiently."}
{"id": "2504.02881", "pdf": "https://arxiv.org/pdf/2504.02881", "abs": "https://arxiv.org/abs/2504.02881", "authors": ["Nick Whitehouse", "Nicole Lincoln", "Stephanie Yiu", "Lizzie Catterson", "Rivindu Perera"], "title": "Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers", "categories": ["cs.CL"], "comment": null, "summary": "Legal invoice review is a costly, inconsistent, and time-consuming process,\ntraditionally performed by Legal Operations, Lawyers or Billing Specialists who\nscrutinise billing compliance line by line. This study presents the first\nempirical comparison of Large Language Models (LLMs) against human invoice\nreviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations\nProfessionals-assessing their accuracy, speed, and cost-effectiveness.\nBenchmarking state-of-the-art LLMs against a ground truth set by expert legal\nprofessionals, our empirically substantiated findings reveal that LLMs\ndecisively outperform humans across every metric. In invoice approval\ndecisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by\nexperienced lawyers. On a granular level, LLMs dominate line-item\nclassification, with top models reaching F-scores of 81%, compared to just 43%\nfor the best-performing human group. Speed comparisons are even more striking -\nwhile lawyers take 194 to 316 seconds per invoice, LLMs are capable of\ncompleting reviews in as fast as 3.6 seconds. And cost? AI slashes review\nexpenses by 99.97%, reducing invoice processing costs from an average of $4.27\nper invoice for human invoice reviewers to mere cents. These results highlight\nthe evolving role of AI in legal spend management. As law firms and corporate\nlegal departments struggle with inefficiencies, this study signals a seismic\nshift: The era of LLM-powered legal spend management is not on the horizon, it\nhas arrived. The challenge ahead is not whether AI can perform as well as human\nreviewers, but how legal teams will strategically incorporate it, balancing\nautomation with human discretion."}
{"id": "2504.03011", "pdf": "https://arxiv.org/pdf/2504.03011", "abs": "https://arxiv.org/abs/2504.03011", "authors": ["Junying Wang", "Jingyuan Liu", "Xin Sun", "Krishna Kumar Singh", "Zhixin Shu", "He Zhang", "Jimei Yang", "Nanxuan Zhao", "Tuanfeng Y. Wang", "Simon S. Chen", "Ulrich Neumann", "Jae Shin Yoon"], "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization", "categories": ["cs.CV"], "comment": "Project page:https://junyingw.github.io/paper/relighting. Accepted by\n  CVPR 2025", "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods."}
{"id": "2504.02882", "pdf": "https://arxiv.org/pdf/2504.02882", "abs": "https://arxiv.org/abs/2504.02882", "authors": ["Sunghee Jung", "Donghun Lee", "Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Junrae Cho", "Kihyun Kim", "Eunggyun Kim", "Myeongcheol Shin"], "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling."}
{"id": "2504.03020", "pdf": "https://arxiv.org/pdf/2504.03020", "abs": "https://arxiv.org/abs/2504.03020", "authors": ["Shaoyuan Xu", "Cheng Lu", "Mark Shaw", "Peter Bauer", "Jan P. Allebach"], "title": "Page Classification for Print Imaging Pipeline", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Digital copiers and printers are widely used nowadays. One of the most\nimportant things people care about is copying or printing quality. In order to\nimprove it, we previously came up with an SVM-based classification method to\nclassify images with only text, only pictures or a mixture of both based on the\nfact that modern copiers and printers are equipped with processing pipelines\ndesigned specifically for different kinds of images. However, in some other\napplications, we need to distinguish more than three classes. In this paper, we\ndevelop a more advanced SVM-based classification method using four more new\nfeatures to classify 5 types of images which are text, picture, mixed, receipt\nand highlight."}
{"id": "2504.02883", "pdf": "https://arxiv.org/pdf/2504.02883", "abs": "https://arxiv.org/abs/2504.02883", "authors": ["Anil Ramakrishna", "Yixin Wan", "Xiaomeng Jin", "Kai-Wei Chang", "Zhiqi Bu", "Bhanukiran Vinzamuri", "Volkan Cevher", "Mingyi Hong", "Rahul Gupta"], "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SemEval-2025 Task 4: unlearning sensitive content from Large\nLanguage Models (LLMs). The task features 3 subtasks for LLM unlearning\nspanning different use cases: (1) unlearn long form synthetic creative\ndocuments spanning different genres; (2) unlearn short form synthetic\nbiographies containing personally identifiable information (PII), including\nfake names, phone number, SSN, email and home addresses, and (3) unlearn real\ndocuments sampled from the target model's training dataset. We received over\n100 submissions from over 30 institutions and we summarize the key techniques\nand lessons in this paper."}
{"id": "2504.03026", "pdf": "https://arxiv.org/pdf/2504.03026", "abs": "https://arxiv.org/abs/2504.03026", "authors": ["Yiran Xu", "Siqi Xie", "Zhuofang Li", "Harris Shadmany", "Yinxiao Li", "Luciano Sbaiz", "Miaosen Wang", "Junjie Ke", "Jose Lezama", "Hang Qi", "Han Zhang", "Jesse Berent", "Ming-Hsuan Yang", "Irfan Essa", "Jia-Bin Huang", "Feng Yang"], "title": "HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations", "categories": ["cs.CV"], "comment": null, "summary": "Image retargeting aims to change the aspect-ratio of an image while\nmaintaining its content and structure with less visual artifacts. Existing\nmethods still generate many artifacts or fail to maintain original content or\nstructure. To address this, we introduce HALO, an end-to-end trainable solution\nfor image retargeting. Since humans are more sensitive to distortions in\nsalient areas than non-salient areas of an image, HALO decomposes the input\nimage into salient/non-salient layers and applies different wrapping fields to\ndifferent layers. To further minimize the structure distortion in the output\nimages, we propose perceptual structure similarity loss which measures the\nstructure similarity between input and output images and aligns with human\nperception. Both quantitative results and a user study on the RetargetMe\ndataset show that HALO achieves SOTA. Especially, our method achieves an 18.4%\nhigher user preference compared to the baselines on average."}
{"id": "2504.02885", "pdf": "https://arxiv.org/pdf/2504.02885", "abs": "https://arxiv.org/abs/2504.02885", "authors": ["Hao Wang", "Shuchang Ye", "Jinghao Lin", "Usman Naseem", "Jinman Kim"], "title": "LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation", "categories": ["cs.CL"], "comment": "10 pages, 3 figures, 1 table", "summary": "Large vision-language models (LVMs) hold a great promise for automating\nmedical report generation, potentially reducing the burden of manual reporting.\nState-of-the-art (SOTA) research fine-tunes general LVMs with medical data to\nalign radiology images to corresponding medical reports. However, there are two\nkey factors that limit these LVM's performance. Firstly, LVMs lack complex\nreasoning capability that leads to logical inconsistencies and potential\ndiagnostic errors in generated reports. Secondly, LVMs lack reflection\nmechanism that leads to an inability to discover errors in the thinking\nprocess. To address these gaps, we propose LVMed-R2, a new fine-tuning strategy\nthat introduces complex reasoning and reflection mechanisms for LVMs to enhance\nmedical report generation. To the best of our knowledge, this is the first work\nto introduce complex reasoning to the medical report generation (MRG) task. Our\nproposed complex reasoning contains medical knowledge injection and\nperception-enhancing modules which improve the accuracy of LVMs diagnosis,\ncoupled with a perception tree to provide guidance to limit the perception\nrange. Further, the reflection mechanism forces self-verification for outputs\nto correct for potential errors. We experimented by fine-tuning LVMs with our\nproposed LVMed-R2 strategy, using IU-Xray and MIMIC-CXR datasets. Our results,\nmeasured on natural language generation (NLG) metrics and clinical efficacy\n(CE) metrics, demonstrate that LVMs fine-tuned with the proposed reflection\nmechanism possess the ability to correct outputs and complex reasoning\neffectively and improve LVMs performance for MRG."}
{"id": "2504.03041", "pdf": "https://arxiv.org/pdf/2504.03041", "abs": "https://arxiv.org/abs/2504.03041", "authors": ["Huiming Sun", "Yikang Li", "Kangning Yang", "Ruineng Li", "Daitao Xing", "Yangbo Xie", "Lan Fu", "Kaiyu Zhang", "Ming Chen", "Jiaming Ding", "Jiang Geng", "Jie Cai", "Zibo Meng", "Chiuman Ho"], "title": "VIP: Video Inpainting Pipeline for Real World Human Removal", "categories": ["cs.CV"], "comment": null, "summary": "Inpainting for real-world human and pedestrian removal in high-resolution\nvideo clips presents significant challenges, particularly in achieving\nhigh-quality outcomes, ensuring temporal consistency, and managing complex\nobject interactions that involve humans, their belongings, and their shadows.\nIn this paper, we introduce VIP (Video Inpainting Pipeline), a novel promptless\nvideo inpainting framework for real-world human removal applications. VIP\nenhances a state-of-the-art text-to-video model with a motion module and\nemploys a Variational Autoencoder (VAE) for progressive denoising in the latent\nspace. Additionally, we implement an efficient human-and-belongings\nsegmentation for precise mask generation. Sufficient experimental results\ndemonstrate that VIP achieves superior temporal consistency and visual fidelity\nacross diverse real-world scenarios, surpassing state-of-the-art methods on\nchallenging datasets. Our key contributions include the development of the VIP\npipeline, a reference frame integration technique, and the Dual-Fusion Latent\nSegment Refinement method, all of which address the complexities of inpainting\nin long, high-resolution video sequences."}
{"id": "2504.02887", "pdf": "https://arxiv.org/pdf/2504.02887", "abs": "https://arxiv.org/abs/2504.02887", "authors": ["John Chen", "Alexandros Lotsos", "Grace Wang", "Lexie Zhao", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "title": "Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "This paper was recommended for acceptance as a long paper by CSCL\n  reviewers, but ends up as a short paper. The arXiv version here is its longer\n  form, revised with reviewers' comments", "summary": "Open coding, a key inductive step in qualitative research, discovers and\nconstructs concepts from human datasets. However, capturing extensive and\nnuanced aspects or \"coding moments\" can be challenging, especially with large\ndiscourse datasets. While some studies explore machine learning (ML)/Generative\nAI (GAI)'s potential for open coding, few evaluation studies exist. We compare\nopen coding results by five recently published ML/GAI approaches and four human\ncoders, using a dataset of online chat messages around a mobile learning\nsoftware. Our systematic analysis reveals ML/GAI approaches' strengths and\nweaknesses, uncovering the complementary potential between humans and AI.\nLine-by-line AI approaches effectively identify content-based codes, while\nhumans excel in interpreting conversational dynamics. We discussed how embedded\nanalytical processes could shape the results of ML/GAI approaches. Instead of\nreplacing humans in open coding, researchers should integrate AI with and\naccording to their analytical processes, e.g., as parallel co-coders."}
{"id": "2504.03043", "pdf": "https://arxiv.org/pdf/2504.03043", "abs": "https://arxiv.org/abs/2504.03043", "authors": ["Joel Sol", "Shadi Alijani", "Homayoun Najjaran"], "title": "Sliced Wasserstein Discrepancy in Disentangling Representation and Adaptation Networks for Unsupervised Domain Adaptation", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, submitted to IEEE conference", "summary": "This paper introduces DRANet-SWD, an extension of existing work that\ndisentangles content and style representations of images for unsupervised\ndomain adaptation (UDA). The approach builds upon DRANet by incorporating the\nsliced Wasserstein discrepancy (SWD) as a style loss instead of the traditional\nGram matrix loss. The potential advantages of SWD over the Gram matrix loss for\ncapturing style variations in domain adaptation are investigated. Experiments\nusing digit classification datasets and driving scenario segmentation validate\nthe method, demonstrating that DRANet-SWD enhances performance. Results\nindicate that SWD provides a more robust statistical comparison of feature\ndistributions, leading to better style adaptation. These findings highlight the\neffectiveness of SWD in refining feature alignment and improving domain\nadaptation tasks across these benchmarks. Our code can be found here."}
{"id": "2504.02888", "pdf": "https://arxiv.org/pdf/2504.02888", "abs": "https://arxiv.org/abs/2504.02888", "authors": ["Wenkang Wang", "Ran Xu", "Jingsen Feng", "Qingfu Zhang", "Xu Chu"], "title": "A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek", "categories": ["cs.CL"], "comment": null, "summary": "We evaluated the performance of OpenFOAMGPT incorporating multiple\nlarge-language models. Some of the present models efficiently manage different\nCFD tasks such as adjusting boundary conditions, turbulence models, and solver\nconfigurations, although their token cost and stability vary. Locally deployed\nsmaller models like QwQ-32B struggled with generating valid solver files for\ncomplex processes. Zero-shot prompting commonly failed in simulations with\nintricate settings, even for large models. Challenges with boundary conditions\nand solver keywords stress the requirement for expert supervision, indicating\nthat further development is needed to fully automate specialized CFD\nsimulations."}
{"id": "2504.03047", "pdf": "https://arxiv.org/pdf/2504.03047", "abs": "https://arxiv.org/abs/2504.03047", "authors": ["Reef Alturki", "Adrian Hilton", "Jean-Yves Guillemaut"], "title": "Attention-Aware Multi-View Pedestrian Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In spite of the recent advancements in multi-object tracking, occlusion poses\na significant challenge. Multi-camera setups have been used to address this\nchallenge by providing a comprehensive coverage of the scene. Recent multi-view\npedestrian detection models have highlighted the potential of an early-fusion\nstrategy, projecting feature maps of all views to a common ground plane or the\nBird's Eye View (BEV), and then performing detection. This strategy has been\nshown to improve both detection and tracking performance. However, the\nperspective transformation results in significant distortion on the ground\nplane, affecting the robustness of the appearance features of the pedestrians.\nTo tackle this limitation, we propose a novel model that incorporates attention\nmechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an\nearly-fusion strategy for detection, and a cross-attention mechanism to\nestablish robust associations between pedestrians in different frames, while\nefficiently propagating pedestrian features across frames, resulting in a more\nrobust feature representation for each pedestrian. Extensive experiments\ndemonstrate that our model outperforms state-of-the-art models, with an IDF1\nscore of $96.1\\%$ on Wildtrack dataset, and $85.7\\%$ on MultiviewX dataset."}
{"id": "2504.02890", "pdf": "https://arxiv.org/pdf/2504.02890", "abs": "https://arxiv.org/abs/2504.02890", "authors": ["Khanh-Tung Tran", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "Scaling Test-time Compute for Low-resource Languages: Multilingual Reasoning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in test-time compute scaling have enabled Large Language\nModels (LLMs) to tackle deep reasoning tasks by generating a chain-of-thought\n(CoT) that includes trial and error, backtracking, and intermediate reasoning\nsteps before producing the final answer. However, these techniques have been\napplied predominantly to popular languages, such as English, leaving reasoning\nin low-resource languages underexplored and misaligned. In this work, we\ninvestigate the multilingual mechanism by which LLMs internally operate in a\nlatent space biased toward their inherently dominant language. To leverage this\nphenomenon for low-resource languages, we train models to generate the CoT in\nEnglish while outputting the final response in the target language, given input\nin the low-resource language. Our experiments demonstrate that this approach,\nnamed English-Pivoted CoT Training, outperforms other baselines, including\ntraining to generate both the CoT and the final response solely in the target\nlanguage, with up to 28.33% improvement. Further analysis provides novel\ninsights into the relationships between reasoning and multilinguality of LLMs,\nprompting for better approaches in developing multilingual large reasoning\nmodels"}
{"id": "2504.03052", "pdf": "https://arxiv.org/pdf/2504.03052", "abs": "https://arxiv.org/abs/2504.03052", "authors": ["Hyun-Ho Choi", "Kangsoo Kim", "Ki-Ho Lee", "Kisong Lee"], "title": "Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 12 figures", "summary": "Accurate and real-time three-dimensional (3D) pose estimation is challenging\nin resource-constrained and dynamic environments owing to its high\ncomputational complexity. To address this issue, this study proposes a novel\ncooperative inference method for real-time 3D human pose estimation in mobile\nedge computing (MEC) networks. In the proposed method, multiple end devices\nequipped with lightweight inference models employ dual confidence thresholds to\nfilter ambiguous images. Only the filtered images are offloaded to an edge\nserver with a more powerful inference model for re-evaluation, thereby\nimproving the estimation accuracy under computational and communication\nconstraints. We numerically analyze the performance of the proposed inference\nmethod in terms of the inference accuracy and end-to-end delay and formulate a\njoint optimization problem to derive the optimal confidence thresholds and\ntransmission time for each device, with the objective of minimizing the mean\nper-joint position error (MPJPE) while satisfying the required end-to-end delay\nconstraint. To solve this problem, we demonstrate that minimizing the MPJPE is\nequivalent to maximizing the sum of the inference accuracies for all devices,\ndecompose the problem into manageable subproblems, and present a low-complexity\noptimization algorithm to obtain a near-optimal solution. The experimental\nresults show that a trade-off exists between the MPJPE and end-to-end delay\ndepending on the confidence thresholds. Furthermore, the results confirm that\nthe proposed cooperative inference method achieves a significant reduction in\nthe MPJPE through the optimal selection of confidence thresholds and\ntransmission times, while consistently satisfying the end-to-end delay\nrequirement in various MEC environments."}
{"id": "2504.02891", "pdf": "https://arxiv.org/pdf/2504.02891", "abs": "https://arxiv.org/abs/2504.02891", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "title": "Automated Survey Collection with LLM-based Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: Traditional phone-based surveys are among the most accessible and\nwidely used methods to collect biomedical and healthcare data, however, they\nare often costly, labor intensive, and difficult to scale effectively. To\novercome these limitations, we propose an end-to-end survey collection\nframework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for\ndesigning the survey and recruiting participants, a conversational phone agent\npowered by an LLM that calls participants and administers the survey, a second\nLLM (GPT-4o) that analyzes the conversation transcripts generated during the\nsurveys, and a database for storing and organizing the results. To test our\nframework, we recruited 8 participants consisting of 5 native and 3 non-native\nenglish speakers and administered 40 surveys. We evaluated the correctness of\nLLM-generated conversation transcripts, accuracy of survey responses inferred\nby GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from\nconversation transcripts with an average accuracy of 98% despite transcripts\nexhibiting an average per-line word error rate of 7.7%. While participants\nnoted occasional errors made by the conversational LLM agent, they reported\nthat the agent effectively conveyed the purpose of the survey, demonstrated\ngood comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting\nand analyzing phone surveys for healthcare applications. By reducing the\nworkload on human interviewers and offering a scalable solution, this approach\npaves the way for real-world, end-to-end AI-powered phone survey collection\nsystems."}
{"id": "2504.03059", "pdf": "https://arxiv.org/pdf/2504.03059", "abs": "https://arxiv.org/abs/2504.03059", "authors": ["Haishan Wang", "Mohammad Hassan Vali", "Arno Solin"], "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D\nreconstruction, achieving high-quality results with real-time radiance field\nrendering. However, a key challenge is the substantial storage cost:\nreconstructing a single scene typically requires millions of Gaussian splats,\neach represented by 59 floating-point parameters, resulting in approximately\n1~GB of memory. To address this challenge, we propose a compression method by\nbuilding separate attribute codebooks and storing only discrete code indices.\nSpecifically, we employ noise-substituted vector quantization technique to\njointly train the codebooks and model features, ensuring consistency between\ngradient descent optimization and parameter discretization. Our method reduces\nthe memory consumption efficiently (around $45\\times$) while maintaining\ncompetitive reconstruction quality on standard 3D benchmark scenes. Experiments\non different codebook sizes show the trade-off between compression ratio and\nimage quality. Furthermore, the trained compressed model remains fully\ncompatible with popular 3DGS viewers and enables faster rendering speed, making\nit well-suited for practical applications."}
{"id": "2504.02894", "pdf": "https://arxiv.org/pdf/2504.02894", "abs": "https://arxiv.org/abs/2504.02894", "authors": ["Ahsan Bilal", "Beiyu Lin", "Mehdi Zaeifi"], "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been widely used for various tasks and\napplications. However, LLMs and fine-tuning are limited to the pre-trained\ndata. For example, ChatGPT's world knowledge until 2021 can be outdated or\ninaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation\n(RAG), is proposed to augment LLMs with additional, new, latest details and\ninformation to LLMs. While RAG offers the correct information, it may not best\npresent it, especially to different population groups with personalizations.\nReinforcement Learning from Human Feedback (RLHF) adapts to user needs by\naligning model responses with human preference through feedback loops. In\nreal-life applications, such as mental health problems, a dynamic and\nfeedback-based model would continuously adapt to new information and offer\npersonalized assistance due to complex factors fluctuating in a daily\nenvironment. Thus, we propose an Online Reinforcement Learning-based\nRetrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the\nresponding systems to mental health problems, such as stress, anxiety, and\ndepression. We use an open-source dataset collected from 2028 College Students\nwith 28 survey questions for each student to demonstrate the performance of our\nproposed system with the existing systems. Our system achieves superior\nperformance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,\nGemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life\napplications of LLMs for personalized services in the everyday environment. The\nresults will also help researchers in the fields of sociology, psychology, and\nneuroscience to align their theories more closely with the actual human daily\nenvironment."}
{"id": "2504.03072", "pdf": "https://arxiv.org/pdf/2504.03072", "abs": "https://arxiv.org/abs/2504.03072", "authors": ["Pascal Chang", "Jingwei Tang", "Markus Gross", "Vinicius C. Azevedo"], "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2024 (Oral)", "summary": "Video editing and generation methods often rely on pre-trained image-based\ndiffusion models. During the diffusion process, however, the reliance on\nrudimentary noise sampling techniques that do not preserve correlations present\nin subsequent frames of a video is detrimental to the quality of the results.\nThis either produces high-frequency flickering, or texture-sticking artifacts\nthat are not amenable to post-processing. With this in mind, we propose a novel\nmethod for preserving temporal correlations in a sequence of noise samples.\nThis approach is materialized by a novel noise representation, dubbed\n$\\int$-noise (integral noise), that reinterprets individual noise samples as a\ncontinuously integrated noise field: pixel values do not represent discrete\nvalues, but are rather the integral of an underlying infinite-resolution noise\nover the pixel area. Additionally, we propose a carefully tailored transport\nmethod that uses $\\int$-noise to accurately advect noise samples over a\nsequence of frames, maximizing the correlation between different frames while\nalso preserving the noise properties. Our results demonstrate that the proposed\n$\\int$-noise can be used for a variety of tasks, such as video restoration,\nsurrogate rendering, and conditional video generation. See\nhttps://warpyournoise.github.io/ for video results."}
{"id": "2504.02898", "pdf": "https://arxiv.org/pdf/2504.02898", "abs": "https://arxiv.org/abs/2504.02898", "authors": ["Lele Cao"], "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content", "categories": ["cs.CL", "cs.LG", "68T50, 68T45, 68T10, 68T30, 94A08, 62H30, 68U10", "I.2.7; I.2.10; I.2.6; H.5.5; K.6.5; K.4.1; I.4.9; H.3.1"], "comment": null, "summary": "Advances in AI-generated content have led to wide adoption of large language\nmodels, diffusion-based visual generators, and synthetic audio tools. However,\nthese developments raise critical concerns about misinformation, copyright\ninfringement, security threats, and the erosion of public trust. In this paper,\nwe explore an extensive range of methods designed to detect and mitigate\nAI-generated textual, visual, and audio content. We begin by discussing\nmotivations and potential impacts associated with AI-based content generation,\nincluding real-world risks and ethical dilemmas. We then outline detection\ntechniques spanning observation-based strategies, linguistic and statistical\nanalysis, model-based pipelines, watermarking and fingerprinting, as well as\nemergent ensemble approaches. We also present new perspectives on robustness,\nadaptation to rapidly improving generative architectures, and the critical role\nof human-in-the-loop verification. By surveying state-of-the-art research and\nhighlighting case studies in academic, journalistic, legal, and industrial\ncontexts, this paper aims to inform robust solutions and policymaking. We\nconclude by discussing open challenges, including adversarial transformations,\ndomain generalization, and ethical concerns, thereby offering a holistic guide\nfor researchers, practitioners, and regulators to preserve content authenticity\nin the face of increasingly sophisticated AI-generated media."}
{"id": "2504.03089", "pdf": "https://arxiv.org/pdf/2504.03089", "abs": "https://arxiv.org/abs/2504.03089", "authors": ["Prashant Kumar", "Dheeraj Vattikonda", "Kshitij Madhav Bhat", "Kunal Dargan", "Prem Kalra"], "title": "SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections", "categories": ["cs.CV"], "comment": null, "summary": "The widespread adoption of learning-based methods for the LiDAR makes\nautonomous vehicles vulnerable to adversarial attacks through adversarial\n\\textit{point injections (PiJ)}. It poses serious security challenges for\nnavigation and map generation. Despite its critical nature, no major work\nexists that studies learning-based attacks on LiDAR-based SLAM. Our work\nproposes SLACK, an end-to-end deep generative adversarial model to attack LiDAR\nscans with several point injections without deteriorating LiDAR quality. To\nfacilitate SLACK, we design a novel yet simple autoencoder that augments\ncontrastive learning with segmentation-based attention for precise\nreconstructions. SLACK demonstrates superior performance on the task of\n\\textit{point injections (PiJ)} compared to the best baselines on KITTI and\nCARLA-64 dataset while maintaining accurate scan quality. We qualitatively and\nquantitatively demonstrate PiJ attacks using a fraction of LiDAR points. It\nseverely degrades navigation and map quality without deteriorating the LiDAR\nscan quality."}
{"id": "2504.02902", "pdf": "https://arxiv.org/pdf/2504.02902", "abs": "https://arxiv.org/abs/2504.02902", "authors": ["Liangjie Huang", "Dawei Li", "Huan Liu", "Lu Cheng"], "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable self-improvement\ncapabilities, whereby models iteratively revise their outputs through\nself-generated feedback. While this reflective mechanism has shown promise in\nenhancing task performance, recent studies suggest that it may also introduce\nundesirable biases-most notably, self-bias, or the tendency of LLMs to favor\ntheir own prior outputs. In this work, we extend this line of inquiry by\ninvestigating the impact on confidence estimation. We evaluate three\nrepresentative self-improvement paradigms-basic prompting, Chain-of-Thought\n(CoT) prompting, and tuning-based methods and find that iterative\nself-improvement can lead to systematic overconfidence, as evidenced by a\nsteadily increasing Expected Calibration Error (ECE) and lower accuracy with\nhigh confidence. We then further explore the integration of confidence\ncalibration techniques with self-improvement. Specifically, we compare three\nstrategies: (1) applying calibration after multiple rounds of self-improvement,\n(2) calibrating before self-improvement, and (3) applying calibration\niteratively at each self-improvement step. Our results show that iterative\ncalibration is most effective in reducing ECE, yielding improved calibration.\nOur work pioneers the study of self-improving LLMs from a calibration\nperspective, offering valuable insights into balancing model performance and\nreliability."}
{"id": "2504.03096", "pdf": "https://arxiv.org/pdf/2504.03096", "abs": "https://arxiv.org/abs/2504.03096", "authors": ["Zhen Hao Sia", "Yogesh Singh Rawat"], "title": "Scaling Open-Vocabulary Action Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we focus on scaling open-vocabulary action detection. Existing\napproaches for action detection are predominantly limited to closed-set\nscenarios and rely on complex, parameter-heavy architectures. Extending these\nmodels to the open-vocabulary setting poses two key challenges: (1) the lack of\nlarge-scale datasets with many action classes for robust training, and (2)\nparameter-heavy adaptations to a pretrained vision-language contrastive model\nto convert it for detection, risking overfitting the additional non-pretrained\nparameters to base action classes. Firstly, we introduce an encoder-only\nmultimodal model for video action detection, reducing the reliance on\nparameter-heavy additions for video action detection. Secondly, we introduce a\nsimple weakly supervised training strategy to exploit an existing closed-set\naction detection dataset for pretraining. Finally, we depart from the ill-posed\nbase-to-novel benchmark used by prior works in open-vocabulary action detection\nand devise a new benchmark to evaluate on existing closed-set action detection\ndatasets without ever using them for training, showing novel results to serve\nas baselines for future work."}
{"id": "2504.02904", "pdf": "https://arxiv.org/pdf/2504.02904", "abs": "https://arxiv.org/abs/2504.02904", "authors": ["Hongzhe Du", "Weikai Li", "Min Cai", "Karim Saraipour", "Zimin Zhang", "Himabindu Lakkaraju", "Yizhou Sun", "Shichang Zhang"], "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by linear vectors in the hidden representation space. The\ntruthfulness direction is highly similar between the base and post-trained\nmodel, and it is effectively transferable for interventions; (3) The refusal\ndirection is different between the base and post-trained models, and it shows\nlimited forward transferability; (4) Differences in confidence between the base\nand post-trained models cannot be attributed to entropy neurons. Our study\nprovides insights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training."}
{"id": "2504.03108", "pdf": "https://arxiv.org/pdf/2504.03108", "abs": "https://arxiv.org/abs/2504.03108", "authors": ["Xuanyu Liu", "Huiyun Yao", "Jinggui Gao", "Zhongyi Guo", "Xue Zhang", "Yulin Dong"], "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT)\nare the main techniques used in Medical image segmentation. However, CNN is\nlimited to local contextual information, and ViT's quadratic complexity results\nin significant computational costs. At the same time, equipping the model to\ndistinguish lesion boundaries with varying degrees of severity is also a\nchallenge encountered in skin lesion segmentation. Purpose:This research aims\nto optimize the balance between computational costs and long-range dependency\nmodelling and achieve excellent generalization across lesions with different\ndegrees of severity. Methods:we propose a lightweight U-shape network that\nutilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the\nadvantages of Fastformer's additive attention mechanism, combining element-wise\nproduct and matrix product for comprehensive feature extraction and channel\nreduction to save computational costs. In order to accurately identify the\nlesion boundaries with varying degrees of severity, we designed Fusion\nMechanism including Multi-Granularity Fusion and Channel Fusion, which can\nprocess the feature maps in the granularity and channel levels to obtain\ndifferent contextual information. Results:Comprehensive experiments on the\nISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms\nexisting state-of-the-art models regarding parameter numbers, computational\ncomplexity and segmentation performance. In short, compared to MISSFormer, our\nmodel achieves superior segmentation performance while reducing parameter and\ncomputation costs by 101x and 15x, respectively. Conclusions:Both quantitative\nand qualitative analyses show that VFFM-UNet sets a new benchmark by reaching\nan ideal balance between parameter numbers, computational complexity, and\nsegmentation performance compared to existing state-of-the-art models."}
{"id": "2504.02906", "pdf": "https://arxiv.org/pdf/2504.02906", "abs": "https://arxiv.org/abs/2504.02906", "authors": ["Zhihan Zhang", "Yixin Cao", "Lizi Liao"], "title": "Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 5 figures", "summary": "Chart-to-code generation, the process of converting chart images into\nexecutable plotting scripts, provides a lossless representation of chart\ninformation, requiring models to accurately capture and summarize all visual\nand structural elements. However, this remains a significant challenge for\nmultimodal large language models (MLLMs), which are not inherently well-aligned\nwith code generation tasks. To bridge this gap, we introduce Chart2Code, a\nnovel iterative dual preference learning framework designed to enhance MLLMs'\nchart-to-code generation capabilities through structured code variant\ngeneration and fine-grained dual reward signals. We validate Chart2Code across\nthree MLLMs and find that iterative preference learning consistently improves\nout-of-distribution chart-to-code generation quality. Throughout this process,\nour dual scoring method, which evaluates both the textual code structure and\nits visual representation, leads to greater performance improvements, even with\na reduced preference dataset size. Further analysis explores the key components\nof our framework and highlights the interplay between chart-to-code generation\nand broader chart reasoning, paving the way for future advancements in chart\ncomprehension."}
{"id": "2504.03118", "pdf": "https://arxiv.org/pdf/2504.03118", "abs": "https://arxiv.org/abs/2504.03118", "authors": ["Ziteng Wei", "Qiang He", "Bing Li", "Feifei Chen", "Yun Yang"], "title": "NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 12 figures, 6 tables", "summary": "Vision Transformers (ViTs) excel in computer vision tasks but lack\nflexibility for edge devices' diverse needs. A vital issue is that ViTs\npre-trained to cover a broad range of tasks are \\textit{over-qualified} for\nedge devices that usually demand only part of a ViT's knowledge for specific\ntasks. Their task-specific accuracy on these edge devices is suboptimal. We\ndiscovered that small ViTs that focus on device-specific tasks can improve\nmodel accuracy and in the meantime, accelerate model inference. This paper\npresents NuWa, an approach that derives small ViTs from the base ViT for edge\ndevices with specific task requirements. NuWa can transfer task-specific\nknowledge extracted from the base ViT into small ViTs that fully leverage\nconstrained resources on edge devices to maximize model accuracy with inference\nlatency assurance. Experiments with three base ViTs on three public datasets\ndemonstrate that compared with state-of-the-art solutions, NuWa improves model\naccuracy by up to $\\text{11.83}\\%$ and accelerates model inference by\n1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at\nhttps://anonymous.4open.science/r/Task_Specific-3A5E."}
{"id": "2504.02911", "pdf": "https://arxiv.org/pdf/2504.02911", "abs": "https://arxiv.org/abs/2504.02911", "authors": ["Mohammad Reza Ghasemi Madani", "Aryo Pradipta Gema", "Gabriele Sarti", "Yu Zhao", "Pasquale Minervini", "Andrea Passerini"], "title": "Noiser: Bounded Input Perturbations for Attributing Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2402.00794 by other authors", "summary": "Feature attribution (FA) methods are common post-hoc approaches that explain\nhow Large Language Models (LLMs) make predictions. Accordingly, generating\nfaithful attributions that reflect the actual inner behavior of the model is\ncrucial. In this paper, we introduce Noiser, a perturbation-based FA method\nthat imposes bounded noise on each input embedding and measures the robustness\nof the model against partially noised input to obtain the input attributions.\nAdditionally, we propose an answerability metric that employs an instructed\njudge model to assess the extent to which highly scored tokens suffice to\nrecover the predicted output. Through a comprehensive evaluation across six\nLLMs and three tasks, we demonstrate that Noiser consistently outperforms\nexisting gradient-based, attention-based, and perturbation-based FA methods in\nterms of both faithfulness and answerability, making it a robust and effective\napproach for explaining language model predictions."}
{"id": "2504.03128", "pdf": "https://arxiv.org/pdf/2504.03128", "abs": "https://arxiv.org/abs/2504.03128", "authors": ["Kahim Wong", "Jicheng Zhou", "Kemou Li", "Yain-Whar Si", "Xiaowei Wu", "Jiantao Zhou"], "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of AI-generated content brings significant concerns on the\nforensic and security issues such as source tracing, copyright protection, etc,\nhighlighting the need for effective watermarking technologies. Font-based text\nwatermarking has emerged as an effective solution to embed information, which\ncould ensure copyright, traceability, and compliance of the generated text\ncontent. Existing font watermarking methods usually neglect essential font\nknowledge, which leads to watermarked fonts of low quality and limited\nembedding capacity. These methods are also vulnerable to real-world\ndistortions, low-resolution fonts, and inaccurate character segmentation. In\nthis paper, we introduce FontGuard, a novel font watermarking model that\nharnesses the capabilities of font models and language-guided contrastive\nlearning. Unlike previous methods that focus solely on the pixel-level\nalteration, FontGuard modifies fonts by altering hidden style features,\nresulting in better font quality upon watermark embedding. We also leverage the\nfont manifold to increase the embedding capacity of our proposed method by\ngenerating substantial font variants closely resembling the original font.\nFurthermore, in the decoder, we employ an image-text contrastive learning to\nreconstruct the embedded bits, which can achieve desirable robustness against\nvarious real-world transmission distortions. FontGuard outperforms\nstate-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under\nsynthetic, cross-media, and online social network distortions, respectively,\nwhile improving the visual quality by 52.7% in terms of LPIPS. Moreover,\nFontGuard uniquely allows the generation of watermarked fonts for unseen fonts\nwithout re-training the network. The code and dataset are available at\nhttps://github.com/KAHIMWONG/FontGuard."}
{"id": "2504.02917", "pdf": "https://arxiv.org/pdf/2504.02917", "abs": "https://arxiv.org/abs/2504.02917", "authors": ["Thanathip Suenghataiphorn", "Narisara Tribuddharat", "Pojsakorn Danpanichkul", "Narathorn Kulthamrongsri"], "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Large language models (LLMs) are rapidly being integrated into\nhealthcare, promising to enhance various clinical tasks. However, concerns\nexist regarding their potential for bias, which could compromise patient care\nand exacerbate health inequities. This systematic review investigates the\nprevalence, sources, manifestations, and clinical implications of bias in LLMs.\nMethods: We conducted a systematic search of PubMed, OVID, and EMBASE from\ndatabase inception through 2025, for studies evaluating bias in LLMs applied to\nclinical tasks. We extracted data on LLM type, bias source, bias manifestation,\naffected attributes, clinical task, evaluation methods, and outcomes. Risk of\nbias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies\nmet inclusion criteria, revealing pervasive bias across various LLMs and\nclinical applications. Both data-related bias (from biased training data) and\nmodel-related bias (from model training) were significant contributors. Biases\nmanifested as: allocative harm (e.g., differential treatment recommendations);\nrepresentational harm (e.g., stereotypical associations, biased image\ngeneration); and performance disparities (e.g., variable output quality). These\nbiases affected multiple attributes, most frequently race/ethnicity and gender,\nbut also age, disability, and language. Conclusions: Bias in clinical LLMs is a\npervasive and systemic issue, with a potential to lead to misdiagnosis and\ninappropriate treatment, particularly for marginalized patient populations.\nRigorous evaluation of the model is crucial. Furthermore, the development and\nimplementation of effective mitigation strategies, coupled with continuous\nmonitoring in real-world clinical settings, are essential to ensure the safe,\nequitable, and trustworthy deployment of LLMs in healthcare."}
{"id": "2504.03133", "pdf": "https://arxiv.org/pdf/2504.03133", "abs": "https://arxiv.org/abs/2504.03133", "authors": ["Zahid Hassan Tushar", "Adeleke Ademakinwa", "Jianwu Wang", "Zhibo Zhang", "Sanjay Purushotham"], "title": "Joint Retrieval of Cloud properties using Attention-based Deep Learning Models", "categories": ["cs.CV"], "comment": "6 Pages, 4 figures, to be published in 2025 IEEE International\n  Geoscience and Remote Sensing Symposium (IGARSS 2025)", "summary": "Accurate cloud property retrieval is vital for understanding cloud behavior\nand its impact on climate, including applications in weather forecasting,\nclimate modeling, and estimating Earth's radiation balance. The Independent\nPixel Approximation (IPA), a widely used physics-based approach, simplifies\nradiative transfer calculations by assuming each pixel is independent of its\nneighbors. While computationally efficient, IPA has significant limitations,\nsuch as inaccuracies from 3D radiative effects, errors at cloud edges, and\nineffectiveness for overlapping or heterogeneous cloud fields. Recent\nAI/ML-based deep learning models have improved retrieval accuracy by leveraging\nspatial relationships across pixels. However, these models are often\nmemory-intensive, retrieve only a single cloud property, or struggle with joint\nproperty retrievals. To overcome these challenges, we introduce CloudUNet with\nAttention Module (CAM), a compact UNet-based model that employs attention\nmechanisms to reduce errors in thick, overlapping cloud regions and a\nspecialized loss function for joint retrieval of Cloud Optical Thickness (COT)\nand Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES)\ndataset show that our CAM model outperforms state-of-the-art deep learning\nmethods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER,\nand achieving 76% and 86% lower MAE for COT and CER retrievals compared to the\nIPA method."}
{"id": "2504.02921", "pdf": "https://arxiv.org/pdf/2504.02921", "abs": "https://arxiv.org/abs/2504.02921", "authors": ["Yuwei An", "Yihua Cheng", "Seo Jin Park", "Junchen Jiang"], "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."}
{"id": "2504.03135", "pdf": "https://arxiv.org/pdf/2504.03135", "abs": "https://arxiv.org/abs/2504.03135", "authors": ["Junkai Zhang", "Bin Li", "Shoujun Zhou", "Yue Du"], "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical Visual Question Answering (Med-VQA) answers clinical questions using\nmedical images, aiding diagnosis. Designing the MedVQA system holds profound\nimportance in assisting clinical diagnosis and enhancing diagnostic accuracy.\nBuilding upon this foundation, Hierarchical Medical VQA extends Medical VQA by\norganizing medical questions into a hierarchical structure and making\nlevel-specific predictions to handle fine-grained distinctions. Recently, many\nstudies have proposed hierarchical MedVQA tasks and established datasets,\nHowever, several issues still remain: (1) imperfect hierarchical modeling leads\nto poor differentiation between question levels causing semantic fragmentation\nacross hierarchies. (2) Excessive reliance on implicit learning in\nTransformer-based cross-modal self-attention fusion methods, which obscures\ncrucial local semantic correlations in medical scenarios. To address these\nissues, this study proposes a HiCA-VQA method, including two modules:\nHierarchical Prompting for fine-grained medical questions and Hierarchical\nAnswer Decoders. The hierarchical prompting module pre-aligns hierarchical text\nprompts with image features to guide the model in focusing on specific image\nregions according to question types, while the hierarchical decoder performs\nseparate predictions for questions at different levels to improve accuracy\nacross granularities. The framework also incorporates a cross-attention fusion\nmodule where images serve as queries and text as key-value pairs. Experiments\non the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better\noutperforms existing state-of-the-art methods in answering hierarchical\nfine-grained questions. This study provides an effective pathway for\nhierarchical visual question answering systems, advancing medical image\nunderstanding."}
{"id": "2504.02953", "pdf": "https://arxiv.org/pdf/2504.02953", "abs": "https://arxiv.org/abs/2504.02953", "authors": ["Chen Cecilia Liu", "Anna Korhonen", "Iryna Gurevych"], "title": "Cultural Learning-Based Culture Adaptation of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Adapting large language models (LLMs) to diverse cultural values is a\nchallenging task, as existing LLMs often reflect the values of specific groups\nby default, and potentially causing harm to others. In this paper, we present\nCLCA, a novel framework for enhancing LLM alignment with cultural values based\non cultural learning. The framework leverages simulated social interactions to\ngenerate conversations in which LLMs engage in role-playing within culturally\nadapted social scenarios, capturing implicit cultural norms for model\nfine-tuning. CLCA improves cultural value alignment across various model\narchitectures measured using World Value Survey data, demonstrating the\neffectiveness of our proposed approach. Our results provide early evidence that\nunderstanding intent and social interactions can enhance cultural value\nadaptation in LLMs, highlighting the promise of training approaches based on\ncultural learning."}
{"id": "2504.03136", "pdf": "https://arxiv.org/pdf/2504.03136", "abs": "https://arxiv.org/abs/2504.03136", "authors": ["Xin Jin", "Simon Niklaus", "Zhoutong Zhang", "Zhihao Xia", "Chunle Guo", "Yuting Yang", "Jiawen Chen", "Chongyi Li"], "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable", "categories": ["cs.CV"], "comment": "Homepage: https://srameo.github.io/projects/levd/", "summary": "Denoising is a crucial step in many video processing pipelines such as in\ninteractive editing, where high quality, speed, and user control are essential.\nWhile recent approaches achieve significant improvements in denoising quality\nby leveraging deep learning, they are prone to unexpected failures due to\ndiscrepancies between training data distributions and the wide variety of noise\npatterns found in real-world videos. These methods also tend to be slow and\nlack user control. In contrast, traditional denoising methods perform reliably\non in-the-wild videos and run relatively quickly on modern hardware. However,\nthey require manually tuning parameters for each input video, which is not only\ntedious but also requires skill. We bridge the gap between these two paradigms\nby proposing a differentiable denoising pipeline based on traditional methods.\nA neural network is then trained to predict the optimal denoising parameters\nfor each specific input, resulting in a robust and efficient approach that also\nsupports user control."}
{"id": "2504.02956", "pdf": "https://arxiv.org/pdf/2504.02956", "abs": "https://arxiv.org/abs/2504.02956", "authors": ["Shu Yang", "Junchao Wu", "Xin Chen", "Yunze Xiao", "Xinyi Yang", "Derek F. Wong", "Di Wang"], "title": "Understanding Aha Moments: from External Observations to Internal Mechanisms", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), capable of reasoning through complex problems,\nhave become crucial for tasks like programming, mathematics, and commonsense\nreasoning. However, a key challenge lies in understanding how these models\nacquire reasoning capabilities and exhibit \"aha moments\" when they reorganize\ntheir methods to allocate more thinking time to problems. In this work, we\nsystematically study \"aha moments\" in LRMs, from linguistic patterns,\ndescription of uncertainty, \"Reasoning Collapse\" to analysis in latent space.\nWe demonstrate that the \"aha moment\" is externally manifested in a more\nfrequent use of anthropomorphic tones for self-reflection and an adaptive\nadjustment of uncertainty based on problem difficulty. This process helps the\nmodel complete reasoning without succumbing to \"Reasoning Collapse\".\nInternally, it corresponds to a separation between anthropomorphic\ncharacteristics and pure reasoning, with an increased anthropomorphic tone for\nmore difficult problems. Furthermore, we find that the \"aha moment\" helps\nmodels solve complex problems by altering their perception of problem\ndifficulty. As the layer of the model increases, simpler problems tend to be\nperceived as more complex, while more difficult problems appear simpler."}
{"id": "2504.03140", "pdf": "https://arxiv.org/pdf/2504.03140", "abs": "https://arxiv.org/abs/2504.03140", "authors": ["Xuran Ma", "Yexin Liu", "Yaofu Liu", "Xianfeng Wu", "Mingzhe Zheng", "Zihao Wang", "Ser-Nam Lim", "Harry Yang"], "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."}
{"id": "2504.02965", "pdf": "https://arxiv.org/pdf/2504.02965", "abs": "https://arxiv.org/abs/2504.02965", "authors": ["Abhishek Sharma", "Dan Goldwasser"], "title": "CoLa -- Learning to Interactively Collaborate with Large LMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "LLMs' remarkable ability to tackle a wide range of language tasks opened new\nopportunities for collaborative human-AI problem solving. LLMs can amplify\nhuman capabilities by applying their intuitions and reasoning strategies at\nscale. We explore whether human guides can be simulated, by generalizing from\nhuman demonstrations of guiding an AI system to solve complex language\nproblems. We introduce CoLa, a novel self-guided learning paradigm for training\nautomated $\\textit{guides}$ and evaluate it on two QA datasets, a\npuzzle-solving task, and a constrained text generation task. Our empirical\nresults show that CoLa consistently outperforms competitive approaches across\nall domains. Moreover, a small-sized trained guide outperforms a strong model\nlike GPT-4 when acting as a guide. We compare the strategies employed by humans\nand automated guides by conducting a human study on a QA dataset. We show that\nautomated guides outperform humans by adapting their strategies to reasoners'\ncapabilities and conduct qualitative analyses highlighting distinct differences\nin guiding strategies."}
{"id": "2504.03154", "pdf": "https://arxiv.org/pdf/2504.03154", "abs": "https://arxiv.org/abs/2504.03154", "authors": ["Junshan Hu", "Jialiang Mao", "Zhikang Liu", "Zhongpu Xia", "Peng Jia", "Xianpeng Lang"], "title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference", "categories": ["cs.CV"], "comment": null, "summary": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding."}
{"id": "2504.02973", "pdf": "https://arxiv.org/pdf/2504.02973", "abs": "https://arxiv.org/abs/2504.02973", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "A Bayesian account of pronoun and neopronoun acquisition", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "A major challenge to equity among members of queer communities is the use of\none's chosen forms of reference, such as personal names or pronouns. Speakers\noften dismiss their misuses of pronouns as \"unintentional\", and claim that\ntheir errors reflect many decades of fossilized mainstream language use, as\nwell as attitudes or expectations about the relationship between one's\nappearance and acceptable forms of reference. We argue for explicitly modeling\nindividual differences in pronoun selection and present a probabilistic\ngraphical modeling approach based on the nested Chinese Restaurant Franchise\nProcess (nCRFP) (Ahmed et al., 2013) to account for flexible pronominal\nreference such as chosen names and neopronouns while moving beyond\nform-to-meaning mappings and without lexical co-occurrence statistics to learn\nreferring expressions, as in contemporary language models. We show that such a\nmodel can account for variability in how quickly pronouns or names are\nintegrated into symbolic knowledge and can empower computational systems to be\nboth flexible and respectful of queer people with diverse gender expression."}
{"id": "2504.03164", "pdf": "https://arxiv.org/pdf/2504.03164", "abs": "https://arxiv.org/abs/2504.03164", "authors": ["Kexin Tian", "Jingrui Mao", "Yunlong Zhang", "Jiwan Jiang", "Yang Zhou", "Zhengzhong Tu"], "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning."}
{"id": "2504.02983", "pdf": "https://arxiv.org/pdf/2504.02983", "abs": "https://arxiv.org/abs/2504.02983", "authors": ["Xiaoyu Tong", "Zhi Zhang", "Martha Lewis", "Ekaterina Shutova"], "title": "Hummus: A Dataset of Humorous Multimodal Metaphor Use", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Metaphor and humor share a lot of common ground, and metaphor is one of the\nmost common humorous mechanisms. This study focuses on the humorous capacity of\nmultimodal metaphors, which has not received due attention in the community. We\ntake inspiration from the Incongruity Theory of humor, the Conceptual Metaphor\nTheory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and\ndeveloped a novel annotation scheme for humorous multimodal metaphor use in\nimage-caption pairs. We create the Hummus Dataset of Humorous Multimodal\nMetaphor Use, providing expert annotation on 1k image-caption pairs sampled\nfrom the New Yorker Caption Contest corpus. Using the dataset, we test\nstate-of-the-art multimodal large language models (MLLMs) on their ability to\ndetect and understand humorous multimodal metaphor use. Our experiments show\nthat current MLLMs still struggle with processing humorous multimodal\nmetaphors, particularly with regard to integrating visual and textual\ninformation. We release our dataset and code at\ngithub.com/xiaoyuisrain/humorous-multimodal-metaphor-use."}
{"id": "2504.03166", "pdf": "https://arxiv.org/pdf/2504.03166", "abs": "https://arxiv.org/abs/2504.03166", "authors": ["Hanbo Bi", "Yingchao Feng", "Boyuan Tong", "Mengyu Wang", "Haichen Yu", "Yongqiang Mao", "Hao Chang", "Wenhui Diao", "Peijin Wang", "Yue Yu", "Hanyang Peng", "Yehong Zhang", "Kun Fu", "Xian Sun"], "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning."}
{"id": "2504.03022", "pdf": "https://arxiv.org/pdf/2504.03022", "abs": "https://arxiv.org/abs/2504.03022", "authors": ["Sheridan Feucht", "Eric Todd", "Byron Wallace", "David Bau"], "title": "The Dual-Route Model of Induction", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "36 pages, 39 figures. Code and data at https://dualroute.baulab.info", "summary": "Prior work on in-context copying has shown the existence of induction heads,\nwhich attend to and promote individual tokens during copying. In this work we\nintroduce a new type of induction head: concept-level induction heads, which\ncopy entire lexical units instead of individual tokens. Concept induction heads\nlearn to attend to the ends of multi-token words throughout training, working\nin parallel with token-level induction heads to copy meaningful text. We show\nthat these heads are responsible for semantic tasks like word-level\ntranslation, whereas token induction heads are vital for tasks that can only be\ndone verbatim, like copying nonsense tokens. These two \"routes\" operate\nindependently: in fact, we show that ablation of token induction heads causes\nmodels to paraphrase where they would otherwise copy verbatim. In light of\nthese findings, we argue that although token induction heads are vital for\nspecific tasks, concept induction heads may be more broadly relevant for\nin-context learning."}
{"id": "2504.03168", "pdf": "https://arxiv.org/pdf/2504.03168", "abs": "https://arxiv.org/abs/2504.03168", "authors": ["Lucas Choi", "Ross Greer"], "title": "Finding the Reflection Point: Unpadding Images to Remove Data Augmentation Artifacts in Large Open Source Image Datasets for Machine Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we address a novel image restoration problem relevant to\nmachine learning dataset curation: the detection and removal of noisy mirrored\npadding artifacts. While data augmentation techniques like padding are\nnecessary for standardizing image dimensions, they can introduce artifacts that\ndegrade model evaluation when datasets are repurposed across domains. We\npropose a systematic algorithm to precisely delineate the reflection boundary\nthrough a minimum mean squared error approach with thresholding and remove\nreflective padding. Our method effectively identifies the transition between\nauthentic content and its mirrored counterpart, even in the presence of\ncompression or interpolation noise. We demonstrate our algorithm's efficacy on\nthe SHEL5k dataset, showing significant performance improvements in zero-shot\nobject detection tasks using OWLv2, with average precision increasing from 0.47\nto 0.61 for hard hat detection and from 0.68 to 0.73 for person detection. By\naddressing annotation inconsistencies and distorted objects in padded regions,\nour approach enhances dataset integrity, enabling more reliable model\nevaluation across computer vision tasks."}
{"id": "2504.03036", "pdf": "https://arxiv.org/pdf/2504.03036", "abs": "https://arxiv.org/abs/2504.03036", "authors": ["Zébulon Goriely", "Paula Buttery"], "title": "IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling", "categories": ["cs.CL"], "comment": "19 pages, 7 figures. Submitted to CoNLL 2025", "summary": "In this paper, we introduce two resources: (i) G2P+, a tool for converting\northographic datasets to a consistent phonemic representation; and (ii) IPA\nCHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior\ntools for grapheme-to-phoneme conversion result in phonemic vocabularies that\nare inconsistent with established phonemic inventories, an issue which G2P+\naddresses by leveraging the inventories in the Phoible database. Using this\ntool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES.\nThis new resource fills several gaps in existing phonemic datasets, which often\nlack multilingual coverage, spontaneous speech, and a focus on child-directed\nlanguage. We demonstrate the utility of this dataset for phonological research\nby training phoneme language models on 11 languages and probing them for\ndistinctive features, finding that the distributional properties of phonemes\nare sufficient to learn major class and place features cross-lingually."}
{"id": "2504.03169", "pdf": "https://arxiv.org/pdf/2504.03169", "abs": "https://arxiv.org/abs/2504.03169", "authors": ["Shabnam Choudhury", "Yash Salunkhe", "Sarthak Mehrotra", "Biplab Banerjee"], "title": "REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The rapid expansion of remote sensing image archives demands the development\nof strong and efficient techniques for content-based image retrieval (RS-CBIR).\nThis paper presents REJEPA (Retrieval with Joint-Embedding Predictive\nArchitecture), an innovative self-supervised framework designed for unimodal\nRS-CBIR. REJEPA utilises spatially distributed context token encoding to\nforecast abstract representations of target tokens, effectively capturing\nhigh-level semantic features and eliminating unnecessary pixel-level details.\nIn contrast to generative methods that focus on pixel reconstruction or\ncontrastive techniques that depend on negative pairs, REJEPA functions within\nfeature space, achieving a reduction in computational complexity of 40-60% when\ncompared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To\nguarantee strong and varied representations, REJEPA incorporates\nVariance-Invariance-Covariance Regularisation (VICReg), which prevents encoder\ncollapse by promoting feature diversity and reducing redundancy. The method\ndemonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K\n(S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel\ncompared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE,\nScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and\nSAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across\nsensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for\nefficient, scalable, and precise RS-CBIR, addressing challenges like varying\nresolutions, high object density, and complex backgrounds with computational\nefficiency."}
{"id": "2504.03045", "pdf": "https://arxiv.org/pdf/2504.03045", "abs": "https://arxiv.org/abs/2504.03045", "authors": ["Antonio Castaldo", "Sheila Castilho", "Joss Moorkens", "Johanna Monti"], "title": "Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing", "categories": ["cs.CL"], "comment": "to be published in the Proceedings of the 20th Machine Translation\n  Summit (MT Summit 2025)", "summary": "Post-editing machine translation (MT) for creative texts, such as literature,\nrequires balancing efficiency with the preservation of creativity and style.\nWhile neural MT systems struggle with these challenges, large language models\n(LLMs) offer improved capabilities for context-aware and creative translation.\nThis study evaluates the feasibility of post-editing literary translations\ngenerated by LLMs. Using a custom research tool, we collaborated with\nprofessional literary translators to analyze editing time, quality, and\ncreativity. Our results indicate that post-editing LLM-generated translations\nsignificantly reduces editing time compared to human translation while\nmaintaining a similar level of creativity. The minimal difference in creativity\nbetween PE and MT, combined with substantial productivity gains, suggests that\nLLMs may effectively support literary translators working with high-resource\nlanguages."}
{"id": "2504.03171", "pdf": "https://arxiv.org/pdf/2504.03171", "abs": "https://arxiv.org/abs/2504.03171", "authors": ["Zeyang Zheng", "Arman Hosseini", "Dong Chen", "Omid Shoghli", "Arsalan Heydarian"], "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)", "summary": "The increasing adoption of electric scooters (e-scooters) in urban areas has\ncoincided with a rise in traffic accidents and injuries, largely due to their\nsmall wheels, lack of suspension, and sensitivity to uneven surfaces. While\ndeep learning-based object detection has been widely used to improve automobile\nsafety, its application for e-scooter obstacle detection remains unexplored.\nThis study introduces a novel ground obstacle detection system for e-scooters,\nintegrating an RGB camera, and a depth camera to enhance real-time road hazard\ndetection. Additionally, the Inertial Measurement Unit (IMU) measures linear\nvertical acceleration to identify surface vibrations, guiding the selection of\nsix obstacle categories: tree branches, manhole covers, potholes, pine cones,\nnon-directional cracks, and truncated domes. All sensors, including the RGB\ncamera, depth camera, and IMU, are integrated within the Intel RealSense Camera\nD435i. A deep learning model powered by YOLO detects road hazards and utilizes\ndepth data to estimate obstacle proximity. Evaluated on the seven hours of\nnaturalistic riding dataset, the system achieves a high mean average precision\n(mAP) of 0.827 and demonstrates excellent real-time performance. This approach\nprovides an effective solution to enhance e-scooter safety through advanced\ncomputer vision and data fusion. The dataset is accessible at\nhttps://zenodo.org/records/14583718, and the project code is hosted on\nhttps://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters."}
{"id": "2504.03051", "pdf": "https://arxiv.org/pdf/2504.03051", "abs": "https://arxiv.org/abs/2504.03051", "authors": ["Chengyang He", "Wenlong Zhang", "Violet Xinying Chen", "Yue Ning", "Ping Wang"], "title": "Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures, 5 Tables, ACM/IEEE International Conference on\n  Connected Health: Applications, Systems and Engineering Technologies (CHASE\n  '25), June 24--26, 2025, New York, NY, USA", "summary": "Accurate medical symptom coding from unstructured clinical text, such as\nvaccine safety reports, is a critical task with applications in\npharmacovigilance and safety monitoring. Symptom coding, as tailored in this\nstudy, involves identifying and linking nuanced symptom mentions to\nstandardized vocabularies like MedDRA, differentiating it from broader medical\ncoding tasks. Traditional approaches to this task, which treat symptom\nextraction and linking as independent workflows, often fail to handle the\nvariability and complexity of clinical narratives, especially for rare cases.\nRecent advancements in Large Language Models (LLMs) offer new opportunities but\nface challenges in achieving consistent performance. To address these issues,\nwe propose Task as Context (TACO) Prompting, a novel framework that unifies\nextraction and linking tasks by embedding task-specific context into LLM\nprompts. Our study also introduces SYMPCODER, a human-annotated dataset derived\nfrom Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage\nevaluation framework to comprehensively assess both symptom linking and mention\nfidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat,\nJackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's\neffectiveness in improving flexibility and accuracy for tailored tasks like\nsymptom coding, paving the way for more specific coding tasks and advancing\nclinical text processing methodologies."}
{"id": "2504.03177", "pdf": "https://arxiv.org/pdf/2504.03177", "abs": "https://arxiv.org/abs/2504.03177", "authors": ["Yuki Kawana", "Tatsuya Harada"], "title": "Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2023", "summary": "We propose an end-to-end trainable, cross-category method for reconstructing\nmultiple man-made articulated objects from a single RGBD image, focusing on\npart-level shape reconstruction and pose and kinematics estimation. We depart\nfrom previous works that rely on learning instance-level latent space, focusing\non man-made articulated objects with predefined part counts. Instead, we\npropose a novel alternative approach that employs part-level representation,\nrepresenting instances as combinations of detected parts. While our\ndetect-then-group approach effectively handles instances with diverse part\nstructures and various part counts, it faces issues of false positives, varying\npart sizes and scales, and an increasing model size due to end-to-end training.\nTo address these challenges, we propose 1) test-time kinematics-aware part\nfusion to improve detection performance while suppressing false positives, 2)\nanisotropic scale normalization for part shape learning to accommodate various\npart sizes and scales, and 3) a balancing strategy for cross-refinement between\nfeature space and output space to improve part detection while maintaining\nmodel size. Evaluation on both synthetic and real data demonstrates that our\nmethod successfully reconstructs variously structured multiple instances that\nprevious works cannot handle, and outperforms prior works in shape\nreconstruction and kinematics estimation."}
{"id": "2504.03071", "pdf": "https://arxiv.org/pdf/2504.03071", "abs": "https://arxiv.org/abs/2504.03071", "authors": ["Ziyu Liu", "Lintao Tang", "Zeliang Sun", "Zhengliang Liu", "Yanjun Lyu", "Wei Ruan", "Yangshuang Xu", "Liang Shan", "Jiyoon Shin", "Xiaohe Chen", "Dajiang Zhu", "Tianming Liu", "Rongjie Liu", "Chao Huang"], "title": "AD-GPT: Large Language Models in Alzheimer's Disease", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for medical\ninformation retrieval, yet their accuracy and depth remain limited in\nspecialized domains such as Alzheimer's disease (AD), a growing global health\nchallenge. To address this gap, we introduce AD-GPT, a domain-specific\ngenerative pre-trained transformer designed to enhance the retrieval and\nanalysis of AD-related genetic and neurobiological information. AD-GPT\nintegrates diverse biomedical data sources, including potential AD-associated\ngenes, molecular genetic information, and key gene variants linked to brain\nregions. We develop a stacked LLM architecture combining Llama3 and BERT,\noptimized for four critical tasks in AD research: (1) genetic information\nretrieval, (2) gene-brain region relationship assessment, (3) gene-AD\nrelationship analysis, and (4) brain region-AD relationship mapping.\nComparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's\nsuperior precision and reliability across these tasks, underscoring its\npotential as a robust and specialized AI tool for advancing AD research and\nbiomarker discovery."}
{"id": "2504.03181", "pdf": "https://arxiv.org/pdf/2504.03181", "abs": "https://arxiv.org/abs/2504.03181", "authors": ["Shabnam Choudhury", "Akhil Vasim", "Michael Schmitt", "Biplab Banerjee"], "title": "MIMRS: A Survey on Masked Image Modeling in Remote Sensing", "categories": ["cs.CV"], "comment": "6 pages", "summary": "Masked Image Modeling (MIM) is a self-supervised learning technique that\ninvolves masking portions of an image, such as pixels, patches, or latent\nrepresentations, and training models to predict the missing information using\nthe visible context. This approach has emerged as a cornerstone in\nself-supervised learning, unlocking new possibilities in visual understanding\nby leveraging unannotated data for pre-training. In remote sensing, MIM\naddresses challenges such as incomplete data caused by cloud cover, occlusions,\nand sensor limitations, enabling applications like cloud removal, multi-modal\ndata fusion, and super-resolution. By synthesizing and critically analyzing\nrecent advancements, this survey (MIMRS) is a pioneering effort to chart the\nlandscape of mask image modeling in remote sensing. We highlight\nstate-of-the-art methodologies, applications, and future research directions,\nproviding a foundational review to guide innovation in this rapidly evolving\nfield."}
{"id": "2504.03101", "pdf": "https://arxiv.org/pdf/2504.03101", "abs": "https://arxiv.org/abs/2504.03101", "authors": ["Weili Cao", "Jianyou Wang", "Youze Zheng", "Longtian Bao", "Qirui Zheng", "Taylor Berg-Kirkpatrick", "Ramamohan Paturi", "Leon Bergen"], "title": "Single-Pass Document Scanning for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Handling extremely large documents for question answering is challenging:\nchunk-based embedding methods often lose track of important global context,\nwhile full-context transformers can be prohibitively expensive for hundreds of\nthousands of tokens. We propose a single-pass document scanning approach that\nprocesses the entire text in linear time, preserving global coherence while\ndeciding which sentences are most relevant to the query. On 41 QA benchmarks,\nour single-pass scanner consistently outperforms chunk-based embedding methods\nand competes with large language models at a fraction of the computational\ncost. By conditioning on the entire preceding context without chunk breaks, the\nmethod preserves global coherence, which is especially important for long\ndocuments. Overall, single-pass document scanning offers a simple solution for\nquestion answering over massive text. All code, datasets, and model checkpoints\nare available at https://github.com/MambaRetriever/MambaRetriever"}
{"id": "2504.03191", "pdf": "https://arxiv.org/pdf/2504.03191", "abs": "https://arxiv.org/abs/2504.03191", "authors": ["Sandra Bergmann", "Fabian Brand", "Christian Riess"], "title": "Three Forensic Cues for JPEG AI Images", "categories": ["cs.CV"], "comment": null, "summary": "The JPEG standard was vastly successful. Currently, the first AI-based\ncompression method ``JPEG AI'' will be standardized. JPEG AI brings remarkable\nbenefits. JPEG AI images exhibit impressive image quality at bitrates that are\nan order of magnitude lower than images compressed with traditional JPEG.\nHowever, forensic analysis of JPEG AI has to be completely re-thought: forensic\ntools for traditional JPEG do not transfer to JPEG AI, and artifacts from JPEG\nAI are easily confused with artifacts from artificially generated images\n(``DeepFakes''). This creates a need for novel forensic approaches to detection\nand distinction of JPEG AI images. In this work, we make a first step towards a\nforensic JPEG AI toolset. We propose three cues for forensic algorithms for\nJPEG AI. These algorithms address three forensic questions: first, we show that\nthe JPEG AI preprocessing introduces correlations in the color channels that do\nnot occur in uncompressed images. Second, we show that repeated compression of\nJPEG AI images leads to diminishing distortion differences. This can be used to\ndetect recompression, in a spirit similar to some classic JPEG forensics\nmethods. Third, we show that the quantization of JPEG AI images in the latent\nspace can be used to distinguish real images with JPEG AI compression from\nsynthetically generated images. The proposed methods are interpretable for a\nforensic analyst, and we hope that they inspire further research in the\nforensics of AI-compressed images."}
{"id": "2504.03151", "pdf": "https://arxiv.org/pdf/2504.03151", "abs": "https://arxiv.org/abs/2504.03151", "authors": ["Jing Bi", "Susan Liang", "Xiaofei Zhou", "Pinxin Liu", "Junjia Guo", "Yunlong Tang", "Luchuan Song", "Chao Huang", "Guangyu Sun", "Jinxi He", "Jiarui Wu", "Shu Yang", "Daoan Zhang", "Chen Chen", "Lianggong Bruce Wen", "Zhang Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research."}
{"id": "2504.03193", "pdf": "https://arxiv.org/pdf/2504.03193", "abs": "https://arxiv.org/abs/2504.03193", "authors": ["Xin Zhang", "Robby T. Tan"], "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser."}
{"id": "2504.03159", "pdf": "https://arxiv.org/pdf/2504.03159", "abs": "https://arxiv.org/abs/2504.03159", "authors": ["Junlang Qian", "Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Zepeng Zhai", "Kezhi Mao"], "title": "Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 (main Oral)", "summary": "Zero-shot text classification typically relies on prompt engineering, but the\ninherent prompt brittleness of large language models undermines its\nreliability. Minor changes in prompt can cause significant discrepancies in\nmodel performance. We attribute this prompt brittleness largely to the narrow\nfocus on nexttoken probabilities in existing methods. To address this, we\npropose Placeholding Parallel Prediction (P3), a novel approach that predicts\ntoken probabilities across multiple positions and simulates comprehensive\nsampling of generation paths in a single run of a language model. Experiments\nshow improved accuracy and up to 98% reduction in the standard deviation across\nprompts, boosting robustness. Even without a prompt, P3 maintains comparable\nperformance, reducing the need for prompt engineering."}
{"id": "2504.03198", "pdf": "https://arxiv.org/pdf/2504.03198", "abs": "https://arxiv.org/abs/2504.03198", "authors": ["Jiaxin Guo", "Wenzhen Dong", "Tianyu Huang", "Hao Ding", "Ziyi Wang", "Haomin Kuang", "Qi Dou", "Yun-Hui Liu"], "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/."}
{"id": "2504.03165", "pdf": "https://arxiv.org/pdf/2504.03165", "abs": "https://arxiv.org/abs/2504.03165", "authors": ["Weitao Li", "Kaiming Liu", "Xiangyu Zhang", "Xuanyu Lei", "Weizhi Ma", "Yang Liu"], "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG."}
{"id": "2504.03219", "pdf": "https://arxiv.org/pdf/2504.03219", "abs": "https://arxiv.org/abs/2504.03219", "authors": ["Simrandeep Singh", "Shreya Bansal", "Abdulmotaleb El Saddik", "Mukesh Saini"], "title": "From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution, Deviation, and Future Implications in AI-Language Models", "categories": ["cs.CV"], "comment": "10 pages, 1 figure, 4 tables", "summary": "The rapid advancement of artificial intelligence (AI) has reshaped the field\nof natural language processing (NLP), with models like OpenAI ChatGPT and\nDeepSeek AI. Although ChatGPT established a strong foundation for\nconversational AI, DeepSeek AI introduces significant improvements in\narchitecture, performance, and ethical considerations. This paper presents a\ndetailed analysis of the evolution from ChatGPT to DeepSeek AI, highlighting\ntheir technical differences, practical applications, and broader implications\nfor AI development. To assess their capabilities, we conducted a case study\nusing a predefined set of multiple choice questions in various domains,\nevaluating the strengths and limitations of each model. By examining these\naspects, we provide valuable insight into the future trajectory of AI, its\npotential to transform industries, and key research directions for improving\nAI-driven language models."}
{"id": "2504.03174", "pdf": "https://arxiv.org/pdf/2504.03174", "abs": "https://arxiv.org/abs/2504.03174", "authors": ["Abhishek Singhania", "Christophe Dupuy", "Shivam Mangale", "Amani Namboori"], "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs", "categories": ["cs.CL"], "comment": "Accepted at TrustNLP@NAACL 2025", "summary": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities."}
{"id": "2504.03221", "pdf": "https://arxiv.org/pdf/2504.03221", "abs": "https://arxiv.org/abs/2504.03221", "authors": ["Jungpil Shin", "Abu Saleh Musa Miah", "Sota Konnai", "Shu Hoshitaka", "Pankoo Kim"], "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG)\nis challenging due to unstable predictions and inefficient time-varying feature\nenhancement. To overcome the lack of signal based time-varying feature\nproblems, we propose a lightweight squeeze-excitation deep learning-based multi\nstream spatial temporal dynamics time-varying feature extraction approach to\nbuild an effective sEMG-based hand gesture recognition system. Each branch of\nthe proposed model was designed to extract hierarchical features, capturing\nboth global and detailed spatial-temporal relationships to ensure feature\neffectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN),\nfocuses on capturing long-term temporal dependencies by modelling past and\nfuture temporal contexts, providing a holistic view of gesture dynamics. The\nsecond branch, incorporating a 1D Convolutional layer, separable CNN, and\nSqueeze-and-Excitation (SE) block, efficiently extracts spatial-temporal\nfeatures while emphasizing critical feature channels, enhancing feature\nrelevance. The third branch, combining a Temporal Convolutional Network (TCN)\nand Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships\nand time-varying patterns. Outputs from all branches are fused using\nconcatenation to capture subtle variations in the data and then refined with a\nchannel attention module, selectively focusing on the most informative features\nwhile improving computational efficiency. The proposed model was tested on the\nNinapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%,\nand 93.34%, respectively. These results demonstrate the capability of the\nsystem to handle complex sEMG dynamics, offering advancements in prosthetic\nlimb control and human-machine interface technologies with significant\nimplications for assistive technologies."}
{"id": "2504.03185", "pdf": "https://arxiv.org/pdf/2504.03185", "abs": "https://arxiv.org/abs/2504.03185", "authors": ["Jaymari Chua", "Chen Wang", "Lina Yao"], "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4; I.2.6; I.2.8"], "comment": null, "summary": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings."}
{"id": "2504.03230", "pdf": "https://arxiv.org/pdf/2504.03230", "abs": "https://arxiv.org/abs/2504.03230", "authors": ["Yasmine Mustafa", "Mohamed Elmahallawy", "Tie Luo"], "title": "Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Alzheimer's disease (AD) leads to progressive cognitive decline, making early\ndetection crucial for effective intervention. While deep learning models have\nshown high accuracy in AD diagnosis, their lack of interpretability limits\nclinical trust and adoption. This paper introduces a novel pre-model approach\nleveraging Jacobian Maps (JMs) within a multi-modal framework to enhance\nexplainability and trustworthiness in AD detection. By capturing localized\nbrain volume changes, JMs establish meaningful correlations between model\npredictions and well-known neuroanatomical biomarkers of AD. We validate JMs\nthrough experiments comparing a 3D CNN trained on JMs versus on traditional\npreprocessed data, which demonstrates superior accuracy. We also employ 3D\nGrad-CAM analysis to provide both visual and quantitative insights, further\nshowcasing improved interpretability and diagnostic reliability."}
{"id": "2504.03197", "pdf": "https://arxiv.org/pdf/2504.03197", "abs": "https://arxiv.org/abs/2504.03197", "authors": ["Jaewoo Park", "Jungyang Park", "Dongju Jang", "Jiwan Chung", "Byungwoo Yoo", "Jaewoo Shin", "Seonjoon Park", "Taehyeong Kim", "Youngjae Yu"], "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation", "categories": ["cs.CL"], "comment": "18 pages, 4 figures", "summary": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly."}
{"id": "2504.03235", "pdf": "https://arxiv.org/pdf/2504.03235", "abs": "https://arxiv.org/abs/2504.03235", "authors": ["Ibne Farabi Shihab", "Anuj Sharma"], "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication."}
{"id": "2504.03206", "pdf": "https://arxiv.org/pdf/2504.03206", "abs": "https://arxiv.org/abs/2504.03206", "authors": ["Yanming Wan", "Jiaxing Wu", "Marwa Abdulhai", "Lior Shani", "Natasha Jaques"], "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them."}
{"id": "2504.03241", "pdf": "https://arxiv.org/pdf/2504.03241", "abs": "https://arxiv.org/abs/2504.03241", "authors": ["Marius Graumann", "Jan Marius Stürmer", "Tobias Koch"], "title": "Rotation Invariance in Floor Plan Digitization using Zernike Moments", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "17 pages, 5 figures", "summary": "Nowadays, a lot of old floor plans exist in printed form or are stored as\nscanned raster images. Slight rotations or shifts may occur during scanning.\nBringing floor plans of this form into a machine readable form to enable\nfurther use, still poses a problem. Therefore, we propose an end-to-end\npipeline that pre-processes the image and leverages a novel approach to create\na region adjacency graph (RAG) from the pre-processed image and predict its\nnodes. By incorporating normalization steps into the RAG feature extraction, we\nsignificantly improved the rotation invariance of the RAG feature calculation.\nMoreover, applying our method leads to an improved F1 score and IoU on rotated\ndata. Furthermore, we proposed a wall splitting algorithm for partitioning\nwalls into segments associated with the corresponding rooms."}
{"id": "2504.03234", "pdf": "https://arxiv.org/pdf/2504.03234", "abs": "https://arxiv.org/abs/2504.03234", "authors": ["Junjie Yang", "Ke Lin", "Xing Yu"], "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages", "summary": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\""}
{"id": "2504.03249", "pdf": "https://arxiv.org/pdf/2504.03249", "abs": "https://arxiv.org/abs/2504.03249", "authors": ["Piet Brömmel", "Dominik Brämer", "Oliver Urbann", "Diana Kleingarn"], "title": "Robot Localization Using a Learned Keypoint Detector and Descriptor with a Floor Camera and a Feature Rich Industrial Floor", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The localization of moving robots depends on the availability of good\nfeatures from the environment. Sensor systems like Lidar are popular, but\nunique features can also be extracted from images of the ground. This work\npresents the Keypoint Localization Framework (KOALA), which utilizes deep\nneural networks that extract sufficient features from an industrial floor for\naccurate localization without having readable markers. For this purpose, we use\na floor covering that can be produced as cheaply as common industrial floors.\nAlthough we do not use any filtering, prior, or temporal information, we can\nestimate our position in 75.7 % of all images with a mean position error of 2\ncm and a rotation error of 2.4 %. Thus, the robot kidnapping problem can be\nsolved with high precision in every frame, even while the robot is moving.\nFurthermore, we show that our framework with our detector and descriptor\ncombination is able to outperform comparable approaches."}
{"id": "2504.03295", "pdf": "https://arxiv.org/pdf/2504.03295", "abs": "https://arxiv.org/abs/2504.03295", "authors": ["Bingqian Wang", "Quan Fang", "Jiachen Sun", "Xiaoxiao Ma"], "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research."}
{"id": "2504.03254", "pdf": "https://arxiv.org/pdf/2504.03254", "abs": "https://arxiv.org/abs/2504.03254", "authors": ["Yimin Wei", "Aoran Xiao", "Yexian Ren", "Yuting Zhu", "Hongruixuan Chen", "Junshi Xia", "Naoto Yokoya"], "title": "SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic Aperture Radar (SAR) is a crucial remote sensing technology,\nenabling all-weather, day-and-night observation with strong surface penetration\nfor precise and continuous environmental monitoring and analysis. However, SAR\nimage interpretation remains challenging due to its complex physical imaging\nmechanisms and significant visual disparities from human perception. Recently,\nVision-Language Models (VLMs) have demonstrated remarkable success in RGB image\nunderstanding, offering powerful open-vocabulary interpretation and flexible\nlanguage interaction. However, their application to SAR images is severely\nconstrained by the absence of SAR-specific knowledge in their training\ndistributions, leading to suboptimal performance. To address this limitation,\nwe introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR\nimage understanding, with a primary focus on integrating SAR with textual\nmodality. SARLANG-1M comprises more than 1 million high-quality SAR image-text\npairs collected from over 59 cities worldwide. It features hierarchical\nresolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions\n(including both concise and detailed captions), diverse remote sensing\ncategories (1,696 object types and 16 land cover classes), and multi-task\nquestion-answering pairs spanning seven applications and 1,012 question types.\nExtensive experiments on mainstream VLMs demonstrate that fine-tuning with\nSARLANG-1M significantly enhances their performance in SAR image\ninterpretation, reaching performance comparable to human experts. The dataset\nand code will be made publicly available at\nhttps://github.com/Jimmyxichen/SARLANG-1M."}
{"id": "2504.03302", "pdf": "https://arxiv.org/pdf/2504.03302", "abs": "https://arxiv.org/abs/2504.03302", "authors": ["Afshin Khadangi", "Amir Sartipi", "Igor Tchappi", "Ramin Bahmani"], "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility."}
{"id": "2504.03258", "pdf": "https://arxiv.org/pdf/2504.03258", "abs": "https://arxiv.org/abs/2504.03258", "authors": ["Shuxiao Ding", "Yutong Yang", "Julian Wiederer", "Markus Braun", "Peizheng Li", "Juergen Gall", "Bin Yang"], "title": "TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Query denoising has become a standard training strategy for DETR-based\ndetectors by addressing the slow convergence issue. Besides that, query\ndenoising can be used to increase the diversity of training samples for\nmodeling complex scenarios which is critical for Multi-Object Tracking (MOT),\nshowing its potential in MOT application. Existing approaches integrate query\ndenoising within the tracking-by-attention paradigm. However, as the denoising\nprocess only happens within the single frame, it cannot benefit the tracker to\nlearn temporal-related information. In addition, the attention mask in query\ndenoising prevents information exchange between denoising and object queries,\nlimiting its potential in improving association using self-attention. To\naddress these issues, we propose TQD-Track, which introduces Temporal Query\nDenoising (TQD) tailored for MOT, enabling denoising queries to carry temporal\ninformation and instance-specific feature representation. We introduce diverse\nnoise types onto denoising queries that simulate real-world challenges in MOT.\nWe analyze our proposed TQD for different tracking paradigms, and find out the\nparadigm with explicit learned data association module, e.g.\ntracking-by-detection or alternating detection and association, benefit from\nTQD by a larger margin. For these paradigms, we further design an association\nmask in the association module to ensure the consistent interaction between\ntrack and detection queries as during inference. Extensive experiments on the\nnuScenes dataset demonstrate that our approach consistently enhances different\ntracking methods by only changing the training process, especially the\nparadigms with explicit association module."}
{"id": "2504.03312", "pdf": "https://arxiv.org/pdf/2504.03312", "abs": "https://arxiv.org/abs/2504.03312", "authors": ["Luís Couto Seller", "Íñigo Sanz Torres", "Adrián Vogel-Fernández", "Carlos González Carballo", "Pedro Miguel Sánchez Sánchez", "Adrián Carruana Martín", "Enrique de Miguel Ambite"], "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices", "categories": ["cs.CL", "cs.LG"], "comment": "Under Revision al SEPLN conference", "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance"}
{"id": "2504.03292", "pdf": "https://arxiv.org/pdf/2504.03292", "abs": "https://arxiv.org/abs/2504.03292", "authors": ["Gia-Nghia Tran", "Quang-Huy Che", "Trong-Tai Dam Vu", "Bich-Nga Pham", "Vinh-Tiep Nguyen", "Trung-Nghia Le", "Minh-Triet Tran"], "title": "FaR: Enhancing Multi-Concept Text-to-Image Diffusion via Concept Fusion and Localized Refinement", "categories": ["cs.CV"], "comment": null, "summary": "Generating multiple new concepts remains a challenging problem in the\ntext-to-image task. Current methods often overfit when trained on a small\nnumber of samples and struggle with attribute leakage, particularly for\nclass-similar subjects (e.g., two specific dogs). In this paper, we introduce\nFuse-and-Refine (FaR), a novel approach that tackles these challenges through\ntwo key contributions: Concept Fusion technique and Localized Refinement loss\nfunction. Concept Fusion systematically augments the training data by\nseparating reference subjects from backgrounds and recombining them into\ncomposite images to increase diversity. This augmentation technique tackles the\noverfitting problem by mitigating the narrow distribution of the limited\ntraining samples. In addition, Localized Refinement loss function is introduced\nto preserve subject representative attributes by aligning each concept's\nattention map to its correct region. This approach effectively prevents\nattribute leakage by ensuring that the diffusion model distinguishes similar\nsubjects without mixing their attention maps during the denoising process. By\nfine-tuning specific modules at the same time, FaR balances the learning of new\nconcepts with the retention of previously learned knowledge. Empirical results\nshow that FaR not only prevents overfitting and attribute leakage while\nmaintaining photorealism, but also outperforms other state-of-the-art methods."}
{"id": "2504.03338", "pdf": "https://arxiv.org/pdf/2504.03338", "abs": "https://arxiv.org/abs/2504.03338", "authors": ["Zébulon Goriely"], "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task", "categories": ["cs.CL"], "comment": "17 pages, 10 figures, submitted to CoNLL 2025", "summary": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers."}
{"id": "2504.03306", "pdf": "https://arxiv.org/pdf/2504.03306", "abs": "https://arxiv.org/abs/2504.03306", "authors": ["Mathis Kruse", "Bodo Rosenhahn"], "title": "Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Visual Anomaly and Novelty Detection 3.0 Workshop at CVPR 2025", "summary": "With more well-performing anomaly detection methods proposed, many of the\nsingle-view tasks have been solved to a relatively good degree. However,\nreal-world production scenarios often involve complex industrial products,\nwhose properties may not be fully captured by one single image. While\nnormalizing flow based approaches already work well in single-camera scenarios,\nthey currently do not make use of the priors in multi-view data. We aim to\nbridge this gap by using these flow-based models as a strong foundation and\npropose Multi-Flow, a novel multi-view anomaly detection method. Multi-Flow\nmakes use of a novel multi-view architecture, whose exact likelihood estimation\nis enhanced by fusing information across different views. For this, we propose\na new cross-view message-passing scheme, letting information flow between\nneighboring views. We empirically validate it on the real-world multi-view data\nset Real-IAD and reach a new state-of-the-art, surpassing current baselines in\nboth image-wise and sample-wise anomaly detection tasks."}
{"id": "2504.03352", "pdf": "https://arxiv.org/pdf/2504.03352", "abs": "https://arxiv.org/abs/2504.03352", "authors": ["Kaustubh Shivshankar Shejole", "Pushpak Bhattacharyya"], "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect."}
{"id": "2504.03313", "pdf": "https://arxiv.org/pdf/2504.03313", "abs": "https://arxiv.org/abs/2504.03313", "authors": ["Bram de Wilde", "Max T. Rietberg", "Guillaume Lajoinie", "Jelmer M. Wolterink"], "title": "Steerable Anatomical Shape Synthesis with Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Generative modeling of anatomical structures plays a crucial role in virtual\nimaging trials, which allow researchers to perform studies without the costs\nand constraints inherent to in vivo and phantom studies. For clinical\nrelevance, generative models should allow targeted control to simulate specific\npatient populations rather than relying on purely random sampling. In this\nwork, we propose a steerable generative model based on implicit neural\nrepresentations. Implicit neural representations naturally support topology\nchanges, making them well-suited for anatomical structures with varying\ntopology, such as the thyroid. Our model learns a disentangled latent\nrepresentation, enabling fine-grained control over shape variations. Evaluation\nincludes reconstruction accuracy and anatomical plausibility. Our results\ndemonstrate that the proposed model achieves high-quality shape generation\nwhile enabling targeted anatomical modifications."}
{"id": "2504.03380", "pdf": "https://arxiv.org/pdf/2504.03380", "abs": "https://arxiv.org/abs/2504.03380", "authors": ["Sanghwan Bae", "Jiwoo Hong", "Min Young Lee", "Hanbyul Kim", "JeongYeon Nam", "Donghyun Kwak"], "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set."}
{"id": "2504.03337", "pdf": "https://arxiv.org/pdf/2504.03337", "abs": "https://arxiv.org/abs/2504.03337", "authors": ["Quanxing Xu", "Ling Zhou", "Xian Zhong", "Feifei Zhang", "Rubing Huang", "Chia-Wen Lin"], "title": "QIRL: Boosting Visual Question Answering via Optimized Question-Image Relation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing debiasing approaches in Visual Question Answering (VQA) primarily\nfocus on enhancing visual learning, integrating auxiliary models, or employing\ndata augmentation strategies. However, these methods exhibit two major\ndrawbacks. First, current debiasing techniques fail to capture the superior\nrelation between images and texts because prevalent learning frameworks do not\nenable models to extract deeper correlations from highly contrasting samples.\nSecond, they do not assess the relevance between the input question and image\nduring inference, as no prior work has examined the degree of input relevance\nin debiasing studies. Motivated by these limitations, we propose a novel\nframework, Optimized Question-Image Relation Learning (QIRL), which employs a\ngeneration-based self-supervised learning strategy. Specifically, two modules\nare introduced to address the aforementioned issues. The Negative Image\nGeneration (NIG) module automatically produces highly irrelevant question-image\npairs during training to enhance correlation learning, while the Irrelevant\nSample Identification (ISI) module improves model robustness by detecting and\nfiltering irrelevant inputs, thereby reducing prediction errors. Furthermore,\nto validate our concept of reducing output errors through filtering unrelated\nquestion-image inputs, we propose a specialized metric to evaluate the\nperformance of the ISI module. Notably, our approach is model-agnostic and can\nbe integrated with various VQA models. Extensive experiments on VQA-CPv2 and\nVQA-v2 demonstrate the effectiveness and generalization ability of our method.\nAmong data augmentation strategies, our approach achieves state-of-the-art\nresults."}
{"id": "2504.03434", "pdf": "https://arxiv.org/pdf/2504.03434", "abs": "https://arxiv.org/abs/2504.03434", "authors": ["Batuhan Ozyurt", "Roya Arkhmammadova", "Deniz Yuret"], "title": "Locations of Characters in Narratives: Andersen and Persuasion Datasets", "categories": ["cs.CL", "I.2.7"], "comment": "14 pages, 3 figures, 10 tables", "summary": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases."}
{"id": "2504.03342", "pdf": "https://arxiv.org/pdf/2504.03342", "abs": "https://arxiv.org/abs/2504.03342", "authors": ["Guide Yang", "Chao Hou", "Weilong Peng", "Xiang Fang", "Yongwei Nie", "Peican Zhu", "Keke Tang"], "title": "EOOD: Entropy-based Out-of-distribution Detection", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN 2025", "summary": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods."}
{"id": "2504.03454", "pdf": "https://arxiv.org/pdf/2504.03454", "abs": "https://arxiv.org/abs/2504.03454", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains."}
{"id": "2504.03349", "pdf": "https://arxiv.org/pdf/2504.03349", "abs": "https://arxiv.org/abs/2504.03349", "authors": ["Denis Coquenet"], "title": "Meta-DAN: towards an efficient prediction strategy for page-level handwritten text recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text recognition led to a paradigm shift for page-level\nrecognition, from multi-step segmentation-based approaches to end-to-end\nattention-based ones. However, the na\\\"ive character-level autoregressive\ndecoding process results in long prediction times: it requires several seconds\nto process a single page image on a modern GPU. We propose the Meta Document\nAttention Network (Meta-DAN) as a novel decoding strategy to reduce the\nprediction time while enabling a better context modeling. It relies on two main\ncomponents: windowed queries, to process several transformer queries\naltogether, enlarging the context modeling with near future; and multi-token\npredictions, whose goal is to predict several tokens per query instead of only\nthe next one. We evaluate the proposed approach on 10 full-page handwritten\ndatasets and demonstrate state-of-the-art results on average in terms of\ncharacter error rate. Source code and weights of trained models are available\nat https://github.com/FactoDeepLearning/meta_dan."}
{"id": "2504.03486", "pdf": "https://arxiv.org/pdf/2504.03486", "abs": "https://arxiv.org/abs/2504.03486", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation."}
{"id": "2504.03376", "pdf": "https://arxiv.org/pdf/2504.03376", "abs": "https://arxiv.org/abs/2504.03376", "authors": ["Edern Le Bot", "Rémi Giraud", "Boris Mansencal", "Thomas Tourdias", "Josè V. Manjon", "Pierrick Coupé"], "title": "FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "This paper introduces a novel method for brain segmentation using only FLAIR\nMRIs, specifically targeting cases where access to other imaging modalities is\nlimited. By leveraging existing automatic segmentation methods, we train a\nnetwork to approximate segmentations, typically obtained from T1-weighted MRIs.\nOur method, called FLAIRBrainSeg, produces segmentations of 132 structures and\nis robust to multiple sclerosis lesions. Experiments on both in-domain and\nout-of-domain datasets demonstrate that our method outperforms\nmodality-agnostic approaches based on image synthesis, the only currently\navailable alternative for performing brain parcellation using FLAIR MRI alone.\nThis technique holds promise for scenarios where T1-weighted MRIs are\nunavailable and offers a valuable alternative for clinicians and researchers in\nneed of reliable anatomical segmentation."}
{"id": "2504.03520", "pdf": "https://arxiv.org/pdf/2504.03520", "abs": "https://arxiv.org/abs/2504.03520", "authors": ["Chen Wei Kuo", "Kevin Chu", "Nouar AlDahoul", "Hazem Ibrahim", "Talal Rahwan", "Yasir Zaki"], "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles", "categories": ["cs.CL", "cs.CY"], "comment": "23 pages, 3 figures", "summary": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting."}
{"id": "2504.03438", "pdf": "https://arxiv.org/pdf/2504.03438", "abs": "https://arxiv.org/abs/2504.03438", "authors": ["Sheng Yang", "Tong Zhan", "Shichen Qiao", "Jicheng Gong", "Qing Yang", "Yanfeng Lu", "Jian Wang"], "title": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving", "categories": ["cs.CV"], "comment": "CVPR 2025 WDFM-AD", "summary": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods."}
{"id": "2504.03541", "pdf": "https://arxiv.org/pdf/2504.03541", "abs": "https://arxiv.org/abs/2504.03541", "authors": ["Mayank Kothyari", "Sunita Sarawagi", "Soumen Chakrabarti", "Gaurav Arora", "Srujana Merugu"], "title": "Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing", "categories": ["cs.CL"], "comment": "To appear at NAACL 2025 (Main)", "summary": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages."}
{"id": "2504.03440", "pdf": "https://arxiv.org/pdf/2504.03440", "abs": "https://arxiv.org/abs/2504.03440", "authors": ["Mirko Borszukovszki", "Ivo Pascal de Jong", "Matias Valdenegro-Toro"], "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 11 figures, TrustNLP Workshop @ NAACL 2025 Camera ready", "summary": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments."}
{"id": "2504.03546", "pdf": "https://arxiv.org/pdf/2504.03546", "abs": "https://arxiv.org/abs/2504.03546", "authors": ["Khai Le-Duc", "Tuyen Tran", "Bach Phan Tat", "Nguyen Kim Hai Bui", "Quan Dang", "Hung-Phong Tran", "Thanh-Thuy Nguyen", "Ly Nguyen", "Tuan-Minh Phan", "Thi Thu Phuong Tran", "Chris Ngo", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint, 122 pages", "summary": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST."}
{"id": "2504.03442", "pdf": "https://arxiv.org/pdf/2504.03442", "abs": "https://arxiv.org/abs/2504.03442", "authors": ["Nasar Iqbal", "Niki Martinel"], "title": "Pyramid-based Mamba Multi-class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in convolutional neural networks (CNNs) and transformer-based\nmethods have improved anomaly detection and localization, but challenges\npersist in precisely localizing small anomalies. While CNNs face limitations in\ncapturing long-range dependencies, transformer architectures often suffer from\nsubstantial computational overheads. We introduce a state space model\n(SSM)-based Pyramidal Scanning Strategy (PSS) for multi-class anomaly detection\nand localization--a novel approach designed to address the challenge of small\nanomaly localization. Our method captures fine-grained details at multiple\nscales by integrating the PSS with a pre-trained encoder for multi-scale\nfeature extraction and a feature-level synthetic anomaly generator. An\nimprovement of $+1\\%$ AP for multi-class anomaly localization and a +$1\\%$\nincrease in AU-PRO on MVTec benchmark demonstrate our method's superiority in\nprecise anomaly localization across diverse industrial scenarios. The code is\navailable at https://github.com/iqbalmlpuniud/Pyramid Mamba."}
{"id": "2504.03553", "pdf": "https://arxiv.org/pdf/2504.03553", "abs": "https://arxiv.org/abs/2504.03553", "authors": ["Shuofei Qiao", "Zhisong Qiu", "Baochang Ren", "Xiaobin Wang", "Xiangyuan Ru", "Ningyu Zhang", "Xiang Chen", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "Agentic Knowledgeable Self-awareness", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf."}
{"id": "2504.03468", "pdf": "https://arxiv.org/pdf/2504.03468", "abs": "https://arxiv.org/abs/2504.03468", "authors": ["Antoine Dumoulin", "Adnane Boukhayma", "Laurence Boissieux", "Bharath Bhushan Damodaran", "Pierre Hellier", "Stefanie Wuhrer"], "title": "D-Garment: Physics-Conditioned Latent Diffusion for Dynamic Garment Deformations", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Adjusting and deforming 3D garments to body shapes, body motion, and cloth\nmaterial is an important problem in virtual and augmented reality. Applications\nare numerous, ranging from virtual change rooms to the entertainment and gaming\nindustry. This problem is challenging as garment dynamics influence geometric\ndetails such as wrinkling patterns, which depend on physical input including\nthe wearer's body shape and motion, as well as cloth material features.\nExisting work studies learning-based modeling techniques to generate garment\ndeformations from example data, and physics-inspired simulators to generate\nrealistic garment dynamics. We propose here a learning-based approach trained\non data generated with a physics-based simulator. Compared to prior work, our\n3D generative model learns garment deformations for loose cloth geometry,\nespecially for large deformations and dynamic wrinkles driven by body motion\nand cloth material. Furthermore, the model can be efficiently fitted to\nobservations captured using vision sensors. We propose to leverage the\ncapability of diffusion models to learn fine-scale detail: we model the 3D\ngarment in a 2D parameter space, and learn a latent diffusion model using this\nrepresentation independent from the mesh resolution. This allows to condition\nglobal and local geometric information with body and material information. We\nquantitatively and qualitatively evaluate our method on both simulated data and\ndata captured with a multi-view acquisition platform. Compared to strong\nbaselines, our method is more accurate in terms of Chamfer distance."}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561", "abs": "https://arxiv.org/abs/2504.03561", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."}
{"id": "2504.03471", "pdf": "https://arxiv.org/pdf/2504.03471", "abs": "https://arxiv.org/abs/2504.03471", "authors": ["Xi Wang", "Ziqi He", "Yang Zhou"], "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025. Appendix & Code:\n  https://github.com/Hytidel/UNetReweighting", "summary": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting"}
{"id": "2504.03595", "pdf": "https://arxiv.org/pdf/2504.03595", "abs": "https://arxiv.org/abs/2504.03595", "authors": ["Fabio Lilliu", "Amir Laadhar", "Christian Thomsen", "Diego Reforgiato Recupero", "Torben Bach Pedersen"], "title": "Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers", "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 4 tables. Submitted to SmartGridComm 2025", "summary": "A key element to support the increased amounts of renewable energy in the\nenergy system is flexibility, i.e., the possibility of changing energy loads in\ntime and amount. Many flexibility models have been designed; however, exact\nmodels fail to scale for long time horizons or many devices. Because of this,\nthe FlexOffer (FOs) model has been designed, to provide device-independent\napproximations of flexibility with good accuracy, and much better scaling for\nlong time horizons and many devices. An important aspect of the real-life\nimplementation of energy flexibility is enabling flexible data exchange with\nmany types of smart energy appliances and market systems, e.g., in smart\nbuildings. For this, ontologies standardizing data formats are required.\nHowever, the current industry standard ontology for integrating smart devices\nfor energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited\nsupport for flexibility and thus cannot support important use cases. In this\npaper we propose an extension of SAREF4ENER that integrates full support for\nthe complete FlexOffer model, including advanced use cases, while maintaining\nbackward compatibility. This novel ontology module can accurately describe\nflexibility for advanced devices such as electric vehicles, batteries, and heat\npumps. It can also capture the inherent uncertainty associated with many\nflexible load types."}
{"id": "2504.03474", "pdf": "https://arxiv.org/pdf/2504.03474", "abs": "https://arxiv.org/abs/2504.03474", "authors": ["Seyedeh Sahar Taheri Otaghsara", "Reza Rahmanzadeh"], "title": "Multi-encoder nnU-Net outperforms Transformer models with self-supervised pretraining", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the essential task of medical image segmentation, which\ninvolves the automatic identification and delineation of anatomical structures\nand pathological regions in medical images. Accurate segmentation is crucial in\nradiology, as it aids in the precise localization of abnormalities such as\ntumors, thereby enabling effective diagnosis, treatment planning, and\nmonitoring of disease progression. Specifically, the size, shape, and location\nof tumors can significantly influence clinical decision-making and therapeutic\nstrategies, making accurate segmentation a key component of radiological\nworkflows. However, challenges posed by variations in MRI modalities, image\nartifacts, and the scarcity of labeled data complicate the segmentation task\nand impact the performance of traditional models. To overcome these\nlimitations, we propose a novel self-supervised learning Multi-encoder nnU-Net\narchitecture designed to process multiple MRI modalities independently through\nseparate encoders. This approach allows the model to capture modality-specific\nfeatures before fusing them for the final segmentation, thus improving\naccuracy. Our Multi-encoder nnU-Net demonstrates exceptional performance,\nachieving a Dice Similarity Coefficient (DSC) of 93.72%, which surpasses that\nof other models such as vanilla nnU-Net, SegResNet, and Swin UNETR. By\nleveraging the unique information provided by each modality, the model enhances\nsegmentation tasks, particularly in scenarios with limited annotated data.\nEvaluations highlight the effectiveness of this architecture in improving tumor\nsegmentation outcomes."}
{"id": "2504.03598", "pdf": "https://arxiv.org/pdf/2504.03598", "abs": "https://arxiv.org/abs/2504.03598", "authors": ["Peter Baile Chen", "Tomer Wolfson", "Michael Cafarella", "Dan Roth"], "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Dataset and code are available at\n  https://peterbaile.github.io/enrichindex/", "summary": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs."}
{"id": "2504.03476", "pdf": "https://arxiv.org/pdf/2504.03476", "abs": "https://arxiv.org/abs/2504.03476", "authors": ["Sheng Lian", "Dengfeng Pan", "Jianlong Cai", "Guang-Yong Chen", "Zhun Zhong", "Zhiming Luo", "Shen Zhao", "Shuo Li"], "title": "ATM-Net: Anatomy-Aware Text-Guided Multi-Modal Fusion for Fine-Grained Lumbar Spine Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate lumbar spine segmentation is crucial for diagnosing spinal\ndisorders. Existing methods typically use coarse-grained segmentation\nstrategies that lack the fine detail needed for precise diagnosis.\nAdditionally, their reliance on visual-only models hinders the capture of\nanatomical semantics, leading to misclassified categories and poor segmentation\ndetails. To address these limitations, we present ATM-Net, an innovative\nframework that employs an anatomy-aware, text-guided, multi-modal fusion\nmechanism for fine-grained segmentation of lumbar substructures, i.e.,\nvertebrae (VBs), intervertebral discs (IDs), and spinal canal (SC). ATM-Net\nadopts the Anatomy-aware Text Prompt Generator (ATPG) to adaptively convert\nimage annotations into anatomy-aware prompts in different views. These insights\nare further integrated with image features via the Holistic Anatomy-aware\nSemantic Fusion (HASF) module, building a comprehensive anatomical context. The\nChannel-wise Contrastive Anatomy-Aware Enhancement (CCAE) module further\nenhances class discrimination and refines segmentation through class-wise\nchannel-level multi-modal contrastive learning. Extensive experiments on the\nMRSpineSeg and SPIDER datasets demonstrate that ATM-Net significantly\noutperforms state-of-the-art methods, with consistent improvements regarding\nclass discrimination and segmentation details. For example, ATM-Net achieves\nDice of 79.39% and HD95 of 9.91 pixels on SPIDER, outperforming the competitive\nSpineParseNet by 8.31% and 4.14 pixels, respectively."}
{"id": "2504.03601", "pdf": "https://arxiv.org/pdf/2504.03601", "abs": "https://arxiv.org/abs/2504.03601", "authors": ["Akshara Prabhakar", "Zuxin Liu", "Weiran Yao", "Jianguo Zhang", "Ming Zhu", "Shiyu Wang", "Zhiwei Liu", "Tulika Awalgaonkar", "Haolin Chen", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages plus references and appendices", "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io"}
{"id": "2504.03490", "pdf": "https://arxiv.org/pdf/2504.03490", "abs": "https://arxiv.org/abs/2504.03490", "authors": ["Zihao He", "Shengchuan Zhang", "Runze Hu", "Yunhang Shen", "Yan Zhang"], "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10; J.0"], "comment": "9 pages, 5 figures, AAAI 2025", "summary": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield."}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612", "abs": "https://arxiv.org/abs/2504.03612", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "categories": ["cs.CL"], "comment": "29 pages, 11 figures", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment."}
{"id": "2504.03501", "pdf": "https://arxiv.org/pdf/2504.03501", "abs": "https://arxiv.org/abs/2504.03501", "authors": ["Ilan Naiman", "Emanuel Ben-Baruch", "Oron Anschel", "Alon Shoshan", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "LV-MAE: Learning Long Video Representations through Masked-Embedding Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we introduce long-video masked-embedding autoencoders (LV-MAE),\na self-supervised learning framework for long video representation. Our\napproach treats short- and long-span dependencies as two separate tasks. Such\ndecoupling allows for a more intuitive video processing where short-span\nspatiotemporal primitives are first encoded and are then used to capture\nlong-range dependencies across consecutive video segments. To achieve this, we\nleverage advanced off-the-shelf multimodal encoders to extract representations\nfrom short segments within the long video, followed by pre-training a\nmasked-embedding autoencoder capturing high-level interactions across segments.\nLV-MAE is highly efficient to train and enables the processing of much longer\nvideos by alleviating the constraint on the number of input frames.\nFurthermore, unlike existing methods that typically pre-train on short-video\ndatasets, our approach offers self-supervised pre-training using long video\nsamples (e.g., 20+ minutes video clips) at scale. Using LV-MAE representations,\nwe achieve state-of-the-art results on three long-video benchmarks -- LVU,\nCOIN, and Breakfast -- employing only a simple classification head for either\nattentive or linear probing. Finally, to assess LV-MAE pre-training and\nvisualize its reconstruction quality, we leverage the video-language aligned\nspace of short video representations to monitor LV-MAE through video-text\nretrieval."}
{"id": "2504.03616", "pdf": "https://arxiv.org/pdf/2504.03616", "abs": "https://arxiv.org/abs/2504.03616", "authors": ["Leonardo Ranaldi", "Barry Haddow", "Alexandra Birch"], "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages."}
{"id": "2504.03510", "pdf": "https://arxiv.org/pdf/2504.03510", "abs": "https://arxiv.org/abs/2504.03510", "authors": ["Tan Shu", "Li Shen"], "title": "FADConv: A Frequency-Aware Dynamic Convolution for Farmland Non-agriculturalization Identification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Cropland non-agriculturalization refers to the conversion of arable land into\nnon-agricultural uses such as forests, residential areas, and construction\nsites. This phenomenon not only directly leads to the loss of cropland\nresources but also poses systemic threats to food security and agricultural\nsustainability. Accurate identification of cropland and non-cropland areas is\ncrucial for detecting and addressing this issue. Traditional CNNs employ static\nconvolution layers, while dynamic convolution studies demonstrate that\nadaptively weighting multiple convolutional kernels through attention\nmechanisms can enhance accuracy. However, existing dynamic convolution methods\nrelying on Global Average Pooling (GAP) for attention weight allocation suffer\nfrom information loss, limiting segmentation precision. This paper proposes\nFrequency-Aware Dynamic Convolution (FADConv) and a Frequency Attention (FAT)\nmodule to address these limitations. Building upon the foundational structure\nof dynamic convolution, we designed FADConv by integrating 2D Discrete Cosine\nTransform (2D DCT) to capture frequency domain features and fuse them. FAT\nmodule generates high-quality attention weights that replace the traditional\nGAP method,making the combination between dynamic convolution kernels more\nreasonable.Experiments on the GID and Hi-CNA datasets demonstrate that FADConv\nsignificantly improves segmentation accuracy with minimal computational\noverhead. For instance, ResNet18 with FADConv achieves 1.9% and 2.7% increases\nin F1-score and IoU for cropland segmentation on GID, with only 58.87M\nadditional MAdds. Compared to other dynamic convolution approaches, FADConv\nexhibits superior performance in cropland segmentation tasks."}
{"id": "2504.03622", "pdf": "https://arxiv.org/pdf/2504.03622", "abs": "https://arxiv.org/abs/2504.03622", "authors": ["Zae Myung Kim", "Anand Ramachandran", "Farideh Tavazoee", "Joo-Kyung Kim", "Oleg Rokhlenko", "Dongyeop Kang"], "title": "Align to Structure: Aligning Large Language Models with Structural Information", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align."}
{"id": "2504.03524", "pdf": "https://arxiv.org/pdf/2504.03524", "abs": "https://arxiv.org/abs/2504.03524", "authors": ["Gianluca Monaci", "Rafael S. Rezende", "Romain Deffayet", "Gabriela Csurka", "Guillaume Bono", "Hervé Déjean", "Stéphane Clinchant", "Christian Wolf"], "title": "RANa: Retrieval-Augmented Navigation", "categories": ["cs.CV", "cs.IR", "cs.RO"], "comment": null, "summary": "Methods for navigation based on large-scale learning typically treat each\nepisode as a new problem, where the agent is spawned with a clean memory in an\nunknown environment. While these generalization capabilities to an unknown\nenvironment are extremely important, we claim that, in a realistic setting, an\nagent should have the capacity of exploiting information collected during\nearlier robot operations. We address this by introducing a new\nretrieval-augmented agent, trained with RL, capable of querying a database\ncollected from previous episodes in the same environment and learning how to\nintegrate this additional context information. We introduce a unique agent\narchitecture for the general navigation task, evaluated on ObjectNav, ImageNav\nand Instance-ImageNav. Our retrieval and context encoding methods are\ndata-driven and heavily employ vision foundation models (FM) for both semantic\nand geometric understanding. We propose new benchmarks for these settings and\nwe show that retrieval allows zero-shot transfer across tasks and environments\nwhile significantly improving performance."}
{"id": "2504.03624", "pdf": "https://arxiv.org/pdf/2504.03624", "abs": "https://arxiv.org/abs/2504.03624", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aarti Basant", "Abhinav Khattar", "Adithya Renduchintala", "Akhiad Bercovich", "Aleksander Ficek", "Alexis Bjorlin", "Ali Taghibakhshi", "Amala Sanjay Deshmukh", "Ameya Sunil Mahabaleshwarkar", "Andrew Tao", "Anna Shors", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Bobby Chen", "Boris Ginsburg", "Boxin Wang", "Brandon Norick", "Brian Butterfield", "Bryan Catanzaro", "Carlo del Mundo", "Chengyu Dong", "Christine Harvey", "Christopher Parisien", "Dan Su", "Daniel Korzekwa", "Danny Yin", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Denys Fridman", "Dima Rekesh", "Ding Ma", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Eileen Long", "Elad Segal", "Ellie Evans", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Ewa Dobrowolska", "Fei Jia", "Fuxiao Liu", "Gargi Prasad", "Gerald Shen", "Guilin Liu", "Guo Chen", "Haifeng Qian", "Helen Ngo", "Hongbin Liu", "Hui Li", "Igor Gitman", "Ilia Karmanov", "Ivan Moshkov", "Izik Golan", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jarno Seppanen", "Jason Lu", "Jason Sewall", "Jiaqi Zeng", "Jiaxuan You", "Jimmy Zhang", "Jing Zhang", "Jining Huang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jon Barker", "Jonathan Cohen", "Joseph Jennings", "Jupinder Parmar", "Karan Sapra", "Kari Briski", "Kateryna Chumachenko", "Katherine Luna", "Keshav Santhanam", "Kezhi Kong", "Kirthi Sivamani", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Lawrence McAfee", "Leon Derczynski", "Lindsey Pavao", "Luis Vega", "Lukas Voegtle", "Maciej Bala", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matthieu Le", "Matvei Novikov", "Mehrzad Samadi", "Michael Andersch", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mike Ranzinger", "Mikolaj Blaz", "Misha Smelyanskiy", "Mohamed Fawzy", "Mohammad Shoeybi", "Mostofa Patwary", "Nayeon Lee", "Nima Tajbakhsh", "Ning Xu", "Oleg Rybakov", "Oleksii Kuchaiev", "Olivier Delalleau", "Osvald Nitski", "Parth Chadha", "Pasha Shamis", "Paulius Micikevicius", "Pavlo Molchanov", "Peter Dykas", "Philipp Fischer", "Pierre-Yves Aquilanti", "Piotr Bialecki", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi", "Rahul Kandu", "Ran El-Yaniv", "Raviraj Joshi", "Roger Waleffe", "Ruoxi Zhang", "Sabrina Kavanaugh", "Sahil Jain", "Samuel Kriman", "Sangkug Lym", "Sanjeev Satheesh", "Saurav Muralidharan", "Sean Narenthiran", "Selvaraj Anandaraj", "Seonmyeong Bak", "Sergey Kashirsky", "Seungju Han", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Sharon Clay", "Shelby Thomas", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shyamala Prayaga", "Siddhartha Jain", "Sirshak Das", "Slawek Kierat", "Somshubra Majumdar", "Song Han", "Soumye Singhal", "Sriharsha Niverty", "Stefania Alborghetti", "Suseella Panguluri", "Swetha Bhendigeri", "Syeda Nahida Akter", "Szymon Migacz", "Tal Shiri", "Terry Kong", "Timo Roman", "Tomer Ronen", "Trisha Saar", "Tugrul Konuk", "Tuomas Rintamaki", "Tyler Poon", "Ushnish De", "Vahid Noroozi", "Varun Singh", "Vijay Korthikanti", "Vitaly Kurin", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenliang Dai", "Wonmin Byeon", "Xiaowei Ren", "Yao Xu", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yoshi Suhara", "Zhiding Yu", "Zhiqi Li", "Zhiyu Li", "Zhongbo Zhu", "Zhuolin Yang", "Zijia Chen"], "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM."}
{"id": "2504.03536", "pdf": "https://arxiv.org/pdf/2504.03536", "abs": "https://arxiv.org/abs/2504.03536", "authors": ["Boyuan Wang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Chaojun Ni", "Guan Huang", "Lihong Liu", "Xingang Wang"], "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration", "categories": ["cs.CV"], "comment": "Project Page: https://humandreamer-x.github.io/", "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, \\textbf{HumanFixer} is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models."}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces."}
{"id": "2504.03563", "pdf": "https://arxiv.org/pdf/2504.03563", "abs": "https://arxiv.org/abs/2504.03563", "authors": ["Kaidong Li", "Tianxiao Zhang", "Kuan-Chuan Peng", "Guanghui Wang"], "title": "PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector", "categories": ["cs.CV"], "comment": "This paper is accepted to the CVPR 2025 Workshop on Distillation of\n  Foundation Models for Autonomous Driving (WDFM-AD)", "summary": "3D object detection is crucial for autonomous driving, leveraging both LiDAR\npoint clouds for precise depth information and camera images for rich semantic\ninformation. Therefore, the multi-modal methods that combine both modalities\noffer more robust detection results. However, efficiently fusing LiDAR points\nand images remains challenging due to the domain gaps. In addition, the\nperformance of many models is limited by the amount of high quality labeled\ndata, which is expensive to create. The recent advances in foundation models,\nwhich use large-scale pre-training on different modalities, enable better\nmulti-modal fusion. Combining the prompt engineering techniques for efficient\ntraining, we propose the Prompted Foundational 3D Detector (PF3Det), which\nintegrates foundation model encoders and soft prompts to enhance LiDAR-camera\nfeature fusion. PF3Det achieves the state-of-the-art results under limited\ntraining data, improving NDS by 1.19% and mAP by 2.42% on the nuScenes dataset,\ndemonstrating its efficiency in 3D detection."}
{"id": "2504.02853", "pdf": "https://arxiv.org/pdf/2504.02853", "abs": "https://arxiv.org/abs/2504.02853", "authors": ["Maciej Skorski", "Alina Landowska", "Krzysztof Rajda"], "title": "Mapping Technological Futures: Anticipatory Discourse Through Text Mining", "categories": ["cs.SI", "cs.CL", "cs.CY", "K.4; H.3.3"], "comment": "Accepted to Humanities and Social Sciences Communications. arXiv\n  admin note: text overlap with arXiv:2407.17522", "summary": "The volatility and unpredictability of emerging technologies, such as\nartificial intelligence (AI), generate significant uncertainty, which is widely\ndiscussed on social media. This study examines anticipatory discourse\nsurrounding technological futures by analysing 1.5 million posts from 400 key\nopinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using\nadvanced text mining techniques, including BERTopic modelling, sentiment,\nemotion, and attitude analyses, the research identifies 100 distinct topics\nreflecting anticipated tech-driven futures. Our findings emphasize the dual\nrole of KOLs in framing \\textit{present futures} -- optimistic visions of\ntransformative technologies like AI and IoT -- and influencing \\textit{future\npresents}, where these projections shape contemporary societal and geopolitical\ndebates. Positive emotions such as Hope dominate, outweighing Anxiety,\nparticularly in topics like ``Machine Learning, Data Science, and Deep\nLearning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and\nTrump People'' elicit \\textit{Anxiety}. By framing technologies as solutions to\nsocietal challenges, KOLs act as mediators of societal narratives, bridging\nimagined futures and current realities. These insights underscore their pivotal\nrole in directing public attention with emerging technologies during periods of\nheightened uncertainty, advancing our understanding of anticipatory discourse\nin technology-mediated contexts."}
{"id": "2504.03587", "pdf": "https://arxiv.org/pdf/2504.03587", "abs": "https://arxiv.org/abs/2504.03587", "authors": ["Niu Lian", "Jun Li", "Jinpeng Wang", "Ruisheng Luo", "Yaowei Wang", "Shu-Tao Xia", "Bin Chen"], "title": "AutoSSVH: Exploring Automated Frame Sampling for Efficient Self-Supervised Video Hashing", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "Accepted by CVPR'25. 11 pages, 5 figures, 3 tables", "summary": "Self-Supervised Video Hashing (SSVH) compresses videos into hash codes for\nefficient indexing and retrieval using unlabeled training videos. Existing\napproaches rely on random frame sampling to learn video features and treat all\nframes equally. This results in suboptimal hash codes, as it ignores\nframe-specific information density and reconstruction difficulty. To address\nthis limitation, we propose a new framework, termed AutoSSVH, that employs\nadversarial frame sampling with hash-based contrastive learning. Our\nadversarial sampling strategy automatically identifies and selects challenging\nframes with richer information for reconstruction, enhancing encoding\ncapability. Additionally, we introduce a hash component voting strategy and a\npoint-to-set (P2Set) hash-based contrastive objective, which help capture\ncomplex inter-video semantic relationships in the Hamming space and improve the\ndiscriminability of learned hash codes. Extensive experiments demonstrate that\nAutoSSVH achieves superior retrieval efficacy and efficiency compared to\nstate-of-the-art approaches. Code is available at\nhttps://github.com/EliSpectre/CVPR25-AutoSSVH."}
{"id": "2504.02922", "pdf": "https://arxiv.org/pdf/2504.02922", "abs": "https://arxiv.org/abs/2504.02922", "authors": ["Julian Minder", "Clement Dumas", "Caden Juang", "Bilal Chugtai", "Neel Nanda"], "title": "Robustly identifying concepts introduced during chat fine-tuning using crosscoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "47 pages, 27 figures", "summary": "Model diffing is the study of how fine-tuning changes a model's\nrepresentations and internal algorithms. Many behaviours of interest are\nintroduced during fine-tuning, and model diffing offers a promising lens to\ninterpret such behaviors. Crosscoders are a recent model diffing method that\nlearns a shared dictionary of interpretable concepts represented as latent\ndirections in both the base and fine-tuned models, allowing us to track how\nconcepts shift or emerge during fine-tuning. Notably, prior work has observed\nconcepts with no direction in the base model, and it was hypothesized that\nthese model-specific latents were concepts introduced during fine-tuning.\nHowever, we identify two issues which stem from the crosscoders L1 training\nloss that can misattribute concepts as unique to the fine-tuned model, when\nthey really exist in both models. We develop Latent Scaling to flag these\nissues by more accurately measuring each latent's presence across models. In\nexperiments comparing Gemma 2 2B base and chat models, we observe that the\nstandard crosscoder suffers heavily from these issues. Building on these\ninsights, we train a crosscoder with BatchTopK loss and show that it\nsubstantially mitigates these issues, finding more genuinely chat-specific and\nhighly interpretable concepts. We recommend practitioners adopt similar\ntechniques. Using the BatchTopK crosscoder, we successfully identify a set of\ngenuinely chat-specific latents that are both interpretable and causally\neffective, representing concepts such as $\\textit{false information}$ and\n$\\textit{personal question}$, along with multiple refusal-related latents that\nshow nuanced preferences for different refusal triggers. Overall, our work\nadvances best practices for the crosscoder-based methodology for model diffing\nand demonstrates that it can provide concrete insights into how chat tuning\nmodifies language model behavior."}
{"id": "2504.03602", "pdf": "https://arxiv.org/pdf/2504.03602", "abs": "https://arxiv.org/abs/2504.03602", "authors": ["Kai Lascheit", "Daniel Barath", "Marc Pollefeys", "Leonidas Guibas", "Francis Engelmann"], "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Registering human meshes to 3D point clouds is essential for applications\nsuch as augmented reality and human-robot interaction but often yields\nimprecise results due to noise and background clutter in real-world data. We\nintroduce a hybrid approach that incorporates body-part segmentation into the\nmesh fitting process, enhancing both human pose estimation and segmentation\naccuracy. Our method first assigns body part labels to individual points, which\nthen guide a two-step SMPL-X fitting: initial pose and orientation estimation\nusing body part centroids, followed by global refinement of the point cloud\nalignment. Additionally, we demonstrate that the fitted human mesh can refine\nbody part labels, leading to improved segmentation. Evaluations on the\ncluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that\nour approach significantly outperforms prior methods in both pose estimation\nand segmentation accuracy. Code and results are available on our project\nwebsite: https://segfit.github.io"}
{"id": "2504.02971", "pdf": "https://arxiv.org/pdf/2504.02971", "abs": "https://arxiv.org/abs/2504.02971", "authors": ["Binh M. Le", "Shaoyuan Xu", "Jinmiao Fu", "Zhishen Huang", "Moyan Li", "Yanhui Guo", "Hongdong Li", "Sameera Ramasinghe", "Bryan Wang"], "title": "QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "8 pages, accepted by CVPR 2025 MULA", "summary": "In Visual Document Understanding (VDU) tasks, fine-tuning a pre-trained\nVision-Language Model (VLM) with new datasets often falls short in optimizing\nthe vision encoder to identify query-specific regions in text-rich document\nimages. Existing methods that directly inject queries into model layers by\nmodifying the network architecture often struggle to adapt to new datasets with\nlimited annotations. To address this, we introduce QID, a novel, streamlined,\narchitecture-preserving approach that integrates query embeddings into the\nvision encoder, leading to notable performance gains, particularly in\ndata-scarce fine-tuning scenarios. Specifically, our approach introduces a\ndual-module framework: a query-aware module that generates a unique query\nvector to precisely guide the model's focus, as well as a query-agnostic module\nthat captures the positional relationships among tokens, ensuring robust\nspatial understanding. Notably, both modules operate independently of the\nvision attention blocks, facilitating targeted learning of query embeddings and\nenhancing visual semantic identification. Experiments with OCR-free VLMs across\nmultiple datasets demonstrate significant performance improvements using our\nmethod, especially in handling text-rich documents in data-scarce environments."}
{"id": "2504.03607", "pdf": "https://arxiv.org/pdf/2504.03607", "abs": "https://arxiv.org/abs/2504.03607", "authors": ["Yuyang Hu", "Suhas Lohit", "Ulugbek S. Kamilov", "Tim K. Marks"], "title": "Multimodal Diffusion Bridge with Attention-Based SAR Fusion for Satellite Image Cloud Removal", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has achieved some success in addressing the challenge of cloud\nremoval in optical satellite images, by fusing with synthetic aperture radar\n(SAR) images. Recently, diffusion models have emerged as powerful tools for\ncloud removal, delivering higher-quality estimation by sampling from cloud-free\ndistributions, compared to earlier methods. However, diffusion models initiate\nsampling from pure Gaussian noise, which complicates the sampling trajectory\nand results in suboptimal performance. Also, current methods fall short in\neffectively fusing SAR and optical data. To address these limitations, we\npropose Diffusion Bridges for Cloud Removal, DB-CR, which directly bridges\nbetween the cloudy and cloud-free image distributions. In addition, we propose\na novel multimodal diffusion bridge architecture with a two-branch backbone for\nmultimodal image restoration, incorporating an efficient backbone and dedicated\ncross-modality fusion blocks to effectively extract and fuse features from\nsynthetic aperture radar (SAR) and optical images. By formulating cloud removal\nas a diffusion-bridge problem and leveraging this tailored architecture, DB-CR\nachieves high-fidelity results while being computationally efficient. We\nevaluated DB-CR on the SEN12MS-CR cloud-removal dataset, demonstrating that it\nachieves state-of-the-art results."}
{"id": "2504.02984", "pdf": "https://arxiv.org/pdf/2504.02984", "abs": "https://arxiv.org/abs/2504.02984", "authors": ["Amir Hadifar", "Christopher Ochs", "Arjan Van Ewijk"], "title": "Language Models Guidance with Multi-Aspect-Cueing: A Case Study for Competitor Analysis", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Competitor analysis is essential in modern business due to the influence of\nindustry rivals on strategic planning. It involves assessing multiple aspects\nand balancing trade-offs to make informed decisions. Recent Large Language\nModels (LLMs) have demonstrated impressive capabilities to reason about such\ntrade-offs but grapple with inherent limitations such as a lack of knowledge\nabout contemporary or future realities and an incomplete understanding of a\nmarket's competitive landscape. In this paper, we address this gap by\nincorporating business aspects into LLMs to enhance their understanding of a\ncompetitive market. Through quantitative and qualitative experiments, we\nillustrate how integrating such aspects consistently improves model\nperformance, thereby enhancing analytical efficacy in competitor analysis."}
{"id": "2504.03615", "pdf": "https://arxiv.org/pdf/2504.03615", "abs": "https://arxiv.org/abs/2504.03615", "authors": ["Aref Azizpour", "Tai D. Nguyen", "Matthew C. Stamm"], "title": "Autonomous and Self-Adapting System for Synthetic Media Detection and Attribution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rapid advances in generative AI have enabled the creation of highly realistic\nsynthetic images, which, while beneficial in many domains, also pose serious\nrisks in terms of disinformation, fraud, and other malicious applications.\nCurrent synthetic image identification systems are typically static, relying on\nfeature representations learned from known generators; as new generative models\nemerge, these systems suffer from severe performance degradation. In this\npaper, we introduce the concept of an autonomous self-adaptive synthetic media\nidentification system -- one that not only detects synthetic images and\nattributes them to known sources but also autonomously identifies and\nincorporates novel generators without human intervention. Our approach\nleverages an open-set identification strategy with an evolvable embedding space\nthat distinguishes between known and unknown sources. By employing an\nunsupervised clustering method to aggregate unknown samples into\nhigh-confidence clusters and continuously refining its decision boundaries, our\nsystem maintains robust detection and attribution performance even as the\ngenerative landscape evolves. Extensive experiments demonstrate that our method\nsignificantly outperforms existing approaches, marking a crucial step toward\nuniversal, adaptable forensic systems in the era of rapidly advancing\ngenerative models."}
{"id": "2504.03029", "pdf": "https://arxiv.org/pdf/2504.03029", "abs": "https://arxiv.org/abs/2504.03029", "authors": ["Nava Haghighi", "Sunny Yu", "James Landay", "Daniela Rosner"], "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models", "categories": ["cs.HC", "cs.CL"], "comment": "20 pages, 1 figure, 2 tables, CHI '25", "summary": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems."}
{"id": "2504.03621", "pdf": "https://arxiv.org/pdf/2504.03621", "abs": "https://arxiv.org/abs/2504.03621", "authors": ["Laziz Hamdi", "Amine Tamasna", "Pascal Boisson", "Thierry Paquet"], "title": "VISTA-OCR: Towards generative and interactive end to end OCR models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance."}
{"id": "2504.03048", "pdf": "https://arxiv.org/pdf/2504.03048", "abs": "https://arxiv.org/abs/2504.03048", "authors": ["Ian Berlot-Attwell", "Frank Rudzicz", "Xujie Si"], "title": "LLM Library Learning Fails: A LEGO-Prover Case Study", "categories": ["cs.LG", "cs.CL"], "comment": "24 pages, 5 figures", "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."}
{"id": "2504.03623", "pdf": "https://arxiv.org/pdf/2504.03623", "abs": "https://arxiv.org/abs/2504.03623", "authors": ["Ciaran Bench", "Spencer A. Thomas"], "title": "Quantifying the uncertainty of model-based synthetic image quality metrics", "categories": ["cs.CV"], "comment": null, "summary": "The quality of synthetically generated images (e.g. those produced by\ndiffusion models) are often evaluated using information about image contents\nencoded by pretrained auxiliary models. For example, the Fr\\'{e}chet Inception\nDistance (FID) uses embeddings from an InceptionV3 model pretrained to classify\nImageNet. The effectiveness of this feature embedding model has considerable\nimpact on the trustworthiness of the calculated metric (affecting its\nsuitability in several domains, including medical imaging). Here, uncertainty\nquantification (UQ) is used to provide a heuristic measure of the\ntrustworthiness of the feature embedding model and an FID-like metric called\nthe Fr\\'{e}chet Autoencoder Distance (FAED). We apply Monte Carlo dropout to a\nfeature embedding model (convolutional autoencoder) to model the uncertainty in\nits embeddings. The distribution of embeddings for each input are then used to\ncompute a distribution of FAED values. We express uncertainty as the predictive\nvariance of the embeddings as well as the standard deviation of the computed\nFAED values. We find that their magnitude correlates with the extent to which\nthe inputs are out-of-distribution to the model's training data, providing some\nvalidation of its ability to assess the trustworthiness of the FAED."}
{"id": "2504.03137", "pdf": "https://arxiv.org/pdf/2504.03137", "abs": "https://arxiv.org/abs/2504.03137", "authors": ["Tu Ao", "Yanhua Yu", "Yuling Wang", "Yang Deng", "Zirui Guo", "Liang Pang", "Pinghui Wang", "Tat-Seng Chua", "Xiao Zhang", "Zhen Cai"], "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph", "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted by AAAI 2025", "summary": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time."}
{"id": "2504.03637", "pdf": "https://arxiv.org/pdf/2504.03637", "abs": "https://arxiv.org/abs/2504.03637", "authors": ["Federica Arrigoni", "Kathlén Kohn", "Andrea Fusiello", "Tomas Pajdla"], "title": "An Algebraic Geometry Approach to Viewing Graph Solvability", "categories": ["cs.CV", "math.AG"], "comment": null, "summary": "The concept of viewing graph solvability has gained significant interest in\nthe context of structure-from-motion. A viewing graph is a mathematical\nstructure where nodes are associated to cameras and edges represent the\nepipolar geometry connecting overlapping views. Solvability studies under which\nconditions the cameras are uniquely determined by the graph. In this paper we\npropose a novel framework for analyzing solvability problems based on Algebraic\nGeometry, demonstrating its potential in understanding structure-from-motion\ngraphs and proving a conjecture that was previously proposed."}
{"id": "2504.03160", "pdf": "https://arxiv.org/pdf/2504.03160", "abs": "https://arxiv.org/abs/2504.03160", "authors": ["Yuxiang Zheng", "Dayuan Fu", "Xiangkun Hu", "Xiaojie Cai", "Lyumanshan Ye", "Pengrui Lu", "Pengfei Liu"], "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."}
{"id": "2504.03639", "pdf": "https://arxiv.org/pdf/2504.03639", "abs": "https://arxiv.org/abs/2504.03639", "authors": ["Ting-Hsuan Liao", "Yi Zhou", "Yu Shen", "Chun-Hao Paul Huang", "Saayan Mitra", "Jia-Bin Huang", "Uttaran Bhattacharya"], "title": "Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://shape-move.github.io", "summary": "We explore how body shapes influence human motion synthesis, an aspect often\noverlooked in existing text-to-motion generation methods due to the ease of\nlearning a homogenized, canonical body shape. However, this homogenization can\ndistort the natural correlations between different body shapes and their motion\ndynamics. Our method addresses this gap by generating body-shape-aware human\nmotions from natural language prompts. We utilize a finite scalar\nquantization-based variational autoencoder (FSQ-VAE) to quantize motion into\ndiscrete tokens and then leverage continuous body shape information to\nde-quantize these tokens back into continuous, detailed motion. Additionally,\nwe harness the capabilities of a pretrained language model to predict both\ncontinuous shape parameters and motion tokens, facilitating the synthesis of\ntext-aligned motions and decoding them into shape-aware motions. We evaluate\nour method quantitatively and qualitatively, and also conduct a comprehensive\nperceptual study to demonstrate its efficacy in generating shape-aware motions."}
{"id": "2504.03255", "pdf": "https://arxiv.org/pdf/2504.03255", "abs": "https://arxiv.org/abs/2504.03255", "authors": ["Garry A. Gabison", "R. Patrick Xian"], "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective", "categories": ["cs.CY", "cs.CL", "cs.MA"], "comment": "12 pages content (incl. appendix) + 12 pages references, comments\n  welcome", "summary": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention over effective governance\npolicies, monitoring and control protocols. Based on emerging landscapes of the\nagentic market, we analyze the potential liability issues stemming from\ndelegated use of LLM agents and their extended systems from a principal-agent\nperspective. Our analysis complements existing risk-based studies on artificial\nagency and covers the spectrum of important aspects of the principal-agent\nrelationship and their potential consequences at deployment. Furthermore, we\nmotivate method developments for technical governance along the directions of\ninterpretability and behavior evaluations, reward and conflict management, and\nthe mitigation of misalignment and misconduct through principled engineering of\ndetection and fail-safe mechanisms. By illustrating the outstanding issues in\nAI liability for LLM-based agentic systems, we aim to inform the system design,\nauditing and monitoring approaches to enhancing transparency and\naccountability."}
{"id": "2504.03641", "pdf": "https://arxiv.org/pdf/2504.03641", "abs": "https://arxiv.org/abs/2504.03641", "authors": ["Wulin Xie", "Yi-Fan Zhang", "Chaoyou Fu", "Yang Shi", "Bingyan Nie", "Hongkai Chen", "Zhang Zhang", "Liang Wang", "Tieniu Tan"], "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models", "categories": ["cs.CV"], "comment": "Project page: https://mme-unify.github.io/", "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/."}
{"id": "2504.03289", "pdf": "https://arxiv.org/pdf/2504.03289", "abs": "https://arxiv.org/abs/2504.03289", "authors": ["Lin yueyu", "Liu Xiao"], "title": "RWKVTTS: Yet another TTS based on RWKV-7", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Human-AI interaction thrives on intuitive and efficient interfaces, among\nwhich voice stands out as a particularly natural and accessible modality.\nRecent advancements in transformer-based text-to-speech (TTS) systems, such as\nFish-Speech, CosyVoice, and MegaTTS 3, have delivered remarkable improvements\nin quality and realism, driving a significant evolution in the TTS domain. In\nthis paper, we introduce RWKV-7 \\cite{peng2025rwkv}, a cutting-edge RNN-based\narchitecture tailored for TTS applications. Unlike traditional transformer\nmodels, RWKV-7 leverages the strengths of recurrent neural networks to achieve\ngreater computational efficiency and scalability, while maintaining\nhigh-quality output. Our comprehensive benchmarks demonstrate that RWKV-7\noutperforms transformer-based models across multiple key metrics, including\nsynthesis speed, naturalness of speech, and resource efficiency. Furthermore,\nwe explore its adaptability to diverse linguistic contexts and low-resource\nenvironments, showcasing its potential to democratize TTS technology. These\nfindings position RWKV-7 as a powerful and innovative alternative, paving the\nway for more accessible and versatile voice synthesis solutions in real-world\napplications.Our code and weights are https://github.com/yynil/RWKVTTS,\nhttps://huggingface.co/spaces/RWKV-Red-Team"}
{"id": "2504.02868", "pdf": "https://arxiv.org/pdf/2504.02868", "abs": "https://arxiv.org/abs/2504.02868", "authors": ["Ariadna Tohà-Dalmau", "Josep Rosinés-Fonoll", "Enrique Romero", "Ferran Mazzanti", "Ruben Martin-Pinardel", "Sonia Marias-Perez", "Carolina Bernal-Morales", "Rafael Castro-Dominguez", "Andrea Mendez", "Emilio Ortega", "Irene Vinagre", "Marga Gimenez", "Alfredo Vellido", "Javier Zarranz-Ventura"], "title": "Machine Learning Prediction of Cardiovascular Risk in Type 1 Diabetes Mellitus Using Radiomics Features from Multimodal Retinal Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "19 pages, 7 figures. Submitted to Ophthalmology Science, under second\n  review", "summary": "This study aimed to develop a machine learning (ML) algorithm capable of\ndetermining cardiovascular risk in multimodal retinal images from patients with\ntype 1 diabetes mellitus, distinguishing between moderate, high, and very\nhigh-risk levels. Radiomic features were extracted from fundus retinography,\noptical coherence tomography (OCT), and OCT angiography (OCTA) images. ML\nmodels were trained using these features either individually or combined with\nclinical data. A dataset of 597 eyes (359 individuals) was analyzed, and models\ntrained only with radiomic features achieved AUC values of (0.79 $\\pm$ 0.03)\nfor identifying moderate risk cases from high and very high-risk cases, and\n(0.73 $\\pm$ 0.07) for distinguishing between high and very high-risk cases. The\naddition of clinical variables improved all AUC values, reaching (0.99 $\\pm$\n0.01) for identifying moderate risk cases and (0.95 $\\pm$ 0.02) for\ndifferentiating between high and very high-risk cases. For very high CV risk,\nradiomics combined with OCT+OCTA metrics and ocular data achieved an AUC of\n(0.89 $\\pm$ 0.02) without systemic data input. These results demonstrate that\nradiomic features obtained from multimodal retinal images are useful for\ndiscriminating and classifying CV risk labels, highlighting the potential of\nthis oculomics approach for CV risk assessment."}
{"id": "2504.03327", "pdf": "https://arxiv.org/pdf/2504.03327", "abs": "https://arxiv.org/abs/2504.03327", "authors": ["Makoto Takamoto", "Daniel Oñoro-Rubio", "Wiem Ben Rim", "Takashi Maruyama", "Bhushan Kotnis"], "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "11 pages, 6 figures, 15 Tables, accepted and to be published in TMLR", "summary": "Knowledge graph embedding (KGE) models encode the structural information of\nknowledge graphs to predicting new links. Effective training of these models\nrequires distinguishing between positive and negative samples with high\nprecision. Although prior research has shown that improving the quality of\nnegative samples can significantly enhance model accuracy, identifying\nhigh-quality negative samples remains a challenging problem. This paper\ntheoretically investigates the condition under which negative samples lead to\noptimal KG embedding and identifies a sufficient condition for an effective\nnegative sample distribution. Based on this theoretical foundation, we propose\n\\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that\n\\emph{generates} negative samples satisfying this condition, in contrast to\nconventional methods that focus on \\emph{identifying} challenging negative\nsamples within the training data. Importantly, the simplicity of \\textsc{EMU}\nensures seamless integration with existing KGE models and negative sampling\nmethods. To evaluate its efficacy, we conducted comprehensive experiments\nacross multiple datasets. The results consistently demonstrate significant\nimprovements in link prediction performance across various KGE models and\nnegative sampling methods. Notably, \\textsc{EMU} enables performance\nimprovements comparable to those achieved by models with embedding dimension\nfive times larger. An implementation of the method and experiments are\navailable at https://github.com/nec-research/EMU-KG."}
{"id": "2504.02880", "pdf": "https://arxiv.org/pdf/2504.02880", "abs": "https://arxiv.org/abs/2504.02880", "authors": ["Junchi Zhou", "Haozhou Wang", "Yoichiro Kato", "Tejasri Nampally", "P. Rajalakshmi", "M. Balram", "Keisuke Katsura", "Hao Lu", "Yue Mu", "Wanneng Yang", "Yangmingrui Gao", "Feng Xiao", "Hongtao Chen", "Yuhao Chen", "Wenjuan Li", "Jingwen Wang", "Fenghua Yu", "Jian Zhou", "Wensheng Wang", "Xiaochun Hu", "Yuanzhu Yang", "Yanfeng Ding", "Wei Guo", "Shouyang Liu"], "title": "Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Developing computer vision-based rice phenotyping techniques is crucial for\nprecision field management and accelerating breeding, thereby continuously\nadvancing rice production. Among phenotyping tasks, distinguishing image\ncomponents is a key prerequisite for characterizing plant growth and\ndevelopment at the organ scale, enabling deeper insights into eco-physiological\nprocesses. However, due to the fine structure of rice organs and complex\nillumination within the canopy, this task remains highly challenging,\nunderscoring the need for a high-quality training dataset. Such datasets are\nscarce, both due to a lack of large, representative collections of rice field\nimages and the time-intensive nature of annotation. To address this gap, we\nestablished the first comprehensive multi-class rice semantic segmentation\ndataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based\nimages from five major rice-growing countries (China, Japan, India, the\nPhilippines, and Tanzania), encompassing over 6,000 genotypes across all growth\nstages. From these original images, 3,078 representative samples were selected\nand annotated with six classes (background, green vegetation, senescent\nvegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably,\nthe sub-dataset from China spans all major genotypes and rice-growing\nenvironments from the northeast to the south. Both state-of-the-art\nconvolutional neural networks and transformer-based semantic segmentation\nmodels were used as baselines. While these models perform reasonably well in\nsegmenting background and green vegetation, they face difficulties during the\nreproductive stage, when canopy structures are more complex and multiple\nclasses are involved. These findings highlight the importance of our dataset\nfor developing specialized segmentation models for rice and other crops."}
{"id": "2504.03360", "pdf": "https://arxiv.org/pdf/2504.03360", "abs": "https://arxiv.org/abs/2504.03360", "authors": ["Erik Johannes Husom", "Arda Goknil", "Merve Astekin", "Lwin Khin Shar", "Andre Kåsen", "Sagar Sen", "Benedikt Andreas Mithassel", "Ahmet Soylu"], "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "30 pages, 14 figures", "summary": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment."}
{"id": "2504.02983", "pdf": "https://arxiv.org/pdf/2504.02983", "abs": "https://arxiv.org/abs/2504.02983", "authors": ["Xiaoyu Tong", "Zhi Zhang", "Martha Lewis", "Ekaterina Shutova"], "title": "Hummus: A Dataset of Humorous Multimodal Metaphor Use", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Metaphor and humor share a lot of common ground, and metaphor is one of the\nmost common humorous mechanisms. This study focuses on the humorous capacity of\nmultimodal metaphors, which has not received due attention in the community. We\ntake inspiration from the Incongruity Theory of humor, the Conceptual Metaphor\nTheory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and\ndeveloped a novel annotation scheme for humorous multimodal metaphor use in\nimage-caption pairs. We create the Hummus Dataset of Humorous Multimodal\nMetaphor Use, providing expert annotation on 1k image-caption pairs sampled\nfrom the New Yorker Caption Contest corpus. Using the dataset, we test\nstate-of-the-art multimodal large language models (MLLMs) on their ability to\ndetect and understand humorous multimodal metaphor use. Our experiments show\nthat current MLLMs still struggle with processing humorous multimodal\nmetaphors, particularly with regard to integrating visual and textual\ninformation. We release our dataset and code at\ngithub.com/xiaoyuisrain/humorous-multimodal-metaphor-use."}
{"id": "2504.03635", "pdf": "https://arxiv.org/pdf/2504.03635", "abs": "https://arxiv.org/abs/2504.03635", "authors": ["Xinyi Wang", "Shawn Tan", "Mingyu Jin", "William Yang Wang", "Rameswar Panda", "Yikang Shen"], "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."}
{"id": "2504.02996", "pdf": "https://arxiv.org/pdf/2504.02996", "abs": "https://arxiv.org/abs/2504.02996", "authors": ["Siqi Wang", "Aoming Liu", "Bryan A. Plummer"], "title": "Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multi-source Domain Generalization (DG) aims to improve model robustness to\nnew distributions. However, DG methods often overlook the effect of label\nnoise, which can confuse a model during training, reducing performance. Limited\nprior work has analyzed DG method's noise-robustness, typically focused on an\nanalysis of existing methods rather than new solutions. In this paper, we\ninvestigate this underexplored space, where models are evaluated under both\ndistribution shifts and label noise, which we refer to as Noise-Aware\nGeneralization (NAG). A natural solution to address label noise would be to\ncombine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL\nmethods aim to detect distribution shifts in a class's samples, i.e., they\nassume that distribution shifts often correspond to label noise. However, in\nNAG distribution shifts can be due to label noise or domain shifts, breaking\nthe assumptions used by LNL methods. A naive solution is to make a similar\nassumption made by many DG methods, where we presume to have domain labels\nduring training, enabling us to isolate the two types of shifts. However, this\nignores valuable cross-domain information. Specifically, our proposed DL4ND\napproach improves noise detection by taking advantage of the observation that\nnoisy samples that may appear indistinguishable within a single domain often\nshow greater variation when compared across domains. Experiments show that\nDL4ND significantly improves performance across four diverse datasets, offering\na promising direction for tackling NAG."}
{"id": "2504.03129", "pdf": "https://arxiv.org/pdf/2504.03129", "abs": "https://arxiv.org/abs/2504.03129", "authors": ["Haozhan Tang", "Tianyi Zhang", "Oliver Kroemer", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Robots operating in unstructured environments often require accurate and\nconsistent object-level representations. This typically requires segmenting\nindividual objects from the robot's surroundings. While recent large models\nsuch as Segment Anything (SAM) offer strong performance in 2D image\nsegmentation. These advances do not translate directly to performance in the\nphysical 3D world, where they often over-segment objects and fail to produce\nconsistent mask correspondences across views. In this paper, we present\nGraphSeg, a framework for generating consistent 3D object segmentations from a\nsparse set of 2D images of the environment without any depth information.\nGraphSeg adds edges to graphs and constructs dual correspondence graphs: one\nfrom 2D pixel-level similarities and one from inferred 3D structure. We\nformulate segmentation as a problem of edge addition, then subsequent graph\ncontraction, which merges multiple 2D masks into unified object-level\nsegmentations. We can then leverage \\emph{3D foundation models} to produce\nsegmented 3D representations. GraphSeg achieves robust segmentation with\nsignificantly fewer images and greater accuracy than prior methods. We\ndemonstrate state-of-the-art performance on tabletop scenes and show that\nGraphSeg enables improved performance on downstream robotic manipulation tasks.\nCode available at https://github.com/tomtang502/graphseg.git."}
{"id": "2504.03146", "pdf": "https://arxiv.org/pdf/2504.03146", "abs": "https://arxiv.org/abs/2504.03146", "authors": ["Fatemeh Javadian", "Zahra Aminparast", "Johannes Stegmaier", "Abin Jose"], "title": "Comparative Analysis of Unsupervised and Supervised Autoencoders for Nuclei Classification in Clear Cell Renal Cell Carcinoma Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.9"], "comment": "Accepted 4-page paper at IEEE ISBI 2025. 3 figures, 3 tables", "summary": "This study explores the application of supervised and unsupervised\nautoencoders (AEs) to automate nuclei classification in clear cell renal cell\ncarcinoma (ccRCC) images, a diagnostic task traditionally reliant on subjective\nvisual grading by pathologists. We evaluate various AE architectures, including\nstandard AEs, contractive AEs (CAEs), and discriminative AEs (DAEs), as well as\na classifier-based discriminative AE (CDAE), optimized using the hyperparameter\ntuning tool Optuna. Bhattacharyya distance is selected from several metrics to\nassess class separability in the latent space, revealing challenges in\ndistinguishing adjacent grades using unsupervised models. CDAE, integrating a\nsupervised classifier branch, demonstrated superior performance in both latent\nspace separation and classification accuracy. Given that CDAE-CNN achieved\nnotable improvements in classification metrics, affirming the value of\nsupervised learning for class-specific feature extraction, F1 score was\nincorporated into the tuning process to optimize classification performance.\nResults show significant improvements in identifying aggressive ccRCC grades by\nleveraging the classification capability of AE through latent clustering\nfollowed by fine-grained classification. Our model outperforms the current\nstate of the art, CHR-Network, across all evaluated metrics. These findings\nsuggest that integrating a classifier branch in AEs, combined with neural\narchitecture search and contrastive learning, enhances grading automation in\nccRCC pathology, particularly in detecting aggressive tumor grades, and may\nimprove diagnostic accuracy."}
{"id": "2504.03238", "pdf": "https://arxiv.org/pdf/2504.03238", "abs": "https://arxiv.org/abs/2504.03238", "authors": ["Akis Nousias", "Efklidis Katsaros", "Evangelos Syrmos", "Panagiotis Radoglou-Grammatikis", "Thomas Lagkas", "Vasileios Argyriou", "Ioannis Moscholios", "Evangelos Markakis", "Sotirios Goudos", "Panagiotis Sarigiannidis"], "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Accepted at ICC-W", "summary": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers."}
{"id": "2504.03369", "pdf": "https://arxiv.org/pdf/2504.03369", "abs": "https://arxiv.org/abs/2504.03369", "authors": ["Chen Hu", "Enrica Tricomi", "Eojin Rho", "Daekyum Kim", "Lorenzo Masia", "Shan Luo", "Letizia Gionfrida"], "title": "Point Cloud-based Grasping for Soft Hand Exoskeleton", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Grasping is a fundamental skill for interacting with and manipulating objects\nin the environment. However, this ability can be challenging for individuals\nwith hand impairments. Soft hand exoskeletons designed to assist grasping can\nenhance or restore essential hand functions, yet controlling these soft\nexoskeletons to support users effectively remains difficult due to the\ncomplexity of understanding the environment. This study presents a vision-based\npredictive control framework that leverages contextual awareness from depth\nperception to predict the grasping target and determine the next control state\nfor activation. Unlike data-driven approaches that require extensive labelled\ndatasets and struggle with generalizability, our method is grounded in\ngeometric modelling, enabling robust adaptation across diverse grasping\nscenarios. The Grasping Ability Score (GAS) was used to evaluate performance,\nwith our system achieving a state-of-the-art GAS of 91% across 15 objects and\nhealthy participants, demonstrating its effectiveness across different object\ntypes. The proposed approach maintained reconstruction success for unseen\nobjects, underscoring its enhanced generalizability compared to learning-based\nmodels."}
{"id": "2504.03415", "pdf": "https://arxiv.org/pdf/2504.03415", "abs": "https://arxiv.org/abs/2504.03415", "authors": ["Zhe Wang", "Yifei Zhu"], "title": "NeRFlex: Resource-aware Real-time High-quality Rendering of Complex Scenes on Mobile Devices", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.MM", "cs.PF"], "comment": "This paper is accepted by 45th IEEE International Conference on\n  Distributed Computing Systems (ICDCS 2025)", "summary": "Neural Radiance Fields (NeRF) is a cutting-edge neural network-based\ntechnique for novel view synthesis in 3D reconstruction. However, its\nsignificant computational demands pose challenges for deployment on mobile\ndevices. While mesh-based NeRF solutions have shown potential in achieving\nreal-time rendering on mobile platforms, they often fail to deliver\nhigh-quality reconstructions when rendering practical complex scenes.\nAdditionally, the non-negligible memory overhead caused by pre-computed\nintermediate results complicates their practical application. To overcome these\nchallenges, we present NeRFlex, a resource-aware, high-resolution, real-time\nrendering framework for complex scenes on mobile devices. NeRFlex integrates\nmobile NeRF rendering with multi-NeRF representations that decompose a scene\ninto multiple sub-scenes, each represented by an individual NeRF network.\nCrucially, NeRFlex considers both memory and computation constraints as\nfirst-class citizens and redesigns the reconstruction process accordingly.\nNeRFlex first designs a detail-oriented segmentation module to identify\nsub-scenes with high-frequency details. For each NeRF network, a lightweight\nprofiler, built on domain knowledge, is used to accurately map configurations\nto visual quality and memory usage. Based on these insights and the resource\nconstraints on mobile devices, NeRFlex presents a dynamic programming algorithm\nto efficiently determine configurations for all NeRF representations, despite\nthe NP-hardness of the original decision problem. Extensive experiments on\nreal-world datasets and mobile devices demonstrate that NeRFlex achieves\nreal-time, high-quality rendering on commercial mobile devices."}
{"id": "2504.03420", "pdf": "https://arxiv.org/pdf/2504.03420", "abs": "https://arxiv.org/abs/2504.03420", "authors": ["Gianluca Maselli", "Vieri Giuliano Santucci"], "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Dealing with environments with sparse rewards has always been crucial for\nsystems developed to operate in autonomous open-ended learning settings.\nIntrinsic Motivations could be an effective way to help Deep Reinforcement\nLearning algorithms learn in such scenarios. In fact, intrinsic reward signals,\nsuch as novelty or curiosity, are generally adopted to improve exploration when\nextrinsic rewards are delayed or absent. Building on previous works, we tackle\nthe problem of learning policies in the presence of sparse rewards by proposing\na two-level architecture that alternates an ''intrinsically driven'' phase of\nexploration and autonomous sub-goal generation, to a phase of sparse reward,\ngoal-directed policy learning. The idea is to build several small networks,\neach one specialized on a particular sub-path, and use them as starting points\nfor future exploration without the need to further explore from scratch\npreviously learnt paths. Two versions of the system have been trained and\ntested in the Gym SuperMarioBros environment without considering any additional\nextrinsic reward. The results show the validity of our approach and the\nimportance of autonomously segment the environment to generate an efficient\npath towards the final goal."}
{"id": "2504.03439", "pdf": "https://arxiv.org/pdf/2504.03439", "abs": "https://arxiv.org/abs/2504.03439", "authors": ["Mohammad Reza Yousefi", "Ali Bakrani", "Amin Dehghani"], "title": "Early detection of diabetes through transfer learning-based eye (vision) screening and improvement of machine learning model performance and advanced parameter setting algorithms", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "25 pages,12 Figures, 1 Table", "summary": "Diabetic Retinopathy (DR) is a serious and common complication of diabetes,\ncaused by prolonged high blood sugar levels that damage the small retinal blood\nvessels. If left untreated, DR can progress to retinal vein occlusion and\nstimulate abnormal blood vessel growth, significantly increasing the risk of\nblindness. Traditional diabetes diagnosis methods often utilize convolutional\nneural networks (CNNs) to extract visual features from retinal images, followed\nby classification algorithms such as decision trees and k-nearest neighbors\n(KNN) for disease detection. However, these approaches face several challenges,\nincluding low accuracy and sensitivity, lengthy machine learning (ML) model\ntraining due to high data complexity and volume, and the use of limited\ndatasets for testing and evaluation. This study investigates the application of\ntransfer learning (TL) to enhance ML model performance in DR detection. Key\nimprovements include dimensionality reduction, optimized learning rate\nadjustments, and advanced parameter tuning algorithms, aimed at increasing\nefficiency and diagnostic accuracy. The proposed model achieved an overall\naccuracy of 84% on the testing dataset, outperforming prior studies. The\nhighest class-specific accuracy reached 89%, with a maximum sensitivity of 97%\nand an F1-score of 92%, demonstrating strong performance in identifying DR\ncases. These findings suggest that TL-based DR screening is a promising\napproach for early diagnosis, enabling timely interventions to prevent vision\nloss and improve patient outcomes."}
{"id": "2504.03478", "pdf": "https://arxiv.org/pdf/2504.03478", "abs": "https://arxiv.org/abs/2504.03478", "authors": ["Spyros Kondylatos", "Nikolaos Ioannis Bountos", "Ioannis Prapas", "Angelos Zavras", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "Probabilistic Machine Learning for Noisy Labels in Earth Observation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Label noise poses a significant challenge in Earth Observation (EO), often\ndegrading the performance and reliability of supervised Machine Learning (ML)\nmodels. Yet, given the critical nature of several EO applications, developing\nrobust and trustworthy ML solutions is essential. In this study, we take a step\nin this direction by leveraging probabilistic ML to model input-dependent label\nnoise and quantify data uncertainty in EO tasks, accounting for the unique\nnoise sources inherent in the domain. We train uncertainty-aware probabilistic\nmodels across a broad range of high-impact EO applications-spanning diverse\nnoise sources, input modalities, and ML configurations-and introduce a\ndedicated pipeline to assess their accuracy and reliability. Our experimental\nresults show that the uncertainty-aware models consistently outperform the\nstandard deterministic approaches across most datasets and evaluation metrics.\nMoreover, through rigorous uncertainty evaluation, we validate the reliability\nof the predicted uncertainty estimates, enhancing the interpretability of model\npredictions. Our findings emphasize the importance of modeling label noise and\nincorporating uncertainty quantification in EO, paving the way for more\naccurate, reliable, and trustworthy ML solutions in the field."}
{"id": "2504.03553", "pdf": "https://arxiv.org/pdf/2504.03553", "abs": "https://arxiv.org/abs/2504.03553", "authors": ["Shuofei Qiao", "Zhisong Qiu", "Baochang Ren", "Xiaobin Wang", "Xiangyuan Ru", "Ningyu Zhang", "Xiang Chen", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "Agentic Knowledgeable Self-awareness", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf."}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561", "abs": "https://arxiv.org/abs/2504.03561", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."}
{"id": "2504.03589", "pdf": "https://arxiv.org/pdf/2504.03589", "abs": "https://arxiv.org/abs/2504.03589", "authors": ["Badhan Kumar Das", "Gengyan Zhao", "Han Liu", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "AdaViT: Adaptive Vision Transformer for Flexible Pretrain and Finetune with Variable 3D Medical Image Modalities", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Pretrain techniques, whether supervised or self-supervised, are widely used\nin deep learning to enhance model performance. In real-world clinical\nscenarios, different sets of magnetic resonance (MR) contrasts are often\nacquired for different subjects/cases, creating challenges for deep learning\nmodels assuming consistent input modalities among all the cases and between\npretrain and finetune. Existing methods struggle to maintain performance when\nthere is an input modality/contrast set mismatch with the pretrained model,\noften resulting in degraded accuracy. We propose an adaptive Vision Transformer\n(AdaViT) framework capable of handling variable set of input modalities for\neach case. We utilize a dynamic tokenizer to encode different input image\nmodalities to tokens and take advantage of the characteristics of the\ntransformer to build attention mechanism across variable length of tokens.\nThrough extensive experiments, we demonstrate that this architecture\neffectively transfers supervised pretrained models to new datasets with\ndifferent input modality/contrast sets, resulting in superior performance on\nzero-shot testing, few-shot finetuning, and backward transferring in brain\ninfarct and brain tumor segmentation tasks. Additionally, for self-supervised\npretrain, the proposed method is able to maximize the pretrain data and\nfacilitate transferring to diverse downstream tasks with variable sets of input\nmodalities."}
{"id": "2504.03600", "pdf": "https://arxiv.org/pdf/2504.03600", "abs": "https://arxiv.org/abs/2504.03600", "authors": ["Jun Ma", "Zongxin Yang", "Sumin Kim", "Bihui Chen", "Mohammed Baharoon", "Adibvafa Fallahpour", "Reza Asakereh", "Hongwei Lyu", "Bo Wang"], "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "https://medsam2.github.io/", "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments."}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces."}
