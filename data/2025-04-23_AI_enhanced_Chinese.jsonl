{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f3D\u52302D\u53ef\u89c6\u5316\u4e0e\u6807\u6ce8\u5de5\u5177Vision6D\uff0c\u7528\u4e8e\u652f\u63016D\u59ff\u6001\u4f30\u8ba1\u7814\u7a76\uff0c\u63d0\u4f9b\u76f4\u89c2\u76843D\u754c\u9762\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u5e2e\u52a9\u7528\u6237\u51c6\u786e\u6807\u6ce8\u7269\u4f53\u59ff\u6001\u3002", "motivation": "6D\u59ff\u6001\u4f30\u8ba1\u5728\u673a\u5668\u4eba\u8f85\u52a9\u4efb\u52a1\u4e2d\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u4ea4\u4e92\u6027\u548c\u76f4\u89c2\u6027\uff0cVision6D\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u652f\u63013D\u7269\u4f53\u57282D\u573a\u666f\u4e2d\u7684\u53ef\u89c6\u5316\u4e0e\u6807\u6ce8\uff0c\u5229\u7528\u89c6\u89c9\u63d0\u793a\u548c\u7a7a\u95f4\u5173\u7cfb\u786e\u5b9a\u7269\u4f53\u4f4d\u59ff\uff0c\u65e0\u9700\u5df2\u77e5\u76f8\u673a\u4e0e\u7269\u4f53\u7684\u53d8\u6362\u77e9\u9635\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u4e0e\u5f00\u6e90\u6570\u636e\u96c6\u7684\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86Vision6D\u80fd\u751f\u6210\u51c6\u786e\u7684\u59ff\u6001\u6807\u6ce8\uff0c\u4e14\u754c\u9762\u76f4\u89c2\u6613\u7528\u3002", "conclusion": "Vision6D\u4e3a6D\u59ff\u6001\u4f30\u8ba1\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u5f00\u6e90\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2504.15437", "pdf": "https://arxiv.org/pdf/2504.15437", "abs": "https://arxiv.org/abs/2504.15437", "authors": ["Ryan Erik Landvater", "Ulysses Balis"], "title": "Iris: A Next Generation Digital Pathology Rendering Engine", "categories": ["cs.GR"], "comment": "11 pages, 8 figures", "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology.Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 {\\mu}s\nper slide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.", "AI": {"tldr": "Iris\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u6570\u5b57\u75c5\u7406\u5b66\u6e32\u67d3\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u7684C++\u548cVulkan\u6280\u672f\u5b9e\u73b0\u5feb\u901f\u56fe\u50cf\u6e32\u67d3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u7684\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u5728\u75c5\u7406\u5b66\u9886\u57df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6280\u672f\u5728\u4f7f\u7528\u4fbf\u6377\u6027\u548c\u56fe\u50cf\u6e32\u67d3\u901f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86Iris Core\u6e32\u67d3\u5f15\u64ce\uff0c\u91c7\u7528C++\u548cVulkan\u7f16\u5199\uff0c\u7ed3\u5408\u5feb\u901f\u74e6\u7247\u7f13\u51b2\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u5904\u7406\u548c\u6e32\u67d3\u3002", "result": "Iris Core\u80fd\u591f\u572810\u6beb\u79d2\u5185\u5b8c\u6210\u65b0\u89c6\u91ce\u7684\u65e0\u91cd\u53e0\u50cf\u7d20\u7f13\u51b2\uff0c\u5e76\u572830\u6beb\u79d2\u5185\u589e\u5f3a\u7ec6\u8282\uff0c\u6027\u80fd\u8fdc\u8d85\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Iris\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u7684\u6e32\u67d3\u901f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15657", "pdf": "https://arxiv.org/pdf/2504.15657", "abs": "https://arxiv.org/abs/2504.15657", "authors": ["Yibo Liu", "Paul Kry", "Kenny Erleben", "Noam Aigerman", "Sune Darkner", "Teseo Schneider"], "title": "Neural Kinematic Bases for Fluids", "categories": ["cs.GR", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We propose mesh-free fluid simulations that exploit a kinematic neural basis\nfor velocity fields represented by an MLP. We design a set of losses that\nensures that these neural bases satisfy fundamental physical properties such as\northogonality, divergence-free, boundary alignment, and smoothness. Our neural\nbases can then be used to fit an input sketch of a flow, which will inherit the\nsame fundamental properties from the bases. We then can animate such flow in\nreal-time using standard time integrators. Our neural bases can accommodate\ndifferent domains and naturally extend to three dimensions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMLP\u7684\u7f51\u683c\u65e0\u5173\u6d41\u4f53\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\u786e\u4fdd\u795e\u7ecf\u57fa\u6ee1\u8db3\u6b63\u4ea4\u6027\u3001\u65e0\u6563\u6027\u3001\u8fb9\u754c\u5bf9\u9f50\u548c\u5e73\u6ed1\u6027\u7b49\u7269\u7406\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u6d41\u4f53\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u52a8\u753b\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u57fa\u7b80\u5316\u6a21\u62df\u8fc7\u7a0b\u5e76\u4fdd\u6301\u7269\u7406\u7279\u6027\u3002", "method": "\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\u8bad\u7ec3MLP\u751f\u6210\u7684\u795e\u7ecf\u57fa\uff0c\u786e\u4fdd\u5176\u6ee1\u8db3\u6b63\u4ea4\u6027\u3001\u65e0\u6563\u6027\u7b49\u7269\u7406\u7279\u6027\uff0c\u5e76\u7528\u4e8e\u62df\u5408\u8f93\u5165\u6d41\u573a\u8349\u56fe\u3002", "result": "\u795e\u7ecf\u57fa\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u57df\u5e76\u81ea\u7136\u6269\u5c55\u5230\u4e09\u7ef4\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u57fa\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7269\u7406\u51c6\u786e\u7684\u6d41\u4f53\u6a21\u62df\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u52a8\u753b\u3002"}}
{"id": "2504.15309", "pdf": "https://arxiv.org/pdf/2504.15309", "abs": "https://arxiv.org/abs/2504.15309", "authors": ["Anran Yu", "Wei Feng", "Yaochen Zhang", "Xiang Li", "Lei Meng", "Lei Wu", "Xiangxu Meng"], "title": "LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The personalized text-to-image generation has rapidly advanced with the\nemergence of Stable Diffusion. Existing methods, which typically fine-tune\nmodels using embedded identifiers, often struggle with insufficient stylization\nand inaccurate image content due to reduced textual controllability. In this\npaper, we propose style refinement and content preservation strategies. The\nstyle refinement strategy leverages the semantic information of visual\nreasoning prompts and reference images to optimize style embeddings, allowing a\nmore precise and consistent representation of style information. The content\npreservation strategy addresses the content bias problem by preserving the\nmodel's generalization capabilities, ensuring enhanced textual controllability\nwithout compromising stylization. Experimental results verify that our approach\nachieves superior performance in generating consistent and personalized\ntext-to-image outputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u98ce\u683c\u4f18\u5316\u548c\u5185\u5bb9\u4fdd\u7559\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u8c03\u6a21\u578b\u65f6\u5b58\u5728\u98ce\u683c\u5316\u4e0d\u8db3\u548c\u56fe\u50cf\u5185\u5bb9\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u672c\u53ef\u63a7\u6027\u964d\u4f4e\u3002", "method": "\u91c7\u7528\u98ce\u683c\u4f18\u5316\u7b56\u7565\uff08\u5229\u7528\u89c6\u89c9\u63a8\u7406\u63d0\u793a\u548c\u53c2\u8003\u56fe\u50cf\u7684\u8bed\u4e49\u4fe1\u606f\u4f18\u5316\u98ce\u683c\u5d4c\u5165\uff09\u548c\u5185\u5bb9\u4fdd\u7559\u7b56\u7565\uff08\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4e00\u81f4\u4e14\u4e2a\u6027\u5316\u7684\u6587\u672c\u5230\u56fe\u50cf\u8f93\u51fa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u5316\u548c\u5185\u5bb9\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.15933", "pdf": "https://arxiv.org/pdf/2504.15933", "abs": "https://arxiv.org/abs/2504.15933", "authors": ["Anh Truong", "Ahmed H. Mahmoud", "Mina Konakovi\u0107 Lukovi\u0107", "Justin Solomon"], "title": "Low-Rank Adaptation of Neural Fields", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u53c2\u6570\u9ad8\u6548\u7b56\u7565\uff0c\u7528\u4e8e\u66f4\u65b0\u795e\u7ecf\u573a\uff08NF\uff09\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u8fc7\u6ee4\u3001\u89c6\u9891\u538b\u7f29\u548c\u51e0\u4f55\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u56fe\u5f62\u6280\u672f\uff08\u5982\u6cd5\u7ebf\u8d34\u56fe\u548c\u89c6\u9891\u538b\u7f29\uff09\u5229\u7528\u5197\u4f59\u9ad8\u6548\u7f16\u7801\u5c0f\u53d8\u5316\uff0c\u4f46\u795e\u7ecf\u573a\u7684\u5c0f\u53d8\u5316\u7f16\u7801\u95ee\u9898\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5c06LoRA\u65b9\u6cd5\u4ece\u53c2\u6570\u9ad8\u6548\u5fae\u8c03LLM\u793e\u533a\u5f15\u5165\u795e\u7ecf\u573a\uff0c\u907f\u514d\u4f7f\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u9002\u5408\u4f4e\u8ba1\u7b97\u786c\u4ef6\u3002", "result": "\u5728\u56fe\u50cf\u8fc7\u6ee4\u3001\u89c6\u9891\u538b\u7f29\u548c\u51e0\u4f55\u7f16\u8f91\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "LoRA\u7b56\u7565\u4e3a\u795e\u7ecf\u573a\u7684\u5c0f\u53d8\u5316\u7f16\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-Taix\u00e9", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLongPerceptualThoughts\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5408\u6210\u6846\u67b6\u751f\u6210\u957f\u601d\u7ef4\u94fe\uff0c\u63d0\u5347\u611f\u77e5\u4efb\u52a1\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u957f\u601d\u7ef4\u94fe\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5f25\u8865\u73b0\u6709\u6a21\u578b\u5728\u7cfb\u7edf2\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u4e09\u9636\u6bb5\u6570\u636e\u5408\u6210\u6846\u67b6\uff1a1) \u4ece\u5bc6\u96c6\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u53ef\u9a8c\u8bc1\u9009\u62e9\u9898\uff1b2) \u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7b80\u5355\u601d\u7ef4\u94fe\uff1b3) \u901a\u8fc7\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u6269\u5c55\u4e3a\u957f\u601d\u7ef4\u94fe\u3002", "result": "\u57285\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53473.4\u5206\uff0cV$^*$ Bench\u63d0\u534711.8\u5206\uff1b\u6587\u672c\u63a8\u7406\u57fa\u51c6MMLU-Pro\u4e5f\u63d0\u53472\u5206\u3002", "conclusion": "\u957f\u601d\u7ef4\u94fe\u4e0d\u4ec5\u9002\u7528\u4e8e\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\uff0c\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u611f\u77e5\u4efb\u52a1\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2504.15349", "pdf": "https://arxiv.org/pdf/2504.15349", "abs": "https://arxiv.org/abs/2504.15349", "authors": ["William Bruns"], "title": "Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)", "categories": ["cs.CL"], "comment": "8 pages main text with 3 figures and 1 table; limitations page and\n  references separate; 4 more figures, 1 image, and 1 more table in the\n  appendices supplement the work. 29 pages of appendix content", "summary": "Humans understand new combinations of words encountered if they are\ncombinations of words recognized from different contexts, an ability called\nCompositional Generalization. The COGS benchmark (Kim and Linzen, 2020)\narXiv:2010.05465 reports 0% accuracy for Transformer models on some structural\ngeneralizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted\nAccess Sequence Processing (RASP), a Transformer-equivalent programming\nlanguage, to prove by construction that a Transformer encoder-decoder can\nperform the semantically equivalent ReCOGS_pos (Wu et al., 2024)\narXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP\nmodel attains 100% semantic exact match on the ReCOGS test set and 100% SEM on\nall generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,\nour RASP model shows the ReCOGS_pos task does not require a hierarchical or\ntree-structured solution: we use word-level tokens with an \"embedding\" layer\nthat tags with possible parts of speech, applying just once per encoder pass 19\nattention-head compatible flat pattern-matching rules, shown using grammar\ncoverage (Zeller et al., 2023) to be learnable from the training data, plus\ngeneral prepositional phrase (pp) handling and sentential complement (cp)\nhandling logic, and output the next logical form (LF) token (repeating until\nthe LF is complete). The model does not apply recursive, tree-structured rules\nlike 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact\nmatch on pp recursion, cp recursion using the decoder loop.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7RASP\u8bed\u8a00\u8bc1\u660eTransformer\u6a21\u578b\u53ef\u4ee5\u5728ReCOGS_pos\u4efb\u52a1\u4e2d\u5b9e\u73b0100%\u8bed\u4e49\u5339\u914d\uff0c\u8868\u660e\u4efb\u52a1\u65e0\u9700\u5c42\u6b21\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76Transformer\u6a21\u578b\u5728\u7ec4\u5408\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662fCOGS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u7ed3\u6784\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528RASP\u7f16\u7a0b\u8bed\u8a00\u6784\u5efaTransformer\u7b49\u6548\u6a21\u578b\uff0c\u901a\u8fc719\u4e2a\u6ce8\u610f\u529b\u5934\u517c\u5bb9\u7684\u6241\u5e73\u6a21\u5f0f\u5339\u914d\u89c4\u5219\u5904\u7406ReCOGS_pos\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728ReCOGS\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0100%\u8bed\u4e49\u5339\u914d\uff0c\u9664obj_pp_to_subj_pp\u5916\u6240\u6709\u6cdb\u5316\u5206\u5272\u5747100%\u5339\u914d\u3002", "conclusion": "ReCOGS_pos\u4efb\u52a1\u53ef\u901a\u8fc7\u6241\u5e73\u89c4\u5219\u89e3\u51b3\uff0c\u65e0\u9700\u9012\u5f52\u6216\u6811\u72b6\u7ed3\u6784\uff0c\u5c55\u793a\u4e86Transformer\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15782", "pdf": "https://arxiv.org/pdf/2504.15782", "abs": "https://arxiv.org/abs/2504.15782", "authors": ["Daniele Baieri", "Riccardo Cicciarella", "Michael Kr\u00fctzen", "Emanuele Rodol\u00e0", "Silvia Zuffi"], "title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos", "categories": ["cs.CV", "cs.GR", "I.4.8; J.3"], "comment": "9 pages, 7 figures", "summary": "We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76ee\u89c6\u9891\u4f30\u8ba1\u91ce\u751f\u6d77\u8c5a\u76843D\u5f62\u72b6\u548c\u8fd0\u52a8\uff0c\u4ee5\u8bc4\u4f30\u5176\u8eab\u4f53\u72b6\u51b5\u3002", "motivation": "\u6c34\u751f\u52a8\u7269\u5728\u81ea\u7136\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u89c2\u6d4b\u56f0\u96be\uff0c\u5bfc\u81f4\u51763D\u91cd\u5efa\u7814\u7a76\u8f83\u5c11\uff0c\u800c\u6d77\u8c5a\u7684\u8eab\u4f53\u72b6\u51b5\u8bc4\u4f30\u9700\u8981\u51c6\u786e\u76843D\u6570\u636e\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u8f93\u6a21\u578b\u5904\u7406\u6c34\u5f15\u8d77\u7684\u906e\u6321\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u4e0d\u540c\u6d77\u51b5\u4e0b\u7684\u89c6\u9891\u6570\u636e\u3002", "result": "\u4f30\u8ba1\u4e86\u6d77\u8c5a\u7684\u8d28\u91cf\u548c\u4f53\u79ef\uff0c\u5e76\u4e0e\u57fa\u4e8e\u624b\u52a82D\u6d4b\u91cf\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c34\u751f\u52a8\u72693D\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u8eab\u4f53\u72b6\u51b5\u8bc4\u4f30\u3002"}}
{"id": "2504.15371", "pdf": "https://arxiv.org/pdf/2504.15371", "abs": "https://arxiv.org/abs/2504.15371", "authors": ["Wei Fang", "Priyadarshini Panda"], "title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aevent2vec\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u4e8b\u4ef6\u76f8\u673a\u8f93\u51fa\u7684\u5f02\u6b65\u3001\u7a00\u758f\u3001\u4e0d\u89c4\u5219\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u5411\u91cf\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4e0e\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u65f6\u95f4\u5206\u8fa8\u7387\u3001\u80fd\u6548\u548c\u52a8\u6001\u8303\u56f4\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u76f8\u673a\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u4e8b\u4ef6\u6570\u636e\u4e0e\u4e3b\u6d41\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u9884\u5904\u7406\u590d\u6742\u3001\u4e22\u5931\u65f6\u95f4\u5206\u8fa8\u7387\u6216\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97\u7b49\u95ee\u9898\u3002", "method": "\u53d7word2vec\u542f\u53d1\uff0c\u4f5c\u8005\u603b\u7ed3\u4e86\u4e8b\u4ef6\u4e0e\u5355\u8bcd\u7684\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u4e86event2vec\u65b9\u6cd5\uff0c\u5c06\u4e8b\u4ef6\u8f6c\u6362\u4e3a\u5411\u91cf\u8868\u793a\u3002", "result": "\u5728ASL-DVS\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cevent2vec\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u56fe/\u56fe\u50cf/\u4f53\u7d20\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "event2vec\u4e0d\u4ec5\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u5c06\u4e8b\u4ef6\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5bf9\u9f50\uff0c\u4e3a\u4e8b\u4ef6\u6570\u636e\u878d\u5165\u5927\u578b\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2504.15392", "pdf": "https://arxiv.org/pdf/2504.15392", "abs": "https://arxiv.org/abs/2504.15392", "authors": ["Myrthe Reuver", "Indira Sen", "Matteo Melis", "Gabriella Lapesa"], "title": "Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted and published at Findings of NAACL 2025: cite published\n  version whenever possible", "summary": "This paper investigates hybrid intelligence and collaboration between\nresearchers of sexism and Large Language Models (LLMs), with a four-component\npipeline. First, nine sexism researchers answer questions about their knowledge\nof sexism and of LLMs. They then participate in two interactive experiments\ninvolving an LLM (GPT3.5). The first experiment has experts assessing the\nmodel's knowledge about sexism and suitability for use in research. The second\nexperiment tasks them with creating three different definitions of sexism: an\nexpert-written definition, an LLM-written one, and a co-created definition.\nLastly, zero-shot classification experiments use the three definitions from\neach expert in a prompt template for sexism detection, evaluating GPT4o on\n2.500 texts sampled from five sexism benchmarks. We then analyze the resulting\n67.500 classification decisions. The LLM interactions lead to longer and more\ncomplex definitions of sexism. Expert-written definitions on average perform\npoorly compared to LLM-generated definitions. However, some experts do improve\nclassification performance with their co-created definitions of sexism, also\nexperts who are inexperienced in using LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6027\u522b\u6b67\u89c6\u7814\u7a76\u8005\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u534f\u4f5c\uff0c\u901a\u8fc7\u56db\u6b65\u6d41\u7a0b\u8bc4\u4f30LLMs\u5728\u6027\u522b\u6b67\u89c6\u7814\u7a76\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6027\u522b\u6b67\u89c6\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u4e13\u5bb6\u4e0eAI\u534f\u4f5c\u7684\u6548\u679c\u3002", "method": "1. \u4e13\u5bb6\u56de\u7b54\u95ee\u9898\uff1b2. \u53c2\u4e0e\u4e24\u9879\u4ea4\u4e92\u5b9e\u9a8c\uff08\u8bc4\u4f30LLM\u77e5\u8bc6\u548c\u5b9a\u4e49\u6027\u522b\u6b67\u89c6\uff09\uff1b3. \u96f6\u6837\u672c\u5206\u7c7b\u5b9e\u9a8c\u3002", "result": "LLM\u751f\u6210\u7684\u5b9a\u4e49\u66f4\u957f\u4e14\u590d\u6742\uff0c\u4e13\u5bb6\u5b9a\u4e49\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u90e8\u5206\u4e13\u5bb6\u901a\u8fc7\u534f\u4f5c\u5b9a\u4e49\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u6027\u522b\u6b67\u89c6\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e13\u5bb6\u534f\u4f5c\u53ef\u6539\u5584\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5bf9LLM\u7ecf\u9a8c\u4e0d\u8db3\u7684\u4e13\u5bb6\u3002"}}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.", "AI": {"tldr": "CameraBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u76f8\u673a\u8fd0\u52a8\u7406\u89e3\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3000\u4e2a\u591a\u6837\u5316\u89c6\u9891\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u673a\u8fd0\u52a8\u5206\u7c7b\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u4e13\u5bb6\u6807\u6ce8\u548c\u57f9\u8bad\u80fd\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86SfM\u548cVLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u751f\u6210\u5f0fVLM\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7406\u89e3\u76f8\u673a\u8fd0\u52a8\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u8bed\u4e49\u548c\u51e0\u4f55\u8fd0\u52a8\u7684\u5206\u8fa8\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86CameraBench\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u9891\u548c\u76f8\u673a\u8fd0\u52a8\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u8bc4\u4f30\u6807\u6ce8\u6027\u80fd\uff0c\u5e76\u6d4b\u8bd5\u4e86SfM\u548cVLM\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "SfM\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4f9d\u8d56\u573a\u666f\u5185\u5bb9\u7684\u8bed\u4e49\u8fd0\u52a8\uff0c\u800cVLM\u6a21\u578b\u5728\u51e0\u4f55\u8fd0\u52a8\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u901a\u8fc7\u5fae\u8c03\u751f\u6210\u5f0fVLM\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u548c\u51e0\u4f55\u8fd0\u52a8\u7684\u7ed3\u5408\u5e94\u7528\u3002", "conclusion": "CameraBench\u7684\u5206\u7c7b\u6cd5\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6559\u7a0b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u4efb\u4f55\u89c6\u9891\u4e2d\u76f8\u673a\u8fd0\u52a8\u7684\u5168\u9762\u7406\u89e3\u3002"}}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.", "AI": {"tldr": "Trillion-7B\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u97e9\u8bed\u4e3a\u4e2d\u5fc3\u7684\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u901a\u8fc7XLDA\u673a\u5236\u548c\u4f18\u5316\u6570\u636e\u6df7\u5408\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4ec5\u752810%\u7684\u591a\u8bed\u8a00\u6570\u636e\u548c\u8f83\u5c11\u8d44\u6e90\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u8d44\u6e90\u6d88\u8017\u9ad8\u548c\u77e5\u8bc6\u8f6c\u79fb\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8de8\u8bed\u8a00\u6587\u6863\u6ce8\u610f\u529b\u673a\u5236\uff08XLDA\uff09\u3001\u4f18\u5316\u6570\u636e\u6df7\u5408\u3001\u8bed\u8a00\u7279\u5b9a\u8fc7\u6ee4\u548c\u5b9a\u5236\u5206\u8bcd\u5668\u3002", "result": "\u572827\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u7a81\u51fa\u3002", "conclusion": "Trillion-7B\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\u3002"}}
{"id": "2504.15378", "pdf": "https://arxiv.org/pdf/2504.15378", "abs": "https://arxiv.org/abs/2504.15378", "authors": ["Scott Sorensen", "Wayne Treible", "Robert Wagner", "Andrew D. Gilliam", "Todd Rovito", "Joseph L. Mundy"], "title": "Physics Driven Image Simulation from Commercial Satellite Imagery", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Physics driven image simulation allows for the modeling and creation of\nrealistic imagery beyond what is afforded by typical rendering pipelines. We\naim to automatically generate a physically realistic scene for simulation of a\ngiven region using satellite imagery to model the scene geometry, drive\nmaterial estimates, and populate the scene with dynamic elements. We present\nautomated techniques to utilize satellite imagery throughout the simulated\nscene to expedite scene construction and decrease manual overhead. Our\ntechnique does not use lidar, enabling simulations that could not be\nconstructed previously. To develop a 3D scene, we model the various components\nof the real location, addressing the terrain, modelling man-made structures,\nand populating the scene with smaller elements such as vegetation and vehicles.\nTo create the scene we begin with a Digital Surface Model, which serves as the\nbasis for scene geometry, and allows us to reason about the real location in a\ncommon 3D frame of reference. These simulated scenes can provide increased\nfidelity with less manual intervention for novel locations on earth, and can\nfacilitate algorithm development, and processing pipelines for imagery ranging\nfrom UV to LWIR $(200nm-20\\mu m)$.", "AI": {"tldr": "\u5229\u7528\u536b\u661f\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u7269\u7406\u771f\u5b9e\u7684\u573a\u666f\u6a21\u62df\uff0c\u65e0\u9700\u6fc0\u5149\u96f7\u8fbe\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u56fe\u50cf\u6a21\u62df\uff0c\u8d85\u8d8a\u4f20\u7edf\u6e32\u67d3\u7ba1\u9053\u7684\u9650\u5236\uff0c\u81ea\u52a8\u751f\u6210\u771f\u5b9e\u573a\u666f\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u57fa\u4e8e\u6570\u5b57\u8868\u9762\u6a21\u578b\uff08DSM\uff09\u6784\u5efa\u573a\u666f\u51e0\u4f55\uff0c\u5229\u7528\u536b\u661f\u56fe\u50cf\u4f30\u8ba1\u6750\u6599\u5e76\u586b\u5145\u52a8\u6001\u5143\u7d20\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u573a\u666f\u6a21\u62df\uff0c\u652f\u6301\u4eceUV\u5230LWIR\uff08200nm-20\u03bcm\uff09\u7684\u7b97\u6cd5\u5f00\u53d1\u548c\u56fe\u50cf\u5904\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5730\u7403\u4e0a\u65b0\u4f4d\u7f6e\u7684\u573a\u666f\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u6a21\u62df\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.15432", "pdf": "https://arxiv.org/pdf/2504.15432", "abs": "https://arxiv.org/abs/2504.15432", "authors": ["Yucheng Lu", "Kazimier Smith"], "title": "Feeding LLM Annotations to BERT Classifiers at Your Own Risk", "categories": ["cs.CL"], "comment": null, "summary": "Using LLM-generated labels to fine-tune smaller encoder-only models for text\nclassification has gained popularity in various settings. While this approach\nmay be justified in simple and low-stakes applications, we conduct empirical\nanalysis to demonstrate how the perennial curse of training on synthetic data\nmanifests itself in this specific setup. Compared to models trained on gold\nlabels, we observe not only the expected performance degradation in accuracy\nand F1 score, but also increased instability across training runs and premature\nperformance plateaus. These findings cast doubts on the reliability of such\napproaches in real-world applications. We contextualize the observed phenomena\nthrough the lens of error propagation and offer several practical mitigation\nstrategies, including entropy-based filtering and ensemble techniques. Although\nthese heuristics offer partial relief, they do not fully resolve the inherent\nrisks of propagating non-random errors from LLM annotations to smaller\nclassifiers, underscoring the need for caution when applying this workflow in\nhigh-stakes text classification tasks.", "AI": {"tldr": "\u4f7f\u7528LLM\u751f\u6210\u7684\u6807\u7b7e\u5fae\u8c03\u5c0f\u578b\u7f16\u7801\u5668\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u6d41\u884c\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3001\u4e0d\u7a33\u5b9a\u548c\u8fc7\u65e9\u6027\u80fd\u505c\u6ede\uff0c\u9700\u8c28\u614e\u7528\u4e8e\u9ad8\u98ce\u9669\u4efb\u52a1\u3002", "motivation": "\u63a2\u8ba8\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u4f7f\u7528LLM\u751f\u6210\u6807\u7b7e\u5fae\u8c03\u5c0f\u578b\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83\u57fa\u4e8e\u5408\u6210\u6570\u636e\u548c\u9ec4\u91d1\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5206\u6790\u8bef\u5dee\u4f20\u64ad\u73b0\u8c61\u3002", "result": "\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5bfc\u81f4\u51c6\u786e\u6027\u3001F1\u5206\u6570\u4e0b\u964d\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u6027\u80fd\u8fc7\u65e9\u505c\u6ede\u3002", "conclusion": "\u9700\u8c28\u614e\u4f7f\u7528LLM\u751f\u6210\u6807\u7b7e\u5fae\u8c03\u6a21\u578b\uff0c\u5efa\u8bae\u91c7\u7528\u71b5\u8fc7\u6ee4\u548c\u96c6\u6210\u6280\u672f\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u98ce\u9669\u3002"}}
{"id": "2504.15380", "pdf": "https://arxiv.org/pdf/2504.15380", "abs": "https://arxiv.org/abs/2504.15380", "authors": ["Huimin Zeng", "Jiacheng Li", "Zhiwei Xiong"], "title": "Plug-and-Play Versatile Compressed Video Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "As a widely adopted technique in data transmission, video compression\neffectively reduces the size of files, making it possible for real-time cloud\ncomputing. However, it comes at the cost of visual quality, posing challenges\nto the robustness of downstream vision models. In this work, we present a\nversatile codec-aware enhancement framework that reuses codec information to\nadaptively enhance videos under different compression settings, assisting\nvarious downstream vision tasks without introducing computation bottleneck.\nSpecifically, the proposed codec-aware framework consists of a\ncompression-aware adaptation (CAA) network that employs a hierarchical\nadaptation mechanism to estimate parameters of the frame-wise enhancement\nnetwork, namely the bitstream-aware enhancement (BAE) network. The BAE network\nfurther leverages temporal and spatial priors embedded in the bitstream to\neffectively improve the quality of compressed input frames. Extensive\nexperimental results demonstrate the superior quality enhancement performance\nof our framework over existing enhancement methods, as well as its versatility\nin assisting multiple downstream tasks on compressed videos as a plug-and-play\nmodule. Code and models are available at\nhttps://huimin-zeng.github.io/PnP-VCVE/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u89e3\u7801\u5668\u4fe1\u606f\u7684\u89c6\u9891\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u589e\u5f3a\u538b\u7f29\u89c6\u9891\u8d28\u91cf\uff0c\u63d0\u5347\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89c6\u9891\u538b\u7f29\u867d\u51cf\u5c11\u6587\u4ef6\u5927\u5c0f\uff0c\u4f46\u4f1a\u964d\u4f4e\u89c6\u89c9\u8d28\u91cf\uff0c\u5f71\u54cd\u4e0b\u6e38\u89c6\u89c9\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u7f16\u89e3\u7801\u5668\u611f\u77e5\u589e\u5f3a\u6846\u67b6\uff0c\u5305\u62ec\u538b\u7f29\u611f\u77e5\u9002\u5e94\u7f51\u7edc\uff08CAA\uff09\u548c\u6bd4\u7279\u6d41\u611f\u77e5\u589e\u5f3a\u7f51\u7edc\uff08BAE\uff09\uff0c\u5229\u7528\u7f16\u89e3\u7801\u4fe1\u606f\u81ea\u9002\u5e94\u589e\u5f3a\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8d28\u91cf\u589e\u5f3a\u548c\u8f85\u52a9\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u6709\u6548\u63d0\u5347\u538b\u7f29\u89c6\u9891\u7684\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2504.15471", "pdf": "https://arxiv.org/pdf/2504.15471", "abs": "https://arxiv.org/abs/2504.15471", "authors": ["Tyler A. Chang", "Benjamin K. Bergen"], "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In Transformer language models, activation vectors transform from current\ntoken embeddings to next token predictions as they pass through the model. To\nisolate a minimal form of this transformation, we identify language model\nsubnetworks that make bigram predictions, naive next token predictions based\nonly on the current token. We find that bigram subnetworks can be found in\nfully trained language models up to 1B parameters, and these subnetworks are\ncritical for model performance even when they consist of less than 0.2% of\nmodel parameters. Bigram subnetworks are concentrated in the first Transformer\nMLP layer, and they overlap significantly with subnetworks trained to optimally\nprune a given model. Mechanistically, the bigram subnetworks often recreate a\npattern from the full models where the first layer induces a sharp change that\naligns activations with next token predictions rather than current token\nrepresentations. Our results demonstrate that bigram subnetworks comprise a\nminimal subset of parameters that are both necessary and sufficient for basic\nnext token predictions in language models, and they help drive the\ntransformation from current to next token activations in the residual stream.\nThese subnetworks can lay a foundation for studying language model circuits by\nbuilding up from a minimal circuit rather than the traditional approach of\nablating circuits from a full model.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u7528\u4e8e\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6700\u5c0f\u5b50\u7f51\u7edc\uff08bigram subnetworks\uff09\uff0c\u53d1\u73b0\u8fd9\u4e9b\u5b50\u7f51\u7edc\u5728\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u5b58\u5728\u4e14\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4ec5\u5360\u4e0d\u52300.2%\u7684\u53c2\u6570\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u4e2d\u4ece\u5f53\u524d\u8bcd\u5d4c\u5165\u5230\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u7684\u6700\u5c0f\u8f6c\u6362\u673a\u5236\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u7684\u57fa\u672c\u5de5\u4f5c\u65b9\u5f0f\u3002", "method": "\u8bc6\u522b\u5e76\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e2d\u7684bigram\u5b50\u7f51\u7edc\uff0c\u8fd9\u4e9b\u5b50\u7f51\u7edc\u4ec5\u57fa\u4e8e\u5f53\u524d\u8bcd\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002", "result": "bigram\u5b50\u7f51\u7edc\u57281B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u5b58\u5728\uff0c\u96c6\u4e2d\u4e8e\u7b2c\u4e00\u5c42MLP\uff0c\u4e14\u4e0e\u6700\u4f18\u526a\u679d\u5b50\u7f51\u7edc\u91cd\u53e0\u3002\u5b83\u4eec\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "bigram\u5b50\u7f51\u7edc\u662f\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u57fa\u672c\u9884\u6d4b\u7684\u6700\u5c0f\u5fc5\u8981\u5b50\u96c6\uff0c\u4e3a\u7814\u7a76\u6a21\u578b\u7535\u8def\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.15384", "pdf": "https://arxiv.org/pdf/2504.15384", "abs": "https://arxiv.org/abs/2504.15384", "authors": ["Chen Zhao", "Anjum Shaik", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Qiuying Sha", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images", "categories": ["cs.CV"], "comment": "23 pages, 4 figures", "summary": "Hip fractures represent a major health concern, particularly among the\nelderly, often leading decreased mobility and increased mortality. Early and\naccurate detection of at risk individuals is crucial for effective\nintervention. In this study, we propose Iterative Cross Graph Matching for Hip\nFracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip\nfractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX\ninvolves iteratively comparing a test (subject) graph with multiple template\ngraphs representing the characteristics of hip fracture subjects to assess the\nsimilarity and accurately to predict hip fracture risk. These graphs are\nobtained as follows. The DXA images are separated into multiple regions of\ninterest (RoIs), such as the femoral head, shaft, and lesser trochanter.\nRadiomic features are then calculated for each RoI, with the central\ncoordinates used as nodes in a graph. The connectivity between nodes is\nestablished according to the Euclidean distance between these coordinates. This\nprocess transforms each DXA image into a graph, where each node represents a\nRoI, and edges derived by the centroids of RoIs capture the spatial\nrelationships between them. If the test graph closely matches a set of template\ngraphs representing subjects with incident hip fractures, it is classified as\nindicating high hip fracture risk. We evaluated our method using 547 subjects\nfrom the UK Biobank dataset, and experimental results show that ICGM-FRAX\nachieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip\nfractures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u80fdX\u5c04\u7ebf\u5438\u6536\u6d4b\u91cf\uff08DXA\uff09\u56fe\u50cf\u7684\u8fed\u4ee3\u4ea4\u53c9\u56fe\u5339\u914d\u65b9\u6cd5\uff08ICGM-FRAX\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06DXA\u56fe\u50cf\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u5e76\u4e0e\u6a21\u677f\u56fe\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u7684\u9884\u6d4b\u3002", "motivation": "\u9acb\u90e8\u9aa8\u6298\u5bf9\u8001\u5e74\u4eba\u5065\u5eb7\u5f71\u54cd\u91cd\u5927\uff0c\u65e9\u671f\u51c6\u786e\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53\u5bf9\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06DXA\u56fe\u50cf\u5206\u5272\u4e3a\u591a\u4e2a\u611f\u5174\u8da3\u533a\u57df\uff08RoIs\uff09\uff0c\u63d0\u53d6\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5e76\u6784\u5efa\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u8fed\u4ee3\u5339\u914d\u6d4b\u8bd5\u56fe\u4e0e\u6a21\u677f\u56fe\u8bc4\u4f30\u9aa8\u6298\u98ce\u9669\u3002", "result": "\u5728547\u540d\u53d7\u8bd5\u8005\u4e2d\uff0cICGM-FRAX\u7684\u7075\u654f\u5ea6\u8fbe\u52300.9869\uff0c\u663e\u793a\u51fa\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "ICGM-FRAX\u662f\u4e00\u79cd\u6709\u6548\u7684\u9acb\u90e8\u9aa8\u6298\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15475", "pdf": "https://arxiv.org/pdf/2504.15475", "abs": "https://arxiv.org/abs/2504.15475", "authors": ["Szymon Kobus", "Deniz G\u00fcnd\u00fcz"], "title": "Speculative Sampling via Exponential Races", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Speculative decoding accelerates large language model inference using a\nsmaller draft model. In this paper, we establish a surprising connection\nbetween speculative decoding and channel simulation, which aims at simulating a\nnoisy channel using as few bits as possible. This connection allows us to\nprovide an information-theoretic analysis of the speed up that can be achieved\nby speculative decoding. Leveraging this link, we derive an explicit relation\nbetween generation speed-up and the number of tokens $k$ generated by the draft\nmodel for large $k$, which serves as an upper bound for all $k$. We also\npropose a novel speculative decoding method via exponential race ERSD that\nmatches state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5efa\u7acb\u63a8\u6d4b\u89e3\u7801\u4e0e\u4fe1\u9053\u6a21\u62df\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u4e86\u751f\u6210\u901f\u5ea6\u63d0\u5347\u4e0e\u8349\u7a3f\u6a21\u578b\u751f\u6210\u4ee4\u724c\u6570k\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5ERSD\u3002", "motivation": "\u63a2\u8ba8\u63a8\u6d4b\u89e3\u7801\u4e0e\u4fe1\u9053\u6a21\u62df\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4ee5\u4fe1\u606f\u8bba\u4e3a\u57fa\u7840\u5206\u6790\u63a8\u6d4b\u89e3\u7801\u7684\u52a0\u901f\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u63a8\u6d4b\u89e3\u7801\u4e0e\u4fe1\u9053\u6a21\u62df\u7684\u8054\u7cfb\uff0c\u63a8\u5bfc\u901f\u5ea6\u63d0\u5347\u4e0e\u4ee4\u724c\u6570k\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5ERSD\u3002", "result": "\u63a8\u5bfc\u51fa\u751f\u6210\u901f\u5ea6\u63d0\u5347\u4e0ek\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u5e76\u9a8c\u8bc1ERSD\u65b9\u6cd5\u7684\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u4e3a\u63a8\u6d4b\u89e3\u7801\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5ERSD\u3002"}}
{"id": "2504.15397", "pdf": "https://arxiv.org/pdf/2504.15397", "abs": "https://arxiv.org/abs/2504.15397", "authors": ["Ankit Dhiman", "Manan Shah", "R Venkatesh Babu"], "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://mirror-verse.github.io/", "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6269\u6563\u6a21\u578b\u751f\u6210\u771f\u5b9e\u955c\u50cf\u53cd\u5c04\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u955c\u50cf\u53cd\u5c04\u65f6\u96be\u4ee5\u5b8c\u5168\u9075\u5faa\u7269\u7406\u89c4\u5f8b\uff0c\u5c24\u5176\u662f\u5728\u7269\u4f53\u4f4d\u7f6e\u548c\u65b9\u5411\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u5408\u6210\u6570\u636e\u589e\u5f3a\uff08\u968f\u673a\u4f4d\u7f6e\u3001\u65cb\u8f6c\u548c\u7269\u4f53\u914d\u5bf9\uff09\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5f00\u53d1MirrorFusion 2.0\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u96c6\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u652f\u6301\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u955c\u50cf\u53cd\u5c04\u751f\u6210\u7684\u771f\u5b9e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2504.15509", "pdf": "https://arxiv.org/pdf/2504.15509", "abs": "https://arxiv.org/abs/2504.15509", "authors": ["Keqi Deng", "Wenxi Chen", "Xie Chen", "Philip C. Woodland"], "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous speech translation (SST) outputs translations in parallel with\nstreaming speech input, balancing translation quality and latency. While large\nlanguage models (LLMs) have been extended to handle the speech modality,\nstreaming remains challenging as speech is prepended as a prompt for the entire\ngeneration process. To unlock LLM streaming capability, this paper proposes\nSimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy\nto guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between\ntraining and inference by extracting boundary-aware speech prompts that allows\nit to be better matched with text input data. SimulS2S-LLM achieves\nsimultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete\noutput speech tokens and then synthesising output speech using a pre-trained\nvocoder. An incremental beam search is designed to expand the search space of\nspeech token prediction without increasing latency. Experiments on the CVSS\nspeech data show that SimulS2S-LLM offers a better translation quality-latency\ntrade-off than existing methods that use the same training data, such as\nimproving ASR-BLEU scores by 3 points at similar latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSimulS2S-LLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bad\u7ec3\u8bed\u97f3\u5927\u6a21\u578b\u548c\u6d4b\u8bd5\u65f6\u7b56\u7565\u5b9e\u73b0\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\uff0c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u5ef6\u8fdf\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u56e0\u8bed\u97f3\u4f5c\u4e3a\u63d0\u793a\u5bfc\u81f4\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u79bb\u7ebf\u8bad\u7ec3\u8bed\u97f3\u5927\u6a21\u578b\uff0c\u63d0\u53d6\u8fb9\u754c\u611f\u77e5\u8bed\u97f3\u63d0\u793a\uff0c\u8bbe\u8ba1\u589e\u91cf\u675f\u641c\u7d22\u9884\u6d4b\u8bed\u97f3\u6807\u8bb0\u3002", "result": "\u5728CVSS\u8bed\u97f3\u6570\u636e\u4e0a\uff0cSimulS2S-LLM\u5728\u76f8\u540c\u5ef6\u8fdf\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8ASR-BLEU\u5206\u65703\u5206\u3002", "conclusion": "SimulS2S-LLM\u6709\u6548\u63d0\u5347\u4e86\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\u5e73\u8861\u3002"}}
{"id": "2504.15404", "pdf": "https://arxiv.org/pdf/2504.15404", "abs": "https://arxiv.org/abs/2504.15404", "authors": ["Tajamul Ashraf", "Rajes Manna", "Partha Sarathi Purkayastha", "Tavaheed Tariq", "Janibul Bashir"], "title": "Context Aware Grounded Teacher for Source Free Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We focus on the Source Free Object Detection (SFOD) problem, when source data\nis unavailable during adaptation, and the model must adapt to the unlabeled\ntarget domain. In medical imaging, several approaches have leveraged a\nsemi-supervised student-teacher architecture to bridge domain discrepancy.\nContext imbalance in labeled training data and significant domain shifts\nbetween domains can lead to biased teacher models that produce inaccurate\npseudolabels, degrading the student model's performance and causing a mode\ncollapse. Class imbalance, particularly when one class significantly outnumbers\nanother, leads to contextual bias. To tackle the problem of context bias and\nthe significant performance drop of the student model in the SFOD setting, we\nintroduce Grounded Teacher (GT) as a standard framework. In this study, we\nmodel contextual relationships using a dedicated relational context module and\nleverage it to mitigate inherent biases in the model. This approach enables us\nto apply augmentations to closely related classes, across and within domains,\nenhancing the performance of underrepresented classes while keeping the effect\non dominant classes minimal. We further improve the quality of predictions by\nimplementing an expert foundational branch to supervise the student model. We\nvalidate the effectiveness of our approach in mitigating context bias under the\nSFOD setting through experiments on three medical datasets supported by\ncomprehensive ablation studies. All relevant resources, including preprocessed\ndata, trained model weights, and code, are publicly available at this\nhttps://github.com/Tajamul21/Grounded_Teacher.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrounded Teacher (GT)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\uff08SFOD\uff09\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u5173\u7cfb\u4e0a\u4e0b\u6587\u6a21\u5757\u548c\u4e13\u5bb6\u57fa\u7840\u5206\u652f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\uff0c\u6e90\u6570\u636e\u4e0d\u53ef\u7528\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u4e0a\u4e0b\u6587\u4e0d\u5e73\u8861\u548c\u9886\u57df\u504f\u79fb\u5bfc\u81f4\u6559\u5e08\u6a21\u578b\u504f\u5dee\uff0c\u5f71\u54cd\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5173\u7cfb\u4e0a\u4e0b\u6587\u6a21\u5757\u5efa\u6a21\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u7ed3\u5408\u4e13\u5bb6\u57fa\u7840\u5206\u652f\u76d1\u7763\u5b66\u751f\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u76f8\u5173\u7c7b\u522b\u6765\u51cf\u5c11\u504f\u5dee\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GT\u6846\u67b6\u5728\u51cf\u5c11\u4e0a\u4e0b\u6587\u504f\u5dee\u548c\u63d0\u5347\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "GT\u6846\u67b6\u901a\u8fc7\u5173\u7cfb\u5efa\u6a21\u548c\u4e13\u5bb6\u76d1\u7763\uff0c\u663e\u8457\u6539\u5584\u4e86SFOD\u8bbe\u7f6e\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.15521", "pdf": "https://arxiv.org/pdf/2504.15521", "abs": "https://arxiv.org/abs/2504.15521", "authors": ["Minghao Wu", "Weixuan Wang", "Sinuo Liu", "Huifeng Yin", "Xintong Wang", "Yu Zhao", "Chenyang Lyu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "categories": ["cs.CL"], "comment": "work in progress; 22 pages, 8 figures, 3 tables;", "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e862000\u591a\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u82f1\u8bed\u4ecd\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4e14\u7ffb\u8bd1\u57fa\u51c6\u4e0e\u672c\u5730\u5316\u57fa\u51c6\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u7684\u76f8\u5173\u6027\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u63a8\u52a8\u591a\u8bed\u8a00\u8bc4\u4f30\u7684\u516c\u5e73\u6027\uff0c\u63ed\u793a\u5f53\u524d\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u5206\u6790\u4e86148\u4e2a\u56fd\u5bb62000\u591a\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u3002", "result": "\u82f1\u8bed\u8fc7\u5ea6\u4ee3\u8868\uff0c\u672c\u5730\u5316\u57fa\u51c6\u4f18\u4e8e\u7ffb\u8bd1\u57fa\u51c6\uff0cSTEM\u4efb\u52a1\u76f8\u5173\u6027\u9ad8\u4e8e\u4f20\u7edfNLP\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u7684\u6307\u5bfc\u539f\u5219\u548c\u7814\u7a76\u65b9\u5411\uff0c\u547c\u5401\u5168\u7403\u5408\u4f5c\u5f00\u53d1\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u57fa\u51c6\u3002"}}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.", "AI": {"tldr": "IV-Bench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u56fe\u50cf\u80cc\u666f\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u4f5c\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b967\u4e2a\u89c6\u9891\u548c2585\u4e2a\u56fe\u50cf-\u6587\u672c\u67e5\u8be2\uff0c\u8986\u76d613\u4e2a\u4efb\u52a1\u3002\u5f53\u524dMLLMs\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4ec528.9%\u3002", "motivation": "\u73b0\u6709MLLMs\u8bc4\u4f30\u6846\u67b6\u5ffd\u89c6\u4e86\u56fe\u50cf\u80cc\u666f\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0cIV-Bench\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faIV-Bench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u89c6\u9891\u548c\u56fe\u50cf-\u6587\u672c\u67e5\u8be2\u4efb\u52a1\uff0c\u5e76\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90MLLMs\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f53\u524dMLLMs\u5728\u56fe\u50cf\u80cc\u666f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u6700\u9ad8\u51c6\u786e\u738728.9%\uff0c\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u5305\u62ec\u63a8\u7406\u6a21\u5f0f\u3001\u5e27\u6570\u548c\u5206\u8fa8\u7387\u3002", "conclusion": "IV-Bench\u63ed\u793a\u4e86MLLMs\u5728\u56fe\u50cf\u80cc\u666f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u7684IP\u4efb\u52a1\u5206\u7c7b\u548c\u5927\u89c4\u6a21\u53cc\u8bed\u57fa\u51c6IPBench\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u77e5\u8bc6\u4ea7\u6743\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u77e5\u8bc6\u4ea7\u6743\u9886\u57df\u590d\u6742\u4e14\u77e5\u8bc6\u5bc6\u96c6\uff0c\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u672a\u80fd\u5168\u9762\u8986\u76d6\u5b9e\u9645\u573a\u666f\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165IPBench\u57fa\u51c6\uff0c\u6db5\u76d68\u79cdIP\u673a\u5236\u548c20\u9879\u4efb\u52a1\uff0c\u8bc4\u4f3016\u79cdLLMs\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a75.8%\uff0c\u5f00\u6e90\u548c\u6cd5\u5f8b\u5bfc\u5411\u6a21\u578b\u8868\u73b0\u843d\u540e\u4e8e\u95ed\u6e90\u901a\u7528\u6a21\u578b\u3002", "conclusion": "IPBench\u586b\u8865\u4e86\u77e5\u8bc6\u4ea7\u6743\u9886\u57df\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u672a\u6765\u5c06\u6301\u7eed\u66f4\u65b0\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5b9e\u9645\u6311\u6218\u3002"}}
{"id": "2504.15470", "pdf": "https://arxiv.org/pdf/2504.15470", "abs": "https://arxiv.org/abs/2504.15470", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025 (The International Conference on Learning\n  Representations)", "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6d41\u5f62\u5206\u6790\u7684\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u548c\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u533a\u5206\u771f\u5b9e\u4e0eAI\u751f\u6210\u56fe\u50cf\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u4e14\u6027\u80fd\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u6982\u7387\u6d41\u5f62\u504f\u5dee\uff0c\u5229\u7528\u5206\u6570\u51fd\u6570\u8fd1\u4f3c\u66f2\u7387\u3001\u68af\u5ea6\u548c\u504f\u5dee\uff0c\u5e76\u6269\u5c55\u81f3\u5c0f\u6837\u672c\u573a\u666f\u3002", "result": "\u572820\u79cd\u751f\u6210\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6d41\u5f62\u5206\u6790\uff0c\u672c\u7814\u7a76\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u504f\u5dee\u7684\u7406\u8bba\u7406\u89e3\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2504.15527", "pdf": "https://arxiv.org/pdf/2504.15527", "abs": "https://arxiv.org/abs/2504.15527", "authors": ["Sophia Maria"], "title": "Compass-V2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Predominant LLMs focus on high-resource languages while leaving low-resource\nlanguages, particularly those in Southeast Asia (SEA), underrepresented. In\naddition, those models are general-purpose and pay limited attention to the\ne-commerce domain. To overcome these limitations, we introduce Compass-v2, a\nlightweight Mixture-of-Experts (MoE) model specifically designed for Southeast\nAsian languages and e-commerce applications. To balance model performance and\ninference cost, the model is designed with 30B total parameters and 5B active\nparameters, incorporating both fine-grained and shared expert modules. To\nenhance multilingual performance, we curated and constructed a high-quality,\nindustry-leading SEA dataset, to the best of our knowledge. To boost\nperformance in the e-commerce domain, we built a dataset comprising hundreds of\nbillions of tokens, sourced through external data mining and internal platform\ncollection. Besides, we pioneered a hybrid reasoning model that supports both\nfast thinking and deep thinking within a unified framework to enhance the\nreasoning capabilities, diverging from the conventional industry practice of\ndeploying two separate models. Through extensive experimental evaluations, our\nmodel demonstrates state-of-the-art SEA multilingual and e-commerce performance\namong sub-30B models, while maintaining significantly lower inference cost.", "AI": {"tldr": "Compass-v2\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684Mixture-of-Experts\u6a21\u578b\uff0c\u4e13\u4e3a\u4e1c\u5357\u4e9a\u8bed\u8a00\u548c\u7535\u5b50\u5546\u52a1\u5e94\u7528\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6df7\u5408\u63a8\u7406\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u4f4e\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e3b\u5bfc\u7684LLMs\u5bf9\u4e1c\u5357\u4e9a\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba130B\u603b\u53c2\u6570\u30015B\u6d3b\u8dc3\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u4e0e\u5171\u4eab\u4e13\u5bb6\u6a21\u5757\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u6df7\u5408\u63a8\u7406\u6a21\u578b\u3002", "result": "\u572830B\u4ee5\u4e0b\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u4e1c\u5357\u4e9a\u591a\u8bed\u8a00\u548c\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "Compass-v2\u6210\u529f\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15473", "pdf": "https://arxiv.org/pdf/2504.15473", "abs": "https://arxiv.org/abs/2504.15473", "authors": ["Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.6; I.2.10"], "comment": "32 pages, 32 figures, preliminary version", "summary": "Diffusion models have become the go-to method for text-to-image generation,\nproducing high-quality images from noise through a process called reverse\ndiffusion. Understanding the dynamics of the reverse diffusion process is\ncrucial in steering the generation and achieving high sample quality. However,\nthe inner workings of diffusion models is still largely a mystery due to their\nblack-box nature and complex, multi-step generation process. Mechanistic\nInterpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at\nuncovering the operating principles of models through granular analysis of\ntheir internal representations. These MI techniques have been successful in\nunderstanding and steering the behavior of large language models at scale.\nHowever, the great potential of SAEs has not yet been applied toward gaining\ninsight into the intricate generative process of diffusion models. In this\nwork, we leverage the SAE framework to probe the inner workings of a popular\ntext-to-image diffusion model, and uncover a variety of human-interpretable\nconcepts in its activations. Interestingly, we find that even before the first\nreverse diffusion step is completed, the final composition of the scene can be\npredicted surprisingly well by looking at the spatial distribution of activated\nconcepts. Moreover, going beyond correlational analysis, we show that the\ndiscovered concepts have a causal effect on the model output and can be\nleveraged to steer the generative process. We design intervention techniques\naimed at manipulating image composition and style, and demonstrate that (1) in\nearly stages of diffusion image composition can be effectively controlled, (2)\nin the middle stages of diffusion image composition is finalized, however\nstylistic interventions are effective, and (3) in the final stages of diffusion\nonly minor textural details are subject to change.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63a2\u7a76\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\uff0c\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u6982\u5ff5\u5bf9\u751f\u6210\u8fc7\u7a0b\u5177\u6709\u56e0\u679c\u5f71\u54cd\uff0c\u53ef\u7528\u4e8e\u63a7\u5236\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u900f\u660e\u3002\u901a\u8fc7\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982SAEs\uff09\u63ed\u793a\u5176\u5de5\u4f5c\u539f\u7406\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528SAEs\u6846\u67b6\u5206\u6790\u6d41\u884c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6fc0\u6d3b\uff0c\u8bc6\u522b\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5e72\u9884\u6280\u672f\u9a8c\u8bc1\u5176\u56e0\u679c\u6548\u5e94\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u6fc0\u6d3b\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u53ef\u7528\u4e8e\u9884\u6d4b\u548c\u64cd\u63a7\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u5305\u62ec\u65e9\u671f\u63a7\u5236\u6784\u56fe\u3001\u4e2d\u671f\u8c03\u6574\u98ce\u683c\u3001\u540e\u671f\u5fae\u8c03\u7ec6\u8282\u3002", "conclusion": "SAEs\u4e3a\u7406\u89e3\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u751f\u6210\u8fc7\u7a0b\u7684\u7cbe\u51c6\u64cd\u63a7\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.15544", "pdf": "https://arxiv.org/pdf/2504.15544", "abs": "https://arxiv.org/abs/2504.15544", "authors": ["Issa Sugiura", "Kouta Nakayama", "Yusuke Oda"], "title": "llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Encoder-only transformer models like BERT are widely adopted as a pre-trained\nbackbone for tasks like sentence classification and retrieval. However,\npretraining of encoder models with large-scale corpora and long contexts has\nbeen relatively underexplored compared to decoder-only transformers. In this\nwork, we present llm-jp-modernbert, a ModernBERT model trained on a publicly\navailable, massive Japanese corpus with a context length of 8192 tokens. While\nour model does not surpass existing baselines on downstream tasks, it achieves\ngood results on fill-mask test evaluations. We also analyze the effect of\ncontext length expansion through pseudo-perplexity experiments. Furthermore, we\ninvestigate sentence embeddings in detail, analyzing their transitions during\ntraining and comparing them with those from other existing models, confirming\nsimilar trends with models sharing the same architecture. To support\nreproducibility and foster the development of long-context BERT, we release our\nmodel, along with the training and evaluation code.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86llm-jp-modernbert\uff0c\u4e00\u79cd\u5728\u65e5\u8bed\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\u7684ModernBERT\u6a21\u578b\uff0c\u652f\u6301\u957f\u4e0a\u4e0b\u6587\uff088192 tokens\uff09\uff0c\u5728\u586b\u5145\u63a9\u7801\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u8d85\u8d8a\u73b0\u6709\u4e0b\u6e38\u4efb\u52a1\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u8bed\u6599\u548c\u957f\u4e0a\u4e0b\u6587\u4e0b\u7f16\u7801\u5668\u6a21\u578b\u7684\u9884\u8bad\u7ec3\uff0c\u586b\u8865\u4e0e\u89e3\u7801\u5668\u6a21\u578b\u76f8\u6bd4\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684\u5927\u89c4\u6a21\u65e5\u8bed\u8bed\u6599\u5e93\u9884\u8bad\u7ec3ModernBERT\u6a21\u578b\uff0c\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u81f38192 tokens\uff0c\u5e76\u901a\u8fc7\u586b\u5145\u63a9\u7801\u6d4b\u8bd5\u548c\u4f2a\u56f0\u60d1\u5ea6\u5b9e\u9a8c\u5206\u6790\u6548\u679c\u3002", "result": "\u6a21\u578b\u5728\u586b\u5145\u63a9\u7801\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u672a\u8d85\u8d8a\u57fa\u7ebf\uff1b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u53e5\u5b50\u5d4c\u5165\u7684\u53d8\u5316\u3002", "conclusion": "llm-jp-modernbert\u4e3a\u957f\u4e0a\u4e0b\u6587BERT\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u9879\u65b0\u4efb\u52a1CAPTURe\uff0c\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u906e\u6321\u7269\u4f53\u7684\u8bc6\u522b\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u906e\u6321\u7269\u4f53\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u5176\u7406\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u6d4b\u8bd5\u548c\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u4e86CAPTURe\u4efb\u52a1\uff0c\u5305\u542b\u771f\u5b9e\u56fe\u50cf\uff08CAPTURe-real\uff09\u548c\u5408\u6210\u56fe\u50cf\uff08CAPTURe-synthetic\uff09\u4e24\u90e8\u5206\uff0c\u8bc4\u4f30\u4e86\u56db\u79cdVLMs\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u906e\u6321\u548c\u975e\u906e\u6321\u60c5\u51b5\u4e0b\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u906e\u6321\u65f6\u66f4\u5dee\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u4f18\u5f02\u3002\u63d0\u4f9b\u906e\u6321\u7269\u4f53\u4f4d\u7f6e\u4fe1\u606f\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "VLMs\u5728\u906e\u6321\u63a8\u7406\u548c\u8ba1\u6570\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2504.15548", "pdf": "https://arxiv.org/pdf/2504.15548", "abs": "https://arxiv.org/abs/2504.15548", "authors": ["Elyas Meguellati", "Assaad Zeghina", "Shazia Sadiq", "Gianluca Demartini"], "title": "LLM-based Semantic Augmentation for Harmful Content Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\u548c\u8bed\u4e49\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u793e\u4ea4\u5a92\u4f53\u4efb\u52a1\uff08\u5982\u5ba3\u4f20\u68c0\u6d4b\u3001\u4ec7\u6068\u5185\u5bb9\u5206\u7c7b\u7b49\uff09\u7684\u6027\u80fd\uff0c\u6548\u679c\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u5f53\u524dLLM\u5728\u7b80\u5355\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u793e\u4ea4\u5a92\u4f53\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5ffd\u7565\u4e86LLM\u5728\u6587\u672c\u9884\u5904\u7406\u548c\u8bed\u4e49\u589e\u5f3a\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u63d0\u793aLLM\u6e05\u7406\u566a\u58f0\u6587\u672c\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u89e3\u91ca\uff0c\u589e\u5f3a\u8bad\u7ec3\u96c6\u8d28\u91cf\uff0c\u800c\u4e0d\u663e\u8457\u589e\u52a0\u6570\u636e\u91cf\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u96f6\u6837\u672cLLM\u5206\u7c7b\u5728\u9ad8\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7ed3\u5408LLM\u8bed\u4e49\u589e\u5f3a\u540e\uff0c\u6027\u80fd\u63a5\u8fd1\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u7b56\u7565\u6027\u5730\u5c06LLM\u6574\u5408\u5230\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u5bf9\u793e\u4ea4\u5a92\u4f53\u5206\u7c7b\u4efb\u52a1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u5728\u7ebf\u6709\u5bb3\u5185\u5bb9\u6cbb\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15513", "pdf": "https://arxiv.org/pdf/2504.15513", "abs": "https://arxiv.org/abs/2504.15513", "authors": ["Yixuan Zhu", "Haolin Wang", "Ao Li", "Wenliang Zhao", "Yansong Tang", "Jingxuan Niu", "Lei Chen", "Jie Zhou", "Jiwen Lu"], "title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive.", "AI": {"tldr": "InstaRevive\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u84b8\u998f\u7684\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u548c\u6587\u672c\u63d0\u793a\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u91c7\u6837\u6b65\u9aa4\u591a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u63a7\u5236\u7684\u6269\u6563\u84b8\u998f\u7ba1\u9053\uff0c\u7ed3\u5408\u6587\u672c\u63d0\u793a\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u9ad8\u6548\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "InstaRevive\u5728\u56fe\u50cf\u589e\u5f3a\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002"}}
{"id": "2504.15573", "pdf": "https://arxiv.org/pdf/2504.15573", "abs": "https://arxiv.org/abs/2504.15573", "authors": ["Yuxin Jiang", "Yufei Wang", "Chuhan Wu", "Xinyi Dai", "Yan Xu", "Weinan Gan", "Yasheng Wang", "Xin Jiang", "Lifeng Shang", "Ruiming Tang", "Wei Wang"], "title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction", "categories": ["cs.CL"], "comment": "15 pages, 11 figures, 9 tables", "summary": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR.", "AI": {"tldr": "WebR\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u7f51\u9875\u6587\u6863\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u79cd\u5b50\u6570\u636e\u8d28\u91cf\u6216\u5f3a\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u79cd\u5b50\u6570\u636e\u8d28\u91cf\u6216\u5f3a\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf\u6307\u4ee4-\u54cd\u5e94\u5bf9\u7684\u751f\u6210\u3002", "method": "\u63d0\u51faWebR\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u8303\u5f0f\uff08Web\u4f5c\u4e3a\u6307\u4ee4\u548cWeb\u4f5c\u4e3a\u54cd\u5e94\uff09\u4ece\u539f\u59cb\u7f51\u9875\u6587\u6863\u5408\u6210\u6570\u636e\u3002", "result": "WebR\u751f\u6210\u7684\u6570\u636e\u5728\u56db\u4e2a\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd516.65%\uff0c\u5e76\u5c55\u793a\u51fa\u66f4\u597d\u7684\u517c\u5bb9\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "WebR\u4e3a\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347LLMs\u6027\u80fd\u3002"}}
{"id": "2504.15599", "pdf": "https://arxiv.org/pdf/2504.15599", "abs": "https://arxiv.org/abs/2504.15599", "authors": ["Shichen Li", "Chenhui Shao"], "title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u9884\u6d4b\u98df\u54c1\u5e72\u71e5\u72b6\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u98df\u54c1\u5e72\u71e5\u7684\u5b9e\u65f6\u9884\u6d4b\u5bf9\u8282\u80fd\u3001\u751f\u4ea7\u6548\u7387\u548c\u4ea7\u54c1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u52a8\u6001\u6027\u548c\u6709\u9650\u6027\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u9891\u6570\u636e\u548c\u5de5\u827a\u53c2\u6570\uff0c\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u3002", "result": "\u6a21\u578b\u5728\u7cd6\u997c\u5e72\u5e72\u71e5\u5b9e\u9a8c\u4e2d\u5e73\u5747\u9884\u6d4b\u8bef\u5dee\u4ec515\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd565.69%\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u89c4\u6a21\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u591a\u6a21\u6001\u878d\u5408\u4efb\u52a1\u3002"}}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86GPT-2\u548cLlama-2-7b-chat-hf\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u589e\u52a0\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u4f1a\u7565\u5fae\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800cLlama-2\u5728\u4f4e\u6e29\u5ea6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u548c\u6e29\u5ea6\u8bbe\u7f6e\u5bf9\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u6545\u4e8b\u7684\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u540c\u6e29\u5ea6\u548c\u63a8\u7406\u5c42\u7ea7\u4e0b\u7684\u8868\u73b0\u3002", "result": "Llama-2\u8868\u73b0\u4f18\u4e8eGPT-2\uff0c\u589e\u52a0\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u6a21\u578b\u5728\u9ad8\u9636\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u66f4\u5927\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u3001\u6e29\u5ea6\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u663e\u8457\u5f71\u54cd\u9884\u6d4b\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2504.15609", "pdf": "https://arxiv.org/pdf/2504.15609", "abs": "https://arxiv.org/abs/2504.15609", "authors": ["Yunfeng Li", "Bo Wang", "Jiahao Wan", "Xueyi Wu", "Ye Li"], "title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6c34\u4e0b\u58f0\u5b66\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6SonarT165\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u6846\u67b6STFTrack\uff0c\u5305\u542b\u591a\u89c6\u89d2\u6a21\u677f\u878d\u5408\u548c\u6700\u4f18\u8f68\u8ff9\u6821\u6b63\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u89c2\u6d4b\u7cfb\u7edf\u5728\u80fd\u89c1\u5ea6\u4e0d\u8db3\u65f6\u4f9d\u8d56\u58f0\u7eb3\u7cfb\u7edf\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faSonarT165\u57fa\u51c6\u548cSTFTrack\u6846\u67b6\uff0c\u5305\u542b\u591a\u89c6\u89d2\u6a21\u677f\u878d\u5408\u6a21\u5757\uff08MTFM\uff09\u548c\u6700\u4f18\u8f68\u8ff9\u6821\u6b63\u6a21\u5757\uff08OTCM\uff09\uff0c\u5e76\u5f15\u5165\u58f0\u5b66\u56fe\u50cf\u589e\u5f3a\u548c\u9891\u7387\u589e\u5f3a\u6a21\u5757\u3002", "result": "STFTrack\u5728SonarT165\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SonarT165\u57fa\u51c6\u548cSTFTrack\u6846\u67b6\u4e3a\u6c34\u4e0b\u58f0\u5b66\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.15630", "pdf": "https://arxiv.org/pdf/2504.15630", "abs": "https://arxiv.org/abs/2504.15630", "authors": ["Xiaowei Yuan", "Zhao Yang", "Ziyang Huang", "Yequan Wang", "Siqi Fan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaLE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3aLLMs\u5185\u90e8\u8868\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u5229\u7528\uff0c\u6539\u8fdb\u4e86\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86LLMs\u5185\u90e8\u72b6\u6001\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5904\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u5176\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002", "method": "\u63d0\u51faContext-aware Layer Enhancement (CaLE)\uff0c\u901a\u8fc7V-usable\u4fe1\u606f\u5206\u6790\u5728\u6700\u4f18\u5c42\u589e\u5f3a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u4e30\u5bcc\u6700\u7ec8\u5c42\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCaLE\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u751f\u6210\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u672a\u77e5\u6216\u51b2\u7a81\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "CaLE\u901a\u8fc7\u4f18\u5316\u5185\u90e8\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2504.15612", "pdf": "https://arxiv.org/pdf/2504.15612", "abs": "https://arxiv.org/abs/2504.15612", "authors": ["Hongxing Peng", "Kang Lin", "Huanai Liu"], "title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684HS-Mamba\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u662f\u9065\u611f\u9886\u57df\u7684\u70ed\u70b9\uff0c\u4f46\u9ad8\u7ef4\u5ea6\u548c\u7279\u5f81\u5185\u8054\u7279\u6027\u4f7fMamba\u67b6\u6784\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\u3002", "method": "HS-Mamba\u91c7\u7528\u53cc\u901a\u9053\u7a7a\u95f4-\u5149\u8c31\u7f16\u7801\u5668\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u5168\u5c40\u5185\u8054\u6ce8\u610f\u529b\u5206\u652f\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHS-Mamba\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HS-Mamba\u901a\u8fc7\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u3002"}}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs.", "AI": {"tldr": "TECL\u662f\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528LLM\u53cd\u9988\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u5185\u5b9e\u73b0\u51c6\u786e\u7684\u6587\u672c\u805a\u7c7b\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u6587\u672c\u805a\u7c7b\u4e2d\u56e0\u5927\u91cf\u67e5\u8be2\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u8d22\u52a1\u5f00\u9500\u95ee\u9898\u3002", "method": "\u91c7\u7528EdgeLLM\u6216TriangleLLM\u6784\u5efa\u6587\u672c\u5bf9\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u7ea6\u675f\u805a\u7c7b\u751f\u6210\u805a\u7c7b\u7ed3\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cTECL\u5728\u76f8\u540c\u67e5\u8be2\u6210\u672c\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TECL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u6587\u672c\u805a\u7c7b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15619", "pdf": "https://arxiv.org/pdf/2504.15619", "abs": "https://arxiv.org/abs/2504.15619", "authors": ["Jinda Lu", "Jinghan Li", "Yuan Gao", "Junkang Wu", "Jiancan Wu", "Xiang Wang", "Xiangnan He"], "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods.", "AI": {"tldr": "AdaViP\u901a\u8fc7\u89c6\u89c9\u589e\u5f3a\u504f\u597d\u4f18\u5316\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u504f\u597d\uff0c\u663e\u8457\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u504f\u597d\uff0c\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faAdaViP\uff0c\u5305\u62ec\u89c6\u89c9\u504f\u597d\u5bf9\u6784\u5efa\u548c\u81ea\u9002\u5e94\u504f\u597d\u4f18\u5316\u3002", "result": "AdaViP-7B\u5728Object HalBench\u4e0a\u5206\u522b\u51cf\u5c1193.7%\u548c96.4%\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "AdaViP\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642", "abs": "https://arxiv.org/abs/2504.15642", "authors": ["Gerhard J\u00e4ger"], "title": "Computational Typology", "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8ba1\u7b97\u7edf\u8ba1\u6a21\u578b\u5728\u8bed\u8a00\u7c7b\u578b\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u4f18\u52bf\u3002", "motivation": "\u8bed\u8a00\u7c7b\u578b\u5b66\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u7279\u5f81\u5206\u7c7b\u8bed\u8a00\uff0c\u63ed\u793a\u8bed\u8a00\u7684\u5171\u6027\u4e0e\u591a\u6837\u6027\u3002\u8fd1\u5e74\u6765\uff0c\u8ba1\u7b97\u65b9\u6cd5\u7684\u5f15\u5165\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u7edf\u8ba1\u5efa\u6a21\u65b9\u6cd5\uff0c\u5206\u6790\u5927\u89c4\u6a21\u8bed\u8a00\u6570\u636e\uff0c\u9a8c\u8bc1\u8bed\u8a00\u7ed3\u6784\u4e0e\u6f14\u5316\u7684\u5047\u8bbe\u3002", "result": "\u8ba1\u7b97\u7edf\u8ba1\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u7c7b\u578b\u5b66\u7814\u7a76\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8ba1\u7b97\u65b9\u6cd5\u7684\u5f15\u5165\u4e3a\u8bed\u8a00\u7c7b\u578b\u5b66\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2504.15624", "pdf": "https://arxiv.org/pdf/2504.15624", "abs": "https://arxiv.org/abs/2504.15624", "authors": ["Jingzhi Li", "Changjiang Luo", "Ruoyu Chen", "Hua Zhang", "Wenqi Ren", "Jianhou Gan", "Xiaochun Cao"], "title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings.", "AI": {"tldr": "FaceInsight\u662f\u4e00\u79cd\u591a\u529f\u80fd\u9762\u90e8\u611f\u77e5MLLM\uff0c\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u548c\u8f85\u52a9\u611f\u77e5\u6a21\u6001\u63d0\u5347\u9762\u90e8\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u901a\u7528MLLMs\u5728\u9762\u90e8\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cFaceInsight\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u548c\u9762\u90e8\u5206\u5272\u56fe\uff0c\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u3002", "result": "\u5728\u4e09\u4e2a\u9762\u90e8\u611f\u77e5\u4efb\u52a1\u4e2d\uff0cFaceInsight\u4f18\u4e8e\u4e5d\u79cd\u5bf9\u6bd4MLLMs\u3002", "conclusion": "FaceInsight\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2504.15683", "pdf": "https://arxiv.org/pdf/2504.15683", "abs": "https://arxiv.org/abs/2504.15683", "authors": ["Simon Jehnen", "Joaqu\u00edn Ordieres-Mer\u00e9", "Javier Villalba-D\u00edez"], "title": "FinTextSim: Enhancing Financial Text Analysis with BERTopic", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.GN", "68T50", "I.2.7; I.5.1; J.4"], "comment": null, "summary": "Recent advancements in information availability and computational\ncapabilities have transformed the analysis of annual reports, integrating\ntraditional financial metrics with insights from textual data. To extract\nvaluable insights from this wealth of textual data, automated review processes,\nsuch as topic modeling, are crucial. This study examines the effectiveness of\nBERTopic, a state-of-the-art topic model relying on contextual embeddings, for\nanalyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies\n(2016-2022). Moreover, we introduce FinTextSim, a finetuned\nsentence-transformer model optimized for clustering and semantic search in\nfinancial contexts. Compared to all-MiniLM-L6-v2, the most widely used\nsentence-transformer, FinTextSim increases intratopic similarity by 81% and\nreduces intertopic similarity by 100%, significantly enhancing organizational\nclarity. We assess BERTopic's performance using embeddings from both FinTextSim\nand all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and\ndistinct economic topic clusters when paired with FinTextSim's embeddings.\nWithout FinTextSim, BERTopic struggles with misclassification and overlapping\ntopics. Thus, FinTextSim is pivotal for advancing financial text analysis.\nFinTextSim's enhanced contextual embeddings, tailored for the financial domain,\nelevate the quality of future research and financial information. This improved\nquality of financial information will enable stakeholders to gain a competitive\nadvantage, streamlining resource allocation and decision-making processes.\nMoreover, the improved insights have the potential to leverage business\nvaluation and stock price prediction models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86BERTopic\u4e0eFinTextSim\u7ed3\u5408\u5728\u91d1\u878d\u6587\u672c\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u805a\u7c7b\u7684\u6e05\u6670\u5ea6\u3002", "motivation": "\u4fe1\u606f\u53ef\u7528\u6027\u548c\u8ba1\u7b97\u80fd\u529b\u7684\u8fdb\u6b65\u4fc3\u4f7f\u91d1\u878d\u6587\u672c\u5206\u6790\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002", "method": "\u4f7f\u7528BERTopic\u548cFinTextSim\u5206\u6790S&P 500\u516c\u53f8\u768410-K\u6587\u4ef6\uff0c\u6bd4\u8f83\u5176\u6027\u80fd\u3002", "result": "FinTextSim\u663e\u8457\u63d0\u5347\u4e3b\u9898\u5185\u76f8\u4f3c\u6027\u5e76\u964d\u4f4e\u4e3b\u9898\u95f4\u76f8\u4f3c\u6027\uff0cBERTopic\u4ec5\u5728\u4e0eFinTextSim\u7ed3\u5408\u65f6\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FinTextSim\u5bf9\u91d1\u878d\u6587\u672c\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u548c\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2504.15627", "pdf": "https://arxiv.org/pdf/2504.15627", "abs": "https://arxiv.org/abs/2504.15627", "authors": ["Doanh C. Bui", "Hoai Luan Pham", "Vu Trung Duong Le", "Tuan Hai Vu", "Van Duy Tran", "Yasuhiko Nakashima"], "title": "ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 1 table, conference submission", "summary": "Lifelong learning for whole slide images (WSIs) poses the challenge of\ntraining a unified model to perform multiple WSI-related tasks, such as cancer\nsubtyping and tumor classification, in a distributed, continual fashion. This\nis a practical and applicable problem in clinics and hospitals, as WSIs are\nlarge, require storage, processing, and transfer time. Training new models\nwhenever new tasks are defined is time-consuming. Recent work has applied\nregularization- and rehearsal-based methods to this setting. However, the rise\nof vision-language foundation models that align diagnostic text with pathology\nimages raises the question: are these models alone sufficient for lifelong WSI\nlearning using zero-shot classification, or is further investigation into\ncontinual learning strategies needed to improve performance? To our knowledge,\nthis is the first study to compare conventional continual-learning approaches\nwith vision-language zero-shot classification for WSIs. Our source code and\nexperimental results will be available soon.", "AI": {"tldr": "\u6bd4\u8f83\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u548c\u89c6\u89c9\u8bed\u8a00\u96f6\u6837\u672c\u5206\u7c7b\u5728WSI\u7ec8\u8eab\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3WSI\u591a\u4efb\u52a1\u7ec8\u8eab\u5b66\u4e60\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u907f\u514d\u91cd\u590d\u8bad\u7ec3\u65b0\u6a21\u578b\u3002", "method": "\u6bd4\u8f83\u5e38\u89c4\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u4e0e\u89c6\u89c9\u8bed\u8a00\u96f6\u6837\u672c\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8db3\u591f\u6216\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "\u9996\u6b21\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff0c\u4e3aWSI\u7ec8\u8eab\u5b66\u4e60\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.15688", "pdf": "https://arxiv.org/pdf/2504.15688", "abs": "https://arxiv.org/abs/2504.15688", "authors": ["Mandy Cartner", "Matthew Kogan", "Nikolas Webster", "Matthew Wagers", "Ivy Sichel"], "title": "Subject islands do not reduce to construction-specific discourse function", "categories": ["cs.CL"], "comment": null, "summary": "The term islands in linguistics refers to phrases from which extracting an\nelement results in ungrammaticality (Ross, 1967). Grammatical subjects are\nconsidered islands because extracting a sub-part of a subject results in an\nill-formed sentence, despite having a clear intended meaning (e.g., \"Which\ntopic did the article about inspire you?\"). The generative tradition, which\nviews syntax as autonomous of meaning and function, attributes this\nungrammaticality to the abstract movement dependency between the wh-phrase and\nthe subject-internal position with which it is associated for interpretation.\nHowever, research on language that emphasizes its communicative function\nsuggests instead that syntactic constraints, including islands, can be\nexplained based on the way different constructions package information.\nAccordingly, Abeill\\'e et al. (2020) suggest that the islandhood of subjects is\nspecific to the information structure of wh-questions, and propose that\nsubjects are not islands for movement, but for focusing, due to their\ndiscourse-backgroundedness. This predicts that other constructions that differ\nin their information structure from wh-questions, but still involve movement,\nshould not create a subject island effect. We test this prediction in three\nlarge-scale acceptability studies, using a super-additive design that singles\nout subject island violations, in three different constructions: wh-questions,\nrelative clauses, and topicalization. We report evidence for a subject island\neffect in each construction type, despite only wh-questions introducing what\nAbeill\\'e et al. (2020) call \"a clash in information structure.\" We argue that\nthis motivates an account of islands in terms of abstract, syntactic\nrepresentations, independent of the communicative function associated with the\nconstructions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u5b66\u4e2d\u7684'\u5c9b\u5c7f\u6548\u5e94'\uff0c\u7279\u522b\u662f\u4e3b\u8bed\u4f5c\u4e3a\u5c9b\u5c7f\u7684\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u53e5\u6cd5\u7ed3\u6784\u4e2d\u4e3b\u8bed\u5c9b\u5c7f\u6548\u5e94\u7684\u666e\u904d\u6027\uff0c\u652f\u6301\u4e86\u53e5\u6cd5\u81ea\u4e3b\u6027\u7684\u89c2\u70b9\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u4e3b\u8bed\u5c9b\u5c7f\u6548\u5e94\u662f\u5426\u4ec5\u4e0e\u7279\u5b9a\u4fe1\u606f\u7ed3\u6784\uff08\u5982\u7591\u95ee\u53e5\uff09\u76f8\u5173\uff0c\u8fd8\u662f\u666e\u904d\u5b58\u5728\u4e8e\u4e0d\u540c\u53e5\u6cd5\u7ed3\u6784\u4e2d\uff0c\u4ee5\u63a2\u8ba8\u53e5\u6cd5\u662f\u5426\u72ec\u7acb\u4e8e\u4ea4\u9645\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5927\u89c4\u6a21\u53ef\u63a5\u53d7\u6027\u7814\u7a76\uff0c\u4f7f\u7528\u8d85\u52a0\u6027\u8bbe\u8ba1\uff0c\u5206\u522b\u5728\u7591\u95ee\u53e5\u3001\u5173\u7cfb\u4ece\u53e5\u548c\u8bdd\u9898\u5316\u7ed3\u6784\u4e2d\u6d4b\u8bd5\u4e3b\u8bed\u5c9b\u5c7f\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e3b\u8bed\u5c9b\u5c7f\u6548\u5e94\u5728\u6240\u6709\u6d4b\u8bd5\u7ed3\u6784\u4e2d\u5747\u5b58\u5728\uff0c\u800c\u4e0d\u4ec5\u9650\u4e8e\u7591\u95ee\u53e5\uff0c\u652f\u6301\u53e5\u6cd5\u81ea\u4e3b\u6027\u7684\u89c2\u70b9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4e3b\u8bed\u5c9b\u5c7f\u6548\u5e94\u4e0e\u62bd\u8c61\u53e5\u6cd5\u8868\u5f81\u76f8\u5173\uff0c\u72ec\u7acb\u4e8e\u4ea4\u9645\u529f\u80fd\uff0c\u652f\u6301\u751f\u6210\u8bed\u6cd5\u4f20\u7edf\u4e2d\u7684\u53e5\u6cd5\u81ea\u4e3b\u6027\u7406\u8bba\u3002"}}
{"id": "2504.15650", "pdf": "https://arxiv.org/pdf/2504.15650", "abs": "https://arxiv.org/abs/2504.15650", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Teli Ma", "Hengzhuang Li", "Yong liu", "Guang Dai", "Lei Zhang"], "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "categories": ["cs.CV"], "comment": "SAM Meets Affordance Grounding", "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions.", "AI": {"tldr": "AffordanceSAM\u901a\u8fc7\u6269\u5c55SAM\u7684\u5206\u5272\u80fd\u529b\u5230\u529f\u80fd\u533a\u57df\u8bc6\u522b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u7269\u4f53\u548c\u529f\u80fd\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u529f\u80fd\u533a\u57df\u8bc6\u522b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u9700\u6c42\u3002", "method": "\u63d0\u51faAffordanceSAM\uff0c\u5305\u542b\u529f\u80fd\u9002\u5e94\u6a21\u5757\u548c\u7531\u7c97\u5230\u7ec6\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316SAM\u7684\u5206\u5272\u8f93\u51fa\u4ee5\u9002\u5e94\u529f\u80fd\u533a\u57df\u8bc6\u522b\u3002", "result": "\u5728AGD20K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u5904\u7406\u65b0\u7269\u4f53\u548c\u529f\u80fd\u7684\u4efb\u52a1\u3002", "conclusion": "AffordanceSAM\u663e\u8457\u63d0\u5347\u4e86\u529f\u80fd\u533a\u57df\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2504.15777", "pdf": "https://arxiv.org/pdf/2504.15777", "abs": "https://arxiv.org/abs/2504.15777", "authors": ["Shangshang Wang", "Julian Asilis", "\u00d6mer Faruk Akg\u00fcl", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "title": "Tina: Tiny Reasoning Models via LoRA", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.", "AI": {"tldr": "Tina\u662f\u4e00\u79cd\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u66f4\u65b0\uff0c\u6027\u80fd\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u4e14\u6210\u672c\u6781\u4f4e\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u4f4e\u6210\u672c\u9ad8\u6548\u5730\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5bf91.5B\u53c2\u6570\u7684\u57fa\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u3002", "result": "Tina\u6a21\u578b\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u5ab2\u7f8e\u6216\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u6210\u672c\u4ec5\u4e3a9\u7f8e\u5143\uff0c\u6027\u80fd\u63d0\u534720%\uff0cPass@1\u51c6\u786e\u7387\u8fbe43.33%\u3002", "conclusion": "LoRA\u5728\u9ad8\u6548RL\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5feb\u901f\u9002\u5e94\u63a8\u7406\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u6a21\u578b\u77e5\u8bc6\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.15661", "pdf": "https://arxiv.org/pdf/2504.15661", "abs": "https://arxiv.org/abs/2504.15661", "authors": ["Xian Wu", "Chang Liu"], "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.", "AI": {"tldr": "DiTPainter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u81ea\u5b9a\u4e49Transformer\u7f51\u7edc\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u7cca\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4fee\u590d\u7b97\u6cd5\u4f9d\u8d56\u5149\u6d41\u4f20\u64ad\u50cf\u7d20\uff0c\u6613\u56e0\u5149\u6d41\u4e0d\u51c6\u786e\u6216\u5927\u906e\u6321\u533a\u57df\u5bfc\u81f4\u6a21\u7cca\u548c\u4e0d\u4e00\u81f4\uff1b\u800c\u9884\u8bad\u7ec3\u7684DiT\u6a21\u578b\u53c2\u6570\u91cf\u5927\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u4fee\u590d\u3002", "method": "\u63d0\u51faDiTPainter\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\uff0c\u57fa\u4e8eDiT\u4f46\u91c7\u7528\u4e13\u4e3a\u89c6\u9891\u4fee\u590d\u8bbe\u8ba1\u7684\u9ad8\u6548Transformer\u7f51\u7edc\uff0c\u4ece\u5934\u8bad\u7ec3\u800c\u975e\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "DiTPainter\u80fd\u5904\u7406\u4efb\u610f\u957f\u5ea6\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u53bb\u5b57\u5e55\u548c\u8865\u5168\u4efb\u52a1\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u8d28\u91cf\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiTPainter\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u548c\u4ece\u5934\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89c6\u9891\u4fee\u590d\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTTCW\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u6587\u672c\u7684\u521b\u9020\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u521b\u9020\u529b\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u6216\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e0d\u4e00\u81f4\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53c2\u8003\u7684Likert\u98ce\u683c\u65b9\u6cd5\uff0c\u5bf9\u751f\u6210\u6587\u672c\u4e0e\u9ad8\u8d28\u91cf\u53c2\u8003\u6587\u672c\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\uff0c\u914d\u5bf9\u51c6\u786e\u7387\u8fbe\u52300.75\uff08\u63d0\u534715%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30LLM\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15665", "pdf": "https://arxiv.org/pdf/2504.15665", "abs": "https://arxiv.org/abs/2504.15665", "authors": ["Pei Liu", "Yisi Luo", "Wenzhen Wang", "Xiangyong Cao"], "title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u589e\u5f3a\u548c\u975e\u5c40\u90e8\u76f8\u4f3c\u6027\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u80cc\u666f\u548c\u76ee\u6807\u4fe1\u53f7\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u7a00\u758f\u6a21\u578b\u96be\u4ee5\u6355\u6349\u52a8\u6001\u80cc\u666f\u548c\u5168\u5c40\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u80cc\u666f\u6cc4\u6f0f\u6216\u76ee\u6807\u4e22\u5931\u3002", "method": "\u7ed3\u5408\u5149\u6d41\u8fd0\u52a8\u4f30\u8ba1\u548c\u591a\u5e27\u878d\u5408\u589e\u5f3a\u8fd0\u52a8\u663e\u8457\u6027\uff0c\u5229\u7528\u975e\u5c40\u90e8\u76f8\u4f3c\u6027\u6784\u5efa\u4f4e\u79e9\u5757\u5f20\u91cf\uff0c\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5206\u79bb\u5f31\u5c0f\u76ee\u6807\u4e0e\u590d\u6742\u80cc\u666f\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u80cc\u666f\u548c\u5f31\u76ee\u6807\u4fe1\u53f7\u7684\u6311\u6218\u3002"}}
{"id": "2504.15801", "pdf": "https://arxiv.org/pdf/2504.15801", "abs": "https://arxiv.org/abs/2504.15801", "authors": ["Valeria Lerman", "Yaniv Dover"], "title": "A closer look at how large language models trust humans: patterns and biases", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5982\u4f55\u57fa\u4e8e\u4eba\u7c7b\u7684\u80fd\u529b\u3001\u5584\u610f\u548c\u8bda\u4fe1\u4e09\u4e2a\u7ef4\u5ea6\u5efa\u7acb\u4fe1\u4efb\uff0c\u5e76\u53d1\u73b0\u5176\u4fe1\u4efb\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u4e5f\u5b58\u5728\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u3002", "motivation": "\u968f\u7740LLM\u5728\u51b3\u7b56\u4e2d\u4e0e\u4eba\u7c7b\u4e92\u52a8\u589e\u591a\uff0c\u7406\u89e3AI\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u7684\u52a8\u6001\u6210\u4e3a\u5173\u952e\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4eba\u7c7b\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u800cAI\u5bf9\u4eba\u7c7b\u7684\u4fe1\u4efb\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u57fa\u4e8e\u884c\u4e3a\u7406\u8bba\uff0c\u7814\u7a76\u4e86LLM\u662f\u5426\u4f9d\u8d56\u80fd\u529b\u3001\u5584\u610f\u548c\u8bda\u4fe1\u4e09\u4e2a\u4fe1\u4efb\u7ef4\u5ea6\uff0c\u5e76\u63a2\u8ba8\u4eba\u53e3\u7edf\u8ba1\u53d8\u91cf\u5bf9\u4fe1\u4efb\u7684\u5f71\u54cd\u3002\u901a\u8fc743,200\u6b21\u6a21\u62df\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4e94\u79cd\u6d41\u884c\u6a21\u578b\u5728\u4e94\u79cd\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLM\u7684\u4fe1\u4efb\u5f62\u6210\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53d7\u5e74\u9f84\u3001\u5b97\u6559\u548c\u6027\u522b\u504f\u89c1\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u91d1\u878d\u573a\u666f\u4e2d\u3002\u4e0d\u540c\u6a21\u578b\u7684\u4fe1\u4efb\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7406\u89e3AI\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u7684\u52a8\u6001\uff0c\u5e76\u76d1\u63a7\u504f\u89c1\u548c\u4fe1\u4efb\u6a21\u5f0f\uff0c\u4ee5\u907f\u514d\u5728\u4fe1\u4efb\u654f\u611f\u5e94\u7528\u4e2d\u4ea7\u751f\u6f5c\u5728\u5371\u5bb3\u3002"}}
{"id": "2504.15669", "pdf": "https://arxiv.org/pdf/2504.15669", "abs": "https://arxiv.org/abs/2504.15669", "authors": ["Wei Zhuo", "Zhiyue Tang", "Wufeng Xue", "Hao Ding", "Linlin Shen"], "title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFS-DINO\uff0c\u4e00\u79cd\u7ed3\u5408DINOv2\u548cSAM\u77e5\u8bc6\u7684\u5c0f\u6837\u672c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u5272\u5668\u548c\u8de8\u6a21\u578b\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u7edf\u4e00\u5229\u7528DINOv2\u548cSAM\u7684\u4e92\u8865\u80fd\u529b\u3002", "method": "\u63d0\u51faFS-DINO\uff0c\u4ec5\u4f7f\u7528DINOv2\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u5206\u5272\u5668\uff0c\u901a\u8fc7\u74f6\u9888\u9002\u914d\u5668\u3001\u5143\u89c6\u89c9\u63d0\u793a\u751f\u6210\u5668\u548c\u89e3\u7801\u5668\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408SAM\u77e5\u8bc6\u8fdb\u884c\u8de8\u6a21\u578b\u84b8\u998f\u3002", "result": "\u5728COCO-20i\u3001PASCAL-5i\u548cFSS-1000\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "FS-DINO\u6210\u529f\u6574\u5408\u4e86\u4e24\u79cd\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u4e3a\u5c0f\u6837\u672c\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815", "abs": "https://arxiv.org/abs/2504.15815", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Barbara Plank"], "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpotlight\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u5206\u6790\uff0c\u901a\u8fc7\u6570\u636e\u6316\u6398\u6280\u672f\u533a\u5206\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u968f\u673a\u53d8\u5316\u4e0e\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e2e\u52a9\u7528\u6237\u9ad8\u6548\u5206\u6790\u63d0\u793a\u548c\u6a21\u578b\u53d8\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u81ea\u52a8\u5316\u6307\u6807\u6216\u4eba\u5de5\u8bc4\u4f30\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u6216\u9ad8\u6548\u5730\u5206\u6790\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "method": "\u57fa\u4e8e\u6570\u636e\u6316\u6398\u6280\u672f\uff0c\u81ea\u52a8\u63d0\u53d6\u63cf\u8ff0\u7cfb\u7edf\u6027\u5dee\u5f02\u7684\u6807\u8bb0\u6a21\u5f0f\uff0c\u5e76\u6307\u5bfc\u7528\u6237\u624b\u52a8\u5206\u6790\u63d0\u793a\u548c\u6a21\u578b\u53d8\u5316\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6807\u8bb0\u6a21\u5f0f\u63d0\u53d6\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9\u73b0\u6709\u63d0\u793a\u6570\u636e\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "Spotlight\u65b9\u6cd5\u4ece\u4eba\u672c\u89c6\u89d2\u51fa\u53d1\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u652f\u6301\u63d0\u793a\u5de5\u7a0b\u548c\u4eba\u672c\u6a21\u578b\u884c\u4e3a\u7814\u7a76\u3002"}}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Vidi\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u7406\u89e3\u548c\u7f16\u8f91\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5bb6\u65cf\uff0c\u4e13\u6ce8\u4e8e\u65f6\u95f4\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u5728\u65b0\u57fa\u51c6VUE-TR\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89c6\u9891\u6210\u4e3a\u4e92\u8054\u7f51\u4e3b\u8981\u4f20\u64ad\u5a92\u4ecb\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001\u548c\u957f\u89c6\u9891\u8f93\u5165\uff0c\u9700\u652f\u6301\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\u3002", "method": "\u63d0\u51faVidi\u6a21\u578b\u5bb6\u65cf\uff0c\u9996\u7248\u4e13\u6ce8\u4e8e\u65f6\u95f4\u68c0\u7d22\u4efb\u52a1\uff0c\u80fd\u5904\u7406\u5c0f\u65f6\u7ea7\u89c6\u9891\u5e76\u5177\u5907\u5f3a\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002", "result": "Vidi\u5728\u65f6\u95f4\u68c0\u7d22\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\u548cGemini\u7b49\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "Vidi\u5728\u89c6\u9891\u7f16\u8f91\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65b0\u57fa\u51c6VUE-TR\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2504.15843", "pdf": "https://arxiv.org/pdf/2504.15843", "abs": "https://arxiv.org/abs/2504.15843", "authors": ["Junshu Pan", "Wei Shen", "Shulin Huang", "Qiji Zhou", "Yue Zhang"], "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.", "AI": {"tldr": "Pre-DPO\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6307\u5bfc\u6027\u53c2\u8003\u6a21\u578b\u63d0\u5347DPO\u548cSimPO\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DPO\u548cSimPO\u65b9\u6cd5\u5728\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPre-DPO\uff0c\u5229\u7528\u53c2\u8003\u6a21\u578b\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\uff0c\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728AlpacaEval 2.0\u548cArena-Hard v0.1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eDPO\u548cSimPO\u3002", "conclusion": "Pre-DPO\u6709\u6548\u63d0\u5347\u4e86\u504f\u597d\u4f18\u5316\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u5916\u90e8\u6a21\u578b\u3002"}}
{"id": "2504.15694", "pdf": "https://arxiv.org/pdf/2504.15694", "abs": "https://arxiv.org/abs/2504.15694", "authors": ["Jun Dong", "Wenli Wu", "Jintao Cheng", "Xiaoyu Tang"], "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u5b9e\u65f6\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u6846\u67b6YSOOB\uff0c\u901a\u8fc7\u591a\u9891\u8c31\u5c0f\u6ce2\u7f16\u7801\u548c\u52a8\u6001\u4fe1\u606f\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d28\u91cf\u6c34\u4e0b\u56fe\u50cf\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u4f4e\u56fe\u50cf\u8d28\u91cf\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u5bf9\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u9891\u8c31\u5c0f\u6ce2\u7f16\u7801\uff08MSWE\uff09\u51cf\u5c11\u5149\u5b66\u989c\u8272\u5931\u771f\uff0c\u52a8\u6001\u9009\u62e9\u5173\u952e\u4fe1\u606f\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u538b\u7f29\u548c\u91cd\u6784\u5927\u6838\u5377\u79ef\uff08RLKC\uff09\u5b9e\u73b0\u6a21\u578b\u8f7b\u91cf\u5316\u3002", "result": "YSOOB\u4ec5120\u4e07\u53c2\u6570\uff0c\u5728URPC2020\u548cDUO\u6570\u636e\u96c6\u4e0amAP50\u5206\u522b\u8fbe\u523083.1%\u548c82.9%\uff0c\u63a8\u7406\u901f\u5ea6\u663e\u8457\u4f18\u4e8eYOLOv12-N\u3002", "conclusion": "YSOOB\u5728\u8f7b\u91cf\u5316\u548c\u9ad8\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2504.15848", "pdf": "https://arxiv.org/pdf/2504.15848", "abs": "https://arxiv.org/abs/2504.15848", "authors": ["Luwei Xiao", "Rui Mao", "Shuai Zhao", "Qika Lin", "Yanhao Jia", "Liang He", "Erik Cambria"], "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "Accepted by TAFFC 2025", "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChimera\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u4ee5\u53ca\u60c5\u611f\u8ba4\u77e5\u5171\u632f\uff0c\u63d0\u5347\u4e86\u5bf9\u7279\u5b9a\u76ee\u6807\u7684\u60c5\u611f\u6781\u6027\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u591a\u6a21\u6001\u5185\u5bb9\u7684\u589e\u52a0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7406\u89e3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5185\u5bb9\u548c\u60c5\u611f\u8ba4\u77e5\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002", "method": "Chimera\u6846\u67b6\u6574\u5408\u4e86\u89c6\u89c9\u5757\u7279\u5f81\u3001\u7c97\u7c92\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u540c\u65f6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u60c5\u611f\u539f\u56e0\u548c\u5370\u8c61\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6bd4GPT-4o\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u7075\u6d3b\u3002", "conclusion": "Chimera\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u60c5\u611f\u8ba4\u77e5\u5171\u632f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86MSCOCO\u6807\u7b7e\u9519\u8bef\u5bf9POPE\u57fa\u51c6\u7684\u5f71\u54cd\uff0c\u91cd\u65b0\u6807\u6ce8\u540e\u53d1\u73b0\u6a21\u578b\u6392\u540d\u663e\u8457\u53d8\u5316\u3002", "motivation": "\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u53ef\u80fd\u5305\u542b\u6807\u7b7e\u9519\u8bef\uff0c\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u3002", "method": "\u91cd\u65b0\u6807\u6ce8POPE\u57fa\u51c6\u56fe\u50cf\uff0c\u5206\u6790\u6807\u7b7e\u9519\u8bef\u5206\u5e03\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4fee\u6b63\u540e\u7684RePOPE\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6807\u7b7e\u9519\u8bef\u5206\u5e03\u4e0d\u5747\uff0c\u6a21\u578b\u6392\u540d\u56e0\u6807\u7b7e\u8d28\u91cf\u53d8\u5316\u800c\u663e\u8457\u6539\u53d8\u3002", "conclusion": "\u6807\u7b7e\u8d28\u91cf\u5bf9\u57fa\u51c6\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0cRePOPE\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u622a\u65ad\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9000\u51fa\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u957f\u94fe\u5f0f\u601d\u7ef4\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u51c6\u786e\u6027\u635f\u5931\uff0c\u9700\u8981\u4e00\u79cd\u52a8\u6001\u622a\u65ad\u65b9\u6cd5\u3002", "method": "\u76d1\u6d4b\u6a21\u578b\u884c\u4e3a\uff0c\u5728\u63a8\u7406\u8fc7\u6e21\u70b9\u52a8\u6001\u7ec8\u6b62\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoT\u5e8f\u5217\u957f\u5ea6\u51cf\u5c1131%-43%\uff0c\u51c6\u786e\u6027\u63d0\u53471.7%-5.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u63a8\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.15723", "pdf": "https://arxiv.org/pdf/2504.15723", "abs": "https://arxiv.org/abs/2504.15723", "authors": ["Dasol Jeong", "Donggoo Kang", "Jiwon Park", "Hyebean Lee", "Joonki Paik"], "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u96f6\u6837\u672c\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u5f15\u5bfc\u548c\u53c2\u8003\u5f15\u5bfc\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u7edf\u4e00\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u56fe\u50cf\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "method": "\u5229\u7528\u6269\u6563\u53cd\u6f14\u548c\u65f6\u95f4\u6b65\u7279\u5b9a\u7684\u7a7a\u6587\u672c\u5d4c\u5165\uff0c\u7ed3\u5408\u5206\u9636\u6bb5\u6f5c\u5728\u6ce8\u5165\u7b56\u7565\uff08\u65e9\u671f\u5f62\u72b6\u6ce8\u5165\uff0c\u540e\u671f\u5c5e\u6027\u6ce8\u5165\uff09\uff0c\u5e76\u901a\u8fc7\u53c2\u8003\u6f5c\u5728\u8fdb\u884c\u8de8\u6ce8\u610f\u529b\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u8868\u60c5\u8f6c\u79fb\u3001\u7eb9\u7406\u53d8\u6362\u548c\u98ce\u683c\u878d\u5408\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u56fe\u50cf\u7f16\u8f91\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900", "abs": "https://arxiv.org/abs/2504.15900", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63d0\u5347\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u97f3\u9891\u63a8\u7406\u6a21\u578bSARI\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u63d0\u5347\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u8bfe\u7a0b\u5f15\u5bfc\u7684GRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u6bd4\u8f83\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u63a8\u7406\u3002", "result": "SARI\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534716.35%\uff0cQwen2.5-Omni\u53d8\u4f53\u8fbe\u523067.08%\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8bfe\u7a0b\u5b66\u4e60\u663e\u8457\u589e\u5f3a\u97f3\u9891\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2504.15728", "pdf": "https://arxiv.org/pdf/2504.15728", "abs": "https://arxiv.org/abs/2504.15728", "authors": ["Manjunath D", "Aniruddh Sikdar", "Prajwal Gurunath", "Sumanth Udupa", "Suresh Sundaram"], "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W PBVS 2025", "summary": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGA\u7684\u65b0\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3RGB\u5230IR\u56fe\u50cf\u57df\u9002\u5e94\u4e2d\u7684\u989c\u8272\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6IndraEye\u3002\u5b9e\u9a8c\u8868\u660eSAGA\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eIR\u56fe\u50cf\u7f3a\u4e4f\u989c\u8272\u548c\u7eb9\u7406\u4fe1\u606f\uff0cRGB\u8bad\u7ec3\u7684\u6a21\u578b\u5728IR\u57df\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u9ad8\u8bef\u68c0\u7387\u548c\u4f4e\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "method": "\u63d0\u51faSemantic-Aware Gray color Augmentation (SAGA)\uff0c\u901a\u8fc7\u63d0\u53d6\u4e0eIR\u56fe\u50cf\u76f8\u5173\u7684\u5bf9\u8c61\u7ea7\u7279\u5f81\u6765\u51cf\u5c11\u989c\u8272\u504f\u5dee\u3002\u540c\u65f6\uff0c\u53d1\u5e03IndraEye\u6570\u636e\u96c6\uff0c\u5305\u542b5,612\u5f20RGB-IR\u56fe\u50cf\u3002", "result": "SAGA\u5728RGB\u5230IR\u7684\u57df\u9002\u5e94\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u53470.4%\u81f37.6%\uff08mAP\uff09\u3002", "conclusion": "SAGA\u548cIndraEye\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u548c\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u5728\u65e0\u4eba\u673a\u56fe\u50cf\u5904\u7406\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "C\u00e9cile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.", "AI": {"tldr": "FairTranslate\u662f\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u82f1\u8bed\u5230\u6cd5\u8bed\u7ffb\u8bd1\u4e2d\u5904\u7406\u975e\u4e8c\u5143\u6027\u522b\u504f\u89c1\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u504f\u89c1\u3002", "motivation": "LLM\u5728\u7ffb\u8bd1\u5305\u5bb9\u6027\u8bed\u8a00\uff08\u5982\u5355\u6570'they'\u4ee3\u8bcd\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u6027\u522b\u504f\u89c1\u3002", "method": "\u521b\u5efaFairTranslate\u6570\u636e\u96c6\uff082418\u53e5\u5bf9\uff09\uff0c\u8bc4\u4f30\u56db\u79cdLLM\u5728\u4e0d\u540c\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u6027\u522b\u8868\u793a\u4e0a\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u516c\u5e73\u7ffb\u8bd1\u3002", "conclusion": "\u9700\u9488\u5bf9\u6027\u7b56\u7565\u786e\u4fddLLM\u7ffb\u8bd1\u7684\u5305\u5bb9\u6027\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.15751", "pdf": "https://arxiv.org/pdf/2504.15751", "abs": "https://arxiv.org/abs/2504.15751", "authors": ["Menan Velayuthan", "Asiri Gawesha", "Purushoth Velayuthan", "Nuwan Kodagoda", "Dharshana Kasthurirathna", "Pradeepa Samarasinghe"], "title": "GADS: A Super Lightweight Model for Head Pose Estimation", "categories": ["cs.CV"], "comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal", "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGADS\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u7ec4\u5730\u6807\u548c\u4f7f\u7528\u5c0f\u578bDeep Set\u5c42\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u5e76\u63d0\u9ad8\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5730\u6807\u7684\u65b9\u6cd5\u8fc7\u4e8e\u6ce8\u91cd\u7cbe\u5ea6\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u7b80\u5355\u6027\uff0c\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528Grouped Attention Deep Sets (GADS)\u67b6\u6784\uff0c\u5206\u7ec4\u5730\u6807\u5e76\u4f7f\u7528\u591a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u7ec4\u95f4\u4fe1\u606f\u3002", "result": "\u6a21\u578b\u6bd4\u5f53\u524d\u6700\u8f7b\u7684SOTA\u6a21\u578b\u5c0f7.5\u500d\uff0c\u901f\u5ea6\u5feb25\u500d\uff0c\u6bd4\u6027\u80fd\u6700\u4f73\u6a21\u578b\u5c0f4321\u500d\u3002", "conclusion": "GADS\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5934\u59ff\u4f30\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aW-PCA\u7684\u65b0\u578b\u96f6\u6837\u672c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\uff0c\u901a\u8fc7\u53c2\u6570\u8ba1\u6570\u548c\u4e3b\u6210\u5206\u5206\u6790\u4f18\u5316\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u5347\u4e86\u6d4b\u8bd5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cNAS\u65b9\u6cd5\u5b58\u5728\u8bc4\u4f30\u6307\u6807\u504f\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6743\u91cd\u52a0\u6743PCA\uff08W-PCA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u8ba1\u6570\u548cFFN\u5c42\u4e3b\u6210\u5206\u5206\u6790\u4f5c\u4e3a\u8bc4\u4f30\u4ee3\u7406\uff0c\u907f\u514d\u68af\u5ea6\u8ba1\u7b97\u4ee5\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728GLUE\u548cSQuAD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u6d4b\u8bd5\u5206\u6570\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5728FlexiBERT\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u6392\u540d\u8bc4\u4f30\u4e5f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "W-PCA\u65b9\u6cd5\u5728\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.15756", "pdf": "https://arxiv.org/pdf/2504.15756", "abs": "https://arxiv.org/abs/2504.15756", "authors": ["Qirui Yang", "Fangpu Zhang", "Yeying Jin", "Qihua Cheng", "Pengtao Jiang", "Huanjing Yue", "Jingyu Yang"], "title": "DSDNet: Raw Domain Demoir\u00e9ing via Dual Color-Space Synergy", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u9636\u6bb5\u539f\u59cb\u57df\u53bb\u6469\u5c14\u7eb9\u6846\u67b6DSDNet\uff0c\u901a\u8fc7\u53cc\u6d41\u7f51\u7edc\u548c\u52a8\u6001\u8c03\u5236\u6a21\u5757\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u62cd\u6444\u5c4f\u5e55\u65f6\u6469\u5c14\u7eb9\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u635f\u5931\u6216\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u53cc\u6d41\u7f51\u7edc\u548c\u52a8\u6001\u8c03\u5236\u6a21\u5757\uff0c\u7ed3\u5408\u539f\u59cb\u57df\u4e0eYCbCr\u56fe\u50cf\uff0c\u63d0\u5347\u53bb\u6469\u5c14\u7eb9\u6548\u679c\u3002", "result": "DSDNet\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u5feb2.4\u500d\u3002", "conclusion": "DSDNet\u5728\u53bb\u6469\u5c14\u7eb9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u4e0e\u9ad8\u8d28\u91cf\u3002"}}
{"id": "2504.15987", "pdf": "https://arxiv.org/pdf/2504.15987", "abs": "https://arxiv.org/abs/2504.15987", "authors": ["Zhenkai Qin", "Dongze Wu", "Yuxin Liu", "Guifang Yang"], "title": "Few-shot Hate Speech Detection Based on the MindSpore Framework", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMS-FSLHate\u7684\u5c11\u6837\u672c\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u63d0\u793a\u5d4c\u5165\u3001CNN-BiLSTM\u67b6\u6784\u548c\u5bf9\u6297\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u6cdb\u6ee5\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c11\u6837\u672c\u6216\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u63d0\u793a\u5d4c\u5165\u3001CNN-BiLSTM\u67b6\u6784\u4e0e\u6ce8\u610f\u529b\u6c60\u5316\uff0c\u7ed3\u5408\u540c\u4e49\u8bcd\u5bf9\u6297\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5728HateXplain\u548cHSOL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u63d0\u793a\u5b66\u4e60\u4e0e\u5bf9\u6297\u589e\u5f3a\u7ed3\u5408\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u4e3a\u5c11\u6837\u672c\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15770", "pdf": "https://arxiv.org/pdf/2504.15770", "abs": "https://arxiv.org/abs/2504.15770", "authors": ["Lei Xu", "Mehmet Yamac", "Mete Ahishali", "Moncef Gabbouj"], "title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMTS-DR\u6a21\u5757\u7684\u795e\u7ecf\u7f51\u7edcMTS-DR-Net\uff0c\u7528\u4e8e\u8fb9\u7f18\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\u5e76\u805a\u7126\u76f8\u5173\u5b50\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8fb9\u7f18\u68c0\u6d4b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u611f\u53d7\u91ce\uff0c\u5bfc\u81f4\u7f51\u7edc\u7ed3\u6784\u8fc7\u6df1\u3002MTS\u56e0\u5b50\u5316\u64cd\u4f5c\u867d\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u63d0\u51faMTS-DR\u6a21\u5757\u4f5c\u4e3a\u65b0\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408MTS\u5c42\u548cMTS-DR\u5757\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\uff0c\u5e76\u5f15\u5165U\u5f62\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728BSDS500\u548cBIPEDv2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MTS-DR-Net\u7684\u6709\u6548\u6027\u3002", "conclusion": "MTS-DR-Net\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Hei\u00df", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.", "AI": {"tldr": "CAPO\u662f\u4e00\u79cd\u57fa\u4e8eAutoML\u6280\u672f\u7684\u6210\u672c\u611f\u77e5\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u65b9\u6cd5\u548c\u591a\u76ee\u6807\u4f18\u5316\u63d0\u9ad8\u6548\u7387\uff0c\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u548c\u63d0\u793a\u957f\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5927\u91cfLLM\u8c03\u7528\u548c\u8f93\u5165\u6807\u8bb0\uff0c\u6210\u672c\u9ad8\u6602\uff0cCAPO\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CAPO\u7ed3\u5408\u8fdb\u5316\u65b9\u6cd5\uff08\u4ee5LLM\u4e3a\u64cd\u4f5c\u7b26\uff09\u3001racing\u8282\u7701\u8bc4\u4f30\u548c\u591a\u76ee\u6807\u4f18\u5316\uff08\u5e73\u8861\u6027\u80fd\u4e0e\u63d0\u793a\u957f\u5ea6\uff09\uff0c\u540c\u65f6\u4f18\u5316\u6307\u4ee4\u548c\u5c11\u6837\u672c\u793a\u4f8b\u3002", "result": "\u572811/15\u6848\u4f8b\u4e2d\uff0cCAPO\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534721%\uff0c\u4e14\u5728\u8f83\u5c0f\u9884\u7b97\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u63d0\u793a\u957f\u5ea6\u66f4\u77ed\u3002", "conclusion": "CAPO\u901a\u8fc7\u63d0\u9ad8\u6210\u672c\u6548\u7387\uff0c\u4f7f\u63d0\u793a\u4f18\u5316\u66f4\u5f3a\u5927\u4e14\u6613\u4e8e\u4f7f\u7528\uff0c\u662f\u8fc8\u5411\u9ad8\u6548\u63d0\u793a\u4f18\u5316\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.15776", "pdf": "https://arxiv.org/pdf/2504.15776", "abs": "https://arxiv.org/abs/2504.15776", "authors": ["Quentin Herau", "Nathan Piasco", "Moussab Bennehar", "Luis Rolado", "Dzmitry Tsishkou", "Bingbing Liu", "Cyrille Migniot", "Pascal Vasseur", "C\u00e9dric Demonceaux"], "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "categories": ["cs.CV", "cs.RO"], "comment": "under review", "summary": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u4f20\u611f\u5668\u4f4d\u59ff\u548c\u6821\u51c6\u53c2\u6570\uff0c\u63d0\u5347\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u516c\u5171\u6570\u636e\u96c6\u4e2d\u4f20\u611f\u5668\u6821\u51c6\u548c\u8f66\u8f86\u4f4d\u59ff\u7684\u4e0d\u51c6\u786e\u6027\u53ef\u80fd\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u8bc4\u4f30\u9519\u8bef\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528NeRF\u8fdb\u884c\u4f20\u611f\u5668\u4f4d\u59ff\u548c\u6821\u51c6\u53c2\u6570\u7684\u9c81\u68d2\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u91cd\u6295\u5f71\u6307\u6807\u3001\u65b0\u89c6\u89d2\u5408\u6210\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u5bf9\u9f50\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f20\u611f\u5668\u4f4d\u59ff\u7684\u51c6\u786e\u6027\uff0c\u63d0\u5347\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u4f20\u611f\u5668\u4f4d\u59ff\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u53ef\u9760\u6027\u53d1\u5c55\u3002"}}
{"id": "2504.16007", "pdf": "https://arxiv.org/pdf/2504.16007", "abs": "https://arxiv.org/abs/2504.16007", "authors": ["Igor Rozhkov", "Natalia Loukachevitch"], "title": "Methods for Recognizing Nested Terms", "categories": ["cs.CL"], "comment": "To be published in Computational Linguistics and Intellectual\n  Technologies proceedings", "summary": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728RuTermEval\u7ade\u8d5b\u4e2d\u5e94\u7528Binder\u6a21\u578b\u63d0\u53d6\u5d4c\u5957\u672f\u8bed\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u4f73\u6210\u7ee9\uff0c\u5e76\u7814\u7a76\u4e86\u4ece\u975e\u5d4c\u5957\u6807\u6ce8\u6570\u636e\u4e2d\u8bc6\u522b\u5d4c\u5957\u672f\u8bed\u7684\u65b0\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u6709\u6548\u63d0\u53d6\u5d4c\u5957\u672f\u8bed\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5d4c\u5957\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5e94\u7528Binder\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u66fe\u6210\u529f\u7528\u4e8e\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u5728RuTermEval\u7ade\u8d5b\u7684\u4e09\u4e2a\u8d5b\u9053\u4e2d\u5747\u83b7\u5f97\u6700\u4f73\u672f\u8bed\u8bc6\u522b\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65e0\u9700\u5d4c\u5957\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u63d0\u53d6\u5d4c\u5957\u672f\u8bed\u3002"}}
{"id": "2504.16046", "pdf": "https://arxiv.org/pdf/2504.16046", "abs": "https://arxiv.org/abs/2504.16046", "authors": ["Jingyu Zhang", "Jiacan Yu", "Marc Marone", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement", "categories": ["cs.CL"], "comment": null, "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBloomScrub\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u8bad\u7ec3\u4e2d\u63a5\u89e6\u7248\u6743\u6750\u6599\u540e\u53ef\u80fd\u5f15\u53d1\u7684\u4fb5\u6743\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5f15\u7528\u68c0\u6d4b\u548c\u91cd\u5199\u6280\u672f\uff0c\u63d0\u4f9b\u53ef\u8ba4\u8bc1\u7684\u7248\u6743\u4fdd\u62a4\u3002", "motivation": "\u9884\u8bad\u7ec3\u4e2dLLMs\u63a5\u89e6\u7248\u6743\u6750\u6599\u53ef\u80fd\u5f15\u53d1\u90e8\u7f72\u540e\u7684\u4fb5\u6743\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6781\u7aef\u98ce\u9669\uff08\u5982\u957f\u6bb5\u9010\u5b57\u5f15\u7528\uff09\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faBloomScrub\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f15\u7528\u68c0\u6d4b\u548c\u91cd\u5199\u6280\u672f\uff0c\u5229\u7528Bloom\u8fc7\u6ee4\u5668\u5b9e\u73b0\u9ad8\u6548\u7248\u6743\u7b5b\u67e5\uff0c\u5e76\u5728\u5fc5\u8981\u65f6\u9009\u62e9\u4e0d\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBloomScrub\u80fd\u663e\u8457\u964d\u4f4e\u4fb5\u6743\u98ce\u9669\uff0c\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u4e25\u683c\u7a0b\u5ea6\u7684\u6267\u884c\u8981\u6c42\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\u5728\u7248\u6743\u9884\u9632\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2504.15783", "pdf": "https://arxiv.org/pdf/2504.15783", "abs": "https://arxiv.org/abs/2504.15783", "authors": ["Johan \u00d6fverstedt", "Elin Lundstr\u00f6m", "H\u00e5kan Ahlstr\u00f6m", "Joel Kullberg"], "title": "Towards prediction of morphological heart age from computed tomography angiography", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7CTA\u56fe\u50cf\u9884\u6d4b\u5e74\u9f84\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u914d\u51c6\u548c\u673a\u5668\u5b66\u4e60\u7684\u5f62\u6001\u5b66\u5fc3\u810f\u5e74\u9f84\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u5206\u6790\u4e86\u5fc3\u810f\u5f62\u6001\u4e0e\u8870\u8001\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u5fc3\u810f\u5f62\u6001\u4e0e\u8870\u8001\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5f62\u6001\u5b66\u5fc3\u810f\u5e74\u9f84\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u6807\u51c6\u5316\u56fe\u50cf\uff0c\u63d0\u53d6\u5bc6\u5ea6\u548c\u5c40\u90e8\u4f53\u79ef\u7684\u7a33\u5065\u7279\u5f81\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u5e74\u9f84\u3002", "result": "\u5728SCAPIS\u6570\u636e\u96c6\u4e2d\uff0c\u5973\u6027\u548c\u7537\u6027\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a2.74\u548c2.77\u5e74\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u5f62\u6001\u5b66\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u5f62\u6001\u5b66\u5fc3\u810f\u5e74\u9f84\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e14\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u901a\u8fc7\u663e\u8457\u6027\u5206\u6790\u5f97\u5230\u63d0\u5347\u3002"}}
{"id": "2504.16053", "pdf": "https://arxiv.org/pdf/2504.16053", "abs": "https://arxiv.org/abs/2504.16053", "authors": ["Zhifan Ye", "Kejing Xia", "Yonggan Fu", "Xin Dong", "Jihoon Hong", "Xiangchi Yuan", "Shizhe Diao", "Jan Kautz", "Pavlo Molchanov", "Yingyan Celine Lin"], "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba.", "AI": {"tldr": "LongMamba\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5206\u7c7b\u548c\u8fc7\u6ee4\u5173\u952etoken\uff0c\u663e\u8457\u63d0\u5347Mamba\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1SSMs\uff08\u5982Mamba\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u6548\u7387\u9ad8\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u4e0d\u53caTransformer\u3002LongMamba\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c06Mamba\u7684\u9690\u85cf\u901a\u9053\u5206\u4e3a\u5c40\u90e8\u548c\u5168\u5c40\u901a\u9053\uff0c\u5e76\u8fc7\u6ee4\u5168\u5c40\u901a\u9053\u4e2d\u7684\u975e\u5173\u952etoken\uff0c\u9632\u6b62\u5185\u5b58\u8870\u51cf\u3002", "result": "LongMamba\u663e\u8457\u63d0\u5347\u4e86Mamba\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "LongMamba\u4e3aMamba\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2504.15786", "pdf": "https://arxiv.org/pdf/2504.15786", "abs": "https://arxiv.org/abs/2504.15786", "authors": ["Ningli Xu", "Rongjun Qin"], "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "categories": ["cs.CV"], "comment": "8 figures", "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4e24\u4e2a\u6761\u4ef6\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u536b\u661f\u56fe\u50cf\u751f\u6210\u5730\u9762\u89c6\u56fe\u65f6\u7684\u89c6\u89d2\u548c\u5206\u8fa8\u7387\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u4e0e\u5730\u9762\u89c6\u56fe\u4e4b\u95f4\u5b58\u5728\u89c6\u89d2\u548c\u5206\u8fa8\u7387\u5dee\u5f02\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5730\u9762\u89c6\u56fe\u5728\u76f8\u90bb\u533a\u57df\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u89c6\u56fe\u751f\u6210\u3002", "method": "\u57fa\u4e8e\u56fa\u5b9a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u536b\u661f\u5f15\u5bfc\u53bb\u566a\u548c\u536b\u661f-\u65f6\u95f4\u53bb\u566a\u4e24\u4e2a\u6761\u4ef6\u6a21\u5757\uff0c\u5206\u522b\u63d0\u53d6\u573a\u666f\u5e03\u5c40\u548c\u76f8\u673a\u8fd0\u52a8\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u548c\u65f6\u95f4\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u591a\u89c6\u56fe\u8f93\u51fa\u5177\u6709\u9ad8\u771f\u5b9e\u611f\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.16056", "pdf": "https://arxiv.org/pdf/2504.16056", "abs": "https://arxiv.org/abs/2504.16056", "authors": ["Daniel Hendriks", "Philipp Spitzer", "Niklas K\u00fchl", "Gerhard Satzger"], "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u77e5\u8bc6\u84b8\u998f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u5176\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u7814\u7a76\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5e94\u7528critique-revision prompting\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u7efc\u5408\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u7528CQA\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5176\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u52a9\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u84b8\u998f\uff0c\u63a8\u52a8LLM\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u5feb\u901f\u6269\u6563\u3002"}}
{"id": "2504.15792", "pdf": "https://arxiv.org/pdf/2504.15792", "abs": "https://arxiv.org/abs/2504.15792", "authors": ["Dinh Nam Pham", "Torsten Rahne"], "title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "categories": ["cs.CV"], "comment": "English version of journal article in HNO 2022", "summary": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5fb7\u8bed\u5507\u8bfb\u7b97\u6cd5\uff0c\u901a\u8fc73D CNN\u548cGRU\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u5507\u90e8\u533a\u57df\u88c1\u526a\u65f6\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u5507\u8bfb\u7b97\u6cd5\u591a\u4e3a\u82f1\u8bed\u8bbe\u8ba1\uff0c\u5fb7\u8bed\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u9519\u8bef\u7387\u9ad8\u3002", "method": "\u4f7f\u75281806\u4e2a\u5fb7\u8bed\u89c6\u9891\u7247\u6bb5\uff0c\u8bad\u7ec33D CNN\u3001GRU\u53ca\u7ec4\u5408\u6a21\u578b\uff08GRUConv\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u56fe\u50cf\u533a\u57df\u548c\u8272\u5f69\u7a7a\u95f4\u3002", "result": "GRUConv\u6a21\u578b\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u8bf4\u8bdd\u8005\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a87%\u548c63%\uff0c\u5507\u90e8\u88c1\u526a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u5fb7\u8bed\u5507\u8bfb\u7b97\u6cd5\u9996\u6b21\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u53ef\u63a8\u5e7f\u81f3\u66f4\u591a\u8bcd\u6c47\u7c7b\u522b\uff0c\u6027\u80fd\u5ab2\u7f8e\u82f1\u8bed\u7b97\u6cd5\u3002"}}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060", "abs": "https://arxiv.org/abs/2504.16060", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u4ece\u8bed\u7528\u5b66\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6307\u4ee3\u8868\u8fbe\u751f\u6210\uff08REG\uff09\uff0c\u63d0\u51fa\u65b0\u6570\u636e\u96c6RefOI\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bed\u7528\u80fd\u529b\u4e0a\u7684\u4e09\u5927\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5e38\u5ffd\u7565\u8bed\u7528\u7ef4\u5ea6\uff0c\u4ec5\u5c06\u5176\u89c6\u4e3a\u57fa\u4e8e\u533a\u57df\u7684\u63cf\u8ff0\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86Gricean\u51c6\u5219\u3002", "method": "\u5f15\u5165\u5305\u542b1.5k\u56fe\u50cf\u7684RefOI\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684VLMs\u5728\u8bed\u7528\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0VLMs\u5728\u8bed\u7528\u80fd\u529b\u4e0a\u7684\u4e09\u5927\u5931\u8d25\uff1a\u65e0\u6cd5\u552f\u4e00\u8bc6\u522b\u6307\u4ee3\u5bf9\u8c61\u3001\u5305\u542b\u5197\u4f59\u4fe1\u606f\u3001\u4e0e\u4eba\u7c7b\u8bed\u7528\u504f\u597d\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u547c\u5401\u5f00\u53d1\u66f4\u7b26\u5408\u4eba\u7c7b\u5b9e\u9645\u4ea4\u6d41\u7684\u8bed\u7528\u6a21\u578b\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.15796", "pdf": "https://arxiv.org/pdf/2504.15796", "abs": "https://arxiv.org/abs/2504.15796", "authors": ["Jiaqi Tang", "Yinsong Xu", "Qingchao Chen"], "title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u56fe\u7684\u6570\u636e\u91c7\u6837\u5757\uff08SM-DSB\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u70b9\u4e91\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u4e2d\u81ea\u76d1\u7763\u4efb\u52a1\u68af\u5ea6\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6846\u67b6\u4e2d\u7ed3\u5408\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u4f46\u67d0\u4e9b\u68af\u5ea6\u53ef\u80fd\u5bf9\u5206\u7c7b\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u663e\u8457\u6027\u56fe\u504f\u5ea6\u7684\u8bc4\u5206\u673a\u5236\uff0c\u52a8\u6001\u7b5b\u9009\u6709\u76ca\u6837\u672c\uff0c\u51cf\u5c11\u68af\u5ea6\u51b2\u7a81\u3002", "result": "\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "SM-DSB\u4e3aUDA\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u5206\u6790\u589e\u5f3a\u4e86\u7406\u89e3\u3002"}}
{"id": "2504.16063", "pdf": "https://arxiv.org/pdf/2504.16063", "abs": "https://arxiv.org/abs/2504.16063", "authors": ["A. Fronzetti Colladon", "R. Vestrelli"], "title": "A Python Tool for Reconstructing Full News Text from GDELT", "categories": ["cs.CL", "cs.DB", "cs.IR", "I.2.7; H.2.8; H.3.1"], "comment": null, "summary": "News data have become an essential resource across various disciplines,\nincluding economics, finance, management, social sciences, and computer\nscience. Researchers leverage newspaper articles to study economic trends,\nmarket dynamics, corporate strategies, public perception, political discourse,\nand the evolution of public opinion. Additionally, news datasets have been\ninstrumental in training large-scale language models, with applications in\nsentiment analysis, fake news detection, and automated news summarization.\nDespite their significance, access to comprehensive news corpora remains a key\nchallenge. Many full-text news providers, such as Factiva and LexisNexis,\nrequire costly subscriptions, while free alternatives often suffer from\nincomplete data and transparency issues. This paper presents a novel approach\nto obtaining full-text newspaper articles at near-zero cost by leveraging data\nfrom the Global Database of Events, Language, and Tone (GDELT). Specifically,\nwe focus on the GDELT Web News NGrams 3.0 dataset, which provides\nhigh-frequency updates of n-grams extracted from global online news sources. We\nprovide Python code to reconstruct full-text articles from these n-grams by\nidentifying overlapping textual fragments and intelligently merging them. Our\nmethod enables researchers to access structured, large-scale newspaper data for\ntext analysis while overcoming the limitations of existing proprietary\ndatasets. The proposed approach enhances the accessibility of news data for\nempirical research, facilitating applications in economic forecasting,\ncomputational social science, and natural language processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GDELT\u6570\u636e\u96c6\u4f4e\u6210\u672c\u83b7\u53d6\u5168\u6587\u65b0\u95fb\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b0\u95fb\u6570\u636e\u96c6\u7684\u8bbf\u95ee\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u65b0\u95fb\u6570\u636e\u5728\u591a\u5b66\u79d1\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u6210\u672c\u9ad8\u6216\u4e0d\u5b8c\u6574\uff0c\u9650\u5236\u4e86\u7814\u7a76\u3002", "method": "\u5229\u7528GDELT Web News NGrams 3.0\u6570\u636e\u96c6\uff0c\u901a\u8fc7Python\u4ee3\u7801\u91cd\u6784\u5168\u6587\u65b0\u95fb\u3002", "result": "\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u83b7\u53d6\u7ed3\u6784\u5316\u3001\u5927\u89c4\u6a21\u7684\u65b0\u95fb\u6570\u636e\uff0c\u652f\u6301\u6587\u672c\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u65b0\u95fb\u6570\u636e\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u52a9\u529b\u7ecf\u6d4e\u9884\u6d4b\u3001\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u3002"}}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u3001\u9690\u853d\u4e14\u5b9e\u7528\u7684\u5bf9\u6297\u6027\u8865\u4e01\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u653b\u51fb\u8fd1\u7ea2\u5916\uff08NIR\uff09\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u5b57\u5f62\u72b6\u548c\u4f4d\u7f6e\uff0c\u5e76\u7ed3\u5408\u5149\u53cd\u5c04\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u8fd1\u7ea2\u5916\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u4f4e\u5149\u6216\u5316\u5986\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7269\u7406\u5bf9\u6297\u653b\u51fb\u5b58\u5728\u8106\u5f31\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u4eba\u773c\u4e0d\u53ef\u89c1\u7684\u7ea2\u5916\u5438\u6536\u58a8\u6c34\u751f\u6210\u6570\u5b57\u4f18\u5316\u7684\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u5149\u53cd\u5c04\u6a21\u578b\u6a21\u62dfNIR\u5149\u53cd\u5c04\uff0c\u51cf\u5c11\u6570\u5b57\u4e0e\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b57\u548c\u7269\u7406\u9886\u57df\u7684\u653b\u51fb\u6210\u529f\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u7269\u7406\u9886\u57df\u5e73\u5747\u6210\u529f\u7387\u8fbe82.46%\uff0c\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u768464.18%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u6297\u6027\u8865\u4e01\u5728NIR\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u653b\u51fb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u79cd\u59ff\u6001\u4e0b\u4fdd\u6301\u9ad8\u6548\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u5a01\u80c1\u3002"}}
{"id": "2504.16073", "pdf": "https://arxiv.org/pdf/2504.16073", "abs": "https://arxiv.org/abs/2504.16073", "authors": ["Zhiyuan Hu", "Shiyun Xiong", "Yifan Zhang", "See-Kiong Ng", "Anh Tuan Luu", "Bo An", "Shuicheng Yan", "Bryan Hooi"], "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLM\u5728\u590d\u6742GUI\u73af\u5883\u4e2d\u751f\u6210\u6b63\u786e\u52a8\u4f5c\u7684\u80fd\u529b\u6709\u9650\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u548c\u4f18\u5316\u6280\u672f\u5b58\u5728\u53cd\u9988\u5ef6\u8fdf\u548c\u5c40\u90e8\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u5bf9VLM\u4ee3\u7406\u8fdb\u884c\u8fc7\u7a0b\u76d1\u7763\uff0c\u4f18\u5316\u6bcf\u4e00\u6b65\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u8f68\u8ff9\u53cd\u601d\u548c\u91cd\u8bd5\u673a\u5236\u3002", "result": "\u5728\u9759\u6001\u73af\u5883\u4e2d\u5355\u6b65\u52a8\u4f5c\u51c6\u786e\u7387\u63d0\u53473.4%\uff0c\u52a8\u6001\u73af\u5883\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u7ea633%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLM\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.15835", "pdf": "https://arxiv.org/pdf/2504.15835", "abs": "https://arxiv.org/abs/2504.15835", "authors": ["Yiqian Wu", "Malte Prinzler", "Xiaogang Jin", "Siyu Tang"], "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "categories": ["cs.CV"], "comment": null, "summary": "The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation.", "AI": {"tldr": "\u63d0\u51faAnimPortrait3D\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548cControlNet\uff0c\u89e3\u51b3\u6587\u672c\u751f\u62103D\u5934\u50cf\u65f6\u7684\u5916\u89c2\u3001\u51e0\u4f55\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u52a8\u753b\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u751f\u62103D\u5934\u50cf\u65b9\u6cd5\u5728\u7ec6\u8282\u771f\u5b9e\u6027\u548c\u52a8\u753b\u5bf9\u9f50\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e02D\u6269\u6563\u9884\u6d4b\u7684\u6a21\u7cca\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u4e0d\u8db3\u3002", "method": "1. \u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u52303D\u6a21\u578b\u521d\u59cb\u5316\u5934\u50cf\uff1b2. \u901a\u8fc7ControlNet\u7ed3\u5408\u53d8\u5f62\u6a21\u578b\u7684\u8bed\u4e49\u548c\u6cd5\u7ebf\u56fe\u4f18\u5316\u52a8\u6001\u8868\u60c5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u8d28\u91cf\u3001\u5bf9\u9f50\u548c\u52a8\u753b\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnimPortrait3D\u5728\u6587\u672c\u751f\u6210\u53ef\u52a8\u753b3D\u5934\u50cf\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074", "abs": "https://arxiv.org/abs/2504.16074", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Muhan Zhang", "Hua Xing Zhu"], "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "21 pages ,8 figures, 4 tables", "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.", "AI": {"tldr": "PHYBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u573a\u666f\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u5305\u542b500\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7269\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807EED Score\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u63a8\u7406\u4e0a\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u7269\u7406\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u5e76\u63a8\u52a8\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u5305\u542b500\u4e2a\u7269\u7406\u95ee\u9898\u7684PHYBench\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6570\u5b66\u8868\u8fbe\u5f0f\u7f16\u8f91\u8ddd\u79bb\u7684EED Score\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u63a8\u7406\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "PHYBench\u548cEED Score\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.15863", "pdf": "https://arxiv.org/pdf/2504.15863", "abs": "https://arxiv.org/abs/2504.15863", "authors": ["Diego de Oliveira Hitzges", "Suman Ghosh", "Guillermo Gallego"], "title": "DERD-Net: Learning Depth from Event-based Ray Densities", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.SP"], "comment": "13 pages, 3 figures, 14 tables. Project page:\n  https://github.com/tub-rip/DERD-Net", "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u5f02\u6b65\u4e8b\u4ef6\u6570\u636e\uff0c\u5728\u5355\u76ee\u548c\u7acb\u4f53\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6df1\u5ea6\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u96be\u4ee5\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u7684\u5f02\u6b65\u6570\u636e\u6d41\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9002\u5e94\u4e8b\u4ef6\u6570\u636e\u7279\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5df2\u77e5\u76f8\u673a\u59ff\u6001\u5c06\u4e8b\u4ef6\u6570\u636e\u53cd\u5411\u6295\u5f71\u5230\u7a7a\u95f4\uff0c\u751f\u6210\u89c6\u5dee\u7a7a\u95f4\u56fe\u50cf\uff08DSIs\uff09\uff0c\u5e76\u901a\u8fc73D\u5377\u79ef\u548c\u5faa\u73af\u7ed3\u6784\u5904\u7406\u5c40\u90e8\u5b50\u533a\u57df\u4ee5\u9884\u6d4b\u6df1\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u5355\u76ee\u6570\u636e\u7ed3\u679c\u5ab2\u7f8e\u73b0\u6709\u7acb\u4f53\u65b9\u6cd5\uff0c\u7acb\u4f53\u6570\u636e\u4e0b\u8bef\u5dee\u964d\u4f4e\u81f3\u5c1142%\uff0c\u6df1\u5ea6\u5b8c\u6574\u6027\u63d0\u53473\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u548cSLAM\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u80fd\u6210\u4e3a\u6807\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084", "abs": "https://arxiv.org/abs/2504.16084", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Shang Qu", "Li Sheng", "Xuekai Zhu", "Biqing Qi", "Youbang Sun", "Ganqu Cui", "Ning Ding", "Bowen Zhou"], "title": "TTRL: Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTTRL\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u7f3a\u4e4f\u663e\u5f0f\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faTest-Time Reinforcement Learning (TTRL)\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u7b49\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u751f\u6210\u5956\u52b1\u4fe1\u53f7\u3002", "result": "TTRL\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f8b\u5982\u5728AIME 2024\u4efb\u52a1\u4e2d\uff0cQwen-2.5-Math-7B\u7684pass@1\u6027\u80fd\u63d0\u9ad8\u4e86\u7ea6159%\u3002", "conclusion": "TTRL\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.", "AI": {"tldr": "MedNNS\u662f\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u67b6\u6784\u9009\u62e9\u548c\u6743\u91cd\u521d\u59cb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u67b6\u6784\u9009\u62e9\u548c\u6743\u91cd\u521d\u59cb\u5316\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982ImageNet\u8fc1\u79fb\u5b66\u4e60\uff09\u6548\u679c\u6709\u9650\u3002", "method": "MedNNS\u6784\u5efa\u4e86\u4e00\u4e2a\u5143\u7a7a\u95f4\uff0c\u7ed3\u5408Supernetwork\u65b9\u6cd5\uff0c\u5f15\u5165rank loss\u548cFID loss\u4f18\u5316\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u7684\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMedNNS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.7%\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "MedNNS\u4e3a\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs.", "AI": {"tldr": "\u63d0\u51faMed-CoDE\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u7597\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5728\u533b\u7597\u9886\u57df\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e34\u5e8a\u98ce\u9669\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6279\u8bc4\u7684\u65b9\u6cd5\uff0c\u91cf\u5316\u6a21\u578b\u751f\u6210\u56de\u7b54\u4e0e\u533b\u5b66\u6807\u51c6\u7b54\u6848\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u6846\u67b6\u80fd\u540c\u65f6\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "Med-CoDE\u4e3a\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis.", "AI": {"tldr": "RadFuse\u662f\u4e00\u79cd\u591a\u8868\u793a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u975e\u7ebf\u6027RadEx\u53d8\u6362\u7684sinogram\u56fe\u50cf\u4e0e\u4f20\u7edf\u773c\u5e95\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7684\u68c0\u6d4b\u548c\u5206\u7ea7\u6027\u80fd\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u51c6\u786e\u5206\u7ea7\u5bf9\u9884\u9632\u89c6\u529b\u4e27\u5931\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u75c5\u53d8\u7684\u590d\u6742\u4e0d\u89c4\u5219\u6a21\u5f0f\u3002", "method": "RadFuse\u901a\u8fc7RadEx\u53d8\u6362\u751f\u6210sinogram\u8868\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u53d8\u6362\u57df\u4fe1\u606f\uff0c\u5229\u7528\u4e09\u79cdCNN\u67b6\u6784\uff08ResNeXt-50\u3001MobileNetV2\u3001VGG19\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728APTOS-2019\u548cDDR\u6570\u636e\u96c6\u4e0a\uff0cRadFuse\u5728\u4e94\u7ea7\u4e25\u91cd\u5ea6\u5206\u7ea7\u4e2d\u8fbe\u523093.24%\u7684\u4e8c\u6b21\u52a0\u6743kappa\uff0c\u4e8c\u5143\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u8fbe99.09%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RadFuse\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u63a8\u52a8\u4e86\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u53ca\u6570\u5b66\u53d8\u6362\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.15888", "pdf": "https://arxiv.org/pdf/2504.15888", "abs": "https://arxiv.org/abs/2504.15888", "authors": ["Zhiqiang Wei", "Lianqing Zheng", "Jianan Liu", "Tao Huang", "Qing-Long Han", "Wenwen Zhang", "Fengdeng Zhang"], "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios.", "AI": {"tldr": "MS-Occ\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u95f4\u548c\u540e\u671f\u878d\u5408\u7ed3\u5408\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u51e0\u4f55\u4e0d\u51c6\u786e\u548cLiDAR\u65b9\u6cd5\u8bed\u4e49\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u878d\u5408\u6846\u67b6\uff0c\u5305\u62ec\u4e2d\u95f4\u9636\u6bb5\u7684Gaussian-Geo\u6a21\u5757\u548cSemantic-Aware\u6a21\u5757\uff0c\u4ee5\u53ca\u540e\u671f\u9636\u6bb5\u7684Adaptive Fusion\u548cHCCVF\u6a21\u5757\u3002", "result": "\u5728nuScenes-OpenOccupancy\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIoU\u8fbe32.1%\uff0cmIoU\u8fbe25.3%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MS-Occ\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u521b\u65b0\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u611f\u77e5\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5c0f\u7269\u4f53\u611f\u77e5\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1In-VAL\uff0c\u6a21\u62df\u4eba\u4e0e\u89c6\u9891\u7684\u591a\u6b21\u4ea4\u4e92\u4ee5\u83b7\u53d6\u89c6\u89c9\u7b54\u6848\uff0c\u5e76\u63d0\u51fa\u4e86Ask2Loc\u6846\u67b6\u6765\u89e3\u51b3\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u7528\u6237\u901a\u5e38\u9700\u8981\u591a\u6b21\u4ea4\u4e92\u624d\u80fd\u83b7\u5f97\u7b26\u5408\u671f\u671b\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6a21\u62df\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faAsk2Loc\u6846\u67b6\uff0c\u5305\u542b\u804a\u5929\u6a21\u5757\u3001\u91cd\u5199\u6a21\u5757\u548c\u641c\u7d22\u6a21\u5757\uff0c\u5206\u522b\u89e3\u51b3\u610f\u56fe\u6a21\u7cca\u3001\u8bed\u8a00\u4e0d\u5b8c\u6574\u548c\u5185\u5bb9\u788e\u7247\u5316\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u91cd\u6784\u7684In-VAL\u6570\u636e\u96c6\u4e0a\uff0cAsk2Loc\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe14.91\uff08mIoU\uff09\u3002", "conclusion": "Ask2Loc\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86In-VAL\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u5dee\u8ddd\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.15921", "pdf": "https://arxiv.org/pdf/2504.15921", "abs": "https://arxiv.org/abs/2504.15921", "authors": ["Jian Hu", "Dimitrios Korkinof", "Shaogang Gong", "Mariano Beguerisse-Diaz"], "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.", "AI": {"tldr": "ViSMaP\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u89c6\u9891\u6458\u8981\u7cfb\u7edf\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u6280\u672f\u751f\u6210\u957f\u89c6\u9891\u6458\u8981\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u6458\u8981\u4e2d\u76f8\u5173\u4e8b\u4ef6\u7a00\u758f\u4e14\u672a\u5206\u6bb5\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u7684\u76d1\u7763\u8bad\u7ec3\u3002", "method": "\u5229\u7528LLMs\u751f\u6210\u4f18\u5316\u7684\u4f2a\u6458\u8981\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u7b56\u7565\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u6458\u8981\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u5148\u8fdb\u6a21\u578b\uff0c\u4e14\u80fd\u8de8\u9886\u57df\u6cdb\u5316\u3002", "conclusion": "ViSMaP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u957f\u89c6\u9891\u6458\u8981\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2504.15448", "pdf": "https://arxiv.org/pdf/2504.15448", "abs": "https://arxiv.org/abs/2504.15448", "authors": ["Yanampally Abhiram Reddy", "Siddhi Agarwal", "Vikram Parashar", "Arshiya Arora"], "title": "Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": "19 pages, 2 figures", "summary": "In the age of social media, understanding public sentiment toward major\ncorporations is crucial for investors, policymakers, and researchers. This\npaper presents a comprehensive sentiment analysis system tailored for corporate\nreputation monitoring, combining Natural Language Processing (NLP) and machine\nlearning techniques to accurately interpret public opinion in real time. The\nmethodology integrates a hybrid sentiment detection framework leveraging both\nrule-based models (VADER) and transformer-based deep learning models\n(DistilBERT), applied to social media data from multiple platforms. The system\nbegins with robust preprocessing involving noise removal and text\nnormalization, followed by sentiment classification using an ensemble approach\nto ensure both interpretability and contextual accuracy. Results are visualized\nthrough sentiment distribution plots, comparative analyses, and temporal\nsentiment trends for enhanced interpretability. Our analysis reveals\nsignificant disparities in public sentiment across major corporations, with\ncompanies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment\nscores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment\nprofiles. These findings demonstrate the utility of our multi-source sentiment\nframework in providing actionable insights regarding corporate public\nperception, enabling stakeholders to make informed strategic decisions based on\ncomprehensive sentiment analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NLP\u548c\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u65f6\u4f01\u4e1a\u58f0\u8a89\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u60c5\u611f\u68c0\u6d4b\u6846\u67b6\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6570\u636e\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4f01\u4e1a\u7684\u516c\u4f17\u60c5\u611f\u5dee\u5f02\u3002", "motivation": "\u5728\u793e\u4ea4\u5a92\u4f53\u65f6\u4ee3\uff0c\u4e86\u89e3\u516c\u4f17\u5bf9\u4f01\u4e1a\u7684\u60c5\u611f\u5bf9\u6295\u8d44\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7814\u7a76\u8005\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6df7\u5408\u60c5\u611f\u68c0\u6d4b\u6846\u67b6\uff08VADER\u89c4\u5219\u6a21\u578b\u548cDistilBERT\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u6570\u636e\u9884\u5904\u7406\u548c\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u4e0d\u540c\u4f01\u4e1a\u60c5\u611f\u5f97\u5206\u5dee\u5f02\u663e\u8457\uff0c\u5982\u4e9a\u9a6c\u900a\uff0881.2\uff09\u548c\u4e09\u661f\uff0845.8\uff09\u8868\u73b0\u4f18\u5f02\uff0c\u5fae\u8f6f\uff0821.7\uff09\u548c\u6c83\u5c14\u739b\uff0821.9\uff09\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u4e3a\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u5168\u9762\u7684\u60c5\u611f\u5206\u6790\uff0c\u652f\u6301\u57fa\u4e8e\u6570\u636e\u7684\u6218\u7565\u51b3\u7b56\u3002"}}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers.", "AI": {"tldr": "GlobeReady\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8de8\u4e34\u5e8a\u4e2d\u5fc3\u4f7f\u7528\u7684AI\u5e73\u53f0\uff0c\u7528\u4e8e\u773c\u79d1\u75be\u75c5\u8bca\u65ad\uff0c\u8868\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3AI\u6a21\u578b\u5728\u4e0d\u540c\u4e34\u5e8a\u4e2d\u5fc3\u90e8\u7f72\u65f6\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63a8\u52a8AI\u5728\u533b\u7597\u5f71\u50cf\u8bca\u65ad\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u7684\u7279\u5f81\u589e\u5f3a\u6280\u672f\uff0c\u5e94\u5bf9\u4e0d\u540c\u4e2d\u5fc3\u548c\u4eba\u7fa4\u7684\u6570\u636e\u57df\u504f\u79fb\uff0c\u5e76\u63d0\u4f9b\u53ef\u91cf\u5316\u7684\u8bca\u65ad\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u5f71\u50cf\u6a21\u6001\u548c\u591a\u56fd\u4e34\u5e8a\u4e2d\u5fc3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe93.9-99.4%\uff0c\u4e34\u5e8a\u8bc4\u52064.6/5\u3002", "conclusion": "GlobeReady\u5c55\u793a\u4e86\u65e0\u9700\u6280\u672f\u969c\u788d\u7684\u3001\u53ef\u6269\u5c55\u7684\u773c\u79d1\u8bca\u65ad\u6f5c\u529b\u3002"}}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.", "AI": {"tldr": "APR\u662f\u4e00\u79cd\u65b0\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e76\u884c\u63a8\u7406\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u8f93\u51fa\u8fc7\u957f\u6216\u534f\u8c03\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faAPR\u6846\u67b6\uff0c\u7ed3\u5408\u4e32\u884c\u548c\u5e76\u884c\u8ba1\u7b97\uff0c\u4f7f\u7528spawn()\u548cjoin()\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u7ebf\u7a0b\u3002", "result": "\u5728Countdown\u4efb\u52a1\u4e2d\uff0cAPR\u5728\u76f8\u540c\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u8ba1\u7b97\u91cf\u548c\u5ef6\u8fdf\u4e0b\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "APR\u4e3a\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Ko\u00e7", "Tolga \u00c7ukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.", "AI": {"tldr": "MedTrim\u662f\u4e00\u79cd\u65b0\u578b\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4e09\u5143\u7ec4\u5b66\u4e60\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u75c5\u7406\u5c5e\u6027\u7684\u533a\u5206\u80fd\u529b\uff0c\u4ece\u800c\u5728\u80f8\u90e8X\u5149\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u6570\u636e\u91cf\u589e\u957f\u5bfc\u81f4\u4e13\u5bb6\u538b\u529b\u589e\u52a0\uff0c\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u7ec6\u7c92\u5ea6\u75c5\u7406\u5c5e\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faMedTrim\u65b9\u6cd5\uff0c\u7ed3\u5408\u75be\u75c5\u7c7b\u522b\u548c\u75c5\u7406\u63cf\u8ff0\u7b26\uff0c\u901a\u8fc7\u4e09\u5143\u7ec4\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u5143\u5b9e\u4f53\u4fe1\u606f\u4f18\u5316\u5bf9\u9f50\u3002", "result": "MedTrim\u5728\u4e0b\u6e38\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MedTrim\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15931", "pdf": "https://arxiv.org/pdf/2504.15931", "abs": "https://arxiv.org/abs/2504.15931", "authors": ["Ekaterina Kondrateva", "Sandzhi Barg", "Mikhail Vasiliev"], "title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86FastSurfer\u548cSynthSeg\u4e24\u79cd\u73b0\u4ee3\u8111\u90e8\u5206\u5272\u65b9\u6cd5\u5728\u7eb5\u5411\u548c\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c0f\u8111\u533a\u5b58\u5728\u663e\u8457\u4f53\u79ef\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5206\u5272\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u6784MRI\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\u5bf9\u76d1\u6d4b\u795e\u7ecf\u89e3\u5256\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u626b\u63cf\u4eea\u5dee\u5f02\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6570\u636e\u96c6\uff08SIMON\u548cSRPBS\uff09\uff0c\u901a\u8fc7Dice\u7cfb\u6570\u3001Surface Dice\u3001Hausdorff\u8ddd\u79bb\u548cMAPE\u91cf\u5316\u5206\u5272\u53d8\u5f02\u6027\u3002", "result": "\u5c0f\u8111\u533a\uff08\u5982\u674f\u4ec1\u6838\uff09\u5728\u63a7\u5236\u6761\u4ef6\u4e0b\u4ecd\u67097-8%\u7684\u4f53\u79ef\u53d8\u5316\uff0c\u6311\u6218\u4e86\u68c0\u6d4b5-10%\u7ec6\u5fae\u53d8\u5316\u7684\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u6539\u8fdb\u5206\u5272\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b9e\u9645\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u4e2d\u6807\u51c6\u5316\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u5168\u6808\u5b89\u5168\u201d\u6982\u5ff5\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5168\u751f\u547d\u5468\u671f\u7684\u5b89\u5168\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5173\u4e8eLLM\u5b89\u5168\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7279\u5b9a\u9636\u6bb5\uff0c\u7f3a\u4e4f\u5bf9\u5168\u751f\u547d\u5468\u671f\u7684\u5168\u9762\u7406\u89e3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49LLM\u5168\u751f\u547d\u5468\u671f\uff08\u6570\u636e\u51c6\u5907\u3001\u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u3001\u90e8\u7f72\u548c\u5546\u4e1a\u5316\uff09\uff0c\u5e76\u57fa\u4e8e800+\u6587\u732e\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u63d0\u51fa\u5b89\u5168\u6846\u67b6\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u89c6\u89d2\u3001\u5e7f\u6cdb\u7684\u6587\u732e\u652f\u6301\u548c\u72ec\u7279\u89c1\u89e3\uff0c\u5305\u62ec\u6570\u636e\u751f\u6210\u5b89\u5168\u3001\u5bf9\u9f50\u6280\u672f\u3001\u6a21\u578b\u7f16\u8f91\u7b49\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u5168\u751f\u547d\u5468\u671f\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.15932", "pdf": "https://arxiv.org/pdf/2504.15932", "abs": "https://arxiv.org/abs/2504.15932", "authors": ["Wang Lin", "Liyu Jia", "Wentao Hu", "Kaihang Pan", "Zhongqi Yue", "Wei Zhao", "Jingyuan Chen", "Fei Wu", "Hanwang Zhang"], "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5Phys-AR\uff0c\u7528\u4e8e\u5728\u89c6\u9891\u751f\u6210\u4e2d\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u3002\u901a\u8fc7Diffusion Timestep Tokenizer\uff08DDT\uff09\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7269\u7406\u6761\u4ef6\uff08\u5982\u901f\u5ea6\uff09\uff0c\u56e0\u5176\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u8fd1\u4f3c\u3002", "method": "\u63d0\u51faPhys-AR\u6846\u67b6\uff1a1\uff09\u4f7f\u7528DDT\u5b66\u4e60\u79bb\u6563\u9012\u5f52\u89c6\u89c9\u6807\u8bb0\uff0c\u652f\u6301\u7b26\u53f7\u63a8\u7406\uff1b2\uff09\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u4f18\u5316\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPhys-AR\u80fd\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u89c6\u9891\u3002", "conclusion": "\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0cPhys-AR\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629", "abs": "https://arxiv.org/abs/2504.15629", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products.", "AI": {"tldr": "RAG\u7ed3\u5408LLMs\u63d0\u5347\u4fe1\u606f\u68c0\u7d22\uff0c\u4f46\u5f15\u7528\u51c6\u786e\u6027\u4f4e\uff08\u7ea674%\uff09\u3002\u672c\u6587\u63d0\u51fa\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u8bcd+\u8bed\u4e49\u5339\u914d\u3001BERTScore\u5fae\u8c03\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7LLM\u6280\u672f\uff0c\u63d0\u534715.46%\u7684\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3LLM\u5728RAG\u7cfb\u7edf\u4e2d\u5f15\u7528\u51c6\u786e\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u5347AI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u91c7\u7528\u5173\u952e\u8bcd+\u8bed\u4e49\u5339\u914d\u3001BERTScore\u5fae\u8c03\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7LLM\u6280\u672f\u8fdb\u884c\u540e\u5904\u7406\u3002", "result": "\u5f15\u7528\u51c6\u786e\u6027\u63d0\u534715.46%\uff0c\u53ef\u6539\u7528\u66f4\u5c0f\u3001\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u6a21\u578b\uff08\u6210\u672c\u964d12\u500d\uff0c\u63a8\u7406\u5feb3\u500d\uff09\u3002", "conclusion": "\u7814\u7a76\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u7684\u5f15\u7528\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u589e\u5f3aAI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5546\u4e1a\u4ea7\u54c1\u3002"}}
{"id": "2504.15958", "pdf": "https://arxiv.org/pdf/2504.15958", "abs": "https://arxiv.org/abs/2504.15958", "authors": ["Zebin Yao", "Lei Ren", "Huixing Jiang", "Chen Wei", "Xiaojie Wang", "Ruifan Li", "Fangxiang Feng"], "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.", "AI": {"tldr": "FreeGraftor\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u56fe\u50cf\u7279\u5f81\u5ac1\u63a5\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u8c03\u4f18\u65b9\u6cd5\u8017\u65f6\u8017\u8d44\u6e90\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u5219\u65e0\u6cd5\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u5339\u914d\u548c\u4f4d\u7f6e\u7ea6\u675f\u6ce8\u610f\u529b\u878d\u5408\uff0c\u7ed3\u5408\u566a\u58f0\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u89c6\u89c9\u7ec6\u8282\u7684\u8de8\u56fe\u50cf\u8f6c\u79fb\u3002", "result": "\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u548c\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u4e3b\u9898\u751f\u6210\u3002", "conclusion": "FreeGraftor\u65e0\u9700\u8c03\u4f18\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation.", "AI": {"tldr": "VERICODER\u662f\u4e00\u4e2a\u9488\u5bf9RTL\u4ee3\u7801\u751f\u6210\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u529f\u80fd\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709RTL\u6570\u636e\u96c6\u591a\u5173\u6ce8\u8bed\u6cd5\u6709\u6548\u6027\u800c\u975e\u529f\u80fd\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u4e0d\u7b26\u5408\u9884\u671f\u884c\u4e3a\u3002", "method": "\u7ed3\u5408\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u548c\u53cd\u9988\u5bfc\u5411\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u6784\u5efa\u529f\u80fd\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u6a21\u578b\u3002", "result": "VERICODER\u5728VerilogEval\u548cRTLLM\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u529f\u80fd\u6b63\u786e\u6027\u6307\u6807\uff0c\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe71.7%\u548c27.4%\u3002", "conclusion": "\u529f\u80fd\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9RTL\u4ee3\u7801\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.15991", "pdf": "https://arxiv.org/pdf/2504.15991", "abs": "https://arxiv.org/abs/2504.15991", "authors": ["Leonardo Olivi", "Edoardo Santero Mormile", "Enzo Tartaglione"], "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6708\u7403\u548c\u706b\u661f\u5730\u5f62\u4e2d\uff0c\u4f7f\u7528\u9002\u914d\u5668\u8fdb\u884c\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u4ee5\u5b9e\u73b0\u5ca9\u77f3\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u5185\u5b58\u8282\u7701\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5916\u661f\u73af\u5883\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u76ee\u6807\u8bbe\u5907\u7684\u5e26\u5bbd\u548c\u5185\u5b58\u9700\u6c42\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u9aa8\u5e72\u6a21\u578b\u4e2d\u96c6\u6210\u9002\u914d\u5668\uff0c\u91c7\u7528\u5c42\u878d\u5408\u548c\u9002\u914d\u5668\u6392\u540d\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u9002\u914d\u5668\u6210\u529f\u51cf\u5c11\u4e86\u5e26\u5bbd\u548c\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8bc4\u4f30\u4e86\u6027\u80fd\u3001\u5185\u5b58\u548c\u8ba1\u7b97\u7684\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5916\u661f\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrustGeoGen\u7684\u6570\u636e\u5f15\u64ce\uff0c\u7528\u4e8e\u751f\u6210\u51e0\u4f55\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5408\u6210\u51e0\u4f55\u95ee\u9898\u57fa\u51c6\u7684\u566a\u58f0\u548c\u81ea\u76f8\u77db\u76fe\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u65b9\u6cd5\u8bba\u548c\u9a8c\u8bc1\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTrustGeoGen\u5f15\u64ce\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u751f\u6210\u3001\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3001\u81ea\u4e3e\u673a\u5236\u548cGeoExplore\u7b97\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u51e0\u4f55\u95ee\u9898\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210GeoTrust-200K\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a49.17%\uff0c\u4f46\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728OOD\u6cdb\u5316\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TrustGeoGen\u4e3a\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.16003", "pdf": "https://arxiv.org/pdf/2504.16003", "abs": "https://arxiv.org/abs/2504.16003", "authors": ["Yachun Mi", "Yu Li", "Weicheng Meng", "Chaofeng Chen", "Chen Hui", "Shaohui Liu"], "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory.", "AI": {"tldr": "MVQA\u7ed3\u5408Mamba\u6a21\u578b\u548cUSDS\u91c7\u6837\u65b9\u6cd5\uff0c\u9ad8\u6548\u5b8c\u6210\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u6027\u80fd\u63a5\u8fd1SOTA\uff0c\u901f\u5ea6\u63d0\u53472\u500d\uff0cGPU\u5185\u5b58\u4ec5\u97001/5\u3002", "motivation": "\u957f\u65f6\u957f\u9ad8\u6e05\u89c6\u9891\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u9ad8\u6548\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u63d0\u51faMVQA\u6a21\u578b\uff0c\u57fa\u4e8eMamba\uff0c\u7ed3\u5408USDS\u91c7\u6837\u65b9\u6cd5\uff0c\u878d\u5408\u8bed\u4e49\u548c\u5931\u771f\u4fe1\u606f\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u63a9\u7801\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "MVQA\u6027\u80fd\u63a5\u8fd1SOTA\u65b9\u6cd5\uff0c\u901f\u5ea6\u63d0\u53472\u500d\uff0cGPU\u5185\u5b58\u4ec5\u97001/5\u3002", "conclusion": "MVQA\u548cUSDS\u4e3a\u9ad8\u6548VQA\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5f62\u5f0f\u9690\u79c1\u7ea6\u675f\u4e0b\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u9690\u79c1\u9884\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5e76\u9996\u6b21\u5bf9\u7ebf\u6027\u56de\u5f52\u4e2dICL\u7684\u9690\u79c1-\u51c6\u786e\u6027\u6743\u8861\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u63a2\u7d22\u5728\u9690\u79c1\u7ea6\u675f\u4e0bICL\u7684\u53ef\u884c\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5dee\u5206\u9690\u79c1\u9884\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5206\u6790\u7ebf\u6027\u56de\u5f52\u4e2dICL\u7684\u9690\u79c1-\u51c6\u786e\u6027\u6743\u8861\u3002", "result": "\u63ed\u793a\u4e86\u4f18\u5316\u4e0e\u9690\u79c1\u566a\u58f0\u4e4b\u95f4\u7684\u57fa\u672c\u77db\u76fe\uff0c\u5e76\u8bc1\u660e\u65b9\u6cd5\u5bf9\u5bf9\u6297\u6027\u8bad\u7ec3\u63d0\u793a\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u4ecd\u80fd\u6709\u6548\u5b9e\u73b0ICL\u3002"}}
{"id": "2504.16016", "pdf": "https://arxiv.org/pdf/2504.16016", "abs": "https://arxiv.org/abs/2504.16016", "authors": ["Xinyuan Song", "Yangfan He", "Sida Li", "Jianhui Wang", "Hongyang He", "Xinhang Yuan", "Ruoyu Wang", "Jiaqi Chen", "Keqin Li", "Kuan Lu", "Menghao Huo", "Binxu Li", "Pei Liu"], "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04606", "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728DDIM\u6a21\u578b\u4e2d\u901a\u8fc7\u9002\u914d\u5668\u4fdd\u6301\u5e27\u4e00\u81f4\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6570\u5b66\u6027\u8d28\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u9700\u8981\u5e27\u95f4\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u6709\u9002\u914d\u5668\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d2\u5165\u53ef\u5b66\u4e60\u6a21\u5757\u5230\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5171\u4eab\u548c\u5e27\u7279\u5b9a\u6807\u8bb0\u7684\u63d0\u793a\u5b66\u4e60\uff0c\u5e76\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u76ee\u6807\u7684\u53ef\u5fae\u6027\u548c\u68af\u5ea6Lipschitz\u754c\uff0c\u5c55\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u6027\uff0c\u5e76\u5206\u6790\u4e86DDIM\u53cd\u6f14\u4e2d\u6a21\u5757\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7406\u8bba\u53d1\u73b0\u589e\u5f3a\u4e86\u57fa\u4e8e\u9002\u914d\u5668\u7684\u6269\u6563\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u89c6\u9891\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u3002"}}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\uff0c\u63a2\u8ba8\u5176\u4f18\u52bf\u3001\u6311\u6218\u3001\u6280\u672f\u57fa\u7840\u3001\u5e94\u7528\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u8fd0\u52a8\u4e00\u81f4\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4f26\u7406\u95ee\u9898\u7b49\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5168\u9762\u68b3\u7406\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u6790\u67b6\u6784\u521b\u65b0\u4e0e\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u7814\u7a76\u5176\u5728\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u3001\u66f4\u65b0\u7684\u89c6\u89d2\uff0c\u6db5\u76d6\u8bc4\u4f30\u6307\u6807\u3001\u884c\u4e1a\u89e3\u51b3\u65b9\u6848\u548c\u8bad\u7ec3\u5de5\u7a0b\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2504.16023", "pdf": "https://arxiv.org/pdf/2504.16023", "abs": "https://arxiv.org/abs/2504.16023", "authors": ["Song Wang", "Xiaolu Liu", "Lingdong Kong", "Jianyun Xu", "Chunyong Hu", "Gongfan Fang", "Wentong Li", "Jianke Zhu", "Xinchao Wang"], "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA.", "AI": {"tldr": "PointLoRA\u662f\u4e00\u79cd\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u591a\u5c3a\u5ea6\u4ee4\u724c\u9009\u62e9\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5fae\u8c03\u70b9\u4e91\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u53ef\u8c03\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u9884\u8bad\u7ec3\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5b8c\u5168\u5fae\u8c03\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\uff0c\u800c\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u673a\u5236\uff0c\u589e\u52a0\u4e86\u53ef\u8c03\u53c2\u6570\u3002", "method": "\u5728\u70b9\u4e91\u53d8\u6362\u5668\u4e2d\u5d4c\u5165LoRA\u5c42\u4ee5\u51cf\u5c11\u53ef\u8c03\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u4ee4\u724c\u9009\u62e9\u63d0\u53d6\u5173\u952e\u5c40\u90e8\u4fe1\u606f\u4f5c\u4e3a\u4e0b\u6e38\u5fae\u8c03\u7684\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPointLoRA\u4ec5\u97003.43%\u7684\u53ef\u8c03\u53c2\u6570\u5373\u53ef\u5728\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "PointLoRA\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.16030", "pdf": "https://arxiv.org/pdf/2504.16030", "abs": "https://arxiv.org/abs/2504.16030", "authors": ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"], "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "categories": ["cs.CV"], "comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu", "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8f6c\u5f55\u8fdb\u884c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video LLM\uff09\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5f0f\u8bad\u7ec3\u548c\u65f6\u5e8f\u5bf9\u9f50\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u4e13\u6709\u6a21\u578bAPI\uff08\u5982GPT-4\uff09\uff0c\u9650\u5236\u4e86\u5176\u5927\u89c4\u6a21\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5229\u7528\u4f4e\u6210\u672cASR\u8f6c\u5f55\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u6d41\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06ASR\u8bcd\u4e0e\u89c6\u9891\u5e27\u6309\u65f6\u95f4\u6233\u5bc6\u96c6\u4ea4\u9519\uff0c\u5e76\u6784\u5efa\u4e86Live-CC-5M\u548cLive-WhisperX-526K\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "ASR\u9884\u8bad\u7ec3\u7684LiveCC-7B-Base\u6a21\u578b\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5b9e\u65f6\u8bc4\u8bba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u7ec8\u6a21\u578bLiveCC-7B-Instruct\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86ASR\u8f6c\u5f55\u5728\u5927\u89c4\u6a21\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4f4e\u6210\u672c\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08RAD-DINO\u3001CheXagent\u548cBiomedCLIP\uff09\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7ec6\u7c92\u5ea6\u7279\u5f81\u6355\u6349\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u7247\u7684\u5206\u7c7b\u3001\u5206\u5272\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u81ea\u5b9a\u4e49\u5206\u5272\u6a21\u578b\u3002", "result": "RAD-DINO\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0cCheXagent\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBiomedCLIP\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002\u81ea\u5b9a\u4e49\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u9884\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u4efb\u52a1\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u65e0\u6587\u672c\u76d1\u7763\u7684\u6a21\u578b\u9002\u5408\u5206\u5272\u4efb\u52a1\uff0c\u6587\u672c\u76d1\u7763\u7684\u6a21\u578b\u5728\u5206\u7c7b\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u66f4\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.", "AI": {"tldr": "VLMs\u5728\u7a7a\u95f4\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u53d7\u63d0\u793a\u8bcd\u5fae\u5c0f\u53d8\u5316\u5f71\u54cd\uff0c\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u6d4b\u8bd5VLMs\u5728\u7b80\u5355\u7a7a\u95f4\u8ba4\u77e5\u4efb\u52a1\uff08\u5982\u7269\u4f53\u5de6\u53f3\u5173\u7cfb\u5224\u65ad\uff09\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5f00\u53d1TableTest\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e3b\u6d41VLMs\u57283D\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63d0\u793a\u8bcd\u7684\u5fae\u5c0f\u903b\u8f91\u7b49\u4ef7\u53d8\u5316\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u63ed\u793a\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "conclusion": "VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u5f85\u6539\u8fdb\uff0c\u9700\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.16064", "pdf": "https://arxiv.org/pdf/2504.16064", "abs": "https://arxiv.org/abs/2504.16064", "authors": ["Theodoros Kouzelis", "Efstathios Karypidis", "Ioannis Kakogeorgiou", "Spyros Gidaris", "Nikos Komodakis"], "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u56fe\u50cf\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u5c42\u6b21\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u548c\u9ad8\u5c42\u6b21\u8bed\u4e49\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u8868\u793a\u5b66\u4e60\u4e0e\u751f\u6210\u5efa\u6a21\u96be\u4ee5\u7ed3\u5408\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u8054\u5408\u5efa\u6a21\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u4f4e\u5c42\u6b21\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u548c\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u7f16\u7801\u5668\uff08\u5982DINO\uff09\u7684\u9ad8\u5c42\u6b21\u8bed\u4e49\u7279\u5f81\u3002", "result": "\u5728\u6761\u4ef6\u548c\u975e\u6761\u4ef6\u8bbe\u7f6e\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u4e3a\u8868\u793a\u611f\u77e5\u7684\u751f\u6210\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u5e76\u89e3\u9501\u4e86\u65b0\u7684\u63a8\u7406\u7b56\u7565\uff08\u5982\u8868\u793a\u5f15\u5bfc\uff09\u3002"}}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.", "AI": {"tldr": "DAM\u6a21\u578b\u901a\u8fc7\u5c40\u90e8\u805a\u7126\u63d0\u793a\u548c\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u63cf\u8ff0\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u6570\u636e\u7ba1\u9053\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u7279\u5b9a\u533a\u57df\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDAM\u6a21\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u805a\u7126\u63d0\u793a\u548c\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff1b\u8bbe\u8ba1\u534a\u76d1\u7763\u5b66\u4e60\u6570\u636e\u7ba1\u9053DLC-SDP\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u65b0\u6700\u4f18\u8868\u73b0\u3002", "conclusion": "DAM\u6a21\u578b\u5728\u5c40\u90e8\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u63cf\u8ff0\u51c6\u786e\u6027\u95ee\u9898\u3002"}}
{"id": "2504.16080", "pdf": "https://arxiv.org/pdf/2504.16080", "abs": "https://arxiv.org/abs/2504.16080", "authors": ["Le Zhuo", "Liangbing Zhao", "Sayak Paul", "Yue Liao", "Renrui Zhang", "Yi Xin", "Peng Gao", "Mohamed Elhoseiny", "Hongsheng Li"], "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "categories": ["cs.CV"], "comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}", "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.", "AI": {"tldr": "ReflectionFlow\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u7ea7\u522b\u3001\u63d0\u793a\u7ea7\u522b\u548c\u53cd\u5c04\u7ea7\u522b\u7684\u6269\u5c55\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u590d\u6742\u573a\u666f\u548c\u7ec6\u8282\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u7ec6\u8282\u8868\u73b0\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u53cd\u601d\u80fd\u529b\u542f\u53d1\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReflectionFlow\u6846\u67b6\uff0c\u5305\u542b\u566a\u58f0\u7ea7\u522b\u3001\u63d0\u793a\u7ea7\u522b\u548c\u53cd\u5c04\u7ea7\u522b\u7684\u6269\u5c55\uff0c\u5e76\u5229\u7528GenRef\u6570\u636e\u96c6\u8fdb\u884c\u53cd\u5c04\u8c03\u4f18\u3002", "result": "ReflectionFlow\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u566a\u58f0\u7ea7\u522b\u6269\u5c55\u65b9\u6cd5\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u65b9\u6848\u3002", "conclusion": "ReflectionFlow\u901a\u8fc7\u591a\u7ea7\u6269\u5c55\u548c\u53cd\u5c04\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.16082", "pdf": "https://arxiv.org/pdf/2504.16082", "abs": "https://arxiv.org/abs/2504.16082", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "categories": ["cs.CV"], "comment": "Preprint", "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video", "AI": {"tldr": "MR. Video\u662f\u4e00\u4e2a\u57fa\u4e8eMapReduce\u539f\u5219\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u611f\u77e5\u77ed\u7247\u6bb5\uff08Map\uff09\u548c\u8054\u5408\u805a\u5408\u4fe1\u606f\uff08Reduce\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5e8f\u5217\u5230\u5e8f\u5217\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u89c6\u9891\u4ee3\u7406\u5728\u957f\u89c6\u9891\u5904\u7406\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u5173\u952e\u7247\u6bb5\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5MapReduce\uff1a1\uff09Captioning\u9636\u6bb5\u751f\u6210\u77ed\u7247\u6bb5\u63cf\u8ff0\u5e76\u6807\u51c6\u5316\uff1b2\uff09Analysis\u9636\u6bb5\u5206\u6790\u7528\u6237\u95ee\u9898\u5e76\u6574\u5408\u7b54\u6848\u3002", "result": "\u5728LVBench\u4e0a\u6bd4\u73b0\u6709VLMs\u548c\u89c6\u9891\u4ee3\u7406\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "MapReduce\u539f\u5219\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8eVLMs\u548c\u89c6\u9891\u4ee3\u7406\u3002"}}
{"id": "2504.16083", "pdf": "https://arxiv.org/pdf/2504.16083", "abs": "https://arxiv.org/abs/2504.16083", "authors": ["Yucheng Li", "Huiqiang Jiang", "Chengruidong Zhang", "Qianhui Wu", "Xufang Luo", "Surin Ahn", "Amir H. Abdi", "Dongsheng Li", "Jianfeng Gao", "Yuqing Yang", "Lili Qiu"], "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.", "AI": {"tldr": "MMInference\u662f\u4e00\u79cd\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u8f93\u5165\u7684\u9884\u586b\u5145\u9636\u6bb5\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u4e0e\u89c6\u89c9\u7406\u89e3\u7684\u7ed3\u5408\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5e26\u6765\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9884\u586b\u5145\u9636\u6bb5\u7684\u4e8c\u6b21\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u5206\u6790\u89c6\u9891\u8f93\u5165\u7684\u65f6\u7a7a\u5c40\u90e8\u6027\uff0c\u53d1\u73b0\u72ec\u7279\u7684Grid\u7a00\u758f\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6392\u5217\u7684\u65b9\u6cd5\u5904\u7406\u6a21\u6001\u8fb9\u754c\u95ee\u9898\u3002\u52a8\u6001\u6784\u5efa\u7a00\u758f\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4f18\u5316\u7684GPU\u5185\u6838\u3002", "result": "\u5728\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMMInference\u5c06\u9884\u586b\u5145\u9636\u6bb5\u52a0\u901f\u9ad8\u8fbe8.3\u500d\uff081M tokens\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "MMInference\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5373\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709VLM\u6d41\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u8f93\u5165\u7684\u6548\u7387\u3002"}}
{"id": "2504.09697", "pdf": "https://arxiv.org/pdf/2504.09697", "abs": "https://arxiv.org/abs/2504.09697", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art\n  Gallery 2025", "summary": "Recent prompt-based image editing models have demonstrated impressive\nprompt-following capability at structural editing tasks. However, existing\nmodels still fail to perform local edits, follow detailed editing prompts, or\nmaintain global image quality beyond a single editing step. To address these\nchallenges, we introduce SPICE, a training-free workflow that accepts arbitrary\nresolutions and aspect ratios, accurately follows user requirements, and\nimproves image quality consistently during more than 100 editing steps. By\nsynergizing the strengths of a base diffusion model and a Canny edge ControlNet\nmodel, SPICE robustly handles free-form editing instructions from the user.\nSPICE outperforms state-of-the-art baselines on a challenging realistic\nimage-editing dataset consisting of semantic editing (object addition, removal,\nreplacement, and background change), stylistic editing (texture changes), and\nstructural editing (action change) tasks. Not only does SPICE achieve the\nhighest quantitative performance according to standard evaluation metrics, but\nit is also consistently preferred by users over existing image-editing methods.\nWe release the workflow implementation for popular diffusion model Web UIs to\nsupport further research and artistic exploration.", "AI": {"tldr": "SPICE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6269\u6563\u6a21\u578b\u548cCanny\u8fb9\u7f18ControlNet\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u6267\u884c\u81ea\u7531\u5f62\u5f0f\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u5e76\u5728100\u591a\u6b65\u7f16\u8f91\u4e2d\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5c40\u90e8\u7f16\u8f91\u3001\u8be6\u7ec6\u63d0\u793a\u9075\u5faa\u548c\u591a\u6b65\u7f16\u8f91\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cSPICE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u6269\u6563\u6a21\u578b\u548cCanny\u8fb9\u7f18ControlNet\u6a21\u578b\uff0c\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\uff0c\u5b9e\u73b0\u81ea\u7531\u5f62\u5f0f\u7684\u7f16\u8f91\u3002", "result": "\u5728\u8bed\u4e49\u3001\u98ce\u683c\u548c\u7ed3\u6784\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0cSPICE\u5728\u5b9a\u91cf\u8bc4\u4f30\u548c\u7528\u6237\u504f\u597d\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SPICE\u4e3a\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u827a\u672f\u63a2\u7d22\u3002"}}
{"id": "2504.15305", "pdf": "https://arxiv.org/pdf/2504.15305", "abs": "https://arxiv.org/abs/2504.15305", "authors": ["Abhishek Tyagi", "Charu Gaur"], "title": "SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "68T40, 68U10, 70Q05", "I.2.9; I.4.8; I.2.10; C.3"], "comment": "18 pages, 21 figures, 4 tables. Onboard processing using Raspberry Pi\n  4 and Arduino Nano. Includes ORB-SLAM3-based navigation, LQR control, rotor\n  fault recovery, object detection, and PCA face recognition. Real-world and\n  simulation tests included. Designed for GPS-denied autonomous UAV\n  surveillance", "summary": "We present an autonomous aerial surveillance platform, Veg, designed as a\nfault-tolerant quadcopter system that integrates visual SLAM for\nGPS-independent navigation, advanced control architecture for dynamic\nstability, and embedded vision modules for real-time object and face\nrecognition. The platform features a cascaded control design with an LQR\ninner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for\n6-DoF localization and loop closure, and supports waypoint-based navigation\nthrough Dijkstra path planning over SLAM-derived maps. A real-time Failure\nDetection and Identification (FDI) system detects rotor faults and executes\nemergency landing through re-routing. The embedded vision system, based on a\nlightweight CNN and PCA, enables onboard object detection and face recognition\nwith high precision. The drone operates fully onboard using a Raspberry Pi 4\nand Arduino Nano, validated through simulations and real-world testing. This\nwork consolidates real-time localization, fault recovery, and embedded AI on a\nsingle platform suitable for constrained environments.", "AI": {"tldr": "Veg\u662f\u4e00\u4e2a\u81ea\u4e3b\u7a7a\u4e2d\u76d1\u89c6\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u89c6\u89c9SLAM\u3001\u5148\u8fdb\u63a7\u5236\u67b6\u6784\u548c\u5d4c\u5165\u5f0f\u89c6\u89c9\u6a21\u5757\uff0c\u652f\u6301GPS\u72ec\u7acb\u5bfc\u822a\u3001\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u7269\u4f53/\u4eba\u8138\u8bc6\u522b\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u6545\u969c\u5bb9\u5fcd\u7684\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u53d7\u9650\u73af\u5883\uff0c\u6574\u5408\u5b9e\u65f6\u5b9a\u4f4d\u3001\u6545\u969c\u6062\u590d\u548c\u5d4c\u5165\u5f0fAI\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u63a7\u5236\u8bbe\u8ba1\uff08LQR\u5185\u73af\u548cPD\u5916\u73af\uff09\uff0cORB-SLAM3\u8fdb\u884c6-DoF\u5b9a\u4f4d\uff0cDijkstra\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7CNN\u548cPCA\u7684\u5d4c\u5165\u5f0f\u89c6\u89c9\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u5e73\u53f0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5b9a\u4f4d\u3001\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u4ee5\u53ca\u9ad8\u7cbe\u5ea6\u7269\u4f53/\u4eba\u8138\u8bc6\u522b\u3002", "conclusion": "Veg\u5e73\u53f0\u6210\u529f\u6574\u5408\u4e86\u591a\u9879\u6280\u672f\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u76d1\u89c6\u4efb\u52a1\u3002"}}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben A\u00efcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u9884\u5904\u7406\u548cSwin Transformer\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728Aptos\u548cIDRiD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523089.65%\u548c97.40%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u5168\u7403\u81f4\u76f2\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5316\u5206\u7c7b\u9762\u4e34\u56fe\u50cf\u8d28\u91cf\u5dee\u5f02\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u50cf\u7d20\u7ea7\u76f8\u4f3c\u6027\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u88c1\u526a\u3001CLAHE\u589e\u5f3a\u548c\u76ee\u6807\u6570\u636e\u589e\u5f3a\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408Swin Transformer\u7684\u5206\u5c42\u4ee4\u724c\u5904\u7406\u548c\u79fb\u4f4d\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728Aptos\u548cIDRiD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b089.65%\u548c97.40%\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u65e9\u671fDR\u68c0\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u81ea\u52a8\u5316\u89c6\u7f51\u819c\u7b5b\u67e5\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u65e9\u671fDR\u68c0\u6d4b\u65b9\u9762\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8ba1\u7b97\u68af\u5ea6\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u65f6\u5fae\u8c03\u5728\u5b9e\u65f6\u6216\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u56e0\u591a\u6b21\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u5bfc\u81f4\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "\u5c06\u68af\u5ea6\u4e0b\u964d\u5efa\u6a21\u4e3aODE\u7684\u6b27\u62c9\u79bb\u6563\u5316\uff0c\u8bad\u7ec3\u8f85\u52a9\u7f51\u7edc\u9884\u6d4b\u4efb\u52a1\u6761\u4ef6\u6f02\u79fb\uff0c\u4ec5\u9700\u5c11\u91cf\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5b8c\u6210\u9002\u5e94\u3002", "result": "\u5728\u8de8\u57df\u5c11\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8ba1\u7b97\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\u4ec5\u4e3a\u6807\u51c6\u5fae\u8c03\u76840.02%\u548c6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u76f4\u63a5\u8fc1\u79fb\u548c\u5b8c\u5168\u5fae\u8c03\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6298\u4e2d\u65b9\u6848\u3002"}}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86OpenAI\u7684\u8ba1\u7b97\u673a\u7528\u6237\u4ee3\u7406\uff08CUA\uff09\u5728Wordle\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u989c\u8272\u8bc6\u522b\u4e0a\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u6210\u529f\u7387\u4ec5\u4e3a5.36%\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001\u4ee3\u7406\uff08\u5982CUA\uff09\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5f53\u524d\u524d\u6cbfAI\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u8ba9CUA\u5728\u7ebd\u7ea6\u65f6\u62a5Wordle\u6e38\u620f\u4e2d\u5b8c\u6210\u4efb\u52a1\uff0c\u5206\u6790\u5176\u884c\u4e3a\u548c\u7f3a\u9677\u3002", "result": "\u6a21\u578b\u5728\u989c\u8272\u8bc6\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4f4e\uff0c\u8868\u660e\u7b80\u5355\u4efb\u52a1\u5bf9AI\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u6f5c\u5728\u539f\u56e0\u3001\u672a\u6765\u53d1\u5c55\u7684\u5f71\u54cd\u53ca\u6539\u8fdbAI\u7cfb\u7edf\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.15479", "pdf": "https://arxiv.org/pdf/2504.15479", "abs": "https://arxiv.org/abs/2504.15479", "authors": ["Jeremy Goldwasser", "Giles Hooker"], "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u907f\u514d\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\u751f\u6210\u5bf9\u6297\u6837\u672c\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u5f52\u56e0\u63d0\u4f9b\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e2d\u53cd\u4e8b\u5b9e\u89e3\u91ca\u751f\u6210\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u907f\u514d\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "method": "\u63d0\u51faCounterfactual Attacks\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u7684\u8868\u793a\u653b\u51fb\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u8f85\u52a9\u6570\u636e\u96c6\u7684\u7279\u5f81\u5f52\u56e0\u91cf\u5316\u53d8\u5316\u3002", "result": "\u5728MNIST\u548cCelebA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15481", "pdf": "https://arxiv.org/pdf/2504.15481", "abs": "https://arxiv.org/abs/2504.15481", "authors": ["Michel Berthier", "Nicoletta Prencipe", "Edoardo Provenzi"], "title": "Split-quaternions for perceptual white balance", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a perceptual chromatic adaptation transform for white balance that\nmakes use of split-quaternions. The novelty of the present work, which is\nmotivated by a recently developed quantum-like model of color perception,\nconsists at stressing the link between the algebraic structures appearing in\nthis model and a certain sub-algebra of the split-quaternions. We show the\npotentiality of this approach for color image processing applications by\nproposing a chromatic adaptation transform, implemented via an appropriate use\nof the split-quaternion multiplication. Moreover, quantitative comparisons with\nthe widely used state-of-the art von Kries chromatic adaptation transform are\nprovided.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u88c2\u56db\u5143\u6570\u7684\u611f\u77e5\u8272\u9002\u5e94\u53d8\u6362\uff0c\u7528\u4e8e\u767d\u5e73\u8861\uff0c\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u3002", "motivation": "\u53d7\u91cf\u5b50\u5316\u989c\u8272\u611f\u77e5\u6a21\u578b\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u4ee3\u6570\u7ed3\u6784\u4e0e\u5206\u88c2\u56db\u5143\u6570\u7684\u8054\u7cfb\u3002", "method": "\u5229\u7528\u5206\u88c2\u56db\u5143\u6570\u4e58\u6cd5\u5b9e\u73b0\u8272\u9002\u5e94\u53d8\u6362\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5f69\u8272\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e0evon Kries\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u5206\u88c2\u56db\u5143\u6570\u65b9\u6cd5\u5728\u8272\u9002\u5e94\u53d8\u6362\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15496", "pdf": "https://arxiv.org/pdf/2504.15496", "abs": "https://arxiv.org/abs/2504.15496", "authors": ["Eammon A. Littler", "Emmanuel A. Mannoh", "Ethan P. M. LaRochelle"], "title": "Fluorescence Reference Target Quantitative Analysis Library", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "comment": "12 pages, 1 table, 4 figures. Code available:\n  https://github.com/QUEL-Imaging/quel-qal), PyPi: quel-qal", "summary": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems.", "AI": {"tldr": "QUEL-QAL\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5e93\uff0c\u65e8\u5728\u6807\u51c6\u5316\u8367\u5149\u6210\u50cf\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u652f\u6301\u5173\u952e\u6307\u6807\u5206\u6790\u5e76\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u8367\u5149\u5f15\u5bfc\u624b\u672f\uff08FGS\uff09\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\uff0c\u73b0\u6709\u65b9\u6cd5\u4e0d\u4e00\u81f4\u4e14\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5f00\u53d1\u4e86QUEL-QAL\u5e93\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u5305\u62ecROI\u68c0\u6d4b\u3001\u7edf\u8ba1\u5206\u6790\u548c\u53ef\u89c6\u5316\u529f\u80fd\u3002", "result": "\u652f\u6301\u54cd\u5e94\u7ebf\u6027\u3001\u68c0\u6d4b\u9650\u3001\u6df1\u5ea6\u654f\u611f\u6027\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u7b49\u5173\u952e\u6307\u6807\uff0c\u7b26\u5408\u76d1\u7ba1\u548c\u5b66\u672f\u8981\u6c42\u3002", "conclusion": "QUEL-QAL\u4e3a\u6807\u51c6\u5316\u8367\u5149\u6210\u50cf\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u5177\uff0c\u4fc3\u8fdb\u900f\u660e\u5ea6\u548c\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2504.15545", "pdf": "https://arxiv.org/pdf/2504.15545", "abs": "https://arxiv.org/abs/2504.15545", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u75c5\u7406\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\uff08VLM\uff09\u7684\u865a\u62df\u67d3\u8272\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u63d0\u793a\u548c\u6982\u5ff5\u951a\u70b9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u865a\u62df\u67d3\u8272\u4e2d\u5ffd\u7565\u75c5\u7406\u77e5\u8bc6\u548c\u7269\u7406\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u4ec5\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\uff0c\u5ffd\u7565\u4e86\u7ec4\u7ec7\u5207\u7247\u7684\u57fa\u672c\u89c6\u89c9\u7279\u5f81\u548c\u67d3\u8272\u5242\u7684\u7269\u7406\u7279\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u5f15\u5165\u75c5\u7406VLM\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u63d0\u793a\u3001\u7ec4\u7ec7\u57fa\u7840\u6982\u5ff5\u951a\u70b9\u548c\u67d3\u8272\u7279\u5f02\u6027\u6982\u5ff5\u951a\u70b9\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eVLM\u7ea6\u675f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u7684\u56fe\u50cf\u9ad8\u5ea6\u771f\u5b9e\uff0c\u5e76\u63d0\u5347\u4e86\u80be\u5c0f\u7403\u68c0\u6d4b\u548c\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u75c5\u7406\u77e5\u8bc6\u548c\u7269\u7406\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u67d3\u8272\u7684\u6548\u679c\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15562", "pdf": "https://arxiv.org/pdf/2504.15562", "abs": "https://arxiv.org/abs/2504.15562", "authors": ["Dip Roy"], "title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "categories": ["cs.LG", "cs.CV"], "comment": "16 pages, 6 figures", "summary": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u8111\u90e8MRI\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5bf9\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u786e\u5b9a\u6027\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u4f30\u8ba1\u8ba4\u77e5\u548c\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728BraTS2020\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cROC AUC\u548cPR AUC\u5747\u4e3a0.83\u3002", "conclusion": "\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u662f\u5f02\u5e38\u68c0\u6d4b\u7684\u5173\u952e\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u4fe1\u5fc3\u4f30\u8ba1\u3002"}}
{"id": "2504.15594", "pdf": "https://arxiv.org/pdf/2504.15594", "abs": "https://arxiv.org/abs/2504.15594", "authors": ["Tatsuhito Hasegawa", "Shunsuke Sakai"], "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": "22 pages, 11 figures, under review", "summary": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0softmax\u6e29\u5ea6\u53c2\u6570$T^*$\u4e0e\u7279\u5f81\u7ef4\u5ea6\u76f8\u5173\uff0c\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u786e\u5b9a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u6e29\u5ea6\u53c2\u6570$T^*$\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u53ca\u5176\u786e\u5b9a\u65b9\u6cd5\uff0c\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d$T^*$\u6ce2\u52a8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u7ef4\u5ea6\u7684\u6e29\u5ea6\u786e\u5b9a\u7cfb\u6570\uff0c\u52a0\u5165\u6279\u5f52\u4e00\u5316\u5c42\u7a33\u5b9a\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u4f18\u5316\u516c\u5f0f\u3002", "result": "\u63a8\u5bfc\u7684\u6e29\u5ea6\u53c2\u6570\u4e0e\u7406\u8bba\u4e00\u81f4\uff0c\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6e29\u5ea6\u53c2\u6570\u786e\u5b9a\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2504.15616", "pdf": "https://arxiv.org/pdf/2504.15616", "abs": "https://arxiv.org/abs/2504.15616", "authors": ["Kai Chen", "Xiaodong Zhao", "Yujie Huang", "Guoyu Fang", "Xiao Song", "Ruiping Wang", "Ziyuan Wang"], "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages,6 figures", "summary": "The analysis and prediction of agent trajectories are crucial for\ndecision-making processes in intelligent systems, with precise short-term\ntrajectory forecasting being highly significant across a range of applications.\nAgents and their social interactions have been quantified and modeled by\nresearchers from various perspectives; however, substantial limitations exist\nin the current work due to the inherent high uncertainty of agent intentions\nand the complex higher-order influences among neighboring groups. SocialMOIF is\nproposed to tackle these challenges, concentrating on the higher-order\nintention interactions among neighboring groups while reinforcing the primary\nrole of first-order intention interactions between neighbors and the target\nagent. This method develops a multi-order intention fusion model to achieve a\nmore comprehensive understanding of both direct and indirect intention\ninformation. Within SocialMOIF, a trajectory distribution approximator is\ndesigned to guide the trajectories toward values that align more closely with\nthe actual data, thereby enhancing model interpretability. Furthermore, a\nglobal trajectory optimizer is introduced to enable more accurate and efficient\nparallel predictions. By incorporating a novel loss function that accounts for\ndistance and direction during training, experimental results demonstrate that\nthe model outperforms previous state-of-the-art baselines across multiple\nmetrics in both dynamic and static datasets.", "AI": {"tldr": "SocialMOIF\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u610f\u56fe\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u667a\u80fd\u7cfb\u7edf\u4e2d\u4ee3\u7406\u8f68\u8ff9\u7684\u5206\u6790\u548c\u9884\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u76f4\u63a5\u548c\u95f4\u63a5\u610f\u56fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u8f68\u8ff9\u9884\u6d4b\u5b58\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u9ad8\u9636\u4ea4\u4e92\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u610f\u56fe\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u591a\u9636\u610f\u56fe\u878d\u5408\u6a21\u578b\uff0c\u8bbe\u8ba1\u8f68\u8ff9\u5206\u5e03\u8fd1\u4f3c\u5668\u548c\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u5f15\u5165\u8003\u8651\u8ddd\u79bb\u548c\u65b9\u5411\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u52a8\u6001\u548c\u9759\u6001\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "SocialMOIF\u901a\u8fc7\u878d\u5408\u591a\u9636\u610f\u56fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.15649", "pdf": "https://arxiv.org/pdf/2504.15649", "abs": "https://arxiv.org/abs/2504.15649", "authors": ["Biao Wu", "Diankai Zhang", "Shaoli Liu", "Si Gao", "Chengjian Zheng", "Ning Wang"], "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Champion Solution for CVPR 2025 MAI VSR Track", "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRepNet-VSR\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5b9e\u65f64x\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u79fb\u52a8\u89c6\u9891\u5904\u7406\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u53ef\u91cd\u53c2\u6570\u5316\u67b6\u6784\uff08RepNet-VSR\uff09\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728REDS\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u5904\u7406180p\u5230720p\u5e27\u65f6\u8fbe\u523027.79 dB PSNR\uff0c\u6bcf10\u5e27\u8017\u65f6103\u6beb\u79d2\u3002", "conclusion": "RepNet-VSR\u5728\u6062\u590d\u8d28\u91cf\u548c\u90e8\u7f72\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u6027\u80fd\u4f18\u4e8e\u4e4b\u524d\u7684\u51a0\u519b\u7b97\u6cd5\u3002"}}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578bAI\u89c6\u89c9\u8f85\u52a9\u513f\u7ae5\u5047\u80a2\u624b\uff0c\u4e13\u4e3a10-12\u5c81\u4e0a\u80a2\u6b8b\u75be\u513f\u7ae5\u8bbe\u8ba1\uff0c\u5177\u6709\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u5316\u548c\u4eff\u751f\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u808c\u7535\u5047\u80a2\u7684\u9ad8\u6210\u672c\u548c\u529f\u80fd\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u4f4e\u6536\u5165\u5bb6\u5ead\u63d0\u4f9b\u53ef\u8d1f\u62c5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u54083D\u6253\u5370\u6280\u672f\u3001\u673a\u5668\u89c6\u89c9\u3001\u4f20\u611f\u548c\u5d4c\u5165\u5f0f\u8ba1\u7b97\uff0c\u91c7\u7528\u4f4e\u529f\u8017FPGA\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u548c\u7cbe\u786e\u6293\u53d6\u3002", "result": "\u7269\u4f53\u68c0\u6d4b\u548c\u6293\u53d6\u5206\u7c7b\u6a21\u578b\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523096%\u548c100%\uff0c\u529b\u9884\u6d4b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.018\u3002", "conclusion": "\u8be5\u5047\u80a2\u624b\u901a\u8fc7AI\u89c6\u89c9\u548c\u4f4e\u529f\u8017\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u513f\u7ae5\u5047\u80a2\u9886\u57df\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15664", "pdf": "https://arxiv.org/pdf/2504.15664", "abs": "https://arxiv.org/abs/2504.15664", "authors": ["Phuong Quynh Le", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "title": "An XAI-based Analysis of Shortcut Learning in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at The World Conference on eXplainable Artificial\n  Intelligence 2025 (XAI-2025)", "summary": "Machine learning models tend to learn spurious features - features that\nstrongly correlate with target labels but are not causal. Existing approaches\nto mitigate models' dependence on spurious features work in some cases, but\nfail in others. In this paper, we systematically analyze how and where neural\nnetworks encode spurious correlations. We introduce the neuron spurious score,\nan XAI-based diagnostic measure to quantify a neuron's dependence on spurious\nfeatures. We analyze both convolutional neural networks (CNNs) and vision\ntransformers (ViTs) using architecture-specific methods. Our results show that\nspurious features are partially disentangled, but the degree of disentanglement\nvaries across model architectures. Furthermore, we find that the assumptions\nbehind existing mitigation methods are incomplete. Our results lay the\ngroundwork for the development of novel methods to mitigate spurious\ncorrelations and make AI models safer to use in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdXAI\u8bca\u65ad\u65b9\u6cd5\uff08\u795e\u7ecf\u5143\u865a\u5047\u5206\u6570\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u5143\u5bf9\u865a\u5047\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u5e76\u5206\u6790\u4e86CNN\u548cViT\u4e2d\u865a\u5047\u7279\u5f81\u7684\u89e3\u8026\u7a0b\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u6a21\u578b\u5bf9\u865a\u5047\u7279\u5f81\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u7f16\u7801\u865a\u5047\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u5143\u865a\u5047\u5206\u6570\uff0c\u7ed3\u5408\u67b6\u6784\u7279\u5b9a\u65b9\u6cd5\u5206\u6790CNN\u548cViT\u3002", "result": "\u865a\u5047\u7279\u5f81\u90e8\u5206\u89e3\u8026\uff0c\u4f46\u7a0b\u5ea6\u56e0\u67b6\u6784\u800c\u5f02\uff1b\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7684\u5047\u8bbe\u4e0d\u5b8c\u6574\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u65b0\u65b9\u6cd5\u7f13\u89e3\u865a\u5047\u76f8\u5173\u6027\u5960\u5b9a\u57fa\u7840\uff0c\u63d0\u5347AI\u6a21\u578b\u5b89\u5168\u6027\u3002"}}
{"id": "2504.15667", "pdf": "https://arxiv.org/pdf/2504.15667", "abs": "https://arxiv.org/abs/2504.15667", "authors": ["Jingchen Zou", "Jianqiang Li", "Gabriel Jimenez", "Qing Zhao", "Daniel Racoceanu", "Matias Cosarinsky", "Enzo Ferrante", "Guanghui Fu"], "title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8bc4\u4f30\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u6027\u80fd\u7684\u6846\u67b6SPE\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6307\u6807\u548c\u6a21\u578b\u67b6\u6784\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u53ef\u9760\u3002", "motivation": "\u5728\u4e34\u5e8a\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u6807\u6ce8\u6240\u6709\u6570\u636e\u4e0d\u73b0\u5b9e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86Segmentation Performance Evaluator (SPE)\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cSPE\u4e0e\u771f\u5b9eDice\u5206\u6570\u76f8\u5173\u6027\u9ad8\uff080.956\u00b10.046\uff09\uff0cMAE\u4f4e\uff080.025\u00b10.019\uff09\u3002", "conclusion": "SPE\u80fd\u53ef\u9760\u4f30\u8ba1\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u6807\u6ce8\uff0c\u4fbf\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.15899", "pdf": "https://arxiv.org/pdf/2504.15899", "abs": "https://arxiv.org/abs/2504.15899", "authors": ["Blerim Abdullai", "Tony Wang", "Xinyuan Qiao", "Florian Shkurti", "Timothy D. Barfoot"], "title": "RaSCL: Radar to Satellite Crossview Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56GNSS\u7684\u5168\u5c40\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5730\u9762\u96f7\u8fbe\u4e0e\u9ad8\u7a7aRGB\u56fe\u50cf\u7684\u914d\u51c6\uff0c\u7ed3\u5408\u91cc\u7a0b\u8ba1\u548c\u5168\u5c40\u4f4d\u59ff\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "GNSS\u5728\u8bb8\u591a\u5b9e\u65f6\u81ea\u4e3b\u5e94\u7528\u4e2d\u4e0d\u53ef\u9760\u3001\u4e0d\u51c6\u786e\u4e14\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66ff\u4ee3\u7684\u5168\u5c40\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5730\u9762\u96f7\u8fbe\u4e0e\u9ad8\u7a7aRGB\u56fe\u50cf\u7684\u914d\u51c6\uff0c\u7ed3\u5408\u91cc\u7a0b\u8ba1\u548c\u5168\u5c40\u4f4d\u59ff\u4f18\u5316\uff0c\u63d0\u53d6RGB\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u7279\u5f81\u8fdb\u884c\u5b9a\u4f4d\u3002", "result": "\u5728\u591a\u79cd\u5730\u7406\u6761\u4ef6\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff08\u5982\u65e0\u4eba\u6c34\u9762\u8247\u3001\u57ce\u5e02\u548c\u90ca\u533a\u9a7e\u9a76\u6570\u636e\u96c6\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56GNSS\u7684\u9ad8\u6548\u5168\u5c40\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2504.15953", "pdf": "https://arxiv.org/pdf/2504.15953", "abs": "https://arxiv.org/abs/2504.15953", "authors": ["Chance J. Hamilton", "Alfredo Weitzenfeld"], "title": "Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping.", "AI": {"tldr": "VPCE\u6a21\u578b\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u6a21\u62df\u4f4d\u7f6e\u7ec6\u80de\u6fc0\u6d3b\uff0c\u5229\u7528\u89c6\u89c9\u5730\u6807\u8fdb\u884c\u7a7a\u95f4\u7f16\u7801\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u533a\u5206\u76f8\u4f3c\u4f46\u7a7a\u95f4\u4e0d\u540c\u7684\u4f4d\u7f6e\uff0c\u5e76\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u5730\u6807\u5728\u7a7a\u95f4\u7f16\u7801\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u9a8c\u8bc1\u4ec5\u51ed\u89c6\u89c9\u8f93\u5165\u662f\u5426\u80fd\u751f\u6210\u7c7b\u4f3c\u751f\u7269\u4f4d\u7f6e\u7ec6\u80de\u7684\u7a7a\u95f4\u8868\u5f81\u3002", "method": "\u901a\u8fc7\u673a\u5668\u4eba\u6444\u50cf\u5934\u6355\u83b7\u56fe\u50cf\uff0c\u63d0\u53d6\u9ad8\u7ef4\u5916\u89c2\u7279\u5f81\u5e76\u805a\u7c7b\uff0c\u4f7f\u7528\u5f84\u5411\u57fa\u51fd\u6570\u8ba1\u7b97\u6fc0\u6d3b\uff0c\u8bc4\u4f30\u5176\u4e0e\u751f\u7269\u4f4d\u7f6e\u7ec6\u80de\u7279\u6027\u7684\u76f8\u5173\u6027\u3002", "result": "VPCE\u80fd\u533a\u5206\u7a7a\u95f4\u4e0d\u540c\u7684\u89c6\u89c9\u76f8\u4f3c\u4f4d\u7f6e\uff0c\u9002\u5e94\u73af\u5883\u53d8\u5316\uff08\u5982\u5899\u58c1\u589e\u51cf\uff09\uff0c\u751f\u6210\u7c7b\u4f3c\u751f\u7269\u4f4d\u7f6e\u7ec6\u80de\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u6784\u5316\u89c6\u89c9\u8f93\u5165\u8db3\u4ee5\u751f\u6210\u4f4d\u7f6e\u7ec6\u80de\u6837\u7a7a\u95f4\u8868\u5f81\uff0c\u652f\u6301\u751f\u7269\u542f\u53d1\u7684\u8ba4\u77e5\u6620\u5c04\uff0c\u65e0\u9700\u8fd0\u52a8\u7ebf\u7d22\u6216\u5956\u52b1\u9a71\u52a8\u5b66\u4e60\u3002"}}
{"id": "2504.15970", "pdf": "https://arxiv.org/pdf/2504.15970", "abs": "https://arxiv.org/abs/2504.15970", "authors": ["Baichuan Zeng"], "title": "Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence", "categories": ["cs.HC", "cs.CV", "cs.MA"], "comment": "7 pages,4 figures", "summary": "Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality\n(VR) and Mixed Reality (MR), is a transformative technology bridging the\nphysical and virtual world and it has diverse potential which will be\nubiquitous in the future. This review examines XR's evolution through\nfoundational framework - hardware ranging from monitors to sensors and software\nranging from visual tasks to user interface; highlights state of the art (SOTA)\nXR products with the comparison and analysis of performance based on their\nfoundational framework; discusses how commercial XR devices can support the\ndemand of high-quality performance focusing on spatial intelligence. For future\ndirections, attention should be given to the integration of multi-modal AI and\nIoT-driven digital twins to enable adaptive XR systems. With the concept of\nspatial intelligence, future XR should establish a new digital space with\nrealistic experience that benefits humanity. This review underscores the\npivotal role of AI in unlocking XR as the next frontier in human-computer\ninteraction.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6db5\u76d6\u786c\u4ef6\u3001\u8f6f\u4ef6\u53ca\u6700\u65b0\u4ea7\u54c1\uff0c\u5f3a\u8c03\u7a7a\u95f4\u667a\u80fd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u671b\u4e86AI\u4e0eIoT\u9a71\u52a8\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8XR\u6280\u672f\u7684\u6f14\u53d8\u53ca\u5176\u5728\u7269\u7406\u4e0e\u865a\u62df\u4e16\u754c\u4e2d\u7684\u6865\u6881\u4f5c\u7528\uff0c\u5206\u6790\u5176\u672a\u6765\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790XR\u7684\u786c\u4ef6\u3001\u8f6f\u4ef6\u6846\u67b6\u53ca\u73b0\u6709\u4ea7\u54c1\u6027\u80fd\uff0c\u7ed3\u5408\u7a7a\u95f4\u667a\u80fd\u9700\u6c42\uff0c\u63d0\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "result": "XR\u6280\u672f\u9700\u7ed3\u5408\u591a\u6a21\u6001AI\u548cIoT\u9a71\u52a8\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "AI\u5c06\u6210\u4e3aXR\u6280\u672f\u53d1\u5c55\u7684\u5173\u952e\uff0c\u63a8\u52a8\u5176\u6210\u4e3a\u4eba\u673a\u4ea4\u4e92\u7684\u65b0\u524d\u6cbf\u3002"}}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4e14\u58f0\u660e\u5f0f\u7684\u65b9\u6cd5\u6765\u8868\u793a\u9012\u5f52\u7ed3\u6784\u7684\u56fe\u6a21\u5f0f\u8bed\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u56fe\u8bed\u6cd5\u7684\u4ea7\u751f\u5f0f\u89c4\u5219\uff0c\u5c06\u8bed\u6cd5\u548c\u6a21\u5f0f\u8868\u793a\u4e3a\u7f51\u7edc\uff0c\u5e76\u5c06\u89e3\u6790\u89c6\u4e3a\u4ece\u6a21\u5f0f\u5230\u8bed\u6cd5\u7684\u540c\u6001\u6784\u9020\u3002", "motivation": "\u4f20\u7edf\u56fe\u8bed\u6cd5\u4f7f\u7528\u4ea7\u751f\u5f0f\u89c4\u5219\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u76f4\u63a5\u548c\u58f0\u660e\u5f0f\u7684\u65b9\u6cd5\u8868\u793a\u9012\u5f52\u7ed3\u6784\u7684\u56fe\u6a21\u5f0f\u8bed\u6cd5\uff0c\u4ee5\u652f\u6301\u5e76\u884c\u89e3\u6790\u548c\u591a\u7ef4\u9012\u5f52\u7ed3\u6784\u7684\u8868\u793a\u3002", "method": "\u5c06\u8bed\u6cd5\u548c\u6a21\u5f0f\u8868\u793a\u4e3a\u7f51\u7edc\uff0c\u89e3\u6790\u8fc7\u7a0b\u6784\u9020\u4ece\u6a21\u5f0f\u5230\u8bed\u6cd5\u7684\u540c\u6001\uff0c\u652f\u6301\u5e76\u884c\u89e3\u6790\u5e76\u96c6\u6210\u7279\u5f81\u68c0\u6d4b\u3001\u5206\u5272\u3001\u89e3\u6790\u7b49\u6b65\u9aa4\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5bb9\u5fcd\u9519\u8bef\u5e76\u89e3\u6790\u590d\u6742\u7684\u9012\u5f52\u7ed3\u6784\u6a21\u5f0f\uff0850-1000\u7b26\u53f7\uff09\uff0c\u5904\u7406\u51e0\u4f55\u5173\u7cfb\u53d8\u5316\u3001\u6a21\u7cca\u7b26\u53f7\u3001\u91cd\u53e0\u7b26\u53f7\u3001\u6742\u4e71\u56fe\u50cf\u548c\u7f3a\u5931\u533a\u57df\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u9012\u5f52\u7ed3\u6784\u56fe\u6a21\u5f0f\u7684\u89e3\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5e76\u884c\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u5176\u5904\u7406\u590d\u6742\u6a21\u5f0f\u7684\u80fd\u529b\u3002"}}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.", "AI": {"tldr": "ForesightNav\u662f\u4e00\u79cd\u53d7\u4eba\u7c7b\u60f3\u8c61\u548c\u63a8\u7406\u542f\u53d1\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u63a2\u7d22\u533a\u57df\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u5360\u7528\u548c\u8bed\u4e49\u7ec6\u8282\uff09\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u4ee5\u5f00\u53d1\u5177\u5907\u7c7b\u4f3c\u80fd\u529b\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u3002", "method": "\u63d0\u51faForesightNav\uff0c\u8d4b\u4e88\u673a\u5668\u4eba\u9884\u6d4b\u672a\u63a2\u7d22\u533a\u57df\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ece\u800c\u9009\u62e9\u6709\u610f\u4e49\u7684\u957f\u671f\u5bfc\u822a\u76ee\u6807\u3002", "result": "\u5728Structured3D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u51c6\u786e\u7684\u5360\u7528\u9884\u6d4b\u548c\u4f18\u8d8a\u7684\u573a\u666f\u51e0\u4f55\u9884\u6d4b\u6027\u80fd\uff0c\u63a2\u7d22\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u60f3\u8c61\u9a71\u52a8\u7684\u63a8\u7406\u80fd\u589e\u5f3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a2\u7d22\u6548\u7387\u3002"}}
